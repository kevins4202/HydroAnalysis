index,text
25865,ensemble tree machine learning ml regression models can be prone to systematic bias small values are overestimated and large values are underestimated additional bias can be introduced if the dependent variable is a transform of the original data six methods were evaluated for their ability to correct systematic and introduced bias method performance was evaluated using four case studies of groundwater quality the units of the dependent variable were ph in two and log concentration in the others when performance metrics bias and rmse for both points and the cdf were computed using the same units as those in the ml model empirical distribution matching edm provided the best results when the metrics were computed using retransformed concentration edm and a method incorporating duan s smearing estimate were both effective a method based on the z score transform approximates edm if the correlation coefficient between rank ordered ml estimates and rank ordered observations approaches one keywords machine learning bias correction ensemble tree methods groundwater water quality 1 introduction ensemble tree machine learning ml models such as random forest rf and boosted regression trees brt are increasingly being applied to the modeling of environmental systems ensemble tree models can be applied to problems of both classification and regression this paper focuses on regression numerous examples of ensemble tree regression models are available in the literature recent examples published in this journal include identification of landslide hazard in the u s pacific northwest stanley et al 2020 correction of digital elevation models in the coastal everglades cooper et al 2019 installation of solar capacity in germany frey et al 2019 urban household water demand in tampa florida duerr et al 2018 low stream flow conditions in the southeastern u s worland et al 2018 sponge species richness in the oceanic shoals commonwealth marine reserve australia li et al 2017 and metamodeling of groundwater flow to wells lake michigan basin u s fienen et al 2016 the u s geological survey usgs national water quality assessment project nawqa has also been using ensemble tree ml regression models particularly for the estimation and mapping of groundwater quality at regional and national scales these studies have focused on constituents that are prevalent at concentrations of concern e g ransom et al 2017 knierim et al 2020 and on the factors affecting groundwater quality e g fienen et al 2018 belitz et al 2019 the work presented in this paper is a part of the usgs nawqa project and is motivated in part by the results from our ongoing studies that include ensemble tree ml regression modeling the estimates provided by ensemble tree ml regression models are generally unbiased in the sense that the sum of the errors observed values compared to estimated is close to zero hastie et al 2009 however ml regression models can produce results that are biased in a different sense zhang and lu 2012 small values are overestimated and large values are underestimated fig 1 for many purposes including mapping of groundwater quality it is important to correctly estimate one or both tails of a distribution for example if one is interested in the suitability of groundwater for human use then it is important to correctly identify the size and location of areas where concentrations are above a threshold of concern u s epa 2020a norman et al 2018 alternatively it might be important to identify areas where groundwater ph is outside the range 6 5 8 5 u s epa 2020b in addition it might be important to identify geochemical conditions that can affect lead concentrations jurgens et al 2019 if the ml models are systematically biased overestimating small values and underestimating large values then the resulting maps can be incorrect consequently the primary objective of this paper is to evaluate methods for correcting the systematic bias introduced by ensemble tree ml models an additional potential source of bias can be introduced into statistical models including ensemble tree ml models if the dependent variable is a transform of the original data bias in the estimate of the mean can be introduced when model estimates are retransformed back into the original units duan 1983 developed an approach for addressing the bias in the mean under the assumption that the residuals are homoscedastic and independent of the estimated values helsel et al 2020 this assumption is not true if there is systematic overestimation of small values and underestimation of large values a secondary objective of this paper is evaluation of bias correction methods for ml models developed using natural log transformed dependent variables bias correction can be viewed from two perspectives the point scale and the distribution scale the point scale approach adjusts estimated values so that they better match their associated observed value the regression line would be colinear with the one to one line on a scatter plot fig 1a the distribution scale approach adjusts estimated values so that their cumulative distribution matches the observed distribution fig 1b both perspectives are considered in this paper the point scale correction of bias in ml models has been addressed in the statistical literature the randomforest package liaw and wiener 2002 incorporates an approach based on linear regression of observations on estimated values with the coefficients computed using out of bag data the resulting equation is then used for adjusting estimates provided by the rf model zhang and lu 2012 generalized the regression approach and recommended using all of the data rather than out of bag data for minimizing prediction error zhang and lu 2012 also developed an alternative approach in which a second rf model is used to estimate the residuals associated with the initial model the second model can then be used for correcting the bias ghosal and hooker 2018 subsequently defined the approach using two rf models as one step boosted forests osbf several studies zhang and lu 2012 xu 2013 song 2015 ghosal and hooker 2018 have shown that osbf with mean square error as the performance metric outperforms regression for correcting point scale bias in rf models point scale bias correction methods have been applied to ml models developed for environmental applications the regression approach has been used to correct the bias in rf models of the expansion of urban areas in china huang et al 2016 soil geochemical mapping in southwest england kirkwood et al 2016 salinity loading in the upper colorado river basin nauman et al 2019 and concentrations of total phosphorus and total nitrogen in baseflow in the western us olsen and hawkin 2020 the regression approach has also been applied to a brt model of global fish stock depletion zhou et al 2017 the osbf approach utilizing two rf models has been used to correct bias in models of drought indices in southeast australia feng et al 2019 and the global export of silica by rivers phillips 2020 the regression approach and an approach comparable to osbf are both examined in this paper distribution scale bias correction the matching of cumulative distribution functions cdfs has not been broadly applied to ml models developed for environmental systems but it has been incorporated into other applications of environmental modeling and measurement these applications include downscaling of climate models wood et al 2004 assimilation of satellite measurements and ground based information reichle and koster 2004 forecasting of streamflow from an ensemble of models hashino et al 2007 and modeling of soil hydraulic properties liao et al 2017 the one example known to the authors of distribution scale bias correction to an ml application in environmental modeling is the adjustment of output from an artificial neural network ann model to match field soil moisture characteristic properties jana et al 2008 ann modeling is not an ensemble tree approach the distribution scale approach presented in this paper relies upon contributions from climate modeling research in that context the matching of cdfs has been referred to as equiprobability transformation quantile mapping probability mapping and distribution matching panofsky and brier 1968 mcginnis et al 2015 the basic idea is that a transfer function is established that transforms model output so that the resulting cdf matches that of an observed dataset the transfer function could be empirical or parametric piani et al 2010 gudmundsson et al 2012 the empirical approach can utilize all of the data to define the cdf or selected percentiles can be used to approximate it kotlarski et al 2017 the parametric approach can be based on a probability distribution or an appropriate functional relationship piani et al 2010 gudmundsson et al 2012 this paper examines empirical and parametric distribution scale approaches four sections follow this introduction section 2 provides a discussion of the methods used to assess and correct for bias section 3 briefly describes the data from four previously published brt case studies that are used in section 4 for assessing the different bias correction methods conclusions are presented in section 5 although the case studies are drawn from usgs nawqa groundwater quality studies the results of this paper are applicable to a wide range of environmental systems 2 methods nomenclature performance metrics and bias correction this section is divided into five subsections the first subsection introduces nomenclature and the second presents performance metrics for assessing bias the third and longest subsection describes several bias correction methods that are subsequently applied to case studies the last two subsections provide additional discussion of selected bias correction methods 2 1 nomenclature observations of the dependent variable are denoted as y obs and estimated values are denoted as y est the empirical cumulative distribution function ecdf is denoted as f y estimated values can be derived from ml models y ml or can be bias corrected values y c obtained by adjusting y ml several methods for bias correction are evaluated in this paper and various subscripts are introduced to correspond with the different methods if the dependent variable is modeled using natural log transformed values then the exponentiated value of y is denoted as w in this paper the values of y or w are water quality parameters measured in samples collected from groundwater wells that are distributed across the landscape 2 2 metrics for assessing bias the bias associated with a set of estimates can be quantified from two perspectives 1 the distribution scale and 2 the point scale the subscripts f and y are used to denote the distribution scale and point scale respectively 2 2 1 metrics computed at the distribution scale at the distribution scale the bias can be assessed by comparing the ecdf for the estimates to the ecdf for the observed values the comparison between the two can be defined in terms of the horizontal distance between them at a given value of p 1a ε f p q o b s p q e s t p the horizontal distance or error ε f p has the same units as the x axis of the ecdf plot q p the quantile function is the inverse of the ecdf in turn two metrics for assessing the match between ecdfs can be defined the root mean square error rmse f and the first order bias bias f 1b r m s e f e ε f 2 1c b i a s f e ε f note that a vertical line can have a bias f equal to zero hence the rmse f provides a better characterization of the fit between observed and estimated ecdfs for computational purposes ε f and ε f 2 were calculated for probability values distributed uniformly in the range 0 1 using increments of 0 01 the generic r command quantile version 3 4 3 r core team 2017 was used which uses linear interpolation to calculate values of q p based on the empirical distribution y f y the plotting position was based on the hazen formula helsel et al 2020 which is specified as type 5 in the quantile command 2 2 2 metrics computed at the point scale the point scale error ε y associated with an estimated value is 2a ε y y o b s y e s t the corresponding point scale metrics are 2b r m s e y e ε y 2 2c b i a s y e ε y bias y quantifies the bias in the mean but does not necessarily address the overestimation of small values and underestimation of large values as illustrated in fig 1a rmse y is more appropriate for that purpose zhang and lu 2012 2 2 3 application of metrics the four metrics rmse f bias f rmse y bias y can be applied to uncorrected estimates y ml and to corrected estimates y c the four metrics can also be applied to the training data y tr and to the holdout data y ho identified in the development of the ml model values computed for the uncorrected ml estimates provide context for assessing the performance of the various bias correction methods the distribution scale metrics rmse f bias f can be applied not only to the assessment of distribution scale bias correction methods but also to the assessment of point scale methods likewise the point scale metrics rmse y bias y can be used for the assessment of methods of either type 2 3 bias correction methods six bias correction methods are evaluated for each method the parameters or coefficients needed for implementing the method are derived using training data the methods can then be applied to the training data and to the holdout data 2 3 1 empirical distribution matching edm empirical distribution matching edm as the name implies is implemented at the distribution scale in the context of ml modeling the edm method is implemented by obtaining two ecdfs fig 2 one for the ml estimates f m l t r and one for the observed values f o b s t r in turn one can obtain the inverse ecdf for the observed values q o b s t r for any given estimate y ml the bias corrected value y edm is 3a y e d m q o b s t r f m l t r y m l in the context of ml modeling the edm method is simple to apply after the model is trained there are two vectors y m l t r and y o b s t r of equal length each vector is ranked from smallest to largest and a set of ordered pairs is obtained by matching corresponding points of corresponding rank the value of y o b s t r that is paired with y m l t r is defined as y e d m t r the set of ordered pairs y m l t r y e d m t r provides the empirical basis for correcting any given value of y ml fig 3 the set of ordered pairs can also be viewed as a look up table if the values to be corrected are within the range of y m l t r then the corrected values are obtained by linear interpolation between the data points ordered pairs the generic r command approx version 3 4 3 r core team 2017 was used for interpolation if the values to be corrected are outside the range of y m l t r then the corrected values are obtained by linear extrapolation from the endpoints fig 3 3b y e d m m a x y o b s t r m e d m y m l m a x y m l t r f o r y m l m a x y m l t r 3c y e d m m i n y o b s t r m e d m m i n y m l t r y m l f o r y m l m i n y m l t r m a x y o b s t r maximum value m i n y o b s t r minimum value m edm slope based on regression of y e d m t r on y m l t r the slope of the line ott and longnecker 2001 m edm is computed using the training data 3d m e d m c o v y m l t r y o b s t r v a r y m l t the edm method by definition provides a set of estimates for the training data that has an ecdf that is identical to the observed ecdf for the holdout data the ecdf for the estimates might or might not be a close match to the observed ecdf the vector y e d m t r has the same statistical moments as the vector y o b s t r because it is simply a reordering of those values 4a e y e d m t r e y o b s t r 4b v a r y e d m t r v a r y o b s t r e y o b s t r e y o b s t r 2 equations 4a and 4b will be useful in subsequent derivations of other bias correction methods 2 3 2 regression of observed on estimated values roe regression of observed on estimated values roe is implemented at the point scale and has been described as simple linear regression by song 2015 regression of observations on estimates might seem counterintuitive given the typical scatter plot fig 1a but it is the appropriate formulation for adjusting a given value of y ml to obtain a corrected value y roe 5a y r o e m r o e y m l b r o e the coefficients are estimated with y o b s t r as the dependent variable and y m l t r as the independent variable 5b m r o e c o v y m l t r y o b s t r v a r y m l t 5c b r o e e y o b s t r m r o e e y m l t r if y obs is subsequently plotted as a function of y roe the points would be distributed along the one to one line the regression line would have a slope of one and an intercept of zero in addition the residuals ε y y o b s y r o e when plotted as a function of y roe would be uniformly distributed along the x axis alternatively stated the residuals resulting from the roe method are homoscedastic and independent 2 3 3 linear transfer function ltf the linear transfer function ltf is a modification of the edm method and is therefore a distribution scale method the set of ordered pairs is replaced by a straight line fit to those data fig 3 6a y l t f m l t f y m l b l t f the coefficients are estimated with y e d m t r as the dependent variable and y m l t r as the independent variable 6b m l t f m e d m c o v y m l t r y e d m t r v a r y m l t 6c b l t f e y e d m t r m l t f e y m l t r e y o b s t r m l t f e y m l t r the intercept b ltf can be expressed in terms of the mean of the observed distribution because the edm vector is simply a reordering of the observed data equation 4a the ltf method has been used to adjust output from climate models and other parametric transfer functions have been proposed piani et al 2010 gudmundsson et al 2012 as will be shown in the results section a linear transfer function closely approximates the data for the case studies evaluated in this paper 2 3 4 linear equation based on z score transform zz the zz method like the roe and ltf methods uses a linear equation to adjust a set of estimates in the zz method the linear coefficients are derived from a z score formulation the estimated value y est is derived from the y m l t r distribution and the bias corrected value y zz is derived from the y o b s t r distribution 7a z 1 y m l e y m l t r v a r y m l t r 7b z 2 y z z e y o b s t r v a r y o b s t r setting z 1 z 2 and rearranging provides an expression for the corrected value y zz 8a y z z m z z y m l b z z 8b m z z v a r y o b s t r v a r y m l t r 8c b z z e y o b s t r m z z e y m l t r the coefficients equations 8b and 8c are based on the first two moments of y m l t r and y o b s t r hence the zz method is a distribution scale method if the dependent variable is normally distributed then the zz method is an exact transform because the normal distribution is fully described by the mean and variance however the zz method can be applied to dependent variables that are not normally distributed under certain circumstances the zz method approximates the ltf method the relationship between the ltf and zz methods is discussed in section 2 4 2 3 5 second machine learning model used to estimate residuals ml2 res zhang and lu 2012 proposed the use of two models to correct for point sale bias as illustrated in fig 1a after the first model is developed the residuals for the training data ε y t r are computed equation 2a the residuals become the dependent variable in the second model ml2 res the independent variables in the second model include the matrix of predictor variables used in the first model along with the observations y o b s t r as an additional predictor variable zhang and lu 2012 developed the procedure for two rf models but we apply it to results from a case study that used brt models stackelberg et al 2020 given the use of two brt models in this paper the term ml2 res is used to distinguish it from osbf ghosal hooker 2020 ml2 res is a point scale method for a new set of data such as holdout data the first model is used to estimate values of y ml the second model is used to estimate values of ε y m l and then the two results are added together to obtain the corrected value y ml2 res 2 3 6 roe duan an additional source of potential bias arises from the development of a model using log transformed data and the subsequent re transformation of model estimates back to the original units if the dependent variable y is the natural log transform of the variable w then the corrected value based on duan s 1983 smearing estimate is 9 w r o e d u a n e x p y r o e e e x p ε y t r the residuals ε y t r are computed after the ml estimates are corrected using the roe method the roe method is used so that the residuals satisfy the assumptions of homoscedasticity and independence required for application of the duan method the duan smearing estimate is a point scale method because the residuals are computed using point data 2 4 the ltf and zz bias correction methods are related the ltf and zz methods can be shown to be related to one another through the use of the linear correlation coefficient ott and longnecker 2001 for the line describing ltf equation 6a the correlation coefficient r ltf is 10 r l t f c o v y m l t r y e d m t r v a r y m l t r v a r y e d m t r c o v y m l t r y e d m t r v a r y m l t r v a r y o b s t r r ltf can be expressed in terms of the variance of the observed distribution because the edm vector is a reordering of the observed data equation 4b equation 10 can be rearranged to derive an expression for c o v y m l t r y e d m t r that expression can be substituted into equation 6b and in turn that result can be simplified using equation 8b 11a m l t f r l t f m z z equation 11a can be substituted into equation 6c 11b b l t f e y o b s t r r l t f m z z e y m l t r as shown in equations 11a and 11b the ltf and zz methods are equivalent if r ltf 1 this will occur when y e d m t r is highly correlated to y m l t r the potential equivalence is discussed in the results section 2 5 application of edm ltf and zz methods to natural log transformed data the duan smearing estimate is not the only alternative for re transforming model estimates back to the original units two of the other methods edm and ltf can also be used because they directly incorporate a mapping of the uncorrected ml estimates into the ecdf of the observed data in addition the zz method might also be applicable if the dependent variable is normally distributed or if y e d m t r is highly correlated to y m l t r for the purpose of discussion an additional metric is introduced 12 n r f b c r m s e f b c r m s e f m l where nrfbc is the normalized rmse f for a given bias correction method nrfbc ranges from zero to one a value of zero indicates a perfect match between the bias corrected ecdf and the observed ecdf no error a value of one indicates that the bias correction method had no effect on the ml estimates a value close to one could occur because the bias correction method was ineffective for adjusting biased yml values or because the initial yml values were nearly unbiased equation 12 is used for comparing performance when rmse f is computed using retransformed units as compared to log units 3 data four case studies used for assessing bias correction methods the performance of the bias correction methods was evaluated using data from four previously published usgs nawqa case studies that applied brt modeling to the regional scale mapping of groundwater quality table 1 two of the case studies were developed with ph as the dependent variable the glacial aquifer system of the northern u s stackelberg et al 2020 and the north atlantic coastal plain nacp desimone et al 2020 the other two case studies were developed with natural log transformed concentration as the dependent variable nitrate in the central valley cv of california ransom et al 2017 and iron in the mississippi embayment mise of the south central u s knierim et al 2020 metrics for the cv and mise case studies were computed in units of log concentration and in units of retransformed concentration nitrate and iron are of concern to human health if concentrations in drinking water exceed 10 mg per liter and 4000 μg per liter respectively u s epa 2020a norman et al 2018 4 results and discussion the glacial ph case study has the most amount of data table 1 and the results from that case study are discussed in the first subsection the results from the glacial ph case study provide context for the subsections that follow the second subsection evaluates results from the other case studies with performance metrics computed in the same units as the dependent variables in the brt models the third subsection evaluates results when the metrics are computed in retransformed units there are six bias correction methods four case studies and four metrics and for each case study there are training data and there are holdout data therefore only selected results are presented in the main body of the text other results are presented in the supplement the observed values ml estimates and bias corrected estimates for the four case studies are also available belitz et al 2021 4 1 glacial ph case study five of the six bias correction methods were applied to the glacial ph data the roe duan method was not applied because it is not applicable to the case study the metrics associated with each method are presented in table 2 along with the metrics computed for the uncorrected ml estimates as expected the metrics for all five methods are better for the training data than for the holdout data additional summary statistics mean variance minimum and maximum for the training and holdout data for the five bias correction methods are presented in table s1 4 1 1 rmse f for glacial ph case study rmse f quantifies the match between estimated and observed ecdfs and the lowest values for the glacial ph case study are provided by the edm bias correction method table 2 by definition the edm method provides an exact match to the observed ecdf for the training data and therefore the rmse f is equal to zero the edm method also provides the lowest value of rmse f for the holdout data and by that standard is the best performing method visual inspection of the ecdfs fig 4 illustrates the point the ecdf for the edm corrected holdout data is relatively close to the observed ecdf which is the objective of the current work the ml2 res method provides a low value of rmse f for the training data 2nd lowest value table 2 but does not provide a correspondingly low value for the holdout data indeed the rmse f for the holdout data 0 15 is closer to the value computed for the uncorrected ml estimates 0 20 than it is to the value computed for the edm method 0 07 likewise the ml2 res ecdf is closer to the ecdf for the uncorrected ml estimates than to the observed ecdf fig 4 the rmse f values for the ltf and zz methods are comparable to one another and are about halfway between the values computed for the uncorrected ml estimates and the edm corrected estimates table 2 the ecdfs for the two methods also are about halfway between the observed ecdf and the uncorrected ecdf for the holdout data fig 4 and for the training data fig s1 the point scale roe method provides values of rmse f for both the training and holdout data that are relatively close to the values computed for the uncorrected ml estimates table 2 the ecdfs for the holdout data fig 4 and training data fig s1 are relatively close to the uncorrected ml ecdfs roe is relatively ineffective for adjusting the ecdf for this case study 4 1 2 bias f rmse y and bias y for glacial ph case study the other three metrics allow for additional assessment of the performance of the bias correction methods the bias f values for the training data are very low for all five methods table 2 the values for the holdout data are not as low as the training data but are relatively close to one another with differences amongst the methods occurring in the third significant digit thousandths of a ph unit a shift of less than one hundredth of a ph unit is not easily discerned on the plot of the ecdfs fig 4 rmse y is an overall measure of fit and can be viewed as an appropriate metric for correcting point scale bias as illustrated in fig 1a zhang and lu 2012 the ml2 res method provides a value of rmse y for the training data that is substantially lower than the other methods table 2 however the ml2 res method does not provide a correspondingly low value of rmse y for the holdout data table 2 overall the five bias correction methods provide comparable rmse y values for the holdout data bias y is the traditional indicator of bias the bias y values for the training data are very low for all five bias correction methods table 2 the values of bias y for the holdout data are not as low as for the training data but are low with respect to the mean and range of the ph data table 1 when performance is assessed using the bias f rmse y and bias y metrics the five bias correction methods are comparable to one another and to the uncorrected ml estimates for the glacial ph case study these results are in contrast to those for rmse f whereby there were differences amongst the methods 4 1 3 comparison of ltf and zz methods for the glacial ph case study the ltf and zz methods are both implemented at the distribution scale and apply a linear transformation to the data for the glacial ph case study the two methods provide values that are nearly identical to one another for all four metrics table 2 the ecdfs for the holdout data fig 4 and training data fig s1 are also similar for the two methods the close correspondence between the two methods arises because the correlation coefficient between y ml as the independent variable and y edm is close to one rltf 0 995 as shown in fig 3 the lines for the ltf and zz methods are nearly colinear by definition the ltf method is a linear approximation of the edm method and given the high correlation between y ml and y edm for the glacial ph case study the zz method can also be seen as an approximation of the edm method the ltf and zz methods are both parametric and some researchers piani et al 2010 citing robustness prefer parametric over empirical methods with rmse f as the primary metric neither the ltf nor zz methods are as good as the edm method for this case study table 2 4 1 4 potential issues related to ml2 res bias correction method several issues arise when applying the ml2 res correction method to the glacial ph case study one issue is the possibility that implementation of a second brt model results in over training as indicated by the values of rmse f and rmse y for the training data the value of rmse f computed for the ml2 res corrected estimates is substantially smaller than the value computed for the uncorrected ml estimates but a comparable improvement is not seen for the holdout data in contrast the performance of the other correction methods for the training data as indicated by the ordinal positions of rmse f is maintained in the holdout data a comparable pattern can be seen in the values of rmse y scatter plots illustrate the point the ml2 res estimates plot close to the one to one line for the training data fig s2 but not for the holdout data fig s3 the potential over training occurred even though the second brt model like the first was developed using ten fold cross validation to identify model meta parameters stackelberg et al 2020 the ml2 res method is also computationally intensive relative to the other methods because it requires development of a second brt model in addition the ml2 res method can be seen as an extension of the brt approach because brt modeling works with residuals given the potential for over training computational intensity and conceptual overlap with brt the ml2 res method was not applied to the other three case studies 4 2 additional case studies north atlantic coastal plain central valley and mississippi embayment given the findings from the glacial ph case study three aspects of the additional case studies are discussed 1 rmse f as the primary metric for comparison of methods supplemented by visual comparison of the ecdfs 2 bias f rmse y and bias y as secondary metrics for comparison and 3 relation between the ltf and zz methods metrics are computed using the units of the dependent variable in the ml models the values for rmse f are presented in table 3 for all three case studies the edm correction method provided the lowest values of rmse f and the roe correction method provided the highest values the rmse f values for the ltf and zz correction methods are comparable to each other the relative performance of the edm roe ltf and zz methods is the same as that for the glacial ph case study the ecdfs for the holdout data illustrate the quantitative results figs 5 7 for all three case studies the edm ecdf is relatively close to the observed ecdf and the roe ecdf is relatively close to the uncorrected ml ecdf additional ecdfs for the three case studies are presented in the supplement figs s4 and s5 the values for bias f rmse y and bias y are presented in tables s2 s3 and s4 respectively for all three case studies the differences amongst the correction methods for each of the three metrics are small when compared to the mean and range of the observed data table 1 additional summary statistics mean variance minimum and maximum are presented in tables s5 s7 the results for the three case studies are similar to those for the glacial ph case study the ltf and zz methods are relatively similar to one another for all four metrics and differ from the other methods tables 3 s2 s3 and s4 the close correspondence between the ltf and zz methods arises because the correlation coefficient between y ml and y edm equation 10 is close to one for all three case studies rltf 0 998 0 997 and 0 998 for the nacp cv and mise case studies respectively as shown in fig 8 the lines for the ltf and zz methods are nearly colinear for all three case studies as they were for the glacial ph case study the results for the nacp cv and mise case studies confirm the results for the glacial ph case study the best match with the observed ecdf based on rmse f is provided by the edm correction method the poorest match with the observed ecdf was provided by the roe method the differences amongst the methods for the three secondary metrics are generally small 4 3 ml models developed using natural log transformed concentration two of the case studies cv nitrate and mise iron were developed with natural log transformed concentration as the dependent variable in this section the performance of four bias correction methods edm ltf zz and roe duan are evaluated in terms of retransformed concentration the metrics for the training and holdout data are presented in tables s8 and 4 respectively performance is assessed using the holdout data summary statistics mean variance minimum and maximum for the training and holdout data for the cv and mise case studies are presented in tables s9 and s10 respectively prior to application of the duan smearing estimate 1983 the roe method was used to adjust the ml estimates the residuals for the training data after adjustment by roe were uniformly distributed around the x axis for both case studies figs s6 and s7 the resulting equations based on the training data were then used to adjust the holdout values for both case studies 4 3 1 rmse f computed in retransformed concentration for the cv holdout data the lowest value of rmse f computed in retransformed concentration is provided by the edm method and the highest value is provided by the roe duan method table 4 the ltf and zz methods provide values of rmse f that are quite close to the value provided by the edm method the ordinal position of the four methods is the same as when rmse f was computed in log concentration table 3 for the mise holdout data the lowest value of rmse f computed in retransformed concentration is provided by the roe duan method and the highest value is provided by the edm method table 4 this is the opposite of the results when the same metric was computed in log concentration table 3 the better performance of the roe duan method reflects the relatively close match at relatively high concentrations between the roe duan and observed ecdfs fig 7 the close match at high concentrations more than offsets the relatively poor match at relatively low concentrations the converse occurs for the edm method the ltf and zz methods provide values of rmse f that are closer to those provided by the roe duan method than to those provided by the edm method nrfbc a dimensionless metric allows for a comparison of rmse f computed in retransformed units relative to results computed in log units table 5 for three of the methods edm ltf and zz and for both case studies nrfbc is smaller when the metric is computed using log concentration for roe duan the opposite is true nrfbc is smaller when computed using retransformed concentration the edm ltf and zz methods were implemented using log concentration data and performance declined when nrfbc was computed in retransformed concentration in contrast the duan 1983 smearing estimate is intended to minimize bias y when it is computed in retransformed units this is achieved by multiplying the retransformed estimate by a constant value equation 9 consequently performance was better when nrfbc was computed in retransformed concentration as compared to when the metric was computed in log concentration overall the lowest value of nrfbc for both case studies is for the edm method when the metric was computed in log concentration 4 3 2 bias f rmse y and bias y computed in retransformed concentration the other three metrics bias f rmse y and bias y allow for additional assessment of the performance of the bias correction methods when these metrics were computed using log concentration the differences amongst the methods were small in contrast the differences are larger when computed in retransformed concentration table 4 particularly for the mise case study for comparison the means and ranges computed in concentrations units are provided in tables s9 and s10 for the cv and mise case studies respectively for the cv holdout data the roe duan method provided the best performance for all three metrics when they were computed in retransformed concentration table 4 for the mise holdout data the roe duan method provided the best performance for the bias f and bias y metrics and the edm method provided the best performance for the rmse y metric table 4 overall roe duan provided the best performance when the other three metrics were computed in retransformed concentration 4 3 3 assessment of performance based on retransformed units as compared to log units when performance is assessed in retransformed units a bias correction method is evaluated on an absolute basis and therefore small values have lesser influence than large values in contrast if metrics are computed in log units performance is assessed on a relative basis and small values can be as influential as large values whether it is more important to obtain a match on an absolute or relative basis is a subjective decision for example if it is important to accurately estimate high values then assessment of performance on an absolute basis would be appropriate similarly if is it important to estimate the total mass of a constituent within a given area of an aquifer then it would be important to correctly estimate the mean value in concentration units and it would therefore be appropriate to assess performance on an absolute basis alternatively if it is important to accurately represent the full range of values then it would be better to assess performance on a relative basis 5 conclusions ensemble tree ml regression models are useful for understanding and evaluating environmental systems output from these models however can be systematically biased small values are overestimated and large values are underestimated for many environmental applications this can be a problem because it is as important or more important to correctly estimate the tails of the distribution as it is to correctly estimate the mean or other central values the primary goal of this paper was identification of a bias correction method that adjusts ml estimates so that the empirical cumulative distribution function ecdf for the adjusted estimates approximates the observed ecdf five methods were evaluated for correcting ml bias empirical distribution matching edm regression of observed on estimated roe values linear transfer function ltf z score transform zz and the use of a second ml model for estimating residuals ml2 res four previously published usgs nawqa case studies that used brt ml modeling of groundwater quality were used for assessing the methods ph in the glacial aquifer of the northern u s ph in the north atlantic coastal plain log nitrate in the central valley of california and log iron in the mississippi embayment southeastern u s an additional source of bias can be introduced into statistical models including ml models if the dependent variable is a transform of the original data a sixth bias correction method roe duan was evaluated for the two case studies developed using log transformed concentration the roe duan method is based on duan s 1983 smearing estimate the roe method was used to ensure that the residuals are homoscedastic and independent an assumption incorporated into the development of duan smearing estimate the bias correction methods were calibrated using training data and the performance of the methods was assessed using training and holdout data two of the performance metrics evaluate the match between the adjusted and observed ecdfs rmse f and bias f two of the metrics evaluate the match between adjusted and observed points rmse y and bias y rmse f was identified as the primary metric for assessing performance because of the goal of reproducing the tails of the observed distributions for the central valley and mississippi embayment case studies the metrics were computed in log concentration the units of the ml models and in retransformed concentration conclusions are based on holdout data when the performance metrics were computed in the units of the ml models the results were comparable for all four case studies rmse f varied amongst the different bias correction methods but the other three metrics did not vary substantively the distribution scale edm method was the most effective and the point scale roe method was the least effective for correcting systematic bias in the four case studies evaluated when the performance metrics for the central valley and mississippi embayment case studies were computed in retransformed concentration the results differed from when the metrics were computed in log concentration with rmse f computed in retransformed concentration the best result was provided by the edm method for the central valley and by the roe duan method for the mississippi embayment the values of the other three metrics varied for both case studies amongst the different methods overall the roe duan method provided the best results when the other three metrics were computed using retransformed concentration the roe duan method was the most effective for correcting introduced bias bias in the mean if performance metrics are computed in retransformed units then a bias correction method is assessed on an absolute basis and large values can dominate the computation if the metrics are computed in log units then performance is assessed on a relative basis and small values can also be important whether it is more important to maximize absolute performance or relative performance is a subjective decision the objectives of a study should be considered when making that decision to the extent that log units are chosen for developing an ml model that decision has been partly made the ltf method is a linear approximation of the edm method and for the four case studies evaluated the results are close to the edm method the zz method can approximate the ltf method if the rank ordered ml estimates are highly correlated to the rank ordered observed values for the four case studies evaluated the correlation was high and the zz method closely approximated the ltf method if a simple parametric method is preferred for bias correction then the zz method might be an appropriate choice an important check would be to evaluate the correlation between the rank ordered ml estimates and rank ordered observed values the ml2 res method was only applied to the glacial ph case study the method did not significantly improve the match to the observed ecdf for either the training or holdout data the method was not applied to the other case studies because of potential over training computational burden and conceptual overlap with the brt approach bias correction can be an important part of ensemble tree ml model development but it is not a replacement for developing a model that provides accurate estimates declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105006 software and data availability this article presents mathematical approaches and equations for correcting for bias in results from ensemble tree regression models this effort did not produce software to be used by others the data for the four case studies presented in this paper are available through the sciencebase catalog of the u s g s at https doi org 10 5066 p9lctyi2 belitz et al 2021 
25865,ensemble tree machine learning ml regression models can be prone to systematic bias small values are overestimated and large values are underestimated additional bias can be introduced if the dependent variable is a transform of the original data six methods were evaluated for their ability to correct systematic and introduced bias method performance was evaluated using four case studies of groundwater quality the units of the dependent variable were ph in two and log concentration in the others when performance metrics bias and rmse for both points and the cdf were computed using the same units as those in the ml model empirical distribution matching edm provided the best results when the metrics were computed using retransformed concentration edm and a method incorporating duan s smearing estimate were both effective a method based on the z score transform approximates edm if the correlation coefficient between rank ordered ml estimates and rank ordered observations approaches one keywords machine learning bias correction ensemble tree methods groundwater water quality 1 introduction ensemble tree machine learning ml models such as random forest rf and boosted regression trees brt are increasingly being applied to the modeling of environmental systems ensemble tree models can be applied to problems of both classification and regression this paper focuses on regression numerous examples of ensemble tree regression models are available in the literature recent examples published in this journal include identification of landslide hazard in the u s pacific northwest stanley et al 2020 correction of digital elevation models in the coastal everglades cooper et al 2019 installation of solar capacity in germany frey et al 2019 urban household water demand in tampa florida duerr et al 2018 low stream flow conditions in the southeastern u s worland et al 2018 sponge species richness in the oceanic shoals commonwealth marine reserve australia li et al 2017 and metamodeling of groundwater flow to wells lake michigan basin u s fienen et al 2016 the u s geological survey usgs national water quality assessment project nawqa has also been using ensemble tree ml regression models particularly for the estimation and mapping of groundwater quality at regional and national scales these studies have focused on constituents that are prevalent at concentrations of concern e g ransom et al 2017 knierim et al 2020 and on the factors affecting groundwater quality e g fienen et al 2018 belitz et al 2019 the work presented in this paper is a part of the usgs nawqa project and is motivated in part by the results from our ongoing studies that include ensemble tree ml regression modeling the estimates provided by ensemble tree ml regression models are generally unbiased in the sense that the sum of the errors observed values compared to estimated is close to zero hastie et al 2009 however ml regression models can produce results that are biased in a different sense zhang and lu 2012 small values are overestimated and large values are underestimated fig 1 for many purposes including mapping of groundwater quality it is important to correctly estimate one or both tails of a distribution for example if one is interested in the suitability of groundwater for human use then it is important to correctly identify the size and location of areas where concentrations are above a threshold of concern u s epa 2020a norman et al 2018 alternatively it might be important to identify areas where groundwater ph is outside the range 6 5 8 5 u s epa 2020b in addition it might be important to identify geochemical conditions that can affect lead concentrations jurgens et al 2019 if the ml models are systematically biased overestimating small values and underestimating large values then the resulting maps can be incorrect consequently the primary objective of this paper is to evaluate methods for correcting the systematic bias introduced by ensemble tree ml models an additional potential source of bias can be introduced into statistical models including ensemble tree ml models if the dependent variable is a transform of the original data bias in the estimate of the mean can be introduced when model estimates are retransformed back into the original units duan 1983 developed an approach for addressing the bias in the mean under the assumption that the residuals are homoscedastic and independent of the estimated values helsel et al 2020 this assumption is not true if there is systematic overestimation of small values and underestimation of large values a secondary objective of this paper is evaluation of bias correction methods for ml models developed using natural log transformed dependent variables bias correction can be viewed from two perspectives the point scale and the distribution scale the point scale approach adjusts estimated values so that they better match their associated observed value the regression line would be colinear with the one to one line on a scatter plot fig 1a the distribution scale approach adjusts estimated values so that their cumulative distribution matches the observed distribution fig 1b both perspectives are considered in this paper the point scale correction of bias in ml models has been addressed in the statistical literature the randomforest package liaw and wiener 2002 incorporates an approach based on linear regression of observations on estimated values with the coefficients computed using out of bag data the resulting equation is then used for adjusting estimates provided by the rf model zhang and lu 2012 generalized the regression approach and recommended using all of the data rather than out of bag data for minimizing prediction error zhang and lu 2012 also developed an alternative approach in which a second rf model is used to estimate the residuals associated with the initial model the second model can then be used for correcting the bias ghosal and hooker 2018 subsequently defined the approach using two rf models as one step boosted forests osbf several studies zhang and lu 2012 xu 2013 song 2015 ghosal and hooker 2018 have shown that osbf with mean square error as the performance metric outperforms regression for correcting point scale bias in rf models point scale bias correction methods have been applied to ml models developed for environmental applications the regression approach has been used to correct the bias in rf models of the expansion of urban areas in china huang et al 2016 soil geochemical mapping in southwest england kirkwood et al 2016 salinity loading in the upper colorado river basin nauman et al 2019 and concentrations of total phosphorus and total nitrogen in baseflow in the western us olsen and hawkin 2020 the regression approach has also been applied to a brt model of global fish stock depletion zhou et al 2017 the osbf approach utilizing two rf models has been used to correct bias in models of drought indices in southeast australia feng et al 2019 and the global export of silica by rivers phillips 2020 the regression approach and an approach comparable to osbf are both examined in this paper distribution scale bias correction the matching of cumulative distribution functions cdfs has not been broadly applied to ml models developed for environmental systems but it has been incorporated into other applications of environmental modeling and measurement these applications include downscaling of climate models wood et al 2004 assimilation of satellite measurements and ground based information reichle and koster 2004 forecasting of streamflow from an ensemble of models hashino et al 2007 and modeling of soil hydraulic properties liao et al 2017 the one example known to the authors of distribution scale bias correction to an ml application in environmental modeling is the adjustment of output from an artificial neural network ann model to match field soil moisture characteristic properties jana et al 2008 ann modeling is not an ensemble tree approach the distribution scale approach presented in this paper relies upon contributions from climate modeling research in that context the matching of cdfs has been referred to as equiprobability transformation quantile mapping probability mapping and distribution matching panofsky and brier 1968 mcginnis et al 2015 the basic idea is that a transfer function is established that transforms model output so that the resulting cdf matches that of an observed dataset the transfer function could be empirical or parametric piani et al 2010 gudmundsson et al 2012 the empirical approach can utilize all of the data to define the cdf or selected percentiles can be used to approximate it kotlarski et al 2017 the parametric approach can be based on a probability distribution or an appropriate functional relationship piani et al 2010 gudmundsson et al 2012 this paper examines empirical and parametric distribution scale approaches four sections follow this introduction section 2 provides a discussion of the methods used to assess and correct for bias section 3 briefly describes the data from four previously published brt case studies that are used in section 4 for assessing the different bias correction methods conclusions are presented in section 5 although the case studies are drawn from usgs nawqa groundwater quality studies the results of this paper are applicable to a wide range of environmental systems 2 methods nomenclature performance metrics and bias correction this section is divided into five subsections the first subsection introduces nomenclature and the second presents performance metrics for assessing bias the third and longest subsection describes several bias correction methods that are subsequently applied to case studies the last two subsections provide additional discussion of selected bias correction methods 2 1 nomenclature observations of the dependent variable are denoted as y obs and estimated values are denoted as y est the empirical cumulative distribution function ecdf is denoted as f y estimated values can be derived from ml models y ml or can be bias corrected values y c obtained by adjusting y ml several methods for bias correction are evaluated in this paper and various subscripts are introduced to correspond with the different methods if the dependent variable is modeled using natural log transformed values then the exponentiated value of y is denoted as w in this paper the values of y or w are water quality parameters measured in samples collected from groundwater wells that are distributed across the landscape 2 2 metrics for assessing bias the bias associated with a set of estimates can be quantified from two perspectives 1 the distribution scale and 2 the point scale the subscripts f and y are used to denote the distribution scale and point scale respectively 2 2 1 metrics computed at the distribution scale at the distribution scale the bias can be assessed by comparing the ecdf for the estimates to the ecdf for the observed values the comparison between the two can be defined in terms of the horizontal distance between them at a given value of p 1a ε f p q o b s p q e s t p the horizontal distance or error ε f p has the same units as the x axis of the ecdf plot q p the quantile function is the inverse of the ecdf in turn two metrics for assessing the match between ecdfs can be defined the root mean square error rmse f and the first order bias bias f 1b r m s e f e ε f 2 1c b i a s f e ε f note that a vertical line can have a bias f equal to zero hence the rmse f provides a better characterization of the fit between observed and estimated ecdfs for computational purposes ε f and ε f 2 were calculated for probability values distributed uniformly in the range 0 1 using increments of 0 01 the generic r command quantile version 3 4 3 r core team 2017 was used which uses linear interpolation to calculate values of q p based on the empirical distribution y f y the plotting position was based on the hazen formula helsel et al 2020 which is specified as type 5 in the quantile command 2 2 2 metrics computed at the point scale the point scale error ε y associated with an estimated value is 2a ε y y o b s y e s t the corresponding point scale metrics are 2b r m s e y e ε y 2 2c b i a s y e ε y bias y quantifies the bias in the mean but does not necessarily address the overestimation of small values and underestimation of large values as illustrated in fig 1a rmse y is more appropriate for that purpose zhang and lu 2012 2 2 3 application of metrics the four metrics rmse f bias f rmse y bias y can be applied to uncorrected estimates y ml and to corrected estimates y c the four metrics can also be applied to the training data y tr and to the holdout data y ho identified in the development of the ml model values computed for the uncorrected ml estimates provide context for assessing the performance of the various bias correction methods the distribution scale metrics rmse f bias f can be applied not only to the assessment of distribution scale bias correction methods but also to the assessment of point scale methods likewise the point scale metrics rmse y bias y can be used for the assessment of methods of either type 2 3 bias correction methods six bias correction methods are evaluated for each method the parameters or coefficients needed for implementing the method are derived using training data the methods can then be applied to the training data and to the holdout data 2 3 1 empirical distribution matching edm empirical distribution matching edm as the name implies is implemented at the distribution scale in the context of ml modeling the edm method is implemented by obtaining two ecdfs fig 2 one for the ml estimates f m l t r and one for the observed values f o b s t r in turn one can obtain the inverse ecdf for the observed values q o b s t r for any given estimate y ml the bias corrected value y edm is 3a y e d m q o b s t r f m l t r y m l in the context of ml modeling the edm method is simple to apply after the model is trained there are two vectors y m l t r and y o b s t r of equal length each vector is ranked from smallest to largest and a set of ordered pairs is obtained by matching corresponding points of corresponding rank the value of y o b s t r that is paired with y m l t r is defined as y e d m t r the set of ordered pairs y m l t r y e d m t r provides the empirical basis for correcting any given value of y ml fig 3 the set of ordered pairs can also be viewed as a look up table if the values to be corrected are within the range of y m l t r then the corrected values are obtained by linear interpolation between the data points ordered pairs the generic r command approx version 3 4 3 r core team 2017 was used for interpolation if the values to be corrected are outside the range of y m l t r then the corrected values are obtained by linear extrapolation from the endpoints fig 3 3b y e d m m a x y o b s t r m e d m y m l m a x y m l t r f o r y m l m a x y m l t r 3c y e d m m i n y o b s t r m e d m m i n y m l t r y m l f o r y m l m i n y m l t r m a x y o b s t r maximum value m i n y o b s t r minimum value m edm slope based on regression of y e d m t r on y m l t r the slope of the line ott and longnecker 2001 m edm is computed using the training data 3d m e d m c o v y m l t r y o b s t r v a r y m l t the edm method by definition provides a set of estimates for the training data that has an ecdf that is identical to the observed ecdf for the holdout data the ecdf for the estimates might or might not be a close match to the observed ecdf the vector y e d m t r has the same statistical moments as the vector y o b s t r because it is simply a reordering of those values 4a e y e d m t r e y o b s t r 4b v a r y e d m t r v a r y o b s t r e y o b s t r e y o b s t r 2 equations 4a and 4b will be useful in subsequent derivations of other bias correction methods 2 3 2 regression of observed on estimated values roe regression of observed on estimated values roe is implemented at the point scale and has been described as simple linear regression by song 2015 regression of observations on estimates might seem counterintuitive given the typical scatter plot fig 1a but it is the appropriate formulation for adjusting a given value of y ml to obtain a corrected value y roe 5a y r o e m r o e y m l b r o e the coefficients are estimated with y o b s t r as the dependent variable and y m l t r as the independent variable 5b m r o e c o v y m l t r y o b s t r v a r y m l t 5c b r o e e y o b s t r m r o e e y m l t r if y obs is subsequently plotted as a function of y roe the points would be distributed along the one to one line the regression line would have a slope of one and an intercept of zero in addition the residuals ε y y o b s y r o e when plotted as a function of y roe would be uniformly distributed along the x axis alternatively stated the residuals resulting from the roe method are homoscedastic and independent 2 3 3 linear transfer function ltf the linear transfer function ltf is a modification of the edm method and is therefore a distribution scale method the set of ordered pairs is replaced by a straight line fit to those data fig 3 6a y l t f m l t f y m l b l t f the coefficients are estimated with y e d m t r as the dependent variable and y m l t r as the independent variable 6b m l t f m e d m c o v y m l t r y e d m t r v a r y m l t 6c b l t f e y e d m t r m l t f e y m l t r e y o b s t r m l t f e y m l t r the intercept b ltf can be expressed in terms of the mean of the observed distribution because the edm vector is simply a reordering of the observed data equation 4a the ltf method has been used to adjust output from climate models and other parametric transfer functions have been proposed piani et al 2010 gudmundsson et al 2012 as will be shown in the results section a linear transfer function closely approximates the data for the case studies evaluated in this paper 2 3 4 linear equation based on z score transform zz the zz method like the roe and ltf methods uses a linear equation to adjust a set of estimates in the zz method the linear coefficients are derived from a z score formulation the estimated value y est is derived from the y m l t r distribution and the bias corrected value y zz is derived from the y o b s t r distribution 7a z 1 y m l e y m l t r v a r y m l t r 7b z 2 y z z e y o b s t r v a r y o b s t r setting z 1 z 2 and rearranging provides an expression for the corrected value y zz 8a y z z m z z y m l b z z 8b m z z v a r y o b s t r v a r y m l t r 8c b z z e y o b s t r m z z e y m l t r the coefficients equations 8b and 8c are based on the first two moments of y m l t r and y o b s t r hence the zz method is a distribution scale method if the dependent variable is normally distributed then the zz method is an exact transform because the normal distribution is fully described by the mean and variance however the zz method can be applied to dependent variables that are not normally distributed under certain circumstances the zz method approximates the ltf method the relationship between the ltf and zz methods is discussed in section 2 4 2 3 5 second machine learning model used to estimate residuals ml2 res zhang and lu 2012 proposed the use of two models to correct for point sale bias as illustrated in fig 1a after the first model is developed the residuals for the training data ε y t r are computed equation 2a the residuals become the dependent variable in the second model ml2 res the independent variables in the second model include the matrix of predictor variables used in the first model along with the observations y o b s t r as an additional predictor variable zhang and lu 2012 developed the procedure for two rf models but we apply it to results from a case study that used brt models stackelberg et al 2020 given the use of two brt models in this paper the term ml2 res is used to distinguish it from osbf ghosal hooker 2020 ml2 res is a point scale method for a new set of data such as holdout data the first model is used to estimate values of y ml the second model is used to estimate values of ε y m l and then the two results are added together to obtain the corrected value y ml2 res 2 3 6 roe duan an additional source of potential bias arises from the development of a model using log transformed data and the subsequent re transformation of model estimates back to the original units if the dependent variable y is the natural log transform of the variable w then the corrected value based on duan s 1983 smearing estimate is 9 w r o e d u a n e x p y r o e e e x p ε y t r the residuals ε y t r are computed after the ml estimates are corrected using the roe method the roe method is used so that the residuals satisfy the assumptions of homoscedasticity and independence required for application of the duan method the duan smearing estimate is a point scale method because the residuals are computed using point data 2 4 the ltf and zz bias correction methods are related the ltf and zz methods can be shown to be related to one another through the use of the linear correlation coefficient ott and longnecker 2001 for the line describing ltf equation 6a the correlation coefficient r ltf is 10 r l t f c o v y m l t r y e d m t r v a r y m l t r v a r y e d m t r c o v y m l t r y e d m t r v a r y m l t r v a r y o b s t r r ltf can be expressed in terms of the variance of the observed distribution because the edm vector is a reordering of the observed data equation 4b equation 10 can be rearranged to derive an expression for c o v y m l t r y e d m t r that expression can be substituted into equation 6b and in turn that result can be simplified using equation 8b 11a m l t f r l t f m z z equation 11a can be substituted into equation 6c 11b b l t f e y o b s t r r l t f m z z e y m l t r as shown in equations 11a and 11b the ltf and zz methods are equivalent if r ltf 1 this will occur when y e d m t r is highly correlated to y m l t r the potential equivalence is discussed in the results section 2 5 application of edm ltf and zz methods to natural log transformed data the duan smearing estimate is not the only alternative for re transforming model estimates back to the original units two of the other methods edm and ltf can also be used because they directly incorporate a mapping of the uncorrected ml estimates into the ecdf of the observed data in addition the zz method might also be applicable if the dependent variable is normally distributed or if y e d m t r is highly correlated to y m l t r for the purpose of discussion an additional metric is introduced 12 n r f b c r m s e f b c r m s e f m l where nrfbc is the normalized rmse f for a given bias correction method nrfbc ranges from zero to one a value of zero indicates a perfect match between the bias corrected ecdf and the observed ecdf no error a value of one indicates that the bias correction method had no effect on the ml estimates a value close to one could occur because the bias correction method was ineffective for adjusting biased yml values or because the initial yml values were nearly unbiased equation 12 is used for comparing performance when rmse f is computed using retransformed units as compared to log units 3 data four case studies used for assessing bias correction methods the performance of the bias correction methods was evaluated using data from four previously published usgs nawqa case studies that applied brt modeling to the regional scale mapping of groundwater quality table 1 two of the case studies were developed with ph as the dependent variable the glacial aquifer system of the northern u s stackelberg et al 2020 and the north atlantic coastal plain nacp desimone et al 2020 the other two case studies were developed with natural log transformed concentration as the dependent variable nitrate in the central valley cv of california ransom et al 2017 and iron in the mississippi embayment mise of the south central u s knierim et al 2020 metrics for the cv and mise case studies were computed in units of log concentration and in units of retransformed concentration nitrate and iron are of concern to human health if concentrations in drinking water exceed 10 mg per liter and 4000 μg per liter respectively u s epa 2020a norman et al 2018 4 results and discussion the glacial ph case study has the most amount of data table 1 and the results from that case study are discussed in the first subsection the results from the glacial ph case study provide context for the subsections that follow the second subsection evaluates results from the other case studies with performance metrics computed in the same units as the dependent variables in the brt models the third subsection evaluates results when the metrics are computed in retransformed units there are six bias correction methods four case studies and four metrics and for each case study there are training data and there are holdout data therefore only selected results are presented in the main body of the text other results are presented in the supplement the observed values ml estimates and bias corrected estimates for the four case studies are also available belitz et al 2021 4 1 glacial ph case study five of the six bias correction methods were applied to the glacial ph data the roe duan method was not applied because it is not applicable to the case study the metrics associated with each method are presented in table 2 along with the metrics computed for the uncorrected ml estimates as expected the metrics for all five methods are better for the training data than for the holdout data additional summary statistics mean variance minimum and maximum for the training and holdout data for the five bias correction methods are presented in table s1 4 1 1 rmse f for glacial ph case study rmse f quantifies the match between estimated and observed ecdfs and the lowest values for the glacial ph case study are provided by the edm bias correction method table 2 by definition the edm method provides an exact match to the observed ecdf for the training data and therefore the rmse f is equal to zero the edm method also provides the lowest value of rmse f for the holdout data and by that standard is the best performing method visual inspection of the ecdfs fig 4 illustrates the point the ecdf for the edm corrected holdout data is relatively close to the observed ecdf which is the objective of the current work the ml2 res method provides a low value of rmse f for the training data 2nd lowest value table 2 but does not provide a correspondingly low value for the holdout data indeed the rmse f for the holdout data 0 15 is closer to the value computed for the uncorrected ml estimates 0 20 than it is to the value computed for the edm method 0 07 likewise the ml2 res ecdf is closer to the ecdf for the uncorrected ml estimates than to the observed ecdf fig 4 the rmse f values for the ltf and zz methods are comparable to one another and are about halfway between the values computed for the uncorrected ml estimates and the edm corrected estimates table 2 the ecdfs for the two methods also are about halfway between the observed ecdf and the uncorrected ecdf for the holdout data fig 4 and for the training data fig s1 the point scale roe method provides values of rmse f for both the training and holdout data that are relatively close to the values computed for the uncorrected ml estimates table 2 the ecdfs for the holdout data fig 4 and training data fig s1 are relatively close to the uncorrected ml ecdfs roe is relatively ineffective for adjusting the ecdf for this case study 4 1 2 bias f rmse y and bias y for glacial ph case study the other three metrics allow for additional assessment of the performance of the bias correction methods the bias f values for the training data are very low for all five methods table 2 the values for the holdout data are not as low as the training data but are relatively close to one another with differences amongst the methods occurring in the third significant digit thousandths of a ph unit a shift of less than one hundredth of a ph unit is not easily discerned on the plot of the ecdfs fig 4 rmse y is an overall measure of fit and can be viewed as an appropriate metric for correcting point scale bias as illustrated in fig 1a zhang and lu 2012 the ml2 res method provides a value of rmse y for the training data that is substantially lower than the other methods table 2 however the ml2 res method does not provide a correspondingly low value of rmse y for the holdout data table 2 overall the five bias correction methods provide comparable rmse y values for the holdout data bias y is the traditional indicator of bias the bias y values for the training data are very low for all five bias correction methods table 2 the values of bias y for the holdout data are not as low as for the training data but are low with respect to the mean and range of the ph data table 1 when performance is assessed using the bias f rmse y and bias y metrics the five bias correction methods are comparable to one another and to the uncorrected ml estimates for the glacial ph case study these results are in contrast to those for rmse f whereby there were differences amongst the methods 4 1 3 comparison of ltf and zz methods for the glacial ph case study the ltf and zz methods are both implemented at the distribution scale and apply a linear transformation to the data for the glacial ph case study the two methods provide values that are nearly identical to one another for all four metrics table 2 the ecdfs for the holdout data fig 4 and training data fig s1 are also similar for the two methods the close correspondence between the two methods arises because the correlation coefficient between y ml as the independent variable and y edm is close to one rltf 0 995 as shown in fig 3 the lines for the ltf and zz methods are nearly colinear by definition the ltf method is a linear approximation of the edm method and given the high correlation between y ml and y edm for the glacial ph case study the zz method can also be seen as an approximation of the edm method the ltf and zz methods are both parametric and some researchers piani et al 2010 citing robustness prefer parametric over empirical methods with rmse f as the primary metric neither the ltf nor zz methods are as good as the edm method for this case study table 2 4 1 4 potential issues related to ml2 res bias correction method several issues arise when applying the ml2 res correction method to the glacial ph case study one issue is the possibility that implementation of a second brt model results in over training as indicated by the values of rmse f and rmse y for the training data the value of rmse f computed for the ml2 res corrected estimates is substantially smaller than the value computed for the uncorrected ml estimates but a comparable improvement is not seen for the holdout data in contrast the performance of the other correction methods for the training data as indicated by the ordinal positions of rmse f is maintained in the holdout data a comparable pattern can be seen in the values of rmse y scatter plots illustrate the point the ml2 res estimates plot close to the one to one line for the training data fig s2 but not for the holdout data fig s3 the potential over training occurred even though the second brt model like the first was developed using ten fold cross validation to identify model meta parameters stackelberg et al 2020 the ml2 res method is also computationally intensive relative to the other methods because it requires development of a second brt model in addition the ml2 res method can be seen as an extension of the brt approach because brt modeling works with residuals given the potential for over training computational intensity and conceptual overlap with brt the ml2 res method was not applied to the other three case studies 4 2 additional case studies north atlantic coastal plain central valley and mississippi embayment given the findings from the glacial ph case study three aspects of the additional case studies are discussed 1 rmse f as the primary metric for comparison of methods supplemented by visual comparison of the ecdfs 2 bias f rmse y and bias y as secondary metrics for comparison and 3 relation between the ltf and zz methods metrics are computed using the units of the dependent variable in the ml models the values for rmse f are presented in table 3 for all three case studies the edm correction method provided the lowest values of rmse f and the roe correction method provided the highest values the rmse f values for the ltf and zz correction methods are comparable to each other the relative performance of the edm roe ltf and zz methods is the same as that for the glacial ph case study the ecdfs for the holdout data illustrate the quantitative results figs 5 7 for all three case studies the edm ecdf is relatively close to the observed ecdf and the roe ecdf is relatively close to the uncorrected ml ecdf additional ecdfs for the three case studies are presented in the supplement figs s4 and s5 the values for bias f rmse y and bias y are presented in tables s2 s3 and s4 respectively for all three case studies the differences amongst the correction methods for each of the three metrics are small when compared to the mean and range of the observed data table 1 additional summary statistics mean variance minimum and maximum are presented in tables s5 s7 the results for the three case studies are similar to those for the glacial ph case study the ltf and zz methods are relatively similar to one another for all four metrics and differ from the other methods tables 3 s2 s3 and s4 the close correspondence between the ltf and zz methods arises because the correlation coefficient between y ml and y edm equation 10 is close to one for all three case studies rltf 0 998 0 997 and 0 998 for the nacp cv and mise case studies respectively as shown in fig 8 the lines for the ltf and zz methods are nearly colinear for all three case studies as they were for the glacial ph case study the results for the nacp cv and mise case studies confirm the results for the glacial ph case study the best match with the observed ecdf based on rmse f is provided by the edm correction method the poorest match with the observed ecdf was provided by the roe method the differences amongst the methods for the three secondary metrics are generally small 4 3 ml models developed using natural log transformed concentration two of the case studies cv nitrate and mise iron were developed with natural log transformed concentration as the dependent variable in this section the performance of four bias correction methods edm ltf zz and roe duan are evaluated in terms of retransformed concentration the metrics for the training and holdout data are presented in tables s8 and 4 respectively performance is assessed using the holdout data summary statistics mean variance minimum and maximum for the training and holdout data for the cv and mise case studies are presented in tables s9 and s10 respectively prior to application of the duan smearing estimate 1983 the roe method was used to adjust the ml estimates the residuals for the training data after adjustment by roe were uniformly distributed around the x axis for both case studies figs s6 and s7 the resulting equations based on the training data were then used to adjust the holdout values for both case studies 4 3 1 rmse f computed in retransformed concentration for the cv holdout data the lowest value of rmse f computed in retransformed concentration is provided by the edm method and the highest value is provided by the roe duan method table 4 the ltf and zz methods provide values of rmse f that are quite close to the value provided by the edm method the ordinal position of the four methods is the same as when rmse f was computed in log concentration table 3 for the mise holdout data the lowest value of rmse f computed in retransformed concentration is provided by the roe duan method and the highest value is provided by the edm method table 4 this is the opposite of the results when the same metric was computed in log concentration table 3 the better performance of the roe duan method reflects the relatively close match at relatively high concentrations between the roe duan and observed ecdfs fig 7 the close match at high concentrations more than offsets the relatively poor match at relatively low concentrations the converse occurs for the edm method the ltf and zz methods provide values of rmse f that are closer to those provided by the roe duan method than to those provided by the edm method nrfbc a dimensionless metric allows for a comparison of rmse f computed in retransformed units relative to results computed in log units table 5 for three of the methods edm ltf and zz and for both case studies nrfbc is smaller when the metric is computed using log concentration for roe duan the opposite is true nrfbc is smaller when computed using retransformed concentration the edm ltf and zz methods were implemented using log concentration data and performance declined when nrfbc was computed in retransformed concentration in contrast the duan 1983 smearing estimate is intended to minimize bias y when it is computed in retransformed units this is achieved by multiplying the retransformed estimate by a constant value equation 9 consequently performance was better when nrfbc was computed in retransformed concentration as compared to when the metric was computed in log concentration overall the lowest value of nrfbc for both case studies is for the edm method when the metric was computed in log concentration 4 3 2 bias f rmse y and bias y computed in retransformed concentration the other three metrics bias f rmse y and bias y allow for additional assessment of the performance of the bias correction methods when these metrics were computed using log concentration the differences amongst the methods were small in contrast the differences are larger when computed in retransformed concentration table 4 particularly for the mise case study for comparison the means and ranges computed in concentrations units are provided in tables s9 and s10 for the cv and mise case studies respectively for the cv holdout data the roe duan method provided the best performance for all three metrics when they were computed in retransformed concentration table 4 for the mise holdout data the roe duan method provided the best performance for the bias f and bias y metrics and the edm method provided the best performance for the rmse y metric table 4 overall roe duan provided the best performance when the other three metrics were computed in retransformed concentration 4 3 3 assessment of performance based on retransformed units as compared to log units when performance is assessed in retransformed units a bias correction method is evaluated on an absolute basis and therefore small values have lesser influence than large values in contrast if metrics are computed in log units performance is assessed on a relative basis and small values can be as influential as large values whether it is more important to obtain a match on an absolute or relative basis is a subjective decision for example if it is important to accurately estimate high values then assessment of performance on an absolute basis would be appropriate similarly if is it important to estimate the total mass of a constituent within a given area of an aquifer then it would be important to correctly estimate the mean value in concentration units and it would therefore be appropriate to assess performance on an absolute basis alternatively if it is important to accurately represent the full range of values then it would be better to assess performance on a relative basis 5 conclusions ensemble tree ml regression models are useful for understanding and evaluating environmental systems output from these models however can be systematically biased small values are overestimated and large values are underestimated for many environmental applications this can be a problem because it is as important or more important to correctly estimate the tails of the distribution as it is to correctly estimate the mean or other central values the primary goal of this paper was identification of a bias correction method that adjusts ml estimates so that the empirical cumulative distribution function ecdf for the adjusted estimates approximates the observed ecdf five methods were evaluated for correcting ml bias empirical distribution matching edm regression of observed on estimated roe values linear transfer function ltf z score transform zz and the use of a second ml model for estimating residuals ml2 res four previously published usgs nawqa case studies that used brt ml modeling of groundwater quality were used for assessing the methods ph in the glacial aquifer of the northern u s ph in the north atlantic coastal plain log nitrate in the central valley of california and log iron in the mississippi embayment southeastern u s an additional source of bias can be introduced into statistical models including ml models if the dependent variable is a transform of the original data a sixth bias correction method roe duan was evaluated for the two case studies developed using log transformed concentration the roe duan method is based on duan s 1983 smearing estimate the roe method was used to ensure that the residuals are homoscedastic and independent an assumption incorporated into the development of duan smearing estimate the bias correction methods were calibrated using training data and the performance of the methods was assessed using training and holdout data two of the performance metrics evaluate the match between the adjusted and observed ecdfs rmse f and bias f two of the metrics evaluate the match between adjusted and observed points rmse y and bias y rmse f was identified as the primary metric for assessing performance because of the goal of reproducing the tails of the observed distributions for the central valley and mississippi embayment case studies the metrics were computed in log concentration the units of the ml models and in retransformed concentration conclusions are based on holdout data when the performance metrics were computed in the units of the ml models the results were comparable for all four case studies rmse f varied amongst the different bias correction methods but the other three metrics did not vary substantively the distribution scale edm method was the most effective and the point scale roe method was the least effective for correcting systematic bias in the four case studies evaluated when the performance metrics for the central valley and mississippi embayment case studies were computed in retransformed concentration the results differed from when the metrics were computed in log concentration with rmse f computed in retransformed concentration the best result was provided by the edm method for the central valley and by the roe duan method for the mississippi embayment the values of the other three metrics varied for both case studies amongst the different methods overall the roe duan method provided the best results when the other three metrics were computed using retransformed concentration the roe duan method was the most effective for correcting introduced bias bias in the mean if performance metrics are computed in retransformed units then a bias correction method is assessed on an absolute basis and large values can dominate the computation if the metrics are computed in log units then performance is assessed on a relative basis and small values can also be important whether it is more important to maximize absolute performance or relative performance is a subjective decision the objectives of a study should be considered when making that decision to the extent that log units are chosen for developing an ml model that decision has been partly made the ltf method is a linear approximation of the edm method and for the four case studies evaluated the results are close to the edm method the zz method can approximate the ltf method if the rank ordered ml estimates are highly correlated to the rank ordered observed values for the four case studies evaluated the correlation was high and the zz method closely approximated the ltf method if a simple parametric method is preferred for bias correction then the zz method might be an appropriate choice an important check would be to evaluate the correlation between the rank ordered ml estimates and rank ordered observed values the ml2 res method was only applied to the glacial ph case study the method did not significantly improve the match to the observed ecdf for either the training or holdout data the method was not applied to the other case studies because of potential over training computational burden and conceptual overlap with the brt approach bias correction can be an important part of ensemble tree ml model development but it is not a replacement for developing a model that provides accurate estimates declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105006 software and data availability this article presents mathematical approaches and equations for correcting for bias in results from ensemble tree regression models this effort did not produce software to be used by others the data for the four case studies presented in this paper are available through the sciencebase catalog of the u s g s at https doi org 10 5066 p9lctyi2 belitz et al 2021 
25866,computer models of environmental systems routinely inform decision making for water resource management in this context quantifying uncertainty in the important simulated outputs and reducing uncertainty through assimilating historic system state observations is as important as the numerical model however implementing high dimensional and stochastic workflows are challenging often requiring that practitioners have theoretical and practical understanding of several advanced topics worse implementing these important analyses can take substantial time and effort this additional effort is often cited as justification for postponing or even forgoing these analyses herein we present scripting tools to facilitate the efficient and repeatable construction of high dimensional geostatistical based pest interfaces including uncertainty analyses as demonstrated these tools can be applied with minimal effort to a model with varied temporal and spatial discretization ultimately these tools can enable low cost access to valuable decision support analyses earlier and more frequently during the modeling workflow keywords uncertainty analysis decision support automating workflows cognitive load model criticism prior data conflict 1 software availability the capabilities presented herein are available within the open source python based pyemu module available at https github com pypest pyemu with documentation provided at https pyemu readthedocs io en latest pyemu examples including for the methods presented in this study are provided in jupyter notebook form jupyter 2016 kluyver et al 2016 in that repository https github com pypest pyemu tree develop examples a specific stepwise demonstration of the automatic pest interface construction methods is provided at https github com pypest pyemu pestpp workflow with a browser based interactive runner housed on binder jupyter et al 2018 at https mybinder org v2 gh pypest pyemu pestpp workflow feat binder filepath setup pestpp interface ipynb the files and script that implement the pest interface construction method in the example application presented here are available at https github com jtwhite79 auto pest 2 introduction computer based environmental models such as numerical groundwater models combined with formal uncertainty analysis and data assimilation provide a quantitative framework for model based water resource management decision support e g anderson et al 2015 doherty and moore 2019 while it is widely recognized that both the quantification of uncertainty in simulated outputs and the reduction of this uncertainty through data assimilation are fundamental and critical components of decision support modeling e g freeze et al 1990 gupta et al 2006 these analyses can require practitioners to understand both advanced theoretical and practical topics underpinning their application see for example the topics in tarantola 2005 oliver et al 2008 doherty 2015a furthermore implementation of these analyses can require a substantial investment of time and effort we believe that the cost of this investment encourages practitioners to postpone such analysis until a later stage in the modeling workflow to a point when the model is deemed ready this is unfortunate because uncertainty analyses and data assimilation much like the model construction process will almost certainly need to be applied iteratively and in a step wise manner e g haitjema 1995 where complexity in the desired uncertainty and or data assimilation analysis is added incrementally potentially requiring revision of the model itself additionally in the authors experience the outcomes of uncertainty and or data assimilation analyses will invariably reveal issues and faults in the underlying model and the data used to inform model properties through data assimilation this reflects the ubiquity of error donoho et al 2008 that accompanies modern computational science finding and addressing these issues and faults early in computational workflow is of primary importance therefore to improve the overall decision support modeling outcomes there are significant benefits to undertaking uncertainty quantification and or data assimilation analyses early and frequently in the numerical modeling workflow hemmings et al 2020 model data assimilation and uncertainty quantification has been facilitated by the development of model independent inverse modeling software such as pest doherty 2015b and pest white et al 2020a these programs are non invasive and operate through the forward model s own input and output files via a file based interface of template for parameters instruction for observations and control files however the construction of interfacing environment can be burdensome and laborious particularly when the desired dimensionality of parameters and observations is high to reduce the cost and burden of implementing uncertainty and data assimilation analyses and therefore help satisfy the need for earlier and more frequent analyses we have developed routines implemented in the python programming language that automate the construction of the pest model interface these python routines can be applied with minimal effort to generate very high dimensional and complex pest interfaces with nested spatial and temporal scales of parameterization e g mckenna et al 2019 these routines also automate the construction of geostatistical based prior parameter covariance matrices and sampling of associated prior parameter ensembles in this way as practitioners are constructing increasingly complex numerical models valuable uncertainty and data assimilation analyses can be applied more easily this will help identify issues and faults earlier in the modeling workflow at a stage where the cost of addressing these issues is much lower additionally as we demonstrate these analyses can also be used as a robust basis for judging the value of increased model complexity during the model development process e g doherty and moore 2019 by reducing the cost of implementing high dimensional stochastic analyses we hope to reduce the cognitive load that practitioners must devote the implementation details so they can devote more cognitive resources to higher order tasks and analyzing results 3 methods we use python based routines encapsulated in the pyemu module white et al 2016 to automate the pest interface construction process these routines are designed to work with array type homogeneous data type with values arranged in two dimensional arrays and list type heterogeneous column oriented data type with values referenced by identifiers in rows model input and output files specifically parameters within the pest interface can be handled using both array type and list type file format structures whereas observations within the pest interface are handled using only list type file format structures by supporting these generic and widely used file formats it is expected that these routines can be easily applied to a wide range of numerical environmental models including but not limited to the modflow family harbaugh 2005 niswonger et al 2011 langevin et al 2017 hughes et al 2017 these tools can also be used in conjunction with other python based model interaction tools such as flopy bakker et al 2016 to further automate the modeling workflow the python routines support a range of parameterization devices that represent differing spatial and temporal scales including pilot points doherty 2003 doherty et al 2010 importantly these parameterization devices can be expressed as direct or multipliers according to the requirements of the practitioner direct parameters are native forward model input values defined in the appropriate units required for the model multiplier parameters act on the initial native forward model input values meaning that they are multipliers against absolute quantities encapsulated in the original array type and list type model input files for the examples presented here only the multiplier parameterization scheme is used however support is provided for combining direct and multiplier parameters in a single interface constructions and even for individual model input files in a bayesian framework e g tarantola 2005 doherty 2015a casting parameters as multipliers allows the existing model input files to serve directly as the mean of the prior parameter distribution the multiplier parameter approach also facilitates a more intuitive process for defining the covariance matrix of the prior parameter distribution a fundamental quantity for uncertainty and data assimilation analyses because the parameters in such an approach represent multipliers against the existing model input files practitioners can think of parameter uncertainties as percent ranges around the initial model input values rather than absolute quantities this can be a more intuitive approach for describing prior uncertainties the multiplier concept also lends itself to apportioning spatial and temporal model input uncertainty into specific spatial and temporal scale parameters e g mckenna et al 2019 white et al 2020b for example practitioners can specify grid scale multiplier parameters where grid scale indicates a unique value in each cell of a computational grid to account for plausible fine scale heterogeneity these grid scale multiplier parameters can be combined with pilot point multiplier parameters which account for broader scale heterogeneity as well as uniform parameters over the entire model domain or within zones corresponding to e g mapped lithological units which can account for large scale biases in model input quantities including boundary conditions e g mckenna et al 2019 white et al 2020b knowling et al 2019 similarly multi scale multiplier parameter schemes can also be defined for time varying parameters so that daily seasonal annual scale and or decade scale long term model input uncertainty can be explicitly represented in the parameters to account for plausible variability across these scales by explicitly partitioning expected uncertainties into these various scales the manifestations of model error such as parameter compensation e g clark and vrugt 2006 white et al 2014 can be more easily detected from an implementation standpoint using multi scale multiplier parameters allows practitioners to access to a wide range of decision support analyses with a single pest interface for example to undertake data worth analyses fienen et al 2010 doherty 2015b white et al 2016 the number of adjustable parameters must be limited to a few thousand to manage computational burden likewise global sensitivity analyses e g saltelli et al 2008 require the parameter dimensionality be limited to less than a few hundred to avoid an unacceptably high computational burden rather than having to construct a separate pest interfaces for each of these analyses practitioners can use the pest and pest input options to simply fix and tie parameters of different scales to reduce dimensionality where necessary for these valuable decision support analyses see 15 for a demonstration of this technique the benefits of a sophisticated multi scale multiplier parameter process described above are compelling however as discussed above implementing such a scheme can be daunting to that end the routines developed in pyemu pstfrom implement the multiplier parameter process seamlessly that is without any practitioner intervention and or manual file manipulation first the pstfrom python routines make copies of the existing model input files that have been nominated for parameterization and store them in a separate sub directory then during the forward run process a preprocessor function which is wrapped in a multi threaded process during the forward run process loads the original model input files performs the requisite multiplication process and then writes the resulting product quantities to the location where the numerical model is expecting to read the corresponding input file 3 1 example function calls the following example functions calls are available from within the pyemu github repository https github com pypest pyemu and from a dedicated example repository https github com pypest pyemu pestpp workflow an example can be run from a stand alone browser based environment available at https mybinder org v2 gh pypest pyemu pestpp workflow feat binder filepath setup pestpp interface ipynb to start the automated pest interface construction one must first instantiate the pstfrom class object example 1 image 1 the arguments passed to this constructor define where the existing model input dataset can be found original d where the combined pest interface and model input dataset should be placed new d as well as other model and interface specific options here we ve only shown the most basic call structure the pstfrom class is designed to be called in the directory level above where an existing set of model files reside indicated by the original d argument the instantiation call e g example 1 will copy these model files into a new directory the new d argument and construct the complete pest interface in the new d directory in this way the original set of model files are preserved and the resulting files in the new d directory are a complete set of model and pest files that can be distributed for parallelizing pest and pest in high throughput and or massively parallel high performance computing environments fienen and hunt 2015 within the new d directory in addition to the files found in original d the pstfrom process will create two subdirectories called org and mlt these subdirectories will hold the original model input files and multiplier parameter files respectively once a pstfrom instance has been created parameters and observations need to be added to the pest interface practitioners can add parameters through the following method example 2 image 2 in this example method call multiplier pilot point parameters were specified for the model input files recharge 1 txt and recharge 2 txt which are array type files that exist in the original d directory for example these files could be modflow 6 input arrays for recharge in this example these files represent the spatial distribution of groundwater recharge for two consecutive stress periods times in the existing model in this case a single set of multiplier parameters is applied in a broadcast operation across both model input files recharge 1 txt recharge 2 txt effectively this constructs pilot point parameters that are temporally constant across these two time periods the broadcasting operation is supported in a completely general way and can be applied to any set of input files provided that the user supplies a list of files to the filenames argument upper and lower bounds specified through the upper bound and lower bound arguments respectively define the allowable range of multiplier values which have an initial value of 1 0 in some cases extreme values in the underlying model input parameters may approach physically important bounds in this case specifying the bounds on multipliers such that those physically important bounds at few locations aren t violated can unnecessarily restrict parameter variability at other spatio temporal locations as a result the arguments ult ubound and ult lbound set ultimate i e after the application of multipliers model parameter upper and lower bounds respectively if multiplier parameters drive model input parameters beyond these ultimate bounds the native model parameter values are reset to these bounds this is particularly important when a number of multipliers parameters are combined e g constant grid etc in the example 2 call the pilot point parameters constructed with a uniform spacing across the model domain their expected spatial correlation is defined by a default geostatistical structure the same geostatistical structure is also used for interpolation from point to grid scale values the default values for these options can be overridden by supplying the requisite arguments to the add parameters method e g pp space and geostruct respectively a model specific default geostatistical structure can also be defined by passing a pyemu geostats geostruct object to the geostruct argument when instantiating a pstfrom object example 1 supported spatial scale parameterization devices specified by the par type argument in example 2 include constant a single parameter value for all entries in a file zone zone parameters of uniform value requires zone array argument pilot points uniformly spaced pilots points and grid unique parameters for every entry in the file combinations of all these types of parameters can be supplied to the pstfrom object through calls to the add parameters method for parameters that apply only to a single model input file without the broadcasting behavior indicated in example 2 a single filename is passed for the filenames argument rather than a list using a similar approach pest interface observations are added through the following method example 3 image 3 in this example method call sfr csv is a modflow 6 stream flow routing sfr niswonger and prudic 2005 observation output file the add observations method writes the corresponding pest instruction file sfr csv ins and sets up pest observations for all columns in this observation file except for the time column which is treated as the index for building automatic observation names depending on the information practitioners pass to the setup routines the resulting observation names may contain both the filename and date time information to facilitate easier interpretation of results once a user has called add parameters and add observations to include the desired model input and output files in the pest interface a call to pf build pst constructs all of the pest interface files needed to execute either pest or pest this call includes constructing and writing the pest control file and associated template and instruction files as well as writing the forward run script needed to execute the forward model run process in pest analyses these files are written to the new d directory practitioners can then construct a geostatistical based prior parameter covariance matrix and draw a prior parameter ensemble with pf build prior and pf draw respectively facilitating a low cost entry point into high dimensional bayesian analyses such as prior based monte carlo 4 example application to demonstrate the use of this python based pest interface construction tool we apply these routines to two different modflow 6 versions of the synthetic model of white et al 2020a which is based on the model of freyberg 1988 fig 1 see also hunt et al 2019 the commands used for this example application are presented in supplementary material in https github com pypest pyemu pestpp workflow blob master prior monte carlo ipynb and in runnable form at https mybinder org v2 gh pypest pyemu pestpp workflow feat binder filepath setup pestpp interface ipynb the two model versions differ in their spatial and temporal resolution but are otherwise identical a lower resolution model described in white et al 2020a has 3 layers 40 rows and 20 cols and simulates a single steady state stress period followed by 24 monthly stress periods a higher resolution version of the model has 120 rows 60 columns and 3 layers and uses 730 daily stress periods the outputs of interest from the numerical simulations include locations where synthetic historical system observations have been collected surface water flow at location sw 1 and groundwater levels at location gw 3 as well as two simulated output quantities surface water groundwater exchange flux during the last stress period aggregated into the upper 20 reaches headwater and lower 20 reaches tailwater fig 1 these simulated outputs constitute the numerical model outputs that are relevant for the specific decision making context that the modeling is supporting they are referred to as decision relevant simulated outputs the simulation has time varying recharge so that each annual cycle includes a dry and wet period herein we focus exclusively on prior based monte carlo as an uncertainty analysis approach prior based monte carlo involves the propagation of realizations drawn from the prior parameter distribution without undertaking any history matching parameter conditioning adjustments and therefore foregos the need to define a likelihood objective function such analysis only requires one model run per parameter realization furthermore in many settings depending on the nature of the decision relevant simulated outputs prior based monte carlo is not only a computationally efficient choice but it may also help to avoid biases induced by history matching e g knowling et al 2019 2020 white et al 2019 we believe prior based monte carlo is an excellent candidate uncertainty analysis approach for the developmental stage of a numerical groundwater model analysis because it has a relatively low computational burden yet it can assist in elucidating both conceptual and numerical issues that may exist in the underlying numerical model it provides opportunities for model criticism e g alfonzo and oliver 2019 oliver 2020 such as prior data conflict assessments e g evans moshonovet al 2006 to ultimately help identify potential model and data deficiencies earlier in the modeling workflow at a point in time when they can be more efficiently addressed given that the prior based monte carlo analysis requires only one run for each realization in contrast with linear uncertainty analysis approaches which require a number of model runs that is proportional to the number of adjustable parameters there is little or no computational penalty for using very large numbers of parameters as such it is computationally feasible to employ several scales of spatial and temporal parameters for both static properties and boundary conditions as discussed above by using a full compliment of multi scale multiplier parameters to represent known expected model input uncertainty we endeavor to yield conservative uncertainty estimates for decision relevant simulated outputs and the simulated equivalents to observations the latter is an important consideration for model criticism and prior data conflict analyses for array type model inputs the following model input quantities were parameterized using a mixture of grid scale and pilot point multiplier parameters horizontal hydraulic conductivity vertical hydraulic conductivity specific storage specific yield exponential variograms with ranges of 500 length units and 2000 length units and sills proportional to the range defined by the parameter bounds were used to describe prior parameter uncertainties for array type grid scale and pilot point parameters respectively note that the default range for geostatistical characterization of pilot points is the pilot points spacing which defaults to 10 model cells multiplied by the maximum row or column spacing the default variogram is of exponential form recharge was parameterized with a single set of time invariant grid scale parameters broadcast across all simulation stress periods these were combined with a constant multiplier parameter for each simulated stress period the grid scale parameters are designed to capture spatial variation and uncertainty in recharge using an exponential variogram with range of 1000 length units the constant recharge parameters were assigned temporal correlation using an exponential variogram with range 60 time units these constant recharge multiplier parameters were designed to capture temporal recharge uncertainty assigning the variogram to the temporal parameters enforces a memory on the recharge parameters preventing spurious correlation over long periods of simulation time the list type model input quantities for groundwater extractions implemented using the wel package in modflow 6 were also parameterized with a mixture of grid scale and constant multipliers across all stress periods again the spatially constant parameters were used to represent temporal uncertainty and expected correlation in these specified extraction flux rates and were assigned an exponential variogram with a range of 60 time units the grid scale parameters effectively parameterize the flux rate for each simulated extraction well for each stress period these grid scale parameters were assigned an exponential variogram with range of 1000 length units the wel package parameterization is conceptually the same as that used to parameterize the recharge to the model the sfr package simulating stream flow routing streambed hydraulic conductivities were also parameterized using grid scale parameters effectively parameterizing the conductance of each simulated reach the prior upper and lower bounds assigned to these parameters are shown in the supplementary material to demonstrate the generality of our approach we use the same python script to construct a high dimensional pest interface as above around both versions lower resolution and higher resolution of the modflow 6 forward model the parameterization scheme for the lower resolution model yielded a total of 9766 parameters and 725 tracked simulated outputs or observations using the same parameterization scheme for the higher resolution model version results in 88 911 parameters and 21 199 tracked simulated outputs or observations for both version the workflow yields a 100 realization geostatistically correlated prior parameter ensemble for each version importantly the script only requires adjustment of the original d argument to point to the desired resolution model it requires no further changes for the analysis herein we evaluated the 100 realization prior ensembles for both model discretizations in parallel using pestpp ies white et al 2020a white 2018 on a laptop computer additionally we used the default parallel run manager settings in pestpp ies to treat any realization that takes more than twice the average runtime as a failed run effectively preempting realizations that cause unacceptably long runtimes this can be thought of as a greedy heuristic approach to improve analysis performance treating the ensemble evaluation in this way can be an important consideration for prior based monte carlo analyses since the analysis is being used in the developmental stage of a modeling workflow and the value of any one realization is relatively low compared to the time penalty of waiting for slow running realizations the script used to construct the pest interfaces for the two models and process the subsequent prior based monte carlo analyses output is available at https github com jtwhite79 auto pest additionally an interactive example of the pstfrom pest interface construction workflow is provided as a jupyter notebook jupyter 2016 kluyver et al 2016 example in the pyemu module github https github com pypest pyemu and also as a stand alone demonstration at https github com pypest pyemu pestpp workflow with a browser based interactive runner housed on binder jupyter et al 2018 at https mybinder org v2 gh pypest pyemu pestpp workflow feat binder filepath setup pestpp interface ipynb additionally the pstfrom function used to construct the pest interface for both models is shown in the supplementary material 5 results figs 2 and 3 summarize the results of the 100 realization ensemble evaluations for the simulated outputs at observation locations and drso quantities respectively in general for the time varying outputs of interest gw 3 and sw 1 the daily stress periods of the higher resolution model yield simulated outputs with higher frequency components compared to the lower resolution outputs fig 2 the models yield similar uncertainty e g variances for the decision relevant simulated output quantities but the higher resolution model yielded less groundwater discharge to surface water that is the higher resolution decision relevant simulated output probability distributions had larger less negative mean values fig 3 for demonstration purposes we display hypothetical observation time series in fig 2 these are hypothetical observations because this is a synthetic demonstration the hypothetical observations are noisy values from a randomly selected higher resolution monte carlo realization comparing the monte carlo results with these hypothetical observations we see that the lower resolution model may not be suitable for history matching because generally it does not reproduce the temporal variability of the observations and in case of the surface water flow observations the lower resolution model does not statistically cover the observed low flow surface water observations depending on the stated goals of the modeling analysis this may be considered an example of prior data conflict discussed in detail below all 100 realizations of lower resolution model terminated successfully and produced an average forward model runtime of 45 s likewise all 100 higher resolution model realization terminated successfully and produced an average forward model runtime of 10 min knowledge of runtime distributions can be used in combination with the simulated outputs at observation locations fig 2 and the decision relevant simulated output quantities fig 3 to judge the value of the higher resolution model as an improved decision support tool 6 discussion and conclusion we have presented a set of python routines that facilitate the efficient and repeatable construction of high dimensional pest model interfaces that can be used to undertake data assimilation and uncertainty analyses for decision support undertaking high dimensional stochastic analyses earlier and more frequently in a modeling workflow will help practitioners gain important insights into model behavior the information contained within a stochastic model analysis far exceeds what is provided by a single forward model run for example as a result of the prior based monte carlo workflow the numerical stability or otherwise and runtimes of the simulation s sampled across parameter space will be well characterized before attempting any form of history matching this can be an important factor when selecting history matching approach and also for managing expectations planning project time lines and coordinating computational resources the comparison of the lower and higher resolution prior based monte carlo results herein demonstrates how choices related model complexity can be evaluated in a rigorous and statistical setting during model development for example if a new and or more complex simulation process is added to the model the effect of this process can and should be compared to previous existing prior based monte carlo results to understand how this new process has effected in a stochastic sense the important simulated outputs in the examaple presented here these outputs include both the decision relevant simulated output quantities and if history matching is to be undertaken then results should also be compared at spatio temporal locations of historic system state observations in this way the prior based monte carlo becomes a partner analysis in the incremental e g step wise 11 model development process we believe one of the most important insights that can be gained from easily implemented but high dimensional prior based monte carlo is related to prior based model criticism e g oliver 2020 specifically the detection of prior data conflict e g evans moshonovet al 2006 if the prior based monte carlo reveals a significant discrepancy between simulated outputs and associated system observations there is limited justification for trying to reproduce these observations during history matching the consequence of attempting to match conflicted observations can result in extreme parameter values and or parameter combinations with implications for model forecast reliability e g knowling et al 2019 clark and vrugt 2006 white et al 2014 if prior data conflict is identified it provides an early opportunity to employ mitigation strategies these strategies must target either the prior parameter distribution or the likelihood function and might include reduction of conflicting observation weights during history matching where the assimilation of observation is not considered to be informative of the decision relevant simulated output e g doherty and welter 2010 an extreme example of this would be removing conflicted observations entirely from the history matching process e g knowling et al 2020 evaluation of the quality of observations that will be used for history matching while this is a typical heuristic element of a history matching workflow it is often performed later in the process when modeled outputs are consistently not deemed to be sufficiently close to observed values after multiple iterations of history matching prior data conflict provides the opportunity to be alerted to this possibility early and systematically prior inflation where the prior parameter uncertainty is inflated to account for structural deficiencies that may be impacting the model s ability to reproduce system observations e g o hagan and pericchi 2012 and model improvements where changes are made to model structure simulated processes and or parameterization to improve the model s ability to represent the processes related to the conflicting observations e g alfonzo and oliver 2019 oliver 2020 identifying and evaluating prior data conflict is only possible in high dimensional stochastic settings such as those available with prior based monte carlo while the process of identifying conflicts is subjective the authors struggled to define which observations in the example should be nominated as conflicts and this process will likely require in depth discussion s between the practitioner s and stakeholders undertaking this process earlier in the modeling workflow will enable a more efficient diagnosis of the cause and implications of these conflicts compared to the surprise that otherwise occurs the first time history matching is attempted the benefits of earlier and more frequent prior based monte carlo analysis as enabled by the tools presented herein cannot be overstated in our opinion the insights gained from performing this simple yet powerful and often overlooked analysis leads to substantially less of what we term stochastic desperation a situation where a practitioner tries varying multiple algorithmic inputs simultaneously and in non objective ways in a last ditch effort to find an acceptable history matching result this usually occurs at a late stage in the project and typically coincides with a holiday weekend as the project budget expires instead the tools we have presented facilitate low cost access to a range of decision support analyses which if implemented earlier in the modeling workflow will afford more complete understanding of how the model performs numerically as well as improved understanding of how well or otherwise the model can simulated historic observations this in turn will provide the practitioner with more realistic expectations as they begin the often burdensome history matching process the techniques presented herein can help implement important steps in a larger modeling workflow as with most computational science the ultimate goal of these workflows is to not just undertake complex and sophisticated analyses in support of natural resource decision making but in reproducible and transparent ways e g fienen and bakker 2016 we believe script based workflows move towards the important goal of reproducibility for example practitioners can combine the tools presented herein with other existing scripting based tools for model construction e g bakker et al 2016 to script large components of a modeling workflow the tools presented herein currently focus on structured grids and as such they are generalizable to multiple numerical modeling contexts and simulators efforts are currently underway to parametrically link parent and child models to exploit the capabilities of modflow 6 hughes et al 2017 some future efforts may also be devoted to unstructured grid models declaration of competing interest the authors declare no conflicts of interest acknowledgments we wish to acknowledge contributors to both pyemu and pest as well as linzy foster and jake knight for thoughtful reviews that improved this manuscript partial funding for this work was provided by gns strategic science investment fund ssif groundwater any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government appendix a supplementary data the following is the supplementary data to this article additional supplementary material may be found in the online version of this article and it includes the prior parameter summary information for both the lower and higher resolution models and a listing of the pstfrom function used to construct the pest interfaces around both models the reader is also directed to various repositories and online examples including the pyemu github repository https github com pypest pyemu a dedicated pstfrom example repository https github com pypest pyemu pestpp workflow and a stand alone ready to run browser based pstfrom notebook example https mybinder org v2 gh pypest pyemu pestpp workflow feat binder filepath setup pestpp interface ipynb multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105022 
25866,computer models of environmental systems routinely inform decision making for water resource management in this context quantifying uncertainty in the important simulated outputs and reducing uncertainty through assimilating historic system state observations is as important as the numerical model however implementing high dimensional and stochastic workflows are challenging often requiring that practitioners have theoretical and practical understanding of several advanced topics worse implementing these important analyses can take substantial time and effort this additional effort is often cited as justification for postponing or even forgoing these analyses herein we present scripting tools to facilitate the efficient and repeatable construction of high dimensional geostatistical based pest interfaces including uncertainty analyses as demonstrated these tools can be applied with minimal effort to a model with varied temporal and spatial discretization ultimately these tools can enable low cost access to valuable decision support analyses earlier and more frequently during the modeling workflow keywords uncertainty analysis decision support automating workflows cognitive load model criticism prior data conflict 1 software availability the capabilities presented herein are available within the open source python based pyemu module available at https github com pypest pyemu with documentation provided at https pyemu readthedocs io en latest pyemu examples including for the methods presented in this study are provided in jupyter notebook form jupyter 2016 kluyver et al 2016 in that repository https github com pypest pyemu tree develop examples a specific stepwise demonstration of the automatic pest interface construction methods is provided at https github com pypest pyemu pestpp workflow with a browser based interactive runner housed on binder jupyter et al 2018 at https mybinder org v2 gh pypest pyemu pestpp workflow feat binder filepath setup pestpp interface ipynb the files and script that implement the pest interface construction method in the example application presented here are available at https github com jtwhite79 auto pest 2 introduction computer based environmental models such as numerical groundwater models combined with formal uncertainty analysis and data assimilation provide a quantitative framework for model based water resource management decision support e g anderson et al 2015 doherty and moore 2019 while it is widely recognized that both the quantification of uncertainty in simulated outputs and the reduction of this uncertainty through data assimilation are fundamental and critical components of decision support modeling e g freeze et al 1990 gupta et al 2006 these analyses can require practitioners to understand both advanced theoretical and practical topics underpinning their application see for example the topics in tarantola 2005 oliver et al 2008 doherty 2015a furthermore implementation of these analyses can require a substantial investment of time and effort we believe that the cost of this investment encourages practitioners to postpone such analysis until a later stage in the modeling workflow to a point when the model is deemed ready this is unfortunate because uncertainty analyses and data assimilation much like the model construction process will almost certainly need to be applied iteratively and in a step wise manner e g haitjema 1995 where complexity in the desired uncertainty and or data assimilation analysis is added incrementally potentially requiring revision of the model itself additionally in the authors experience the outcomes of uncertainty and or data assimilation analyses will invariably reveal issues and faults in the underlying model and the data used to inform model properties through data assimilation this reflects the ubiquity of error donoho et al 2008 that accompanies modern computational science finding and addressing these issues and faults early in computational workflow is of primary importance therefore to improve the overall decision support modeling outcomes there are significant benefits to undertaking uncertainty quantification and or data assimilation analyses early and frequently in the numerical modeling workflow hemmings et al 2020 model data assimilation and uncertainty quantification has been facilitated by the development of model independent inverse modeling software such as pest doherty 2015b and pest white et al 2020a these programs are non invasive and operate through the forward model s own input and output files via a file based interface of template for parameters instruction for observations and control files however the construction of interfacing environment can be burdensome and laborious particularly when the desired dimensionality of parameters and observations is high to reduce the cost and burden of implementing uncertainty and data assimilation analyses and therefore help satisfy the need for earlier and more frequent analyses we have developed routines implemented in the python programming language that automate the construction of the pest model interface these python routines can be applied with minimal effort to generate very high dimensional and complex pest interfaces with nested spatial and temporal scales of parameterization e g mckenna et al 2019 these routines also automate the construction of geostatistical based prior parameter covariance matrices and sampling of associated prior parameter ensembles in this way as practitioners are constructing increasingly complex numerical models valuable uncertainty and data assimilation analyses can be applied more easily this will help identify issues and faults earlier in the modeling workflow at a stage where the cost of addressing these issues is much lower additionally as we demonstrate these analyses can also be used as a robust basis for judging the value of increased model complexity during the model development process e g doherty and moore 2019 by reducing the cost of implementing high dimensional stochastic analyses we hope to reduce the cognitive load that practitioners must devote the implementation details so they can devote more cognitive resources to higher order tasks and analyzing results 3 methods we use python based routines encapsulated in the pyemu module white et al 2016 to automate the pest interface construction process these routines are designed to work with array type homogeneous data type with values arranged in two dimensional arrays and list type heterogeneous column oriented data type with values referenced by identifiers in rows model input and output files specifically parameters within the pest interface can be handled using both array type and list type file format structures whereas observations within the pest interface are handled using only list type file format structures by supporting these generic and widely used file formats it is expected that these routines can be easily applied to a wide range of numerical environmental models including but not limited to the modflow family harbaugh 2005 niswonger et al 2011 langevin et al 2017 hughes et al 2017 these tools can also be used in conjunction with other python based model interaction tools such as flopy bakker et al 2016 to further automate the modeling workflow the python routines support a range of parameterization devices that represent differing spatial and temporal scales including pilot points doherty 2003 doherty et al 2010 importantly these parameterization devices can be expressed as direct or multipliers according to the requirements of the practitioner direct parameters are native forward model input values defined in the appropriate units required for the model multiplier parameters act on the initial native forward model input values meaning that they are multipliers against absolute quantities encapsulated in the original array type and list type model input files for the examples presented here only the multiplier parameterization scheme is used however support is provided for combining direct and multiplier parameters in a single interface constructions and even for individual model input files in a bayesian framework e g tarantola 2005 doherty 2015a casting parameters as multipliers allows the existing model input files to serve directly as the mean of the prior parameter distribution the multiplier parameter approach also facilitates a more intuitive process for defining the covariance matrix of the prior parameter distribution a fundamental quantity for uncertainty and data assimilation analyses because the parameters in such an approach represent multipliers against the existing model input files practitioners can think of parameter uncertainties as percent ranges around the initial model input values rather than absolute quantities this can be a more intuitive approach for describing prior uncertainties the multiplier concept also lends itself to apportioning spatial and temporal model input uncertainty into specific spatial and temporal scale parameters e g mckenna et al 2019 white et al 2020b for example practitioners can specify grid scale multiplier parameters where grid scale indicates a unique value in each cell of a computational grid to account for plausible fine scale heterogeneity these grid scale multiplier parameters can be combined with pilot point multiplier parameters which account for broader scale heterogeneity as well as uniform parameters over the entire model domain or within zones corresponding to e g mapped lithological units which can account for large scale biases in model input quantities including boundary conditions e g mckenna et al 2019 white et al 2020b knowling et al 2019 similarly multi scale multiplier parameter schemes can also be defined for time varying parameters so that daily seasonal annual scale and or decade scale long term model input uncertainty can be explicitly represented in the parameters to account for plausible variability across these scales by explicitly partitioning expected uncertainties into these various scales the manifestations of model error such as parameter compensation e g clark and vrugt 2006 white et al 2014 can be more easily detected from an implementation standpoint using multi scale multiplier parameters allows practitioners to access to a wide range of decision support analyses with a single pest interface for example to undertake data worth analyses fienen et al 2010 doherty 2015b white et al 2016 the number of adjustable parameters must be limited to a few thousand to manage computational burden likewise global sensitivity analyses e g saltelli et al 2008 require the parameter dimensionality be limited to less than a few hundred to avoid an unacceptably high computational burden rather than having to construct a separate pest interfaces for each of these analyses practitioners can use the pest and pest input options to simply fix and tie parameters of different scales to reduce dimensionality where necessary for these valuable decision support analyses see 15 for a demonstration of this technique the benefits of a sophisticated multi scale multiplier parameter process described above are compelling however as discussed above implementing such a scheme can be daunting to that end the routines developed in pyemu pstfrom implement the multiplier parameter process seamlessly that is without any practitioner intervention and or manual file manipulation first the pstfrom python routines make copies of the existing model input files that have been nominated for parameterization and store them in a separate sub directory then during the forward run process a preprocessor function which is wrapped in a multi threaded process during the forward run process loads the original model input files performs the requisite multiplication process and then writes the resulting product quantities to the location where the numerical model is expecting to read the corresponding input file 3 1 example function calls the following example functions calls are available from within the pyemu github repository https github com pypest pyemu and from a dedicated example repository https github com pypest pyemu pestpp workflow an example can be run from a stand alone browser based environment available at https mybinder org v2 gh pypest pyemu pestpp workflow feat binder filepath setup pestpp interface ipynb to start the automated pest interface construction one must first instantiate the pstfrom class object example 1 image 1 the arguments passed to this constructor define where the existing model input dataset can be found original d where the combined pest interface and model input dataset should be placed new d as well as other model and interface specific options here we ve only shown the most basic call structure the pstfrom class is designed to be called in the directory level above where an existing set of model files reside indicated by the original d argument the instantiation call e g example 1 will copy these model files into a new directory the new d argument and construct the complete pest interface in the new d directory in this way the original set of model files are preserved and the resulting files in the new d directory are a complete set of model and pest files that can be distributed for parallelizing pest and pest in high throughput and or massively parallel high performance computing environments fienen and hunt 2015 within the new d directory in addition to the files found in original d the pstfrom process will create two subdirectories called org and mlt these subdirectories will hold the original model input files and multiplier parameter files respectively once a pstfrom instance has been created parameters and observations need to be added to the pest interface practitioners can add parameters through the following method example 2 image 2 in this example method call multiplier pilot point parameters were specified for the model input files recharge 1 txt and recharge 2 txt which are array type files that exist in the original d directory for example these files could be modflow 6 input arrays for recharge in this example these files represent the spatial distribution of groundwater recharge for two consecutive stress periods times in the existing model in this case a single set of multiplier parameters is applied in a broadcast operation across both model input files recharge 1 txt recharge 2 txt effectively this constructs pilot point parameters that are temporally constant across these two time periods the broadcasting operation is supported in a completely general way and can be applied to any set of input files provided that the user supplies a list of files to the filenames argument upper and lower bounds specified through the upper bound and lower bound arguments respectively define the allowable range of multiplier values which have an initial value of 1 0 in some cases extreme values in the underlying model input parameters may approach physically important bounds in this case specifying the bounds on multipliers such that those physically important bounds at few locations aren t violated can unnecessarily restrict parameter variability at other spatio temporal locations as a result the arguments ult ubound and ult lbound set ultimate i e after the application of multipliers model parameter upper and lower bounds respectively if multiplier parameters drive model input parameters beyond these ultimate bounds the native model parameter values are reset to these bounds this is particularly important when a number of multipliers parameters are combined e g constant grid etc in the example 2 call the pilot point parameters constructed with a uniform spacing across the model domain their expected spatial correlation is defined by a default geostatistical structure the same geostatistical structure is also used for interpolation from point to grid scale values the default values for these options can be overridden by supplying the requisite arguments to the add parameters method e g pp space and geostruct respectively a model specific default geostatistical structure can also be defined by passing a pyemu geostats geostruct object to the geostruct argument when instantiating a pstfrom object example 1 supported spatial scale parameterization devices specified by the par type argument in example 2 include constant a single parameter value for all entries in a file zone zone parameters of uniform value requires zone array argument pilot points uniformly spaced pilots points and grid unique parameters for every entry in the file combinations of all these types of parameters can be supplied to the pstfrom object through calls to the add parameters method for parameters that apply only to a single model input file without the broadcasting behavior indicated in example 2 a single filename is passed for the filenames argument rather than a list using a similar approach pest interface observations are added through the following method example 3 image 3 in this example method call sfr csv is a modflow 6 stream flow routing sfr niswonger and prudic 2005 observation output file the add observations method writes the corresponding pest instruction file sfr csv ins and sets up pest observations for all columns in this observation file except for the time column which is treated as the index for building automatic observation names depending on the information practitioners pass to the setup routines the resulting observation names may contain both the filename and date time information to facilitate easier interpretation of results once a user has called add parameters and add observations to include the desired model input and output files in the pest interface a call to pf build pst constructs all of the pest interface files needed to execute either pest or pest this call includes constructing and writing the pest control file and associated template and instruction files as well as writing the forward run script needed to execute the forward model run process in pest analyses these files are written to the new d directory practitioners can then construct a geostatistical based prior parameter covariance matrix and draw a prior parameter ensemble with pf build prior and pf draw respectively facilitating a low cost entry point into high dimensional bayesian analyses such as prior based monte carlo 4 example application to demonstrate the use of this python based pest interface construction tool we apply these routines to two different modflow 6 versions of the synthetic model of white et al 2020a which is based on the model of freyberg 1988 fig 1 see also hunt et al 2019 the commands used for this example application are presented in supplementary material in https github com pypest pyemu pestpp workflow blob master prior monte carlo ipynb and in runnable form at https mybinder org v2 gh pypest pyemu pestpp workflow feat binder filepath setup pestpp interface ipynb the two model versions differ in their spatial and temporal resolution but are otherwise identical a lower resolution model described in white et al 2020a has 3 layers 40 rows and 20 cols and simulates a single steady state stress period followed by 24 monthly stress periods a higher resolution version of the model has 120 rows 60 columns and 3 layers and uses 730 daily stress periods the outputs of interest from the numerical simulations include locations where synthetic historical system observations have been collected surface water flow at location sw 1 and groundwater levels at location gw 3 as well as two simulated output quantities surface water groundwater exchange flux during the last stress period aggregated into the upper 20 reaches headwater and lower 20 reaches tailwater fig 1 these simulated outputs constitute the numerical model outputs that are relevant for the specific decision making context that the modeling is supporting they are referred to as decision relevant simulated outputs the simulation has time varying recharge so that each annual cycle includes a dry and wet period herein we focus exclusively on prior based monte carlo as an uncertainty analysis approach prior based monte carlo involves the propagation of realizations drawn from the prior parameter distribution without undertaking any history matching parameter conditioning adjustments and therefore foregos the need to define a likelihood objective function such analysis only requires one model run per parameter realization furthermore in many settings depending on the nature of the decision relevant simulated outputs prior based monte carlo is not only a computationally efficient choice but it may also help to avoid biases induced by history matching e g knowling et al 2019 2020 white et al 2019 we believe prior based monte carlo is an excellent candidate uncertainty analysis approach for the developmental stage of a numerical groundwater model analysis because it has a relatively low computational burden yet it can assist in elucidating both conceptual and numerical issues that may exist in the underlying numerical model it provides opportunities for model criticism e g alfonzo and oliver 2019 oliver 2020 such as prior data conflict assessments e g evans moshonovet al 2006 to ultimately help identify potential model and data deficiencies earlier in the modeling workflow at a point in time when they can be more efficiently addressed given that the prior based monte carlo analysis requires only one run for each realization in contrast with linear uncertainty analysis approaches which require a number of model runs that is proportional to the number of adjustable parameters there is little or no computational penalty for using very large numbers of parameters as such it is computationally feasible to employ several scales of spatial and temporal parameters for both static properties and boundary conditions as discussed above by using a full compliment of multi scale multiplier parameters to represent known expected model input uncertainty we endeavor to yield conservative uncertainty estimates for decision relevant simulated outputs and the simulated equivalents to observations the latter is an important consideration for model criticism and prior data conflict analyses for array type model inputs the following model input quantities were parameterized using a mixture of grid scale and pilot point multiplier parameters horizontal hydraulic conductivity vertical hydraulic conductivity specific storage specific yield exponential variograms with ranges of 500 length units and 2000 length units and sills proportional to the range defined by the parameter bounds were used to describe prior parameter uncertainties for array type grid scale and pilot point parameters respectively note that the default range for geostatistical characterization of pilot points is the pilot points spacing which defaults to 10 model cells multiplied by the maximum row or column spacing the default variogram is of exponential form recharge was parameterized with a single set of time invariant grid scale parameters broadcast across all simulation stress periods these were combined with a constant multiplier parameter for each simulated stress period the grid scale parameters are designed to capture spatial variation and uncertainty in recharge using an exponential variogram with range of 1000 length units the constant recharge parameters were assigned temporal correlation using an exponential variogram with range 60 time units these constant recharge multiplier parameters were designed to capture temporal recharge uncertainty assigning the variogram to the temporal parameters enforces a memory on the recharge parameters preventing spurious correlation over long periods of simulation time the list type model input quantities for groundwater extractions implemented using the wel package in modflow 6 were also parameterized with a mixture of grid scale and constant multipliers across all stress periods again the spatially constant parameters were used to represent temporal uncertainty and expected correlation in these specified extraction flux rates and were assigned an exponential variogram with a range of 60 time units the grid scale parameters effectively parameterize the flux rate for each simulated extraction well for each stress period these grid scale parameters were assigned an exponential variogram with range of 1000 length units the wel package parameterization is conceptually the same as that used to parameterize the recharge to the model the sfr package simulating stream flow routing streambed hydraulic conductivities were also parameterized using grid scale parameters effectively parameterizing the conductance of each simulated reach the prior upper and lower bounds assigned to these parameters are shown in the supplementary material to demonstrate the generality of our approach we use the same python script to construct a high dimensional pest interface as above around both versions lower resolution and higher resolution of the modflow 6 forward model the parameterization scheme for the lower resolution model yielded a total of 9766 parameters and 725 tracked simulated outputs or observations using the same parameterization scheme for the higher resolution model version results in 88 911 parameters and 21 199 tracked simulated outputs or observations for both version the workflow yields a 100 realization geostatistically correlated prior parameter ensemble for each version importantly the script only requires adjustment of the original d argument to point to the desired resolution model it requires no further changes for the analysis herein we evaluated the 100 realization prior ensembles for both model discretizations in parallel using pestpp ies white et al 2020a white 2018 on a laptop computer additionally we used the default parallel run manager settings in pestpp ies to treat any realization that takes more than twice the average runtime as a failed run effectively preempting realizations that cause unacceptably long runtimes this can be thought of as a greedy heuristic approach to improve analysis performance treating the ensemble evaluation in this way can be an important consideration for prior based monte carlo analyses since the analysis is being used in the developmental stage of a modeling workflow and the value of any one realization is relatively low compared to the time penalty of waiting for slow running realizations the script used to construct the pest interfaces for the two models and process the subsequent prior based monte carlo analyses output is available at https github com jtwhite79 auto pest additionally an interactive example of the pstfrom pest interface construction workflow is provided as a jupyter notebook jupyter 2016 kluyver et al 2016 example in the pyemu module github https github com pypest pyemu and also as a stand alone demonstration at https github com pypest pyemu pestpp workflow with a browser based interactive runner housed on binder jupyter et al 2018 at https mybinder org v2 gh pypest pyemu pestpp workflow feat binder filepath setup pestpp interface ipynb additionally the pstfrom function used to construct the pest interface for both models is shown in the supplementary material 5 results figs 2 and 3 summarize the results of the 100 realization ensemble evaluations for the simulated outputs at observation locations and drso quantities respectively in general for the time varying outputs of interest gw 3 and sw 1 the daily stress periods of the higher resolution model yield simulated outputs with higher frequency components compared to the lower resolution outputs fig 2 the models yield similar uncertainty e g variances for the decision relevant simulated output quantities but the higher resolution model yielded less groundwater discharge to surface water that is the higher resolution decision relevant simulated output probability distributions had larger less negative mean values fig 3 for demonstration purposes we display hypothetical observation time series in fig 2 these are hypothetical observations because this is a synthetic demonstration the hypothetical observations are noisy values from a randomly selected higher resolution monte carlo realization comparing the monte carlo results with these hypothetical observations we see that the lower resolution model may not be suitable for history matching because generally it does not reproduce the temporal variability of the observations and in case of the surface water flow observations the lower resolution model does not statistically cover the observed low flow surface water observations depending on the stated goals of the modeling analysis this may be considered an example of prior data conflict discussed in detail below all 100 realizations of lower resolution model terminated successfully and produced an average forward model runtime of 45 s likewise all 100 higher resolution model realization terminated successfully and produced an average forward model runtime of 10 min knowledge of runtime distributions can be used in combination with the simulated outputs at observation locations fig 2 and the decision relevant simulated output quantities fig 3 to judge the value of the higher resolution model as an improved decision support tool 6 discussion and conclusion we have presented a set of python routines that facilitate the efficient and repeatable construction of high dimensional pest model interfaces that can be used to undertake data assimilation and uncertainty analyses for decision support undertaking high dimensional stochastic analyses earlier and more frequently in a modeling workflow will help practitioners gain important insights into model behavior the information contained within a stochastic model analysis far exceeds what is provided by a single forward model run for example as a result of the prior based monte carlo workflow the numerical stability or otherwise and runtimes of the simulation s sampled across parameter space will be well characterized before attempting any form of history matching this can be an important factor when selecting history matching approach and also for managing expectations planning project time lines and coordinating computational resources the comparison of the lower and higher resolution prior based monte carlo results herein demonstrates how choices related model complexity can be evaluated in a rigorous and statistical setting during model development for example if a new and or more complex simulation process is added to the model the effect of this process can and should be compared to previous existing prior based monte carlo results to understand how this new process has effected in a stochastic sense the important simulated outputs in the examaple presented here these outputs include both the decision relevant simulated output quantities and if history matching is to be undertaken then results should also be compared at spatio temporal locations of historic system state observations in this way the prior based monte carlo becomes a partner analysis in the incremental e g step wise 11 model development process we believe one of the most important insights that can be gained from easily implemented but high dimensional prior based monte carlo is related to prior based model criticism e g oliver 2020 specifically the detection of prior data conflict e g evans moshonovet al 2006 if the prior based monte carlo reveals a significant discrepancy between simulated outputs and associated system observations there is limited justification for trying to reproduce these observations during history matching the consequence of attempting to match conflicted observations can result in extreme parameter values and or parameter combinations with implications for model forecast reliability e g knowling et al 2019 clark and vrugt 2006 white et al 2014 if prior data conflict is identified it provides an early opportunity to employ mitigation strategies these strategies must target either the prior parameter distribution or the likelihood function and might include reduction of conflicting observation weights during history matching where the assimilation of observation is not considered to be informative of the decision relevant simulated output e g doherty and welter 2010 an extreme example of this would be removing conflicted observations entirely from the history matching process e g knowling et al 2020 evaluation of the quality of observations that will be used for history matching while this is a typical heuristic element of a history matching workflow it is often performed later in the process when modeled outputs are consistently not deemed to be sufficiently close to observed values after multiple iterations of history matching prior data conflict provides the opportunity to be alerted to this possibility early and systematically prior inflation where the prior parameter uncertainty is inflated to account for structural deficiencies that may be impacting the model s ability to reproduce system observations e g o hagan and pericchi 2012 and model improvements where changes are made to model structure simulated processes and or parameterization to improve the model s ability to represent the processes related to the conflicting observations e g alfonzo and oliver 2019 oliver 2020 identifying and evaluating prior data conflict is only possible in high dimensional stochastic settings such as those available with prior based monte carlo while the process of identifying conflicts is subjective the authors struggled to define which observations in the example should be nominated as conflicts and this process will likely require in depth discussion s between the practitioner s and stakeholders undertaking this process earlier in the modeling workflow will enable a more efficient diagnosis of the cause and implications of these conflicts compared to the surprise that otherwise occurs the first time history matching is attempted the benefits of earlier and more frequent prior based monte carlo analysis as enabled by the tools presented herein cannot be overstated in our opinion the insights gained from performing this simple yet powerful and often overlooked analysis leads to substantially less of what we term stochastic desperation a situation where a practitioner tries varying multiple algorithmic inputs simultaneously and in non objective ways in a last ditch effort to find an acceptable history matching result this usually occurs at a late stage in the project and typically coincides with a holiday weekend as the project budget expires instead the tools we have presented facilitate low cost access to a range of decision support analyses which if implemented earlier in the modeling workflow will afford more complete understanding of how the model performs numerically as well as improved understanding of how well or otherwise the model can simulated historic observations this in turn will provide the practitioner with more realistic expectations as they begin the often burdensome history matching process the techniques presented herein can help implement important steps in a larger modeling workflow as with most computational science the ultimate goal of these workflows is to not just undertake complex and sophisticated analyses in support of natural resource decision making but in reproducible and transparent ways e g fienen and bakker 2016 we believe script based workflows move towards the important goal of reproducibility for example practitioners can combine the tools presented herein with other existing scripting based tools for model construction e g bakker et al 2016 to script large components of a modeling workflow the tools presented herein currently focus on structured grids and as such they are generalizable to multiple numerical modeling contexts and simulators efforts are currently underway to parametrically link parent and child models to exploit the capabilities of modflow 6 hughes et al 2017 some future efforts may also be devoted to unstructured grid models declaration of competing interest the authors declare no conflicts of interest acknowledgments we wish to acknowledge contributors to both pyemu and pest as well as linzy foster and jake knight for thoughtful reviews that improved this manuscript partial funding for this work was provided by gns strategic science investment fund ssif groundwater any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government appendix a supplementary data the following is the supplementary data to this article additional supplementary material may be found in the online version of this article and it includes the prior parameter summary information for both the lower and higher resolution models and a listing of the pstfrom function used to construct the pest interfaces around both models the reader is also directed to various repositories and online examples including the pyemu github repository https github com pypest pyemu a dedicated pstfrom example repository https github com pypest pyemu pestpp workflow and a stand alone ready to run browser based pstfrom notebook example https mybinder org v2 gh pypest pyemu pestpp workflow feat binder filepath setup pestpp interface ipynb multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105022 
25867,spatial simulation and inversion problems are omnipresent in earth and environmental sciences an open source python package rmwspy for conditional spatial random field simulation and inversion based on a generalized implementation of the random mixing whittaker shannon rmws algorithm is presented in this paper the rmws algorithm has successfully been applied to a variety of environmental modelling problems ranging from inverse groundwater flow and transport modelling to precipitation simulation incorporating incomplete observations rmwspy provides great flexibility due to its variety of linear and non linear conditioning constraints the generalized implementation isolates the core algorithm from the user defined problem statement in this paper rmwspy is introduced using a synthetic inversion example for spatial precipitation estimation which combines rain gauge data and integral rain rates obtained from commercial microwave link data the required python scripts are described and the results of one precipitation event are presented and discussed keywords geostatistical simulation stochastic inversion environmental modelling random mixing python 1 introduction the simulation of conditional spatial random fields is a common task in earth and environmental sciences there is a variety of established methods to perform such simulations for example turning band simulation kriging simulation or sequential gaussian simulation journel 1974 isaaks 1990 wackernagel 1998 however the simulation problem becomes much more challenging if more complex types of conditioning constraints such as non linear constraints are involved non linear constraints can for example arise when measurements of a secondary variable which is related to the primary variable of interest via complex mathematical functions such as partial differential equations are used to condition the simulation of the primary variable some examples of these include using hydraulic pressure head observations to condition the simulation of hydraulic conductivity evensen 2003 hendricks franssen et al 2009 li et al 2015 using groundwater chemistry observations such as tracer or contaminant concentrations to condition the simulation of hydraulic conductivity hendricks franssen et al 2003 hassan et al 2009 or using electrical resistivity tomography data to condition the simulation of electrical conductivity loke and barker 1996 codd and gross 2018 hörning et al 2020 in general most geophysical measurements record information on a secondary variable rather than the actual primary variable of interest in this paper a generalized approach based on random mixing rm for simulating conditional spatial random fields and inverse modelling is presented rm is a geostatistical simulation approach that was first introduced by bárdossy and hörning 2016b and bárdossy and hörning 2016a where the authors applied it to inverse groundwater flow and transport problems a detailed description of the rm theory can be found in hörning 2016 an application to inverse hydrological modelling is presented in grundmann et al 2019 and its extension via whittaker shannon interpolation random mixing whittaker shannon hence rmws is presented in hörning et al 2019 rmws is model independent and allows flexible integration and combination of different conditioning constraints including non linear constraints the approach is developed as a set of python scripts python is an interpreted high level object oriented programming language that is widely used in the scientific community pérez et al 2011 it has a more powerful syntax and a more complete set of data structures compared to low level languages such as c or fortran python runs on virtually every operating system that is available besides the python core language there is a variety of python packages for almost any type of scientific analysis for example numpy is for working with arrays oliphant 2015 scipy offers a wide range of optimization and statistics routines jones et al 2001 virtanen et al 2020 and matplotlib provides high quality graphics routines hunter 2007 python and most of its packages are open source software the generalized implementation of the random mixing whittaker shannon algorithm presented in this paper is an open source python package rmwspy for simulating conditional spatial random fields and inverse modelling the idea is to provide a flexible conditional simulation and inversion environment that is applicable to a wide range of environmental modelling problems as such rmwspy exhibits similarities to the mad inverse modelling framework osorio murillo et al 2015 however the rmwspy concept is that the user writes python scripts that define the problem domain constraints specific objective function etc creates a link to the rmws simulation algorithm and performs the desired pre and post processing while writing scripts seems less user friendly than using a gui for example it has advantages in terms of flexibility rmwspy has been developed such that it can easily be coupled with a wide range of numerical models such as modflow harbaugh 2005 harbaugh et al 2017 or hydrogeosphere therrien and sudicky 1996 users can also develop their own forward model that can be incorporated into python or connected to the rmwspy scripts further rmwspy enables straight forward parallelization for example via multi threading or mpi the paper is divided into four sections after the introduction the theory of random mixing is briefly introduced in section 2 the example application which is also included in the github repository is presented in section 3 it introduces the synthetic data set and describes the application of rmwspy to that specific example such that the reader can follow and repeat the individual steps of the algorithm the results of the example application are discussed and the paper ends with conclusions in section 4 2 theory in the following only a brief description of the theory of rmws is presented the interested reader is referred to hörning et al 2019 where the random mixing whittaker shannon algorithm is described comprehensively in general the goal of rmws is to find a field z x with x d such that this field honors the observed spatial dependence structure the observed marginal distribution as well as potential linear and non linear observations linear or direct observations are values of the actual parameter field z x at a specific location x while non linear or indirect observations are values of a parameter which has a non linear functional relation to the field z x to find such a field z x that honors the above described criteria rmws uses a two step approach based on linear combinations of unconditional spatial random fields the first linear combination ensures that the linear observations are honored while the second linear combination enables the conditioning on non linear observations fig 1 shows a flowchart of the algorithm its individual steps are briefly described in the following 1 the linear observations are denoted z x j with x being a location within the domain of interest d the non linear observations are denoted h i note that h i can have a location x but it is not a requirement h i can also represent integrals etc 2 the linear observations z x j are transformed to standard normal values w x j using a simple q q transformation this requires the knowledge of the univariate marginal distribution of z which can fitted to the available data if the data is insufficient to achieve a reasonable fit a distribution can be assumed based on literature expert knowledge the spatial covariance γ also has to be fitted to w x j or assumed 3 using γ l unconditional spatial random fields v l are simulated for example using the spectral representation method described in shinozuka and deodatis 1991 1996 4 the equation system l 1 l α l v l x j w j is solved where α l denotes the unknown weights of the linear combination and w j are the standard normal values at the locations x j obtained in step 2 5 if l 1 l α l 2 1 go to step 6 otherwise increase l and go back to step 3 6 construct the field w using the weights α via w l 1 l α l v l note that this field has a low variance due to the condition imposed in step 5 further define β 1 l 1 l α l 2 7 generate two additional fields u s and u t which both fulfill the homogeneous conditions i e u s x j 0 and u t x j 0 the homogeneous conditions ensure that the linear observations step 4 will still be honored later in the process 8 generate w m w β cos θ m u t sin θ m u s for θ m 2 π m 1 m with m 1 m note that for any θ w m x j w j further all w m s exhibit the spatial covariance γ due to the re scaling using β 9 transform the fields w m back to z m using the inverse q q transformation and calculate the corresponding non linear constraints h z m using whittaker shannon interpolation whittaker 1915 shannon 1948 interpolate h z m for any θ 0 2 π 10 calculate the objective function using the interpolated h z θ for example via o θ h z θ h i 2 if for any θ o θ ε with ε denoting a pre defined stopping criteria go to step 11 otherwise take the θ that results in the best objective function value to generate a new field u t 1 cos θ u t sin θ u s additionally generate a new field u s and go back to step 8 11 take the θ fulfilling the condition in step 10 and calculate the corresponding final field z o p t a new realization can be generated restarting the procedure from step 3 3 a rmwspy example the use of rmwspy requires a basic understanding of the python programming language as well as of object oriented code design its very basic use case as a conditional spatial random field simulation tool is presented as a jupyter notebook which can be found in the github repository the example application presented in this paper is a complex synthetic inversion case based on haese et al 2017 it demonstrates how to handle a combination of linear and non linear constraints and it has been selected for two main reasons first it consists of python code only i e no external numerical model forward model is required for the non linear component second a synthetic test case allows repeatability and the results can easily be evaluated against the synthetic truth the aim of the case study is to simulate a precipitation field that is conditioned on rain gauge observations as well as on integral rain rates which are estimated using commercial microwave links cml derived information exploiting data from cml networks was introduced by messer et al 2006 as an alternative for rainfall estimation it utilizes the fact that rain attenuates the microwave radiation between a transmitter and a receiver antenna olsen et al 1978 showed that the observed attenuation can be related to an integral rain rate which can then be incorporated as conditioning constraint into the precipitation simulation it is worth pointing out that these integrals represent non linear constraints as the integral value has to be transformed into point precipitation values that match the integral and also fit into the overall spatial distribution of the precipitation in the following a brief description of the synthetic data set and the application of rmwspy to this data set is presented 3 1 data set the synthetic precipitation data set used in this paper is generated using the regional climate simulation which was running on the gcs supercomputer juqueen jülich supercomputing centre 2015 for a study area covering approx 57 850 km2 in southern germany for the period from 2007 to 2015 this simulation was performed using the cosmo model v4 21 consortium for small scale modeling baldauf et al 2011 coupled to the community land model clm a land surface model lawrence and chase 2007 running with a horizontal resolution of 1 1 km the lateral boundary conditions are produced by the cosmo de model running with a horizontal resolution of 2 8 km and covering the area of germany hence the lateral boundary conditions and all constant fields e g topography and land mask are downscaled to the 1 1 resolution using linear interpolation the lateral boundary conditions for the atmospheric forcing were nudged against the internal model solution for this nudging a transition zone of 12 km between the two domains has been used the catchment area is restricted from from 47 8 n to 49 63 n and 7 95 w 10 37 w fig 2 to avoid boundary issues the synthetic data set is denoted the virtual truth vt in the following based on the vt we generate 71 data points which mimic hourly rain gauge observations the virtual rain gauges are located where also real hourly measurements over the period 2007 to 2015 exist for the cml path integrated rain rates we choose 100 link locations imitating a real network 3 2 application of rmwspy for the demonstration of rmwspy we select the time step of the 23th august 2015 at 15pm as it shows a relatively complex precipitation pattern see fig 4 c as described above 71 rain gauge observations and 100 cml observations are available in order to use rmwspy as an inversion tool for this example one has to 1 create a python script which defines the non linear problem here cml py this script has to contain a class for the non linear problem here cmlmodel which overwrites the nonlinearproblemtemplate defined in random mixing whittaker shannon py this class must at least include the three functions objective function allforwards forward defined in nonlinearproblemtemplate additional functions can be added as required fig 3 shows the implementation of the cmlmodel class for the present example the allforwards function represents the link between rmws and cmlmodel as it takes the current fields from rmws see steps 7 and 8 in section 2 and calls the forward function for each of them this forward function represents the forward model which takes the current simulated field and transforms its marginal distribution then the simulated rain values along the cml paths are extracted using the get cml on path function and the corresponding average rain rates are calculated and returned for the interpolation described in step 9 in section 2 the objective function in this example is the root mean squared error rmse between the cml observations and the corresponding simulated cml values note that this setup allows for straight forward parallelization as part of the allforwards function the individual forward function calls are independent of each other thus they can be parallelized 2 create a python script here run inv py that performs all required pre processing post processing and initialization tasks for the present example these are a load the input data with its corresponding coordinates and define the size of the simulation domain here two types of input data are available on the one hand the rain gauge observations p data and on the other hand the non linear cml observations mwl data both data sets contain coordinates with their corresponding values note that the coordinates are given in latitudes and longitudes which for simplicity are transformed to a regular x y grid with 0 0 origin b as the coordinates corresponding to the cml data represent the locations of the transmitter and receiver antenna the paths between these two locations have to be defined which grid cells are located along a straight line from transmitters to receivers this is done using bresenham s line algorithm daintith and wright 2008 which is applied to the already transformed grid c fit a marginal distribution to the rain gauge data this can be any parametric distribution function or a non parametric distribution function here a non parametric approach via kernel density estimation using scikit learn pedregosa et al 2011 is applied d transform the linear observations rain gauge observations into standard normal values using the previously fitted marginal distribution function cdf and the inverse standard normal distribution scipy stats norm ppf note that this step is essential as the rmws simulation algorithm works in standard normal space further note that linear inequality constraints are transformed differently than linear equality constraints the theoretical background can be found in hörning 2016 e fit a spatial covariance function to the linear observations rain gauge observations here this is done using the maximum likelihood approach described in li 2010 but any other approach can be applied as well f initialize cmlmodel using the following arguments mwl prec are the rain integrals based on the cml measurements which were loaded in a marginal contains the marginal distribution fitted to the linear rain gauge observations in c mwl integrals defines the integral paths corresponding to mwl prec defined in b g initialize rmws using the following arguments my cmlmodel makes the cmlmodel an instance of rmws domainsize ysize xsize defines the size of the simulation grid covmod cmod defines the covariance model that was fitted in step d nfields nfields defines the number of conditional fields one wants to simulate cp cp defines the equality constraint coordinates on the simulation grid cv cv defines the equality constraint values corresponding to cp le cp lecp defines the coordinates of less or equal constraints on the simulation grid le cv lecv defines the less or equal constraint values which in the present example are observations that recorded zero rainfall corresponding to le cp optmethod circleopt defines that rmwscondsim runs in inversion mode minobj 0 4 defines the stopping criteria i e if the objective function reaches that value the inversion terminates maxiter 300 defines the maximum number of iterations per realization after which the inversion terminates if minobj has not been reached h call rmws which will start the inversion process rmws follows the procedure described in section 2 i post processing of the resulting conditional fields cs finalfields in the present example the post processing involves the back transformation of the simulated conditional fields into their actual data space using the fitted inverse marginal distribution further the back transformed fields are saved as a npy file the individual fields the mean field and the variance field are plotted and a box plot for the simulated cml data is created this box plot shows how the simulated cmls match the observed cmls 3 run the run inv py script this starts the whole process i e all pre processing the actual inversion and the subsequent post processing the scripts described above are part of the rmwspy repository a random seed np random seed 121 defined in run inv py ensures that the results are identical to the results presented in this paper note in order to obtain the same figure style as shown in this paper for figs 4 6 and 8 the variable paperstyle plot defined in run inv py needs to be set to true this however requires the installation of some non standard python modules as described in the github repository if paperstyle plot false a more basic figure style is applied which uses python s standard matplotlib only changing the random seed will result in different random numbers and thus different results as rmwspy is a stochastic approach in order to test how different the results of the precipitation estimation can be without cml data the reader can repeat the simulation based on the rain gauge observations only therefore optmethod circleopt needs to be changed to optmethod no nl constraints which will ignore the non linear i e the cml observations rmwspy then runs in linear simulation mode taking the linear observations rain gauges into account only the resulting box plots exhibit a wider spread around the observed values as they are not used for conditioning anymore and the estimation uncertainty in terms of the standard deviation increases other variables that impact the results are for example minobj and maxiter increasing the value of minobj would lead to a weaker optimization of the cml observations thus to a higher estimation uncertainty however this also means that the process can finish with less iterations similarly if maxiter is reduced the inversion process might not reach minobj but it again will be faster as less iteration are performed in general the chosen minimum objective function value minobj should be sensible for the respective inversion problem in the case of the cml constraints for example a minobj value of less than 0 4 would not be very sensible as that is less than the measurement uncertainty of the cmls 3 3 results and discussion for comparison an example of a simulated random field rf the ensemble mean over all 50 simulations and the vt are shown at fig 4 both the single simulation a and the ensemble mean b showing the same main patterns as the vt c while the single simulation shows a slightly higher spatial variability as the vt the ensemble mean slightly underestimates the spatial variability this behavior can also be seen in the pixelwise comparison at fig 5 while the single rf has a similar range of precipitation values the ensemble mean underestimates the higher precipitation values the pearson correlation coefficient between the vt and all fields of the ensemble is r e n s e m b l e 0 73 r s i n g l e r f 0 77 for the example single rf for the ensemble mean the pearson correlation coefficient is r e n s m e a n 0 86 exploring the uncertainty of the simulated ensemble of precipitation fields by the usage of the standard deviation calculated for each grid cell fig 6 show the highest values in areas of heavier precipitation since figs 5 and 6 indicate a slightly displacement of the precipitation values compared to the vt we will use the fraction skill score fss roberts and lean 2008 as an additional quality measure for the three chosen thresholds of 2 m m h 4 m m h and 6 m m h we define a rainy grid and calculate the fss by 1 f s s 1 i j 1 n p v t i j p s i m i j 2 i j 1 n p v t i j 2 i j 1 n p s i m i j 2 the fss is a fuzzy verification measure which compares the fractional coverage of a certain value of two patterns here the chosen thresholds for the precipitation amount for a given neighborhood with size n the fss compares the precipitation of the vt p v t and the simulated pattern p s i m its values range between 0 and 1 where 1 indicates perfect coverage the reasonable skill is defined as the wet area ratio of the reference field 0 5 p v t t r u e 2 the fss passes if its value is higher than this reasonable skill fig 7 visualize the fss analysis for the three thresholds using a threshold of 2 m m h the ens mean and most of the ensemble members reach immediately the reasonable skill of 0 66 at 1 1 km however the example single rf reaches the reasonable skill at 2 5 km which may be explained by the heavy noise of the single field see also fig 4 a since the boxplot for the whole ensemble fig 7 b indicate with a neighborhood size of 1 1 km a better skill the chosen example is a outlier using a 4 m m h threshold the reasonable skill with 0 56 is reached from both the ens mean and the single random field at a 5 km neighborhood only the fss of the single random field has a steeper ascent the boxplot indicate that the 25 quantile up to the 75 quantile of the ensemble reach the reasonable scale earlier by 1 1 km the reasonable skill of 0 52 for the threshold of 6 m m h is reached by the single realization by 10 km and for most the ensemble members by 9 km since the ens mean smooths the precipitation field it does not reach the reasonable skill for the 6 m m h threshold as stated in section 3 2 to satisfy the non linear cml observations the rmse between the synthetic cml observations and their simulated counterparts is minimized the quality of the simulated path integrated rain rates regarding the synthetic observations is shown in fig 8 for all 100 cmls the simulated rates are in a good agreement with the synthetic observations for most cmls the mean difference between the observations and the simulated counterparts is below 0 5 m m h which is well within the range of the cml observation uncertainty for only 20 cmls the maximal difference between the observation and an ensemble member is higher than 1 m m h again it should be mentioned that these differences also depend on the choice of the minobj and maxiter parameters summarizing using rmwspy to simulate precipitation fields in an inversion framework based on linear rain gauge observations and non linear cmls observations conditions results in an ensemble with any given count of members the example shows that the single realizations are in a good agreement with the vt it is demonstrated that the cml observations are represented reasonably i e their values are reproduced closely but still allowing enough variation to represent the measurement uncertainty the overall estimation uncertainty in terms of the standard deviation is obtained using the ensemble of realizations 4 conclusions the use of rmwspy offers a very general framework for conditional spatial simulation and inversion for environmental modelling problems it can be used as an independent standalone python package or in conjunction with other programming languages and numerical forward models using a high level programming language such as python allows complicated tasks to be achieved within a few lines of code which is also straightforward to read the scripting approach rather than using a gui offers a great flexibility especially due to the availability of hundreds of python modules for almost every possible scientific task the example presented in this paper shows how to apply rmwspy to an inverse modelling problem for improved precipitation estimation using rain gauge data and cml data it is demonstrated how the cmlmodel class which describes the non linear problem is defined all functions within this class are delineated the main script run inv py including its variables and the pre and post processing steps are described and the results of the inversion are discussed in detail rmwspy is an open source project hosted on github all scripts discussed in this paper and the corresponding data files are part of this repository further examples such as a jupyter notebook covering the basic use cases of rmwspy and groundwater inversion study with modflow using multi threading are also available on github code and data availability all code developments presented here are open source tools released under the gnu general public license v3 0 source code version 1 1 used in this paper is archived online at https github com sebastianhoerning rmwspy declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research for this paper was partly supported by the german research foundation deutsche forschungsgemeinschaft for2131 data assimilation for improved characterization of fluxes across compartmental interfaces projektnummer 652387 ku and partly by the energi simulation program the authors would also like to thank the university of queensland centre for natural gas and its industry members aplng arrow energy and santos for providing research funding the authors gratefully acknowledge bernd schalge for providing the synthetic data set finally the authors highly welcome additions suggestions improvements and assistance from the spatial statistics and the broader scientific community 
25867,spatial simulation and inversion problems are omnipresent in earth and environmental sciences an open source python package rmwspy for conditional spatial random field simulation and inversion based on a generalized implementation of the random mixing whittaker shannon rmws algorithm is presented in this paper the rmws algorithm has successfully been applied to a variety of environmental modelling problems ranging from inverse groundwater flow and transport modelling to precipitation simulation incorporating incomplete observations rmwspy provides great flexibility due to its variety of linear and non linear conditioning constraints the generalized implementation isolates the core algorithm from the user defined problem statement in this paper rmwspy is introduced using a synthetic inversion example for spatial precipitation estimation which combines rain gauge data and integral rain rates obtained from commercial microwave link data the required python scripts are described and the results of one precipitation event are presented and discussed keywords geostatistical simulation stochastic inversion environmental modelling random mixing python 1 introduction the simulation of conditional spatial random fields is a common task in earth and environmental sciences there is a variety of established methods to perform such simulations for example turning band simulation kriging simulation or sequential gaussian simulation journel 1974 isaaks 1990 wackernagel 1998 however the simulation problem becomes much more challenging if more complex types of conditioning constraints such as non linear constraints are involved non linear constraints can for example arise when measurements of a secondary variable which is related to the primary variable of interest via complex mathematical functions such as partial differential equations are used to condition the simulation of the primary variable some examples of these include using hydraulic pressure head observations to condition the simulation of hydraulic conductivity evensen 2003 hendricks franssen et al 2009 li et al 2015 using groundwater chemistry observations such as tracer or contaminant concentrations to condition the simulation of hydraulic conductivity hendricks franssen et al 2003 hassan et al 2009 or using electrical resistivity tomography data to condition the simulation of electrical conductivity loke and barker 1996 codd and gross 2018 hörning et al 2020 in general most geophysical measurements record information on a secondary variable rather than the actual primary variable of interest in this paper a generalized approach based on random mixing rm for simulating conditional spatial random fields and inverse modelling is presented rm is a geostatistical simulation approach that was first introduced by bárdossy and hörning 2016b and bárdossy and hörning 2016a where the authors applied it to inverse groundwater flow and transport problems a detailed description of the rm theory can be found in hörning 2016 an application to inverse hydrological modelling is presented in grundmann et al 2019 and its extension via whittaker shannon interpolation random mixing whittaker shannon hence rmws is presented in hörning et al 2019 rmws is model independent and allows flexible integration and combination of different conditioning constraints including non linear constraints the approach is developed as a set of python scripts python is an interpreted high level object oriented programming language that is widely used in the scientific community pérez et al 2011 it has a more powerful syntax and a more complete set of data structures compared to low level languages such as c or fortran python runs on virtually every operating system that is available besides the python core language there is a variety of python packages for almost any type of scientific analysis for example numpy is for working with arrays oliphant 2015 scipy offers a wide range of optimization and statistics routines jones et al 2001 virtanen et al 2020 and matplotlib provides high quality graphics routines hunter 2007 python and most of its packages are open source software the generalized implementation of the random mixing whittaker shannon algorithm presented in this paper is an open source python package rmwspy for simulating conditional spatial random fields and inverse modelling the idea is to provide a flexible conditional simulation and inversion environment that is applicable to a wide range of environmental modelling problems as such rmwspy exhibits similarities to the mad inverse modelling framework osorio murillo et al 2015 however the rmwspy concept is that the user writes python scripts that define the problem domain constraints specific objective function etc creates a link to the rmws simulation algorithm and performs the desired pre and post processing while writing scripts seems less user friendly than using a gui for example it has advantages in terms of flexibility rmwspy has been developed such that it can easily be coupled with a wide range of numerical models such as modflow harbaugh 2005 harbaugh et al 2017 or hydrogeosphere therrien and sudicky 1996 users can also develop their own forward model that can be incorporated into python or connected to the rmwspy scripts further rmwspy enables straight forward parallelization for example via multi threading or mpi the paper is divided into four sections after the introduction the theory of random mixing is briefly introduced in section 2 the example application which is also included in the github repository is presented in section 3 it introduces the synthetic data set and describes the application of rmwspy to that specific example such that the reader can follow and repeat the individual steps of the algorithm the results of the example application are discussed and the paper ends with conclusions in section 4 2 theory in the following only a brief description of the theory of rmws is presented the interested reader is referred to hörning et al 2019 where the random mixing whittaker shannon algorithm is described comprehensively in general the goal of rmws is to find a field z x with x d such that this field honors the observed spatial dependence structure the observed marginal distribution as well as potential linear and non linear observations linear or direct observations are values of the actual parameter field z x at a specific location x while non linear or indirect observations are values of a parameter which has a non linear functional relation to the field z x to find such a field z x that honors the above described criteria rmws uses a two step approach based on linear combinations of unconditional spatial random fields the first linear combination ensures that the linear observations are honored while the second linear combination enables the conditioning on non linear observations fig 1 shows a flowchart of the algorithm its individual steps are briefly described in the following 1 the linear observations are denoted z x j with x being a location within the domain of interest d the non linear observations are denoted h i note that h i can have a location x but it is not a requirement h i can also represent integrals etc 2 the linear observations z x j are transformed to standard normal values w x j using a simple q q transformation this requires the knowledge of the univariate marginal distribution of z which can fitted to the available data if the data is insufficient to achieve a reasonable fit a distribution can be assumed based on literature expert knowledge the spatial covariance γ also has to be fitted to w x j or assumed 3 using γ l unconditional spatial random fields v l are simulated for example using the spectral representation method described in shinozuka and deodatis 1991 1996 4 the equation system l 1 l α l v l x j w j is solved where α l denotes the unknown weights of the linear combination and w j are the standard normal values at the locations x j obtained in step 2 5 if l 1 l α l 2 1 go to step 6 otherwise increase l and go back to step 3 6 construct the field w using the weights α via w l 1 l α l v l note that this field has a low variance due to the condition imposed in step 5 further define β 1 l 1 l α l 2 7 generate two additional fields u s and u t which both fulfill the homogeneous conditions i e u s x j 0 and u t x j 0 the homogeneous conditions ensure that the linear observations step 4 will still be honored later in the process 8 generate w m w β cos θ m u t sin θ m u s for θ m 2 π m 1 m with m 1 m note that for any θ w m x j w j further all w m s exhibit the spatial covariance γ due to the re scaling using β 9 transform the fields w m back to z m using the inverse q q transformation and calculate the corresponding non linear constraints h z m using whittaker shannon interpolation whittaker 1915 shannon 1948 interpolate h z m for any θ 0 2 π 10 calculate the objective function using the interpolated h z θ for example via o θ h z θ h i 2 if for any θ o θ ε with ε denoting a pre defined stopping criteria go to step 11 otherwise take the θ that results in the best objective function value to generate a new field u t 1 cos θ u t sin θ u s additionally generate a new field u s and go back to step 8 11 take the θ fulfilling the condition in step 10 and calculate the corresponding final field z o p t a new realization can be generated restarting the procedure from step 3 3 a rmwspy example the use of rmwspy requires a basic understanding of the python programming language as well as of object oriented code design its very basic use case as a conditional spatial random field simulation tool is presented as a jupyter notebook which can be found in the github repository the example application presented in this paper is a complex synthetic inversion case based on haese et al 2017 it demonstrates how to handle a combination of linear and non linear constraints and it has been selected for two main reasons first it consists of python code only i e no external numerical model forward model is required for the non linear component second a synthetic test case allows repeatability and the results can easily be evaluated against the synthetic truth the aim of the case study is to simulate a precipitation field that is conditioned on rain gauge observations as well as on integral rain rates which are estimated using commercial microwave links cml derived information exploiting data from cml networks was introduced by messer et al 2006 as an alternative for rainfall estimation it utilizes the fact that rain attenuates the microwave radiation between a transmitter and a receiver antenna olsen et al 1978 showed that the observed attenuation can be related to an integral rain rate which can then be incorporated as conditioning constraint into the precipitation simulation it is worth pointing out that these integrals represent non linear constraints as the integral value has to be transformed into point precipitation values that match the integral and also fit into the overall spatial distribution of the precipitation in the following a brief description of the synthetic data set and the application of rmwspy to this data set is presented 3 1 data set the synthetic precipitation data set used in this paper is generated using the regional climate simulation which was running on the gcs supercomputer juqueen jülich supercomputing centre 2015 for a study area covering approx 57 850 km2 in southern germany for the period from 2007 to 2015 this simulation was performed using the cosmo model v4 21 consortium for small scale modeling baldauf et al 2011 coupled to the community land model clm a land surface model lawrence and chase 2007 running with a horizontal resolution of 1 1 km the lateral boundary conditions are produced by the cosmo de model running with a horizontal resolution of 2 8 km and covering the area of germany hence the lateral boundary conditions and all constant fields e g topography and land mask are downscaled to the 1 1 resolution using linear interpolation the lateral boundary conditions for the atmospheric forcing were nudged against the internal model solution for this nudging a transition zone of 12 km between the two domains has been used the catchment area is restricted from from 47 8 n to 49 63 n and 7 95 w 10 37 w fig 2 to avoid boundary issues the synthetic data set is denoted the virtual truth vt in the following based on the vt we generate 71 data points which mimic hourly rain gauge observations the virtual rain gauges are located where also real hourly measurements over the period 2007 to 2015 exist for the cml path integrated rain rates we choose 100 link locations imitating a real network 3 2 application of rmwspy for the demonstration of rmwspy we select the time step of the 23th august 2015 at 15pm as it shows a relatively complex precipitation pattern see fig 4 c as described above 71 rain gauge observations and 100 cml observations are available in order to use rmwspy as an inversion tool for this example one has to 1 create a python script which defines the non linear problem here cml py this script has to contain a class for the non linear problem here cmlmodel which overwrites the nonlinearproblemtemplate defined in random mixing whittaker shannon py this class must at least include the three functions objective function allforwards forward defined in nonlinearproblemtemplate additional functions can be added as required fig 3 shows the implementation of the cmlmodel class for the present example the allforwards function represents the link between rmws and cmlmodel as it takes the current fields from rmws see steps 7 and 8 in section 2 and calls the forward function for each of them this forward function represents the forward model which takes the current simulated field and transforms its marginal distribution then the simulated rain values along the cml paths are extracted using the get cml on path function and the corresponding average rain rates are calculated and returned for the interpolation described in step 9 in section 2 the objective function in this example is the root mean squared error rmse between the cml observations and the corresponding simulated cml values note that this setup allows for straight forward parallelization as part of the allforwards function the individual forward function calls are independent of each other thus they can be parallelized 2 create a python script here run inv py that performs all required pre processing post processing and initialization tasks for the present example these are a load the input data with its corresponding coordinates and define the size of the simulation domain here two types of input data are available on the one hand the rain gauge observations p data and on the other hand the non linear cml observations mwl data both data sets contain coordinates with their corresponding values note that the coordinates are given in latitudes and longitudes which for simplicity are transformed to a regular x y grid with 0 0 origin b as the coordinates corresponding to the cml data represent the locations of the transmitter and receiver antenna the paths between these two locations have to be defined which grid cells are located along a straight line from transmitters to receivers this is done using bresenham s line algorithm daintith and wright 2008 which is applied to the already transformed grid c fit a marginal distribution to the rain gauge data this can be any parametric distribution function or a non parametric distribution function here a non parametric approach via kernel density estimation using scikit learn pedregosa et al 2011 is applied d transform the linear observations rain gauge observations into standard normal values using the previously fitted marginal distribution function cdf and the inverse standard normal distribution scipy stats norm ppf note that this step is essential as the rmws simulation algorithm works in standard normal space further note that linear inequality constraints are transformed differently than linear equality constraints the theoretical background can be found in hörning 2016 e fit a spatial covariance function to the linear observations rain gauge observations here this is done using the maximum likelihood approach described in li 2010 but any other approach can be applied as well f initialize cmlmodel using the following arguments mwl prec are the rain integrals based on the cml measurements which were loaded in a marginal contains the marginal distribution fitted to the linear rain gauge observations in c mwl integrals defines the integral paths corresponding to mwl prec defined in b g initialize rmws using the following arguments my cmlmodel makes the cmlmodel an instance of rmws domainsize ysize xsize defines the size of the simulation grid covmod cmod defines the covariance model that was fitted in step d nfields nfields defines the number of conditional fields one wants to simulate cp cp defines the equality constraint coordinates on the simulation grid cv cv defines the equality constraint values corresponding to cp le cp lecp defines the coordinates of less or equal constraints on the simulation grid le cv lecv defines the less or equal constraint values which in the present example are observations that recorded zero rainfall corresponding to le cp optmethod circleopt defines that rmwscondsim runs in inversion mode minobj 0 4 defines the stopping criteria i e if the objective function reaches that value the inversion terminates maxiter 300 defines the maximum number of iterations per realization after which the inversion terminates if minobj has not been reached h call rmws which will start the inversion process rmws follows the procedure described in section 2 i post processing of the resulting conditional fields cs finalfields in the present example the post processing involves the back transformation of the simulated conditional fields into their actual data space using the fitted inverse marginal distribution further the back transformed fields are saved as a npy file the individual fields the mean field and the variance field are plotted and a box plot for the simulated cml data is created this box plot shows how the simulated cmls match the observed cmls 3 run the run inv py script this starts the whole process i e all pre processing the actual inversion and the subsequent post processing the scripts described above are part of the rmwspy repository a random seed np random seed 121 defined in run inv py ensures that the results are identical to the results presented in this paper note in order to obtain the same figure style as shown in this paper for figs 4 6 and 8 the variable paperstyle plot defined in run inv py needs to be set to true this however requires the installation of some non standard python modules as described in the github repository if paperstyle plot false a more basic figure style is applied which uses python s standard matplotlib only changing the random seed will result in different random numbers and thus different results as rmwspy is a stochastic approach in order to test how different the results of the precipitation estimation can be without cml data the reader can repeat the simulation based on the rain gauge observations only therefore optmethod circleopt needs to be changed to optmethod no nl constraints which will ignore the non linear i e the cml observations rmwspy then runs in linear simulation mode taking the linear observations rain gauges into account only the resulting box plots exhibit a wider spread around the observed values as they are not used for conditioning anymore and the estimation uncertainty in terms of the standard deviation increases other variables that impact the results are for example minobj and maxiter increasing the value of minobj would lead to a weaker optimization of the cml observations thus to a higher estimation uncertainty however this also means that the process can finish with less iterations similarly if maxiter is reduced the inversion process might not reach minobj but it again will be faster as less iteration are performed in general the chosen minimum objective function value minobj should be sensible for the respective inversion problem in the case of the cml constraints for example a minobj value of less than 0 4 would not be very sensible as that is less than the measurement uncertainty of the cmls 3 3 results and discussion for comparison an example of a simulated random field rf the ensemble mean over all 50 simulations and the vt are shown at fig 4 both the single simulation a and the ensemble mean b showing the same main patterns as the vt c while the single simulation shows a slightly higher spatial variability as the vt the ensemble mean slightly underestimates the spatial variability this behavior can also be seen in the pixelwise comparison at fig 5 while the single rf has a similar range of precipitation values the ensemble mean underestimates the higher precipitation values the pearson correlation coefficient between the vt and all fields of the ensemble is r e n s e m b l e 0 73 r s i n g l e r f 0 77 for the example single rf for the ensemble mean the pearson correlation coefficient is r e n s m e a n 0 86 exploring the uncertainty of the simulated ensemble of precipitation fields by the usage of the standard deviation calculated for each grid cell fig 6 show the highest values in areas of heavier precipitation since figs 5 and 6 indicate a slightly displacement of the precipitation values compared to the vt we will use the fraction skill score fss roberts and lean 2008 as an additional quality measure for the three chosen thresholds of 2 m m h 4 m m h and 6 m m h we define a rainy grid and calculate the fss by 1 f s s 1 i j 1 n p v t i j p s i m i j 2 i j 1 n p v t i j 2 i j 1 n p s i m i j 2 the fss is a fuzzy verification measure which compares the fractional coverage of a certain value of two patterns here the chosen thresholds for the precipitation amount for a given neighborhood with size n the fss compares the precipitation of the vt p v t and the simulated pattern p s i m its values range between 0 and 1 where 1 indicates perfect coverage the reasonable skill is defined as the wet area ratio of the reference field 0 5 p v t t r u e 2 the fss passes if its value is higher than this reasonable skill fig 7 visualize the fss analysis for the three thresholds using a threshold of 2 m m h the ens mean and most of the ensemble members reach immediately the reasonable skill of 0 66 at 1 1 km however the example single rf reaches the reasonable skill at 2 5 km which may be explained by the heavy noise of the single field see also fig 4 a since the boxplot for the whole ensemble fig 7 b indicate with a neighborhood size of 1 1 km a better skill the chosen example is a outlier using a 4 m m h threshold the reasonable skill with 0 56 is reached from both the ens mean and the single random field at a 5 km neighborhood only the fss of the single random field has a steeper ascent the boxplot indicate that the 25 quantile up to the 75 quantile of the ensemble reach the reasonable scale earlier by 1 1 km the reasonable skill of 0 52 for the threshold of 6 m m h is reached by the single realization by 10 km and for most the ensemble members by 9 km since the ens mean smooths the precipitation field it does not reach the reasonable skill for the 6 m m h threshold as stated in section 3 2 to satisfy the non linear cml observations the rmse between the synthetic cml observations and their simulated counterparts is minimized the quality of the simulated path integrated rain rates regarding the synthetic observations is shown in fig 8 for all 100 cmls the simulated rates are in a good agreement with the synthetic observations for most cmls the mean difference between the observations and the simulated counterparts is below 0 5 m m h which is well within the range of the cml observation uncertainty for only 20 cmls the maximal difference between the observation and an ensemble member is higher than 1 m m h again it should be mentioned that these differences also depend on the choice of the minobj and maxiter parameters summarizing using rmwspy to simulate precipitation fields in an inversion framework based on linear rain gauge observations and non linear cmls observations conditions results in an ensemble with any given count of members the example shows that the single realizations are in a good agreement with the vt it is demonstrated that the cml observations are represented reasonably i e their values are reproduced closely but still allowing enough variation to represent the measurement uncertainty the overall estimation uncertainty in terms of the standard deviation is obtained using the ensemble of realizations 4 conclusions the use of rmwspy offers a very general framework for conditional spatial simulation and inversion for environmental modelling problems it can be used as an independent standalone python package or in conjunction with other programming languages and numerical forward models using a high level programming language such as python allows complicated tasks to be achieved within a few lines of code which is also straightforward to read the scripting approach rather than using a gui offers a great flexibility especially due to the availability of hundreds of python modules for almost every possible scientific task the example presented in this paper shows how to apply rmwspy to an inverse modelling problem for improved precipitation estimation using rain gauge data and cml data it is demonstrated how the cmlmodel class which describes the non linear problem is defined all functions within this class are delineated the main script run inv py including its variables and the pre and post processing steps are described and the results of the inversion are discussed in detail rmwspy is an open source project hosted on github all scripts discussed in this paper and the corresponding data files are part of this repository further examples such as a jupyter notebook covering the basic use cases of rmwspy and groundwater inversion study with modflow using multi threading are also available on github code and data availability all code developments presented here are open source tools released under the gnu general public license v3 0 source code version 1 1 used in this paper is archived online at https github com sebastianhoerning rmwspy declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research for this paper was partly supported by the german research foundation deutsche forschungsgemeinschaft for2131 data assimilation for improved characterization of fluxes across compartmental interfaces projektnummer 652387 ku and partly by the energi simulation program the authors would also like to thank the university of queensland centre for natural gas and its industry members aplng arrow energy and santos for providing research funding the authors gratefully acknowledge bernd schalge for providing the synthetic data set finally the authors highly welcome additions suggestions improvements and assistance from the spatial statistics and the broader scientific community 
25868,forest disturbance regimes are changing around the globe of particular concern are biotic disturbance agents as they respond strongly to climate warming and invade new ecosystems as alien pests and pathogens to date biotic disturbances are either ignored in simulations of vegetation dynamics or only a small number of common agents are considered explicitly here we present bite a general process based approach to simulate biotic forest disturbance agents from fungi to large mammals bite considers the processes of agent introduction dispersal colonization population dynamics and vegetation impact explicitly here we parameterize the model for six widely different biotic disturbance agents heterobasidion annosum hymenoscyphus fraxineus lymanthia dispar anoplophora glabripennis capreolus capreolus mammut americanum and evaluate it using pattern oriented modeling bite enables the inclusion of both established and novel biotic disturbance agents in vegetation models and is a step towards the comprehensive simulation of forest disturbance regimes in a changing world keywords biotic disturbance pests pathogens invasive alien species mechanistic modeling 1 introduction natural disturbances i e discrete events that disrupt the structure of an ecosystem shape forests around the world the disturbance regime of a given area is characterized by a typical frequency size and severity of disturbance turner 2010 a key determinant of disturbance regimes are the prevailing disturbance agents i e the factors causing disturbance as they strongly influence the spatial patterns and climate sensitivity of disturbances seidl et al 2020 ecosystems are generally well adapted to the prevailing disturbance regime yet disturbances are changing as a result of climate change for example the frequency of wildfires schoennagel et al 2017 and the severity of insect outbreaks raffa et al 2008 is increasing in many parts of the world these changes are of concern as the ecosystem services forests provide to society are predominately negatively affected by disturbances thom and seidl 2016 quantifying and managing the impacts caused by current and potential future disturbance regimes is thus increasingly important investigations of the effects of natural disturbances often focus on severe and abrupt abiotic events such as large wildfires flood events or windstorms however the impacts of biotic disturbances i e those caused by a variety of organisms from fungi to insects and herbivorous mammals are rivaling the impacts of their abiotic counterparts healey et al 2016 kautz et al 2017 furthermore a warming climate is predicted to benefit many biotic agents significantly bentz et al 2019 la porta et al 2008 compared to abiotic disturbances our understanding of biotic disturbance agents remains limited not least because of the complex biology of the various life forms that can cause disturbances furthermore biotic disturbances are often modulated by complex interactions with other abiotic and biotic disturbance agents seidl et al 2017 which makes their analysis and quantification challenging due to the high level of complexity involved simulation modeling is a particularly important tool for understanding biotic disturbances global change is considerably altering biotic disturbance regimes many biotic disturbance agents are for instance shifting polewards bebber et al 2013 and species that are benign in their native range can become impactful pests in new systems desprez loustau et al 2007 økland et al 2019 in addition new biotic disturbance agents are emerging as global trade accelerates the introduction of non native pests and pathogens chapman et al 2017 santini et al 2013 seebens et al 2017 climate change sometimes benefits the establishment and spread of non native species exacerbating the issue of non native pests and pathogens further seidl et al 2018b walther et al 2009 invasive pests and pathogens can cause dramatic changes in their new environment by diminishing their host population or even driving it into extinction lovett et al 2006 mack et al 2000 wardle et al 2001 in addition they can interact with native disturbance agents gonthier et al 2007 or have indirect effects on human health donovan et al 2013 a common characteristic of all these novel biotic disturbance agents is that the available information on them is limited this means that strongly data driven approaches e g empirical models machine learning see rammer and seidl 2019 are often not feasible for modeling novel biotic disturbance agents and rather simple process based or theoretical models are needed to provide timely model based inference for management the potential negative impacts of biotic disturbance agents can be mediated through a range of potential management options these extend from aiming to eradicate a newly introduced agent liebhold and bascompte 2003 to changing the host population structure and configuration on the landscape honkaniemi et al 2020 and applying chemical or biological control to decrease the abundance of the agent holmes and macquarrie 2016 furthermore an efficient monitoring of biotic agents is a key element of management especially in the case of emerging pests and pathogens as the timing and spatial focus of countermeasures can critically influence management success cunniffe et al 2016 simberloff 2003 simberloff et al 2005 simulation models have proven to be valuable tools for informing management on where when and which measures to apply in order to optimally contain biotic disturbances computer simulations of ecosystem dynamics have developed rapidly in recent years seidl 2017 the increase in computational capacity for instance has enabled a shift from modeling forest stands to focusing on their larger spatial context at the landscape scale shifley et al 2017 this development is important especially in the context of simulating biotic disturbances as they can spread from stand to stand occur across relatively large extents and are driven by factors across multiple spatial scales cushman and meentemeyer 2008 seidl et al 2016 nonetheless most studies simulating the potential risk from pests and pathogens have to date focused on the potential for introduction and spread of an agent de la fuente et al 2018 ferrari et al 2014 with considerably less attention on simulating the potential impacts of biotic agents seidl et al 2018b process based models of biotic disturbances operating at different scales cushman and meentemeyer 2008 and coupled with dynamic vegetation models cunniffe et al 2015 remain still rare to date one reason is that process based models often require detailed information on agent biology that often is not available simplified models such as sir models borrowed from epidemiology kermack and mckendrick 1927 have been successfully applied to pests and pathogens in agricultural crops whish et al 2015 and livestock keeling 2005 however generally applicable biotic disturbance models in forest ecosystems remain scarce to date but see kriticos et al 2013 lustig et al 2017 sturtevant et al 2004 tonini et al 2018 wildemeersch et al 2019 here we present bite the biotic disturbance engine a general model to simulate biotic disturbances in forest ecosystems our objective was to develop a modeling framework that is general enough to simulate a wide range of biotic disturbance agents from fungi to insects and large mammals further objectives were to keep the framework simple and modular in order to also be applicable in situations where knowledge about an agent is limited as is the case with new invaders coupled with a forest landscape simulation model bite allows the quantification of the impacts of emerging pests and pathogens on forests in time and space here we present the bite modeling framework and illustrate its generality by applying it to six widely different biotic disturbance agents two pathogens an ascomycete and a basidiomycete two insects a defoliator and a bark beetle and two mammals browsers of different size and life history strategy we analyzed the different patterns produced by bite for this wide variety of agents following the principles of pattern oriented modeling grimm et al 2005 2 materials and methods 2 1 model overview bite is a general model to simulate the dynamics of biotic disturbance agents with a special focus on being able to accommodate also emerging pests and pathogens bite was designed to be easily adaptable to different agents and conditions and allows the flexible assimilation of emerging knowledge on agent dynamics due to its modular structure the key design elements of our modeling approach are 1 agents are simulated on a regular grid with an agent specific resolution between 10 m 1000 m grid cells while agent specific variables are homogeneous within a grid cell vegetation i e availability of host trees and environment e g climatic indicators can vary within a grid cell 2 biomass is used as a common currency for simulating the abundance of agents in a grid cell but also for simulating the impact of biotic disturbance agents on forest vegetation an individual based modeling approach railsback and grimm 2019 was considered but was not deemed suitable for the simulation of very small organism such as fungal spores across large spatial extents 3 the movement of individual agents in a given time step is approximated by probabilistic dispersal kernels 4 agent population dynamics within a grid cell is modeled via empirical growth equations the growth rate of the population is limited by a carrying capacity determined by the host availability within a cell 5 the impact of biotic disturbance agents on vegetation can be generalized into the consumption of foliage biomass the consumption of root biomass or tree mortality e g by disrupting physiological processes such as phloem conductivity an important design strategy for achieving general applicability across a wide range of biotic disturbance agents is modularity bite models biotic disturbance agents in six distinct modules i potential habitat ii introduction iii dispersal iv colonization v population dynamics and vi impact fig 1 each module represents an important aspect of agent biology and provides specific options in the parameterization of a specific agent e g different dispersal kernel functions to accurately characterize agent behavior the level of detail implemented in each module can vary from simple to complex and computations in each module can potentially use state variables such as agent biomass in the previous time step vegetation structure and composition and environmental conditions modules can also be bypassed for selected agents if they are not applicable or if not enough information for their parametrization is available bite is a general framework that can be coupled with a wide variety of dynamic vegetation models conceptually any model that provides gridded host biomass for the agents to consume could be coupled with bite however specific implementations of the bite framework can be tailored to a model s representation of vegetation and environment such as the specific rendering of stand structure tree dimensions and available climate variables in a given model in the current implementation the framework is coupled with the individual based forest landscape and disturbance model iland seidl et al 2012a thom et al 2017 consequently we here focus on the dynamics of biotic disturbance agents in forest ecosystems at the landscape scale i e an extent of 103 105 ha ha iland is a process based model simulating forest landscape dynamics at the spatial grain of individual trees see http iland boku ac at for a technical model documentation disturbances can have long term impacts on forest structure and composition johnstone et al 2016 with regeneration and growth processes being key determinants of ecosystem resilience and the persistence of disturbance impacts in iland these processes are simulated in detail based on first principles of ecology e g spatio temporal dispersal in combination with local light availability and environmental filters determines the establishment of new trees after a disturbance a radiation use efficiency approach landsberg and waring 1997 is used to calculate gross primary production from daily weather data and local water and nutrient availability the assimilated carbon is subsequently allocated to different tree compartments based on species specific allometric ratios between compartments in bite these compartments are consumed by the simulated agents leading to single tree mortality which is related to carbon starvation thus mortality increases if stressors such as the biotic agents simulated in bite lead to the depletion of a trees carbohydrate reserves in addition an age and size related background mortality rate is calculated the spatial grain of simulations in iland varies with local light environment being simulated on 2 m grid cells while heterogeneity in climate and soil is considered at the resolution of 100 m iland was successfully evaluated and applied in central europe silva pedro et al 2015 thom et al 2018 and the western us braziunas et al 2018 seidl et al 2012b in the current implementation of bite relevant elements of vegetation e g host tree biomass and climate e g growing degree days are provided dynamically from the iland simulation environment in turn vegetation impacts from bite disturbance agent activity e g trees killed biomass consumed are dynamically fed back into iland where they change the simulated vegetation state bite is thus fully integrated within iland with dynamic feedbacks between vegetation and biotic disturbance agents 2 2 detailed model description 2 2 1 technical implementation besides general applicability across a wide range of biotic disturbance agents important design principles of bite were flexibility in the application in order to allow tailoring the model to emerging research questions and computational efficiency to accommodate a potential large number of simulations in scenario analyses therefore agents in bite are defined in javascript granting ease of use and flexibility and executed with an engine written in c ensuring its computational efficiency the agent code is mostly declarative i e describing the what rather than the how but can be augmented by imperative code for more complex agent behavior see supplementary material s1 and javascript files for the agents in figshare multiple agents can be active within a single simulation run and events such as agent introductions or management interventions can be triggered at any time during a simulation technical details and example code are provided in supplementary material s1 full source code and documentation is available under a gnu general public license gnu gpl www gnu org licenses gpl 3 0 html from http iland boku ac at bite the implementation of bite is generally model agnostic and provides a clear technical interface between bite and the vegetation model coupling bite with a new vegetation model is fairly straightforward via this interface however since the structure of vegetation models differ from each other e g represented biomass pools spatial resolution different environmental drivers coupling a new model might also require an adaptation of bite processes and functions the following description is based on the current implementation of bite within the iland landscape and disturbance modeling framework 2 2 2 agent life history important information on the life history of the simulated agent s need to be provided to the model these include information on voltinism i e univoltine bivoltine multivoltine the frequency of dispersal number of dispersal sequences per timestep and generation and prerequisites for initiating dispersal after colonization i e possible delay due to inoculum accumulation in addition in case of cyclic agents e g insect or vole outbreaks the outbreak duration and time between outbreaks are specified 2 2 3 potential habitat this module determines the general habitat suitability of an agent for the simulated landscape it uses binary raster files of long term climate suitability e g derived from species distribution models land use e g deer exclosures or other environmental filters of relevance e g elevation water bodies all these filters are not calculated dynamically in bite but are provided as external input to the model simulation essentially the provided rasters identify the potential spatial domain of simulation for each agent within the landscape please note that filters can change over time e g when habitat suitability changes in response to climate change 2 2 4 introduction this module simulates the introduction of a new biotic disturbance agent into the simulated landscape in bite an agent can be introduced at time t in n cells of the landscape either randomly or in predefined locations grids with introduction probability can also be provided e g increased probability of introduction closer to roads or human settlements 2 2 5 dispersal the dispersal module is responsible for simulating the movement of the agent across the landscape dispersal is simulated in a probabilistic way in bite calculating the spatiotemporal establishment probability of an agent at a new location a species specific dispersal kernel indicating the probability that a cell is the target of dispersal from a focal cell as determined by the distance from the focal cell and a maximum dispersal distance determine agent movement in space and time bite first calculates dispersal individually for all cells where the agent is present and subsequently combines dispersal probabilities for cells that can be reached from multiple source cells of the agent population 2 2 6 colonization the colonization module calculates whether an agent is actually able to colonize a previously uninhabited cell successful colonization is contingent on several conditions first the agent must be able to successfully disperse into the cell dispersal success is calculated from the dispersal probability derived in the dispersal module by either invoking a fixed threshold or by drawing a random number user parameter in addition the successful colonization of a cell depends on the vegetation of a cell e g the presence of host trees of the agent furthermore environmental conditions can limit colonization of a cell e g the need to exceed certain temperature sums indicating the thermal ability to complete a full development cycle for insects both vegetation and environmental conditions are implemented as boolean filters i e colonization is successful only when all conditions are met successfully colonized cells are simulated to have an initial agent biomass which serves as the basis for further calculations of population dynamics user parameter 2 2 7 population dynamics once an agent has successfully colonized a cell its biomass in the cell bm a can grow with growth being limited by the relevant host biomass e g canopy biomass root biomass depending on the compartment that is affected by the agent that is available at a cell as well as environmental conditions e g temperature soil moisture population growth is simulated with a growth equation that can be provided by the user for each specific case by default a logistic growth equation is used incorporating a relative maximum growth rate r the agent biomass m in the previous time step and the carrying capacity of a cell k see supplementary material s1 eq s2 the carrying capacity is calculated as a function of the targeted host biomass in the cell and the potential annual biomass consumption per biomass unit of the agent the output of the population dynamics module is the agent biomass bm a in each cell at timestep t although the default function for population dynamics in bite is a simple logistic growth equation any other type of function e g second order equations lotka volterra to describe population dynamics could be used the use of more complex growth equations depends on improved data availability for parametrization but equations can also be used experimentally to test the effect of different assumptions of agent growth on simulated disturbance dynamics 2 2 8 impact the impact module calculates the effect of biotic agent activity on vegetation and provides a feedback to the simulated vegetation dynamics in the cell in bite impact is calculated as the host biomass loss due to agent activity this information is subsequently fed back to the vegetation simulator where impacts alter the simulated vegetation structure and composition impacts in bite either affect a specified host compartment with a maximum annual consumption rate per unit agent biomass or an entire tree in the case of direct mortality in iland the consumption of different tree compartments foliage or roots increases the probability of mortality as it changes the physiological abilities of a tree increases maintenance respiration and can lead to carbon starvation possible impact types for trees 4 m in height are consumption of foliage or root biomass and tree mortality for saplings height 4 m the simulated impacts are either tree mortality or consumption of the leader shoot by browsing impacts can be stratified by tree dimension and agent preferences can be considered e g shortest trees are targeted first for example an agent can be parameterized to affect only trees 15 m in height or have varying impact rates between diameter classes agents can also impact different tree compartments at the same time in a simplified implementation of bite where agent biomass dynamics are not simulated explicitly due to data limitations the impact of an agent can be simulated phenomenologically by specifying the share of trees affected per cell or the share of compartment consumed in affected trees trees in iland also die from other causes of mortality such as self thinning harvesting and abiotic stress these causes of mortality are computed independently from biotic disturbance mortality but do interact with each other indirectly as they all modify forest structure and composition which in turn influences mortality 2 3 model evaluation 2 3 1 biotic agents to demonstrate the utility and versatility of bite we parameterized and tested it for six widely different biotic disturbance agents i e heterobasidion root rot heterobasidion annosum fr bref european gypsy moth lymantria dispar l roe deer capreolus capreolus l ash dieback hymenoscyphus fraxineus baral et al 2014 asian long horned beetle anolophora glabripennis motchulsky and mastodon mammut americanum kerr table 1 see figshare folder for biteagent javascript codes the agents were chosen to cover a wide variety of biota from fungi to insects and large mammals differing strongly in their life history and impact on tree vegetation in addition the agents represent a wide variety of information available from organisms that have been studied in depth heterobasidion root rot gypsy moth roe deer to those for which less information is available ash dieback asian long horned beetle mastodon in order to test the robustness of the model also in situations where information is limited such as is often the case with a newly invading pest species all species were parameterized based on available information from the literature we also want to highlight that while some of the simulated example agents are invasive alien species we do not aim to capture invasion dynamics with bite and iland as this would require a much larger spatial scope for the simulations we here explicitly focus on the dynamics of biotic agents and their impacts on vegetation in the landscape scale heterobasidion annosum is one of the most destructive forest pathogens in the northern hemisphere causing root rot specifically on conifers garbelotto and gonthier 2013 different species of the group are well established in many regions of eurasia and north america but it is currently also spreading into new areas e g bérubé et al 2018 heterobasidion sp spread via spore infections through fresh stumps e g created by tree harvesting within the stand the fungus spreads vegetatively via mycelia garbelotto and gonthier 2013 while heterobasidion sp spreads only over short distances kallio 1970 möykkynen et al 1997 it can endure at a site for several tree generations stenlid and redfern 1998 99 5 of the spores land within a few hundred meters with the remaining 0 5 being responsible for the long distance dispersal of the pathogen kallio 1970 in bite dispersal was simulated with a power law function kallio 1970 möykkynen et al 1997 and the colonization was restricted to cells were fresh stump surfaces were available for spore germination rishbeth 1951 population dynamics were simulated based on a logistic growth model with consumption and growth rate parameters derived from honkaniemi et al 2017b the impact on infested trees is simulated as a reduction in root biomass european gypsy moth hereafter referred to as gypsy moth is a defoliator native to europe where it causes substantial disturbance especially on oaks mcmanus and csóka 2007 in 1869 it was also introduced to north america where it became an invasive pest seriously threatening oak forests in the northeastern usa elkinton and liebhold 1990 adult gypsy moths are poor dispersers but the first instar larvae spread passively over short distances via wind hunter and elkinton 2000 however human aided long distance dispersal is driving the invasion in north america liebhold et al 1992 the development of a gypsy moth from egg to adult takes one season and during that development each larvae consume about 3 4 g of foliage sharov and colbert 1996 outbreaks of gypsy moth typically last for several years and occur both in europe and north america in 8 12 year intervals johnson et al 2005 gypsy moth dispersal was approximated with a gaussian dispersal kernel in bite elderd et al 2013 and its population dynamics was simulated with a logistic growth equation based on growth rates modified from lustig et al 2017 the simulated impact was consumption of foliage biomass with preference for small over large trees roe deer is a species of deer native to europe it is widespread throughout the whole continent from southern europe to the nordic countries and is expanding its range due to warming climate and changes in land use danilov et al 2017 valente et al 2014 roe deer is a relatively small deer species with an average body mass of 20 30 kg andersen et al 1998 pettorelli et al 2002 they are territorial animals and their habitat can range from agricultural landscapes to woodlands tixier and duncan 1996 roe deer graze fresh grass but a significant part of the diet consists also of sapling shoots tixier and duncan 1996 silver fir abies alba is one of the most favored tree species for browsing in many areas browsing rates on the species are substantial and can even lead to regeneration failure of silver fir senn and suter 2003 in bite we assumed roe deer to populate the entire landscape with a constant density of 14 individuals per 100 ha senn and suter 2003 the consumption was derived by combining daily diet preferences and consumption rates reported from different environments drozdz and osiecki 1973 tixier and duncan 1996 the simulated impact was a loss of the leader shoot and thus a loss of current year height growth for saplings with a height of 1 3 m hymenoscyphus fraxineus is the causal agent of ash dieback a non native disease that has affected europe s forests over the past three decades kowalski and holdenrieder 2009 mckinney et al 2014 and is currently threatening ash primarily faxinus excelsior l populations all over the continent pautasso et al 2013 environmental factors such as soil moisture and temperature as well as stand variables like stand age and density have been linked to the epidemiology of the fungus skovsgaard et al 2017 in bite its dispersal was simulated with an inverse power law dispersal kernel grosdidier et al 2018 as recent results show a decrease of the disease with decreasing host density bakys et al 2013 we assumed that only cells with a host tree density of 100 stems ha 1 over more than 3 years were susceptible to ash dieback as we could not gather enough information to build a reliable agent population dynamics module we omitted this aspect in simulations we assumed that if the pathogen is present it causes heavy defoliation 50 100 of foliage mass removed for a maximum of 30 of the host trees of a cell timmermann et al 2017 in addition we assumed that 1 of the trees are resistant to the disease kjaer et al 2012 wohlmuth et al 2018 asian long horned beetle alb is an insect species native to china and korea attacking the stems of multiple deciduous tree species its larvae consume the wood which can eventually lead to tree mortality haack et al 1997 hérard et al 2006 global trade has resulted in the introduction of alb to many areas outside its native range eyre and haack 2017 as the species effectively disperses in wood packaging material alb is a moderate disperser and we here used a general leptokurtic dispersal kernel shatz et al 2016 we assumed that to colonize a cell the presence of a host with dbh 7 5 cm was needed dodds and orwig 2011 even though the life cycle of the agent is generally well known haack et al 2009 we didn t find enough reliable quantitative information to parameterize the detailed agent population dynamics module of bite there are several different estimates of the potential impact of alb with reported mortality rates varying from 3 to 30 faccoli and gatto 2016 nowak et al 2001 impact data often stem from poplar plantations in the native habitat of the beetle in china see hu et al 2009 and the references therein however quick eradication measures at infested sites interfere with the quantification of the true impacts alb could cause on host populations dodds and orwig 2011 studied the only large scale infestation outside the native range in a non urban environment in massachusetts usa and found that tree mortality and growth losses were extremely rare even more than 5 years after an infestation we here assumed a linearly increasing mortality rate from 0 to 2 over a 10 year period to simulate the protracted mortality caused by alb mastodons were large mammals distantly related to elephants inhabiting the forests of north america and eurasia until their extinction 10 11 000 years ago compared to mammoths mammuthus sp which were grazers mastodons were forest dwelling browsers with picea spp forming a significant part of their diet birks et al 2018 teale and miller 2012 their estimated body mass was 8000 kg mastodons were thus slightly heavier than modern elephants although their shoulder height was roughly comparable larramendi 2015 we assumed mastodons to inhabit the whole test landscape with an initial density of 1 5 individuals per 100 ha corresponding to the estimated densities of pleistocene megaherbivores 120 kg ha 1 bakker et al 2016 mastodon population growth rate was assumed to be 1 yr 1 using a logistic growth model we assumed that mastodons were able to browse trees up to 4 m height with a preference for trees between 0 and 2 m guy 1976 and the occasional uprooting of trees similar to modern elephants scheiter and higgins 2012 shannon et al 2008 the diet was assumed to consist of 20 norway spruce picea abies l karst 2 3 2 simulation design to demonstrate the model s utility and evaluate the patterns emerging from simulations we simulated the dynamics of each agent separately in a generic landscape with tree species and climate typical for the temperate biome we aimed for maximum comparability between agents of different traits and thus eliminated potential confounding factors such as variability in climate and host tree distribution in our simulations specifically we simulated a rectangular 5 5 km forest area solely populated by the main host species of each agent table 1 initial stand age was uniformly distributed on the landscape between 0 and 100 years stands were created as voronoi polygons with an average stand size of 1 ha tree dimensions e g tree diameter tree height stand density were derived from yield tables marschall 1975 the environment i e soil climate was assumed to be uniform in space but climate varied over time around a stationary mean temperature of 8 6 c and annual precipitation of 810 mm the climate data were derived from a typical temperate forest landscape in central europe see honkaniemi et al 2020 by taking the landscape mean and variation of a time series from 1981 to 2010 in the simulations including management see below stands were clearcut at a stand age of 100 years and replanted with the host tree species natural regeneration as well as abiotic disturbances e g wind wildfire were omitted 2 3 3 analyses we analyzed model behavior for the six agents in a pattern oriented framework grimm et al 2005 grimm and railsback 2012 specifically we focused on three patterns produced by bite 1 impact on host mortality or browsing rate 2 spatial patterns of agent spread from a single point of introduction and 3 temporal patterns of agent dynamics over time impact rates pattern 1 were simulated assuming that the agent is present in each cell of the landscape and thus controls for differences in colonization times analyzed separately in pattern 2 the analyzed impact solely relates to the direct effects of the focal agent and excludes other causes of mortality such as density dependent mortality or age related mortality simulated impact rates were compared against independent data sources not used for model parameterization table 2 supplementary material s3 except for asian long horned beetle and mastodon where no field data were available the simulations were run for 50 years simulations of pattern 1 excluded forest management to avoid confounding effects between simulated management and disturbance in addition to mean impact rates we also analyzed structural effects of tree mortality and compared the diameter distribution of trees killed by a focal agent to background mortality from competition and age related causes we also carried out a sensitivity analysis for simulating biotic disturbance impact on varying tree sizes see supplementary material s2 spatial patterns of biotic disturbance pattern 2 were simulated in the same agent specific landscapes but agents were introduced in a single cell in the center of the landscape and their spread simulated over a 50 year period the two browsing mammals roe deer and mastodon were excluded from this analysis as they were assumed to be present throughout the landscape analysis variables were the infestation pattern emerging from the landscape as well as the impact on the host species no field data were available for direct comparison but the emerging infestation patterns were assessed qualitatively against expected patterns see table 2 forest management was simulated in all model runs for testing pattern 2 in order to maintain a comparable landscape state in areas not yet infested by the agent agent dynamics over time pattern 3 was studied over 50 years for the four agents for which population dynamics was explicitly simulated i e root rot gypsy moth roe deer and mastodon root rot and gypsy moth were introduced in the center cell of the landscape as described for pattern 2 above roe deer and mastodon were assumed to be present in each simulated cell of the landscape the output was normalized to the number of cells colonized in each time step to facilitate comparison across agents the development of agent populations and their impact on host tree vegetation were compared against independent observations from the literature table 2 3 results 3 1 impact pattern 1 simulated annual mortality rates corresponded well with independent observations for well described agents i e root rot and gypsy moth fig 2 although bite on average underestimated the impact of these agents slightly browsing rates for roe deer were also well in line with expectations though slightly higher than those reported in the literature bite simulations overestimated the annual mortality from ash dieback although the variation on the landscape was high asian long horned beetle and mastodon presented the extreme ends of the range in terms of impact on vegetation which was consistent with the model internal logic and parameterization field data for evaluation were unfortunately not available for these two species tree mortality caused by gypsy moth and ash dieback were higher than background mortality i e mortality caused by other reasons than the agent such as competition or age related causes and amounted to 2 5 and 5 3 of mortality per year respectively compared to 0 6 and 1 3 of background mortality respectively conversely background mortality was higher than mortality caused by root rot and alb in our simulations agent induced mortality 0 3 and 0 5 respectively compared to background mortality of 1 8 and 1 5 respectively the diameter distributions of trees affected by biotic agents reflected the different size preferences of agents well see supplementary material s2 3 2 spatial patterns of agent spread pattern 2 spatial patterns of spread varied widely between the simulated agents after 50 simulation years ash dieback was active in 100 of the landscape alb in 25 6 gypsy moth in 1 7 and heterobasidion root rot in 0 3 in line with expectations ash dieback spread throughout the landscape table 2 fig 3 a and caused mortality over the entire simulation period but affected trees in a scattered pattern fig 3b asian long horned beetle behaved similarly to ash dieback by spreading throughout the landscape 100 the relatively low mortality rate kept the infestation going throughout the entire simulation period as host trees remained available for infestation see supplementary material s2 for a management related eradication experiment the impact patterns of alb were generally patchier with synchronous mortality occurring in larger areas fig 3b gypsy moth only dispersed to a small area 1 7 of the landscape over 50 years of simulation and cells were frequently infested only once fig 3a we note that human aided long distance dispersal of gypsy moth was not considered here due to the periodicity of outbreaks gypsy moth impact occurred synchronous in small patches fig 3b heterobasidion was a poor disperser being only able to colonize 0 3 of the landscape in our simulation it s pronounced long distance dispersal in combination with the establishment requirement of fresh stumps resulted in an almost random pattern of infested cells on the landscape fig 3a the resulting impact pattern of small mortality centers in stands corresponds well with expectations from the literature table 2 fig 3b 3 3 temporal patterns of agent dynamics pattern 3 the four agents for which population dynamics was explicitly simulated showed widely varying development over time the relatively fast growth rate of heterobasidion root rot together with a high carrying capacity as determined by the root biomass of both fresh stumps and live trees enabled the agent biomass to increase to high levels fig 4 a because root rot is introduced via stumps and only after a considerable lag colonizes live trees tree mortality lagged behind infestation and increased only towards the end of the simulation period fig 4b gypsy moth outbreaks followed the periodicity as specified in the species parameters with population biomass fluctuating strongly over time fig 4a the overall biomass affected increased over time with the expanding colonized area host mortality due to defoliation reached its peak 1 3 years after the gypsy moth population peak which is well in line with independent observations table 2 fig 4b roe deer consumed its host at a steady rate over time in contrast mastodon populations consumed high levels of host biomass in a short period of time with negative feedbacks on habitat suitability and agent population levels this indicates that they would have had to either shift their diet to other plants or migrate to another landscape also suggesting that megaherbivore levels as in the pleistocene could not be sustained in modern landscapes 4 discussion and conclusions biotic disturbances are an integral part of forest ecosystems and are particularly sensitive to climate change seidl et al 2017 turner 2010 in the past the impacts of biotic disturbances have been modeled using predominately statistical approaches correlating disturbance presence abundance and impact seidl et al 2011 however the no analog conditions expected for the future steffen et al 2018 require alternative approaches as it is unlikely that future ecosystem dynamics can be faithfully predicted from the past gustafson 2013 data driven theoretical models e g based on approaches developed in epidemiology are a possible alternative to traditional empirical models e g jeger et al 2007 wildemeersch et al 2019 also novel machine learning approaches such as deep neural networks e g rammer and seidl 2019 reichstein et al 2019 are better able to deal with extrapolation than traditional correlative models yet both of these approaches thrive on the availability of large amounts of data and are thus of limited use in situations where information is scarce as is the case e g for the invasion of pests and pathogens into new areas process based models i e approaches focusing on the underlying mechanisms and modeling the system based on first principles of ecology are important tools in this regard they are also better suited to address unprecedented future changes e g climate change novel agent interactions in developing bite as a general framework that should be applicable to both established and novel biotic disturbance agents we have chosen a modular process oriented framework we here demonstrate the generality of our framework by simulating six widely different biotic disturbance agents ranging from fungi to large mammals a key question with all process based models is whether the relevant processes have been addressed and whether their interactions have been faithfully represented in the model pattern oriented modeling grimm et al 2005 grimm and railsback 2012 offers a consistent pathway for testing whether key patterns of the system are emerging from the simulation here we applied a pattern oriented approach assessing the behavior of the newly developed model and demonstrating that the model can handle a wide range of different types of agents we document that realistic patterns of disturbance impact as well as spatio temporal disturbance dynamics are emerging from simulations with bite biotic disturbances remain incompletely represented in existing dynamic vegetation models in fact most vegetation modeling approaches still ignore biotic disturbances completely huang et al 2020 if biotic disturbances are considered in the simulation of vegetation dynamics models usually focus on a small number of well known agents such as ips typographus in europe e g fahse and heurich 2011 honkaniemi et al 2018 seidl and rammer 2017 and dendroctonus ponderosae in north america e g bone and altaweel 2014 powell and bentz 2014 this practice is problematic because it can misrepresent the vulnerability of ecosystems to biotic disturbances overstating the susceptibility of host species of these prominent disturbance agents relative to other tree species this issue is particularly relevant when models are used to develop management strategies for reducing disturbance risk e g dobor et al 2020 seidl et al 2018a consequently an important goal of disturbance modeling has to be a broad and comprehensive representation of biotic disturbance agents and their interactions and the approach presented here is a step stone towards this goal see lustig et al 2017 sturtevant et al 2004 tonini et al 2018 for other broadly applicable approaches in particular design goals of bite were to provide useful approximations even under data scarcity and broad applicability for different types of agents e g by using biomass as a common currency for both agents and vegetation simulations with process based models are afflicted with several sources of uncertainty o neill and rust 1979 one important aspect is process uncertainty i e whether all relevant processes have been adequately represented in the model for example in the case of ash dieback and asian long horned beetle we did not have enough data to allow the in depth simulation of population dynamics instead we made phenomenological assumptions of agent development and impact nonetheless we could show that our parsimonious approach was well able to reproduce key patterns of biotic disturbance regimes even when individual processes had to be bypassed in the simulation due to data gaps see table 1 another important dimension of uncertainty is parameter uncertainty parameters in process based models are frequently derived from the literature and often hinge on a small number of studies furthermore non native species might behave different in their invasive range compared to their native range e g carnegie and pegg 2018 and information on their ecology collected in their native range might not necessarily be applicable in their invasive range future applications of bite should thus carefully scrutinize parameters and test the model against observations relevant for future study systems while we focused on disturbance impacts and spatio temporal patterns in our evaluation future work should also scrutinize the climate sensitivity of the simulated agents seidl et al 2020 more broadly more empirical and experimental research on biotic disturbance agents is needed to improve simulated forest disturbance regimes bite was deliberately designed to also work in conditions were information on biotic disturbance agents is limited as is often the case in the context of invasive alien pests and pathogens as a strong test for agents with limited data availability we included an extinct species the mastodon in the set of agents parameterized and evaluated here to our surprise it was easier to parameterize key processes of disturbance dynamics from the literature for the mastodon compared to species of high current relevance such as the asian long horned beetle the fact that our quantitative knowledge of charismatic species long extinct seems to be higher than that of species of high current management relevance e g dietary requirements of mastodons vs suitable host species for asian long horned beetle underlines the need for basic research on a broad range of biotic disturbance agents nonetheless including species with potentially high impact on forest ecosystems in models even when data is scarce is important to avoid the above mentioned biases in simulation studies and while simulating mastodon disturbance in a temperate forest ecosystem may at first sound like a highly hypothetical exercise the quantitative analysis of interactions between vegetation and past megaherbivores can shed light onto important questions of long term ecosystem development gill et al 2009 malhi et al 2016 and provide insights into how possible reintroductions of megaherbivores could shape forest ecosystems simulation models have for instance been used to show how abiotic disturbances have influenced critical transitions of ecosystems in the holocene e g henne et al 2015 which is something that could also be investigated for biotic disturbance agents in the future here we have demonstrated the utility of our new modeling framework for six widely different biotic disturbance agents and while we here have focused on agents individually a key strength of a general modeling framework like bite is that interactions between agents can be simulated interactions between disturbance agents are a key element of forest disturbance regimes buma 2015 honkaniemi et al 2018 seidl and rammer 2017 an attack by hymenoscyphus fraxineus for instance frequently results in infestations by armillaria sp which decreases the stability of a tree and greatly increases its mortality risk chandelier et al 2016 enderle et al 2013 similarly heterobasidion root rot decreases the rooting strength of trees and makes them considerably more susceptible to windthrow honkaniemi et al 2017a because such disturbance interactions are often amplifying they are a key element in the high sensitivity of disturbance regimes to climate change seidl et al 2017 seidl and rammer 2017 disturbance interactions should thus be explored in depth in the future in order to more comprehensively simulate past and future forest disturbance regimes to simulate forest disturbance regimes dynamically models of disturbance agents need to be coupled with models of vegetation dynamics huang et al 2020 seidl et al 2011 here bite was coupled with the individual based forest landscape and disturbance model iland to demonstrate the patterns emerging from an integrated simulation of disturbance agents and forest development however bite was designed with clear interfaces and a modular structure and thus could also be easily adopted to other vegetation modeling frameworks this is particularly relevant as the simulation of disturbances has been identified as an important weakness of current dynamic global vegetation models fisher et al 2018 huang et al 2020 mcdowell et al 2020 the approach presented here could provide important building blocks for an improved consideration of biotic disturbances in models used to inform policy makers around the globe changes in disturbance regimes and novel emerging biotic disturbance agents will have unprecedented impacts on forests and challenge our knowledge of how to address disturbances in management ayres and lombardero 2018 hobbs et al 2006 improved simulation approaches hold great potential to help address the challenge of changing forest disturbance regimes 5 software and data availability section software namebite biotic disturbance engine developerwerner rammer year first official release2021 hardware requirementspc system requirementswindows linux program languagec javascript program size 2 8 mb availabilityhttp iland boku ac at startpage licensegpl 3 0 form of repositoryjavascript files size of archive43 6 kb access form figshare figshare com articles software agent javascript files for bite model 13603439 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements rs and wr acknowledge support from the austrian science fund fwf through start grant y895 b25 author contributions rs initiated the study all authors jointly developed the idea and study design jh and wr developed the technical solutions wr implemented the model code jh conducted the simulations and analyzed the simulated data jh wrote the initial draft of the manuscript all authors contributed critically to revising the text and gave final approval for publication appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 104977 
25868,forest disturbance regimes are changing around the globe of particular concern are biotic disturbance agents as they respond strongly to climate warming and invade new ecosystems as alien pests and pathogens to date biotic disturbances are either ignored in simulations of vegetation dynamics or only a small number of common agents are considered explicitly here we present bite a general process based approach to simulate biotic forest disturbance agents from fungi to large mammals bite considers the processes of agent introduction dispersal colonization population dynamics and vegetation impact explicitly here we parameterize the model for six widely different biotic disturbance agents heterobasidion annosum hymenoscyphus fraxineus lymanthia dispar anoplophora glabripennis capreolus capreolus mammut americanum and evaluate it using pattern oriented modeling bite enables the inclusion of both established and novel biotic disturbance agents in vegetation models and is a step towards the comprehensive simulation of forest disturbance regimes in a changing world keywords biotic disturbance pests pathogens invasive alien species mechanistic modeling 1 introduction natural disturbances i e discrete events that disrupt the structure of an ecosystem shape forests around the world the disturbance regime of a given area is characterized by a typical frequency size and severity of disturbance turner 2010 a key determinant of disturbance regimes are the prevailing disturbance agents i e the factors causing disturbance as they strongly influence the spatial patterns and climate sensitivity of disturbances seidl et al 2020 ecosystems are generally well adapted to the prevailing disturbance regime yet disturbances are changing as a result of climate change for example the frequency of wildfires schoennagel et al 2017 and the severity of insect outbreaks raffa et al 2008 is increasing in many parts of the world these changes are of concern as the ecosystem services forests provide to society are predominately negatively affected by disturbances thom and seidl 2016 quantifying and managing the impacts caused by current and potential future disturbance regimes is thus increasingly important investigations of the effects of natural disturbances often focus on severe and abrupt abiotic events such as large wildfires flood events or windstorms however the impacts of biotic disturbances i e those caused by a variety of organisms from fungi to insects and herbivorous mammals are rivaling the impacts of their abiotic counterparts healey et al 2016 kautz et al 2017 furthermore a warming climate is predicted to benefit many biotic agents significantly bentz et al 2019 la porta et al 2008 compared to abiotic disturbances our understanding of biotic disturbance agents remains limited not least because of the complex biology of the various life forms that can cause disturbances furthermore biotic disturbances are often modulated by complex interactions with other abiotic and biotic disturbance agents seidl et al 2017 which makes their analysis and quantification challenging due to the high level of complexity involved simulation modeling is a particularly important tool for understanding biotic disturbances global change is considerably altering biotic disturbance regimes many biotic disturbance agents are for instance shifting polewards bebber et al 2013 and species that are benign in their native range can become impactful pests in new systems desprez loustau et al 2007 økland et al 2019 in addition new biotic disturbance agents are emerging as global trade accelerates the introduction of non native pests and pathogens chapman et al 2017 santini et al 2013 seebens et al 2017 climate change sometimes benefits the establishment and spread of non native species exacerbating the issue of non native pests and pathogens further seidl et al 2018b walther et al 2009 invasive pests and pathogens can cause dramatic changes in their new environment by diminishing their host population or even driving it into extinction lovett et al 2006 mack et al 2000 wardle et al 2001 in addition they can interact with native disturbance agents gonthier et al 2007 or have indirect effects on human health donovan et al 2013 a common characteristic of all these novel biotic disturbance agents is that the available information on them is limited this means that strongly data driven approaches e g empirical models machine learning see rammer and seidl 2019 are often not feasible for modeling novel biotic disturbance agents and rather simple process based or theoretical models are needed to provide timely model based inference for management the potential negative impacts of biotic disturbance agents can be mediated through a range of potential management options these extend from aiming to eradicate a newly introduced agent liebhold and bascompte 2003 to changing the host population structure and configuration on the landscape honkaniemi et al 2020 and applying chemical or biological control to decrease the abundance of the agent holmes and macquarrie 2016 furthermore an efficient monitoring of biotic agents is a key element of management especially in the case of emerging pests and pathogens as the timing and spatial focus of countermeasures can critically influence management success cunniffe et al 2016 simberloff 2003 simberloff et al 2005 simulation models have proven to be valuable tools for informing management on where when and which measures to apply in order to optimally contain biotic disturbances computer simulations of ecosystem dynamics have developed rapidly in recent years seidl 2017 the increase in computational capacity for instance has enabled a shift from modeling forest stands to focusing on their larger spatial context at the landscape scale shifley et al 2017 this development is important especially in the context of simulating biotic disturbances as they can spread from stand to stand occur across relatively large extents and are driven by factors across multiple spatial scales cushman and meentemeyer 2008 seidl et al 2016 nonetheless most studies simulating the potential risk from pests and pathogens have to date focused on the potential for introduction and spread of an agent de la fuente et al 2018 ferrari et al 2014 with considerably less attention on simulating the potential impacts of biotic agents seidl et al 2018b process based models of biotic disturbances operating at different scales cushman and meentemeyer 2008 and coupled with dynamic vegetation models cunniffe et al 2015 remain still rare to date one reason is that process based models often require detailed information on agent biology that often is not available simplified models such as sir models borrowed from epidemiology kermack and mckendrick 1927 have been successfully applied to pests and pathogens in agricultural crops whish et al 2015 and livestock keeling 2005 however generally applicable biotic disturbance models in forest ecosystems remain scarce to date but see kriticos et al 2013 lustig et al 2017 sturtevant et al 2004 tonini et al 2018 wildemeersch et al 2019 here we present bite the biotic disturbance engine a general model to simulate biotic disturbances in forest ecosystems our objective was to develop a modeling framework that is general enough to simulate a wide range of biotic disturbance agents from fungi to insects and large mammals further objectives were to keep the framework simple and modular in order to also be applicable in situations where knowledge about an agent is limited as is the case with new invaders coupled with a forest landscape simulation model bite allows the quantification of the impacts of emerging pests and pathogens on forests in time and space here we present the bite modeling framework and illustrate its generality by applying it to six widely different biotic disturbance agents two pathogens an ascomycete and a basidiomycete two insects a defoliator and a bark beetle and two mammals browsers of different size and life history strategy we analyzed the different patterns produced by bite for this wide variety of agents following the principles of pattern oriented modeling grimm et al 2005 2 materials and methods 2 1 model overview bite is a general model to simulate the dynamics of biotic disturbance agents with a special focus on being able to accommodate also emerging pests and pathogens bite was designed to be easily adaptable to different agents and conditions and allows the flexible assimilation of emerging knowledge on agent dynamics due to its modular structure the key design elements of our modeling approach are 1 agents are simulated on a regular grid with an agent specific resolution between 10 m 1000 m grid cells while agent specific variables are homogeneous within a grid cell vegetation i e availability of host trees and environment e g climatic indicators can vary within a grid cell 2 biomass is used as a common currency for simulating the abundance of agents in a grid cell but also for simulating the impact of biotic disturbance agents on forest vegetation an individual based modeling approach railsback and grimm 2019 was considered but was not deemed suitable for the simulation of very small organism such as fungal spores across large spatial extents 3 the movement of individual agents in a given time step is approximated by probabilistic dispersal kernels 4 agent population dynamics within a grid cell is modeled via empirical growth equations the growth rate of the population is limited by a carrying capacity determined by the host availability within a cell 5 the impact of biotic disturbance agents on vegetation can be generalized into the consumption of foliage biomass the consumption of root biomass or tree mortality e g by disrupting physiological processes such as phloem conductivity an important design strategy for achieving general applicability across a wide range of biotic disturbance agents is modularity bite models biotic disturbance agents in six distinct modules i potential habitat ii introduction iii dispersal iv colonization v population dynamics and vi impact fig 1 each module represents an important aspect of agent biology and provides specific options in the parameterization of a specific agent e g different dispersal kernel functions to accurately characterize agent behavior the level of detail implemented in each module can vary from simple to complex and computations in each module can potentially use state variables such as agent biomass in the previous time step vegetation structure and composition and environmental conditions modules can also be bypassed for selected agents if they are not applicable or if not enough information for their parametrization is available bite is a general framework that can be coupled with a wide variety of dynamic vegetation models conceptually any model that provides gridded host biomass for the agents to consume could be coupled with bite however specific implementations of the bite framework can be tailored to a model s representation of vegetation and environment such as the specific rendering of stand structure tree dimensions and available climate variables in a given model in the current implementation the framework is coupled with the individual based forest landscape and disturbance model iland seidl et al 2012a thom et al 2017 consequently we here focus on the dynamics of biotic disturbance agents in forest ecosystems at the landscape scale i e an extent of 103 105 ha ha iland is a process based model simulating forest landscape dynamics at the spatial grain of individual trees see http iland boku ac at for a technical model documentation disturbances can have long term impacts on forest structure and composition johnstone et al 2016 with regeneration and growth processes being key determinants of ecosystem resilience and the persistence of disturbance impacts in iland these processes are simulated in detail based on first principles of ecology e g spatio temporal dispersal in combination with local light availability and environmental filters determines the establishment of new trees after a disturbance a radiation use efficiency approach landsberg and waring 1997 is used to calculate gross primary production from daily weather data and local water and nutrient availability the assimilated carbon is subsequently allocated to different tree compartments based on species specific allometric ratios between compartments in bite these compartments are consumed by the simulated agents leading to single tree mortality which is related to carbon starvation thus mortality increases if stressors such as the biotic agents simulated in bite lead to the depletion of a trees carbohydrate reserves in addition an age and size related background mortality rate is calculated the spatial grain of simulations in iland varies with local light environment being simulated on 2 m grid cells while heterogeneity in climate and soil is considered at the resolution of 100 m iland was successfully evaluated and applied in central europe silva pedro et al 2015 thom et al 2018 and the western us braziunas et al 2018 seidl et al 2012b in the current implementation of bite relevant elements of vegetation e g host tree biomass and climate e g growing degree days are provided dynamically from the iland simulation environment in turn vegetation impacts from bite disturbance agent activity e g trees killed biomass consumed are dynamically fed back into iland where they change the simulated vegetation state bite is thus fully integrated within iland with dynamic feedbacks between vegetation and biotic disturbance agents 2 2 detailed model description 2 2 1 technical implementation besides general applicability across a wide range of biotic disturbance agents important design principles of bite were flexibility in the application in order to allow tailoring the model to emerging research questions and computational efficiency to accommodate a potential large number of simulations in scenario analyses therefore agents in bite are defined in javascript granting ease of use and flexibility and executed with an engine written in c ensuring its computational efficiency the agent code is mostly declarative i e describing the what rather than the how but can be augmented by imperative code for more complex agent behavior see supplementary material s1 and javascript files for the agents in figshare multiple agents can be active within a single simulation run and events such as agent introductions or management interventions can be triggered at any time during a simulation technical details and example code are provided in supplementary material s1 full source code and documentation is available under a gnu general public license gnu gpl www gnu org licenses gpl 3 0 html from http iland boku ac at bite the implementation of bite is generally model agnostic and provides a clear technical interface between bite and the vegetation model coupling bite with a new vegetation model is fairly straightforward via this interface however since the structure of vegetation models differ from each other e g represented biomass pools spatial resolution different environmental drivers coupling a new model might also require an adaptation of bite processes and functions the following description is based on the current implementation of bite within the iland landscape and disturbance modeling framework 2 2 2 agent life history important information on the life history of the simulated agent s need to be provided to the model these include information on voltinism i e univoltine bivoltine multivoltine the frequency of dispersal number of dispersal sequences per timestep and generation and prerequisites for initiating dispersal after colonization i e possible delay due to inoculum accumulation in addition in case of cyclic agents e g insect or vole outbreaks the outbreak duration and time between outbreaks are specified 2 2 3 potential habitat this module determines the general habitat suitability of an agent for the simulated landscape it uses binary raster files of long term climate suitability e g derived from species distribution models land use e g deer exclosures or other environmental filters of relevance e g elevation water bodies all these filters are not calculated dynamically in bite but are provided as external input to the model simulation essentially the provided rasters identify the potential spatial domain of simulation for each agent within the landscape please note that filters can change over time e g when habitat suitability changes in response to climate change 2 2 4 introduction this module simulates the introduction of a new biotic disturbance agent into the simulated landscape in bite an agent can be introduced at time t in n cells of the landscape either randomly or in predefined locations grids with introduction probability can also be provided e g increased probability of introduction closer to roads or human settlements 2 2 5 dispersal the dispersal module is responsible for simulating the movement of the agent across the landscape dispersal is simulated in a probabilistic way in bite calculating the spatiotemporal establishment probability of an agent at a new location a species specific dispersal kernel indicating the probability that a cell is the target of dispersal from a focal cell as determined by the distance from the focal cell and a maximum dispersal distance determine agent movement in space and time bite first calculates dispersal individually for all cells where the agent is present and subsequently combines dispersal probabilities for cells that can be reached from multiple source cells of the agent population 2 2 6 colonization the colonization module calculates whether an agent is actually able to colonize a previously uninhabited cell successful colonization is contingent on several conditions first the agent must be able to successfully disperse into the cell dispersal success is calculated from the dispersal probability derived in the dispersal module by either invoking a fixed threshold or by drawing a random number user parameter in addition the successful colonization of a cell depends on the vegetation of a cell e g the presence of host trees of the agent furthermore environmental conditions can limit colonization of a cell e g the need to exceed certain temperature sums indicating the thermal ability to complete a full development cycle for insects both vegetation and environmental conditions are implemented as boolean filters i e colonization is successful only when all conditions are met successfully colonized cells are simulated to have an initial agent biomass which serves as the basis for further calculations of population dynamics user parameter 2 2 7 population dynamics once an agent has successfully colonized a cell its biomass in the cell bm a can grow with growth being limited by the relevant host biomass e g canopy biomass root biomass depending on the compartment that is affected by the agent that is available at a cell as well as environmental conditions e g temperature soil moisture population growth is simulated with a growth equation that can be provided by the user for each specific case by default a logistic growth equation is used incorporating a relative maximum growth rate r the agent biomass m in the previous time step and the carrying capacity of a cell k see supplementary material s1 eq s2 the carrying capacity is calculated as a function of the targeted host biomass in the cell and the potential annual biomass consumption per biomass unit of the agent the output of the population dynamics module is the agent biomass bm a in each cell at timestep t although the default function for population dynamics in bite is a simple logistic growth equation any other type of function e g second order equations lotka volterra to describe population dynamics could be used the use of more complex growth equations depends on improved data availability for parametrization but equations can also be used experimentally to test the effect of different assumptions of agent growth on simulated disturbance dynamics 2 2 8 impact the impact module calculates the effect of biotic agent activity on vegetation and provides a feedback to the simulated vegetation dynamics in the cell in bite impact is calculated as the host biomass loss due to agent activity this information is subsequently fed back to the vegetation simulator where impacts alter the simulated vegetation structure and composition impacts in bite either affect a specified host compartment with a maximum annual consumption rate per unit agent biomass or an entire tree in the case of direct mortality in iland the consumption of different tree compartments foliage or roots increases the probability of mortality as it changes the physiological abilities of a tree increases maintenance respiration and can lead to carbon starvation possible impact types for trees 4 m in height are consumption of foliage or root biomass and tree mortality for saplings height 4 m the simulated impacts are either tree mortality or consumption of the leader shoot by browsing impacts can be stratified by tree dimension and agent preferences can be considered e g shortest trees are targeted first for example an agent can be parameterized to affect only trees 15 m in height or have varying impact rates between diameter classes agents can also impact different tree compartments at the same time in a simplified implementation of bite where agent biomass dynamics are not simulated explicitly due to data limitations the impact of an agent can be simulated phenomenologically by specifying the share of trees affected per cell or the share of compartment consumed in affected trees trees in iland also die from other causes of mortality such as self thinning harvesting and abiotic stress these causes of mortality are computed independently from biotic disturbance mortality but do interact with each other indirectly as they all modify forest structure and composition which in turn influences mortality 2 3 model evaluation 2 3 1 biotic agents to demonstrate the utility and versatility of bite we parameterized and tested it for six widely different biotic disturbance agents i e heterobasidion root rot heterobasidion annosum fr bref european gypsy moth lymantria dispar l roe deer capreolus capreolus l ash dieback hymenoscyphus fraxineus baral et al 2014 asian long horned beetle anolophora glabripennis motchulsky and mastodon mammut americanum kerr table 1 see figshare folder for biteagent javascript codes the agents were chosen to cover a wide variety of biota from fungi to insects and large mammals differing strongly in their life history and impact on tree vegetation in addition the agents represent a wide variety of information available from organisms that have been studied in depth heterobasidion root rot gypsy moth roe deer to those for which less information is available ash dieback asian long horned beetle mastodon in order to test the robustness of the model also in situations where information is limited such as is often the case with a newly invading pest species all species were parameterized based on available information from the literature we also want to highlight that while some of the simulated example agents are invasive alien species we do not aim to capture invasion dynamics with bite and iland as this would require a much larger spatial scope for the simulations we here explicitly focus on the dynamics of biotic agents and their impacts on vegetation in the landscape scale heterobasidion annosum is one of the most destructive forest pathogens in the northern hemisphere causing root rot specifically on conifers garbelotto and gonthier 2013 different species of the group are well established in many regions of eurasia and north america but it is currently also spreading into new areas e g bérubé et al 2018 heterobasidion sp spread via spore infections through fresh stumps e g created by tree harvesting within the stand the fungus spreads vegetatively via mycelia garbelotto and gonthier 2013 while heterobasidion sp spreads only over short distances kallio 1970 möykkynen et al 1997 it can endure at a site for several tree generations stenlid and redfern 1998 99 5 of the spores land within a few hundred meters with the remaining 0 5 being responsible for the long distance dispersal of the pathogen kallio 1970 in bite dispersal was simulated with a power law function kallio 1970 möykkynen et al 1997 and the colonization was restricted to cells were fresh stump surfaces were available for spore germination rishbeth 1951 population dynamics were simulated based on a logistic growth model with consumption and growth rate parameters derived from honkaniemi et al 2017b the impact on infested trees is simulated as a reduction in root biomass european gypsy moth hereafter referred to as gypsy moth is a defoliator native to europe where it causes substantial disturbance especially on oaks mcmanus and csóka 2007 in 1869 it was also introduced to north america where it became an invasive pest seriously threatening oak forests in the northeastern usa elkinton and liebhold 1990 adult gypsy moths are poor dispersers but the first instar larvae spread passively over short distances via wind hunter and elkinton 2000 however human aided long distance dispersal is driving the invasion in north america liebhold et al 1992 the development of a gypsy moth from egg to adult takes one season and during that development each larvae consume about 3 4 g of foliage sharov and colbert 1996 outbreaks of gypsy moth typically last for several years and occur both in europe and north america in 8 12 year intervals johnson et al 2005 gypsy moth dispersal was approximated with a gaussian dispersal kernel in bite elderd et al 2013 and its population dynamics was simulated with a logistic growth equation based on growth rates modified from lustig et al 2017 the simulated impact was consumption of foliage biomass with preference for small over large trees roe deer is a species of deer native to europe it is widespread throughout the whole continent from southern europe to the nordic countries and is expanding its range due to warming climate and changes in land use danilov et al 2017 valente et al 2014 roe deer is a relatively small deer species with an average body mass of 20 30 kg andersen et al 1998 pettorelli et al 2002 they are territorial animals and their habitat can range from agricultural landscapes to woodlands tixier and duncan 1996 roe deer graze fresh grass but a significant part of the diet consists also of sapling shoots tixier and duncan 1996 silver fir abies alba is one of the most favored tree species for browsing in many areas browsing rates on the species are substantial and can even lead to regeneration failure of silver fir senn and suter 2003 in bite we assumed roe deer to populate the entire landscape with a constant density of 14 individuals per 100 ha senn and suter 2003 the consumption was derived by combining daily diet preferences and consumption rates reported from different environments drozdz and osiecki 1973 tixier and duncan 1996 the simulated impact was a loss of the leader shoot and thus a loss of current year height growth for saplings with a height of 1 3 m hymenoscyphus fraxineus is the causal agent of ash dieback a non native disease that has affected europe s forests over the past three decades kowalski and holdenrieder 2009 mckinney et al 2014 and is currently threatening ash primarily faxinus excelsior l populations all over the continent pautasso et al 2013 environmental factors such as soil moisture and temperature as well as stand variables like stand age and density have been linked to the epidemiology of the fungus skovsgaard et al 2017 in bite its dispersal was simulated with an inverse power law dispersal kernel grosdidier et al 2018 as recent results show a decrease of the disease with decreasing host density bakys et al 2013 we assumed that only cells with a host tree density of 100 stems ha 1 over more than 3 years were susceptible to ash dieback as we could not gather enough information to build a reliable agent population dynamics module we omitted this aspect in simulations we assumed that if the pathogen is present it causes heavy defoliation 50 100 of foliage mass removed for a maximum of 30 of the host trees of a cell timmermann et al 2017 in addition we assumed that 1 of the trees are resistant to the disease kjaer et al 2012 wohlmuth et al 2018 asian long horned beetle alb is an insect species native to china and korea attacking the stems of multiple deciduous tree species its larvae consume the wood which can eventually lead to tree mortality haack et al 1997 hérard et al 2006 global trade has resulted in the introduction of alb to many areas outside its native range eyre and haack 2017 as the species effectively disperses in wood packaging material alb is a moderate disperser and we here used a general leptokurtic dispersal kernel shatz et al 2016 we assumed that to colonize a cell the presence of a host with dbh 7 5 cm was needed dodds and orwig 2011 even though the life cycle of the agent is generally well known haack et al 2009 we didn t find enough reliable quantitative information to parameterize the detailed agent population dynamics module of bite there are several different estimates of the potential impact of alb with reported mortality rates varying from 3 to 30 faccoli and gatto 2016 nowak et al 2001 impact data often stem from poplar plantations in the native habitat of the beetle in china see hu et al 2009 and the references therein however quick eradication measures at infested sites interfere with the quantification of the true impacts alb could cause on host populations dodds and orwig 2011 studied the only large scale infestation outside the native range in a non urban environment in massachusetts usa and found that tree mortality and growth losses were extremely rare even more than 5 years after an infestation we here assumed a linearly increasing mortality rate from 0 to 2 over a 10 year period to simulate the protracted mortality caused by alb mastodons were large mammals distantly related to elephants inhabiting the forests of north america and eurasia until their extinction 10 11 000 years ago compared to mammoths mammuthus sp which were grazers mastodons were forest dwelling browsers with picea spp forming a significant part of their diet birks et al 2018 teale and miller 2012 their estimated body mass was 8000 kg mastodons were thus slightly heavier than modern elephants although their shoulder height was roughly comparable larramendi 2015 we assumed mastodons to inhabit the whole test landscape with an initial density of 1 5 individuals per 100 ha corresponding to the estimated densities of pleistocene megaherbivores 120 kg ha 1 bakker et al 2016 mastodon population growth rate was assumed to be 1 yr 1 using a logistic growth model we assumed that mastodons were able to browse trees up to 4 m height with a preference for trees between 0 and 2 m guy 1976 and the occasional uprooting of trees similar to modern elephants scheiter and higgins 2012 shannon et al 2008 the diet was assumed to consist of 20 norway spruce picea abies l karst 2 3 2 simulation design to demonstrate the model s utility and evaluate the patterns emerging from simulations we simulated the dynamics of each agent separately in a generic landscape with tree species and climate typical for the temperate biome we aimed for maximum comparability between agents of different traits and thus eliminated potential confounding factors such as variability in climate and host tree distribution in our simulations specifically we simulated a rectangular 5 5 km forest area solely populated by the main host species of each agent table 1 initial stand age was uniformly distributed on the landscape between 0 and 100 years stands were created as voronoi polygons with an average stand size of 1 ha tree dimensions e g tree diameter tree height stand density were derived from yield tables marschall 1975 the environment i e soil climate was assumed to be uniform in space but climate varied over time around a stationary mean temperature of 8 6 c and annual precipitation of 810 mm the climate data were derived from a typical temperate forest landscape in central europe see honkaniemi et al 2020 by taking the landscape mean and variation of a time series from 1981 to 2010 in the simulations including management see below stands were clearcut at a stand age of 100 years and replanted with the host tree species natural regeneration as well as abiotic disturbances e g wind wildfire were omitted 2 3 3 analyses we analyzed model behavior for the six agents in a pattern oriented framework grimm et al 2005 grimm and railsback 2012 specifically we focused on three patterns produced by bite 1 impact on host mortality or browsing rate 2 spatial patterns of agent spread from a single point of introduction and 3 temporal patterns of agent dynamics over time impact rates pattern 1 were simulated assuming that the agent is present in each cell of the landscape and thus controls for differences in colonization times analyzed separately in pattern 2 the analyzed impact solely relates to the direct effects of the focal agent and excludes other causes of mortality such as density dependent mortality or age related mortality simulated impact rates were compared against independent data sources not used for model parameterization table 2 supplementary material s3 except for asian long horned beetle and mastodon where no field data were available the simulations were run for 50 years simulations of pattern 1 excluded forest management to avoid confounding effects between simulated management and disturbance in addition to mean impact rates we also analyzed structural effects of tree mortality and compared the diameter distribution of trees killed by a focal agent to background mortality from competition and age related causes we also carried out a sensitivity analysis for simulating biotic disturbance impact on varying tree sizes see supplementary material s2 spatial patterns of biotic disturbance pattern 2 were simulated in the same agent specific landscapes but agents were introduced in a single cell in the center of the landscape and their spread simulated over a 50 year period the two browsing mammals roe deer and mastodon were excluded from this analysis as they were assumed to be present throughout the landscape analysis variables were the infestation pattern emerging from the landscape as well as the impact on the host species no field data were available for direct comparison but the emerging infestation patterns were assessed qualitatively against expected patterns see table 2 forest management was simulated in all model runs for testing pattern 2 in order to maintain a comparable landscape state in areas not yet infested by the agent agent dynamics over time pattern 3 was studied over 50 years for the four agents for which population dynamics was explicitly simulated i e root rot gypsy moth roe deer and mastodon root rot and gypsy moth were introduced in the center cell of the landscape as described for pattern 2 above roe deer and mastodon were assumed to be present in each simulated cell of the landscape the output was normalized to the number of cells colonized in each time step to facilitate comparison across agents the development of agent populations and their impact on host tree vegetation were compared against independent observations from the literature table 2 3 results 3 1 impact pattern 1 simulated annual mortality rates corresponded well with independent observations for well described agents i e root rot and gypsy moth fig 2 although bite on average underestimated the impact of these agents slightly browsing rates for roe deer were also well in line with expectations though slightly higher than those reported in the literature bite simulations overestimated the annual mortality from ash dieback although the variation on the landscape was high asian long horned beetle and mastodon presented the extreme ends of the range in terms of impact on vegetation which was consistent with the model internal logic and parameterization field data for evaluation were unfortunately not available for these two species tree mortality caused by gypsy moth and ash dieback were higher than background mortality i e mortality caused by other reasons than the agent such as competition or age related causes and amounted to 2 5 and 5 3 of mortality per year respectively compared to 0 6 and 1 3 of background mortality respectively conversely background mortality was higher than mortality caused by root rot and alb in our simulations agent induced mortality 0 3 and 0 5 respectively compared to background mortality of 1 8 and 1 5 respectively the diameter distributions of trees affected by biotic agents reflected the different size preferences of agents well see supplementary material s2 3 2 spatial patterns of agent spread pattern 2 spatial patterns of spread varied widely between the simulated agents after 50 simulation years ash dieback was active in 100 of the landscape alb in 25 6 gypsy moth in 1 7 and heterobasidion root rot in 0 3 in line with expectations ash dieback spread throughout the landscape table 2 fig 3 a and caused mortality over the entire simulation period but affected trees in a scattered pattern fig 3b asian long horned beetle behaved similarly to ash dieback by spreading throughout the landscape 100 the relatively low mortality rate kept the infestation going throughout the entire simulation period as host trees remained available for infestation see supplementary material s2 for a management related eradication experiment the impact patterns of alb were generally patchier with synchronous mortality occurring in larger areas fig 3b gypsy moth only dispersed to a small area 1 7 of the landscape over 50 years of simulation and cells were frequently infested only once fig 3a we note that human aided long distance dispersal of gypsy moth was not considered here due to the periodicity of outbreaks gypsy moth impact occurred synchronous in small patches fig 3b heterobasidion was a poor disperser being only able to colonize 0 3 of the landscape in our simulation it s pronounced long distance dispersal in combination with the establishment requirement of fresh stumps resulted in an almost random pattern of infested cells on the landscape fig 3a the resulting impact pattern of small mortality centers in stands corresponds well with expectations from the literature table 2 fig 3b 3 3 temporal patterns of agent dynamics pattern 3 the four agents for which population dynamics was explicitly simulated showed widely varying development over time the relatively fast growth rate of heterobasidion root rot together with a high carrying capacity as determined by the root biomass of both fresh stumps and live trees enabled the agent biomass to increase to high levels fig 4 a because root rot is introduced via stumps and only after a considerable lag colonizes live trees tree mortality lagged behind infestation and increased only towards the end of the simulation period fig 4b gypsy moth outbreaks followed the periodicity as specified in the species parameters with population biomass fluctuating strongly over time fig 4a the overall biomass affected increased over time with the expanding colonized area host mortality due to defoliation reached its peak 1 3 years after the gypsy moth population peak which is well in line with independent observations table 2 fig 4b roe deer consumed its host at a steady rate over time in contrast mastodon populations consumed high levels of host biomass in a short period of time with negative feedbacks on habitat suitability and agent population levels this indicates that they would have had to either shift their diet to other plants or migrate to another landscape also suggesting that megaherbivore levels as in the pleistocene could not be sustained in modern landscapes 4 discussion and conclusions biotic disturbances are an integral part of forest ecosystems and are particularly sensitive to climate change seidl et al 2017 turner 2010 in the past the impacts of biotic disturbances have been modeled using predominately statistical approaches correlating disturbance presence abundance and impact seidl et al 2011 however the no analog conditions expected for the future steffen et al 2018 require alternative approaches as it is unlikely that future ecosystem dynamics can be faithfully predicted from the past gustafson 2013 data driven theoretical models e g based on approaches developed in epidemiology are a possible alternative to traditional empirical models e g jeger et al 2007 wildemeersch et al 2019 also novel machine learning approaches such as deep neural networks e g rammer and seidl 2019 reichstein et al 2019 are better able to deal with extrapolation than traditional correlative models yet both of these approaches thrive on the availability of large amounts of data and are thus of limited use in situations where information is scarce as is the case e g for the invasion of pests and pathogens into new areas process based models i e approaches focusing on the underlying mechanisms and modeling the system based on first principles of ecology are important tools in this regard they are also better suited to address unprecedented future changes e g climate change novel agent interactions in developing bite as a general framework that should be applicable to both established and novel biotic disturbance agents we have chosen a modular process oriented framework we here demonstrate the generality of our framework by simulating six widely different biotic disturbance agents ranging from fungi to large mammals a key question with all process based models is whether the relevant processes have been addressed and whether their interactions have been faithfully represented in the model pattern oriented modeling grimm et al 2005 grimm and railsback 2012 offers a consistent pathway for testing whether key patterns of the system are emerging from the simulation here we applied a pattern oriented approach assessing the behavior of the newly developed model and demonstrating that the model can handle a wide range of different types of agents we document that realistic patterns of disturbance impact as well as spatio temporal disturbance dynamics are emerging from simulations with bite biotic disturbances remain incompletely represented in existing dynamic vegetation models in fact most vegetation modeling approaches still ignore biotic disturbances completely huang et al 2020 if biotic disturbances are considered in the simulation of vegetation dynamics models usually focus on a small number of well known agents such as ips typographus in europe e g fahse and heurich 2011 honkaniemi et al 2018 seidl and rammer 2017 and dendroctonus ponderosae in north america e g bone and altaweel 2014 powell and bentz 2014 this practice is problematic because it can misrepresent the vulnerability of ecosystems to biotic disturbances overstating the susceptibility of host species of these prominent disturbance agents relative to other tree species this issue is particularly relevant when models are used to develop management strategies for reducing disturbance risk e g dobor et al 2020 seidl et al 2018a consequently an important goal of disturbance modeling has to be a broad and comprehensive representation of biotic disturbance agents and their interactions and the approach presented here is a step stone towards this goal see lustig et al 2017 sturtevant et al 2004 tonini et al 2018 for other broadly applicable approaches in particular design goals of bite were to provide useful approximations even under data scarcity and broad applicability for different types of agents e g by using biomass as a common currency for both agents and vegetation simulations with process based models are afflicted with several sources of uncertainty o neill and rust 1979 one important aspect is process uncertainty i e whether all relevant processes have been adequately represented in the model for example in the case of ash dieback and asian long horned beetle we did not have enough data to allow the in depth simulation of population dynamics instead we made phenomenological assumptions of agent development and impact nonetheless we could show that our parsimonious approach was well able to reproduce key patterns of biotic disturbance regimes even when individual processes had to be bypassed in the simulation due to data gaps see table 1 another important dimension of uncertainty is parameter uncertainty parameters in process based models are frequently derived from the literature and often hinge on a small number of studies furthermore non native species might behave different in their invasive range compared to their native range e g carnegie and pegg 2018 and information on their ecology collected in their native range might not necessarily be applicable in their invasive range future applications of bite should thus carefully scrutinize parameters and test the model against observations relevant for future study systems while we focused on disturbance impacts and spatio temporal patterns in our evaluation future work should also scrutinize the climate sensitivity of the simulated agents seidl et al 2020 more broadly more empirical and experimental research on biotic disturbance agents is needed to improve simulated forest disturbance regimes bite was deliberately designed to also work in conditions were information on biotic disturbance agents is limited as is often the case in the context of invasive alien pests and pathogens as a strong test for agents with limited data availability we included an extinct species the mastodon in the set of agents parameterized and evaluated here to our surprise it was easier to parameterize key processes of disturbance dynamics from the literature for the mastodon compared to species of high current relevance such as the asian long horned beetle the fact that our quantitative knowledge of charismatic species long extinct seems to be higher than that of species of high current management relevance e g dietary requirements of mastodons vs suitable host species for asian long horned beetle underlines the need for basic research on a broad range of biotic disturbance agents nonetheless including species with potentially high impact on forest ecosystems in models even when data is scarce is important to avoid the above mentioned biases in simulation studies and while simulating mastodon disturbance in a temperate forest ecosystem may at first sound like a highly hypothetical exercise the quantitative analysis of interactions between vegetation and past megaherbivores can shed light onto important questions of long term ecosystem development gill et al 2009 malhi et al 2016 and provide insights into how possible reintroductions of megaherbivores could shape forest ecosystems simulation models have for instance been used to show how abiotic disturbances have influenced critical transitions of ecosystems in the holocene e g henne et al 2015 which is something that could also be investigated for biotic disturbance agents in the future here we have demonstrated the utility of our new modeling framework for six widely different biotic disturbance agents and while we here have focused on agents individually a key strength of a general modeling framework like bite is that interactions between agents can be simulated interactions between disturbance agents are a key element of forest disturbance regimes buma 2015 honkaniemi et al 2018 seidl and rammer 2017 an attack by hymenoscyphus fraxineus for instance frequently results in infestations by armillaria sp which decreases the stability of a tree and greatly increases its mortality risk chandelier et al 2016 enderle et al 2013 similarly heterobasidion root rot decreases the rooting strength of trees and makes them considerably more susceptible to windthrow honkaniemi et al 2017a because such disturbance interactions are often amplifying they are a key element in the high sensitivity of disturbance regimes to climate change seidl et al 2017 seidl and rammer 2017 disturbance interactions should thus be explored in depth in the future in order to more comprehensively simulate past and future forest disturbance regimes to simulate forest disturbance regimes dynamically models of disturbance agents need to be coupled with models of vegetation dynamics huang et al 2020 seidl et al 2011 here bite was coupled with the individual based forest landscape and disturbance model iland to demonstrate the patterns emerging from an integrated simulation of disturbance agents and forest development however bite was designed with clear interfaces and a modular structure and thus could also be easily adopted to other vegetation modeling frameworks this is particularly relevant as the simulation of disturbances has been identified as an important weakness of current dynamic global vegetation models fisher et al 2018 huang et al 2020 mcdowell et al 2020 the approach presented here could provide important building blocks for an improved consideration of biotic disturbances in models used to inform policy makers around the globe changes in disturbance regimes and novel emerging biotic disturbance agents will have unprecedented impacts on forests and challenge our knowledge of how to address disturbances in management ayres and lombardero 2018 hobbs et al 2006 improved simulation approaches hold great potential to help address the challenge of changing forest disturbance regimes 5 software and data availability section software namebite biotic disturbance engine developerwerner rammer year first official release2021 hardware requirementspc system requirementswindows linux program languagec javascript program size 2 8 mb availabilityhttp iland boku ac at startpage licensegpl 3 0 form of repositoryjavascript files size of archive43 6 kb access form figshare figshare com articles software agent javascript files for bite model 13603439 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements rs and wr acknowledge support from the austrian science fund fwf through start grant y895 b25 author contributions rs initiated the study all authors jointly developed the idea and study design jh and wr developed the technical solutions wr implemented the model code jh conducted the simulations and analyzed the simulated data jh wrote the initial draft of the manuscript all authors contributed critically to revising the text and gave final approval for publication appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 104977 
25869,the piper diagram is the most widely used chart in groundwater hydrochemical studies to represent the chemical facies of a set of water samples so far most modifications of the original piper diagram over time show a common problem the piper chart is not suitable for representing big data sets when there are more than 40 or 50 analyses the symbols overlap and the data distribution becomes obscured to overcome this limitation the d piper diagram displays the spatial point density instead of individual points with this modification both visualization and interpretation improve as the number of points increases besides several representation methods to account for distributional characteristics of the data allow the user to unravel hidden hydrochemical structures the d piper diagram outperforms other point density based solutions particularly in the representation of low density areas furthermore the d piper implementation is coded in python and it is freely available keywords piper diagram big data set hydrochemistry software availability software name d piper developers miguel gonzález luis moreno héctor aguilera year first official release 2021 hardware requirements pc system requirements windows linux mac program language python program size 68 kb in three files availability https github com chesstor d piper license gpl 3 0 documentation readme in github repository 1 introduction in hydrogeology graphical representation tools of water analyses allow both interpreting hydrochemical processes and comparing samples from different sources most of these tools represent the concentrations of the major components of the water or their relationships the main graphs used for this purpose are the stiff 1951 piper 1944 durov 1948 chadha 1999 diagrams the collins 1923 bar diagrams and among the general use diagrams pie charts and x y scatterplots in practice the most widely used hydrochemical diagram is that proposed by piper 1944 and to a lesser extent the stiff and schoeller diagrams schoeller 1964 almost all of these diagrams were designed more than fifty years ago and although they are still very useful none were intended to represent hundreds or thousands of analyses on the same graph the idea of interpreting water analyses using triangular diagrams arose around the year 1942 when simultaneously hill 1940 and langelier and ludwig 1942 each arrived at the same method of graphic representation on their own later piper proposed his namesake diagram piper 1944 since then different proposals and variations of the initial diagram have appeared such as the aforementioned durov diagram durov 1948 the extended durov diagram burdon and mazloum 1958 taken up and improved by lloyd 1965 and the chadha diagram 1999 the usefulness of the piper diagram in the fields of geochemistry and hydrochemistry is not limited to the classification and identification of facies and the comparison and grouping of samples but it also allows analyzing water mixing processes and some geochemical reactions although the piper diagram was devised with a hydrochemical and hydrogeological approach applications of the original version or some variant can also be found in many other fields for example teng et al 2016 suggested using the piper diagram as a visualization tool for the design and optimization of chemical engineering processes such as the production of cinnamaldehyde ray and mukherjee 2008 represented a combination of major ions in rectangular coordinates reproducing similar patterns to those of the piper diagram easily implementable in excel shelton et al 2018 proposed a modification of the piper diagram using compositional data analysis however these diagrams are not yet widely used because either they do not provide complementary information to the original or their interpretation is too complex the piper diagram fig 1 shows major cations and anions of water analyses expressed in percentage of equivalents per million epm in three diagram panels shaped by a mesh of equal sized triangular cells two triangular and one rhomboidal cations ca2 mg2 na k and anions so4 2 co3 2 hco3 cl are represented in the triangular panels and then projected onto the central rhomboidal panel for cationic anionic facies identification fig 2 a shows a typical example of the use of the piper diagram in which the hydrochemical facies of three groups of water of different origins are compared in this example each group can be easily differentiated based on its anionic facies and yet the cationic facies are similar in the three groups fig 2b shows how the graph looks like when a large set of analyses is represented this graph evidences the main drawback of the piper diagram only general patterns such as the dominant calcium magnesium bicarbonate facies are observable however no other structures in the data can be detected as a result of individual samples overlap and it is even impossible to get an idea of the number of analyses represented on the other hand due to the notable reduction in the cost of analytical techniques and the ease of storing a large number of analyses in an accessible way it is now easy to access databases in which there are hundreds or even thousands of water analyses the representation of these analyses in the piper diagram is often meaningless since point overlap hides most of the samples fig 2b recently russionello and lautz 2020 have proposed a new implementation of the piper diagram pied piper in matlab that allows displaying big data sets using translucent points contours and heatmaps of data density and convex hulls for groupings while being an excellent tool to improve the visualization and interpretation of large data sets it has an important limitation there are no available representations based on the statistical distribution of the data incorporating information on the distributional properties of the data is essential to achieve suitable representations conolly and lake 2006 otherwise relevant information on the hydrochemical characteristics could be missed optimal displaying requires the selection of class intervals in density graphs to be carried out based on this information given the above mentioned limitations of the piper diagram the objective of this study is to design a modification of said diagram that allows representing any number of samples keeping the maximum amount of information about the structure of the original data similarly to russionello and lautz 2020 this objective is met by displaying point density inside each of the triangular panels and in the rhomboidal panel rather than representing each of the analyses independently moreover several displaying methods to account for distributional characteristics of the data are provided in the implementation 2 methods the new density d piper v 1 diagram is implemented in python 3 7 6 using the standard libraries numpy 1 17 4 harris et al 2020 pandas 1 1 3 mckinney 2010 and matplotlib 3 2 2 hunter 2007 the library jenkspy 0 2 0 used to calculate the interval cutoff values with the jenks method can be downloaded from the website https github com mthh jenkspy the data sets shown in this work come from the water database of the geological survey of spain http info igme es bdaguas comprising 32 732 samples synthetic samples were generated for the different hydrochemical facies present in the database to evaluate the performance of the d piper diagram a total of 14 facies were identified based on different combinations of 50 epm equivalents per million thresholds for cations and anions for example if a sample has more than 50 epm in ca2 and more than 50 epm in hco3 then it is classified as hydrochemical facies calcium bicarbonate synthetic minority over sampling technique smote was used to create new samples for each facies chawla et al 2002 the smote algorithm generates new data of each class by forming convex combinations of neighboring instances and sampling along the lines this way the hydrochemical facies is preserved in the new samples the amount of new data for each hydrochemical facies was chosen to approximately equal that of the dominant facies in the original data set which was calcium bicarbonate 10 291 samples the resulting extended data set comprised 143 327 samples 2 1 program description and available options the code performs the following operations 1 read the ascii file d piper v1 options txt that describes all the variables and options necessary to draw the output graphs with the desired format the format of the input data is described in the header of the source code d piper v1 py as well as in the installation guidelines d piper v1 how to pdf 2 read the input data txt chemical analyses must be expressed as mg l 1 3 transform the units from mgl 1 to meqv percentage of milliequivalents 4 draw a standard piper diagram with all the input points or a selected subset with optional transparency 5 user defined discretization of the rhomboidal panel and the cation and anion triangular panels of the piper chart in equally sized polygon cells bins sizes 6 based on the input data determine the number of points that fall in each of the cells defined in the previous step 7 calculate the number of samples that fall within the most populated and the least 1 populated cells this range is divided into a discrete number of intervals according to one of the representation methods described below 8 display point density in each of the cells using a color scale additionally the percentage of samples per interval is also shown note than in step 7 zero density cells i e cells that do not contain any data points are excluded from the analysis and they are plotted as blank cells in the d piper diagram there are two reasons for this i cells with no samples will be quite abundant in most piper diagrams and their inclusion in the representation intervals would hinder the detection of density patterns in the lower range ii blank zero density cells make it easy to discard areas of seldom hydrochemical characteristics for a particular data set different class interval methods can be used to achieve suitable graphical representations of the data it is important to have some understanding of the distribution of the data to select the best method for each data set conolly and lake 2006 therefore the program offers the user the possibility of plotting the histogram of point density in all the cells of the three piper diagram panels with different options number of bins custom range and log transformed data six standard classification methods for data representation are implemented where the user simply chooses the classification scheme and sets the number of classes a user defined interval the user specifies the breakpoints of the class intervals b equal interval this method is useful when the histogram has a rectangular shape i e uniform distributions the range of the data is divided by the number of desired classes k up to a maximum of eight classes and then class limits are calculated as follows l h l k eq 1 l 2 h l k l k 1 h l k where h and l are the highest and the lowest value in the data respectively c geometric interval this classification method is used for visualizing continuous data where distributions are skewed and show very pronounced rates of change conolly and lake 2006 the following equality is solved to calculate the common ratio of the geometric progression eq 2 h r k l eq 3 r l h 1 k where h and l are the maximum and the minimum number of points in any cell of the d piper diagram respectively r is the common ratio of the geometric progression and k is the number of classes specified by the user then the interval limits i i are defined according to eq 4 i i h r k i i 1 k d quantile this method is well suited to linearly distributed data it produces irregular intervals assuring an equal number of values in each class and minimizing the importance of the class boundaries dent 1999 the attribute values are then sorted in ascending order and the number of values in each class k calculated by dividing the total number of observations by the number of classes quantiles specified by the user beginning with the lowest value k values are included in the first class k values in the next class etc the function to compute quantiles has a parameter that specifies the interpolation method to use when the desired quantile lies between two data points e standard deviation this would be the preferred method if the data displays a normal or lognormal distribution conolly and lake 2006 dent 1999 this classification method shows how much the feature s attribute value varies from the mean it finds the mean value of the observations and then places class breaks above and below the mean at intervals of either 1 4 1 3 1 2 or 1 standard deviation until all the data values are contained within the classes f jenks natural breaks optimization jenks 1967 this data clustering algorithm optimizes the arrangement of a set of values into natural classes by seeking to minimize the average deviation from the class meanwhile maximizing the deviation from the means of the other groups the method is an iterative process to repeatedly test different breaks in the data set to determine which set of breaks has the smallest in class variance further details on the calculations can be found in dent 1999 the major disadvantage of this method is that it is difficult to replicate as small changes in the data can change the classification scheme all methods can be applied to the raw data or the log transformed data the log transformation applied is log10 point density 1 to avoid losing the information of cells with point density equal to 1 the source code d piper v1 py and installation guidelines d piper v1 how to pdf can be downloaded from the supplementary material furthermore all options and parameters are defined and explained as comments in the options file d piper v1 options txt and in the d piper v1 how to pdf file in addition the source code is widely commented 3 results and discussion fig 3 a f shows examples of the application of the d piper diagram with some of the main graphical options and the different methods to display point density the graphs allow visualizing the most abundant facies in the data set and their distribution the data structure appears clearly in areas where the standard piper diagram only depicted a masked surface fig 2b except for the user defined interval fig 3f all the d piper representations have the same number of intervals for comparison purposes fig 3a shows d piper diagram with equal intervals where it is difficult to discriminate patterns in the lower range of point density as the first two intervals contain almost 75 of the samples the standard deviation method shows the opposite problem 50 of the samples concentrate in the last interval due to the small mean and high deviation of point density fig 3b here it is recommendable to plot the histogram of the log transformed data to check whether it follows a lognormal distribution and then apply the standard deviation representation to the transformed data as will be discussed later in this example the geometric based representation offers much richer information on the hydrochemical facies fig 3c both patterns in the lower and the higher ranges of point density are distinguishable similar to the standard deviation method the quantiles representation shows poor resolution in the higher ranges of point density and finer resolution in the lower ranges fig 3d the jenks natural breaks optimization yields a diagram where the proportion of samples in each interval is more evenly distributed fig 3e the last plot is a d piper diagram with user defined intervals fig 3f these intervals have been selected based on the information provided by the other representations this way an enhanced d piper diagram showing all the different hydrochemical patterns present in the data set is produced the proposed diagram allows to precisely delimit the dominant facies and the areas with the highest concentration of points for example it is easy to see in fig 3f the existence of a high number of points with facies with more than 80 in hco3 however the cationic facies are more distributed between ca and mg probably because in the aquifers in which these waters have been taken predominates dolomite versus calcite all this detailed information remains hidden in the standard piper diagram fig 2b the code is highly efficient being process time to plot a 300 dpi standard piper diagram frequency histogram and six intervals geometric d piper of a 1901 sample data set of 4 97 s on a system with intel r core tm i5 4210u cpu 1 70 ghz x64 based processor with 8 gb ram this time is lowered to 2 64 s on a system with intel r core tm i7 10700hq cpu 3 80 ghz x64 based processor with 32 gb ram similar to the background color scheme implementation proposed by peeters 2014 the color scales of the d piper plots can be used for the spatial representation of the samples allowing for the interpretation of the main patterns and processes in the aquifer deduced from the piper plot the full potential of the d piper diagram can be assessed on the large data set with synthetic samples comprising 14 hydrochemicial facies fig 4 adding transparency to points in such a big data set is useless due to the huge overlap fig 4a the histogram of the distribution of point density shows positive skewness and pronounced rates of change fig 4b therefore a representation based on geometric intervals is a good choice fig 4c the data density in the lower ranges is easily differentiated from the most abundant areas of the dominant hydrochemical facies fig 4c the quantile representation shows poor resolution in low density cells but provides an enriched visualization of high density areas fig 4d thus a mixed version of the geometrical and quantile representations can be manually built where all the different hydrochemical structures are visible fig 4e the distribution of anionic facies is more homogeneous than that of cationic ones there are clusters of high point density showing differentiated bicarbonate and chloride hydrochemical facies and sulfate forms mixed facies with carbonate the structure of cationic species shows a clear predominance of calcium and calcium magnesium hydrochemical facies and a transition towards chloride and highly chloride facies the heatmap obtained with pied piper lautz and russionello 2020 with the same number of bins is shown in fig 4f it can be seen that it fails to represent hydrochemical patterns particularly in low density areas fig 4f this information is relevant to analyze the likelihood of facies occurrence and to detect mixing patterns or dynamic hydrochemical processes causing continuous facies evolution due to ionic displacement e g seawater intrusion in fig 5 only two hydrochemical facies from the extended data set are plotted calcium bicarbonate and magnesium bicarbonate these two facies are easily separable in the cation triangular panel but overlap in the anion triangular panel and in the rhomboidal panel fig 5a the d piper diagram with geometric intervals again shows good performance in the lower ranges but poor resolution to represent cells with high density of points fig 5c the d piper diagram with the jenks natural breaks provides a good representation of the overlapping features in the anion triangular panel and in the rhomboidal panel in the mid to high density areas fig 5d as the density of points is not log normally distributed fig 5b the d piper diagram based on the standard deviation of the log transformed data fails to represent patterns in high density areas fig 5e finally the pied piper heatmap does not offer a clear visualization of the two facies fig 5f moreover it points out the limitation of not excluding zero density cells from the color plot which makes it very difficult to identify low density areas the presented examples evidenced the main strength of the d piper implementation for large data sets which is the versatility to plot point density with different methods according to the distribution of the data this in turn leads to the identification of hidden patterns in the hydrochemical facies the d piper diagram was compared to the pied piper tool lautz and russionello 2020 which also displays point density using an extended data set with more than 140 000 samples the results show that d piper outperforms pied piper particularly in visualizing low density areas moreover d piper is an open source python implementation whereas pied piper is distributed as a matlab library nevertheless both methods can complement each other and can be used in combination given the additional functionalities of pied piper to display contours and lasso groups of points with convex hulls 4 conclusions hydrochemical databases contain an increasing amount of information sometimes in the order of hundreds or thousands of analyses the traditional piper diagram is unsuitable to display these big data sets the d piper code presented in this paper effectively displays point density in large sets of water analyses it makes it easy to visualize the structure of the hydrochemical facies by allowing the user to choose among a wide variety of representation methods depending on the data distribution this information would have remained hidden with the traditional representation the d piper implementation has shown to be more suitable to display extremely big data sets compared to another similar tool recently released the pied piper diagram particularly in the lower range of data density the combined use of both methods can boost hydrochemical interpretation of large data sets the idea of representing point densities rather than discrete points can be successfully applied to other similar charts such as durov s that have the same limitations as the piper diagram declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research has been funded by the cligro proyect of the spanish national plan for scientific and technical research cgl2016 77473 c3 1 r the national system of youth guarantee pej2018 002477 is co founded under the youth employment operational program with financial resources from yei and esf and the proyect igme hidrocambio ref 2616 appendix a supplementary data the following is the supplementary data to this article a compressed file is provided which contains the file structure and the files necessary to make the d piper diagrams file 1 d piper v1 7z compressed file it contains the files necessary to make the d piper diagrams and the file structure necessary for the correct operation of the code this compressed file contains the following files file 2 d piper v1 py python code to generate the d piper charts file to run file 3 tools py python code containing all the required functions it is treated as a module an executed from the d piper v1 py file file 4 d piper v1 options txt ascii file necessary to run the d piper v1 py code contains the different options for executing the program and help text with the meaning of each one file 5 d piper v1 how to pdf brief description of the steps to take to make a d piper chart also the data folder in the compressed file provides the input files with which figs 2 5 have been built multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 104979 
25869,the piper diagram is the most widely used chart in groundwater hydrochemical studies to represent the chemical facies of a set of water samples so far most modifications of the original piper diagram over time show a common problem the piper chart is not suitable for representing big data sets when there are more than 40 or 50 analyses the symbols overlap and the data distribution becomes obscured to overcome this limitation the d piper diagram displays the spatial point density instead of individual points with this modification both visualization and interpretation improve as the number of points increases besides several representation methods to account for distributional characteristics of the data allow the user to unravel hidden hydrochemical structures the d piper diagram outperforms other point density based solutions particularly in the representation of low density areas furthermore the d piper implementation is coded in python and it is freely available keywords piper diagram big data set hydrochemistry software availability software name d piper developers miguel gonzález luis moreno héctor aguilera year first official release 2021 hardware requirements pc system requirements windows linux mac program language python program size 68 kb in three files availability https github com chesstor d piper license gpl 3 0 documentation readme in github repository 1 introduction in hydrogeology graphical representation tools of water analyses allow both interpreting hydrochemical processes and comparing samples from different sources most of these tools represent the concentrations of the major components of the water or their relationships the main graphs used for this purpose are the stiff 1951 piper 1944 durov 1948 chadha 1999 diagrams the collins 1923 bar diagrams and among the general use diagrams pie charts and x y scatterplots in practice the most widely used hydrochemical diagram is that proposed by piper 1944 and to a lesser extent the stiff and schoeller diagrams schoeller 1964 almost all of these diagrams were designed more than fifty years ago and although they are still very useful none were intended to represent hundreds or thousands of analyses on the same graph the idea of interpreting water analyses using triangular diagrams arose around the year 1942 when simultaneously hill 1940 and langelier and ludwig 1942 each arrived at the same method of graphic representation on their own later piper proposed his namesake diagram piper 1944 since then different proposals and variations of the initial diagram have appeared such as the aforementioned durov diagram durov 1948 the extended durov diagram burdon and mazloum 1958 taken up and improved by lloyd 1965 and the chadha diagram 1999 the usefulness of the piper diagram in the fields of geochemistry and hydrochemistry is not limited to the classification and identification of facies and the comparison and grouping of samples but it also allows analyzing water mixing processes and some geochemical reactions although the piper diagram was devised with a hydrochemical and hydrogeological approach applications of the original version or some variant can also be found in many other fields for example teng et al 2016 suggested using the piper diagram as a visualization tool for the design and optimization of chemical engineering processes such as the production of cinnamaldehyde ray and mukherjee 2008 represented a combination of major ions in rectangular coordinates reproducing similar patterns to those of the piper diagram easily implementable in excel shelton et al 2018 proposed a modification of the piper diagram using compositional data analysis however these diagrams are not yet widely used because either they do not provide complementary information to the original or their interpretation is too complex the piper diagram fig 1 shows major cations and anions of water analyses expressed in percentage of equivalents per million epm in three diagram panels shaped by a mesh of equal sized triangular cells two triangular and one rhomboidal cations ca2 mg2 na k and anions so4 2 co3 2 hco3 cl are represented in the triangular panels and then projected onto the central rhomboidal panel for cationic anionic facies identification fig 2 a shows a typical example of the use of the piper diagram in which the hydrochemical facies of three groups of water of different origins are compared in this example each group can be easily differentiated based on its anionic facies and yet the cationic facies are similar in the three groups fig 2b shows how the graph looks like when a large set of analyses is represented this graph evidences the main drawback of the piper diagram only general patterns such as the dominant calcium magnesium bicarbonate facies are observable however no other structures in the data can be detected as a result of individual samples overlap and it is even impossible to get an idea of the number of analyses represented on the other hand due to the notable reduction in the cost of analytical techniques and the ease of storing a large number of analyses in an accessible way it is now easy to access databases in which there are hundreds or even thousands of water analyses the representation of these analyses in the piper diagram is often meaningless since point overlap hides most of the samples fig 2b recently russionello and lautz 2020 have proposed a new implementation of the piper diagram pied piper in matlab that allows displaying big data sets using translucent points contours and heatmaps of data density and convex hulls for groupings while being an excellent tool to improve the visualization and interpretation of large data sets it has an important limitation there are no available representations based on the statistical distribution of the data incorporating information on the distributional properties of the data is essential to achieve suitable representations conolly and lake 2006 otherwise relevant information on the hydrochemical characteristics could be missed optimal displaying requires the selection of class intervals in density graphs to be carried out based on this information given the above mentioned limitations of the piper diagram the objective of this study is to design a modification of said diagram that allows representing any number of samples keeping the maximum amount of information about the structure of the original data similarly to russionello and lautz 2020 this objective is met by displaying point density inside each of the triangular panels and in the rhomboidal panel rather than representing each of the analyses independently moreover several displaying methods to account for distributional characteristics of the data are provided in the implementation 2 methods the new density d piper v 1 diagram is implemented in python 3 7 6 using the standard libraries numpy 1 17 4 harris et al 2020 pandas 1 1 3 mckinney 2010 and matplotlib 3 2 2 hunter 2007 the library jenkspy 0 2 0 used to calculate the interval cutoff values with the jenks method can be downloaded from the website https github com mthh jenkspy the data sets shown in this work come from the water database of the geological survey of spain http info igme es bdaguas comprising 32 732 samples synthetic samples were generated for the different hydrochemical facies present in the database to evaluate the performance of the d piper diagram a total of 14 facies were identified based on different combinations of 50 epm equivalents per million thresholds for cations and anions for example if a sample has more than 50 epm in ca2 and more than 50 epm in hco3 then it is classified as hydrochemical facies calcium bicarbonate synthetic minority over sampling technique smote was used to create new samples for each facies chawla et al 2002 the smote algorithm generates new data of each class by forming convex combinations of neighboring instances and sampling along the lines this way the hydrochemical facies is preserved in the new samples the amount of new data for each hydrochemical facies was chosen to approximately equal that of the dominant facies in the original data set which was calcium bicarbonate 10 291 samples the resulting extended data set comprised 143 327 samples 2 1 program description and available options the code performs the following operations 1 read the ascii file d piper v1 options txt that describes all the variables and options necessary to draw the output graphs with the desired format the format of the input data is described in the header of the source code d piper v1 py as well as in the installation guidelines d piper v1 how to pdf 2 read the input data txt chemical analyses must be expressed as mg l 1 3 transform the units from mgl 1 to meqv percentage of milliequivalents 4 draw a standard piper diagram with all the input points or a selected subset with optional transparency 5 user defined discretization of the rhomboidal panel and the cation and anion triangular panels of the piper chart in equally sized polygon cells bins sizes 6 based on the input data determine the number of points that fall in each of the cells defined in the previous step 7 calculate the number of samples that fall within the most populated and the least 1 populated cells this range is divided into a discrete number of intervals according to one of the representation methods described below 8 display point density in each of the cells using a color scale additionally the percentage of samples per interval is also shown note than in step 7 zero density cells i e cells that do not contain any data points are excluded from the analysis and they are plotted as blank cells in the d piper diagram there are two reasons for this i cells with no samples will be quite abundant in most piper diagrams and their inclusion in the representation intervals would hinder the detection of density patterns in the lower range ii blank zero density cells make it easy to discard areas of seldom hydrochemical characteristics for a particular data set different class interval methods can be used to achieve suitable graphical representations of the data it is important to have some understanding of the distribution of the data to select the best method for each data set conolly and lake 2006 therefore the program offers the user the possibility of plotting the histogram of point density in all the cells of the three piper diagram panels with different options number of bins custom range and log transformed data six standard classification methods for data representation are implemented where the user simply chooses the classification scheme and sets the number of classes a user defined interval the user specifies the breakpoints of the class intervals b equal interval this method is useful when the histogram has a rectangular shape i e uniform distributions the range of the data is divided by the number of desired classes k up to a maximum of eight classes and then class limits are calculated as follows l h l k eq 1 l 2 h l k l k 1 h l k where h and l are the highest and the lowest value in the data respectively c geometric interval this classification method is used for visualizing continuous data where distributions are skewed and show very pronounced rates of change conolly and lake 2006 the following equality is solved to calculate the common ratio of the geometric progression eq 2 h r k l eq 3 r l h 1 k where h and l are the maximum and the minimum number of points in any cell of the d piper diagram respectively r is the common ratio of the geometric progression and k is the number of classes specified by the user then the interval limits i i are defined according to eq 4 i i h r k i i 1 k d quantile this method is well suited to linearly distributed data it produces irregular intervals assuring an equal number of values in each class and minimizing the importance of the class boundaries dent 1999 the attribute values are then sorted in ascending order and the number of values in each class k calculated by dividing the total number of observations by the number of classes quantiles specified by the user beginning with the lowest value k values are included in the first class k values in the next class etc the function to compute quantiles has a parameter that specifies the interpolation method to use when the desired quantile lies between two data points e standard deviation this would be the preferred method if the data displays a normal or lognormal distribution conolly and lake 2006 dent 1999 this classification method shows how much the feature s attribute value varies from the mean it finds the mean value of the observations and then places class breaks above and below the mean at intervals of either 1 4 1 3 1 2 or 1 standard deviation until all the data values are contained within the classes f jenks natural breaks optimization jenks 1967 this data clustering algorithm optimizes the arrangement of a set of values into natural classes by seeking to minimize the average deviation from the class meanwhile maximizing the deviation from the means of the other groups the method is an iterative process to repeatedly test different breaks in the data set to determine which set of breaks has the smallest in class variance further details on the calculations can be found in dent 1999 the major disadvantage of this method is that it is difficult to replicate as small changes in the data can change the classification scheme all methods can be applied to the raw data or the log transformed data the log transformation applied is log10 point density 1 to avoid losing the information of cells with point density equal to 1 the source code d piper v1 py and installation guidelines d piper v1 how to pdf can be downloaded from the supplementary material furthermore all options and parameters are defined and explained as comments in the options file d piper v1 options txt and in the d piper v1 how to pdf file in addition the source code is widely commented 3 results and discussion fig 3 a f shows examples of the application of the d piper diagram with some of the main graphical options and the different methods to display point density the graphs allow visualizing the most abundant facies in the data set and their distribution the data structure appears clearly in areas where the standard piper diagram only depicted a masked surface fig 2b except for the user defined interval fig 3f all the d piper representations have the same number of intervals for comparison purposes fig 3a shows d piper diagram with equal intervals where it is difficult to discriminate patterns in the lower range of point density as the first two intervals contain almost 75 of the samples the standard deviation method shows the opposite problem 50 of the samples concentrate in the last interval due to the small mean and high deviation of point density fig 3b here it is recommendable to plot the histogram of the log transformed data to check whether it follows a lognormal distribution and then apply the standard deviation representation to the transformed data as will be discussed later in this example the geometric based representation offers much richer information on the hydrochemical facies fig 3c both patterns in the lower and the higher ranges of point density are distinguishable similar to the standard deviation method the quantiles representation shows poor resolution in the higher ranges of point density and finer resolution in the lower ranges fig 3d the jenks natural breaks optimization yields a diagram where the proportion of samples in each interval is more evenly distributed fig 3e the last plot is a d piper diagram with user defined intervals fig 3f these intervals have been selected based on the information provided by the other representations this way an enhanced d piper diagram showing all the different hydrochemical patterns present in the data set is produced the proposed diagram allows to precisely delimit the dominant facies and the areas with the highest concentration of points for example it is easy to see in fig 3f the existence of a high number of points with facies with more than 80 in hco3 however the cationic facies are more distributed between ca and mg probably because in the aquifers in which these waters have been taken predominates dolomite versus calcite all this detailed information remains hidden in the standard piper diagram fig 2b the code is highly efficient being process time to plot a 300 dpi standard piper diagram frequency histogram and six intervals geometric d piper of a 1901 sample data set of 4 97 s on a system with intel r core tm i5 4210u cpu 1 70 ghz x64 based processor with 8 gb ram this time is lowered to 2 64 s on a system with intel r core tm i7 10700hq cpu 3 80 ghz x64 based processor with 32 gb ram similar to the background color scheme implementation proposed by peeters 2014 the color scales of the d piper plots can be used for the spatial representation of the samples allowing for the interpretation of the main patterns and processes in the aquifer deduced from the piper plot the full potential of the d piper diagram can be assessed on the large data set with synthetic samples comprising 14 hydrochemicial facies fig 4 adding transparency to points in such a big data set is useless due to the huge overlap fig 4a the histogram of the distribution of point density shows positive skewness and pronounced rates of change fig 4b therefore a representation based on geometric intervals is a good choice fig 4c the data density in the lower ranges is easily differentiated from the most abundant areas of the dominant hydrochemical facies fig 4c the quantile representation shows poor resolution in low density cells but provides an enriched visualization of high density areas fig 4d thus a mixed version of the geometrical and quantile representations can be manually built where all the different hydrochemical structures are visible fig 4e the distribution of anionic facies is more homogeneous than that of cationic ones there are clusters of high point density showing differentiated bicarbonate and chloride hydrochemical facies and sulfate forms mixed facies with carbonate the structure of cationic species shows a clear predominance of calcium and calcium magnesium hydrochemical facies and a transition towards chloride and highly chloride facies the heatmap obtained with pied piper lautz and russionello 2020 with the same number of bins is shown in fig 4f it can be seen that it fails to represent hydrochemical patterns particularly in low density areas fig 4f this information is relevant to analyze the likelihood of facies occurrence and to detect mixing patterns or dynamic hydrochemical processes causing continuous facies evolution due to ionic displacement e g seawater intrusion in fig 5 only two hydrochemical facies from the extended data set are plotted calcium bicarbonate and magnesium bicarbonate these two facies are easily separable in the cation triangular panel but overlap in the anion triangular panel and in the rhomboidal panel fig 5a the d piper diagram with geometric intervals again shows good performance in the lower ranges but poor resolution to represent cells with high density of points fig 5c the d piper diagram with the jenks natural breaks provides a good representation of the overlapping features in the anion triangular panel and in the rhomboidal panel in the mid to high density areas fig 5d as the density of points is not log normally distributed fig 5b the d piper diagram based on the standard deviation of the log transformed data fails to represent patterns in high density areas fig 5e finally the pied piper heatmap does not offer a clear visualization of the two facies fig 5f moreover it points out the limitation of not excluding zero density cells from the color plot which makes it very difficult to identify low density areas the presented examples evidenced the main strength of the d piper implementation for large data sets which is the versatility to plot point density with different methods according to the distribution of the data this in turn leads to the identification of hidden patterns in the hydrochemical facies the d piper diagram was compared to the pied piper tool lautz and russionello 2020 which also displays point density using an extended data set with more than 140 000 samples the results show that d piper outperforms pied piper particularly in visualizing low density areas moreover d piper is an open source python implementation whereas pied piper is distributed as a matlab library nevertheless both methods can complement each other and can be used in combination given the additional functionalities of pied piper to display contours and lasso groups of points with convex hulls 4 conclusions hydrochemical databases contain an increasing amount of information sometimes in the order of hundreds or thousands of analyses the traditional piper diagram is unsuitable to display these big data sets the d piper code presented in this paper effectively displays point density in large sets of water analyses it makes it easy to visualize the structure of the hydrochemical facies by allowing the user to choose among a wide variety of representation methods depending on the data distribution this information would have remained hidden with the traditional representation the d piper implementation has shown to be more suitable to display extremely big data sets compared to another similar tool recently released the pied piper diagram particularly in the lower range of data density the combined use of both methods can boost hydrochemical interpretation of large data sets the idea of representing point densities rather than discrete points can be successfully applied to other similar charts such as durov s that have the same limitations as the piper diagram declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research has been funded by the cligro proyect of the spanish national plan for scientific and technical research cgl2016 77473 c3 1 r the national system of youth guarantee pej2018 002477 is co founded under the youth employment operational program with financial resources from yei and esf and the proyect igme hidrocambio ref 2616 appendix a supplementary data the following is the supplementary data to this article a compressed file is provided which contains the file structure and the files necessary to make the d piper diagrams file 1 d piper v1 7z compressed file it contains the files necessary to make the d piper diagrams and the file structure necessary for the correct operation of the code this compressed file contains the following files file 2 d piper v1 py python code to generate the d piper charts file to run file 3 tools py python code containing all the required functions it is treated as a module an executed from the d piper v1 py file file 4 d piper v1 options txt ascii file necessary to run the d piper v1 py code contains the different options for executing the program and help text with the meaning of each one file 5 d piper v1 how to pdf brief description of the steps to take to make a d piper chart also the data folder in the compressed file provides the input files with which figs 2 5 have been built multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 104979 
