index,text
18600,recently ultraviolet light emitting diodes uv leds have emerged as a new uv source bringing flexibility for various uv wavelength combinations due to their unique feature of wavelength diversity in this study we investigated inactivation mechanisms of representative microorganisms at different wavelength combinations using uv leds two types of indicator microorganisms were examined namely escherichia coli e coli as a representative bacteria and bacteriophage ms2 as a representative virus different inactivation effects were observed and the results for uva pretreatment followed by uvc inactivation were particularly interesting while a substantial shoulder in the e coli uvc inactivation curve was observed this was reduced by uva pretreatment 365 nm at 17 j cm2 further 52 j cm2 uva eliminated the shoulder in the fluence response curves resulting in improved uvc 265 nm inactivation of e coli by over two orders of magnitude no inactivation improvement was observed for ms2 moreover uva pretreatment eliminated photoreactivation of e coli but did not affect dark repair detailed investigation of inactivation mechanisms revealed that hydroxyl radicals oh played a significant role in the effects of uva pretreatment this study demonstrated that oh radicals were generated inside e coli cells during uva pretreatment which accounted for the subsequent effects on e coli the impact of uva pretreatment on e coli inactivation and reactivation was mainly due to increased levels of oh radicals in e coli cells impairing cell functions such as dna self repair graphical abstract image 1 keywords uv inactivation mechanisms wavelength combinations ultraviolet light emitting diode uv led disinfection reactivation synergistic effect 1 introduction ultraviolet uv disinfection has been demonstrated to be an effective method to inactivate pathogenic microorganisms hijnen et al 2006 conventional uv sources include mercury lamps such as low pressure lp mercury lamps for monochromatic uv at 254 nm and medium pressure mp mercury lamps for polychromatic uv with a broad spectrum bolton and cotton 2008 recently a new uv source termed ultraviolet light emitting diode uv led has been developed with many special features and advantages such as being free from mercury compact robust with low power requirements and a long lifetime muramoto et al 2014 song et al 2016 therefore uv led has gained increasing interests as a promising alternative to conventional uv mercury lamps for disinfection ibrahim et al 2014 one of the special features of uv leds is wavelength diversity through various combinations of semiconductor materials taniyasu and kasu 2010 that is a variety of wavelengths are viable on uv leds covering a wide uv range including uvc uvb and uva uv leds with wavelengths from 250 nm to 365 nm have been typically used for the inactivation of bacteria and viruses for water disinfection studies song et al 2016 unlike lp mercury lamps with monochromatic uv at only 254 nm or mp mercury lamps with a fixed broad spectrum the wavelength diversity of uv leds not only allows the selection of specific uv wavelengths for a particular target but also provides the freedom and flexibility to combine specific wavelengths for a potentially cumulative effect though researchers have investigated uv leds wavelength combinations to explore potential synergy for disinfection existing results have been inconsistent due to differing wavelengths and microorganisms used in these studies beck et al 2017 chevremont et al 2012 li et al 2017 oguma et al 2013 our previous work examined uv leds wavelength combinations covering the full uv range uvc uvb uva in various possibilities simultaneous sequential and analyzed their effects on microorganisms bacterium e coli bacteriophage ms2 song et al 2019 it was found that combinations of uvc and uvb leds had additive inactivation effects but without synergy however wavelength combinations involving uva led showed markedly different effects depending on the manner with which uva radiation was applied for example uvc inactivation followed by visible light resulted in photoreactivation while uvc inactivation followed by uva exposure provided enhanced bacteria photoreactivation however uva exposure under a certain fluence followed by uvc radiation increased the extent of inactivation song et al 2019 the mechanisms of biological impact by single wavelength across different uv ranges has been previously examined inactivation of microorganisms by uv radiation is primarily based on the uv induced photochemical reactions of genetic materials e g dna in the cells of microorganisms for example uvc uvb radiation are strongly absorbed by dna besaratinia et al 2011 since uvc and uvb radiation induce the same photochemical reactions on dna the combinations of uvc and uvb radiation on inactivation follow the second law of photochemistry thus only offer additive effects beck et al 2017 contrastingly uva radiation is inefficient in inducing dna damages for inactivation due to its low absorption by dna as compared to uvc and uvb radiation sinha and hader 2002 however uva radiation has biological effects other than direct photochemical reactions on dna uva radiation can be absorbed by chromophoric molecules like uv absorbing pigments and cytochromes inducing indirect photosensitizing reactions to produce reactive intermediates cadet et al 2015 sinha and hader 2002 these reactive intermediates such as reactive oxygen species ros can damage cellular components including membranes proteins dna and contribute to growth delays mutations and even cell death eisenstark 1987 oppezzo and pizarro 2001 pizarro 1995 pizarro and orce 1988 ramabhadran and jagger 1976 further some cell enzymes may utilize the energy of uva to repair damaged dna for photoreactivation payne and sancar 1990 sancar 2003 applying uva either simultaneously with uvc or after uvc would reduce inactivation due to the photoreactivation effects of uva however applying uva prior to uvc inactivation has been shown to improve inactivation song et al 2019 due to the complex biological effects of uva radiation the mechanisms for uva involved wavelength combinations are not yet fully understood hence in this work we focused on investigating the inactivation mechanisms of uva and uvc wavelength combinations with an emphasis on applying uva exposure as pretreatment followed by uvc inactivation this study examined inactivation of two indicator microorganisms namely escherichia coli e coli as a representative bacteria and ms2 as a typical bacteriophage following application of wavelength combinations especially uva pretreatment followed by uvc inactivation reactivation including photoreactivation and dark repair bolton and cotton 2008 quek and hu 2008 after uva pretreatment were also investigated for e coli further a series of experiments were designed to explore the mechanisms and roles including the scavenger method of different ros during uva pretreatment based on this experimental work inactivation mechanisms for uva pretreatment were proposed 2 materials and methods 2 1 uv leds setup and irradiation two uv led chips 365 nm uva and 265 nm uvc led were located above a glass petri dish 9 cm diameter for uv irradiation as illustrated in fig s1 fig s2 and table s1 in the supplementary information si the distance between the uv leds and the water surface was 2 cm two separate dc power supplies model aim tti ex355r were used to drive the uv leds at a constant current mode as thermal management is essential for the operation of uv leds each uv led was attached to an aluminum heat sink for heat dissipation kheyrandish et al 2017 for the disinfection test a 50 ml water sample was exposed to uva led emission as uva pretreatment followed by uvc led exposure for inactivation the temperature of the water sample was monitored using a thermocouple and remained at room temperature of 22 c and no significant change was observed before and after uv exposure before and during the uv exposure the water sample was thoroughly mixed by a magnetic stirrer to ensure the uniformity of uv fluence to the sample in a fully mixed state the experimental setup was sheltered from ambient light with a black box the disinfection tests were performed with various uv exposure periods to observe the effects under different uv fluences for example uva pretreatment was performed for 1 min 10 min and 30 min and the corresponding uv fluences were determined to be 1 7 j cm2 17 j cm2 and 52 j cm2 respectively using the following method measurement of uv irradiation was performed using a monochromatic chemical actinometry method namely potassium iodide iodate actinometer for 265 nm and ferrioxalate actinometer for 365 nm bolton et al 2011 goldstein and rabani 2008 after determining the incident uv fluence by actinometry the uv fluence delivered to microorganisms was calculated by taking into account uv transmittance and depth of the medium to estimate the water factor of actual microbial water sample bolton and linden 2003 oguma et al 2016 the details of chemical actinometry and uv fluence determination method are described in supplementary text s1 2 2 microorganisms cultivation and enumeration two types of microorganisms bacterium e coli atcc 11229 and coliphage ms2 atcc 15597 b1 were examined for inactivation these microorganisms along with ms2 host bacterium e coli atcc 15597 were obtained from the american type culture collection atcc manassas va usa and cultivated following the supplier s product instruction agar plating methods spread agar plate method for e coli and double agar layers method for ms2 were used to enumerate these microorganisms in water as described elsewhere song et al 2018 the initial concentration for uv irradiation was approximately 106 cfu ml for e coli and 106 pfu ml for ms2 in sterile phosphate buffered saline pbs 2 3 photoreactivation and dark repair after uv inactivation the irradiated water sample was transferred to two glass petri dishes 9 cm diameter for photoreactivation and dark repair examination for photoreactivation the water sample was placed 10 cm beneath two fluorescent lamps philips f15t8 18w cool white 4100 k while stirring for 4 h the emission spectrum from these fluorescent lamps was measured using an ocean optics usb2000 spectrometer fig s3 the irradiance was measured to be 4 mw cm2 at the surface of the water sample using a newport optical 1917 r power meter with a 918d st uv detector measurement range 200 1100 nm for dark repair the water sample was placed in the dark while stirring for 4 h for both photoreactivation and dark repair tests samples were taken at 30 min intervals to determine the microorganism concentration then the percentage of reactivation was calculated as follows quek and hu 2008 l o g r e p a i r l o g 10 n r t l o g 10 n t l o g 10 n 0 l o g 10 n t where n0 is the concentration of microorganism before uv disinfection cfu ml nt is the concentration of microorganism immediately after uv disinfection cfu ml and nrt is the concentration of microorganism after reactivation cfu ml a control study was conducted with unirradiated water samples using the same procedure 2 4 effect of ros scavengers the role of three primary ros including superoxide radical o2 hydroxyl radical oh and hydrogen peroxide h2o2 he and hader 2002 hoerter et al 2005 were investigated using corresponding scavengers 4 hydroxy tempo tempol or 4 hydroxy 2 2 6 6 tetramethylpiperidin 1 oxyl was used to scavenge o2 radicals chen et al 2011 liang et al 2016 mannitol c6h14o6 was used to remove oh radicals fridovich and porter 1981 shen et al 1997 and catalase was used as a h2o2 scavenger novo and parola 2008 ruh et al 2000 during the uv irradiation of microorganisms in water the ros may present as intermediates either in water in cells of microorganisms or in a combination of both thus these scavengers were deliberately selected as they can not only remove the corresponding ros in water but also permeate the cells as intracellular scavengers goldstein and czapski 1984 lejeune et al 2006 reiter et al 1995 thamilselvan et al 2000 wilcox and pearlman 2008 yamada et al 2003 these scavengers were obtained from sigma aldrich co llc before uv irradiation each scavenger was mixed with microorganism suspension for 1 mm tempol 0 5 m mannitol and 1 mg ml catalase respectively then the scavenger containing microorganism suspension was stirred in the dark for 30 min to allow the scavenger to dissolve in the water and permeate the cells prior to uv exposure li et al 2010 inactivation and reactivation effects in the presence and absence of these scavengers were examined and compared to identify the role of each ros the control experiment was conducted using the same procedures without exposure to uv radiation to determine the concentration of ros in the water a probe compound carbamazepine cbz was utilized to indirectly determine the concentration of oh in the water samples details are described in supplementary text s2 to identify ros in the cells of microorganisms surface disinfection tests were designed and performed by excluding free liquid water during uv irradiation the scavenger was mixed with the microorganism suspension while stirring in the dark for 30 min to allow cell permeation then 20 μl of microorganism suspension was spread on an agar plate the agar plate was kept in the dark for 10 min to allow it to dry ensuring the scavenger permeated microorganism cells remained on the agar plate without the presence of free liquid water uv inactivation tests were performed in triplicate for each condition on the microorganism laden agar plate the surface disinfection tests both in the presence and absence of scavengers were performed and compared to identify the role of ros in the cells of microorganisms performing the experiments both with and without water enabled independent analysis of ros in water and in cells of microorganisms the test for each experimental condition was conducted independently three times using three replicates of each sample for measurement data were presented as averages with error bars representing the standard deviation a two tailed paired t test was utilized to perform statistical analysis to determine the significance of the data at 95 confidence level p 0 05 3 results and discussion 3 1 combined uva and uvc inactivation of e coli and ms2 uva and uvc irradiation were combined in various manners for the inactivation of e coli and ms2 results were compared with the addition of log inactivation by each separately applied wavelength to identify the effects of various combinations of uv wavelengths e coli inactivation differed depending on the method of 365 nm uva and 265 nm uvc application fig 1 a simultaneously applying uva with uvc reduced overall inactivation of e coli as compared to the addition of inactivation applying each wavelength separately similar results were observed on application of uvc followed by uva this observation was associated with photoreactivation effect of uva occurring on uv damaged dna when exposed to uva song et al 2019 contrastingly ms2 inactivation appeared comparable regardless of the uva and uvc combination method fig 1b this difference in results for ms2 and e coli could be linked to variance in the species of microorganisms unlike the bacterium e coli ms2 a typical virus is unable to repair uv damaged rna through photoreactivation due to a lack of necessary cellular components such as repair enzymes thus no additional effect occurred in ms2 although both applying uvc simultaneously with uva and applying uvc followed by uva reduced e coli inactivation the combination of uva radiation followed by uvc have a potential to enhance overall microbial inactivation song et al 2019 therefore this specific wavelength combination was further investigated 3 2 uva pretreatment effect on uvc inactivation of e coli and ms2 inactivation of e coli a representative bacteria with 265 nm uvc irradiation was examined using differing 365 nm uva pretreatment fluences fig 2 a after 1 7 j cm2 365 nm uva pretreatment no detectable e coli inactivation based on statistical analysis data not shown e coli inactivation by 265 nm uvc was comparable to that without uva pretreatment however 17 j cm2 uva pretreatment no statistically noticeable e coli inactivation data not shown significantly improved the e coli inactivation of 265 nm uvc while 52 j cm2 uva pretreatment which provided mere 0 2 log inactivation data not shown further enhanced the inactivation fig 2a thus after uva pretreatment at appropriate fluence the same uvc fluence achieved higher inactivation on e coli showing a significantly synergistic effect since higher uva fluence resulted in further improvement of downstream uvc inactivation it suggested that uva pretreatment accounted for the synergistic effect and the effect appeared to be fluence dependent moreover fig 2a revealed a shoulder effect in e coli inactivation by 265 nm uvc alone which is consistent with previous studies applying uv leds for e coli inactivation nyangaresi et al 2018 rattanakul and oguma 2018 scant inactivation of e coli was achieved at low uv fluence the slope of fluence response curve was significantly lower than that at high uv fluence hijnen et al 2006 this shoulder effect did not follow the second law of photochemistry which suggests the effect of photochemical reactions e g formation of pyrimidine dimers on dna under uvc radiation for inactivation is supposed to be directly proportional to the photons amount e g uv fluence the shoulder effect is believed to be closely related to biological processes other than photochemical reactions in cells and has been attributed to self repair ability such as photo repair and dark repair hoyer 1998 morton and haynes 1969 however with 17 j cm2 uva pretreatment the shoulder effect was slightly reduced while 52 j cm2 uva pretreatment nearly eliminated the shoulder fig 2a the absence of the shoulder after uva pretreatment suggested the suppression of biological processes such as self repair during uvc irradiation this likely resulted from elimination of self repair ability by the uva pretreatment bacterial cells can continuously repair uv induced dna damage even during the process of uv irradiation thus cells with self repair ability would exhibit superior resistance to uv inactivation quek and hu 2008 therefore the effect of uva pretreatment implied the elimination of self repair ability as evidenced by significant improvement on the fluence response curve of uvc inactivation hypotheses and interpretation regarding uva pretreatment on e coli inactivation is discussed further in subsequent sections to quantify the improvement of e coli inactivation by uva pretreatment a linear relationship of log inactivation and uvc fluence at high uv fluence region i e excluding the shoulder at low fluence region was fit using a shoulder model hijnen et al 2006 as shown in fig 2b there are no statistically significant difference between the slopes regardless of the absence or presence of uva pretreatment with varied fluence the slope of the fluence response curve formerly known as the inactivation rate constant indicates the uv sensitivity of a microorganism to a specific uv wavelength hijnen et al 2006 hence results indicated that uva pretreatment did not change the sensitivity of e coli to 265 nm uvc however the intercept of the fluence response curves varied with differing 365 nm uva fluence although no significant change on the intercept e g 2 98 vs 2 84 after 1 7 j cm2 uva pretreatment application of 17 j cm2 uva remarkably changed the intercept from 2 98 to 2 54 based on statistical analysis improving the whole fluence response curve by 0 44 log furthermore 52 j cm2 uva pretreatment caused dramatic improvement of inactivation by 2 2 log i e 0 79 vs 2 98 in terms of intercept in fig 2b these results suggested that uvc inactivation improvement was due to the elimination of the shoulder rather than an alteration of the inactivation rate constant moreover these quantitative results demonstrated the effect of uva pretreatment on e coli inactivation is uva fluence dependent with a threshold to take effect it appears the threshold for 365 nm uva pretreatment to take effect on e coli is between 1 7 j cm2 and 17 j cm2 it also implies that maximum inactivation would be reached when uva pretreatment eliminates the shoulder for the linear fluence response curve at zero intercept which closely aligns with the second law of photochemistry the effect of uva pretreatment on coliphage ms2 a representative virus was also examined 20 mj cm2 265 nm uvc alone resulted in 1 6 log inactivation of ms2 in water however with various fluence applied by 365 nm uva pretreatment up to 162 j cm2 no significant change was observed on ms2 inactivation based on the statistical analysis p 0 05 fig s4 unlike e coli no additional effect was found for ms2 inactivation by uva pretreatment even at a much higher fluence this might be attributed to the difference in microorganism type due to the lack of repair enzymes viruses such as ms2 have minimal cellular and biological functionality necessary for reactivation bolton and cotton 2008 harris et al 1987 hoyer 1998 thus the interaction of uv radiation with viruses depends only on photochemical reactions with either dna or rna following the second law of photochemistry without any additional effects this was also supported by the absence of shoulder effect in ms2 inactivation fluence response curve fig s5 3 3 uva pretreatment effect on reactivation of uvc inactivated e coli based on the results and discussion above the significant improvement by uva pretreatment on e coli inactivation may be related to suppression of e coli self repair ability to further investigate this hypothesis the effects of uva pretreatment on e coli reactivation including photoreactivation and dark repair were examined blank control experiments were conducted by stirring unirradiated e coli samples under fluorescent lamps visible light or in the dark no change in e coli concentration was observed during these 4 h control experiments fig s6 dark repair of e coli after uvc inactivation with uva pretreatment showed comparable repair extent and trend as that without uva pretreatment fig 3 a in both cases dark repair reached 12 in the first hour with no further recovery afterwards from this it appears uva pretreatment did not affect e coli dark repair as for photoreactivation fig 3b uvc inactivated e coli with 3 3 log inactivation recovered up to 60 during 4 h of photoreactivation this aligned with previous studies that documented up to 80 recovery by photoreactivation after applying 254 nm uv mercury lamps for e coli inactivation oguma et al 2002 quek and hu 2008 demonstrating significant recovery through photoreactivation however with uva pretreatment e coli only recovered to 15 through photoreactivation fig 3b this is dramatically lower than that without uva pretreatment and is indicative of lost e coli photo repair ability moreover with uva pretreatment the photoreactivation of e coli reached 15 during the first 2 5 h exposure to visible light fig 3b in which photoreactivation extent and trend was similar to that of dark repair fig 3a as the dark repair process does not require the presence of visible light it can take place both in the dark and under visible light nair 2010 usepa 2006 therefore 15 reactivation during the first 2 5 h exposure to visible light fig 3b may be mostly accounted for by a 12 rate of dark repair fig 3a an insignificant difference of less than 3 within the standard deviation of measurement suggests uva pretreatment led to a complete loss of photo repair ability in e coli this is consistent with the elimination of the shoulder in e coli inactivation after uva pretreatment which also indicates impaired self repair ability furthermore after 3 5 h visible light exposure the e coli concentration decreased to even lower than its initial concentration after uvc inactivation as shown in the two data points with negative percentage in fig 3b suggesting further inactivation under visible light considering there are two small peaks in uva range in fluorescent visible light lamp emission spectrum fig s3 the drop of e coli concentration further indicates that e coli cannot resist even slight uva emission in visible light this further indicates the severe damage to self repair systems after uva pretreatment in addition uva pretreatment mainly suppressed photoreactivation but hardly affected dark repair of e coli this is probably due to the different mechanisms of photoreactivation and dark repair photoreactivation is a simple repair system consisting of a single enzyme photolyase whereas dark repair involves multiple pathways and enzymes sinha and hader 2002 thus photoreactivation system may be more vulnerable while dark repair system has alternative repair pathways when one of the enzymes is dysfunctional as such effects of uva pretreatment on e coli reactivation support the hypothesis that significant inactivation improvement in e coli inactivation by uva pretreatment is related to suppression of e coli self repair ability such as photoreactivation 3 4 mechanisms investigation on uva pretreatment of e coli two important effects revealed from uva pretreatment of e coli include a considerable improvement in inactivation as well as a significant suppression of subsequent reactivation to explore the mechanisms for these phenomena experiments were designed and performed to examine the roles of ros during uva pretreatment followed by uvc inactivation different scavengers were added to the e coli suspension to remove corresponding ros under uv irradiation control experiments were also performed by adding scavengers only to e coli suspensions without uv irradiation no change in e coli concentrations were observed during these control experiments fig s7 indicating that these scavengers did not have a harmful effect on e coli cells within the duration of the uv irradiation tests e coli inactivation by 265 nm uvc irradiation with a variety of scavengers is shown in fig 4 a no significant difference was observed for uvc inactivation in the presence or absence of each scavenger indicating that the included ros such as superoxide radical o2 hydroxyl radical oh and hydrogen peroxide h2o2 were not involved in uvc inactivation of e coli this observation is consistent with fundamental mechanisms of uvc inactivation dna directly absorb uvc radiation in the formation of pyrimidine dimers through photochemical reactions without any intermediate steps besaratinia et al 2011 fig 4b shows e coli inactivation by uvc irradiation after uva pretreatment with different scavengers when tempol a scavenger of o2 was added into the inactivation system e coli inactivation was comparable to inactivation without a scavenger this suggests that o2 does not play a role in uva pretreatment a similar result was observed with the addition of catalase a scavenger of h2o2 indicating that h2o2 was similarly not involved in the inactivation process however when adding mannitol to remove oh e coli inactivation was significantly decreased as to inactivation without the scavenger suggesting that oh played an important role since oh was only involved in the uva pretreatment followed by uvc inactivation process fig 4b but not in solitary uvc inactivation fig 4a oh might result from uva pretreatment hydroxyl radical is highly reactive enabling non selective reaction and oxidization with most organic compounds and inorganic ions with high rate constants usually on the order of 106 109 m 1 s 1 cheng et al 2016 dorfman and adams 1973 wang and xu 2012 it is reasonable to infer that oh would react with e coli cells to induce cellular damage which would account for a considerable improvement in inactivation the highly reactive oh radicals are thought to significantly damage cells and inactivate e coli as suggested by many studies on advanced oxidation processes aops for disinfection cho et al 2004 chong et al 2010 malato et al 2009 however during 52 j cm2 365 nm uva pretreatment only 0 2 log inactivation on e coli was achieved data not shown indicating insignificant inactivation during uva pretreatment this implies that oh might only damage the cellular functionality inducing a nonlethal effect on e coli without destroying the e coli cells hence after uva pretreatment though the impaired e coli cells were not significantly inactivated they were more vulnerable to subsequent uvc irradiation unable to reactivate afterwards this deduced mechanism differs from the role of oh generated in aops for disinfection therefore further investigation was conducted to clarify the mechanism in uva pretreatment to explore the mechanism for the significant suppression of reactivation the role of oh on e coli reactivation was examined using mannitol as scavenger as shown in fig 5 after uva pretreatment followed by uvc inactivation the presence of mannitol in the system increased the photoreactivation of e coli as compared to experiments without a scavenger this suggests that oh played an important role in reducing e coli reactivation by uva pretreatment since oh played an important role in inactivation improvement and reactivation suppression following uva pretreatment it was essential to quantitatively determine oh in order to clarify the mechanism this process consisted of two stages including uva pretreatment and subsequent uvc inactivation thus it was necessary to identify at which stage the oh was produced and present before performing the oh measurement mannitol the scavenger of oh was added into the system at different stages in the uva pretreatment followed by uvc inactivation process fig 6 when mannitol was present during the process in entirety e coli inactivation significantly dropped however when adding mannitol after uva pretreatment to confirm its presence during uvc inactivation only e coli inactivation was not affected as compared to experiments without a scavenger this observation suggested that e coli cells were already impacted by oh during the uva pretreatment and were no longer protected if oh was removed during the subsequent uvc irradiation thus it demonstrated that oh was produced and present only during uva pretreatment but not during the following uvc inactivation since oh was produced during the uva pretreatment its concentration in water was indirectly determined using the probe compound cbz the control experiment showed no change of e coli concentration in the presence of cbz fig s8 indicating cbz has no harmful effect to e coli cells during the test period with uva irradiation whether e coli presented in the system or not no degradation of cbz was observed under 365 nm uva irradiation fig s9 indicating no oh present in water considering water in this system only contained uva radiation and e coli cells this observation supports the concept that uva radiation alone was not sufficient to directly generate oh in water unless photocatalysts e g tio2 or chemicals e g h2o2 cl2 natural organic matters for aops were present andreozzi et al 1999 kabra et al 2004 wang and xu 2012 contrastingly scavenger experiments above have demonstrated oh can be generated during uva pretreatment of e coli considering that scavengers can remove the corresponding ros both in water and in cells a lack of oh detection in water indicated that oh radicals were produced and present within the e coli cells during uva irradiation in biological systems oh radicals have high reactivity and very short in vivo half lives of 10 9 s novo and parola 2008 sies 1993 thus oh concentration inside cells cannot be directly measured in order to verify that oh was generated within e coli cells experiments on surface disinfection in the absence of free liquid water were designed and performed as shown in fig 7 e coli on the surface of agar plates were exposed to 4 mj cm2 265 nm uvc irradiation and 1 2 log inactivation was observed when 14 j cm2 365 nm uva pretreatment was applied the same 4 mj cm2 265 nm uvc irradiation achieved 1 6 log inactivation this significant improvement proved that uva pretreatment can also enhance surface disinfection however when mannitol treated e coli cells underwent 14 j cm2 365 nm uva pretreatment 4 mj cm2 265 nm uvc irradiation only resulted in 1 2 log inactivation a level equivalent to no uva pretreatment this observation indicated removal of oh in e coli cells cancelled out the inactivation improvement by uva pretreatment hence these results demonstrated the role of oh in uva pretreatment and verified that oh was generated within e coli cells but not within water 3 5 proposed mechanisms for uva pretreatment of e coli based on the role of ros in e coli inactivation and reactivation during uva pretreatment the mechanisms for uva pretreatment largely relate to biological processes and ros in e coli cells rather than direct photochemical reactions on dna importantly proposed mechanisms must integrate into the broader context of fundamental biology in a biological context ros are formed as natural by products during normal processes of cell metabolism novo and parola 2008 these ros are highly reactive and can react with cellular components such as lipids proteins and dna inducing adverse effects on the cell halliwell 1996 through millions of years evolution organisms have developed multileveled strategies and mechanisms to carefully control the generation of ros and defend against deleterious effects of ros in cells sies 1993 however when cells are exposed to adverse environmental conditions such as uv heat or toxic chemicals the delicately maintained balance of ros in cells can be disturbed cellular ros levels can increase dramatically resulting in significant oxidative damage to cell components impairing cellular functions and even leading to cell death termed oxidative stress devasagayam et al 2004 lushchak 2014 sies 1986 among these ros o2 and h2o2 can be scavenged by efficient enzymatic reactions using superoxide dismutase and catalase respectively contained in most organisms cells as a defensive mechanisms against oxidative stress muller et al 2007 reiter et al 1995 however oh cannot be eliminated through an enzymatic reaction additionally oh has a short half life of 10 9 s with high reactivity thus it does not diffuse from the site of generation and can rapidly react with surrounding molecules oh can damage all types of macromolecules in cells including carbohydrates leading to degradation nucleic acids leading to dna damage lipids leading to cell membranes damage and amino acids leading to proteins denaturation and enzymes inactivation making it a very dangerous radical to the organisms novo and parola 2008 reiter et al 1995 unlike efficient inactivation by uvc radiation uva radiation is poorly absorbed by dna and is inefficient in inducing dna damage for inactivation sinha and hader 2002 usually 105 times more uv fluence of uva than uvc radiation is required to form pyrimidine dimers on dna for inactivation due to low absorption of uva radiation by dna gayan et al 2014 however uva radiation at low uv fluence compared to the high fluence requirement for inactivation has been reported to induce sublethal effects on microorganisms especially e coli this includes growth delay membrane damage protein oxidation decreased energy metabolism and mutation bosshard et al 2010a 2010b eisenstark 1987 girard et al 2011 hoerter et al 2005 oppezzo and pizarro 2001 pizarro 1995 pizarro and orce 1988 ramabhadran and jagger 1976 these sublethal effects are believed to relate to ros and oxidative stress induced by uva cabiscol et al 2000 hoerter et al 2005 smirnova and oktyabrsky 1994 tyrrell and keyse 1990 some endogenous photosensitizers like cytochromes can absorb uva radiation and induce photosensitizing reactions for ros generation cadet et al 2015 sinha and hader 2002 as such by integrating the experimental results in this study within the biological context above mechanisms for uva pretreatment on microorganism inactivation and reactivation can be proposed for e coli a representative bacteria exposure to uva radiation with a certain uv fluence can impact cellular metabolism systems disturbing the ros balance in cells this results in significant increases in ros levels mostly oh to generate oxidative stress the excess ros in cells induce oxidative damage to cellular components such as dna self repair enzymes leading to enzymatic function failure and loss of self repair ability thus e coli becomes increasingly vulnerable to uvc inactivation and is no longer able to reactivate due to an inability to repair uvc induced dna damage resulting in considerable inactivation improvement and significant reactivation suppression on e coli as for ms2 a representative virus exposure to uv radiation only induces photochemical reactions on rna without additional biological effects this is due to ms2 composition namely rna inside a protein coat without a cellular metabolic system madigan et al 2009 therefore no inactivation improvement was achieved on ms2 following application of uva pretreatment and uvc inactivation to the best of our knowledge this is the first study to experimentally demonstrate that ros inside e coli cells are involved in the mechanisms of uva pretreatment on uvc inactivation understanding mechanisms of uva pretreatment are essential to further optimize this method for potential application as uva pretreatment can significantly improve inactivation and eliminate reactivation of e coli the present findings have the potential to reduce both energy consumption and cost in practical application although uv leds are used in this study to facilitate wavelength combinations the beneficial effects of uva pretreatment are not limited to uv leds more uv sources including other uva radiation e g unlimited cost free sunlight could be utilized when designing systems for practical application 4 conclusions based on the special feature on wavelength diversity of uv leds various wavelengths combinations were conducted on the inactivation of different types of microorganisms among them 365 nm uva pretreatment followed by 265 nm uvc inactivation showed interesting effects 365 nm uva pretreatment dramatically improved the e coli inactivation of 265 nm uvc by eliminating shoulder in the fluence response curves this combination also significantly reduced the reactivation of e coli 52 j cm2 365 nm uva pretreatment improved the whole inactivation fluence response curve by 2 2 log inactivation while eliminating the reactivation of e coli afterwards among reactive oxygen species ros oh radicals played an important role on these effects of uva pretreatment on e coli these oh radicals were proved to be produced inside e coli cells during uva pretreatment the mechanisms for effects of uva pretreatment were proposed uva irradiation with a certain fluence can impact the metabolism of bacteria disturb the ros balance in cells resulting in increased ros levels mainly oh radical and oxidative damage to cellular components such as dna self repair enzymes after enzymes function failure and loss of self repair ability bacteria become more vulnerable to uvc inactivation and are no longer able to reactivate afterwards 365 nm uva pretreatment followed by 265 nm uvc inactivation has no additional effect on coliphage ms2 with the uva pretreatment up to 162 j cm2 probably due to minimal metabolic activities in viruses as the effects and mechanisms of uva pretreatment depends on differing microorganisms bacteria of a similar nature are expected to show similar mechanisms further investigation of more microorganisms are needed to better understand the biological effects of uva pretreatment moreover as the proposed mechanisms of uva pretreatment largely involve biological processes such as metabolism and ros in cells further investigation incorporating molecular microbiology is highly encouraged to provide more insightful understanding and optimization of this process for practical applications declaration of competing interest x the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper the authors declare the following financial interests personal relationships which may be considered as potential competing interests n a acknowledgements this research was supported by natural science and engineering research council nserc of canada k song was partially supported by china scholarship council csc appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j watres 2019 114875 
18600,recently ultraviolet light emitting diodes uv leds have emerged as a new uv source bringing flexibility for various uv wavelength combinations due to their unique feature of wavelength diversity in this study we investigated inactivation mechanisms of representative microorganisms at different wavelength combinations using uv leds two types of indicator microorganisms were examined namely escherichia coli e coli as a representative bacteria and bacteriophage ms2 as a representative virus different inactivation effects were observed and the results for uva pretreatment followed by uvc inactivation were particularly interesting while a substantial shoulder in the e coli uvc inactivation curve was observed this was reduced by uva pretreatment 365 nm at 17 j cm2 further 52 j cm2 uva eliminated the shoulder in the fluence response curves resulting in improved uvc 265 nm inactivation of e coli by over two orders of magnitude no inactivation improvement was observed for ms2 moreover uva pretreatment eliminated photoreactivation of e coli but did not affect dark repair detailed investigation of inactivation mechanisms revealed that hydroxyl radicals oh played a significant role in the effects of uva pretreatment this study demonstrated that oh radicals were generated inside e coli cells during uva pretreatment which accounted for the subsequent effects on e coli the impact of uva pretreatment on e coli inactivation and reactivation was mainly due to increased levels of oh radicals in e coli cells impairing cell functions such as dna self repair graphical abstract image 1 keywords uv inactivation mechanisms wavelength combinations ultraviolet light emitting diode uv led disinfection reactivation synergistic effect 1 introduction ultraviolet uv disinfection has been demonstrated to be an effective method to inactivate pathogenic microorganisms hijnen et al 2006 conventional uv sources include mercury lamps such as low pressure lp mercury lamps for monochromatic uv at 254 nm and medium pressure mp mercury lamps for polychromatic uv with a broad spectrum bolton and cotton 2008 recently a new uv source termed ultraviolet light emitting diode uv led has been developed with many special features and advantages such as being free from mercury compact robust with low power requirements and a long lifetime muramoto et al 2014 song et al 2016 therefore uv led has gained increasing interests as a promising alternative to conventional uv mercury lamps for disinfection ibrahim et al 2014 one of the special features of uv leds is wavelength diversity through various combinations of semiconductor materials taniyasu and kasu 2010 that is a variety of wavelengths are viable on uv leds covering a wide uv range including uvc uvb and uva uv leds with wavelengths from 250 nm to 365 nm have been typically used for the inactivation of bacteria and viruses for water disinfection studies song et al 2016 unlike lp mercury lamps with monochromatic uv at only 254 nm or mp mercury lamps with a fixed broad spectrum the wavelength diversity of uv leds not only allows the selection of specific uv wavelengths for a particular target but also provides the freedom and flexibility to combine specific wavelengths for a potentially cumulative effect though researchers have investigated uv leds wavelength combinations to explore potential synergy for disinfection existing results have been inconsistent due to differing wavelengths and microorganisms used in these studies beck et al 2017 chevremont et al 2012 li et al 2017 oguma et al 2013 our previous work examined uv leds wavelength combinations covering the full uv range uvc uvb uva in various possibilities simultaneous sequential and analyzed their effects on microorganisms bacterium e coli bacteriophage ms2 song et al 2019 it was found that combinations of uvc and uvb leds had additive inactivation effects but without synergy however wavelength combinations involving uva led showed markedly different effects depending on the manner with which uva radiation was applied for example uvc inactivation followed by visible light resulted in photoreactivation while uvc inactivation followed by uva exposure provided enhanced bacteria photoreactivation however uva exposure under a certain fluence followed by uvc radiation increased the extent of inactivation song et al 2019 the mechanisms of biological impact by single wavelength across different uv ranges has been previously examined inactivation of microorganisms by uv radiation is primarily based on the uv induced photochemical reactions of genetic materials e g dna in the cells of microorganisms for example uvc uvb radiation are strongly absorbed by dna besaratinia et al 2011 since uvc and uvb radiation induce the same photochemical reactions on dna the combinations of uvc and uvb radiation on inactivation follow the second law of photochemistry thus only offer additive effects beck et al 2017 contrastingly uva radiation is inefficient in inducing dna damages for inactivation due to its low absorption by dna as compared to uvc and uvb radiation sinha and hader 2002 however uva radiation has biological effects other than direct photochemical reactions on dna uva radiation can be absorbed by chromophoric molecules like uv absorbing pigments and cytochromes inducing indirect photosensitizing reactions to produce reactive intermediates cadet et al 2015 sinha and hader 2002 these reactive intermediates such as reactive oxygen species ros can damage cellular components including membranes proteins dna and contribute to growth delays mutations and even cell death eisenstark 1987 oppezzo and pizarro 2001 pizarro 1995 pizarro and orce 1988 ramabhadran and jagger 1976 further some cell enzymes may utilize the energy of uva to repair damaged dna for photoreactivation payne and sancar 1990 sancar 2003 applying uva either simultaneously with uvc or after uvc would reduce inactivation due to the photoreactivation effects of uva however applying uva prior to uvc inactivation has been shown to improve inactivation song et al 2019 due to the complex biological effects of uva radiation the mechanisms for uva involved wavelength combinations are not yet fully understood hence in this work we focused on investigating the inactivation mechanisms of uva and uvc wavelength combinations with an emphasis on applying uva exposure as pretreatment followed by uvc inactivation this study examined inactivation of two indicator microorganisms namely escherichia coli e coli as a representative bacteria and ms2 as a typical bacteriophage following application of wavelength combinations especially uva pretreatment followed by uvc inactivation reactivation including photoreactivation and dark repair bolton and cotton 2008 quek and hu 2008 after uva pretreatment were also investigated for e coli further a series of experiments were designed to explore the mechanisms and roles including the scavenger method of different ros during uva pretreatment based on this experimental work inactivation mechanisms for uva pretreatment were proposed 2 materials and methods 2 1 uv leds setup and irradiation two uv led chips 365 nm uva and 265 nm uvc led were located above a glass petri dish 9 cm diameter for uv irradiation as illustrated in fig s1 fig s2 and table s1 in the supplementary information si the distance between the uv leds and the water surface was 2 cm two separate dc power supplies model aim tti ex355r were used to drive the uv leds at a constant current mode as thermal management is essential for the operation of uv leds each uv led was attached to an aluminum heat sink for heat dissipation kheyrandish et al 2017 for the disinfection test a 50 ml water sample was exposed to uva led emission as uva pretreatment followed by uvc led exposure for inactivation the temperature of the water sample was monitored using a thermocouple and remained at room temperature of 22 c and no significant change was observed before and after uv exposure before and during the uv exposure the water sample was thoroughly mixed by a magnetic stirrer to ensure the uniformity of uv fluence to the sample in a fully mixed state the experimental setup was sheltered from ambient light with a black box the disinfection tests were performed with various uv exposure periods to observe the effects under different uv fluences for example uva pretreatment was performed for 1 min 10 min and 30 min and the corresponding uv fluences were determined to be 1 7 j cm2 17 j cm2 and 52 j cm2 respectively using the following method measurement of uv irradiation was performed using a monochromatic chemical actinometry method namely potassium iodide iodate actinometer for 265 nm and ferrioxalate actinometer for 365 nm bolton et al 2011 goldstein and rabani 2008 after determining the incident uv fluence by actinometry the uv fluence delivered to microorganisms was calculated by taking into account uv transmittance and depth of the medium to estimate the water factor of actual microbial water sample bolton and linden 2003 oguma et al 2016 the details of chemical actinometry and uv fluence determination method are described in supplementary text s1 2 2 microorganisms cultivation and enumeration two types of microorganisms bacterium e coli atcc 11229 and coliphage ms2 atcc 15597 b1 were examined for inactivation these microorganisms along with ms2 host bacterium e coli atcc 15597 were obtained from the american type culture collection atcc manassas va usa and cultivated following the supplier s product instruction agar plating methods spread agar plate method for e coli and double agar layers method for ms2 were used to enumerate these microorganisms in water as described elsewhere song et al 2018 the initial concentration for uv irradiation was approximately 106 cfu ml for e coli and 106 pfu ml for ms2 in sterile phosphate buffered saline pbs 2 3 photoreactivation and dark repair after uv inactivation the irradiated water sample was transferred to two glass petri dishes 9 cm diameter for photoreactivation and dark repair examination for photoreactivation the water sample was placed 10 cm beneath two fluorescent lamps philips f15t8 18w cool white 4100 k while stirring for 4 h the emission spectrum from these fluorescent lamps was measured using an ocean optics usb2000 spectrometer fig s3 the irradiance was measured to be 4 mw cm2 at the surface of the water sample using a newport optical 1917 r power meter with a 918d st uv detector measurement range 200 1100 nm for dark repair the water sample was placed in the dark while stirring for 4 h for both photoreactivation and dark repair tests samples were taken at 30 min intervals to determine the microorganism concentration then the percentage of reactivation was calculated as follows quek and hu 2008 l o g r e p a i r l o g 10 n r t l o g 10 n t l o g 10 n 0 l o g 10 n t where n0 is the concentration of microorganism before uv disinfection cfu ml nt is the concentration of microorganism immediately after uv disinfection cfu ml and nrt is the concentration of microorganism after reactivation cfu ml a control study was conducted with unirradiated water samples using the same procedure 2 4 effect of ros scavengers the role of three primary ros including superoxide radical o2 hydroxyl radical oh and hydrogen peroxide h2o2 he and hader 2002 hoerter et al 2005 were investigated using corresponding scavengers 4 hydroxy tempo tempol or 4 hydroxy 2 2 6 6 tetramethylpiperidin 1 oxyl was used to scavenge o2 radicals chen et al 2011 liang et al 2016 mannitol c6h14o6 was used to remove oh radicals fridovich and porter 1981 shen et al 1997 and catalase was used as a h2o2 scavenger novo and parola 2008 ruh et al 2000 during the uv irradiation of microorganisms in water the ros may present as intermediates either in water in cells of microorganisms or in a combination of both thus these scavengers were deliberately selected as they can not only remove the corresponding ros in water but also permeate the cells as intracellular scavengers goldstein and czapski 1984 lejeune et al 2006 reiter et al 1995 thamilselvan et al 2000 wilcox and pearlman 2008 yamada et al 2003 these scavengers were obtained from sigma aldrich co llc before uv irradiation each scavenger was mixed with microorganism suspension for 1 mm tempol 0 5 m mannitol and 1 mg ml catalase respectively then the scavenger containing microorganism suspension was stirred in the dark for 30 min to allow the scavenger to dissolve in the water and permeate the cells prior to uv exposure li et al 2010 inactivation and reactivation effects in the presence and absence of these scavengers were examined and compared to identify the role of each ros the control experiment was conducted using the same procedures without exposure to uv radiation to determine the concentration of ros in the water a probe compound carbamazepine cbz was utilized to indirectly determine the concentration of oh in the water samples details are described in supplementary text s2 to identify ros in the cells of microorganisms surface disinfection tests were designed and performed by excluding free liquid water during uv irradiation the scavenger was mixed with the microorganism suspension while stirring in the dark for 30 min to allow cell permeation then 20 μl of microorganism suspension was spread on an agar plate the agar plate was kept in the dark for 10 min to allow it to dry ensuring the scavenger permeated microorganism cells remained on the agar plate without the presence of free liquid water uv inactivation tests were performed in triplicate for each condition on the microorganism laden agar plate the surface disinfection tests both in the presence and absence of scavengers were performed and compared to identify the role of ros in the cells of microorganisms performing the experiments both with and without water enabled independent analysis of ros in water and in cells of microorganisms the test for each experimental condition was conducted independently three times using three replicates of each sample for measurement data were presented as averages with error bars representing the standard deviation a two tailed paired t test was utilized to perform statistical analysis to determine the significance of the data at 95 confidence level p 0 05 3 results and discussion 3 1 combined uva and uvc inactivation of e coli and ms2 uva and uvc irradiation were combined in various manners for the inactivation of e coli and ms2 results were compared with the addition of log inactivation by each separately applied wavelength to identify the effects of various combinations of uv wavelengths e coli inactivation differed depending on the method of 365 nm uva and 265 nm uvc application fig 1 a simultaneously applying uva with uvc reduced overall inactivation of e coli as compared to the addition of inactivation applying each wavelength separately similar results were observed on application of uvc followed by uva this observation was associated with photoreactivation effect of uva occurring on uv damaged dna when exposed to uva song et al 2019 contrastingly ms2 inactivation appeared comparable regardless of the uva and uvc combination method fig 1b this difference in results for ms2 and e coli could be linked to variance in the species of microorganisms unlike the bacterium e coli ms2 a typical virus is unable to repair uv damaged rna through photoreactivation due to a lack of necessary cellular components such as repair enzymes thus no additional effect occurred in ms2 although both applying uvc simultaneously with uva and applying uvc followed by uva reduced e coli inactivation the combination of uva radiation followed by uvc have a potential to enhance overall microbial inactivation song et al 2019 therefore this specific wavelength combination was further investigated 3 2 uva pretreatment effect on uvc inactivation of e coli and ms2 inactivation of e coli a representative bacteria with 265 nm uvc irradiation was examined using differing 365 nm uva pretreatment fluences fig 2 a after 1 7 j cm2 365 nm uva pretreatment no detectable e coli inactivation based on statistical analysis data not shown e coli inactivation by 265 nm uvc was comparable to that without uva pretreatment however 17 j cm2 uva pretreatment no statistically noticeable e coli inactivation data not shown significantly improved the e coli inactivation of 265 nm uvc while 52 j cm2 uva pretreatment which provided mere 0 2 log inactivation data not shown further enhanced the inactivation fig 2a thus after uva pretreatment at appropriate fluence the same uvc fluence achieved higher inactivation on e coli showing a significantly synergistic effect since higher uva fluence resulted in further improvement of downstream uvc inactivation it suggested that uva pretreatment accounted for the synergistic effect and the effect appeared to be fluence dependent moreover fig 2a revealed a shoulder effect in e coli inactivation by 265 nm uvc alone which is consistent with previous studies applying uv leds for e coli inactivation nyangaresi et al 2018 rattanakul and oguma 2018 scant inactivation of e coli was achieved at low uv fluence the slope of fluence response curve was significantly lower than that at high uv fluence hijnen et al 2006 this shoulder effect did not follow the second law of photochemistry which suggests the effect of photochemical reactions e g formation of pyrimidine dimers on dna under uvc radiation for inactivation is supposed to be directly proportional to the photons amount e g uv fluence the shoulder effect is believed to be closely related to biological processes other than photochemical reactions in cells and has been attributed to self repair ability such as photo repair and dark repair hoyer 1998 morton and haynes 1969 however with 17 j cm2 uva pretreatment the shoulder effect was slightly reduced while 52 j cm2 uva pretreatment nearly eliminated the shoulder fig 2a the absence of the shoulder after uva pretreatment suggested the suppression of biological processes such as self repair during uvc irradiation this likely resulted from elimination of self repair ability by the uva pretreatment bacterial cells can continuously repair uv induced dna damage even during the process of uv irradiation thus cells with self repair ability would exhibit superior resistance to uv inactivation quek and hu 2008 therefore the effect of uva pretreatment implied the elimination of self repair ability as evidenced by significant improvement on the fluence response curve of uvc inactivation hypotheses and interpretation regarding uva pretreatment on e coli inactivation is discussed further in subsequent sections to quantify the improvement of e coli inactivation by uva pretreatment a linear relationship of log inactivation and uvc fluence at high uv fluence region i e excluding the shoulder at low fluence region was fit using a shoulder model hijnen et al 2006 as shown in fig 2b there are no statistically significant difference between the slopes regardless of the absence or presence of uva pretreatment with varied fluence the slope of the fluence response curve formerly known as the inactivation rate constant indicates the uv sensitivity of a microorganism to a specific uv wavelength hijnen et al 2006 hence results indicated that uva pretreatment did not change the sensitivity of e coli to 265 nm uvc however the intercept of the fluence response curves varied with differing 365 nm uva fluence although no significant change on the intercept e g 2 98 vs 2 84 after 1 7 j cm2 uva pretreatment application of 17 j cm2 uva remarkably changed the intercept from 2 98 to 2 54 based on statistical analysis improving the whole fluence response curve by 0 44 log furthermore 52 j cm2 uva pretreatment caused dramatic improvement of inactivation by 2 2 log i e 0 79 vs 2 98 in terms of intercept in fig 2b these results suggested that uvc inactivation improvement was due to the elimination of the shoulder rather than an alteration of the inactivation rate constant moreover these quantitative results demonstrated the effect of uva pretreatment on e coli inactivation is uva fluence dependent with a threshold to take effect it appears the threshold for 365 nm uva pretreatment to take effect on e coli is between 1 7 j cm2 and 17 j cm2 it also implies that maximum inactivation would be reached when uva pretreatment eliminates the shoulder for the linear fluence response curve at zero intercept which closely aligns with the second law of photochemistry the effect of uva pretreatment on coliphage ms2 a representative virus was also examined 20 mj cm2 265 nm uvc alone resulted in 1 6 log inactivation of ms2 in water however with various fluence applied by 365 nm uva pretreatment up to 162 j cm2 no significant change was observed on ms2 inactivation based on the statistical analysis p 0 05 fig s4 unlike e coli no additional effect was found for ms2 inactivation by uva pretreatment even at a much higher fluence this might be attributed to the difference in microorganism type due to the lack of repair enzymes viruses such as ms2 have minimal cellular and biological functionality necessary for reactivation bolton and cotton 2008 harris et al 1987 hoyer 1998 thus the interaction of uv radiation with viruses depends only on photochemical reactions with either dna or rna following the second law of photochemistry without any additional effects this was also supported by the absence of shoulder effect in ms2 inactivation fluence response curve fig s5 3 3 uva pretreatment effect on reactivation of uvc inactivated e coli based on the results and discussion above the significant improvement by uva pretreatment on e coli inactivation may be related to suppression of e coli self repair ability to further investigate this hypothesis the effects of uva pretreatment on e coli reactivation including photoreactivation and dark repair were examined blank control experiments were conducted by stirring unirradiated e coli samples under fluorescent lamps visible light or in the dark no change in e coli concentration was observed during these 4 h control experiments fig s6 dark repair of e coli after uvc inactivation with uva pretreatment showed comparable repair extent and trend as that without uva pretreatment fig 3 a in both cases dark repair reached 12 in the first hour with no further recovery afterwards from this it appears uva pretreatment did not affect e coli dark repair as for photoreactivation fig 3b uvc inactivated e coli with 3 3 log inactivation recovered up to 60 during 4 h of photoreactivation this aligned with previous studies that documented up to 80 recovery by photoreactivation after applying 254 nm uv mercury lamps for e coli inactivation oguma et al 2002 quek and hu 2008 demonstrating significant recovery through photoreactivation however with uva pretreatment e coli only recovered to 15 through photoreactivation fig 3b this is dramatically lower than that without uva pretreatment and is indicative of lost e coli photo repair ability moreover with uva pretreatment the photoreactivation of e coli reached 15 during the first 2 5 h exposure to visible light fig 3b in which photoreactivation extent and trend was similar to that of dark repair fig 3a as the dark repair process does not require the presence of visible light it can take place both in the dark and under visible light nair 2010 usepa 2006 therefore 15 reactivation during the first 2 5 h exposure to visible light fig 3b may be mostly accounted for by a 12 rate of dark repair fig 3a an insignificant difference of less than 3 within the standard deviation of measurement suggests uva pretreatment led to a complete loss of photo repair ability in e coli this is consistent with the elimination of the shoulder in e coli inactivation after uva pretreatment which also indicates impaired self repair ability furthermore after 3 5 h visible light exposure the e coli concentration decreased to even lower than its initial concentration after uvc inactivation as shown in the two data points with negative percentage in fig 3b suggesting further inactivation under visible light considering there are two small peaks in uva range in fluorescent visible light lamp emission spectrum fig s3 the drop of e coli concentration further indicates that e coli cannot resist even slight uva emission in visible light this further indicates the severe damage to self repair systems after uva pretreatment in addition uva pretreatment mainly suppressed photoreactivation but hardly affected dark repair of e coli this is probably due to the different mechanisms of photoreactivation and dark repair photoreactivation is a simple repair system consisting of a single enzyme photolyase whereas dark repair involves multiple pathways and enzymes sinha and hader 2002 thus photoreactivation system may be more vulnerable while dark repair system has alternative repair pathways when one of the enzymes is dysfunctional as such effects of uva pretreatment on e coli reactivation support the hypothesis that significant inactivation improvement in e coli inactivation by uva pretreatment is related to suppression of e coli self repair ability such as photoreactivation 3 4 mechanisms investigation on uva pretreatment of e coli two important effects revealed from uva pretreatment of e coli include a considerable improvement in inactivation as well as a significant suppression of subsequent reactivation to explore the mechanisms for these phenomena experiments were designed and performed to examine the roles of ros during uva pretreatment followed by uvc inactivation different scavengers were added to the e coli suspension to remove corresponding ros under uv irradiation control experiments were also performed by adding scavengers only to e coli suspensions without uv irradiation no change in e coli concentrations were observed during these control experiments fig s7 indicating that these scavengers did not have a harmful effect on e coli cells within the duration of the uv irradiation tests e coli inactivation by 265 nm uvc irradiation with a variety of scavengers is shown in fig 4 a no significant difference was observed for uvc inactivation in the presence or absence of each scavenger indicating that the included ros such as superoxide radical o2 hydroxyl radical oh and hydrogen peroxide h2o2 were not involved in uvc inactivation of e coli this observation is consistent with fundamental mechanisms of uvc inactivation dna directly absorb uvc radiation in the formation of pyrimidine dimers through photochemical reactions without any intermediate steps besaratinia et al 2011 fig 4b shows e coli inactivation by uvc irradiation after uva pretreatment with different scavengers when tempol a scavenger of o2 was added into the inactivation system e coli inactivation was comparable to inactivation without a scavenger this suggests that o2 does not play a role in uva pretreatment a similar result was observed with the addition of catalase a scavenger of h2o2 indicating that h2o2 was similarly not involved in the inactivation process however when adding mannitol to remove oh e coli inactivation was significantly decreased as to inactivation without the scavenger suggesting that oh played an important role since oh was only involved in the uva pretreatment followed by uvc inactivation process fig 4b but not in solitary uvc inactivation fig 4a oh might result from uva pretreatment hydroxyl radical is highly reactive enabling non selective reaction and oxidization with most organic compounds and inorganic ions with high rate constants usually on the order of 106 109 m 1 s 1 cheng et al 2016 dorfman and adams 1973 wang and xu 2012 it is reasonable to infer that oh would react with e coli cells to induce cellular damage which would account for a considerable improvement in inactivation the highly reactive oh radicals are thought to significantly damage cells and inactivate e coli as suggested by many studies on advanced oxidation processes aops for disinfection cho et al 2004 chong et al 2010 malato et al 2009 however during 52 j cm2 365 nm uva pretreatment only 0 2 log inactivation on e coli was achieved data not shown indicating insignificant inactivation during uva pretreatment this implies that oh might only damage the cellular functionality inducing a nonlethal effect on e coli without destroying the e coli cells hence after uva pretreatment though the impaired e coli cells were not significantly inactivated they were more vulnerable to subsequent uvc irradiation unable to reactivate afterwards this deduced mechanism differs from the role of oh generated in aops for disinfection therefore further investigation was conducted to clarify the mechanism in uva pretreatment to explore the mechanism for the significant suppression of reactivation the role of oh on e coli reactivation was examined using mannitol as scavenger as shown in fig 5 after uva pretreatment followed by uvc inactivation the presence of mannitol in the system increased the photoreactivation of e coli as compared to experiments without a scavenger this suggests that oh played an important role in reducing e coli reactivation by uva pretreatment since oh played an important role in inactivation improvement and reactivation suppression following uva pretreatment it was essential to quantitatively determine oh in order to clarify the mechanism this process consisted of two stages including uva pretreatment and subsequent uvc inactivation thus it was necessary to identify at which stage the oh was produced and present before performing the oh measurement mannitol the scavenger of oh was added into the system at different stages in the uva pretreatment followed by uvc inactivation process fig 6 when mannitol was present during the process in entirety e coli inactivation significantly dropped however when adding mannitol after uva pretreatment to confirm its presence during uvc inactivation only e coli inactivation was not affected as compared to experiments without a scavenger this observation suggested that e coli cells were already impacted by oh during the uva pretreatment and were no longer protected if oh was removed during the subsequent uvc irradiation thus it demonstrated that oh was produced and present only during uva pretreatment but not during the following uvc inactivation since oh was produced during the uva pretreatment its concentration in water was indirectly determined using the probe compound cbz the control experiment showed no change of e coli concentration in the presence of cbz fig s8 indicating cbz has no harmful effect to e coli cells during the test period with uva irradiation whether e coli presented in the system or not no degradation of cbz was observed under 365 nm uva irradiation fig s9 indicating no oh present in water considering water in this system only contained uva radiation and e coli cells this observation supports the concept that uva radiation alone was not sufficient to directly generate oh in water unless photocatalysts e g tio2 or chemicals e g h2o2 cl2 natural organic matters for aops were present andreozzi et al 1999 kabra et al 2004 wang and xu 2012 contrastingly scavenger experiments above have demonstrated oh can be generated during uva pretreatment of e coli considering that scavengers can remove the corresponding ros both in water and in cells a lack of oh detection in water indicated that oh radicals were produced and present within the e coli cells during uva irradiation in biological systems oh radicals have high reactivity and very short in vivo half lives of 10 9 s novo and parola 2008 sies 1993 thus oh concentration inside cells cannot be directly measured in order to verify that oh was generated within e coli cells experiments on surface disinfection in the absence of free liquid water were designed and performed as shown in fig 7 e coli on the surface of agar plates were exposed to 4 mj cm2 265 nm uvc irradiation and 1 2 log inactivation was observed when 14 j cm2 365 nm uva pretreatment was applied the same 4 mj cm2 265 nm uvc irradiation achieved 1 6 log inactivation this significant improvement proved that uva pretreatment can also enhance surface disinfection however when mannitol treated e coli cells underwent 14 j cm2 365 nm uva pretreatment 4 mj cm2 265 nm uvc irradiation only resulted in 1 2 log inactivation a level equivalent to no uva pretreatment this observation indicated removal of oh in e coli cells cancelled out the inactivation improvement by uva pretreatment hence these results demonstrated the role of oh in uva pretreatment and verified that oh was generated within e coli cells but not within water 3 5 proposed mechanisms for uva pretreatment of e coli based on the role of ros in e coli inactivation and reactivation during uva pretreatment the mechanisms for uva pretreatment largely relate to biological processes and ros in e coli cells rather than direct photochemical reactions on dna importantly proposed mechanisms must integrate into the broader context of fundamental biology in a biological context ros are formed as natural by products during normal processes of cell metabolism novo and parola 2008 these ros are highly reactive and can react with cellular components such as lipids proteins and dna inducing adverse effects on the cell halliwell 1996 through millions of years evolution organisms have developed multileveled strategies and mechanisms to carefully control the generation of ros and defend against deleterious effects of ros in cells sies 1993 however when cells are exposed to adverse environmental conditions such as uv heat or toxic chemicals the delicately maintained balance of ros in cells can be disturbed cellular ros levels can increase dramatically resulting in significant oxidative damage to cell components impairing cellular functions and even leading to cell death termed oxidative stress devasagayam et al 2004 lushchak 2014 sies 1986 among these ros o2 and h2o2 can be scavenged by efficient enzymatic reactions using superoxide dismutase and catalase respectively contained in most organisms cells as a defensive mechanisms against oxidative stress muller et al 2007 reiter et al 1995 however oh cannot be eliminated through an enzymatic reaction additionally oh has a short half life of 10 9 s with high reactivity thus it does not diffuse from the site of generation and can rapidly react with surrounding molecules oh can damage all types of macromolecules in cells including carbohydrates leading to degradation nucleic acids leading to dna damage lipids leading to cell membranes damage and amino acids leading to proteins denaturation and enzymes inactivation making it a very dangerous radical to the organisms novo and parola 2008 reiter et al 1995 unlike efficient inactivation by uvc radiation uva radiation is poorly absorbed by dna and is inefficient in inducing dna damage for inactivation sinha and hader 2002 usually 105 times more uv fluence of uva than uvc radiation is required to form pyrimidine dimers on dna for inactivation due to low absorption of uva radiation by dna gayan et al 2014 however uva radiation at low uv fluence compared to the high fluence requirement for inactivation has been reported to induce sublethal effects on microorganisms especially e coli this includes growth delay membrane damage protein oxidation decreased energy metabolism and mutation bosshard et al 2010a 2010b eisenstark 1987 girard et al 2011 hoerter et al 2005 oppezzo and pizarro 2001 pizarro 1995 pizarro and orce 1988 ramabhadran and jagger 1976 these sublethal effects are believed to relate to ros and oxidative stress induced by uva cabiscol et al 2000 hoerter et al 2005 smirnova and oktyabrsky 1994 tyrrell and keyse 1990 some endogenous photosensitizers like cytochromes can absorb uva radiation and induce photosensitizing reactions for ros generation cadet et al 2015 sinha and hader 2002 as such by integrating the experimental results in this study within the biological context above mechanisms for uva pretreatment on microorganism inactivation and reactivation can be proposed for e coli a representative bacteria exposure to uva radiation with a certain uv fluence can impact cellular metabolism systems disturbing the ros balance in cells this results in significant increases in ros levels mostly oh to generate oxidative stress the excess ros in cells induce oxidative damage to cellular components such as dna self repair enzymes leading to enzymatic function failure and loss of self repair ability thus e coli becomes increasingly vulnerable to uvc inactivation and is no longer able to reactivate due to an inability to repair uvc induced dna damage resulting in considerable inactivation improvement and significant reactivation suppression on e coli as for ms2 a representative virus exposure to uv radiation only induces photochemical reactions on rna without additional biological effects this is due to ms2 composition namely rna inside a protein coat without a cellular metabolic system madigan et al 2009 therefore no inactivation improvement was achieved on ms2 following application of uva pretreatment and uvc inactivation to the best of our knowledge this is the first study to experimentally demonstrate that ros inside e coli cells are involved in the mechanisms of uva pretreatment on uvc inactivation understanding mechanisms of uva pretreatment are essential to further optimize this method for potential application as uva pretreatment can significantly improve inactivation and eliminate reactivation of e coli the present findings have the potential to reduce both energy consumption and cost in practical application although uv leds are used in this study to facilitate wavelength combinations the beneficial effects of uva pretreatment are not limited to uv leds more uv sources including other uva radiation e g unlimited cost free sunlight could be utilized when designing systems for practical application 4 conclusions based on the special feature on wavelength diversity of uv leds various wavelengths combinations were conducted on the inactivation of different types of microorganisms among them 365 nm uva pretreatment followed by 265 nm uvc inactivation showed interesting effects 365 nm uva pretreatment dramatically improved the e coli inactivation of 265 nm uvc by eliminating shoulder in the fluence response curves this combination also significantly reduced the reactivation of e coli 52 j cm2 365 nm uva pretreatment improved the whole inactivation fluence response curve by 2 2 log inactivation while eliminating the reactivation of e coli afterwards among reactive oxygen species ros oh radicals played an important role on these effects of uva pretreatment on e coli these oh radicals were proved to be produced inside e coli cells during uva pretreatment the mechanisms for effects of uva pretreatment were proposed uva irradiation with a certain fluence can impact the metabolism of bacteria disturb the ros balance in cells resulting in increased ros levels mainly oh radical and oxidative damage to cellular components such as dna self repair enzymes after enzymes function failure and loss of self repair ability bacteria become more vulnerable to uvc inactivation and are no longer able to reactivate afterwards 365 nm uva pretreatment followed by 265 nm uvc inactivation has no additional effect on coliphage ms2 with the uva pretreatment up to 162 j cm2 probably due to minimal metabolic activities in viruses as the effects and mechanisms of uva pretreatment depends on differing microorganisms bacteria of a similar nature are expected to show similar mechanisms further investigation of more microorganisms are needed to better understand the biological effects of uva pretreatment moreover as the proposed mechanisms of uva pretreatment largely involve biological processes such as metabolism and ros in cells further investigation incorporating molecular microbiology is highly encouraged to provide more insightful understanding and optimization of this process for practical applications declaration of competing interest x the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper the authors declare the following financial interests personal relationships which may be considered as potential competing interests n a acknowledgements this research was supported by natural science and engineering research council nserc of canada k song was partially supported by china scholarship council csc appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j watres 2019 114875 
18601,environmental and measure implementation costs are two key factors to be considered by river managers in decision making to balance effects and costs of an action practitioners can rely on diagnostic analysis of presence absence freshwater species distribution models sdms trained to over or underestimating species presence prevalence adjusted model training aims to balance under and overestimation depending on study objectives and training data characteristics the objective of minimising under and overestimation is a typical example of multi objective optimisation moo the aim of this paper is to address for the first time the practice of moo based prevalence adjusted sdm training for freshwater decision management in a numerical experiment the use of pareto based moo specifically the non dominated sorting genetic algorithm ii nsga ii is compared to commonly used single objective optimisation sdms for 11 pollution sensitive freshwater macroinvertebrate species are trained with a subset of the limnodata a large data set holding records in the netherlands over 30 years at 20 000 locations an increase of two to four times is observed for the ability to identify a large range distribution of the solutions in the pareto space when using nsga ii counter to repeated single objective optimisation this by increasing the average runtime with only four percent for a single run in addition the use of nsga ii is found to be effective to identify reliable sdms useful for diagnostic analysis by applying and comparing a broad range of moo methodologies for prevalence adjusted model training we believe a closer collaboration between model developers and freshwater managers can be facilitated and environmental standard limits can be set on a more objective basis in conclusion the use of moo for prevalence adjusted model training is assessed as a valuable tool to support river and potentially all environmental decision making graphical abstract image 1 keywords multi objective optimisation river decision management environmental standard limits species distribution models prevalence adjusted model training non dominated sorting genetic algorithm ii nsga ii 1 introduction setting an environmental standard limit requires balancing environmental and measure implementation costs freshwater managers typically rely on the diagnostic analysis of presence absence species distribution models sdms of freshwater species to weight the effect and implementation costs of an action strakosh et al 2003 by analysing sdms trained to overestimate species presence one can present an optimistic diagnostic lowering implementation costs but increasing risks for deterioration of the freshwater ecosystems in contrast using sdms trained to underestimate species presence can be considered conservative potentially lowering environmental costs while considerably increasing implementation costs gimeno et al 2018 fitting models for freshwater species that approximate the relation between presence absence and the environment remains a challenging practice in species distribution modelling gallardo and aldridge 2018 model fitting involves amongst other challenges the formulation of an objective measure the objective measure quantifies the distance between simulated and observed species occurrence and is used by a training algorithm to identify near optimal models objective functions can be based on very simple metrics for example the number of correctly classified instances to complex ones considering expert and field knowledge allouche et al 2006 bennetsen et al 2016 a major challenge is to deal with a biased sample prevalence in the data as this often affects the reliability and ecological relevance of the generated models mouton et al 2010 when presence and absence data reflect respectively suitable and non suitable habitats models trained with these data will likely produce a good estimate of the species habitat preference however developing a model with imbalanced data can lead to a biased insight in the species preference for example assume a species is difficult to observe and it is often absent from suitable sites training a model with these data can lead to a model that potentially underestimates the true habitat suitability a solution to this issue is to inspect the result of model training as a function of study objectives and sample prevalence i e prevalence adjusted model training in this practice one would train a model and inspect over and underestimation for an obtained set of optimal models as a function of the study objective and sample prevalence i e number of presence over the total number of samples essentially this is equal to optimising a model based on two objectives referred to as multi objective optimisation moo by using moo for prevalence adjusted sdm training overall optimal solutions in relation to degree of underestimation are inspected in relation to degree of overestimation while many papers inspect the effect of sdm training with a single objective referred to as single objective optimisation soo allouche et al 2006 mouton et al 2010 somodi et al 2017 current literature lacks on guidelines how to deal with this challenge in terms of two objectives i e moo the aim of this paper is to fill this gap of knowledge and present moo as a novel way to perform and inspect prevalence adjusted training of sdms specifically for this paper two main objectives are defined the first objective is to obtain a comprehensive insight in the state of the art of prevalence adjusted model training and use of soo and moo in species distribution modelling section 2 1 the second objective is to compare pareto based moo to constrained soo by means of a numerical experiment the theoretical background for this experiment is explained in section 2 2 in the experiment the non dominated sorting genetic algorithm nsga ii see section 3 1 is used to train sdms for 11 macroinvertebrate species the limnodata from the netherlands a data set containing more than three million species records over 20 000 sampling locations spread over 30 years knoben and van der wal 2015 are used to develop the models see section 3 2 and 3 3 the results of the experiment are presented in section 4 and findings of the literature study and experiment are discussed in section 5 2 prevalence adjusted model training in species distribution modelling 2 1 current state currently there are three ways to facilitate prevalence adjusted sdm training a first often used approach is to train an sdm by means of a single objective quantifying the pooled degree of correct estimation of species presence and absence these approaches make use of a single measure for example the number of correctly classified instances the true skill statistic tss the cohen s kappa or area under the receiver operator curve after model training an optimal model is sought by adjusting the model threshold or intercept discriminating between presence and absence jiménez valverde and lobo 2007 lawson et al 2014 this first approach will be referred to as threshold based soo implicitly this action is equal to moving along a pareto curve defined by commission and omission errors omission and commission errors are defined as false negative underestimation or false positive overestimation errors respectively see also table 1 although straightforward this approach provides no information about variability on the estimated species response i e no structurally different models are obtained as a consequence this approach is not suited for decision managers to analyse the change in estimated species response caused by balancing commission and omission errors a second option is to define a single objective and add any additional objective as a constraint i e constrained soo ehrgott 2005 in a number of genetic algorithm for rule set prediction garp case studies training is repeated a number of times and models are selected based on predefined constraints on the commission and omission errors townsend peterson et al 2007 qin et al 2015 since garp is a stochastic optimiser i e genetic algorithm it is hypothesized that after a number of runs a model satisfying the constraints will be obtained a commonly used alternative to the garp approach is to repeat model training each with a different single objective measure or by bootstrapping training data according to a selected prevalence allouche et al 2006 mouton et al 2010 the above mentioned tss the cohen s kappa or area under the receiver operator curve are examples of popular objective measures each weighting commission and omission errors to a certain degree models with varying commission and omission errors are obtained by using different measures to repeat model training although useful success of training depends on a fixed number of criteria or training samples ignoring other values for weighting under and overestimation a third approach aggregation is very rarely used in this approach weights are iteratively assigned to the sub objectives and models are repeatedly trained with an aggregated single objective i e aggregation based moo mouton et al 2009 developed an adjusted average deviation criterion based on aggregation to train fuzzy sdms for the spawning grayling in the aare river switzerland a parameter α ranging from zero to one was defined to weight commission and omission errors training was repeated with a consecutively increased α starting from zero ending with one the authors concluded that more reliable models are obtained with the adjusted criterion because the optimised models describe the trade off between species under and overestimation aggregation is simple yet by using this approach the increase in one objective can be compensated by a loss in another as such the trade off between both objectives can be masked by weighting even more weighting can be considered as subjective and repeating the training for each aggregation is computationally inefficient vrugt et al 2006 efstratiadis and koutsoyiannis 2010 an alternative to aggregation based moo is pareto based moo pareto based moo methods make use of the concept of pareto optimality by searching for the optimal solution to a pareto front during model training kaim et al 2018 in this approach the decision maker defines a preference after the generation of the pareto set by choosing a limited number of solutions from the set pareto based moo methods focus on the use of the identification of the optimal pareto set this allows to identify an ensemble of equally performant but structurally different models given the objectives an advantage over aggregation based methods is their computational efficiency a disadvantage is that it can be difficult to identify the near optimal solution to the pareto front 2 2 prevalence adjusted training as a multi objective optimisation problem moo identifies trade offs by optimising two or more objectives simultaneously this way variability in model structures can be explicitly inspected considering the objectives at hand popular in computer engineering water resource management and environmental modelling moo approaches have shown to deliver increased insights in many problems each characterized by their own specific challenges eiben and smith 2015 the use of the moo framework to inspect prevalence dependency in model training of sdms remains yet unexplored when considering a vector of objective functions j s to be optimised unconstrained moo translates to 1 max s ω j s max j 1 s j 2 s j z s with ω the space of all possible solutions models s that compute j and z being the number of objectives with respect to sdm training a single objective measure is typically used a large share of sdm literature has focussed on the effects of the objective measure on model training key references being manel et al 2001 allouche et al 2006 and mouton et al 2010 generally the tss 1 1 is identified as the most appropriate single objective measure to train sdms and inspect prevalence dependency fukuda and de baets 2016 somodi et al 2017 2 tss sn sp 1 with sn being the sensitivity and sp the specificity these measures quantify the degree of correct estimation of species presence and absence respectively mouton et al 2010 sn t p t p f n 3 sp t n t n f p with t p being the number of true positives f p the number of false positives f n the number of false negatives and t n the number of true negatives in table 1 one can find the confusion matrix defining t p f p f n and t n it is important to note that an increase in sn δ sn will be coupled to a decrease in sp δ sp because ecological data are very rarely linearly separable for example an increase in t p with δ t p will induce an increase in sn t p δ t p t p f n t p t p f n δ t p t p f n in case data are not linearly separable this will induce a δ t n and decrease in sp t n δ t n t n f p t n t n f p δ t n t n f p in case data are linearly separable a δ t p would induce a δ t n of zero from this it follows that training sn and sp involves training of two conflicting measures next it is expected that δ t n and δ t p on the estimated pareto optimal curve are linearly related as the model exists out of a set of piecewise functions with one optimum for every decision variable see section 3 2 it is important to note that minimising commission f p and omission f n errors is equal to maximising sp and sn respectively standard practice in sdm training is to use one sole measure assigning a weight to omission and commission errors each in their unique way after model training commission and omission errors are compared with a priori set threshold and training is repeated until the threshold is satisfied as indicated in section 2 1 this approach is categorized under constrained soo a disadvantage of this approach is the iterative component in addition mostly non stochastic optimisers are used elith and graham 2009 leading to a single optimal model in these cases users typically selects one measure to train their sdm and depending on the results retrain it with another measure in rare cases an ad how pooled measure is defined mouton et al 2009 muñoz mas et al 2017 the risk of using these predefined measures is bias induction in training a way to cope with this issue is to separate the components defining a good estimation of species presence and absence and train them simultaneously see equation 1 4 max s ω j s max j 1 s j 2 s max sn s sp s all models s o p t form an estimated pareto optimal set p o p t a set of models s is said to be pareto optimal if and only if there is no other set of models s for which j s j s 5 p o p t s ω s ω j s j s one way to classify methods as soo or moo is to identify whether a single objective measure is used to train the model standard practice in sdm training is to select a single objective measure available in literature fig 1 left part of flow chart in fig 1 the different colours present structurally different models in terms of habitat suitability to an environmental gradient see lower plots these different models are positioned within the pareto space defined by sn and sp see example pareto plots a b c and d and are assumed to be form a near optimal solution to the pareto front depending on whether structurally different models are required by the end user one can choose to vary the threshold classifying species occurrence threshold based soo a or retrain the model with a different objective so to obtain different models s opt constrained soo in the first case a the obtained ensemble of models rely on the same model structure in the second case b one risks to only identify near optimal solutions in a specific part of the pareto space an alternative to using predefined single objective criteria is to iteratively define and maximise an objective measure j by weighting sn and sp c or to define a vector of objectives j sn sp that have to be maximised in a limited number of training cycle d fig 1 right part of flow chart the choice between aggregation based and pareto based training depends on whether a limited number of training instances can be run respectively no and yes the optimisation is conditioned in a way that models stimulated to over and under estimate are coupled to a species response over a broader or narrower environmental range fig 1 lower three plots in these plots the species response habitat suitability index 0 1 as a function of one environmental gradient in casu river water dissolved oxygen is shown the dotted line represent the limit a decision makers would identify as a lower environmental limit for dissolved oxygen in this case decision makers can balance measure implementation efforts and environmental protection by inspecting models with a narrower and broader environmental range respectively shown in the lower right orange and lower left panel green another way to classify methods as either soo and moo for prevalence adjusted model training is to identify how model selection is positioned in the model development workflow in the case of moo model training is followed by model selection based on expert knowledge of end users for soo model selection is done during model training based on partial results and an iterative selection process 3 methodology 3 1 the non dominated sorting genetic algorithm ii a pareto based moo method specifically the nsga ii approach of deb et al 2002 is implemented and compared to a constrained soo approach to test the ability to identify a near optimal solution to the pareto front nsga ii is a fast and elitist multi objective evolutionary algorithm used to propagate a population of candidate solutions to a pareto front defined by a set of objectives genetic algorithms make use of concepts observed in natural evolution such as mutation crossover and selection to find near optimal solutions to the pareto curve the solutions to an moo problem are encoded in chromosomes the algorithm assigns fitness values to these chromosomes based on a non dominated sorting approach non dominated solutions are equally valued solutions to a pareto front the non dominated sorting approach aims to preserve diversity among the non dominated solutions making use of crowding distance the crowding distance is a measure of solution density that is used in the selection operator the less close a solution is to another solution the higher the chance for selection nsga ii is chosen as it is a fairly easy to comprehend pareto based moo algorithm in addition it has proven to be an adequate algorithm to solve moo problems also in water resource management e g dotto et al 2012 and gimeno et al 2018 it is important to note that other algorithms are available such as decomposition based or aggregation based moo algorithms the use of nsga ii will be compared to these other algorithms in section 5 1 a detailed explanation of how the nsga ii algorithm works and how it is implemented can be found in supportive information 1 in the next section it is explained how the solutions s are implemented in the chromosomes genome and which model structures lie at the basis for generating these solutions 3 2 model structure and chromosome encoding in this section it is explained how the solutions s are implemented in the chromosomes of nsga ii in addition the model structure of the sdms generating the solutions s is formulated sdms are developed for 11 macroinvertebrate species in this research macroinvertebrates are selected as the group of target species as their different response to environmental pressures is typically analysed to support freshwater decision management lock et al 2011 the sdms are developed based on the conceptual model presented by guisan and rahbek 2011 and bennetsen et al 2016 in the remainder of this section the model formulation is shortly summarized the habitat suitability index hsi is used as a measure to express the suitability of the habitat shaped by abiotic conditions 6 hsi k j 1 m si x k j 1 m with hsi k being the habitat suitability index of the data point k total of n data points si x k j is the suitability index calculated by applying the species response curve for the input value of record k for variable j total of m variables see equation 7 the si is calculated by taking the logit of a linear function brewer et al 2016 and fig 2 7 si x k j 0 if x k j θ 1 2 1 exp p 1 x k j p 2 if x k j θ 1 θ 2 1 if x k j θ 2 θ 3 2 1 exp p 3 x k j p 4 if x k j θ 3 θ 4 0 if θ 4 x k j with p 1 l o g 2 ε ε θ 2 θ 1 p 2 l o g 2 ε ε θ 3 θ 4 p 3 p 1 θ 2 p 4 p 2 θ 3 the values for p are found by assuming the curve is bound by four parameters θ the parameters θ 2 and θ 3 describe the optimal range si 1 parameters θ 1 and θ 4 describe the suboptimal conditions and the total range ε is the minimal possible si in the range and is set to 0 01 smaller values for ε are considered yet these did not considerably affect the shape of the logistic curves this type of piece wise curve is considered as a good option to describe distributions of species responses observed in many training data brewer et al 2016 it is important to note that other shapes can be assumed depending on the ecological reasoning used for the study a simple practice to encode the solutions s in the chromosomes genome is to consider a fixed chromosome length for example the encoding for input variable selection is done by assigning a zero or one for respectively an exclusion and inclusion of the input variable in case of two variables being implemented in the chromosomes genome three solutions can be obtained 8 ω 0 1 1 1 1 0 in this study the aim is to implement parameter values and in or exclusion of input variables since specific parameters are coupled to specific input variables a variable length encoding gobeyn and goethals 2018 is used in variable length encoding the genome is programmed as a list of lists where a second order list composed out of four continuous values θ is defined when a bit of the first order string has the value of one in an example for two variables the solution space is defined by 9 ω 0 θ variable 2 θ variable 1 θ variable 2 θ variable 1 0 it is important to note that the values in θ are encoded as continuous values thus the size of ω will vary according to the number of computational bytes used to implement a continuous value in supportive information 1 an in depth explanation of the encoding is provided along with the implemented crossover and mutation operators 3 3 training data limnodata the limnology neerlandica limnodata data set knoben and van der wal 2015 and http www stowa nl is used to obtain a one on one in time and space coupled data set on the freshwater species environment relation supportive information 2 in total the data set contains over three million records covering 30 years over more than 20 000 sampling locations from this data set all ephemeroptera plecoptera and trichoptera ept macroinvertebrate species are extracted because these species are sensitive to pollutants in the stream environment verberk et al 2012 biological and abiotic data are processed and 11 ept macroinvertebrate species are selected based on number of occurrence greater than 30 the limnodata was selected because it is the largest open data set available in europe see https doi org 10 15468 ennulm that contains one on one coupled biological and physico chemical data in addition the limnodata are one of the only data sets containing data on a species resolution making the analysis of species response curves robust another key question for an efficient model development and application is what to include in sdms depending on available staff budget and time frame efforts can be categorized in four elements the selection of input variables parameters processes and output variables goethals et al 2018 concerning relevance of input and output variables an advantage of the limnodata is that both species and physico chemical data are collected in a standardized manner which is of major importance for model development everaert et al 2014 this long history of standardized sampling in the netherlands increases the change for having more records of species sampled in extreme conditions e g very low oxygen concentration in addition the distribution of the input features available in the limnodata will be quasi similar compared to other studies i e positively skewed for pollutants bennetsen et al 2016 even more sample prevalence of the species are comparable to those reported in other studies for example low prevalence in case of baetidae finally the format of the data set complies to the standards set by the european water framework directive consequently the trained models are potentially highly relevant for river management in the netherlands for details about how data exploration and cleansing was performed one is referred to supportive information 2 4 results 4 1 multi versus single objective optimisation in this section the moo approach is compared to a constrained soo approach sensitivity and specificity are optimised in case of moo while the tss is used as objective function for the soo to perform moo nsga ii is employed whereas for soo a tournament selection based on one fitness function no non dominated sorting is used as implemented in simple genetic algorithms the mutation and crossover operators are equal a threshold of 0 5 is selected to avoid that different values for classification are obtained by varying the threshold the threshold maximising the tss could also be selected however this would lead to balancing sensitivity and specificity by means of changing a threshold in this paper the aim to identify alternative structures for the sdms and not thresholds classifying occurrence from suitability see threshold based soo section 2 1 in addition the threshold of 0 5 has been assessed as adequate to classify occurrence when using the geometric mean for the model structure fukuda et al 2011 and equation 6 training is repeated a number of times to track the difference in behaviour between soo and moo this way the soo approach is comparable to the repeated training methodology used in garp constrained soo see section 2 1 the hypervolume h v indicator zitzler and thiele 1999 is calculated to obtain an indication about average of and variability on nsga ii peformance this indicator estimates the portion of the objective space which is dominated by a model set s opt the indicator accounts for diversity and convergence of the estimated solutions elarbi et al 2017 higher values indicate a better result for the calculation the 0 0 point was selected as a reference point cao et al 2015 to obtain an indication about the range of the solutions distribution in the pareto space obtained with soo and moo the maximum of the euclidean distance d between every two points in the pareto space is computed the distance values are normalized by the maximum possible distance between two points in the pareto space being equal to 2 with this indicator an ensemble of models describing the whole range of over and underestimation is assigned a value of one in table 2 the mean and standard deviation on h v are shown for each species in addition d is tabulated for 5 25 and 100 repeated training cycles of soo and moo the results for the multi and single objective optimisation of sdms for cloeon dipterum are shown in fig 3 respectively in the left and right panels results are shown for five upper 25 middle and 100 lower panels training cycles using the same training data nsga ii is able to identify different trade offs between sensitivity and specificity in addition nsga ii is assessed as superior to the soo approach in identifying this trade off by repeating the analysis five times the multi objective approach is able to delineate a set of models enclosing a trade off between of sn and sp values this is also observed in the maximum distance d being close to one table 2 in contrast the single objective approach identifies in five runs a number of models in a limited area of the sensitivity specificity space upper right panel d 0 31 a similar pattern is observed when the training is repeated 25 times for the moo approach a large number of solutions are found in the upper left corner of the pareto space middle left panel this observation is confirmed for 100 training cycles the lower left panel it is concluded that for cloen dipterum training with moo results in a set of a solutions within a broad range of the pareto space while training with soo leads a narrower range even when training is repeated many times when using moo instead of soo d converges faster to a value of one with an increased number of training cycles after five training cycles distance d is higher than 0 7 in 10 of the 11 cases when using moo when using soo a value of 0 5 or higher is obtained for only two cases after 100 repeated training instances visual inspection of the results for the other species shown in supportive information 3 confirm this pattern after five repeated runs the range of distribution quantified by d is estimated approximately four times larger for moo μ d 0 82 compared to soo 0 18 after 100 training cycles this value decreases to approximately three 0 97 versus 0 35 and two when compared to five training cycles of moo 0 82 versus 0 35 it shows that repeated soo training with a stochastic optimiser results in solutions positioned within a limited range of the near optimal pareto front when inspecting the optimal solutions it is observed that the pooled accuracy expressed in tss obtained with single optimisation is in some cases a bit higher than the one obtained with moo this can be observed in fig 4 lower panels when comparing the points close to the tss isoline of 0 6 the average h v varies over the different species being the highest for baetis rhodani and lowest for cloeon dipterum most values for the average h v are within the boundary of 0 6 and 0 8 the variability of h v over the different species is comparable for the species anabolia nervosa it is difficult for the single and multi objective approach to identify solutions that estimate species presence very well see fig 4 after five training cycles the moo approach is not able to identify models with a sensitivity higher than 0 9 for the soo case no models have a sensitivity above 0 75 after 25 cycles the multi objective approach is able to identify a number of models estimating species presence well similarly as for five training cycles the soo approach is not able to identify models with a high sensitivity in case of 25 and even a 100 training cycles in general a bias is observed toward the identification of models that predict absence very well but presence poorly figs 3 and 4 and supportive information 3 this implies that solutions with a higher specificity i e correct estimation of absence are more easily found than solutions with a high sensitivity when inspecting the model formulation with the geometric mean as aggregation function one would expect that filtering gets a higher weight in other words if the range of one species response curves is narrow then this will lead to a higher chance for absence especially for a large number of input variables in conclusion the multi objective approach is able to identify a set of sdms presenting a trade off between sensitivity and specificity yet there is a strong bias present towards identifying models with a high specificity the soo approach is less efficient in identifying this trade off however it is able to identify models with a slightly higher tss when compared to the moo approach 4 2 ecological relevance the estimated species response curves found with the nsga ii approach for baetis rhodani a and cyrnus flavidus b are shown in fig 5 to obtain these results the experiment presented in section 4 1 is repeated but now with bootstrap samples of the training data bootstrap sampling is used to obtain an insight in the variability on the species response curves see light grey colour fig 5 an average species response curve of the best 10 models based on tss is shown with the black solid line an average response curve computed from models with a high specificity and sensitivity are shown in blue and orange respectively in this text sdms with sn sp sp sn are named sensitivity specificity models for the three classes of models an average response is computed by taken the median response the support shows the number of times a variable is included in the subset of models only species response curves are shown for variables with a support higher than 25 in addition the median model complexity m expressed as the number of parameters is indicated between brackets in the legend sdms located in different areas of the pareto space have a different model structure for baetis rhodani panel a fig 5 the response curves of sensitivity models describe on average a larger range than specificity models see transparency dissolved oxygyen do and nitrite no2 n in addition less variables are selected as explanatory when comparing the sensitivity to the specificity models an average of four variables number of parameters divided by four similar conclusions are drawn for cyrnus flavidus fig 5 panel b the sub optimal range of the response curves is broader for a number of variables in addition model complexity is on average lower for sensitivity sdms similar patterns are observed for other species in supportive information 4 the average species response curves comply with the model assumptions made in the conceptual model i e they have a clear optimum and can be asymmetric some ill defined average curves are identified see transparency and sulfate so4 however note that these ill defined curves are a result of computing a median response from all curves as such it shows that the optimum of the identified curves varies in the domain of possible values in order to check agreement of the results with ecological knowledge the model structures are compared with the environmental and habitat preference data set published by verberk et al 2012 the preference towards saproby eutrophy and acidity are extracted because they express preference towards oxygen saproby organic carbon saproby nutrient eutrophy saproby concentrations and ph acidity table 3 these preferences are fuzzy coded adding up to ten and indicate a degree of membership to a specific class this information is derived from expert knowledge for example for the species baetis rhodani the preference for eutrophic conditions is expressed in a very high value 9 this indicates that the species has a very high preference for these eutrophic conditions the situation for cloeon dipterum is different as the values are spread over the mesotrophic meso eutrophic and eutrophic class the species has a preference for eutrophic conditions yet less specific in contrast to baetis rhodani note that for this analysis the 10 best models based on tss are used to interpret the results a number of identified models show compliance with the extracted information dissolved oxygen is estimated to be an important variable based on support steering species occurrence eight out of 11 sdms have a support above 50 see supportive information 4 black bar graphs in addition the estimated responses show that oxygen levels for the species should generally be higher than 3 mg o2 l 1 see for example fig 5 this resembles to the observation in table 3 that most species are identified as beta or alpha mesosaprobe species do resp 6 8 and 2 6 mg o2 l 1 the preference for saproby indicates that the species cyrnus flavidus also has a preference for oligosaprobic conditions membership 3 oligosaprobe do 8 mg o2 l 1 bod 1 mg o2 l 1 this preference is confirmed by a high support and a rather narrow optimal range observed for the variable bod5 1 mg o2 l 1 fig 5 b however this preference is not reflected in do since the lower boundary is defined at approximately 2 4 mg o2 l 1 which is lower than 8 mg o2 l 1 here it is important to note that the support for dissolved oxygen for cyrnus flavidus is relatively low i e 40 in addition species limnephilus lunatus also has a preference towards oligosaprobic conditions and this is reflected in the response curve for dissolved oxygen 7 mg o2 l 1 see supportive information 4 the variable ph generally has a high support 10 of the 11 species have a support for ph above 50 see supportive information 4 the range for transparency for the species baetis rhodani fig 5 is limited to a range characterized by low transparency support 90 when inspecting table 3 one observes that this species has a specific preference for eutrophic conditions characterized by high nutrient concentrations 2 2 mg n l 1 and 0 15 mg p l 1 therefore the assumption is made that the preference for eutrophic conditions is reflected in the variable transparency in order to investigate this link the median total nitrogen and phosphorus for lower 0 5 m and higher 0 5 m transparencies are compared with the mood s median test the test shows that total nitrogen and phosphorus concentrations are on average expressed in median significantly higher for low than for high transparencies resp p value 5 7 10 48 and 1 9 10 46 5 significance this link between eutrophication and transparency could thus be explained by the observed preference for low transparencies of the species baetis rhodani in contrast this assumption is not confirmed by looking at individual components reflecting nutrient enrichment p and n variables when inspecting the species baetis vernus and hydropsyche angustipennis supportive information 4 one can draw similar conclusions as for baetis rhodani in conclusion the above results show that the data driven sdms are in line with ecological knowledge however it is not possible to clearly link each preference with all obtained species response curves and model structures 5 discussion 5 1 pareto based moo with nsga ii the aim of this paper is to present and use moo as a novel way to perform prevalence adjusted sdm training to this end nsga ii a pareto based method is used to quantify differences with a soo method in this experiment the soo method relates to the methodology used in many garp papers the use of nsga ii for prevalence adjusted training is assessed to be more effective than the use of soo because the aims of using nsga ii moo in general better comply with the aims of using prevalence adjusted model training provide a full overview of structurally different models equally valued in the pareto space defined by over and underestimation from section 4 1 it is shown that soo can comply with these aims yet it is assessed as less efficient and tractable another key element to assess moo as more effective is the selection of the best model options for moo these are selected after model training by end users allowing them to focus on the diagnostic analysis of the obtained model structure in contrast model selection with soo is based on partial results and iterative model training this requires end users to be sufficiently aware of training options and influence of specific objective measures in order to steer model selection with respect to nsga ii it is observed in this study that the method was biased towards identifying models with a high specificity in addition the moo approach was able to identify less accurate sdms in term of tss than soo the differences between the fitness assignment and chromosome selection procedure in nsga ii and the soo method explain the lower accuracy obtained with nsga ii the remained of this section explains why this bias is observed and what mechanisms lead to a lower accuracy in addition alternative methods are discussed the solutions in the pareto space showed to be biased towards the correct estimation of species absence to inspect if there is a link with sample prevalence the sensitivity and specificity of the sdms were inspected as a function of the prevalence results not shown here a declining trend of the mean sensitivity and specificity with increasing sample prevalence was delineated the mean specificity was higher than sensitivity suggesting that patterns are more noticeable present in the absence than in the presence data furthermore the difference between mean sensitivity and specificity became smaller with increasing prevalence this would suggest there is an effect of prevalence in the way the algorithm exploits areas in the search space which offer less resistance this bias can be explained by dominance resistant solutions which are solutions with a near optimal value in one or more objective s but a poor value in the remaining objective s these solutions are identified as non dominant far from the optimum in the pareto space here the 1 1 point and grow with the number of objectives since they are identified as non dominant in the first front these solutions can cause a bias in the identified final front jaimes and coello coello 2015 a potential solution to this issue is to use users preference to bias search to specific regions in pareto space fernandez et al 2019 with respect to nsga ii a modified fitness sharing mechanism has been developed by deb 2011 to obtain pareto optimal solutions in a desired region also for novel pareto based methods such as decomposition based algorithms user preference has been integrated in the algorithms search mechanism li et al 2017 mohammadi et al 2012 for river management a step forward in combining modellers and decision managers their expertise is to define the position and range of this preference guisan et al 2013 the nsga ii implementation for the moo is assessed as fairly efficient in simultaneously optimising sensitivity and specificity nsga ii is a long established algorithm with a number of advantages it is robust it is easy to comprehend and it requires little computational resources zambrano vega et al 2016 cui et al 2017 however the performance of the approach is potentially less satisfying for difficult or more complex problems deb and jain 2013 reed et al 2013 jaimes and coello coello 2015 in this study the approach was used to balance two objectives and therefore the nsga ii is assessed as effective yet it is important to mention that both effectiveness and efficiency of nsga ii can enhanced by considering alternatives such as an improved non dominated sorting method tian et al 2017 two notable disadvantages of using nsga ii are the risk of losing the best solutions during optimisation and the possibility of premature convergence the first limitation is caused by using the crowding distance in the tournament selection operator although this approach is used to provide a good diversity amongst solutions it can exclude well performing solutions from dense areas cui et al 2017 this exclusion is a notable limitation of nsga ii and can explain why the algorithm identifies slightly less accurate solutions than the soo approach the second limitation premature convergence can cause that the optimal solution to the pareto is not found within the time frame the guidelines of gibbs et al 2008 are used to determine the algorithm settings the robustness of the guidelines of gibbs et al 2008 for nsga ii was tested in a number of pilot runs in this study from these experiments it was concluded that the hyper parameters were near optimal as such the results indicated that the algorithm did converge and that the used guidelines are appropriate for this case study it is concluded that nsga ii can thus be used for prevalence adjusted model training given that future studies focus on increasing search efficiency with nsga ii or alternative pareto based methods this way it should be possible to obtain with moo equally accurate results in the single dimension tss than those obtained with soo besides domination based multi objective evolutionary algorithms other pareto based methods such as indicator and decomposition based algorithms exist indicator based algorithms use performance metrics to improve the efficiency of the selection operator decomposition based algorithms split the moo problem in different scalar sub objectives based on a set of weights elarbi et al 2017 in the latter approach the sub objectives are optimised simultaneously and cooperatively zhang and li 2007 decomposition based methods have a number of strengths high selection pressure toward the pareto front 1 easy to work with local search operators 2 efficient to solve many objective optimisation problems 3 and find solutions to irregular pareto fronts 4 also in water resource management decomposition algorithms have shown to be very efficient zheng et al 2014 ishibuchi et al 2017 with respect to the optimisation problem presented in this paper a decrease of computational resources and better identification of the pareto front are two advantages motivating the future exploration of decomposition based algorithms in addition they circumvent the problem of dominance resistant solutions discussed above by not relying on pareto dominance santos and takahashi 2018 consequently they are evaluated as highly competitive alternatives to nsga ii to deal with prevalence adjusted sdm training as such the value of decomposition algorithms for prevalence adjusted sdm training should be researched in future studies 5 2 evaluation of multi objective optimisation for prevalence adjusted model training in section 2 1 approaches to perform prevalence adjusted model training were categorized into four classes threshold based soo constrained soo aggregation based moo and pareto based moo the former two are typically applied within the field of species distribution modelling and are valid approaches when habitat suitability is reflected in species occurrence in the training data threshold based soo can be used in case no prevalence adjusted training is required and when the obtained models are well performing examples of studies with a satisfactory accuracy area under the receiver operator curve 0 7 can be found for freshwater species fukuda et al 2013 as well as for birds and plants townsend peterson et al 2007 elith and graham 2009 yet other studies show that the accuracy of freshwater sdms can be unsatisfactory after optimisation bennetsen et al 2016 in case of an unsatisfactory performance and or in case of imbalanced training data prevalence adjusted model training is a good option to allow experts and river managers to evaluate models mouton et al 2009 threshold based soo methods are often not the preferred option as river managers require structurally different models for environmental limit setting and evaluation in section 4 2 it was shown that structurally different models can be obtained by employing moo the use of constrained soo based on repeated training with a stochastic optimiser can be considered as an alternative to obtain structural different models see section 4 1 even more it was observed that training with soo can result in better pooled accuracy in terms of tss compared to pareto based moo a disadvantage of soo is the limited range of the solutions distribution within the pareto space five times lower after five repeated runs compare d of 0 82 to 0 18 making the approach less attractive for prevalence adjusted model training consequently the use of constrained soo is advised when a high accuracy can be obtained and a prevalence adjusted training approach is not needed to meet the study objectives an alternative to constrained soo is to make use of aggregation based moo mouton et al 2009 in aggregation based moo sdms are repeatedly trained with varying aggregated single objectives obtained by an iterated weighting procedure ishibuchi et al 2017 it is noteworthy that the computational efficiency of pareto based moo is estimated to be higher in order to test the computational efficiency training was repeated 10 times with soo and moo using the same training data an average runtime of 518 16 and 537 seconds 69 was obtained for soo and moo respectively four cpus with a clock rate of 1 90 ghz this is an increase of four percent in runtime for moo compared to soo in this case the use of the nsga ii using one training cycle is preferred over repeated training with a single objective optimiser relying on multiple training instances to compute an estimate of the pareto optimal front with increasing computational demands for performing experiments with newly developed sdms for example sdms based on individual based models see bruneel et al 2018 it will be interesting to inspect what the potential role of aggregation and pareto based methods is for prevalence adjusted model training one is advised to not make use of aggregation based moo in combination with stochastic optimisers garp bootstrapping or cross validation as the stochastic component in training makes the approach increasingly computational intractable in these cases practitioners are advised to resort to pareto based methods or to use deterministic methods to increase tractability 5 3 application of sdms for freshwater decision management and environmental limit setting in this study sdms are developed so they can be used as diagnostic tools to support environmental limit setting the evaluation of the sdms showed that the models can be used for system understanding however inspection of the obtained model structure section 4 2 also reveal that different model structures can simulate similar presence patterns for a number of inputs this is not unexpected as each response function is based on the same model assumptions section 3 2 thereby the effect of highly influential parameters and or input variables on the model output can be cancelled or compensated by another input variable and or parameter to avoid this stricter boundary conditions can be defined by using expert knowledge van broekhoven et al 2006 process based modelling can also be of use if sufficient qualitative data are available to identify model parameters beale and lennon 2012 bruneel et al 2018 the results show that the optimisation of the sdms does not necessarily lead to satisfying models suitable to inspect by river managers for example the performance of the sdms developed for cloeon dipterum could be further improved the efficiency cannot only be increased by applying a more efficient optimisation algorithm but also by relying on more qualitative training data fukuda and de baets 2016 as indicated in the introduction training data on species presence and absence should reflect habitat preference which is often not the case a reason for the choice of the dutch limnodata was its coverage towards species presence in extreme conditions aiming to obtain a better insight of these preference in extreme conditions however habitat conditions linked to species presence and absence should be observed more adequately by investing in time series observations rather than point observations this remains a big challenge in river water quality studies as the cost for the maintenance of in situ and remote networks are considerable alilou et al 2018 potentially the use of internet of things applications can increase temporal and spatial coverage resulting in more reliable training data as indicated in section 3 3 it is expected that the develop method will be transferable to other case studies because the characteristics of the currently available freshwater data sets physico chemical input data and species prevalence are comparable however future practical studies should confirm whether this assumption is valid a key element in this research is the link between on the one hand conservation measure and environmental costs and on the other the use of sdms describing varying environmental ranges in which the species are able to survive donoso et al 2018 the use of sdms trained to overestimate species presence will describe a larger range which will reflect in an optimistic view on measure implementation and environmental costs in contrast a conservative interpretation will be reflected by using sdms trained to underestimate species absence the developers of these presence absence models are required to present an unbiased view of possible model options by treating the trained sdms as model hypothesis the opportunity is created for modellers to consult expert and decision managers and use their expertise to further refine sdms jarnevich et al 2015 the role of prevalence adjusted model training is to present these model options and facilitate a closer collaboration between model developers and end users in order to meet study objectives even though effective use of sdms for decision management is found in literature guisan et al 2013 urge to conduct more practice oriented case studies to increase reliability they state that more decision makers should make use of these models and provide feedback guisan et al 2013 also recommend that model developers should obtain a better insight in the decision makers process example of a studies employing this philosophy are presented by van broekhoven et al 2006 and bennetsen et al 2016 they mainly aim to further fine tune requirements of sdms for river management by relying on expert knowledge the interaction between developers and users is assumed to be key to achieve more reliable models mouton et al 2009 presenting a number of choices to stakeholders cannot only increase transparency but also enhance trust in the developed approaches an important aspect is to consider how these pareto optimal trade off curves can be used by decision makers to choose the most appropriate alternative esmail and geneletti 2018 a key element is the reduction of the number of potential solutions that have to be considered by the decision makers ascough et al 2008 in a final note environmental and conservation costs are not considered explicitly in this paper since costs are not linearly related to shifting an environmental limit it is important to note that the axis of the pareto space do not explicitly reflect costs therefore future research could focus on considering these costs coupled to environmental limit targets and the consequence of model selection and use 6 conclusion prevalence adjusted model training is an important tool for river managers to analyse species distribution models sdms in essence prevalence adjusted model training can be translated to multi objective optimisation moo dealing with two conflicting objectives the minimisation of under and overestimation of species presence in this study four classes of approaches being threshold based single objective optimisation threshold based soo constrained soo aggregation based moo and pareto based moo are identified one study that makes use of aggregation based moo could be identified in literature whereas all other sdm studies make use of soo to identify an ensemble of models each weighting species over and underestimation for the first time a pareto based moo was applied to perform prevalence adjusted training this approach was compared with an soo approach and was found to be two to four times more efficient in identifying a wide range set of pareto optimal models with only a four percent increase in runtime per training in addition the use of nsga ii is found to be effective to identify reliable sdms useful for diagnostic analysis this way we believe stakeholders can focus on prominent questions how well do our models perform in estimating species presence and absence and what is the trade off between those objectives how does this trade off relate to the properties of the data the model formulation and the species characteristics in addition which expert knowledge is needed to further improve sdms finally what environmental limits will be considered and how do they relate to conservation measure and environmental costs this way prevalence adjusted trained sdms can support river and potentially all environmental decision management acknowledgments the authors thank peter van puijenbroek and jan janse pbl netherlands environmental assessment agency for providing the data and explanation of the limnodata data set in addition the authors would like to thank ans mouton ghent university ugent bernard de baets ugent stijn van hoey research institute for nature and forest inbo jana van butsel ugent and tijs d hulster for comments on earlier versions of this manuscript we also wish to express gratitude to the reviewers and associate editor who reviewed this work and allowed us the opportunity to improve the paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j watres 2019 114863 author contribution s g conceived the idea designed the methodology analysed and interpreted the data and led the writing of the code and the manuscript p g contributed to interpretation and writing of main text 
18601,environmental and measure implementation costs are two key factors to be considered by river managers in decision making to balance effects and costs of an action practitioners can rely on diagnostic analysis of presence absence freshwater species distribution models sdms trained to over or underestimating species presence prevalence adjusted model training aims to balance under and overestimation depending on study objectives and training data characteristics the objective of minimising under and overestimation is a typical example of multi objective optimisation moo the aim of this paper is to address for the first time the practice of moo based prevalence adjusted sdm training for freshwater decision management in a numerical experiment the use of pareto based moo specifically the non dominated sorting genetic algorithm ii nsga ii is compared to commonly used single objective optimisation sdms for 11 pollution sensitive freshwater macroinvertebrate species are trained with a subset of the limnodata a large data set holding records in the netherlands over 30 years at 20 000 locations an increase of two to four times is observed for the ability to identify a large range distribution of the solutions in the pareto space when using nsga ii counter to repeated single objective optimisation this by increasing the average runtime with only four percent for a single run in addition the use of nsga ii is found to be effective to identify reliable sdms useful for diagnostic analysis by applying and comparing a broad range of moo methodologies for prevalence adjusted model training we believe a closer collaboration between model developers and freshwater managers can be facilitated and environmental standard limits can be set on a more objective basis in conclusion the use of moo for prevalence adjusted model training is assessed as a valuable tool to support river and potentially all environmental decision making graphical abstract image 1 keywords multi objective optimisation river decision management environmental standard limits species distribution models prevalence adjusted model training non dominated sorting genetic algorithm ii nsga ii 1 introduction setting an environmental standard limit requires balancing environmental and measure implementation costs freshwater managers typically rely on the diagnostic analysis of presence absence species distribution models sdms of freshwater species to weight the effect and implementation costs of an action strakosh et al 2003 by analysing sdms trained to overestimate species presence one can present an optimistic diagnostic lowering implementation costs but increasing risks for deterioration of the freshwater ecosystems in contrast using sdms trained to underestimate species presence can be considered conservative potentially lowering environmental costs while considerably increasing implementation costs gimeno et al 2018 fitting models for freshwater species that approximate the relation between presence absence and the environment remains a challenging practice in species distribution modelling gallardo and aldridge 2018 model fitting involves amongst other challenges the formulation of an objective measure the objective measure quantifies the distance between simulated and observed species occurrence and is used by a training algorithm to identify near optimal models objective functions can be based on very simple metrics for example the number of correctly classified instances to complex ones considering expert and field knowledge allouche et al 2006 bennetsen et al 2016 a major challenge is to deal with a biased sample prevalence in the data as this often affects the reliability and ecological relevance of the generated models mouton et al 2010 when presence and absence data reflect respectively suitable and non suitable habitats models trained with these data will likely produce a good estimate of the species habitat preference however developing a model with imbalanced data can lead to a biased insight in the species preference for example assume a species is difficult to observe and it is often absent from suitable sites training a model with these data can lead to a model that potentially underestimates the true habitat suitability a solution to this issue is to inspect the result of model training as a function of study objectives and sample prevalence i e prevalence adjusted model training in this practice one would train a model and inspect over and underestimation for an obtained set of optimal models as a function of the study objective and sample prevalence i e number of presence over the total number of samples essentially this is equal to optimising a model based on two objectives referred to as multi objective optimisation moo by using moo for prevalence adjusted sdm training overall optimal solutions in relation to degree of underestimation are inspected in relation to degree of overestimation while many papers inspect the effect of sdm training with a single objective referred to as single objective optimisation soo allouche et al 2006 mouton et al 2010 somodi et al 2017 current literature lacks on guidelines how to deal with this challenge in terms of two objectives i e moo the aim of this paper is to fill this gap of knowledge and present moo as a novel way to perform and inspect prevalence adjusted training of sdms specifically for this paper two main objectives are defined the first objective is to obtain a comprehensive insight in the state of the art of prevalence adjusted model training and use of soo and moo in species distribution modelling section 2 1 the second objective is to compare pareto based moo to constrained soo by means of a numerical experiment the theoretical background for this experiment is explained in section 2 2 in the experiment the non dominated sorting genetic algorithm nsga ii see section 3 1 is used to train sdms for 11 macroinvertebrate species the limnodata from the netherlands a data set containing more than three million species records over 20 000 sampling locations spread over 30 years knoben and van der wal 2015 are used to develop the models see section 3 2 and 3 3 the results of the experiment are presented in section 4 and findings of the literature study and experiment are discussed in section 5 2 prevalence adjusted model training in species distribution modelling 2 1 current state currently there are three ways to facilitate prevalence adjusted sdm training a first often used approach is to train an sdm by means of a single objective quantifying the pooled degree of correct estimation of species presence and absence these approaches make use of a single measure for example the number of correctly classified instances the true skill statistic tss the cohen s kappa or area under the receiver operator curve after model training an optimal model is sought by adjusting the model threshold or intercept discriminating between presence and absence jiménez valverde and lobo 2007 lawson et al 2014 this first approach will be referred to as threshold based soo implicitly this action is equal to moving along a pareto curve defined by commission and omission errors omission and commission errors are defined as false negative underestimation or false positive overestimation errors respectively see also table 1 although straightforward this approach provides no information about variability on the estimated species response i e no structurally different models are obtained as a consequence this approach is not suited for decision managers to analyse the change in estimated species response caused by balancing commission and omission errors a second option is to define a single objective and add any additional objective as a constraint i e constrained soo ehrgott 2005 in a number of genetic algorithm for rule set prediction garp case studies training is repeated a number of times and models are selected based on predefined constraints on the commission and omission errors townsend peterson et al 2007 qin et al 2015 since garp is a stochastic optimiser i e genetic algorithm it is hypothesized that after a number of runs a model satisfying the constraints will be obtained a commonly used alternative to the garp approach is to repeat model training each with a different single objective measure or by bootstrapping training data according to a selected prevalence allouche et al 2006 mouton et al 2010 the above mentioned tss the cohen s kappa or area under the receiver operator curve are examples of popular objective measures each weighting commission and omission errors to a certain degree models with varying commission and omission errors are obtained by using different measures to repeat model training although useful success of training depends on a fixed number of criteria or training samples ignoring other values for weighting under and overestimation a third approach aggregation is very rarely used in this approach weights are iteratively assigned to the sub objectives and models are repeatedly trained with an aggregated single objective i e aggregation based moo mouton et al 2009 developed an adjusted average deviation criterion based on aggregation to train fuzzy sdms for the spawning grayling in the aare river switzerland a parameter α ranging from zero to one was defined to weight commission and omission errors training was repeated with a consecutively increased α starting from zero ending with one the authors concluded that more reliable models are obtained with the adjusted criterion because the optimised models describe the trade off between species under and overestimation aggregation is simple yet by using this approach the increase in one objective can be compensated by a loss in another as such the trade off between both objectives can be masked by weighting even more weighting can be considered as subjective and repeating the training for each aggregation is computationally inefficient vrugt et al 2006 efstratiadis and koutsoyiannis 2010 an alternative to aggregation based moo is pareto based moo pareto based moo methods make use of the concept of pareto optimality by searching for the optimal solution to a pareto front during model training kaim et al 2018 in this approach the decision maker defines a preference after the generation of the pareto set by choosing a limited number of solutions from the set pareto based moo methods focus on the use of the identification of the optimal pareto set this allows to identify an ensemble of equally performant but structurally different models given the objectives an advantage over aggregation based methods is their computational efficiency a disadvantage is that it can be difficult to identify the near optimal solution to the pareto front 2 2 prevalence adjusted training as a multi objective optimisation problem moo identifies trade offs by optimising two or more objectives simultaneously this way variability in model structures can be explicitly inspected considering the objectives at hand popular in computer engineering water resource management and environmental modelling moo approaches have shown to deliver increased insights in many problems each characterized by their own specific challenges eiben and smith 2015 the use of the moo framework to inspect prevalence dependency in model training of sdms remains yet unexplored when considering a vector of objective functions j s to be optimised unconstrained moo translates to 1 max s ω j s max j 1 s j 2 s j z s with ω the space of all possible solutions models s that compute j and z being the number of objectives with respect to sdm training a single objective measure is typically used a large share of sdm literature has focussed on the effects of the objective measure on model training key references being manel et al 2001 allouche et al 2006 and mouton et al 2010 generally the tss 1 1 is identified as the most appropriate single objective measure to train sdms and inspect prevalence dependency fukuda and de baets 2016 somodi et al 2017 2 tss sn sp 1 with sn being the sensitivity and sp the specificity these measures quantify the degree of correct estimation of species presence and absence respectively mouton et al 2010 sn t p t p f n 3 sp t n t n f p with t p being the number of true positives f p the number of false positives f n the number of false negatives and t n the number of true negatives in table 1 one can find the confusion matrix defining t p f p f n and t n it is important to note that an increase in sn δ sn will be coupled to a decrease in sp δ sp because ecological data are very rarely linearly separable for example an increase in t p with δ t p will induce an increase in sn t p δ t p t p f n t p t p f n δ t p t p f n in case data are not linearly separable this will induce a δ t n and decrease in sp t n δ t n t n f p t n t n f p δ t n t n f p in case data are linearly separable a δ t p would induce a δ t n of zero from this it follows that training sn and sp involves training of two conflicting measures next it is expected that δ t n and δ t p on the estimated pareto optimal curve are linearly related as the model exists out of a set of piecewise functions with one optimum for every decision variable see section 3 2 it is important to note that minimising commission f p and omission f n errors is equal to maximising sp and sn respectively standard practice in sdm training is to use one sole measure assigning a weight to omission and commission errors each in their unique way after model training commission and omission errors are compared with a priori set threshold and training is repeated until the threshold is satisfied as indicated in section 2 1 this approach is categorized under constrained soo a disadvantage of this approach is the iterative component in addition mostly non stochastic optimisers are used elith and graham 2009 leading to a single optimal model in these cases users typically selects one measure to train their sdm and depending on the results retrain it with another measure in rare cases an ad how pooled measure is defined mouton et al 2009 muñoz mas et al 2017 the risk of using these predefined measures is bias induction in training a way to cope with this issue is to separate the components defining a good estimation of species presence and absence and train them simultaneously see equation 1 4 max s ω j s max j 1 s j 2 s max sn s sp s all models s o p t form an estimated pareto optimal set p o p t a set of models s is said to be pareto optimal if and only if there is no other set of models s for which j s j s 5 p o p t s ω s ω j s j s one way to classify methods as soo or moo is to identify whether a single objective measure is used to train the model standard practice in sdm training is to select a single objective measure available in literature fig 1 left part of flow chart in fig 1 the different colours present structurally different models in terms of habitat suitability to an environmental gradient see lower plots these different models are positioned within the pareto space defined by sn and sp see example pareto plots a b c and d and are assumed to be form a near optimal solution to the pareto front depending on whether structurally different models are required by the end user one can choose to vary the threshold classifying species occurrence threshold based soo a or retrain the model with a different objective so to obtain different models s opt constrained soo in the first case a the obtained ensemble of models rely on the same model structure in the second case b one risks to only identify near optimal solutions in a specific part of the pareto space an alternative to using predefined single objective criteria is to iteratively define and maximise an objective measure j by weighting sn and sp c or to define a vector of objectives j sn sp that have to be maximised in a limited number of training cycle d fig 1 right part of flow chart the choice between aggregation based and pareto based training depends on whether a limited number of training instances can be run respectively no and yes the optimisation is conditioned in a way that models stimulated to over and under estimate are coupled to a species response over a broader or narrower environmental range fig 1 lower three plots in these plots the species response habitat suitability index 0 1 as a function of one environmental gradient in casu river water dissolved oxygen is shown the dotted line represent the limit a decision makers would identify as a lower environmental limit for dissolved oxygen in this case decision makers can balance measure implementation efforts and environmental protection by inspecting models with a narrower and broader environmental range respectively shown in the lower right orange and lower left panel green another way to classify methods as either soo and moo for prevalence adjusted model training is to identify how model selection is positioned in the model development workflow in the case of moo model training is followed by model selection based on expert knowledge of end users for soo model selection is done during model training based on partial results and an iterative selection process 3 methodology 3 1 the non dominated sorting genetic algorithm ii a pareto based moo method specifically the nsga ii approach of deb et al 2002 is implemented and compared to a constrained soo approach to test the ability to identify a near optimal solution to the pareto front nsga ii is a fast and elitist multi objective evolutionary algorithm used to propagate a population of candidate solutions to a pareto front defined by a set of objectives genetic algorithms make use of concepts observed in natural evolution such as mutation crossover and selection to find near optimal solutions to the pareto curve the solutions to an moo problem are encoded in chromosomes the algorithm assigns fitness values to these chromosomes based on a non dominated sorting approach non dominated solutions are equally valued solutions to a pareto front the non dominated sorting approach aims to preserve diversity among the non dominated solutions making use of crowding distance the crowding distance is a measure of solution density that is used in the selection operator the less close a solution is to another solution the higher the chance for selection nsga ii is chosen as it is a fairly easy to comprehend pareto based moo algorithm in addition it has proven to be an adequate algorithm to solve moo problems also in water resource management e g dotto et al 2012 and gimeno et al 2018 it is important to note that other algorithms are available such as decomposition based or aggregation based moo algorithms the use of nsga ii will be compared to these other algorithms in section 5 1 a detailed explanation of how the nsga ii algorithm works and how it is implemented can be found in supportive information 1 in the next section it is explained how the solutions s are implemented in the chromosomes genome and which model structures lie at the basis for generating these solutions 3 2 model structure and chromosome encoding in this section it is explained how the solutions s are implemented in the chromosomes of nsga ii in addition the model structure of the sdms generating the solutions s is formulated sdms are developed for 11 macroinvertebrate species in this research macroinvertebrates are selected as the group of target species as their different response to environmental pressures is typically analysed to support freshwater decision management lock et al 2011 the sdms are developed based on the conceptual model presented by guisan and rahbek 2011 and bennetsen et al 2016 in the remainder of this section the model formulation is shortly summarized the habitat suitability index hsi is used as a measure to express the suitability of the habitat shaped by abiotic conditions 6 hsi k j 1 m si x k j 1 m with hsi k being the habitat suitability index of the data point k total of n data points si x k j is the suitability index calculated by applying the species response curve for the input value of record k for variable j total of m variables see equation 7 the si is calculated by taking the logit of a linear function brewer et al 2016 and fig 2 7 si x k j 0 if x k j θ 1 2 1 exp p 1 x k j p 2 if x k j θ 1 θ 2 1 if x k j θ 2 θ 3 2 1 exp p 3 x k j p 4 if x k j θ 3 θ 4 0 if θ 4 x k j with p 1 l o g 2 ε ε θ 2 θ 1 p 2 l o g 2 ε ε θ 3 θ 4 p 3 p 1 θ 2 p 4 p 2 θ 3 the values for p are found by assuming the curve is bound by four parameters θ the parameters θ 2 and θ 3 describe the optimal range si 1 parameters θ 1 and θ 4 describe the suboptimal conditions and the total range ε is the minimal possible si in the range and is set to 0 01 smaller values for ε are considered yet these did not considerably affect the shape of the logistic curves this type of piece wise curve is considered as a good option to describe distributions of species responses observed in many training data brewer et al 2016 it is important to note that other shapes can be assumed depending on the ecological reasoning used for the study a simple practice to encode the solutions s in the chromosomes genome is to consider a fixed chromosome length for example the encoding for input variable selection is done by assigning a zero or one for respectively an exclusion and inclusion of the input variable in case of two variables being implemented in the chromosomes genome three solutions can be obtained 8 ω 0 1 1 1 1 0 in this study the aim is to implement parameter values and in or exclusion of input variables since specific parameters are coupled to specific input variables a variable length encoding gobeyn and goethals 2018 is used in variable length encoding the genome is programmed as a list of lists where a second order list composed out of four continuous values θ is defined when a bit of the first order string has the value of one in an example for two variables the solution space is defined by 9 ω 0 θ variable 2 θ variable 1 θ variable 2 θ variable 1 0 it is important to note that the values in θ are encoded as continuous values thus the size of ω will vary according to the number of computational bytes used to implement a continuous value in supportive information 1 an in depth explanation of the encoding is provided along with the implemented crossover and mutation operators 3 3 training data limnodata the limnology neerlandica limnodata data set knoben and van der wal 2015 and http www stowa nl is used to obtain a one on one in time and space coupled data set on the freshwater species environment relation supportive information 2 in total the data set contains over three million records covering 30 years over more than 20 000 sampling locations from this data set all ephemeroptera plecoptera and trichoptera ept macroinvertebrate species are extracted because these species are sensitive to pollutants in the stream environment verberk et al 2012 biological and abiotic data are processed and 11 ept macroinvertebrate species are selected based on number of occurrence greater than 30 the limnodata was selected because it is the largest open data set available in europe see https doi org 10 15468 ennulm that contains one on one coupled biological and physico chemical data in addition the limnodata are one of the only data sets containing data on a species resolution making the analysis of species response curves robust another key question for an efficient model development and application is what to include in sdms depending on available staff budget and time frame efforts can be categorized in four elements the selection of input variables parameters processes and output variables goethals et al 2018 concerning relevance of input and output variables an advantage of the limnodata is that both species and physico chemical data are collected in a standardized manner which is of major importance for model development everaert et al 2014 this long history of standardized sampling in the netherlands increases the change for having more records of species sampled in extreme conditions e g very low oxygen concentration in addition the distribution of the input features available in the limnodata will be quasi similar compared to other studies i e positively skewed for pollutants bennetsen et al 2016 even more sample prevalence of the species are comparable to those reported in other studies for example low prevalence in case of baetidae finally the format of the data set complies to the standards set by the european water framework directive consequently the trained models are potentially highly relevant for river management in the netherlands for details about how data exploration and cleansing was performed one is referred to supportive information 2 4 results 4 1 multi versus single objective optimisation in this section the moo approach is compared to a constrained soo approach sensitivity and specificity are optimised in case of moo while the tss is used as objective function for the soo to perform moo nsga ii is employed whereas for soo a tournament selection based on one fitness function no non dominated sorting is used as implemented in simple genetic algorithms the mutation and crossover operators are equal a threshold of 0 5 is selected to avoid that different values for classification are obtained by varying the threshold the threshold maximising the tss could also be selected however this would lead to balancing sensitivity and specificity by means of changing a threshold in this paper the aim to identify alternative structures for the sdms and not thresholds classifying occurrence from suitability see threshold based soo section 2 1 in addition the threshold of 0 5 has been assessed as adequate to classify occurrence when using the geometric mean for the model structure fukuda et al 2011 and equation 6 training is repeated a number of times to track the difference in behaviour between soo and moo this way the soo approach is comparable to the repeated training methodology used in garp constrained soo see section 2 1 the hypervolume h v indicator zitzler and thiele 1999 is calculated to obtain an indication about average of and variability on nsga ii peformance this indicator estimates the portion of the objective space which is dominated by a model set s opt the indicator accounts for diversity and convergence of the estimated solutions elarbi et al 2017 higher values indicate a better result for the calculation the 0 0 point was selected as a reference point cao et al 2015 to obtain an indication about the range of the solutions distribution in the pareto space obtained with soo and moo the maximum of the euclidean distance d between every two points in the pareto space is computed the distance values are normalized by the maximum possible distance between two points in the pareto space being equal to 2 with this indicator an ensemble of models describing the whole range of over and underestimation is assigned a value of one in table 2 the mean and standard deviation on h v are shown for each species in addition d is tabulated for 5 25 and 100 repeated training cycles of soo and moo the results for the multi and single objective optimisation of sdms for cloeon dipterum are shown in fig 3 respectively in the left and right panels results are shown for five upper 25 middle and 100 lower panels training cycles using the same training data nsga ii is able to identify different trade offs between sensitivity and specificity in addition nsga ii is assessed as superior to the soo approach in identifying this trade off by repeating the analysis five times the multi objective approach is able to delineate a set of models enclosing a trade off between of sn and sp values this is also observed in the maximum distance d being close to one table 2 in contrast the single objective approach identifies in five runs a number of models in a limited area of the sensitivity specificity space upper right panel d 0 31 a similar pattern is observed when the training is repeated 25 times for the moo approach a large number of solutions are found in the upper left corner of the pareto space middle left panel this observation is confirmed for 100 training cycles the lower left panel it is concluded that for cloen dipterum training with moo results in a set of a solutions within a broad range of the pareto space while training with soo leads a narrower range even when training is repeated many times when using moo instead of soo d converges faster to a value of one with an increased number of training cycles after five training cycles distance d is higher than 0 7 in 10 of the 11 cases when using moo when using soo a value of 0 5 or higher is obtained for only two cases after 100 repeated training instances visual inspection of the results for the other species shown in supportive information 3 confirm this pattern after five repeated runs the range of distribution quantified by d is estimated approximately four times larger for moo μ d 0 82 compared to soo 0 18 after 100 training cycles this value decreases to approximately three 0 97 versus 0 35 and two when compared to five training cycles of moo 0 82 versus 0 35 it shows that repeated soo training with a stochastic optimiser results in solutions positioned within a limited range of the near optimal pareto front when inspecting the optimal solutions it is observed that the pooled accuracy expressed in tss obtained with single optimisation is in some cases a bit higher than the one obtained with moo this can be observed in fig 4 lower panels when comparing the points close to the tss isoline of 0 6 the average h v varies over the different species being the highest for baetis rhodani and lowest for cloeon dipterum most values for the average h v are within the boundary of 0 6 and 0 8 the variability of h v over the different species is comparable for the species anabolia nervosa it is difficult for the single and multi objective approach to identify solutions that estimate species presence very well see fig 4 after five training cycles the moo approach is not able to identify models with a sensitivity higher than 0 9 for the soo case no models have a sensitivity above 0 75 after 25 cycles the multi objective approach is able to identify a number of models estimating species presence well similarly as for five training cycles the soo approach is not able to identify models with a high sensitivity in case of 25 and even a 100 training cycles in general a bias is observed toward the identification of models that predict absence very well but presence poorly figs 3 and 4 and supportive information 3 this implies that solutions with a higher specificity i e correct estimation of absence are more easily found than solutions with a high sensitivity when inspecting the model formulation with the geometric mean as aggregation function one would expect that filtering gets a higher weight in other words if the range of one species response curves is narrow then this will lead to a higher chance for absence especially for a large number of input variables in conclusion the multi objective approach is able to identify a set of sdms presenting a trade off between sensitivity and specificity yet there is a strong bias present towards identifying models with a high specificity the soo approach is less efficient in identifying this trade off however it is able to identify models with a slightly higher tss when compared to the moo approach 4 2 ecological relevance the estimated species response curves found with the nsga ii approach for baetis rhodani a and cyrnus flavidus b are shown in fig 5 to obtain these results the experiment presented in section 4 1 is repeated but now with bootstrap samples of the training data bootstrap sampling is used to obtain an insight in the variability on the species response curves see light grey colour fig 5 an average species response curve of the best 10 models based on tss is shown with the black solid line an average response curve computed from models with a high specificity and sensitivity are shown in blue and orange respectively in this text sdms with sn sp sp sn are named sensitivity specificity models for the three classes of models an average response is computed by taken the median response the support shows the number of times a variable is included in the subset of models only species response curves are shown for variables with a support higher than 25 in addition the median model complexity m expressed as the number of parameters is indicated between brackets in the legend sdms located in different areas of the pareto space have a different model structure for baetis rhodani panel a fig 5 the response curves of sensitivity models describe on average a larger range than specificity models see transparency dissolved oxygyen do and nitrite no2 n in addition less variables are selected as explanatory when comparing the sensitivity to the specificity models an average of four variables number of parameters divided by four similar conclusions are drawn for cyrnus flavidus fig 5 panel b the sub optimal range of the response curves is broader for a number of variables in addition model complexity is on average lower for sensitivity sdms similar patterns are observed for other species in supportive information 4 the average species response curves comply with the model assumptions made in the conceptual model i e they have a clear optimum and can be asymmetric some ill defined average curves are identified see transparency and sulfate so4 however note that these ill defined curves are a result of computing a median response from all curves as such it shows that the optimum of the identified curves varies in the domain of possible values in order to check agreement of the results with ecological knowledge the model structures are compared with the environmental and habitat preference data set published by verberk et al 2012 the preference towards saproby eutrophy and acidity are extracted because they express preference towards oxygen saproby organic carbon saproby nutrient eutrophy saproby concentrations and ph acidity table 3 these preferences are fuzzy coded adding up to ten and indicate a degree of membership to a specific class this information is derived from expert knowledge for example for the species baetis rhodani the preference for eutrophic conditions is expressed in a very high value 9 this indicates that the species has a very high preference for these eutrophic conditions the situation for cloeon dipterum is different as the values are spread over the mesotrophic meso eutrophic and eutrophic class the species has a preference for eutrophic conditions yet less specific in contrast to baetis rhodani note that for this analysis the 10 best models based on tss are used to interpret the results a number of identified models show compliance with the extracted information dissolved oxygen is estimated to be an important variable based on support steering species occurrence eight out of 11 sdms have a support above 50 see supportive information 4 black bar graphs in addition the estimated responses show that oxygen levels for the species should generally be higher than 3 mg o2 l 1 see for example fig 5 this resembles to the observation in table 3 that most species are identified as beta or alpha mesosaprobe species do resp 6 8 and 2 6 mg o2 l 1 the preference for saproby indicates that the species cyrnus flavidus also has a preference for oligosaprobic conditions membership 3 oligosaprobe do 8 mg o2 l 1 bod 1 mg o2 l 1 this preference is confirmed by a high support and a rather narrow optimal range observed for the variable bod5 1 mg o2 l 1 fig 5 b however this preference is not reflected in do since the lower boundary is defined at approximately 2 4 mg o2 l 1 which is lower than 8 mg o2 l 1 here it is important to note that the support for dissolved oxygen for cyrnus flavidus is relatively low i e 40 in addition species limnephilus lunatus also has a preference towards oligosaprobic conditions and this is reflected in the response curve for dissolved oxygen 7 mg o2 l 1 see supportive information 4 the variable ph generally has a high support 10 of the 11 species have a support for ph above 50 see supportive information 4 the range for transparency for the species baetis rhodani fig 5 is limited to a range characterized by low transparency support 90 when inspecting table 3 one observes that this species has a specific preference for eutrophic conditions characterized by high nutrient concentrations 2 2 mg n l 1 and 0 15 mg p l 1 therefore the assumption is made that the preference for eutrophic conditions is reflected in the variable transparency in order to investigate this link the median total nitrogen and phosphorus for lower 0 5 m and higher 0 5 m transparencies are compared with the mood s median test the test shows that total nitrogen and phosphorus concentrations are on average expressed in median significantly higher for low than for high transparencies resp p value 5 7 10 48 and 1 9 10 46 5 significance this link between eutrophication and transparency could thus be explained by the observed preference for low transparencies of the species baetis rhodani in contrast this assumption is not confirmed by looking at individual components reflecting nutrient enrichment p and n variables when inspecting the species baetis vernus and hydropsyche angustipennis supportive information 4 one can draw similar conclusions as for baetis rhodani in conclusion the above results show that the data driven sdms are in line with ecological knowledge however it is not possible to clearly link each preference with all obtained species response curves and model structures 5 discussion 5 1 pareto based moo with nsga ii the aim of this paper is to present and use moo as a novel way to perform prevalence adjusted sdm training to this end nsga ii a pareto based method is used to quantify differences with a soo method in this experiment the soo method relates to the methodology used in many garp papers the use of nsga ii for prevalence adjusted training is assessed to be more effective than the use of soo because the aims of using nsga ii moo in general better comply with the aims of using prevalence adjusted model training provide a full overview of structurally different models equally valued in the pareto space defined by over and underestimation from section 4 1 it is shown that soo can comply with these aims yet it is assessed as less efficient and tractable another key element to assess moo as more effective is the selection of the best model options for moo these are selected after model training by end users allowing them to focus on the diagnostic analysis of the obtained model structure in contrast model selection with soo is based on partial results and iterative model training this requires end users to be sufficiently aware of training options and influence of specific objective measures in order to steer model selection with respect to nsga ii it is observed in this study that the method was biased towards identifying models with a high specificity in addition the moo approach was able to identify less accurate sdms in term of tss than soo the differences between the fitness assignment and chromosome selection procedure in nsga ii and the soo method explain the lower accuracy obtained with nsga ii the remained of this section explains why this bias is observed and what mechanisms lead to a lower accuracy in addition alternative methods are discussed the solutions in the pareto space showed to be biased towards the correct estimation of species absence to inspect if there is a link with sample prevalence the sensitivity and specificity of the sdms were inspected as a function of the prevalence results not shown here a declining trend of the mean sensitivity and specificity with increasing sample prevalence was delineated the mean specificity was higher than sensitivity suggesting that patterns are more noticeable present in the absence than in the presence data furthermore the difference between mean sensitivity and specificity became smaller with increasing prevalence this would suggest there is an effect of prevalence in the way the algorithm exploits areas in the search space which offer less resistance this bias can be explained by dominance resistant solutions which are solutions with a near optimal value in one or more objective s but a poor value in the remaining objective s these solutions are identified as non dominant far from the optimum in the pareto space here the 1 1 point and grow with the number of objectives since they are identified as non dominant in the first front these solutions can cause a bias in the identified final front jaimes and coello coello 2015 a potential solution to this issue is to use users preference to bias search to specific regions in pareto space fernandez et al 2019 with respect to nsga ii a modified fitness sharing mechanism has been developed by deb 2011 to obtain pareto optimal solutions in a desired region also for novel pareto based methods such as decomposition based algorithms user preference has been integrated in the algorithms search mechanism li et al 2017 mohammadi et al 2012 for river management a step forward in combining modellers and decision managers their expertise is to define the position and range of this preference guisan et al 2013 the nsga ii implementation for the moo is assessed as fairly efficient in simultaneously optimising sensitivity and specificity nsga ii is a long established algorithm with a number of advantages it is robust it is easy to comprehend and it requires little computational resources zambrano vega et al 2016 cui et al 2017 however the performance of the approach is potentially less satisfying for difficult or more complex problems deb and jain 2013 reed et al 2013 jaimes and coello coello 2015 in this study the approach was used to balance two objectives and therefore the nsga ii is assessed as effective yet it is important to mention that both effectiveness and efficiency of nsga ii can enhanced by considering alternatives such as an improved non dominated sorting method tian et al 2017 two notable disadvantages of using nsga ii are the risk of losing the best solutions during optimisation and the possibility of premature convergence the first limitation is caused by using the crowding distance in the tournament selection operator although this approach is used to provide a good diversity amongst solutions it can exclude well performing solutions from dense areas cui et al 2017 this exclusion is a notable limitation of nsga ii and can explain why the algorithm identifies slightly less accurate solutions than the soo approach the second limitation premature convergence can cause that the optimal solution to the pareto is not found within the time frame the guidelines of gibbs et al 2008 are used to determine the algorithm settings the robustness of the guidelines of gibbs et al 2008 for nsga ii was tested in a number of pilot runs in this study from these experiments it was concluded that the hyper parameters were near optimal as such the results indicated that the algorithm did converge and that the used guidelines are appropriate for this case study it is concluded that nsga ii can thus be used for prevalence adjusted model training given that future studies focus on increasing search efficiency with nsga ii or alternative pareto based methods this way it should be possible to obtain with moo equally accurate results in the single dimension tss than those obtained with soo besides domination based multi objective evolutionary algorithms other pareto based methods such as indicator and decomposition based algorithms exist indicator based algorithms use performance metrics to improve the efficiency of the selection operator decomposition based algorithms split the moo problem in different scalar sub objectives based on a set of weights elarbi et al 2017 in the latter approach the sub objectives are optimised simultaneously and cooperatively zhang and li 2007 decomposition based methods have a number of strengths high selection pressure toward the pareto front 1 easy to work with local search operators 2 efficient to solve many objective optimisation problems 3 and find solutions to irregular pareto fronts 4 also in water resource management decomposition algorithms have shown to be very efficient zheng et al 2014 ishibuchi et al 2017 with respect to the optimisation problem presented in this paper a decrease of computational resources and better identification of the pareto front are two advantages motivating the future exploration of decomposition based algorithms in addition they circumvent the problem of dominance resistant solutions discussed above by not relying on pareto dominance santos and takahashi 2018 consequently they are evaluated as highly competitive alternatives to nsga ii to deal with prevalence adjusted sdm training as such the value of decomposition algorithms for prevalence adjusted sdm training should be researched in future studies 5 2 evaluation of multi objective optimisation for prevalence adjusted model training in section 2 1 approaches to perform prevalence adjusted model training were categorized into four classes threshold based soo constrained soo aggregation based moo and pareto based moo the former two are typically applied within the field of species distribution modelling and are valid approaches when habitat suitability is reflected in species occurrence in the training data threshold based soo can be used in case no prevalence adjusted training is required and when the obtained models are well performing examples of studies with a satisfactory accuracy area under the receiver operator curve 0 7 can be found for freshwater species fukuda et al 2013 as well as for birds and plants townsend peterson et al 2007 elith and graham 2009 yet other studies show that the accuracy of freshwater sdms can be unsatisfactory after optimisation bennetsen et al 2016 in case of an unsatisfactory performance and or in case of imbalanced training data prevalence adjusted model training is a good option to allow experts and river managers to evaluate models mouton et al 2009 threshold based soo methods are often not the preferred option as river managers require structurally different models for environmental limit setting and evaluation in section 4 2 it was shown that structurally different models can be obtained by employing moo the use of constrained soo based on repeated training with a stochastic optimiser can be considered as an alternative to obtain structural different models see section 4 1 even more it was observed that training with soo can result in better pooled accuracy in terms of tss compared to pareto based moo a disadvantage of soo is the limited range of the solutions distribution within the pareto space five times lower after five repeated runs compare d of 0 82 to 0 18 making the approach less attractive for prevalence adjusted model training consequently the use of constrained soo is advised when a high accuracy can be obtained and a prevalence adjusted training approach is not needed to meet the study objectives an alternative to constrained soo is to make use of aggregation based moo mouton et al 2009 in aggregation based moo sdms are repeatedly trained with varying aggregated single objectives obtained by an iterated weighting procedure ishibuchi et al 2017 it is noteworthy that the computational efficiency of pareto based moo is estimated to be higher in order to test the computational efficiency training was repeated 10 times with soo and moo using the same training data an average runtime of 518 16 and 537 seconds 69 was obtained for soo and moo respectively four cpus with a clock rate of 1 90 ghz this is an increase of four percent in runtime for moo compared to soo in this case the use of the nsga ii using one training cycle is preferred over repeated training with a single objective optimiser relying on multiple training instances to compute an estimate of the pareto optimal front with increasing computational demands for performing experiments with newly developed sdms for example sdms based on individual based models see bruneel et al 2018 it will be interesting to inspect what the potential role of aggregation and pareto based methods is for prevalence adjusted model training one is advised to not make use of aggregation based moo in combination with stochastic optimisers garp bootstrapping or cross validation as the stochastic component in training makes the approach increasingly computational intractable in these cases practitioners are advised to resort to pareto based methods or to use deterministic methods to increase tractability 5 3 application of sdms for freshwater decision management and environmental limit setting in this study sdms are developed so they can be used as diagnostic tools to support environmental limit setting the evaluation of the sdms showed that the models can be used for system understanding however inspection of the obtained model structure section 4 2 also reveal that different model structures can simulate similar presence patterns for a number of inputs this is not unexpected as each response function is based on the same model assumptions section 3 2 thereby the effect of highly influential parameters and or input variables on the model output can be cancelled or compensated by another input variable and or parameter to avoid this stricter boundary conditions can be defined by using expert knowledge van broekhoven et al 2006 process based modelling can also be of use if sufficient qualitative data are available to identify model parameters beale and lennon 2012 bruneel et al 2018 the results show that the optimisation of the sdms does not necessarily lead to satisfying models suitable to inspect by river managers for example the performance of the sdms developed for cloeon dipterum could be further improved the efficiency cannot only be increased by applying a more efficient optimisation algorithm but also by relying on more qualitative training data fukuda and de baets 2016 as indicated in the introduction training data on species presence and absence should reflect habitat preference which is often not the case a reason for the choice of the dutch limnodata was its coverage towards species presence in extreme conditions aiming to obtain a better insight of these preference in extreme conditions however habitat conditions linked to species presence and absence should be observed more adequately by investing in time series observations rather than point observations this remains a big challenge in river water quality studies as the cost for the maintenance of in situ and remote networks are considerable alilou et al 2018 potentially the use of internet of things applications can increase temporal and spatial coverage resulting in more reliable training data as indicated in section 3 3 it is expected that the develop method will be transferable to other case studies because the characteristics of the currently available freshwater data sets physico chemical input data and species prevalence are comparable however future practical studies should confirm whether this assumption is valid a key element in this research is the link between on the one hand conservation measure and environmental costs and on the other the use of sdms describing varying environmental ranges in which the species are able to survive donoso et al 2018 the use of sdms trained to overestimate species presence will describe a larger range which will reflect in an optimistic view on measure implementation and environmental costs in contrast a conservative interpretation will be reflected by using sdms trained to underestimate species absence the developers of these presence absence models are required to present an unbiased view of possible model options by treating the trained sdms as model hypothesis the opportunity is created for modellers to consult expert and decision managers and use their expertise to further refine sdms jarnevich et al 2015 the role of prevalence adjusted model training is to present these model options and facilitate a closer collaboration between model developers and end users in order to meet study objectives even though effective use of sdms for decision management is found in literature guisan et al 2013 urge to conduct more practice oriented case studies to increase reliability they state that more decision makers should make use of these models and provide feedback guisan et al 2013 also recommend that model developers should obtain a better insight in the decision makers process example of a studies employing this philosophy are presented by van broekhoven et al 2006 and bennetsen et al 2016 they mainly aim to further fine tune requirements of sdms for river management by relying on expert knowledge the interaction between developers and users is assumed to be key to achieve more reliable models mouton et al 2009 presenting a number of choices to stakeholders cannot only increase transparency but also enhance trust in the developed approaches an important aspect is to consider how these pareto optimal trade off curves can be used by decision makers to choose the most appropriate alternative esmail and geneletti 2018 a key element is the reduction of the number of potential solutions that have to be considered by the decision makers ascough et al 2008 in a final note environmental and conservation costs are not considered explicitly in this paper since costs are not linearly related to shifting an environmental limit it is important to note that the axis of the pareto space do not explicitly reflect costs therefore future research could focus on considering these costs coupled to environmental limit targets and the consequence of model selection and use 6 conclusion prevalence adjusted model training is an important tool for river managers to analyse species distribution models sdms in essence prevalence adjusted model training can be translated to multi objective optimisation moo dealing with two conflicting objectives the minimisation of under and overestimation of species presence in this study four classes of approaches being threshold based single objective optimisation threshold based soo constrained soo aggregation based moo and pareto based moo are identified one study that makes use of aggregation based moo could be identified in literature whereas all other sdm studies make use of soo to identify an ensemble of models each weighting species over and underestimation for the first time a pareto based moo was applied to perform prevalence adjusted training this approach was compared with an soo approach and was found to be two to four times more efficient in identifying a wide range set of pareto optimal models with only a four percent increase in runtime per training in addition the use of nsga ii is found to be effective to identify reliable sdms useful for diagnostic analysis this way we believe stakeholders can focus on prominent questions how well do our models perform in estimating species presence and absence and what is the trade off between those objectives how does this trade off relate to the properties of the data the model formulation and the species characteristics in addition which expert knowledge is needed to further improve sdms finally what environmental limits will be considered and how do they relate to conservation measure and environmental costs this way prevalence adjusted trained sdms can support river and potentially all environmental decision management acknowledgments the authors thank peter van puijenbroek and jan janse pbl netherlands environmental assessment agency for providing the data and explanation of the limnodata data set in addition the authors would like to thank ans mouton ghent university ugent bernard de baets ugent stijn van hoey research institute for nature and forest inbo jana van butsel ugent and tijs d hulster for comments on earlier versions of this manuscript we also wish to express gratitude to the reviewers and associate editor who reviewed this work and allowed us the opportunity to improve the paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j watres 2019 114863 author contribution s g conceived the idea designed the methodology analysed and interpreted the data and led the writing of the code and the manuscript p g contributed to interpretation and writing of main text 
18602,polychlorinated biphenyls in stormwater sediments relationships with land use and particle characteristics siqi cao a staci l capozzi a b birthe v kjellerup a allen p davis a a department of civil and environmental engineering university of maryland college park md 20742 usa department of civil and environmental engineering university of maryland college park md 20742 usa department of civil and environmental engineering university of maryland college park maryland 20742 b geosyntec consultants columbia md 21046 usa geosyntec consultants columbia md 21046 usa geosyntec consultants columbia maryland 21046 corresponding author polychlorinated biphenyls pcbs are classified as persistent organic pollutants pops concentrations of 209 pcb congeners as well as profiles of the ten homologues were determined in stormwater sediments collected from various primarily roadway sites with different land use the total pcb concentrations ranged from 8 3 to 57 4 ng g dry weight dw with a mean value of 29 2 ng g dw pcb concentrations varied with nearby land use higher stormwater sediment pcb concentrations were found in dense urban areas average 39 8 10 5 ng g and residential areas average 35 3 6 2 ng g compared to highways passing through greenspace average 18 0 0 4 ng g the number of chlorines per biphenyl ranged from 3 63 to 5 39 and the toxic equivalency teqs of the pcbs were between 1 5 and 18 0 pg g at all sites a non aroclor congener pcb 11 was detected in all samples and was dominant at two sites pcbs were sorbed to smaller stormwater particulate matter 75 μm at higher concentrations compared to larger particles 75 μm pcb sorption tended to increase with the total organic carbon toc of the particulate matter in the sediment samples however greater pcb mass almost 80 was present in the larger particles information on sediment pcb concentrations from different land uses along with stormwater particulate matter data can allow the estimation of pcb loads and load reductions using stormwater control measures graphical abstract image 1 keywords polychlorinated biphenyls pcbs urban stormwater congeners pcb 11 toxicity teqs 1 introduction polychlorinated biphenyls pcbs are a group of chlorinated organic compounds derived from biphenyl with 1 10 of the hydrogen atoms substituted by chlorine thus 209 different congeners exist pcbs were commercially produced from 1929 to 1977 in the united states u s and widely used in industrial processes as coolants in transformer oils and as flame retardants kimbrough 1995 aroclor is the trade name for specific pcb mixtures that were manufactured in the u s in other countries names such as clophens phenoclors pyralenes were used safe 1994 the aroclors are identified by a four digit code such as aroclor 1254 the second two numbers in aroclors indicate the percentage of chlorine by mass in the mixture atsdr 2000 the most commonly manufactured aroclors in the u s were a1016 a1232 a1242 a1248 a1254 a1260 atsdr 2000 due to their high stability pcbs are persistent in the environment and are classified as persistent organic pollutants pops stockholm convention 2008 jones and de voogt 1999 in spite of the ban pcbs are still being released into the urban environment andersson et al 2015 buildings constructed before the 1970s can be sources of pcbs from building sealants and caulking diamond and hodge 2007 zhang et al 2011 while roads constructed with recycled construction and demolition waste c dw also can serve as pcb sources butera et al 2014 wahlström et al 2000 increased air concentrations and subsequent atmospheric deposition of pcbs contribute to the presence of pcbs in stormwater and surface waters diamond and hodge 2007 thus runoff from building roof and wall surfaces may contain pcbs urban runoff storm sewers are considered important sources of pollution for impaired rivers streams lakes reservoirs and ponds accounting for 13 19 of the total load epa 2012 other important probable sources include atmospheric deposition agriculture and contaminated sediments most previous stormwater studies have primarily addressed the presence of dissolved pcbs and did not consider particulate matter in stormwater as a separate phase for pcb transport gilbreath and mckee 2015 granier et al 1990 rossi et al 2004 a few studies evaluated particulate matter and the particle bound characteristics of pcbs hwang and foster 2008 jartun et al 2008 zgheib et al 2011b hwang and foster 2008 found that pcbs in stormwater particulate matter accounted for more than 90 of the total pcbs in storm flow this indicates that particulate matter should be the focal point when evaluating pcbs in stormwater hwang and foster 2008 however studied only 85 of the 209 pcb congeners jartun et al 2008 studied seven pcbs pcb 28 52 101 118 138 153 and 180 in urban runoff sediments similarly zgheib et al 2011b also focused on these seven pcb congeners because they were listed as priority pollutants in urban stormwater zgheib et al 2008 thus information about all 209 congeners in stormwater particulate matter is lacking in order to reduce pcb loadings to surface waters an understanding of the sources of pcbs in the watersheds is required hwang and foster 2008 to achieve this goal information about concentrations of all 209 congeners is important the abundance of some specific congeners may indicate certain sources also comparison of congener profiles in stormwater contaminated with pcb products may assist in source identification one congener of special concern is pcb 11 3 3 dichlorobiphenyl which is an unintentional by product from the manufacturing process of diarylide yellow pigments grossman 2013 because pcbs are sorbed to particulate matter they are expected to be removed from stormwater via particulate matter removal processes such as sedimentation and filtration thus it is important to study the affiliation of pcbs with particulate matter based on particle size however most relevant studies focused on the organic carbon fraction instead of particle size beless et al 2014 bucheli and gustafsson 2003 choi and al abed 2009 little research has been completed regarding pcb concentration and particle sizes ghosh et al 2003 especially as related to stormwater andersson et al 2015 marsalek and ng 1989 the objectives of the current study were 1 to evaluate pcb concentrations and congener distributions for stormwater particulates collected near urban areas 2 to identify potential sources of stormwater pcbs in urban areas and 3 to use the above information to assist with stormwater control measure scm implementation and design recommendations for the effective removal of pcbs from urban stormwater runoff to address these objectives surface sediment soil samples were collected from parking highway and residential areas in maryland u s and tested for pcbs 2 materials and methods 2 1 sampling sites samples were collected from september 2016 to july 2018 seven sampling sites were included in this study to represent different land uses 1 dense urban area stormwater gutters along roadways in bladensburg bbg u and baltimore btm u md 2 institutional area an inlet to a bioretention cell at the university of maryland umd i college park campus 3 commercial area a scm near a 4 lane state highway bv c 4 greenspace a stormwater channel adjacent to a highway r1 g and a stormwater gutter along roadways through wooded areas in laurel md r2 g and 5 residential area storm drains in a residential area in college park md cp r fig s1 shows a map of sampling sites in this study at bbg u two samples were collected from the gutter one bbg u1 was at 1 m and the other bbg u2 was 2 m from the drainage point the sample from the baltimore roadway btm u was collected from the catch basin of a gutter by dr james hunter from morgan state university at umd i one sediment sample was collected at the entrance of the stormwater inlet to a bioretention cell three samples were collected at bv c at the entrance bv c1 and the discharge bv c2 of the rip rap channel to the bioretention scm as well as inside the scm bv c3 at r1 g two sediment samples were collected at two parallel locations in the stormwater channel r1 g1 and r1 g2 one sediment sample was sampled from the gutter at r2 g at cp r three samples were collected from the gutter nearby cp r1 and near two different storm drains cp r2 and cp r3 site names with short descriptions as well as sampling details are summarized in table s1 at each site samples were collected once temperature difference in very different periods could affect the solubility and volatility of pcbs in addition the frequency of rain could affect the age of the sediments and pcbs accumulated in them thus all samples were collected during may to september with an antecedent dry period ranging from 1 to 7 days the temperature ranged from 23 c to 30 c during sample collection during the collection process a clean and sterile stainless steel soil scoop was used to collect surface sediments 0 10 cm deep at different sites to minimize biological and chemical transformations all samples were stored at 20 c in the dark in glass containers with teflon lids until analysis 2 2 sediment fractionation selected sediment samples were separated into three fractions as done by kim and sansalone 2008 briefly wet sieving no 200 sieve ele international canada was performed to separate the sediment fraction 75 μm from the other two fractions the filtrate was transferred to an imhoff cone and settled for 2 h the settleable fraction 25 75 μm was settled out in the imhoff cone and the remaining fraction was defined as the suspended fraction 25 μm the suspended fraction accounted for 1 of the total sediment mass in all collected samples thus this fraction was not further studied the 75 μm fraction and the 25 75 μm fraction were air dried in the fume hood for two days until they appeared dry clean sea sand merck u s was used as laboratory blank control triplicate and was exposed to the same treatments as the samples triplicate 2 3 extraction of pcbs microwave assisted extraction mae mars 6 cem u s was used to extract pcbs from the sediment samples the extraction method was based on lopez avila et al 1995 with minor modifications briefly 6 g of air dried sample was weighed and transferred to teflon extraction vessels 100 ml cem u s then 36 ml hexane 95 n hexane for organic residue analysis acetone hplc grade honeywell 1 1 was added into the vessels prior to extraction 20 μl of the mixed solution of surrogates 0 5 μg ml 1 tetrachloro m xylene tcmx 2 4 6 trichlorobiphenyl pcb 30 and 2 2 3 4 4 5 6 6 octachlorobiphenyl pcb 204 was added into each sample to calculate the recovery of the extraction procedure extractions were performed at 115 c for 10 min at 1000 w after extraction the vessels cooled to room temperature before they were opened the extracts were settled by gravity for 30 min and the supernatants were transferred into 60 ml amber vials the residues were washed with five ml of hexane and settled for 30 min five ml of hexane acetone 1 1 and five ml of acetone were added separately to wash the residues as mentioned above the supernatants were combined and then concentrated under nitrogen flow to less than 100 μl finally 1 ml of hexane hplc grade was added to dissolve the extracts extracts were stored at 20 c until cleanup 2 4 cleanup of pcbs extracts the cleanup method was based on epa method 3611b epa 1996 and 3620c epa 2014 with minor modifications briefly alumina fisher scientific u s was heated at 550 c for at least 24 h and cooled to room temperature in a desiccator deionized di water was added 30 μl di water g alumina to deactivate it to 3 columns for cleanup were made with six g of the prepared alumina a layer of oven dried sodium sulfate fisher scientific u s to remove water from the extracts and glass wool acros organics germany at the bottom of a glass disposable pipet pyrex u s first 20 ml hexane was added to equilibrate the column then the extracts were transferred into the column and the effluent was collected the amber vials were rinsed with 15 ml of hexane and the rinsate was added into the column the collected effluent was reduced in volume with nitrogen flow to concentrate the samples to less than 1 ml and spiked with 20 μl of the mixed solution of internal standards 0 5 μg ml 1 4 bromobiphenyl 4 bb and 2 2 4 5 5 pentabromobiphenyl penta bb hexane hplc grade was added to the concentrated effluent to reach a final volume of 1 ml and the samples were vortexed for 10 s before being transferred into gc vials for further analysis 2 5 gc analysis samples were analyzed by gas chromatography electron capture detector gc ecd 7890b agilent technologies u s equipped with an agilent j w hp 5ms column 60 m 250 μm x 0 25 μm the samples were autosampled autosampler 7693 agilent technologies u s with an injection volume of 1 μl helium was used as the carrier gas the temperature program table s2 was developed based on a congener specific pcb analysis method used for analysis of pcb 11 guo 2013 target compounds surrogate standards and internal standards were purchased from accustandard u s and restek u s 2 6 quality control all laboratory materials were made either of glass or teflon to avoid sample contamination to avoid contamination during sample preparation all glassware used were cleaned by detergent rinsed with hexane acetone methanol and di water and baked in a muffle furnace at 550 c for 4 h teflon containers were cleaned by detergent ultrasonicated with hexane and acetone and rinsed with di water during each run clean hexane vials were added at the beginning and the end of each run to avoid significant carry over from previous runs the results from the laboratory blanks and clean hexane at the end of each run indicated that carry over did not occur between samples for each sample an extra treatment group triplicate was prepared as a standard control the standard control treatment was treated the same way as other treatments except for the addition of surrogate standards or internal standards this treatment was carried out to confirm the standards were not present in the sediment samples all 209 pcb congeners were analyzed and 131 peaks were detected in the samples all peaks were verified by mass spectrometry by m z and retention time 5977a msd agilent technologies u s detection limits for pcb congeners ranged from 0 0008 to 0 33 ng g the method detection limits were obtained by dividing the instrument detection limits by the sample masses for statistical analysis all values below the detection limits were substituted with half of the detection limits concentrations of the target compounds were compared to average concentrations in laboratory blanks and calculated based on audy et al 2018 if the blank average was 10 of the amount in the sample 178 195 congeners no correction was applied if the blank average was 10 35 of the concentration in the samples 11 21 congeners the average concentration in the blanks were subtracted from the sample if the blank average was above 35 3 10 congeners the compound was reported as below the detection limit in the sample the measured concentrations of mono to tetra cbs were corrected for the recovery of tcmx and the measured concentrations of penta to deca cbs were corrected for the recovery of pcb 204 average surrogate recoveries were 73 0 for tcmx range 54 8 91 9 and 83 0 for pcb 204 range 50 8 114 0 all were within the acceptable range of 50 125 hermanson and johnson 2007 4 bb was the internal standard for mono to tetra cbs and penta bb was the internal standard for penta to deca cbs 2 7 total organic carbon toc measurement total carbon tc and inorganic carbon ic were measured using a toc analyzer toc l shimadzu japan with solid sampling module ssm 5000a all sediment samples were measured in triplicate toc was calculated by subtracting ic from tc 2 8 data analysis concentrations of the homologues were calculated based on data from gc analysis concentrations were equally distributed between congeners when there was co elution 12 dioxin like pcbs dlpcbs pcbs 77 81 126 169 105 114 118 123 156 167 189 are of special concern due to their elevated toxicity and their individual concentrations were also calculated the average number of chlorines per biphenyl was determined based on a molar average for the comparison of two groups of data f tests were performed to test the equality of two variances and student t tests were performed to examine differences among different sites and sediment fractions statistical significance was set at p 0 05 for comparison of three groups of data pairwise t tests was performed and holm s sequential bonferroni method was applied to correct the results by reducing the possibility of type ι error bunzel et al 2013 huang et al 2018 3 results and discussion 3 1 pcb concentrations at different land uses pcbs were detected in the samples from all the studied sites fig 1 the highest total pcb concentration 51 6 5 6 ng g in this study was found in bbg u1 1 m from a stormwater inlet fig 1a bbg u2 had a total pcb concentration of 36 4 1 4 ng g the second highest pcb concentration 41 4 5 6 ng g was found in cp r from the residential area fig 1c where a parking area and residential buildings are nearby umd i collected at the entrance of the bioretention cell contained high concentrations of pcbs at 37 3 6 3 ng g fig 1g btm u contained pcbs at 31 5 2 3 ng g fig 1b total pcb concentrations varied at bv c fig 1d the lowest pcb concentrations in this study were found at r1 g 17 8 3 9 ng g and r2 g 18 3 1 1 ng g fig 1e and f the concentrations of total pcbs in stormwater particulate matter 9 8 51 6 ng g were in the range of concentrations observed in several previous studies 0 4 755 ng g table s3 zgheib et al 2011b measured concentrations of seven selected pcb congeners with values ranging from below 10 ng g to 60 ng g in the particulate matter in stormwater higher than those in this study 0 00167 1 92 ng g potential reasons for the higher concentrations found in zgheib et al 2011b include 1 the area in the zgheib study was a watershed in a dense urban area with commercial centers apartments and buildings and 2 the study was carried out at least six years ago in france where pcbs were banned several years later than the u s zgheib et al 2011b the consensus based threshold effect concentration tec based on various sediment quality guidelines for pcbs in freshwater ecosystems is 59 8 ng g macdonald et al 2000 for a total pcb concentration below 59 8 ng g harmful effects on sediment dwelling organisms are unlikely to be observed the interim sediment quality guidelines isqgs and the probable effect levels pel for pcbs in freshwater sediment are 34 1 ng g and 277 ng g respectively ccme 2001 both sediment samples from the bbg u gutter as well as sediment samples from cp r1 cp r2 and umd i bioretention cells were above the isqgs for pcbs in freshwater sediment suggesting adverse effects from pcbs exposure but were below the consensus based tec and pel all the other samples were below the isqg indicating a low possibility of adverse effects at these sites both bbg u samples as well as umd i had similar pcb homologue patterns penta to hepta cbs were the most dominant among all the homologues di cbs also had a high abundance at umd i accounting for 25 1 2 0 of total pcbs btm u was also similar to bbg u where pcbs with two five and six chlorines dominated all three cp r residential samples had similar homologue patterns di cbs and hexa cbs were dominant followed by penta cbs and tetra cbs at the bv c scm the two channel locations bv c1 and bv c2 showed similar homologue patterns penta and hexa cbs were the most dominant however the homologue pattern inside the scm bv c3 was different from that in the rip rap channel tetra to hexa cbs were present at high concentrations 1 64 0 31 ng g 2 33 0 60 ng g compared to other homologues at r1 g and r2 g di cbs was the most dominant homologue 7 61 1 55 and 7 03 1 13 ng g respectively the amounts of mono to tetra cbs and penta to deca cbs were similar at these two sites this result was noticeably different from the other sites where the concentrations of penta to deca cbs were higher than mono to tetra cbs the highest number of chlorines per biphenyl was found in bbg u 1 m from the stormwater inlet table 1 highly chlorinated pcbs cl 5 mainly experience biotransformation through dechlorination anaerobic conditions are essential for this process the process however is slow compared to degradation of lower chlorinated pcbs cl 4 which can be degraded aerobically passatore et al 2014 payne et al 2011 products of anaerobic dichlorination act as substrates for the aerobic biodegradation processes passatore et al 2014 except for cp r1 r1 g r2 g and bv c3 all the other samples had a chlorine number greater than four the lowest numbers of chlorines were found at r1 g and r2 g due to the dominance of di cbs at these two sites the easier loss of chlorines in the meta and para positions results in an increase in the level of ortho chlorinated pcbs during microbial dechlorination epa 2004 thus the low levels of ortho chlorinated pcbs in bbg u cp r2 cp r3 and r2 g suggest limited or no reductive dechlorination table 1 the level of ortho chlorinated pcbs at the other sites were at least four fold higher which suggest impacts from microbial dechlorination the level of ortho chlorinated pcbs in btm u was high compared to bbg u samples indicating dechlorination taking place at this site a lower chlorine number per biphenyl in btm u could also support the occurrence of dechlorination 3 1 1 comparison of sites results from this study suggest an influence of land use on the pcb concentrations and homologue patterns the ranges and means of pcb concentrations based on different land uses are summarized in table 1 higher pcb concentrations were found in dense urban areas average 39 8 ng g and residential areas average 35 3 ng g compared to greenspace average 18 0 ng g a similar trend was also found in a study carried out in france zgheib et al 2011a in the zgheib et al study the median concentration of seven selected pcbs pcb 28 52 101 118 138 153 180 in stormwater was greatest in a highly dense area at 468 ng l the median concentrations in residential area and dense urban area were lower at 211 ng l and 259 ng l respectively the building fronting the bbg u samples was brick built in 1968 pcbs can be leached or washed from sealants and caulking on building exteriors also elevated pcb concentrations in indoor air could vent into the outdoor air the total emission of pcbs estimated from one intensively studied office in toronto canada was 280 5879 ng h and up to 90 of total losses could be to the outdoors based on the air exchange rate zhang et al 2011 additionally stormwater runoff could wash flakes of material containing pcbs from the surface of older buildings this is supported by research which found that among 80 buildings constructed from 1945 to 1980 in toronto canada the mean concentration of pcbs was 4630 mg kg 1 of sealants diamond et al 2010 diamond et al 2010 also estimated that concentrations of pcbs in sealants and caulking were geographically higher in residential areas within buildings constructed during 1950 1970 brick research educational buildings dating from the 1970 s to 1990 s are 50 100 m away from umd i the cp r residential buildings primarily built from wood were constructed in 2011 the scm at bv c is about 40 m from a low rise commercial building built in 1986 and is surrounded by a small parking lot approximately 40 spots on the rip rap channel side on the other sides it is surrounded by a 4 lane highway and local roads the stormwater treatment media elevation was high at the end of the channel which allowed sediments to deposit and not enter the scm possibly causing a higher pcb concentration at the end of the channel no buildings were nearby r1 g and r2 g a major difference compared to the other sites instead these two highways traversed through forested areas as a result the lower levels of pcbs detected in the sediments at these locations could be a consequence of the absence of buildings surrounding the collection site therefore the measured pcb concentrations at these two sites can be assumed to result only from highway sources overall the results of this analysis suggest that buildings and especially older buildings contribute significantly as pcb sources in urban areas also roads themselves may play a lesser role in pcb contamination compared to buildings since cp r had comparatively new buildings the relatively high pcb concentrations at this site may result from the use of recycled building materials or pcb byproducts from paints anezaki and nakano 2014 3 2 comparison of congener profiles among different sites none of the sediment samples had a similar congener pattern with any aroclor mixture indicating that aroclors were not the direct sources or the aroclor mixtures were weathered due to biodegradation and or mixing however some congeners detected at relatively high abundance in the sediment samples were also present in a1254 or a1260 indicating that some congeners may have originated from these aroclors relative abundance of mass of the dominant congeners are summarized in table 2 pcb 11 was only congener detected at all sites at a relatively high abundance it is a non aroclor congener and is inadvertently produced during the manufacturing of paint pigments hu and hornbuckle 2010 pcb 99 110 180 193 and 194 were detected with high abundance in samples from bbg u btm u and umd i pcb 99 and pcb 110 are important components of a1254 pcb 99 is also a dechlorination product from a1260 fagervold et al 2007 pcb 194 accounted for 2 1 of the total pcbs in a1260 frame et al 1996 pcb 194 showed no dechlorination after 500 days of incubation with baltimore harbor sediment microcosms fagervold et al 2007 indicating its stability pcb 180 accounted for 11 4 of a1260 overall the dominant congeners in these sediment samples were present and in some cases abundant in a1254 and a1260 the cp r congener fingerprints were different from the other sites with pcbs 11 14 141 and 161 dominating pcb 141 accounted for 2 6 of total pcbs in a1260 and 1 0 in a1254 frame et al 1996 pcb 99 was dominant at bv c 4 5 22 5 pcb 8 was detected with high abundance at this site it was one of the major congeners detected in polycyclic type pigments anezaki and nakano 2014 overall sediments from dense urban and institutional sites were dominated by congeners present in a1254 residential sites had smaller signatures of a1254 a1254 has been reported to have been added into sealants and caulking compounds atsdr 2000 this information supports the hypothesis that old buildings remain as important pcb sources to stormwater pcb 11 and 14 dominated in the sediments from greenspace r1 g and r2 g 3 3 concentrations and potential sources of pcb 11 the concentration and relative abundance of pcb 11 at the different study sites showed that the highest concentration was found at umd i fig 2 ranging between 4 47 and 6 58 ng g it was the most dominant congener and accounted for 15 of the total pcb mass r2 g also had a high concentration of pcb 11 5 66 1 07 ng g accounting for 30 9 of total pcbs at this site at the other sites pcb 11 was also detected but at lower concentrations compared to umd i and r2 g ranging from 0 33 0 08 to 3 52 0 10 ng g the relative abundance of pcb 11 compared to total pcbs at these sites ranged from 1 3 to 11 2 these results indicate that pcb 11 is an important component of the total pcb fingerprint in urban stormwater and the surrounding area yellow flakes were noted in the umd i samples the larger yellow flakes 0 5 4 mm in diameter were separated from the sediment by hand and both were analyzed separately for pcb concentrations without the large yellow flakes the concentration of pcb 11 was 3 8 0 7 ng g in the sediment which was significantly different from the concentration 5 4 0 1 ng g measured with the flakes included p 0 02 the separated yellow flakes were tested using the same procedure as the sediment and the concentration of pcb 11 was 182 10 ng g the yellow flakes appear to be yellow traffic paint pcb 11 has been detected as the major congener present in azo type pigments anezaki and nakano 2014 it has been frequently detected in urban air chicago and pigments or dyes hu and hornbuckle 2010 hu et al 2008 among all 209 congeners pcb 11 was found in 13 of the 33 commercial paint pigments tested hu and hornbuckle 2010 near new york new jersey harbor pcb 11 concentrations in the effluents of the two wastewater treatment plants which receive wastewater from pigment manufacturing plants ranged from 5 to 116 ng l while pcb 11 concentrations ranged from 0 0016 to 9 4 ng l in the effluents from other wastewater treatment plants rodenburg et al 2010 pcb 11 has been frequently detected in commercial goods such as newspapers magazines and cardboard boxes rodenburg et al 2010 thus pcb 11 is emerging as a marker of non legacy pcb contamination 3 4 pcb concentration dependencies on particle size and toc bbg u1 and umd i samples were separated into 75 μm and 25 75 μm size fractions at bbg u1 both fractions shared a similar homologue pattern and concentrations of all but mono and deca cbs were significantly higher in the 25 75 fraction compared to the 75 fraction p 0 001 fig 3 a in the 25 75 fraction total pcb concentration was 180 6 ng g which was approximately three times of the concentration in the 75 fraction 56 3 6 3 ng g for umd i hexa and hepta cbs were the most dominant in the 25 75 μm fraction while di cbs dominated the 75 μm fraction both fractions had mono and deca cbs below detection limit fig 3b no significant difference was observed between the di cbs in the two fractions p 0 10 both sites showed higher proportion of ortho chlorinated pcbs in the 75 μm fraction suggesting greater degree of pcb dechlorination in larger particles table 1 larger particles have lower pcb concentrations and as a result lower toxicity to microorganisms abraham et al 2002 the 75 μm fraction accounted for 98 1 of the total mass of the umd i sediment sample as a result 94 9 of the total pcb mass 96 5 of mono to tetra cbs and 94 1 of penta to deca cbs were sorbed to the 75 μm fraction thus the removal of the particles larger than 75 μm which is relatively easy to perform in a scm because they are easy to settle or be filtered kim and sansalone 2008 can remove approximately 94 of the pcb mass applied to a scm for bbg u1 the 75 μm fraction accounted for 93 3 of the total sediment mass when considering the mass of different bbg u1 fractions nearly 80 of total pcbs mass were sorbed to larger particles pcb distribution based on particle size have been studied on sediment samples from harbor point new york where the fine fraction 63 μm contributed approximately 80 of the pcb mass in the total sediment attributed to their large surface area ghosh et al 2003 this disagreement with the stormwater results may be attributed to the differences in the toc of different particle size fractions and the particle mass distribution in the sediment samples which were not mentioned carbonaceous particles contributed 60 90 of the pcbs in the sediment samples even though they accounted for only 5 7 of the total mass ghosh et al 2003 pcbs are generally associated with sediment or soil via two mechanisms adsorbed to carbonaceous sorbents on the surfaces and absorbed within the sorbent matrix by solvation with amorphous organic carbons beckingham and ghosh 2017 in both processes carbonaceous matter in sediments or soils plays an important role the toc content of bbg u1 was 1 30 which was higher than the value at 2 m 1 02 correspondingly the total pcb concentration in the total sediment at bbg u1 51 6 5 6 ng g was significantly higher than that at bbg u2 36 4 1 4 ng g p 0 01 fig 1a also the 25 75 μm fraction at bbg u1 with toc of 4 70 had a significantly higher pcb concentration than the 75 μm fraction toc 1 63 p 0 01 fig 3a the toc content in the umd i 25 75 μm fraction 9 54 was higher than that in the 75 μm fraction 1 14 higher pcb concentrations were found in the 25 75 μm fraction in both bbg u and umd i the total concentration of penta to deca cbs in the 25 75 μm fraction was more than two times of the concentration in the 75 μm fraction correlating with the toc contents of the two fractions pcbs with more chlorines are more hydrophobic resulting in a stronger sorption of pcbs to the organic matter on the sediment atsdr 2000 relations between pcbs and toc in stormwater sediments were also found in a study in norway which reported that the concentration of seven selected pcb congeners was strongly correlated with the toc content of sediments using pca jartun et al 2008 3 5 toxic equivalency values of the 209 congeners dlpcbs are of special concern due to their higher levels of health risks dlpcbs are congeners which have been shown to exert toxic responses similar to those observed for 2 3 7 8 tetrachlorodibenzodioxin 2 3 7 8 tcdd van den berg et al 1998 toxic equivalency factors tef for dlpcbs were set by the world health organization who van den berg et al 1998 van den berg et al 2006 toxic equivalency teq values are calculated by multiplying the mass concentrations of the 12 dlpcbs with the respective tef table 1 and are used by the u s environmental protection agency epa to account for how dioxin and dioxin like compounds vary in toxicity epa 2018 the teqpcb levels varied from 1 48 0 06 pg g to 14 8 3 1 pg g in the total sediment samples all samples except the 25 75 μm bbg u1 fraction were below the safe sediment value of 20 pg teq g eljarrat et al 2001 the teqpcb levels fall within the range of 0 03 24 8 pg g found in northwest mediterranean sediment eljarrat et al 2001 and found in core samples in indiana harbor and ship canal ihsc lake michigan u s 0 68 120 pg g and are similar to the teq value at the surface layer of one of the core samples martinez and hornbuckle 2011 the 25 75 μm fraction in bbg u1 had the highest teqpcb 40 3 3 6 pg g due to the consistently high total pcb concentrations found in that fraction except for btm u and cp r all other samples were close to the u s background level 2 5 ppb teq g epa 2010 btm u and cp r had values larger than 9 pg teq g indicating higher risk potentials at these sites however pcbs have been found to account for 1 84 of the total teqs in sediments eljarrat et al 2001 more information about other dioxin or dioxin like compounds is therefore needed to assess the risk of these stormwater sediment samples 4 conclusions this study investigated the concentrations of all 209 pcb congeners as well as homologue distribution in stormwater sediments at seven sites with different land use patterns results from this study suggest that land use pattern has an impact on pcb concentrations and homologue patterns in stormwater sediments smaller stormwater particles had an increased tendency to sorb pcbs than larger particles however greater pcb mass more than 80 was present in larger particles targeting sediments from high density urban areas could reduce a large portion of pcb stormwater load via particulate matter capture and removal pcb 11 was frequently detected in stormwater sediments and appears to be related to yellow pigments used in roadways additional studies are needed to clarify this relationship and to determine if pigments should be targeted as a nonpoint pcb source declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this study was funded by the maryland state highway administration office of environmental design appendix a supplementary data the following is the supplementary data to this article map and summary of the sediment sampling sites fig s1 and table s1 temperature program for gc table s2 comparison of pcb concentrations in the particulate phase in stormwater analyzed in this study with concentrations reported from other locations around the world table s3 multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j watres 2019 114865 
18602,polychlorinated biphenyls in stormwater sediments relationships with land use and particle characteristics siqi cao a staci l capozzi a b birthe v kjellerup a allen p davis a a department of civil and environmental engineering university of maryland college park md 20742 usa department of civil and environmental engineering university of maryland college park md 20742 usa department of civil and environmental engineering university of maryland college park maryland 20742 b geosyntec consultants columbia md 21046 usa geosyntec consultants columbia md 21046 usa geosyntec consultants columbia maryland 21046 corresponding author polychlorinated biphenyls pcbs are classified as persistent organic pollutants pops concentrations of 209 pcb congeners as well as profiles of the ten homologues were determined in stormwater sediments collected from various primarily roadway sites with different land use the total pcb concentrations ranged from 8 3 to 57 4 ng g dry weight dw with a mean value of 29 2 ng g dw pcb concentrations varied with nearby land use higher stormwater sediment pcb concentrations were found in dense urban areas average 39 8 10 5 ng g and residential areas average 35 3 6 2 ng g compared to highways passing through greenspace average 18 0 0 4 ng g the number of chlorines per biphenyl ranged from 3 63 to 5 39 and the toxic equivalency teqs of the pcbs were between 1 5 and 18 0 pg g at all sites a non aroclor congener pcb 11 was detected in all samples and was dominant at two sites pcbs were sorbed to smaller stormwater particulate matter 75 μm at higher concentrations compared to larger particles 75 μm pcb sorption tended to increase with the total organic carbon toc of the particulate matter in the sediment samples however greater pcb mass almost 80 was present in the larger particles information on sediment pcb concentrations from different land uses along with stormwater particulate matter data can allow the estimation of pcb loads and load reductions using stormwater control measures graphical abstract image 1 keywords polychlorinated biphenyls pcbs urban stormwater congeners pcb 11 toxicity teqs 1 introduction polychlorinated biphenyls pcbs are a group of chlorinated organic compounds derived from biphenyl with 1 10 of the hydrogen atoms substituted by chlorine thus 209 different congeners exist pcbs were commercially produced from 1929 to 1977 in the united states u s and widely used in industrial processes as coolants in transformer oils and as flame retardants kimbrough 1995 aroclor is the trade name for specific pcb mixtures that were manufactured in the u s in other countries names such as clophens phenoclors pyralenes were used safe 1994 the aroclors are identified by a four digit code such as aroclor 1254 the second two numbers in aroclors indicate the percentage of chlorine by mass in the mixture atsdr 2000 the most commonly manufactured aroclors in the u s were a1016 a1232 a1242 a1248 a1254 a1260 atsdr 2000 due to their high stability pcbs are persistent in the environment and are classified as persistent organic pollutants pops stockholm convention 2008 jones and de voogt 1999 in spite of the ban pcbs are still being released into the urban environment andersson et al 2015 buildings constructed before the 1970s can be sources of pcbs from building sealants and caulking diamond and hodge 2007 zhang et al 2011 while roads constructed with recycled construction and demolition waste c dw also can serve as pcb sources butera et al 2014 wahlström et al 2000 increased air concentrations and subsequent atmospheric deposition of pcbs contribute to the presence of pcbs in stormwater and surface waters diamond and hodge 2007 thus runoff from building roof and wall surfaces may contain pcbs urban runoff storm sewers are considered important sources of pollution for impaired rivers streams lakes reservoirs and ponds accounting for 13 19 of the total load epa 2012 other important probable sources include atmospheric deposition agriculture and contaminated sediments most previous stormwater studies have primarily addressed the presence of dissolved pcbs and did not consider particulate matter in stormwater as a separate phase for pcb transport gilbreath and mckee 2015 granier et al 1990 rossi et al 2004 a few studies evaluated particulate matter and the particle bound characteristics of pcbs hwang and foster 2008 jartun et al 2008 zgheib et al 2011b hwang and foster 2008 found that pcbs in stormwater particulate matter accounted for more than 90 of the total pcbs in storm flow this indicates that particulate matter should be the focal point when evaluating pcbs in stormwater hwang and foster 2008 however studied only 85 of the 209 pcb congeners jartun et al 2008 studied seven pcbs pcb 28 52 101 118 138 153 and 180 in urban runoff sediments similarly zgheib et al 2011b also focused on these seven pcb congeners because they were listed as priority pollutants in urban stormwater zgheib et al 2008 thus information about all 209 congeners in stormwater particulate matter is lacking in order to reduce pcb loadings to surface waters an understanding of the sources of pcbs in the watersheds is required hwang and foster 2008 to achieve this goal information about concentrations of all 209 congeners is important the abundance of some specific congeners may indicate certain sources also comparison of congener profiles in stormwater contaminated with pcb products may assist in source identification one congener of special concern is pcb 11 3 3 dichlorobiphenyl which is an unintentional by product from the manufacturing process of diarylide yellow pigments grossman 2013 because pcbs are sorbed to particulate matter they are expected to be removed from stormwater via particulate matter removal processes such as sedimentation and filtration thus it is important to study the affiliation of pcbs with particulate matter based on particle size however most relevant studies focused on the organic carbon fraction instead of particle size beless et al 2014 bucheli and gustafsson 2003 choi and al abed 2009 little research has been completed regarding pcb concentration and particle sizes ghosh et al 2003 especially as related to stormwater andersson et al 2015 marsalek and ng 1989 the objectives of the current study were 1 to evaluate pcb concentrations and congener distributions for stormwater particulates collected near urban areas 2 to identify potential sources of stormwater pcbs in urban areas and 3 to use the above information to assist with stormwater control measure scm implementation and design recommendations for the effective removal of pcbs from urban stormwater runoff to address these objectives surface sediment soil samples were collected from parking highway and residential areas in maryland u s and tested for pcbs 2 materials and methods 2 1 sampling sites samples were collected from september 2016 to july 2018 seven sampling sites were included in this study to represent different land uses 1 dense urban area stormwater gutters along roadways in bladensburg bbg u and baltimore btm u md 2 institutional area an inlet to a bioretention cell at the university of maryland umd i college park campus 3 commercial area a scm near a 4 lane state highway bv c 4 greenspace a stormwater channel adjacent to a highway r1 g and a stormwater gutter along roadways through wooded areas in laurel md r2 g and 5 residential area storm drains in a residential area in college park md cp r fig s1 shows a map of sampling sites in this study at bbg u two samples were collected from the gutter one bbg u1 was at 1 m and the other bbg u2 was 2 m from the drainage point the sample from the baltimore roadway btm u was collected from the catch basin of a gutter by dr james hunter from morgan state university at umd i one sediment sample was collected at the entrance of the stormwater inlet to a bioretention cell three samples were collected at bv c at the entrance bv c1 and the discharge bv c2 of the rip rap channel to the bioretention scm as well as inside the scm bv c3 at r1 g two sediment samples were collected at two parallel locations in the stormwater channel r1 g1 and r1 g2 one sediment sample was sampled from the gutter at r2 g at cp r three samples were collected from the gutter nearby cp r1 and near two different storm drains cp r2 and cp r3 site names with short descriptions as well as sampling details are summarized in table s1 at each site samples were collected once temperature difference in very different periods could affect the solubility and volatility of pcbs in addition the frequency of rain could affect the age of the sediments and pcbs accumulated in them thus all samples were collected during may to september with an antecedent dry period ranging from 1 to 7 days the temperature ranged from 23 c to 30 c during sample collection during the collection process a clean and sterile stainless steel soil scoop was used to collect surface sediments 0 10 cm deep at different sites to minimize biological and chemical transformations all samples were stored at 20 c in the dark in glass containers with teflon lids until analysis 2 2 sediment fractionation selected sediment samples were separated into three fractions as done by kim and sansalone 2008 briefly wet sieving no 200 sieve ele international canada was performed to separate the sediment fraction 75 μm from the other two fractions the filtrate was transferred to an imhoff cone and settled for 2 h the settleable fraction 25 75 μm was settled out in the imhoff cone and the remaining fraction was defined as the suspended fraction 25 μm the suspended fraction accounted for 1 of the total sediment mass in all collected samples thus this fraction was not further studied the 75 μm fraction and the 25 75 μm fraction were air dried in the fume hood for two days until they appeared dry clean sea sand merck u s was used as laboratory blank control triplicate and was exposed to the same treatments as the samples triplicate 2 3 extraction of pcbs microwave assisted extraction mae mars 6 cem u s was used to extract pcbs from the sediment samples the extraction method was based on lopez avila et al 1995 with minor modifications briefly 6 g of air dried sample was weighed and transferred to teflon extraction vessels 100 ml cem u s then 36 ml hexane 95 n hexane for organic residue analysis acetone hplc grade honeywell 1 1 was added into the vessels prior to extraction 20 μl of the mixed solution of surrogates 0 5 μg ml 1 tetrachloro m xylene tcmx 2 4 6 trichlorobiphenyl pcb 30 and 2 2 3 4 4 5 6 6 octachlorobiphenyl pcb 204 was added into each sample to calculate the recovery of the extraction procedure extractions were performed at 115 c for 10 min at 1000 w after extraction the vessels cooled to room temperature before they were opened the extracts were settled by gravity for 30 min and the supernatants were transferred into 60 ml amber vials the residues were washed with five ml of hexane and settled for 30 min five ml of hexane acetone 1 1 and five ml of acetone were added separately to wash the residues as mentioned above the supernatants were combined and then concentrated under nitrogen flow to less than 100 μl finally 1 ml of hexane hplc grade was added to dissolve the extracts extracts were stored at 20 c until cleanup 2 4 cleanup of pcbs extracts the cleanup method was based on epa method 3611b epa 1996 and 3620c epa 2014 with minor modifications briefly alumina fisher scientific u s was heated at 550 c for at least 24 h and cooled to room temperature in a desiccator deionized di water was added 30 μl di water g alumina to deactivate it to 3 columns for cleanup were made with six g of the prepared alumina a layer of oven dried sodium sulfate fisher scientific u s to remove water from the extracts and glass wool acros organics germany at the bottom of a glass disposable pipet pyrex u s first 20 ml hexane was added to equilibrate the column then the extracts were transferred into the column and the effluent was collected the amber vials were rinsed with 15 ml of hexane and the rinsate was added into the column the collected effluent was reduced in volume with nitrogen flow to concentrate the samples to less than 1 ml and spiked with 20 μl of the mixed solution of internal standards 0 5 μg ml 1 4 bromobiphenyl 4 bb and 2 2 4 5 5 pentabromobiphenyl penta bb hexane hplc grade was added to the concentrated effluent to reach a final volume of 1 ml and the samples were vortexed for 10 s before being transferred into gc vials for further analysis 2 5 gc analysis samples were analyzed by gas chromatography electron capture detector gc ecd 7890b agilent technologies u s equipped with an agilent j w hp 5ms column 60 m 250 μm x 0 25 μm the samples were autosampled autosampler 7693 agilent technologies u s with an injection volume of 1 μl helium was used as the carrier gas the temperature program table s2 was developed based on a congener specific pcb analysis method used for analysis of pcb 11 guo 2013 target compounds surrogate standards and internal standards were purchased from accustandard u s and restek u s 2 6 quality control all laboratory materials were made either of glass or teflon to avoid sample contamination to avoid contamination during sample preparation all glassware used were cleaned by detergent rinsed with hexane acetone methanol and di water and baked in a muffle furnace at 550 c for 4 h teflon containers were cleaned by detergent ultrasonicated with hexane and acetone and rinsed with di water during each run clean hexane vials were added at the beginning and the end of each run to avoid significant carry over from previous runs the results from the laboratory blanks and clean hexane at the end of each run indicated that carry over did not occur between samples for each sample an extra treatment group triplicate was prepared as a standard control the standard control treatment was treated the same way as other treatments except for the addition of surrogate standards or internal standards this treatment was carried out to confirm the standards were not present in the sediment samples all 209 pcb congeners were analyzed and 131 peaks were detected in the samples all peaks were verified by mass spectrometry by m z and retention time 5977a msd agilent technologies u s detection limits for pcb congeners ranged from 0 0008 to 0 33 ng g the method detection limits were obtained by dividing the instrument detection limits by the sample masses for statistical analysis all values below the detection limits were substituted with half of the detection limits concentrations of the target compounds were compared to average concentrations in laboratory blanks and calculated based on audy et al 2018 if the blank average was 10 of the amount in the sample 178 195 congeners no correction was applied if the blank average was 10 35 of the concentration in the samples 11 21 congeners the average concentration in the blanks were subtracted from the sample if the blank average was above 35 3 10 congeners the compound was reported as below the detection limit in the sample the measured concentrations of mono to tetra cbs were corrected for the recovery of tcmx and the measured concentrations of penta to deca cbs were corrected for the recovery of pcb 204 average surrogate recoveries were 73 0 for tcmx range 54 8 91 9 and 83 0 for pcb 204 range 50 8 114 0 all were within the acceptable range of 50 125 hermanson and johnson 2007 4 bb was the internal standard for mono to tetra cbs and penta bb was the internal standard for penta to deca cbs 2 7 total organic carbon toc measurement total carbon tc and inorganic carbon ic were measured using a toc analyzer toc l shimadzu japan with solid sampling module ssm 5000a all sediment samples were measured in triplicate toc was calculated by subtracting ic from tc 2 8 data analysis concentrations of the homologues were calculated based on data from gc analysis concentrations were equally distributed between congeners when there was co elution 12 dioxin like pcbs dlpcbs pcbs 77 81 126 169 105 114 118 123 156 167 189 are of special concern due to their elevated toxicity and their individual concentrations were also calculated the average number of chlorines per biphenyl was determined based on a molar average for the comparison of two groups of data f tests were performed to test the equality of two variances and student t tests were performed to examine differences among different sites and sediment fractions statistical significance was set at p 0 05 for comparison of three groups of data pairwise t tests was performed and holm s sequential bonferroni method was applied to correct the results by reducing the possibility of type ι error bunzel et al 2013 huang et al 2018 3 results and discussion 3 1 pcb concentrations at different land uses pcbs were detected in the samples from all the studied sites fig 1 the highest total pcb concentration 51 6 5 6 ng g in this study was found in bbg u1 1 m from a stormwater inlet fig 1a bbg u2 had a total pcb concentration of 36 4 1 4 ng g the second highest pcb concentration 41 4 5 6 ng g was found in cp r from the residential area fig 1c where a parking area and residential buildings are nearby umd i collected at the entrance of the bioretention cell contained high concentrations of pcbs at 37 3 6 3 ng g fig 1g btm u contained pcbs at 31 5 2 3 ng g fig 1b total pcb concentrations varied at bv c fig 1d the lowest pcb concentrations in this study were found at r1 g 17 8 3 9 ng g and r2 g 18 3 1 1 ng g fig 1e and f the concentrations of total pcbs in stormwater particulate matter 9 8 51 6 ng g were in the range of concentrations observed in several previous studies 0 4 755 ng g table s3 zgheib et al 2011b measured concentrations of seven selected pcb congeners with values ranging from below 10 ng g to 60 ng g in the particulate matter in stormwater higher than those in this study 0 00167 1 92 ng g potential reasons for the higher concentrations found in zgheib et al 2011b include 1 the area in the zgheib study was a watershed in a dense urban area with commercial centers apartments and buildings and 2 the study was carried out at least six years ago in france where pcbs were banned several years later than the u s zgheib et al 2011b the consensus based threshold effect concentration tec based on various sediment quality guidelines for pcbs in freshwater ecosystems is 59 8 ng g macdonald et al 2000 for a total pcb concentration below 59 8 ng g harmful effects on sediment dwelling organisms are unlikely to be observed the interim sediment quality guidelines isqgs and the probable effect levels pel for pcbs in freshwater sediment are 34 1 ng g and 277 ng g respectively ccme 2001 both sediment samples from the bbg u gutter as well as sediment samples from cp r1 cp r2 and umd i bioretention cells were above the isqgs for pcbs in freshwater sediment suggesting adverse effects from pcbs exposure but were below the consensus based tec and pel all the other samples were below the isqg indicating a low possibility of adverse effects at these sites both bbg u samples as well as umd i had similar pcb homologue patterns penta to hepta cbs were the most dominant among all the homologues di cbs also had a high abundance at umd i accounting for 25 1 2 0 of total pcbs btm u was also similar to bbg u where pcbs with two five and six chlorines dominated all three cp r residential samples had similar homologue patterns di cbs and hexa cbs were dominant followed by penta cbs and tetra cbs at the bv c scm the two channel locations bv c1 and bv c2 showed similar homologue patterns penta and hexa cbs were the most dominant however the homologue pattern inside the scm bv c3 was different from that in the rip rap channel tetra to hexa cbs were present at high concentrations 1 64 0 31 ng g 2 33 0 60 ng g compared to other homologues at r1 g and r2 g di cbs was the most dominant homologue 7 61 1 55 and 7 03 1 13 ng g respectively the amounts of mono to tetra cbs and penta to deca cbs were similar at these two sites this result was noticeably different from the other sites where the concentrations of penta to deca cbs were higher than mono to tetra cbs the highest number of chlorines per biphenyl was found in bbg u 1 m from the stormwater inlet table 1 highly chlorinated pcbs cl 5 mainly experience biotransformation through dechlorination anaerobic conditions are essential for this process the process however is slow compared to degradation of lower chlorinated pcbs cl 4 which can be degraded aerobically passatore et al 2014 payne et al 2011 products of anaerobic dichlorination act as substrates for the aerobic biodegradation processes passatore et al 2014 except for cp r1 r1 g r2 g and bv c3 all the other samples had a chlorine number greater than four the lowest numbers of chlorines were found at r1 g and r2 g due to the dominance of di cbs at these two sites the easier loss of chlorines in the meta and para positions results in an increase in the level of ortho chlorinated pcbs during microbial dechlorination epa 2004 thus the low levels of ortho chlorinated pcbs in bbg u cp r2 cp r3 and r2 g suggest limited or no reductive dechlorination table 1 the level of ortho chlorinated pcbs at the other sites were at least four fold higher which suggest impacts from microbial dechlorination the level of ortho chlorinated pcbs in btm u was high compared to bbg u samples indicating dechlorination taking place at this site a lower chlorine number per biphenyl in btm u could also support the occurrence of dechlorination 3 1 1 comparison of sites results from this study suggest an influence of land use on the pcb concentrations and homologue patterns the ranges and means of pcb concentrations based on different land uses are summarized in table 1 higher pcb concentrations were found in dense urban areas average 39 8 ng g and residential areas average 35 3 ng g compared to greenspace average 18 0 ng g a similar trend was also found in a study carried out in france zgheib et al 2011a in the zgheib et al study the median concentration of seven selected pcbs pcb 28 52 101 118 138 153 180 in stormwater was greatest in a highly dense area at 468 ng l the median concentrations in residential area and dense urban area were lower at 211 ng l and 259 ng l respectively the building fronting the bbg u samples was brick built in 1968 pcbs can be leached or washed from sealants and caulking on building exteriors also elevated pcb concentrations in indoor air could vent into the outdoor air the total emission of pcbs estimated from one intensively studied office in toronto canada was 280 5879 ng h and up to 90 of total losses could be to the outdoors based on the air exchange rate zhang et al 2011 additionally stormwater runoff could wash flakes of material containing pcbs from the surface of older buildings this is supported by research which found that among 80 buildings constructed from 1945 to 1980 in toronto canada the mean concentration of pcbs was 4630 mg kg 1 of sealants diamond et al 2010 diamond et al 2010 also estimated that concentrations of pcbs in sealants and caulking were geographically higher in residential areas within buildings constructed during 1950 1970 brick research educational buildings dating from the 1970 s to 1990 s are 50 100 m away from umd i the cp r residential buildings primarily built from wood were constructed in 2011 the scm at bv c is about 40 m from a low rise commercial building built in 1986 and is surrounded by a small parking lot approximately 40 spots on the rip rap channel side on the other sides it is surrounded by a 4 lane highway and local roads the stormwater treatment media elevation was high at the end of the channel which allowed sediments to deposit and not enter the scm possibly causing a higher pcb concentration at the end of the channel no buildings were nearby r1 g and r2 g a major difference compared to the other sites instead these two highways traversed through forested areas as a result the lower levels of pcbs detected in the sediments at these locations could be a consequence of the absence of buildings surrounding the collection site therefore the measured pcb concentrations at these two sites can be assumed to result only from highway sources overall the results of this analysis suggest that buildings and especially older buildings contribute significantly as pcb sources in urban areas also roads themselves may play a lesser role in pcb contamination compared to buildings since cp r had comparatively new buildings the relatively high pcb concentrations at this site may result from the use of recycled building materials or pcb byproducts from paints anezaki and nakano 2014 3 2 comparison of congener profiles among different sites none of the sediment samples had a similar congener pattern with any aroclor mixture indicating that aroclors were not the direct sources or the aroclor mixtures were weathered due to biodegradation and or mixing however some congeners detected at relatively high abundance in the sediment samples were also present in a1254 or a1260 indicating that some congeners may have originated from these aroclors relative abundance of mass of the dominant congeners are summarized in table 2 pcb 11 was only congener detected at all sites at a relatively high abundance it is a non aroclor congener and is inadvertently produced during the manufacturing of paint pigments hu and hornbuckle 2010 pcb 99 110 180 193 and 194 were detected with high abundance in samples from bbg u btm u and umd i pcb 99 and pcb 110 are important components of a1254 pcb 99 is also a dechlorination product from a1260 fagervold et al 2007 pcb 194 accounted for 2 1 of the total pcbs in a1260 frame et al 1996 pcb 194 showed no dechlorination after 500 days of incubation with baltimore harbor sediment microcosms fagervold et al 2007 indicating its stability pcb 180 accounted for 11 4 of a1260 overall the dominant congeners in these sediment samples were present and in some cases abundant in a1254 and a1260 the cp r congener fingerprints were different from the other sites with pcbs 11 14 141 and 161 dominating pcb 141 accounted for 2 6 of total pcbs in a1260 and 1 0 in a1254 frame et al 1996 pcb 99 was dominant at bv c 4 5 22 5 pcb 8 was detected with high abundance at this site it was one of the major congeners detected in polycyclic type pigments anezaki and nakano 2014 overall sediments from dense urban and institutional sites were dominated by congeners present in a1254 residential sites had smaller signatures of a1254 a1254 has been reported to have been added into sealants and caulking compounds atsdr 2000 this information supports the hypothesis that old buildings remain as important pcb sources to stormwater pcb 11 and 14 dominated in the sediments from greenspace r1 g and r2 g 3 3 concentrations and potential sources of pcb 11 the concentration and relative abundance of pcb 11 at the different study sites showed that the highest concentration was found at umd i fig 2 ranging between 4 47 and 6 58 ng g it was the most dominant congener and accounted for 15 of the total pcb mass r2 g also had a high concentration of pcb 11 5 66 1 07 ng g accounting for 30 9 of total pcbs at this site at the other sites pcb 11 was also detected but at lower concentrations compared to umd i and r2 g ranging from 0 33 0 08 to 3 52 0 10 ng g the relative abundance of pcb 11 compared to total pcbs at these sites ranged from 1 3 to 11 2 these results indicate that pcb 11 is an important component of the total pcb fingerprint in urban stormwater and the surrounding area yellow flakes were noted in the umd i samples the larger yellow flakes 0 5 4 mm in diameter were separated from the sediment by hand and both were analyzed separately for pcb concentrations without the large yellow flakes the concentration of pcb 11 was 3 8 0 7 ng g in the sediment which was significantly different from the concentration 5 4 0 1 ng g measured with the flakes included p 0 02 the separated yellow flakes were tested using the same procedure as the sediment and the concentration of pcb 11 was 182 10 ng g the yellow flakes appear to be yellow traffic paint pcb 11 has been detected as the major congener present in azo type pigments anezaki and nakano 2014 it has been frequently detected in urban air chicago and pigments or dyes hu and hornbuckle 2010 hu et al 2008 among all 209 congeners pcb 11 was found in 13 of the 33 commercial paint pigments tested hu and hornbuckle 2010 near new york new jersey harbor pcb 11 concentrations in the effluents of the two wastewater treatment plants which receive wastewater from pigment manufacturing plants ranged from 5 to 116 ng l while pcb 11 concentrations ranged from 0 0016 to 9 4 ng l in the effluents from other wastewater treatment plants rodenburg et al 2010 pcb 11 has been frequently detected in commercial goods such as newspapers magazines and cardboard boxes rodenburg et al 2010 thus pcb 11 is emerging as a marker of non legacy pcb contamination 3 4 pcb concentration dependencies on particle size and toc bbg u1 and umd i samples were separated into 75 μm and 25 75 μm size fractions at bbg u1 both fractions shared a similar homologue pattern and concentrations of all but mono and deca cbs were significantly higher in the 25 75 fraction compared to the 75 fraction p 0 001 fig 3 a in the 25 75 fraction total pcb concentration was 180 6 ng g which was approximately three times of the concentration in the 75 fraction 56 3 6 3 ng g for umd i hexa and hepta cbs were the most dominant in the 25 75 μm fraction while di cbs dominated the 75 μm fraction both fractions had mono and deca cbs below detection limit fig 3b no significant difference was observed between the di cbs in the two fractions p 0 10 both sites showed higher proportion of ortho chlorinated pcbs in the 75 μm fraction suggesting greater degree of pcb dechlorination in larger particles table 1 larger particles have lower pcb concentrations and as a result lower toxicity to microorganisms abraham et al 2002 the 75 μm fraction accounted for 98 1 of the total mass of the umd i sediment sample as a result 94 9 of the total pcb mass 96 5 of mono to tetra cbs and 94 1 of penta to deca cbs were sorbed to the 75 μm fraction thus the removal of the particles larger than 75 μm which is relatively easy to perform in a scm because they are easy to settle or be filtered kim and sansalone 2008 can remove approximately 94 of the pcb mass applied to a scm for bbg u1 the 75 μm fraction accounted for 93 3 of the total sediment mass when considering the mass of different bbg u1 fractions nearly 80 of total pcbs mass were sorbed to larger particles pcb distribution based on particle size have been studied on sediment samples from harbor point new york where the fine fraction 63 μm contributed approximately 80 of the pcb mass in the total sediment attributed to their large surface area ghosh et al 2003 this disagreement with the stormwater results may be attributed to the differences in the toc of different particle size fractions and the particle mass distribution in the sediment samples which were not mentioned carbonaceous particles contributed 60 90 of the pcbs in the sediment samples even though they accounted for only 5 7 of the total mass ghosh et al 2003 pcbs are generally associated with sediment or soil via two mechanisms adsorbed to carbonaceous sorbents on the surfaces and absorbed within the sorbent matrix by solvation with amorphous organic carbons beckingham and ghosh 2017 in both processes carbonaceous matter in sediments or soils plays an important role the toc content of bbg u1 was 1 30 which was higher than the value at 2 m 1 02 correspondingly the total pcb concentration in the total sediment at bbg u1 51 6 5 6 ng g was significantly higher than that at bbg u2 36 4 1 4 ng g p 0 01 fig 1a also the 25 75 μm fraction at bbg u1 with toc of 4 70 had a significantly higher pcb concentration than the 75 μm fraction toc 1 63 p 0 01 fig 3a the toc content in the umd i 25 75 μm fraction 9 54 was higher than that in the 75 μm fraction 1 14 higher pcb concentrations were found in the 25 75 μm fraction in both bbg u and umd i the total concentration of penta to deca cbs in the 25 75 μm fraction was more than two times of the concentration in the 75 μm fraction correlating with the toc contents of the two fractions pcbs with more chlorines are more hydrophobic resulting in a stronger sorption of pcbs to the organic matter on the sediment atsdr 2000 relations between pcbs and toc in stormwater sediments were also found in a study in norway which reported that the concentration of seven selected pcb congeners was strongly correlated with the toc content of sediments using pca jartun et al 2008 3 5 toxic equivalency values of the 209 congeners dlpcbs are of special concern due to their higher levels of health risks dlpcbs are congeners which have been shown to exert toxic responses similar to those observed for 2 3 7 8 tetrachlorodibenzodioxin 2 3 7 8 tcdd van den berg et al 1998 toxic equivalency factors tef for dlpcbs were set by the world health organization who van den berg et al 1998 van den berg et al 2006 toxic equivalency teq values are calculated by multiplying the mass concentrations of the 12 dlpcbs with the respective tef table 1 and are used by the u s environmental protection agency epa to account for how dioxin and dioxin like compounds vary in toxicity epa 2018 the teqpcb levels varied from 1 48 0 06 pg g to 14 8 3 1 pg g in the total sediment samples all samples except the 25 75 μm bbg u1 fraction were below the safe sediment value of 20 pg teq g eljarrat et al 2001 the teqpcb levels fall within the range of 0 03 24 8 pg g found in northwest mediterranean sediment eljarrat et al 2001 and found in core samples in indiana harbor and ship canal ihsc lake michigan u s 0 68 120 pg g and are similar to the teq value at the surface layer of one of the core samples martinez and hornbuckle 2011 the 25 75 μm fraction in bbg u1 had the highest teqpcb 40 3 3 6 pg g due to the consistently high total pcb concentrations found in that fraction except for btm u and cp r all other samples were close to the u s background level 2 5 ppb teq g epa 2010 btm u and cp r had values larger than 9 pg teq g indicating higher risk potentials at these sites however pcbs have been found to account for 1 84 of the total teqs in sediments eljarrat et al 2001 more information about other dioxin or dioxin like compounds is therefore needed to assess the risk of these stormwater sediment samples 4 conclusions this study investigated the concentrations of all 209 pcb congeners as well as homologue distribution in stormwater sediments at seven sites with different land use patterns results from this study suggest that land use pattern has an impact on pcb concentrations and homologue patterns in stormwater sediments smaller stormwater particles had an increased tendency to sorb pcbs than larger particles however greater pcb mass more than 80 was present in larger particles targeting sediments from high density urban areas could reduce a large portion of pcb stormwater load via particulate matter capture and removal pcb 11 was frequently detected in stormwater sediments and appears to be related to yellow pigments used in roadways additional studies are needed to clarify this relationship and to determine if pigments should be targeted as a nonpoint pcb source declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this study was funded by the maryland state highway administration office of environmental design appendix a supplementary data the following is the supplementary data to this article map and summary of the sediment sampling sites fig s1 and table s1 temperature program for gc table s2 comparison of pcb concentrations in the particulate phase in stormwater analyzed in this study with concentrations reported from other locations around the world table s3 multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j watres 2019 114865 
18603,emerging water treatment technologies using ferrous and zero valent iron show promising virus mitigation by both inactivation and adsorption in this study iron electrocoagulation was investigated for virus mitigation in drinking water via bench scale batch experiments relative contributions of physical removal and inactivation as determined by recovery via ph 9 5 beef broth elution were investigated for three mammalian viruses adenovirus echovirus and feline calicivirus and four bacteriophage surrogates fr ms2 p22 and φx174 though no one bacteriophage exactly represented mitigation of the mammalian viruses in all water matrices bacteriophage φx174 was the only surrogate that showed overall removal comparable to that of the mammalian viruses bacteriophages fr ms2 and p22 were all more susceptible to inactivation than the three mammalian viruses raising concerns about the suitability of these common surrogates as indicators of virus mitigation to determine why some bacteriophages were particularly susceptible to inactivation mechanisms of bacteriophage mitigation due to electrocoagulation were investigated physical removal was primarily due to inclusion in flocs while inactivation was primarily due to ferrous iron oxidation greater electrostatic attraction virus aggregation and capsid durability were proposed as reasons for virus susceptibility to ferrous based inactivation results suggest that overall treatment claims based on bacteriophage mitigation for any iron based technology should be critically considered due to higher susceptibility of bacteriophages to inactivation via ferrous oxidation graphical abstract image 1 keywords adenovirus coagulation disinfection echovirus feline calicivirus ferrous iron 1 introduction from 1993 to 2012 viruses were responsible for at least 24 us drinking water outbreaks reported to the centers for disease control and prevention cdc or 9 of all reported drinking water outbreaks in the us centers for disease control and prevention 2015 viruses may be responsible for many more outbreaks that are unreported or of unknown etiology xagoraraki et al 2014 most waterborne viruses follow a fecal oral route of infection meaning sewage impaired waters are a primary cause of infection xagoraraki et al 2014 worldwide 1 8 billion people rely on sewage contaminated drinking water gall et al 2015 viruses are persistent in the environment and resistant to many water treatment disinfection processes centers for disease control and prevention 2012 in addition virus small size makes them difficult to remove by particle separation tanneru and chellam 2013 among the viruses identified on the us environmental protection agency s contaminant candidate list ccl4 are caliciviruses including norovirus adenoviruses and enteroviruses including echovirus us environmental protection agency 2016 norovirus is the leading cause of infectious diarrhea worldwide causing as many as half of all gastroenteritis outbreaks grabow 2007 hall 2012 norovirus is characterized by high contagiousness effective transmission and rapid evolution hall 2012 due to difficulty in culturing human norovirus surrogates such as feline calicivirus or murine norovirus are often used in laboratory tests bae and schwab 2008 cannon et al 2006 adenoviruses can cause gastroenteritis in humans as well as conjunctivitis and respiratory disease world health organization 1996 adenoviruses are persistent in the environment and resistant to adverse conditions as well as ultraviolet uv irradiation grabow 2007 echoviruses are common pathogens in human impacted water systems echoviruses cause a range of diseases in humans including gastroenteritis meningitis fever and respiratory disease world health organization 1996 with diameters typically less than 30 nm echoviruses are also among the smallest viruses grabow 2007 therefore norovirus adenovirus and echovirus provide a representative suite of viruses for evaluating treatment process efficacy due to relevance e g ccl4 resistance to inactivation and resistance to physical separation electrocoagulation ec is a promising technology for small scale water treatment systems due to its portability and potential for automation ec is the in situ production of coagulant by passing electrical current through a zero valent sacrificial electrode typically consisting of iron or aluminum portability and potential for automation make ec a good candidate for small scale water treatment in rural or emergency applications small scale treatment systems are an important market as more than half of the public water systems in the us serve fewer than 500 people edr group 2013 recently ec has been considered for mitigating viruses in drinking water heffron et al 2019b heffron and mayer 2016 tanneru and chellam 2013 2012 zhu et al 2005a ec has shown promising results in treating bacteriophage ms2 surpassing the surface water treatment rule of 4 log virus reduction and outperforming conventional chemical coagulation for ms2 mitigation in some water matrices tanneru and chellam 2013 zhu et al 2005b in iron ec iron is released in solution as ferrous ions fe2 lakshmanan and clifford 2009 li et al 2012 oxidation of ferrous iron during ec can inactivate e coli delaire et al 2015 and steel electrodes have demonstrated higher effectiveness than aluminum or graphite electrodes for mitigating e coli ndjomgoue yossa et al 2015 ferrous iron oxidation also inactivates bacteriophages jeong et al 2006 kim et al 2011 inactivation may be preferable to physical removal because active viruses removed via coagulation flocculation could create a disposal hazard for the settled solids in addition promoting inactivation as well as physical removal via ec may lead to greater overall virus mitigation however the relative contributions of ferrous iron inactivation and physical removal have not been determined for virus inactivation during iron ec bacteriophages are used as surrogates for human viruses in water treatment process research amarasiri et al 2017 grabow 2001 heffron and mayer 2016 compared to human viruses bacteriophage surrogates have simpler quantification and propagation protocols propagate rapidly and are safer to handle to the authors knowledge bacteriophage ms2 has been the only virus investigated for ec or ferrous iron inactivation kim et al 2011 tanneru et al 2014 tanneru and chellam 2013 2012 zhu et al 2005a ms2 is small approximately 25 nm diameter and negatively charged at neutral ph mayer et al 2015 therefore ms2 is a representative surrogate for physical treatment processes because its small charged capsid is difficult to destabilize by charge neutralization or remove by size exclusion however the suitability of any surrogate must be investigated for each novel application in the case of ec ms2 s negative charge and small size may make the bacteriophage more susceptible to transport to the anode surface and or electrostatic attraction to a ferrous disinfectant in comparison to human viruses the goal of this research was to determine the fate of viruses during ec as well as the suitability of bacteriophage surrogates to indicate enteric virus mitigation in drinking water due to ec fate of viruses was distinguished as physical removal or apparent inactivation by comparing physical removal of flocs by microfiltration and elution of the bulk solution for recovery of infectious viruses the effect of ph and other water parameters on virus mitigation was also investigated to assess the suitability of bacteriophage surrogates in a range of water matrices to determine the mechanisms of bacteriophage mitigation log reduction of bacteriophages due to ec was compared to chemical coagulation with ferrous and ferric chloride sorption on floc surfaces and electrooxidation with insoluble titanium electrodes 2 materials and methods 2 1 electrocoagulation ec tests were conducted in a 500 ml glass beaker with two plate electrodes 60 cm2 submerged area 1 cm inter electrode distance consisting of iron mild steel as described by maher et al 2019 constant current 100 ma was supplied by a sorensen xel 60 1 5 variable dc power supply ametek san diego ca over a retention time of 5 min this current and retention time were selected to achieve measurable log reduction of viruses in a range of water matrices current polarity was alternated at regular intervals 30 s to maintain even electrode wear and prevent passivation maher et al 2019 the reactor was stirred with a magnetic stir bar at a rate of 60 rpm ḡ of approximately 25 s 1 although the presence of the large plate electrodes precludes accurate calculation this stir rate was sufficient to maintain circulation between the electrodes and achieved greater bacteriophage mitigation than more rapid stirring 120 rpm heffron 2019 the electrodes and polarity alternating controller were kindly provided by a o smith corporation electrodes were polished with 400 si c sandpaper washed with ultrapure water and sterilized with uv light 30 min on each side in a biological safety cabinet before each test all tests were performed in triplicate and compared to a control reactor not receiving treatment all tests were performed in synthetic water matrices by adding constituents to purelab ultrapure water elga labwater uk sodium nitrate 3 3 mm was chosen as a monovalent background electrolyte because multivalent ions can form complexes with protein moieties and thus impact surface charge chen and soucie 1986 michen and graule 2010 nitrate was chosen over chloride to avoid inactivation due to free chlorine because chloride ions can be oxidized to form free chlorine during ec tanneru et al 2014 sodium bicarbonate was also added to achieve alkalinity typical of soft to moderately alkaline water 50 mg l as caco3 and prevent dramatic ph fluctuations not representative of natural water matrices mechenich and andrews 2004 total and ferrous iron generation due to ec was measured using hach ferrover total iron and ferrous iron reagent hach loveland co respectively after ec electrodes were rinsed with a small volume 5 ml of ultrapure water to remove adsorbed flocs generation of free chlorine was measured using hach dpd free chlorine reagent after the addition of reagent sample absorbance was measured using a genesys 20 spectrophotometer thermo fisher scientific waltham ma at 510 nm total and ferrous iron and 530 nm free chlorine 2 2 effect of water constituents on virus mitigation in independent tests ph chloride turbidity and natural organic matter nom were adjusted to assess their impact on virus mitigation the water constituents and concentrations for all test waters are provided in table 1 to determine the effect of water quality on virus mitigation the background electrolyte solution was altered to compare to ec performance in the nano3 nahco3 electrolyte the ph of the test water was adjusted using 0 5 n hno3 or naoh a symphony benchtop multi meter vwr batavia il was used to measure ph chloride 115 mg l cl was added by replacing the background electrolyte nano3 with nacl to assess the impact of nom total organic carbon was increased by adding 15 mg l c suwannee river nom ihss st paul mn the total organic carbon contributed by virus stocks to the synthetic waters was approximately 6 5 mg l c for turbidity tests a2 test dust powder technology inc arden hills mn was added to achieve approximately 50 ntu nom and turbidity conditions were chosen to represent challenging surface waters for drinking water treatment 2 2 1 chemical coagulation chemical coagulation using ferrous chloride fecl2 and ferric chloride fecl3 was compared to ec to help determine the susceptibility of bacteriophages to inactivation physical removal fecl2 versus physical adsorption alone fecl3 doses of 2 3 mg fe l were used to approximate doses achieved by ec batch tests test waters were prepared to maintain similar conductivity to ec tests while also providing more sodium bicarbonate alkalinity 150 mg l as caco3 to prevent ph fluctuation upon addition of coagulant salts as shown in table 1 2 2 2 pre formed flocs viruses were added to pre formed flocs created by ec to test for the importance of sorption to the surfaces of flocs ec reactors were operated as for regular ec tests except that viruses were only added to the solution after the reaction had completed viruses were retained for the same amount of time 5 min under slow mixing 60 rpm prior to sampling 2 2 3 titanium electrodes to determine the potential for non ferrous oxidant generation and oxidation at the electrode surface iron electrodes were replaced with non sacrificial grade 2 titanium plate electrodes performance titanium san diego ca of the same dimensions 60 cm2 submerged area 1 cm inter electrode distance titanium is oxidized in air to form a passive inert electrode surface bagotsky 2006 titanium electrooxidation reactors were operated with the same parameters as the iron ec reactor 100 ma 5 min 30 s polarity reversal interval as described in section 2 1 2 3 virus propagation four bacteriophages were used as model viruses ms2 fr p22 and φx174 the properties of these bacteriophages are summarized in table 2 in addition three mammalian viruses were tested in varying water matrices adenovirus 4 adv echovirus 12 ecv and feline calicivirus fcv a surrogate for human norovirus also summarized in table 2 bacteriophages were stored at 4 c while viruses were stored at 20 c cryopreservant was not used to prevent adding oxidant demand associated with the virus stock solutions bacteriophages were spiked at concentrations of approximately 107 pfu ml while mammalian viruses were spiked at approximately 104 tcid50 ml due to limitations in virus propagation bacteriophages were propagated using the double agar layer dal method in tryptic soy agar bd franklin lakes nj adams 1959 mammalian viruses were propagated in cell cultures see supplementary information si 1 in sterile 175 cm2 culture flasks until cell monolayers were reduced to approximately 10 20 confluence then subjected to three freeze thaw cycles 20 c 22 c all bacteriophages and mammalian viruses were purified by two cycles of polyethylene glycol peg precipitation followed by a vertrel xf dupont wilmington de extraction as described by mayer et al 2008 2 4 virus sampling and quantification virus samples were taken immediately after ec two samples were taken from each reactor including the control untreated reactor first a filtered sample was collected using sterile 20 ml syringes and 0 45 μm ptfe syringe filters some form of physical separation is required in any coagulation process microfiltration was chosen for this study to thoroughly separate flocs without a long flocculation step the filter was primed with 15 ml of sample before reserving 4 ml of filtrate the reactor was then homogenized by rapid stirring 600 rpm for 15 s and a 20 ml sample was taken for virus elution to determine the total concentration of viable viruses in the bulk solution to dissolve flocs and increase electrostatic repulsion between coagulant and viruses elution was performed by adding an equal volume of ph 9 5 6 beef extract himedia west chester pa to homogenize samples and vortexing for approximately 10 s samples containing bacteriophages were diluted in tenfold series and ten 10 μl drops of each dilution were plated using the spot titer plaque assay method as described by beck et al 2009 mammalian viruses were quantified using the reed muench tcid50 method reed and muench 1938 virus recovery was confirmed in numerous tests e g at ph 8 fig 1 and in waters containing turbidity and nom fig 2 confirmation of bacteriophage recovery by elution was demonstrated using chemical coagulation with ferric chloride fig 3 virus mitigation total reduction in the number of infectious viruses was distinguished as inactivation or physical removal based on recovery of infectious viruses from the filtered and eluted samples the log reduction in infectious viruses between the filtered control and filtered treated samples represented total mitigation eqn 1 the log reduction in infectious viruses between the eluted control and eluted treated samples represented inactivation i e viruses that could not be recovered from the bulk solution including flocs eqn 2 mitigation due to physical removal was therefore the difference between total mitigation and inactivation i e the fraction of total mitigation that was recoverable from the bulk solution by elution eqn 3 1 t o t a l m i t i g a t i o n f i l t r a t e c o n t r o l f i l t r a t e t r e a t e d 2 i n a c t i v a t i o n e l u a t e c o n t r o l e l u a t e t r e a t e d 3 p h y s i c a l r e m o v a l t o t a l m i t i g a t i o n i n a c t i v a t i o n notably we attributed all irrecoverable loss of infectious viruses to apparent inactivation in this study however recovery of viruses from solid phases such as flocs is challenging and losses could also reflect variation in elution efficiency as discussed further in section 3 2 3 the elution method used in this study was previously used to determine virus inactivation due to ec and chemical coagulation heffron et al 2019a matsui et al 2003 tanneru et al 2014 here it showed reliable recovery of bacteriophages after coagulation with ferric chloride fr 98 ms2 59 p22 55 φx174 99 fig 3 the inactivated fraction represents non recoverable viruses for fecl3 treatment recovery of mammalian viruses after ec also showed reliable recovery for example at ph 7 adv 100 ecv 58 fcv 100 fig 1 irrecoverable virus mitigation via ec was compared to these benchmark recoveries in order to verify inactivation and in many cases inactivation was multiple logs greater molecular methods e g qpcr can be effective in qualitatively determining inactivation by either demonstrating genome damage or in combination with cultural techniques showing the presence of viral genomes in the absence of infectious viruses previous work has demonstrated bacteriophage inactivation due to ferrous and zero valent iron using both immunosorbent assay and qpcr kim et al 2011 however a reliable molecular method for quantifying virus inactivation is not currently available likewise molecular methods cannot distinguish between aggregated and dispersed virions so techniques like qpcr cannot be used to validate virus elution since nucleic acids are also susceptible to both physical adsorption and oxidative fragmentation even time and resource intensive combined cultural and molecular methods still have a degree of uncertainty in quantitatively determining fate therefore cultural assays showing irrecoverable mitigation with a proven method of recovery remain the most direct means of quantifying inactivation in processes that also exhibit physical removal heffron and mayer 2016 2 5 mechanisms of virus mitigation to establish mechanisms of virus mitigation log reduction due to ec was compared to similar physical chemical processes chemical coagulation and electrochemical oxidation these tests were only performed with bacteriophages due to limited inactivation of mammalian viruses by ec 2 6 zeta potential and particle size measurement the zeta potential of bacteriophage fr and a2 test dust were confirmed by dynamic light scattering dls using a zetasizer nano zs malvern panalytical malvern uk software version 7 11 bacteriophage fr was chosen for zeta potential analysis ph 1 0 9 3 due to wide discrepancy in isoelectric point values reported in the literature as shown in table 2 bacteriophage aggregation was similarly evaluated by measuring particle size via dls over a range of ph values ph 6 8 for both zeta potential and particle size measurements the buffered demand free bdf solution used for bacteriophage propagation was replaced with baseline electrolyte table 1 by dialysis bacteriophage stocks were transferred to slide a lyzer 20 kda mwco dialysis cassettes thermo scientific waltham ma and stirred at 4 c for 3 days with daily replacement of electrolyte solution a2 test dust was diluted to 0 6 g l in ultrapure water the baseline electrolyte was adjusted to near target ph with 0 5 m naoh or hno3 samples were added to ph adjusted electrolyte in a 1 4 dilution for a final bacteriophage concentration of approximately 108 109 pfu ml final ph was read simultaneously with zeta potential and particle size readings 2 7 data analysis all statistical analyses were performed in the r statistical language using the stats package r core team 2014 mean log reduction by physical removal and inactivation was compared between test conditions using independent 2 tailed student s t tests α 0 05 with a bonferroni correction for multiple comparisons the effect of ph on bacteriophage inactivation was evaluated by linear regression the mammalian viruses did not show a uniform trend of inactivation so inactivation at ph 6 7 and 8 was compared by t tests models were evaluated for residual distribution normality and leverage points using the plot lm function and significance of variables was evaluated by analysis of variance with the anova function r core team 2014 a link for all r scripts is provided in si 2 3 results and discussion 3 1 effect of water constituents on virus mitigation the mitigation of bacteriophages and viruses via ec was evaluated over a wide range of water matrices in order to isolate the effects of ph nom turbidity and chloride on virus mitigation by ec artificial water matrices were prepared by varying these parameters individually the relative importance of physical removal and inactivation via ec here determined as recoverable vs irrecoverable virus mitigation respectively was compared between bacteriophages and viruses 3 1 1 effect of ph both mammalian viruses and bacteriophages were inactivated and physically removed to some degree over the ph range tested ph 6 8 as shown in fig 1 however whereas inactivation was the dominant fate for bacteriophages fr ms2 and p22 bacteriophage φx174 and mammalian viruses showed the greatest mitigation due to physical removal the physical removal of viruses in flocs is influenced by numerous factors including electrostatic repulsion and van der waals attraction modeled by the dlvo theory and non dlvo factors such as hydrophobicity steric hindrance virus aggregation and interactions with water matrix constituents heffron and mayer 2016 inactivation was most pronounced at low ph all four bacteriophages including φx174 demonstrated a significant exponential relationship between log inactivation and ph as summarized in si 3 similarly inactivation was greatest at low ph ph 6 for all mammalian viruses except fcv which was not effectively inactivated at any ph p 0 21 possible reasons for differences in virus resistance to inactivation are discussed in section 3 3 inactivation was significantly greater at ph 6 than ph 7 for adv p 0 0027 and ecv p 0 00025 though only approximately 0 7 log inactivation was achieved at ph 6 for either virus these results support previous findings heffron et al 2019a kim et al 2011 that ms2 and p22 inactivation in ferrous iron based treatment processes is greater at lower ph however this phenomenon has only been demonstrated previously with bacteriophages these results show that bacteriophages commonly used in water treatment testing were inactivated to a far greater degree than the mammalian viruses in this study bacteriophage φx174 was far more resistant to inactivation than the other bacteriophages with only 0 6 log inactivation at ph 6 total φx174 mitigation was greatest at ph 7 since the isoelectric point pi of φx174 is near neutral michen and graule 2010 φx174 would be more likely to destabilize and aggregate due to van der waals interactions at ph 7 which likely contributed to greater physical removal at ph 7 in addition aggregation can reduce the efficacy of disinfection gerba and betancourt 2017 the impact of ph on physical removal was difficult to interpret for bacteriophages fr ms2 and p22 because differences in physical removal may have been an artifact of the decrease in total mitigation at higher ph as with inactivation physical removal of the mammalian viruses was more similar to that of φx174 than the other bacteriophage surrogates total mitigation varied slightly with ph for the mammalian viruses though no unifying trend was apparent ecv showed a weak trend of greater physical removal at low ph the theoretical pi of ecv is approximately 6 2 mayer et al 2015 which could explain greater physical removal at ph 6 only fcv showed a significant difference in physical removal between ph levels with poorer removal at ph 8 than ph 7 p 0 000250 conversely adv showed a weak trend of greater physical removal with increasing ph however the low mitigation of the mammalian viruses relative to the variance makes it difficult to make meaningful inferences between means for the purpose of identifying a representative virus surrogate the very fact that mammalian virus removal was consistently low 2 5 log is more important only bacteriophage φx174 mitigation remained below the bar of 2 5 log total mitigation over the ph range tested 3 1 2 effect of natural organic matter the presence of nom was generally inhibitory to both inactivation and physical removal as shown in fig 2 suwannee river nom consists primarily of fulvic acid 65 by weight with a lesser fraction of humic acid 10 averett et al 1994 the pka of fulvic acids found in suwannee river nom is in the range of 2 4 indicating a negative charge at neutral ph leenheer et al 1995 therefore nom may inhibit physical removal and disinfection by sorbing the iron required for virus destabilization and disinfection once complexed with nom ferrous iron is resistant to oxidation by dissolved oxygen or free chlorine crittenden et al 2012 tanneru and chellam 2012 similarly found poor mitigation of ms2 using iron ec in natural river water and synthetic waters containing humic acid bacteriophage p22 was not inhibited by nom in this study though the reasons are unclear we are not aware of any direct comparison of p22 hydrophobicity to that of other bacteriophages that might help explain the lesser impact of nom as a large virus more of the p22 capsid may be exposed to oxidants when sorbed to nom bacteriophage φx174 mitigation was nearly completely inhibited 0 25 log reduction indicating that φx174 continued to be an appropriate surrogate for the mammalian viruses in high nom water matrices 3 1 3 effect of turbidity turbidity also inhibited inactivation though the impact of turbidity on physical removal was mixed as shown in fig 2 bacteriophages fr ms2 and p22 all demonstrated poorer inactivation in turbid water while φx174 showed minimal inactivation even without added turbidity a2 test dust consists primarily of silica 69 77 and alumina 8 14 as well as various metal oxides powder technology inc 2016 the presence of reduced metal species in sand can present a significant oxidant demand huling and pivetz 2006 accordingly metal oxides in a2 test dust may scavenge oxidants and therefore inhibit viral inactivation turbidity also inhibited physical removal of fr ms2 and φx174 a2 dust was demonstrated by dls to have a strong negative zeta potential around neutral ph as shown in si 4 therefore the test dust likely had a coagulant demand that inhibited virus removal at low coagulant doses zhu et al 2005a found that silica increased ms2 reduction by ferric chloride coagulation microfiltration however silica created a coagulant demand that impaired treatment at low coagulant doses 5 mg l similar to those used in this experiment diverse constituents contribute to turbidity in the environment accordingly the effect of turbidity may vary in natural water sources both bacteriophage p22 and adv had greater removal by physical removal with increased turbidity as the largest viruses tested 50 100 nm diameter p22 and adv were likely retained due to internal fouling or formation of a cake layer during filtration of the turbid samples using the same filtration technique as in ec experiments filters fouled with ec flocs and turbidity significantly rejected p22 1 27 log reduction p 2 01 10 5 to a greater degree than ms2 0 66 log reduction p 0 00014 as detailed in si 5 the greater degree of rejection for large viruses may override the coagulant demand of the a2 dust smaller bacteriophages like ms2 which saw a small increase in rejection by the fouled filter may have been adversely affected to a greater degree by the decrease in available coagulant in zhu s study 2005a development of a cake layer did not enhance dead end microfiltration of the smaller ms2 bacteriophage following ferric chloride coagulation 3 1 4 effect of chloride chloride was expected to increase inactivation through the production of free chlorine at the anode tanneru et al 2014 however inactivation significantly increased only for p22 and adv while inactivation decreased slightly for bacteriophage fr fig 2 no other viruses showed significant changes in mitigation with the addition of chloride in the absence of viruses the chlorine residual in the bulk solution during ec remained below the detection limit of 0 02 mg l cl2 most of the chlorine generated by chloride oxidation would likely be scavenged by ferrous iron which is also produced at the anode surface tanneru et al 2014 similarly found poor inactivation of bacteriophage ms2 due to free chlorine generation with aluminum ec aluminum ec would be expected to show greater efficiency in producing free chlorine than iron ec because aluminum ions are oxidized to a stable form at the electrode and would not exert oxidant demand in solution cañizares et al 2007 further research comparing relative virus susceptibility to free chlorine and ferrous iron is needed to understand why inactivation increased for the two large viruses bacteriophage p22 and adv but remained the same or slightly decreased for the remaining bacteriophages and viruses the rate of iron generation by ec increased dramatically in the presence of chloride as shown in si 6 carbon steel is susceptible to increased corrosion rates and pitting in the presence of chloride song et al 2017 therefore the greater iron generation was likely due to chemical corrosion the greater iron dose 6 6 mg l fe may have impacted physical removal increasing mitigation of p22 and adv by physical removal but decreasing φx174 mitigation again the largest viruses p22 and adv showed increased physical removal possibly indicating retention of viruses due to membrane fouling during filtration in the case of φx174 lower removal at higher doses may seem paradoxical however total removal of φx174 was not significantly different from total removal without chloride so the decrease in physical removal represents only a shift in mechanism of mitigation 3 2 mechanisms of virus mitigation to determine why some bacteriophages demonstrated inactivation due to ec the mechanisms of bacteriophage mitigation were investigated understanding the reason why some bacteriophages are inactivated by ferrous iron may help in selection of better virus surrogates or to identify more susceptible pathogen targets as shown in fig 3 ferric chloride coagulation and ferrous chloride coagulation reasonably predicted whether inactivation or physical removal was the predominate bacteriophage fate in ec whereas adsorption to preformed flocs and electrooxidation were not important mechanisms previous research heffron et al 2019a kim et al 2011 tanneru and chellam 2012 has found a correlation between oxidation of ferrous iron feii and bacteriophage inactivation therefore chemical coagulation with fecl2 was expected to achieve inactivation whereas the already oxidized ferric coagulant fecl3 should achieve only physical removal compared to chemical coagulation with fecl3 ec resulted in significant inactivation for all bacteriophages p values fr 3 69 10 6 ms2 1 33 10 6 p22 5 63 10 6 and φx174 1 01 10 3 though φx174 mitigation was predominately due to physical removal like ec chemical coagulation with fecl2 showed substantial inactivation of fr ms2 and p22 but only slight inactivation of φx174 more importantly chemical coagulation with fecl2 resulted in an even greater discrepancy in inactivation between φx174 and the other bacteriophages than was observed with ec inactivation of fr and p22 was greater with fecl2 than ec though ms2 inactivation was slightly greater with ec than fecl2 greater inactivation with fecl2 might have occurred because the entire concentration of ferrous iron was added at once and thoroughly mixed to provide a higher and more homogenous ferrous concentration throughout the reactor despite differences in the final log inactivation between fecl2 and ec the effect of ferrous iron is sufficient to explain inactivation observed in ec conversely chemical coagulation with fecl3 achieved only physical removal for fr ms2 and p22 ec achieved a similar degree of physical removal as fecl3 coagulation and ec outperformed chemical coagulation for φx174 3 2 1 pre formed flocs no bacteriophages demonstrated mitigation neither inactivation nor physical removal when added to reactors containing flocs pre formed by ec therefore sorption to flocs was not a significant mechanism of virus mitigation in simple electrolyte solution instead physical removal in ec is due to inclusion of viruses within the developing floc other researchers kreiβel et al 2014 shirasaki et al 2016 2009 have similarly found greater virus mitigation during rapid mixing and floc formation the importance of inclusion of viruses in the floc may also explain why ec was more effective than fecl3 chemical coagulation for mitigating φx174 in ec coagulant is gradually added to solution which typically slows floc formation in comparison to chemical coagulation harif et al 2012 thus ec allows longer contact time for virus inclusion within the floc 3 2 2 titanium electrodes uncoated titanium electrodes were used to evaluate the potential for bacteriophage mitigation due to generation of non ferrous oxidants e g reactive oxygen species and or oxidation at the anode surface air oxidized titanium anodes are stable in aqueous solutions extracting electrons from species in solution rather than dissolving like iron bagotsky 2006 wilhelmsen 1987 titanium electrooxidation mitigated both ms2 and fr though less than one log total reduction was achieved no significant mitigation was found for p22 or φx174 titanium electrodes are likely to overestimate the effects of inactivation because a ferrous iron may scavenge oxidants and b oxidation of the iron electrode competes with other oxidation reactions at the electrode surface nevertheless inactivation with titanium electrodes was far less than with iron electrodes therefore neither anodic oxidation nor generation of non ferrous oxidants can be considered important mechanisms of virus mitigation under the conditions investigated in this study this finding further confirms that ferrous oxidation is the primary determinant of inactivation due to ec 3 2 3 inactivation or irreversible coagulation in this study the irrecoverable loss of infectious bacteriophages was attributed to inactivation the ph 9 5 beef broth elution used to recover bacteriophages and mammalian viruses is an established method for both ec and chemical coagulation heffron et al 2019a matsui et al 2003 tanneru et al 2014 bacteriophage recovery was established using fecl3 chemical coagulation fig 3 while virus recovery was demonstrated at neutral ph using ec fig 1 however the difference in virus concentration between the control and the treated eluate here defined as inactivation could possibly reflect differences in elution efficacy between viruses and coagulant types and therefore fail to accurately assess inactivation across all tests flocs generated by chemical coagulation tend to be more structurally robust than those formed by ec due to differing kinetic limitations harif et al 2012 therefore elution methods successful for chemical coagulation i e with fecl3 should suffice for ec based on floc structure alone perhaps more importantly flocs formed by ec may be composed to varying degrees of ferrous as well as ferric iron dubrawski et al 2015 since fecl2 chemical coagulation also resulted in irrecoverable bacteriophage mitigation fecl2 could not be used to validate virus elution from flocs however heffron et al 2019a used the same elution method to demonstrate that irrecoverable bacteriophage mitigation was proportional to ferrous iron oxidation and that bacteriophages exposed to ferrous iron could not be recovered even after complete oxidation to ferric iron since the irrecoverable loss of bacteriophages increased as the iron oxidized to the ferric form for which the elution method was proven effective the irrecoverable fraction must be due to oxidative inactivation in addition the excellent recovery of human viruses in this study indicates that this elution method is appropriate for ec as well as conventional coagulation for these reasons the irrecoverable fraction was attributed to apparent inactivation while losses may include inactivation as well as irreversible coagulation this does not impact the primary finding of this study regarding inactivation that fr ms2 and p22 bacteriophages are too susceptible to ec and ferrous iron to serve as reliable indicators of virus mitigation 3 3 virion properties and ferrous susceptibility 3 3 1 isoelectric point of the mechanisms of bacteriophage mitigation discussed in section 3 2 susceptibility to ferrous inactivation was the primary cause of differences in log reduction between bacteriophages fr ms2 and p22 on the one hand and bacteriophage φx174 and the mammalian viruses on the other ferrous cations differ from neutrally or negatively charged disinfectants such as free chlorine though the positive ferrous charge may enhance disinfection of negatively charged pathogens pathogens with a positive charge near neutral ph may be repelled in addition aggregation can shield viruses and reduce the efficacy of disinfection gerba and betancourt 2017 since iron based inactivation is more effective at lower ph kim et al 2011 viruses with pis near ph 6 7 would therefore tend to aggregate due to charge neutralization and become shielded under the conditions of greatest disinfection capacity in this study bacteriophage φx174 showed marked aggregation near ph 7 based on measurement of particle size by dls as shown in si 7 other bacteriophages did not demonstrate similar aggregation at circumneutral ph thus electrostatic repulsion and aggregation may explain the poor inactivation of φx174 pi 6 0 7 0 compared to bacteriophages fr ms2 and p22 which have low pis 4 see table 2 because pi values reported in the literature varied widely for bacteriophage fr pi 3 5 to 9 0 the pi for fr was experimentally validated in this study at approximately 2 7 as shown in si 4 enteric viruses often enter the water cycle as aggregates gerba and betancourt 2017 and much of the viral load for drinking water treatment is associated with particles springthorpe and sattar 2007 therefore the tendency of viruses to aggregate is similarly an important factor for ec treatment of natural waters bacteriophage φx174 may also have been mitigated to a lesser extent than other bacteriophage surrogates due to structural robustness whereas f specific bacteriophages like fr and ms2 as well as tailed bacteriophages like p22 have a single locus of attachment and penetration φx174 can attach to and penetrate host cells at any of 12 spikes occurring at the capsid s 5 fold vertices sun et al 2013 however φx174 has not been shown to have similarly high resistance to other disinfectants and the single maturation protein of f specific bacteriophages does not appear to be an achilles heel for chemical disinfection heffron and mayer 2016 therefore there is little evidence to support the theory that φx174 is inherently more robust than other bacteriophages while experimental values are not available for adv and ecv isoelectric points both viruses are resistant to inactivation and have theoretical isoelectric points close to neutral 5 2 and 6 2 respectively see table 2 however fcv is one possible exception to the hypothesis that electrostatic forces determine ferrous disinfection fcv has a theoretical pi of 4 6 and virus like particles consisting of fcv capsid proteins have a similar reported pi of 3 9 samandoulgou et al 2015 therefore the fcv capsid likely has a negative charge at neutral ph and should attract ferrous ions yet fcv remains resistant to ferrous based inactivation therefore isoelectric point may be insufficient to fully explain ferrous susceptibility 3 3 2 capsid structure a review of capsid structure provides some insight into the resistance of mammalian viruses protein structures for bacteriophages and viruses were accessed from the viperdb database tsri 2018 as summarized in table 3 structural files for adenovirus 4 were not available so adenovirus serotypes 5 and 26 were used instead both adv5 and adv26 shared similar dimensions despite representing different species crenulations and protuberances on the capsid surface can result in outer diameter values not representative of actual capsid thickness and the method of structural analysis influences the degree of detail captured on the capsid surface pettigrew et al 2006 to minimize the influence of surface features adjusted capsid thickness was obtained by subtracting the inside diameter from the average diameter rather than from the outside diameter capsid thickness increased from bacteriophages to the mammalian viruses ms2 fr p22 φx174 fcv ecv adv the three bacteriophages with the thinnest capsids fr ms2 and p22 were also the most susceptible to inactivation due to ec though φx174 has only a slightly thicker capsid than p22 13 electrostatic repulsion and aggregation can still explain the recalcitrance of φx174 to iron based disinfection on the other hand the recalcitrance of fcv to iron based disinfection may be due more to capsid structure given its theoretically low pi but thicker 9 nm capsid the susceptibility of viruses to inactivation due to iron ec may therefore be a combination of electrostatic interactions and capsid structure capsid thickness would likely not play as large a role for uncharged disinfectants like hypochlorous acid that could permeate capsid pores more readily though thickness may be a rough indicator of capsid durability a more detailed evaluation of capsid structure and function could provide greater insight into why mammalian viruses are more resistant to inactivation 4 conclusions this is the first work to evaluate human virus mitigation and quantitatively assess the fate of viruses in iron ec this research evaluated the effect of several water parameters on virus fate via ec however the complexity of natural water matrices merits further testing of virus mitigation in natural waters both apparent inactivation and physical removal were important mechanisms of mitigation via ec for three of the four bacteriophages evaluated fr ms2 and p22 however φx174 and the three mammalian viruses adv ecv and fcv showed the greatest mitigation due to physical removal and were less susceptible to ferrous inactivation in representing virus mitigation φx174 was the only bacteriophage surrogate resistant to ferrous inactivation possibly due to electrostatic repulsion between φx174 and ferrous iron at ph 6 and or shielding of φx174 virions in aggregates near neutral ph though electrostatic interactions between ferrous ions and virions likely explains at least some of the differences in inactivation efficacy between viruses resistant viruses also had thicker capsids the lack of experimental isoelectric point data for human viruses prevents a full analysis of this hypothesis however a detailed theoretical evaluation of capsid structure may provide additional insight where empirical methods are prohibitive declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments funding for this project was provided by the national science foundation grant number 1433003 the ghr foundation also provided partial support for the project including purchase of the zetasizer and partial undergraduate research support for mr mcdermid additional support for mr mcdermid was provided by the lafferty family foundation mr heffron s stipend was partially funded by a marquette university richard w jobling research assistantship and a fellowship from the arthur j schmitt foundation the authors do not claim any competing interests appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j watres 2019 114877 
18603,emerging water treatment technologies using ferrous and zero valent iron show promising virus mitigation by both inactivation and adsorption in this study iron electrocoagulation was investigated for virus mitigation in drinking water via bench scale batch experiments relative contributions of physical removal and inactivation as determined by recovery via ph 9 5 beef broth elution were investigated for three mammalian viruses adenovirus echovirus and feline calicivirus and four bacteriophage surrogates fr ms2 p22 and φx174 though no one bacteriophage exactly represented mitigation of the mammalian viruses in all water matrices bacteriophage φx174 was the only surrogate that showed overall removal comparable to that of the mammalian viruses bacteriophages fr ms2 and p22 were all more susceptible to inactivation than the three mammalian viruses raising concerns about the suitability of these common surrogates as indicators of virus mitigation to determine why some bacteriophages were particularly susceptible to inactivation mechanisms of bacteriophage mitigation due to electrocoagulation were investigated physical removal was primarily due to inclusion in flocs while inactivation was primarily due to ferrous iron oxidation greater electrostatic attraction virus aggregation and capsid durability were proposed as reasons for virus susceptibility to ferrous based inactivation results suggest that overall treatment claims based on bacteriophage mitigation for any iron based technology should be critically considered due to higher susceptibility of bacteriophages to inactivation via ferrous oxidation graphical abstract image 1 keywords adenovirus coagulation disinfection echovirus feline calicivirus ferrous iron 1 introduction from 1993 to 2012 viruses were responsible for at least 24 us drinking water outbreaks reported to the centers for disease control and prevention cdc or 9 of all reported drinking water outbreaks in the us centers for disease control and prevention 2015 viruses may be responsible for many more outbreaks that are unreported or of unknown etiology xagoraraki et al 2014 most waterborne viruses follow a fecal oral route of infection meaning sewage impaired waters are a primary cause of infection xagoraraki et al 2014 worldwide 1 8 billion people rely on sewage contaminated drinking water gall et al 2015 viruses are persistent in the environment and resistant to many water treatment disinfection processes centers for disease control and prevention 2012 in addition virus small size makes them difficult to remove by particle separation tanneru and chellam 2013 among the viruses identified on the us environmental protection agency s contaminant candidate list ccl4 are caliciviruses including norovirus adenoviruses and enteroviruses including echovirus us environmental protection agency 2016 norovirus is the leading cause of infectious diarrhea worldwide causing as many as half of all gastroenteritis outbreaks grabow 2007 hall 2012 norovirus is characterized by high contagiousness effective transmission and rapid evolution hall 2012 due to difficulty in culturing human norovirus surrogates such as feline calicivirus or murine norovirus are often used in laboratory tests bae and schwab 2008 cannon et al 2006 adenoviruses can cause gastroenteritis in humans as well as conjunctivitis and respiratory disease world health organization 1996 adenoviruses are persistent in the environment and resistant to adverse conditions as well as ultraviolet uv irradiation grabow 2007 echoviruses are common pathogens in human impacted water systems echoviruses cause a range of diseases in humans including gastroenteritis meningitis fever and respiratory disease world health organization 1996 with diameters typically less than 30 nm echoviruses are also among the smallest viruses grabow 2007 therefore norovirus adenovirus and echovirus provide a representative suite of viruses for evaluating treatment process efficacy due to relevance e g ccl4 resistance to inactivation and resistance to physical separation electrocoagulation ec is a promising technology for small scale water treatment systems due to its portability and potential for automation ec is the in situ production of coagulant by passing electrical current through a zero valent sacrificial electrode typically consisting of iron or aluminum portability and potential for automation make ec a good candidate for small scale water treatment in rural or emergency applications small scale treatment systems are an important market as more than half of the public water systems in the us serve fewer than 500 people edr group 2013 recently ec has been considered for mitigating viruses in drinking water heffron et al 2019b heffron and mayer 2016 tanneru and chellam 2013 2012 zhu et al 2005a ec has shown promising results in treating bacteriophage ms2 surpassing the surface water treatment rule of 4 log virus reduction and outperforming conventional chemical coagulation for ms2 mitigation in some water matrices tanneru and chellam 2013 zhu et al 2005b in iron ec iron is released in solution as ferrous ions fe2 lakshmanan and clifford 2009 li et al 2012 oxidation of ferrous iron during ec can inactivate e coli delaire et al 2015 and steel electrodes have demonstrated higher effectiveness than aluminum or graphite electrodes for mitigating e coli ndjomgoue yossa et al 2015 ferrous iron oxidation also inactivates bacteriophages jeong et al 2006 kim et al 2011 inactivation may be preferable to physical removal because active viruses removed via coagulation flocculation could create a disposal hazard for the settled solids in addition promoting inactivation as well as physical removal via ec may lead to greater overall virus mitigation however the relative contributions of ferrous iron inactivation and physical removal have not been determined for virus inactivation during iron ec bacteriophages are used as surrogates for human viruses in water treatment process research amarasiri et al 2017 grabow 2001 heffron and mayer 2016 compared to human viruses bacteriophage surrogates have simpler quantification and propagation protocols propagate rapidly and are safer to handle to the authors knowledge bacteriophage ms2 has been the only virus investigated for ec or ferrous iron inactivation kim et al 2011 tanneru et al 2014 tanneru and chellam 2013 2012 zhu et al 2005a ms2 is small approximately 25 nm diameter and negatively charged at neutral ph mayer et al 2015 therefore ms2 is a representative surrogate for physical treatment processes because its small charged capsid is difficult to destabilize by charge neutralization or remove by size exclusion however the suitability of any surrogate must be investigated for each novel application in the case of ec ms2 s negative charge and small size may make the bacteriophage more susceptible to transport to the anode surface and or electrostatic attraction to a ferrous disinfectant in comparison to human viruses the goal of this research was to determine the fate of viruses during ec as well as the suitability of bacteriophage surrogates to indicate enteric virus mitigation in drinking water due to ec fate of viruses was distinguished as physical removal or apparent inactivation by comparing physical removal of flocs by microfiltration and elution of the bulk solution for recovery of infectious viruses the effect of ph and other water parameters on virus mitigation was also investigated to assess the suitability of bacteriophage surrogates in a range of water matrices to determine the mechanisms of bacteriophage mitigation log reduction of bacteriophages due to ec was compared to chemical coagulation with ferrous and ferric chloride sorption on floc surfaces and electrooxidation with insoluble titanium electrodes 2 materials and methods 2 1 electrocoagulation ec tests were conducted in a 500 ml glass beaker with two plate electrodes 60 cm2 submerged area 1 cm inter electrode distance consisting of iron mild steel as described by maher et al 2019 constant current 100 ma was supplied by a sorensen xel 60 1 5 variable dc power supply ametek san diego ca over a retention time of 5 min this current and retention time were selected to achieve measurable log reduction of viruses in a range of water matrices current polarity was alternated at regular intervals 30 s to maintain even electrode wear and prevent passivation maher et al 2019 the reactor was stirred with a magnetic stir bar at a rate of 60 rpm ḡ of approximately 25 s 1 although the presence of the large plate electrodes precludes accurate calculation this stir rate was sufficient to maintain circulation between the electrodes and achieved greater bacteriophage mitigation than more rapid stirring 120 rpm heffron 2019 the electrodes and polarity alternating controller were kindly provided by a o smith corporation electrodes were polished with 400 si c sandpaper washed with ultrapure water and sterilized with uv light 30 min on each side in a biological safety cabinet before each test all tests were performed in triplicate and compared to a control reactor not receiving treatment all tests were performed in synthetic water matrices by adding constituents to purelab ultrapure water elga labwater uk sodium nitrate 3 3 mm was chosen as a monovalent background electrolyte because multivalent ions can form complexes with protein moieties and thus impact surface charge chen and soucie 1986 michen and graule 2010 nitrate was chosen over chloride to avoid inactivation due to free chlorine because chloride ions can be oxidized to form free chlorine during ec tanneru et al 2014 sodium bicarbonate was also added to achieve alkalinity typical of soft to moderately alkaline water 50 mg l as caco3 and prevent dramatic ph fluctuations not representative of natural water matrices mechenich and andrews 2004 total and ferrous iron generation due to ec was measured using hach ferrover total iron and ferrous iron reagent hach loveland co respectively after ec electrodes were rinsed with a small volume 5 ml of ultrapure water to remove adsorbed flocs generation of free chlorine was measured using hach dpd free chlorine reagent after the addition of reagent sample absorbance was measured using a genesys 20 spectrophotometer thermo fisher scientific waltham ma at 510 nm total and ferrous iron and 530 nm free chlorine 2 2 effect of water constituents on virus mitigation in independent tests ph chloride turbidity and natural organic matter nom were adjusted to assess their impact on virus mitigation the water constituents and concentrations for all test waters are provided in table 1 to determine the effect of water quality on virus mitigation the background electrolyte solution was altered to compare to ec performance in the nano3 nahco3 electrolyte the ph of the test water was adjusted using 0 5 n hno3 or naoh a symphony benchtop multi meter vwr batavia il was used to measure ph chloride 115 mg l cl was added by replacing the background electrolyte nano3 with nacl to assess the impact of nom total organic carbon was increased by adding 15 mg l c suwannee river nom ihss st paul mn the total organic carbon contributed by virus stocks to the synthetic waters was approximately 6 5 mg l c for turbidity tests a2 test dust powder technology inc arden hills mn was added to achieve approximately 50 ntu nom and turbidity conditions were chosen to represent challenging surface waters for drinking water treatment 2 2 1 chemical coagulation chemical coagulation using ferrous chloride fecl2 and ferric chloride fecl3 was compared to ec to help determine the susceptibility of bacteriophages to inactivation physical removal fecl2 versus physical adsorption alone fecl3 doses of 2 3 mg fe l were used to approximate doses achieved by ec batch tests test waters were prepared to maintain similar conductivity to ec tests while also providing more sodium bicarbonate alkalinity 150 mg l as caco3 to prevent ph fluctuation upon addition of coagulant salts as shown in table 1 2 2 2 pre formed flocs viruses were added to pre formed flocs created by ec to test for the importance of sorption to the surfaces of flocs ec reactors were operated as for regular ec tests except that viruses were only added to the solution after the reaction had completed viruses were retained for the same amount of time 5 min under slow mixing 60 rpm prior to sampling 2 2 3 titanium electrodes to determine the potential for non ferrous oxidant generation and oxidation at the electrode surface iron electrodes were replaced with non sacrificial grade 2 titanium plate electrodes performance titanium san diego ca of the same dimensions 60 cm2 submerged area 1 cm inter electrode distance titanium is oxidized in air to form a passive inert electrode surface bagotsky 2006 titanium electrooxidation reactors were operated with the same parameters as the iron ec reactor 100 ma 5 min 30 s polarity reversal interval as described in section 2 1 2 3 virus propagation four bacteriophages were used as model viruses ms2 fr p22 and φx174 the properties of these bacteriophages are summarized in table 2 in addition three mammalian viruses were tested in varying water matrices adenovirus 4 adv echovirus 12 ecv and feline calicivirus fcv a surrogate for human norovirus also summarized in table 2 bacteriophages were stored at 4 c while viruses were stored at 20 c cryopreservant was not used to prevent adding oxidant demand associated with the virus stock solutions bacteriophages were spiked at concentrations of approximately 107 pfu ml while mammalian viruses were spiked at approximately 104 tcid50 ml due to limitations in virus propagation bacteriophages were propagated using the double agar layer dal method in tryptic soy agar bd franklin lakes nj adams 1959 mammalian viruses were propagated in cell cultures see supplementary information si 1 in sterile 175 cm2 culture flasks until cell monolayers were reduced to approximately 10 20 confluence then subjected to three freeze thaw cycles 20 c 22 c all bacteriophages and mammalian viruses were purified by two cycles of polyethylene glycol peg precipitation followed by a vertrel xf dupont wilmington de extraction as described by mayer et al 2008 2 4 virus sampling and quantification virus samples were taken immediately after ec two samples were taken from each reactor including the control untreated reactor first a filtered sample was collected using sterile 20 ml syringes and 0 45 μm ptfe syringe filters some form of physical separation is required in any coagulation process microfiltration was chosen for this study to thoroughly separate flocs without a long flocculation step the filter was primed with 15 ml of sample before reserving 4 ml of filtrate the reactor was then homogenized by rapid stirring 600 rpm for 15 s and a 20 ml sample was taken for virus elution to determine the total concentration of viable viruses in the bulk solution to dissolve flocs and increase electrostatic repulsion between coagulant and viruses elution was performed by adding an equal volume of ph 9 5 6 beef extract himedia west chester pa to homogenize samples and vortexing for approximately 10 s samples containing bacteriophages were diluted in tenfold series and ten 10 μl drops of each dilution were plated using the spot titer plaque assay method as described by beck et al 2009 mammalian viruses were quantified using the reed muench tcid50 method reed and muench 1938 virus recovery was confirmed in numerous tests e g at ph 8 fig 1 and in waters containing turbidity and nom fig 2 confirmation of bacteriophage recovery by elution was demonstrated using chemical coagulation with ferric chloride fig 3 virus mitigation total reduction in the number of infectious viruses was distinguished as inactivation or physical removal based on recovery of infectious viruses from the filtered and eluted samples the log reduction in infectious viruses between the filtered control and filtered treated samples represented total mitigation eqn 1 the log reduction in infectious viruses between the eluted control and eluted treated samples represented inactivation i e viruses that could not be recovered from the bulk solution including flocs eqn 2 mitigation due to physical removal was therefore the difference between total mitigation and inactivation i e the fraction of total mitigation that was recoverable from the bulk solution by elution eqn 3 1 t o t a l m i t i g a t i o n f i l t r a t e c o n t r o l f i l t r a t e t r e a t e d 2 i n a c t i v a t i o n e l u a t e c o n t r o l e l u a t e t r e a t e d 3 p h y s i c a l r e m o v a l t o t a l m i t i g a t i o n i n a c t i v a t i o n notably we attributed all irrecoverable loss of infectious viruses to apparent inactivation in this study however recovery of viruses from solid phases such as flocs is challenging and losses could also reflect variation in elution efficiency as discussed further in section 3 2 3 the elution method used in this study was previously used to determine virus inactivation due to ec and chemical coagulation heffron et al 2019a matsui et al 2003 tanneru et al 2014 here it showed reliable recovery of bacteriophages after coagulation with ferric chloride fr 98 ms2 59 p22 55 φx174 99 fig 3 the inactivated fraction represents non recoverable viruses for fecl3 treatment recovery of mammalian viruses after ec also showed reliable recovery for example at ph 7 adv 100 ecv 58 fcv 100 fig 1 irrecoverable virus mitigation via ec was compared to these benchmark recoveries in order to verify inactivation and in many cases inactivation was multiple logs greater molecular methods e g qpcr can be effective in qualitatively determining inactivation by either demonstrating genome damage or in combination with cultural techniques showing the presence of viral genomes in the absence of infectious viruses previous work has demonstrated bacteriophage inactivation due to ferrous and zero valent iron using both immunosorbent assay and qpcr kim et al 2011 however a reliable molecular method for quantifying virus inactivation is not currently available likewise molecular methods cannot distinguish between aggregated and dispersed virions so techniques like qpcr cannot be used to validate virus elution since nucleic acids are also susceptible to both physical adsorption and oxidative fragmentation even time and resource intensive combined cultural and molecular methods still have a degree of uncertainty in quantitatively determining fate therefore cultural assays showing irrecoverable mitigation with a proven method of recovery remain the most direct means of quantifying inactivation in processes that also exhibit physical removal heffron and mayer 2016 2 5 mechanisms of virus mitigation to establish mechanisms of virus mitigation log reduction due to ec was compared to similar physical chemical processes chemical coagulation and electrochemical oxidation these tests were only performed with bacteriophages due to limited inactivation of mammalian viruses by ec 2 6 zeta potential and particle size measurement the zeta potential of bacteriophage fr and a2 test dust were confirmed by dynamic light scattering dls using a zetasizer nano zs malvern panalytical malvern uk software version 7 11 bacteriophage fr was chosen for zeta potential analysis ph 1 0 9 3 due to wide discrepancy in isoelectric point values reported in the literature as shown in table 2 bacteriophage aggregation was similarly evaluated by measuring particle size via dls over a range of ph values ph 6 8 for both zeta potential and particle size measurements the buffered demand free bdf solution used for bacteriophage propagation was replaced with baseline electrolyte table 1 by dialysis bacteriophage stocks were transferred to slide a lyzer 20 kda mwco dialysis cassettes thermo scientific waltham ma and stirred at 4 c for 3 days with daily replacement of electrolyte solution a2 test dust was diluted to 0 6 g l in ultrapure water the baseline electrolyte was adjusted to near target ph with 0 5 m naoh or hno3 samples were added to ph adjusted electrolyte in a 1 4 dilution for a final bacteriophage concentration of approximately 108 109 pfu ml final ph was read simultaneously with zeta potential and particle size readings 2 7 data analysis all statistical analyses were performed in the r statistical language using the stats package r core team 2014 mean log reduction by physical removal and inactivation was compared between test conditions using independent 2 tailed student s t tests α 0 05 with a bonferroni correction for multiple comparisons the effect of ph on bacteriophage inactivation was evaluated by linear regression the mammalian viruses did not show a uniform trend of inactivation so inactivation at ph 6 7 and 8 was compared by t tests models were evaluated for residual distribution normality and leverage points using the plot lm function and significance of variables was evaluated by analysis of variance with the anova function r core team 2014 a link for all r scripts is provided in si 2 3 results and discussion 3 1 effect of water constituents on virus mitigation the mitigation of bacteriophages and viruses via ec was evaluated over a wide range of water matrices in order to isolate the effects of ph nom turbidity and chloride on virus mitigation by ec artificial water matrices were prepared by varying these parameters individually the relative importance of physical removal and inactivation via ec here determined as recoverable vs irrecoverable virus mitigation respectively was compared between bacteriophages and viruses 3 1 1 effect of ph both mammalian viruses and bacteriophages were inactivated and physically removed to some degree over the ph range tested ph 6 8 as shown in fig 1 however whereas inactivation was the dominant fate for bacteriophages fr ms2 and p22 bacteriophage φx174 and mammalian viruses showed the greatest mitigation due to physical removal the physical removal of viruses in flocs is influenced by numerous factors including electrostatic repulsion and van der waals attraction modeled by the dlvo theory and non dlvo factors such as hydrophobicity steric hindrance virus aggregation and interactions with water matrix constituents heffron and mayer 2016 inactivation was most pronounced at low ph all four bacteriophages including φx174 demonstrated a significant exponential relationship between log inactivation and ph as summarized in si 3 similarly inactivation was greatest at low ph ph 6 for all mammalian viruses except fcv which was not effectively inactivated at any ph p 0 21 possible reasons for differences in virus resistance to inactivation are discussed in section 3 3 inactivation was significantly greater at ph 6 than ph 7 for adv p 0 0027 and ecv p 0 00025 though only approximately 0 7 log inactivation was achieved at ph 6 for either virus these results support previous findings heffron et al 2019a kim et al 2011 that ms2 and p22 inactivation in ferrous iron based treatment processes is greater at lower ph however this phenomenon has only been demonstrated previously with bacteriophages these results show that bacteriophages commonly used in water treatment testing were inactivated to a far greater degree than the mammalian viruses in this study bacteriophage φx174 was far more resistant to inactivation than the other bacteriophages with only 0 6 log inactivation at ph 6 total φx174 mitigation was greatest at ph 7 since the isoelectric point pi of φx174 is near neutral michen and graule 2010 φx174 would be more likely to destabilize and aggregate due to van der waals interactions at ph 7 which likely contributed to greater physical removal at ph 7 in addition aggregation can reduce the efficacy of disinfection gerba and betancourt 2017 the impact of ph on physical removal was difficult to interpret for bacteriophages fr ms2 and p22 because differences in physical removal may have been an artifact of the decrease in total mitigation at higher ph as with inactivation physical removal of the mammalian viruses was more similar to that of φx174 than the other bacteriophage surrogates total mitigation varied slightly with ph for the mammalian viruses though no unifying trend was apparent ecv showed a weak trend of greater physical removal at low ph the theoretical pi of ecv is approximately 6 2 mayer et al 2015 which could explain greater physical removal at ph 6 only fcv showed a significant difference in physical removal between ph levels with poorer removal at ph 8 than ph 7 p 0 000250 conversely adv showed a weak trend of greater physical removal with increasing ph however the low mitigation of the mammalian viruses relative to the variance makes it difficult to make meaningful inferences between means for the purpose of identifying a representative virus surrogate the very fact that mammalian virus removal was consistently low 2 5 log is more important only bacteriophage φx174 mitigation remained below the bar of 2 5 log total mitigation over the ph range tested 3 1 2 effect of natural organic matter the presence of nom was generally inhibitory to both inactivation and physical removal as shown in fig 2 suwannee river nom consists primarily of fulvic acid 65 by weight with a lesser fraction of humic acid 10 averett et al 1994 the pka of fulvic acids found in suwannee river nom is in the range of 2 4 indicating a negative charge at neutral ph leenheer et al 1995 therefore nom may inhibit physical removal and disinfection by sorbing the iron required for virus destabilization and disinfection once complexed with nom ferrous iron is resistant to oxidation by dissolved oxygen or free chlorine crittenden et al 2012 tanneru and chellam 2012 similarly found poor mitigation of ms2 using iron ec in natural river water and synthetic waters containing humic acid bacteriophage p22 was not inhibited by nom in this study though the reasons are unclear we are not aware of any direct comparison of p22 hydrophobicity to that of other bacteriophages that might help explain the lesser impact of nom as a large virus more of the p22 capsid may be exposed to oxidants when sorbed to nom bacteriophage φx174 mitigation was nearly completely inhibited 0 25 log reduction indicating that φx174 continued to be an appropriate surrogate for the mammalian viruses in high nom water matrices 3 1 3 effect of turbidity turbidity also inhibited inactivation though the impact of turbidity on physical removal was mixed as shown in fig 2 bacteriophages fr ms2 and p22 all demonstrated poorer inactivation in turbid water while φx174 showed minimal inactivation even without added turbidity a2 test dust consists primarily of silica 69 77 and alumina 8 14 as well as various metal oxides powder technology inc 2016 the presence of reduced metal species in sand can present a significant oxidant demand huling and pivetz 2006 accordingly metal oxides in a2 test dust may scavenge oxidants and therefore inhibit viral inactivation turbidity also inhibited physical removal of fr ms2 and φx174 a2 dust was demonstrated by dls to have a strong negative zeta potential around neutral ph as shown in si 4 therefore the test dust likely had a coagulant demand that inhibited virus removal at low coagulant doses zhu et al 2005a found that silica increased ms2 reduction by ferric chloride coagulation microfiltration however silica created a coagulant demand that impaired treatment at low coagulant doses 5 mg l similar to those used in this experiment diverse constituents contribute to turbidity in the environment accordingly the effect of turbidity may vary in natural water sources both bacteriophage p22 and adv had greater removal by physical removal with increased turbidity as the largest viruses tested 50 100 nm diameter p22 and adv were likely retained due to internal fouling or formation of a cake layer during filtration of the turbid samples using the same filtration technique as in ec experiments filters fouled with ec flocs and turbidity significantly rejected p22 1 27 log reduction p 2 01 10 5 to a greater degree than ms2 0 66 log reduction p 0 00014 as detailed in si 5 the greater degree of rejection for large viruses may override the coagulant demand of the a2 dust smaller bacteriophages like ms2 which saw a small increase in rejection by the fouled filter may have been adversely affected to a greater degree by the decrease in available coagulant in zhu s study 2005a development of a cake layer did not enhance dead end microfiltration of the smaller ms2 bacteriophage following ferric chloride coagulation 3 1 4 effect of chloride chloride was expected to increase inactivation through the production of free chlorine at the anode tanneru et al 2014 however inactivation significantly increased only for p22 and adv while inactivation decreased slightly for bacteriophage fr fig 2 no other viruses showed significant changes in mitigation with the addition of chloride in the absence of viruses the chlorine residual in the bulk solution during ec remained below the detection limit of 0 02 mg l cl2 most of the chlorine generated by chloride oxidation would likely be scavenged by ferrous iron which is also produced at the anode surface tanneru et al 2014 similarly found poor inactivation of bacteriophage ms2 due to free chlorine generation with aluminum ec aluminum ec would be expected to show greater efficiency in producing free chlorine than iron ec because aluminum ions are oxidized to a stable form at the electrode and would not exert oxidant demand in solution cañizares et al 2007 further research comparing relative virus susceptibility to free chlorine and ferrous iron is needed to understand why inactivation increased for the two large viruses bacteriophage p22 and adv but remained the same or slightly decreased for the remaining bacteriophages and viruses the rate of iron generation by ec increased dramatically in the presence of chloride as shown in si 6 carbon steel is susceptible to increased corrosion rates and pitting in the presence of chloride song et al 2017 therefore the greater iron generation was likely due to chemical corrosion the greater iron dose 6 6 mg l fe may have impacted physical removal increasing mitigation of p22 and adv by physical removal but decreasing φx174 mitigation again the largest viruses p22 and adv showed increased physical removal possibly indicating retention of viruses due to membrane fouling during filtration in the case of φx174 lower removal at higher doses may seem paradoxical however total removal of φx174 was not significantly different from total removal without chloride so the decrease in physical removal represents only a shift in mechanism of mitigation 3 2 mechanisms of virus mitigation to determine why some bacteriophages demonstrated inactivation due to ec the mechanisms of bacteriophage mitigation were investigated understanding the reason why some bacteriophages are inactivated by ferrous iron may help in selection of better virus surrogates or to identify more susceptible pathogen targets as shown in fig 3 ferric chloride coagulation and ferrous chloride coagulation reasonably predicted whether inactivation or physical removal was the predominate bacteriophage fate in ec whereas adsorption to preformed flocs and electrooxidation were not important mechanisms previous research heffron et al 2019a kim et al 2011 tanneru and chellam 2012 has found a correlation between oxidation of ferrous iron feii and bacteriophage inactivation therefore chemical coagulation with fecl2 was expected to achieve inactivation whereas the already oxidized ferric coagulant fecl3 should achieve only physical removal compared to chemical coagulation with fecl3 ec resulted in significant inactivation for all bacteriophages p values fr 3 69 10 6 ms2 1 33 10 6 p22 5 63 10 6 and φx174 1 01 10 3 though φx174 mitigation was predominately due to physical removal like ec chemical coagulation with fecl2 showed substantial inactivation of fr ms2 and p22 but only slight inactivation of φx174 more importantly chemical coagulation with fecl2 resulted in an even greater discrepancy in inactivation between φx174 and the other bacteriophages than was observed with ec inactivation of fr and p22 was greater with fecl2 than ec though ms2 inactivation was slightly greater with ec than fecl2 greater inactivation with fecl2 might have occurred because the entire concentration of ferrous iron was added at once and thoroughly mixed to provide a higher and more homogenous ferrous concentration throughout the reactor despite differences in the final log inactivation between fecl2 and ec the effect of ferrous iron is sufficient to explain inactivation observed in ec conversely chemical coagulation with fecl3 achieved only physical removal for fr ms2 and p22 ec achieved a similar degree of physical removal as fecl3 coagulation and ec outperformed chemical coagulation for φx174 3 2 1 pre formed flocs no bacteriophages demonstrated mitigation neither inactivation nor physical removal when added to reactors containing flocs pre formed by ec therefore sorption to flocs was not a significant mechanism of virus mitigation in simple electrolyte solution instead physical removal in ec is due to inclusion of viruses within the developing floc other researchers kreiβel et al 2014 shirasaki et al 2016 2009 have similarly found greater virus mitigation during rapid mixing and floc formation the importance of inclusion of viruses in the floc may also explain why ec was more effective than fecl3 chemical coagulation for mitigating φx174 in ec coagulant is gradually added to solution which typically slows floc formation in comparison to chemical coagulation harif et al 2012 thus ec allows longer contact time for virus inclusion within the floc 3 2 2 titanium electrodes uncoated titanium electrodes were used to evaluate the potential for bacteriophage mitigation due to generation of non ferrous oxidants e g reactive oxygen species and or oxidation at the anode surface air oxidized titanium anodes are stable in aqueous solutions extracting electrons from species in solution rather than dissolving like iron bagotsky 2006 wilhelmsen 1987 titanium electrooxidation mitigated both ms2 and fr though less than one log total reduction was achieved no significant mitigation was found for p22 or φx174 titanium electrodes are likely to overestimate the effects of inactivation because a ferrous iron may scavenge oxidants and b oxidation of the iron electrode competes with other oxidation reactions at the electrode surface nevertheless inactivation with titanium electrodes was far less than with iron electrodes therefore neither anodic oxidation nor generation of non ferrous oxidants can be considered important mechanisms of virus mitigation under the conditions investigated in this study this finding further confirms that ferrous oxidation is the primary determinant of inactivation due to ec 3 2 3 inactivation or irreversible coagulation in this study the irrecoverable loss of infectious bacteriophages was attributed to inactivation the ph 9 5 beef broth elution used to recover bacteriophages and mammalian viruses is an established method for both ec and chemical coagulation heffron et al 2019a matsui et al 2003 tanneru et al 2014 bacteriophage recovery was established using fecl3 chemical coagulation fig 3 while virus recovery was demonstrated at neutral ph using ec fig 1 however the difference in virus concentration between the control and the treated eluate here defined as inactivation could possibly reflect differences in elution efficacy between viruses and coagulant types and therefore fail to accurately assess inactivation across all tests flocs generated by chemical coagulation tend to be more structurally robust than those formed by ec due to differing kinetic limitations harif et al 2012 therefore elution methods successful for chemical coagulation i e with fecl3 should suffice for ec based on floc structure alone perhaps more importantly flocs formed by ec may be composed to varying degrees of ferrous as well as ferric iron dubrawski et al 2015 since fecl2 chemical coagulation also resulted in irrecoverable bacteriophage mitigation fecl2 could not be used to validate virus elution from flocs however heffron et al 2019a used the same elution method to demonstrate that irrecoverable bacteriophage mitigation was proportional to ferrous iron oxidation and that bacteriophages exposed to ferrous iron could not be recovered even after complete oxidation to ferric iron since the irrecoverable loss of bacteriophages increased as the iron oxidized to the ferric form for which the elution method was proven effective the irrecoverable fraction must be due to oxidative inactivation in addition the excellent recovery of human viruses in this study indicates that this elution method is appropriate for ec as well as conventional coagulation for these reasons the irrecoverable fraction was attributed to apparent inactivation while losses may include inactivation as well as irreversible coagulation this does not impact the primary finding of this study regarding inactivation that fr ms2 and p22 bacteriophages are too susceptible to ec and ferrous iron to serve as reliable indicators of virus mitigation 3 3 virion properties and ferrous susceptibility 3 3 1 isoelectric point of the mechanisms of bacteriophage mitigation discussed in section 3 2 susceptibility to ferrous inactivation was the primary cause of differences in log reduction between bacteriophages fr ms2 and p22 on the one hand and bacteriophage φx174 and the mammalian viruses on the other ferrous cations differ from neutrally or negatively charged disinfectants such as free chlorine though the positive ferrous charge may enhance disinfection of negatively charged pathogens pathogens with a positive charge near neutral ph may be repelled in addition aggregation can shield viruses and reduce the efficacy of disinfection gerba and betancourt 2017 since iron based inactivation is more effective at lower ph kim et al 2011 viruses with pis near ph 6 7 would therefore tend to aggregate due to charge neutralization and become shielded under the conditions of greatest disinfection capacity in this study bacteriophage φx174 showed marked aggregation near ph 7 based on measurement of particle size by dls as shown in si 7 other bacteriophages did not demonstrate similar aggregation at circumneutral ph thus electrostatic repulsion and aggregation may explain the poor inactivation of φx174 pi 6 0 7 0 compared to bacteriophages fr ms2 and p22 which have low pis 4 see table 2 because pi values reported in the literature varied widely for bacteriophage fr pi 3 5 to 9 0 the pi for fr was experimentally validated in this study at approximately 2 7 as shown in si 4 enteric viruses often enter the water cycle as aggregates gerba and betancourt 2017 and much of the viral load for drinking water treatment is associated with particles springthorpe and sattar 2007 therefore the tendency of viruses to aggregate is similarly an important factor for ec treatment of natural waters bacteriophage φx174 may also have been mitigated to a lesser extent than other bacteriophage surrogates due to structural robustness whereas f specific bacteriophages like fr and ms2 as well as tailed bacteriophages like p22 have a single locus of attachment and penetration φx174 can attach to and penetrate host cells at any of 12 spikes occurring at the capsid s 5 fold vertices sun et al 2013 however φx174 has not been shown to have similarly high resistance to other disinfectants and the single maturation protein of f specific bacteriophages does not appear to be an achilles heel for chemical disinfection heffron and mayer 2016 therefore there is little evidence to support the theory that φx174 is inherently more robust than other bacteriophages while experimental values are not available for adv and ecv isoelectric points both viruses are resistant to inactivation and have theoretical isoelectric points close to neutral 5 2 and 6 2 respectively see table 2 however fcv is one possible exception to the hypothesis that electrostatic forces determine ferrous disinfection fcv has a theoretical pi of 4 6 and virus like particles consisting of fcv capsid proteins have a similar reported pi of 3 9 samandoulgou et al 2015 therefore the fcv capsid likely has a negative charge at neutral ph and should attract ferrous ions yet fcv remains resistant to ferrous based inactivation therefore isoelectric point may be insufficient to fully explain ferrous susceptibility 3 3 2 capsid structure a review of capsid structure provides some insight into the resistance of mammalian viruses protein structures for bacteriophages and viruses were accessed from the viperdb database tsri 2018 as summarized in table 3 structural files for adenovirus 4 were not available so adenovirus serotypes 5 and 26 were used instead both adv5 and adv26 shared similar dimensions despite representing different species crenulations and protuberances on the capsid surface can result in outer diameter values not representative of actual capsid thickness and the method of structural analysis influences the degree of detail captured on the capsid surface pettigrew et al 2006 to minimize the influence of surface features adjusted capsid thickness was obtained by subtracting the inside diameter from the average diameter rather than from the outside diameter capsid thickness increased from bacteriophages to the mammalian viruses ms2 fr p22 φx174 fcv ecv adv the three bacteriophages with the thinnest capsids fr ms2 and p22 were also the most susceptible to inactivation due to ec though φx174 has only a slightly thicker capsid than p22 13 electrostatic repulsion and aggregation can still explain the recalcitrance of φx174 to iron based disinfection on the other hand the recalcitrance of fcv to iron based disinfection may be due more to capsid structure given its theoretically low pi but thicker 9 nm capsid the susceptibility of viruses to inactivation due to iron ec may therefore be a combination of electrostatic interactions and capsid structure capsid thickness would likely not play as large a role for uncharged disinfectants like hypochlorous acid that could permeate capsid pores more readily though thickness may be a rough indicator of capsid durability a more detailed evaluation of capsid structure and function could provide greater insight into why mammalian viruses are more resistant to inactivation 4 conclusions this is the first work to evaluate human virus mitigation and quantitatively assess the fate of viruses in iron ec this research evaluated the effect of several water parameters on virus fate via ec however the complexity of natural water matrices merits further testing of virus mitigation in natural waters both apparent inactivation and physical removal were important mechanisms of mitigation via ec for three of the four bacteriophages evaluated fr ms2 and p22 however φx174 and the three mammalian viruses adv ecv and fcv showed the greatest mitigation due to physical removal and were less susceptible to ferrous inactivation in representing virus mitigation φx174 was the only bacteriophage surrogate resistant to ferrous inactivation possibly due to electrostatic repulsion between φx174 and ferrous iron at ph 6 and or shielding of φx174 virions in aggregates near neutral ph though electrostatic interactions between ferrous ions and virions likely explains at least some of the differences in inactivation efficacy between viruses resistant viruses also had thicker capsids the lack of experimental isoelectric point data for human viruses prevents a full analysis of this hypothesis however a detailed theoretical evaluation of capsid structure may provide additional insight where empirical methods are prohibitive declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments funding for this project was provided by the national science foundation grant number 1433003 the ghr foundation also provided partial support for the project including purchase of the zetasizer and partial undergraduate research support for mr mcdermid additional support for mr mcdermid was provided by the lafferty family foundation mr heffron s stipend was partially funded by a marquette university richard w jobling research assistantship and a fellowship from the arthur j schmitt foundation the authors do not claim any competing interests appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j watres 2019 114877 
18604,in this article a new characterization model and factors are proposed for the life cycle impact assessment lcia of water consumption on instream freshwater ecosystems impact pathways of freshwater consumption leading to ecosystem damage are described and the alteration of instream physical habitat is identified as a critical midpoint for ecosystem quality the lcia characterization model aims to assess the change in habitat quantity due to consumptive water use it is based on statistical physical habitat simulation for benthic invertebrates fish species and their size classes and guilds of fish sharing common habitat preferences a habitat change potential hcp midpoint mechanistic indicator is developed and computed on the french river network at the river reach scale the river segment with variable length between the upstream and downstream nodes in the hydrographic network for median annual discharges and dry seasons aggregated multi species hcps at a river reach are proposed using various aggregation approaches subsequently the characterization factors are spatially aggregated at watershed and sub watershed scales hcp is highly correlated with median and low flow discharges which determine hydraulic characteristics of reaches aggregation of individual hcps at reach scale is driven by the species most sensitive to water consumption in spatially aggregated hcps consistently with their reduced smaller average discharge rate small stream habitats determine the overall watershed characterization the study is aimed primarily at life cycle assessment lca practitioners and lcia modelers however since it is the result of a productive cross fertilization between the ecohydrology and lca domains it could be potentially useful for watershed management and risk assessment as well at the moment the proposed model is applicable in france for a broader implementation the development of global high resolution river databases or the generalization of the model are needed our new factor represents nevertheless an advancement in freshwater ecosystems lcia laying the basis for new metrics for biodiversity assessment graphical abstract image 1 keywords life cycle assessment water consumption watershed ecology hydraulic habitat environmental flows abbreviations lca life cycle assessment lcia life cycle impact assessment cf characterization factor ff fate factor xf exposure factor ef effect factor sar species area relationship sdr species discharge relationship hcp habitat change potential q river discharge cwu consumptive water use hs habitat suitability wua weighted usable area re reynolds number w river width 1 introduction inland waters are a habitat for rich species diversity approximately 126 000 inland aquatic species have been described according to iucn 2009 representing the 9 5 of all currently identified species and circumscribed in a living environment which is equal to just 0 01 of the total terrestrial surface balian et al 2008 nevertheless 65 of continental waters are moderately or highly threatened by anthropogenic disturbance and climate change with the prospect of an intensification of anthropic pressure on ecosystems and an increase in freshwater needs driven by population growth a lot of efforts have been dedicated to the development of more sustainable water management strategies davis et al 2015 lapointe et al 2014 while these efforts have been capable of ensuring substantial improvement of water security for humans there is still a mismatch with what has been achieved in terms of biodiversity conservation partly because of the likelihood that the ways to meet water needs of humans and ecosystems can be substantially antagonistic vörösmarty et al 2010 in this context several life cycle impact assessment lcia models have been proposed to link freshwater consumptive use to biodiversity loss núñez et al 2016 and addressing specifically wetlands and surface water dependent ecosystems amores et al 2013 hanafiah et al 2011 tendall et al 2014 verones et al 2013b these approaches provide endpoint indicators built on cause effect pathways in which the impact characterization factor cf eq 1 results from the combination of three sub factors núñez et al 2018 the fate factor ff represents the environmental change e g change in m3 y river discharge or m2 wetland area due to water consumption defined as the withdrawn water that is not returned to the original drainage basin international organization for standardization iso tc 207 sc 5 2014 the exposure factor xf indicates how far this alteration can be offset e g in a river it can be approximated to 1 since most freshwater species have less mobility than terrestrial species to compensate the lack of water finally the effect factor ef describes the consequence on the ecosystem e g potentially disappeared fraction of species pdf 1 cf ff xf ef in the literature impact scores related to volumetric change in water availability and as a consequence indirectly linked to water quantity needs of affected taxa have been calculated based on species area relationships in wetlands sar verones et al 2017 2013a 2013b or species discharge relationships sdr in rivers hanafiah et al 2011 tendall et al 2014 xenopoulos et al 2005 despite the relative ease of applying sar and sdr to lcia such empirical approaches to relate species richness to water quantity involve some underlying necessary assumptions making these models less suitable to be used in certain circumstances regarding for instance the completeness of covered taxa sar based methods do not consider instream species namely fish species and invertebrates on the other hand sdr aim at quantifying species occurrence related to river discharge and therefore estimating species mortality of fish and macroinvertebrates derived from flow reduction tendall et al 2014 addressed some limits of sdr for instance by better regionalizing species discharge curves however building a mechanistic lcia model on sdr implies considering equal responses to stress for highly differentiated taxa concealing the complexity of the relationships between living organisms and their habitat it could also lead to interpreting discharge calculated at the catchment outlet as the direct cause of species richness in the catchment which remains unproven in particular more species can be found in large catchments due to larger available space and not only greater discharge and this is interestingly what an approach based on sar would suggest iwasaki et al 2012 mcgarvey and terra 2016 moreover at the local scale species traits and specific flow preferences which can be essential for shaping community structures are not taken into account these considerations are important to evaluate ecosystem response occurring below the extinction threshold which is determined usually by extreme or prolonged events lytle and poff 2004 rather than marginal water flow change what most of lcia models appraise in addition when evaluating long term effects of flow reduction on species such as in modeling global climate change scenarios fish communities background extinction rates natural extinction rates in undisturbed conditions and extinction time horizons when species committed to extinction go actually extinct are not defined leading to a potential overestimation of species loss induced by flow reduction tedesco et al 2013 xenopoulos et al 2005 while lcia methods based on sar and sdr have the advantage of taking into account biological aspects compared to stress based indicators berger and finkbeiner 2013 boulay et al 2018 exploring complementary options to model aspects of biological communities other than species richness number of species such as species abundance number of individuals per species or diversity tuomisto 2010 would therefore support a more comprehensive analysis of ecosystem quality curran et al 2011 damiani et al 2018 this raises the question on whether it would be possible to improve current lcia models to appropriate recent available knowledge concerning freshwater ecology and especially environmental flow management angus webb et al 2013 damiani et al 2018 poff and zimmerman 2010 the aim of this study is first to trace in detail the potential water consumption impact pathways on freshwater habitat and ecosystems the purpose is to identify the main relevant environmental mechanisms and develop a local bottom up mechanistic model addressing the limitations of current lcia models since lotic habitats are currently the most vulnerable to freshwater consumptive use vörösmarty et al 2010 a habitat based characterization factor linking marginal hydrological alteration to instream ecosystems effects is proposed based on existing literature on the applicability of ecohydrological methods in lcia damiani et al 2018 the model is subsequently applied on french streams and the feasibility ecological relevance and limitations of upscaling from the river to the watershed scale is discussed 2 impact pathway analysis the simplified diagram in fig 1 outlines the impact pathways of consumptive freshwater use linked to water dependent ecosystems damage the picture is built on existing environmental flow management and ecohydrology literature angus webb et al 2013 gillespie et al 2015 poff and zimmerman 2010 ecological bibliographical sources in appendix a section 1 provide additional details on the environmental mechanisms represented that would otherwise be difficult to include in the chart freshwater habitats are multifaceted and it is essential for mechanistic lcia to identify relevant impact pathways to be modeled separately particularly when these are linked to specific environmental interventions e g water withdrawal river damming as for the abiotic components of habitat e g morphodynamic and physicochemical aspects of groundwater and surface water bodies the whole ecosystem may undergo cascading effects triggered by water balance changes in fact while an ecological system remains relatively stable over time a human induced alteration may promote a directional shift to a new dynamic equilibrium resulting from the ecological response of the first affected ecosystem compartments and of all those subsequently connected these mechanisms involve all inter and intra specific relationships between different biota and are only generically represented in fig 1 as an example species loss and reduced riparian vegetation cover limits shading on rivers which in turn influences food and shelter availability for reproduction and juvenile growth in fish and macroinvertebrate species li and dudgeon 2008 mokany et al 2008 riley et al 2009 moreover under environmental stress trophic webs play an important role in determining the stability and evolution of an ecological community downing and leibold 2010 thompson et al 2012 it is also necessary to bear in mind that habitat characteristics can be influenced by multiple drivers for instance nutrient availability in water can be determined by flow regime thermal regime and the presence or absence of forest buffers kennen et al 2008 kløve et al 2014 this draws attention to the fact that habitats not only drive the establishment of a specific type of ecosystem but can also be shaped themselves by the biota they harbor these ecosystem mechanisms involve all abiotic and biotic changes induced or limited by the presence of certain ecological communities berke 2010 jones et al 2010 in addition habitat modifications can be further amplified by the proliferation of non native species crooks 2002 ward and ricciardi 2010 for these reasons lcia modeling would require describing ecological mechanisms at different scales from species response to community composition and with short to long term time horizons with this respect hydrologic alteration of flow regimes should be characterized in all its components magnitude timing duration frequency and rate of change as well as flow dependent habitat characteristics the brisbane declaration 2007 in the present article we analyze for the first time in lcia modeling the midpoint effect of flow magnitude alteration i e river discharge on physical habitat i e hydraulics for fish species and stream invertebrates this represents a bottom up approach aimed at determining the relations between freshwater species and their habitat at an early stage of the mechanistic impact pathway the alteration of river flow conditions and hydraulics can result from surface water consumption groundwater consumption and from water infrastructures building and operation the study focuses mainly on the development of an effect factor for marginal discharge alteration in the following section a simplified fate factor is adopted water balance between groundwater and surface water is not modeled and non marginal change in flow regime is not considered e g from river damming for these reasons at present the proposed model would perform better for assessing the impact of direct surface water withdrawal and release 3 materials and methods 3 1 freshwater habitat modeling in lcia water consumption may result in the alteration of river discharge and other related physical variables such as reach hydraulics velocities depths bed forces turbulence depending on the reach morphology and on their different habitat preferences species can be favored or disfavored by these changes in ecohydrology habitat preferences are modeled by means of habitat suitability equations for physical habitat variables such as microhabitat hydraulics width depth velocity the substrate composition and the turbulence bovee 1982 payne and jowett 2013 among ecohydrological habitat models generalized statistical habitat models have been developed based on the findings that species habitat suitability in reaches strongly depends on reach scale hydraulic geometries i e variations in reach average width and water depth with discharge lamouroux and capra 2002 lamouroux and jowett 2005 lamouroux and souchon 2002 this approach to freshwater habitat modeling is well suited for mechanistic lcia indicators evaluating habitat alteration effects on instream assemblages damiani et al 2018 in particular because reach hydraulic geometries can be modeled over the whole hydrographic networks lamouroux 2008 miguel et al 2016 snelder et al 2011 on this basis a habitat based midpoint characterization factor is proposed in eq 2 to quantify the change in habitat availability for freshwater fish species and stream invertebrates according to river discharge alteration 2 cf i ff i ef i d q i d c w u i hcp i cfi is the characterization factor for the river reach i the fate factor ffi represents the marginal change in discharge dq i m3 s for marginal change in consumptive water use dcwu i m3 s in the present study the fate factor is considered equal to 1 as done by hanafiah et al 2011 and tendall et al 2014 lcia models meaning that 1 m3 s of water withdrawn or released in the environment causes 1 m3 s discharge alteration this could be modeled more precisely using a mass balance multimedia fate modeling approach as proposed by núñez et al 2018 the effect factor efi is calculated as the habitat change potential hcpi in m2 s m3 of habitat surface derived from marginal discharge alteration eq 3 in the proposed approach the value of the characterization factor corresponds therefore to the value of the effect factor and will be indicated indifferently as cf or hcp hereafter 3 h c p ij h s ij j 1 n h s ij d w u a ij d q i hcpij is calculated from seventeen multivariate microhabitat suitability hs equations j developed empirically based on the abundance of eight fish species at different ontogenetic stages four fish guilds of species with similar habitat preference and the production of invertebrates biomass table 1 the definitions of the four guilds are adopted from lamouroux et al 2002 the pool guild includes species or size classes preferring deep and slow flowing habitats with fine substrate sediment the bank guild individuals are adapted to shallow and slow flowing waters with fine sediment shallow microhabitats harbor also riffle species if velocities are intermediate to high and with intermediate particle size midstream guild species are instead adapted to fast flowing and deep waters with coarse substrate composition more details on guilds composition are given in appendix a table a 2 most habitat suitability equations considered here are included in the generalized statistical habitat simulation model estimhab lamouroux and capra 2002 lamouroux and souchon 2002 souchon et al 2003 which constitutes the habitat modeling module of the modeling platform estimkart lamouroux et al 2010 the first term of the hcp equation therefore weights the species guild or invertebrates habitat suitability at a given reach against the overall habitat suitability of all fish species guilds or invertebrates respectively in the same river in short the weighted hs represents the reference habitat condition for the chosen species guild or for benthic macroinvertebrates in the second term of equation 3 wua is the weighted usable area in m2 calculated as hs ij w i 100 where w i is the width of the river reach i in meters which is multiplied by 100 m length wua represents therefore the surface of suitable habitat in a reach of a given width bovee 1982 and it is quantified on a constant 100 m length to allow comparability between river segments of different length e g 100 m of 0 4 hs and 50 m of 0 8 hs would give the same wua as a result of two completely different ecological conditions the derivative of wua in the second term is the change in habitat area to discharge change calculated through two different models lamouroux and capra 2002 lamouroux and jowett 2005 lamouroux and souchon 2002 depending on the wua equation attributed in literature to the different taxa based on the best fit to observed abundance data table 1 eqs 4 and 5 the different models correspond to different types of species response to flow model 1 4 w u a i a i r e i c exp k r e i w i 100 model 2 5 w u a i a i 1 c exp k r e i w i 100 in both models re i is the reynolds number representing specific river discharge and turbulence in the river reach i re i is defined as q i v w i where v is the kinematic viscosity of water considered equal to 10 6 m2 s 1 lamouroux et al 1999 the dimensionless parameter a i is a distinctive static descriptor of the reach it is based on its average characteristics at a median discharge level i e it is independent of discharge and its alteration appendix a section 3 conversely the constants c and k shared by all river segments determine the rate of change of wua i with re i within reaches as in estimhab viscosity is multiplied by 107 to run the calculation with low re i numbers since river width varies with discharge w i can in turn be written as a i q i bi where a i and b i are the hydraulic geometry coefficient and exponent of the width discharge power relation leopold and maddock 1953 miguel et al 2016 the analytical derivative of wua i on discharge q i can be calculated as model 1 6 d w u a i d q i a i q i v a i q i b i c exp k q i v a i q i b i a i q i b i 1 c c b i b i k v 1 b i 100 model 2 7 d w u a i d q i a i c exp k q i v a i q i b i k v 1 b i b i a i q i b i 1 b i a i q i b i 1 100 the values for k and c as well as the models used for the parameter a are included in appendix a table a 3 k c and a are applied indifferently to hs empirical equations and to the related wua derivatives since the parameter a represents the average river characteristics values lower or equal to 0 mean that the habitat is not suitable for a given species or group in such cases the terms of the hcp equation are therefore set to 0 in the calculation of the characterization factor the comparison of wua analytical and numerical derivatives confirmed the consistency of calculations see appendix a fig a1 to provide the reader with an overall interpretation of the characterization factor the proposed indicator hcp represents therefore the change in m2 habitat quantity wua from baseline habitat suitability conditions hs induced by river discharge alteration m3 s for instance consuming water in a river with hcp equal to 100 m2 s m3 means altering ten times more usable habitat surface than in a river with hcp equal to 10 m2 s m3 3 2 application of the characterization model and aggregation the characterization model based on hcp has been implemented in the software suite r r core team 2016 rstudio team 2016 using some elements developed by miguel et al 2016 and applied to the french hydrographic network rht pella et al 2012 which includes 114 332 river segments with the associated discharges and other topographical information e g altitude river length strahler order the mean river reach length is 24 7 km 20 4 km standard deviation reach length is generally sufficient for including the diversity of available aquatic habitats in order to appraise the sensitivity of habitat conditions to dry seasons the cf has been calculated for rht q50 median and q90 low flows which are the water discharges in cubic meters per second equaling or exceeding respectively the 50 and 90 percent of the time in the year q90 applies to dry seasons and q50 is the median discharge that is assumed to be characteristic for the rest of the year the reason of this choice is that most water abstraction works except reservoirs do not alter flows much higher than q50 and low flow quantiles q90 are good predictors of aquatic community characteristics lamouroux et al 1999 the ecological consequences of high flow pulses and temporal variability of flow events are out of the scope of the present study and the preference models used here are relevant for low to intermediate discharge rates only where q50 and q90 data were not available 3635 river segments which represent 3 of the overall rht river network database median and low flows have been calculated from the given inter annual average discharge qm based on the coefficients of the linear model fitted to available qm q50 and q90 data the regression has been carried out on rht reaches with qm between 0 001 and 950 m3 s since all missing values were for rivers within this range of discharge appendix a fig a2 in order to calculate the hcp model input variables that were not included in the rht namely the parameters of the width discharge relation and hydraulic geometry dependent variables the hydraulic geometry and the habitat simulation estimhab modules of estimkart have been used in this way it has been possible to calculate all model s input variables from the hydrological and topographical information provided by the rht database specifically discharges strahler order drainage area and river slope for each river segment the hcps indicated in table 1 have been calculated separately aggregated characterization factors are also provided to enable the applicability of habitat models in life cycle assessment lca which requires aggregation into coarser spatial resolutions in order to align with the resolution of life cycle inventory data quantifying elementary flows such as a water abstraction or discharge multi species aggregated indicators at the reach scale have been calculated one for species and one including guilds and invertebrates biomass production species and guilds hcps cannot be combined because some species are already counted in one or more guilds depending on the size class appendix a table a 2 hs functions are not monotonic and can increase or decrease depending on discharge therefore wua derivatives and hcp values can be positive or negative meaning that discharge alteration may respectively lead to habitat loss or habitat gain in order to test the results sensitivity to positive and negative hcps multi species aggregation has been carried out respectively with the individualist hierarchist and egalitarian cultural perspectives thompson et al 1990 the choice between different aggregated lcia indicators is based on the consideration of different perceptions of nature as previously addressed in environmental risk assessment steg and sievers 2000 and life cycle assessment goedkoop and spriensma 2001 huijbregts et al 2016 according to the individualist perspective eq 8 nature is in equilibrium and able to compensate for anthropogenic environmental alterations positive and negative hcps are therefore not weighted and habitat gain counterbalances habitat loss among species 8 i h c p i j 1 n h c p ij the hierarchist approach eq 9 assumes that nature can offset an impact within certain acceptable limits that can be defined and controlled by expert judgement in such a regulation oriented perspective only the most vulnerable species are considered and therefore habitat loss corresponding to positive hcp this approach is based on the same logic adopted in miguel et al 2016 where maximum percent habitat alteration is calculated 9 h h c p i j 1 n h c p ij 0 under the egalitarian perspective eq 10 nature is ephemeral every perturbation of its equilibrium is equally weighted and judged negatively according to the precautionary principle 10 e h c p i j 1 n h c p ij to allow for regionalized water consumption lcia of instream habitats each individual and multi species aggregated characterization factor for a given reach has been upscaled to the sub watershed and watershed scale the spatial aggregation has been performed based on the length of each river segment and thus on the related habitat quantity against the total habitat availability in the watershed eq 11 the latter being identified according to four hydrobasins pfafstetter levels lehner and grill 2013 pfafstetter codes have been merged to rht attributes using the quantum gis geographic information system quantum gis development team 2017 11 c f w f f w e f w d q w d c w u w i 1 n h c p i l i i 1 n l i in the above equation cf ff and ef are calculated at watershed w and l is the river reach length weighted to total length of river segments in the watershed 4 results characterization results in rht river segments are highly variable as represented in fig 2 for riffle species and in fig 3 for all four guilds depending on the observed biological group river reach hcps fall within three or four order of magnitude ranges the detail of riffle species hcp density distribution confirms that habitat sensitivity to water consumption is predictably greater in low flow periods q90 than in normal conditions q50 leptokurtic heavy tailed and right skewed distributions are highlighted in the graphs indicating a non normal distribution of the data sample confirmed by the q q plots in appendix a fig a3 suggesting a gamma distribution for this reason robust statistical measures have been used to analyze the characterization results namely the median absolute deviation mad for statistical dispersion and the medcouple measure of skewness to identify outliers hubert and vandervieren 2008 in order to limit the influence of extreme values in fig 2 compared to median yearly flow conditions the dry season moves the medians m toward higher hcps mq50 54 3 m2 s m3 mq90 137 7 m2 s m3 increases the number of extreme values maxq50 2519 3 m2 s m3 maxq90 4958 8 m2 s m3 and increments the statistical dispersion madq50 62 7 m2 s m3 madq90 139 2 m2 s m3 because of the large size of the sample and the nature of data distribution a relevant number of outliers were identified fig 3 the corresponding river segments were however kept unmodified in the resulting hcp database a check of outlying reaches was performed and no artifact was detected for this reason it was assumed the general validity of the modeled extreme habitat conditions based essentially on their morphological and hydrological characteristics low order streams small size and low discharge 4 1 hcp multi species aggregation at reach scale hcp frequency distributions of the other taxa included in this study follow a tendency akin to the one discussed above for riffle species however results demonstrate that for the same amount of water consumed some species are more sensitive to habitat change than others fig 3 shows that fish guilds adapted to shallow water habitats are in fact the most vulnerable to habitat loss from water consumption see appendix a figures a4 and a5 for all species and invertebrates hcps which is most likely accentuated by the adoption of the weighting factor in eq 3 it is also evident that positive effect factors meaning habitat loss are more frequent than negative ones fig 3 this is due to the fact that the wua derivatives are generally positive for the considered flows for these reasons the multi species aggregation of hcps at the reach scale is driven by a limited number of taxa most likely subject to habitat loss from water consumption the adopted individualist hierarchist and egalitarian aggregation approaches therefore do not show substantial differences and following the parsimony principle the individualist perspective should be used for aggregating lcia habitat indicators at the reach scale see example in appendix a fig a 8 for an application of the three perspectives at watershed scale as an illustrative example the application of habitat indicators aggregated at reach scale to the durance verdon river basin in france fig 4 shows the distribution of habitat sensitivity to water consumption in that watershed fish guilds and benthic invertebrates hcps result in a cumulative characterization factor to which riffle and bank species have a major contribution individualist hcp ihcpgi for guilds and invertebrates were chosen for the representation in fig 4 ihcp classes are defined by percentiles p10 p100 indicating the river segments amount that falls below or is equal to the upper bounds of each class for instance in 90 of river reaches ihcpgi 1711 m2 s m3 as a consequence of a gamma like distribution of hcp values 10 of rivers between p90 and p100 present the highest scores varying by almost a factor of five from the lower to the upper boundary of the same class physical habitat indicators at the river reach scale are highly correlated to the reynolds number as it is the discharge dependent input parameter of the habitat model spearman s ρ 0 99 indicates a non linear negative and monotonic correlation between ihcpgi and reynolds number following the definition of the reynolds number given in materials and methods it is straightforward that habitat change potentials depend largely on discharge q90 ρ 0 97 and river size width ρ 0 81 depth ρ 0 91 velocity ρ 0 86 these variables are in fact at the root of the differences represented in fig 4 between the north eastern and the south western area of the watershed it is also interesting to note that altitude is not significant for the habitat change potential definition ρ 0 05 while stream order strahler 1957 is a necessary condition but not sufficient to determine habitat sensitivity negative correlation ρ 0 67 with strahler order in other terms high strahler stream orders are generally less sensitive to water consumption while low order tributaries show high hcps unless certain conditions of discharge and size are satisfied fig a 6 in appendix a shows the difference between guilds and invertebrates ihcpgi in normal condition and dry season at the national scale median and average values at q50 increase in q90 periods median from 175 5 to 442 8 m2 s m3 average from 254 9 to 764 5 m2 s m3 results are consistent with species specific aggregated habitat change potentials in appendix a fig a 7 where aggregated hcp are driven by individual habitat indicators of brown trout juvenile gudgeon stone loach and minnow 4 2 hcp spatial aggregation at watershed scale in addition to the reach scale characterization factors have been aggregated at four different spatial scales based on the hydrobasins data base watershed boundaries fig 5 hcps at reach scale are weighted by the relative river length against the total length of watershed river segments weighted habitat surface represents therefore the habitat frequency in the watershed it is positively correlated to the probability of habitat alteration at watershed scale due to water consumption should site specific information is not available in fig 5 ihcpgi values are progressively averaged as the spatial resolution decreases maximum hcp is 2625 8 m2 s m3 and 759 2 m2 s m3 at hydrobasins levels 6 and 3 respectively for 1 m3 of water consumed in dry season the picture highlights how sub watersheds characterized by high habitat change potentials determine the overall watershed cf at each step of the spatial aggregation a valid alternative to the spatial aggregation formula proposed in the present study implies using weighted medians to limit the influence of extremes in aggregated cfs however since hcp distribution is the same in all watersheds the change in cf values would not be significant for comparative lca moreover moving the cf closer to the median value of the data sample would imply higher risk of underestimating the habitat change potential of the watershed considering potential model uncertainties for this reason and following a precautionary principle the proposed aggregation method should be used see appendix a section 8 for an example of the application of the weighted median spatial aggregation method 5 discussion river reach characterization factors may represent a useful instrument for site specific lca complementary to environmental risk assessment era and environmental impact assessment eia larrey lassalle et al 2017 multi species aggregation at the reach scale and spatial aggregation at the watershed scale represent a parsimonious approach to modeling habitat change potential which is necessary to respond to the need for large scale spatialized water management and lcia approaches advocated by iso 14044 international organization for standardization iso tc 207 sc 5 2006 and increasingly applied in lca loiseau et al 2014 nitschelm et al 2016 patouillard et al 2018 5 1 model uncertainty optimizing the spatial resolution is crucial to reduce the uncertainty of the impact assessment coming from neglecting spatial variability in principle the higher the spatial resolution of the cf the smaller the uncertainty contribution due to spatial variability however the choice of the most appropriate scale depends largely on the availability of regionalized inventory data henderson et al 2017 mutel et al 2012 in addition the characterization factor presented in this study is built on the hcp model which is essentially an effect factor the development of a regionalized fate factor at comparable spatial resolutions is therefore necessary for the optimization of the resolution of the characterization model section 8 of appendix a discusses another potential source of spatial uncertainty deriving from the aggregation formula of watershed cf the weighted average approach implies overestimating impact assessment results for certain river segments compared to the weighted median method which on the contrary shows higher risk of underestimation the choice of the best aggregation method can thus be subject to the need of more or less conservative approaches depending on the specific application scenario e g lca in critical regions where small stream habitats are endangered or exposed to multiple stressors the uncertainty deriving from model parameters should also be considered in the interpretation of characterization results for instance since discharge data for ungauged river reaches are hardly available in the rht database flow duration curves are modeled for the whole river network and therefore q50 and q90 values the same goes for hydraulic geometry variables modeled through estimkart uncertainty plays a relevant role especially for small catchments low discharges and mountainous areas on the contrary parameters estimates are more accurate in downstream river segments and bigger catchments where the boundaries are defined more precisely lamouroux et al 2014 contrary to spatial uncertainty model parameters uncertainty is therefore potentially higher at the river reach scale than it is for habitat alteration quantification over large areas and thus spatially aggregated cfs taking into account average watershed conditions in support of these considerations the uncertainty analysis by miguel et al 2016 demonstrated that habitat changes at the regional scale are generally robust despite high uncertainties at the reach scale for this reason a reliable cf for lcia should aim at minimizing spatial variability of inventory data ff and ef although ensuring reasonable modeling of average watershed characteristics 5 2 operationalization and model extension for a complete operationalization of the cf associating a multimedia fate factor to the hcp model would be needed núñez et al 2018 this implies including the consideration of different water sources groundwater and surface water the water flow transport between different compartments in the hydrological cycle e g lateral flow from irrigated land returning to the original water basin along with water withdrawal and discharge areas and therefore the spatialization of hydrologic alteration induced by water consumption e g direct and indirect alteration potential longitudinal cascade effects on surface waters as in loubet et al 2013 in the same way the characterization of different water uses in the life cycle inventory should justify the adoption of an effect factor for marginal water consumption i e life cycle impact of river damming would imply non marginal hydrologic alteration in river basins data availability could be the major constraint for refining and extending the model outside of france at present comprehensive databases including all necessary input data i e discharge substrate composition hydraulic geometry are not available globally although useful data sources are currently being developed e g lehner 2018 the model can be therefore applied using local data for an application on the larger scale the input variables should be modeled if not available such as in the french rht hydrographic network or the model should be simplified retaining the factors that most determine characterization results see section 4 1 for improving taxa coverage more habitat equations should be included when possible other freshwater fish species could be assigned to the four guilds based on habitat preference in addition using q50 and q90 at monthly resolution would improve the precision and temporal relevance of the cf since river flow regimes can be different in relation to topographical and climatic conditions 6 conclusions in this study a new mechanistic characterization factor based on freshwater physical habitat modeling has been presented the effect factor model has been applied at the river and watershed scale to assess marginal impact of water consumption on instream ecosystems hcp has been calculated at q50 and q90 as representative of the median discharge condition and dry season respectively a simplified fate factor has been associated to the hcp model limiting the cf applicability to surface water consumption lcia in addition the appropriate impact assessment spatial scale has to be evaluated considering spatialized life cycle inventory data availability and the resolution of the fate factor to limit the risk of hyper regionalization heijungs 2012 and the uncertainty of the model s parameters however modeling the potential midpoint impact on freshwater physical habitat availability can be considered as a first breakthrough from empirical towards mechanistic quantification of instream ecosystems damage due to marginal water consumption notwithstanding the environmental relevance of hcp as a proxy for ecosystem damage the development of large scale endpoint indicators for lcia against this background should consider actual community structures through species distribution data or probability of presence absence miguel et al 2016 this would allow moving from the characterization of potential habitat alteration to species damage developing for example vulnerability metrics for endangered specialists woods et al 2018 ultimately species density and abundance number of individuals per species linked to habitat change could be used to derive lcia density and abundance indicators contributing to a more comprehensive assessment of freshwater ecosystems quality through different complementary biodiversity indicators associated content appendix a and the hcp modeling dataset file are available with this article the rht database and the r script are available upon request to the authors acknowledgements the authors are grateful to arnaud hélias for its contribution on mathematical computation and to all other members of the elsa research group environmental life cycle and sustainability assessment http www elsa lca org for their advice the authors acknowledge anr the occitanie region onema and the industrial partners brl scp suez groupe vinadeis compagnie fruitière for financial support of the industrial chair for environmental and social sustainability assessment elsa pact grant no 13 chin 0005 01 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j watres 2019 114884 
18604,in this article a new characterization model and factors are proposed for the life cycle impact assessment lcia of water consumption on instream freshwater ecosystems impact pathways of freshwater consumption leading to ecosystem damage are described and the alteration of instream physical habitat is identified as a critical midpoint for ecosystem quality the lcia characterization model aims to assess the change in habitat quantity due to consumptive water use it is based on statistical physical habitat simulation for benthic invertebrates fish species and their size classes and guilds of fish sharing common habitat preferences a habitat change potential hcp midpoint mechanistic indicator is developed and computed on the french river network at the river reach scale the river segment with variable length between the upstream and downstream nodes in the hydrographic network for median annual discharges and dry seasons aggregated multi species hcps at a river reach are proposed using various aggregation approaches subsequently the characterization factors are spatially aggregated at watershed and sub watershed scales hcp is highly correlated with median and low flow discharges which determine hydraulic characteristics of reaches aggregation of individual hcps at reach scale is driven by the species most sensitive to water consumption in spatially aggregated hcps consistently with their reduced smaller average discharge rate small stream habitats determine the overall watershed characterization the study is aimed primarily at life cycle assessment lca practitioners and lcia modelers however since it is the result of a productive cross fertilization between the ecohydrology and lca domains it could be potentially useful for watershed management and risk assessment as well at the moment the proposed model is applicable in france for a broader implementation the development of global high resolution river databases or the generalization of the model are needed our new factor represents nevertheless an advancement in freshwater ecosystems lcia laying the basis for new metrics for biodiversity assessment graphical abstract image 1 keywords life cycle assessment water consumption watershed ecology hydraulic habitat environmental flows abbreviations lca life cycle assessment lcia life cycle impact assessment cf characterization factor ff fate factor xf exposure factor ef effect factor sar species area relationship sdr species discharge relationship hcp habitat change potential q river discharge cwu consumptive water use hs habitat suitability wua weighted usable area re reynolds number w river width 1 introduction inland waters are a habitat for rich species diversity approximately 126 000 inland aquatic species have been described according to iucn 2009 representing the 9 5 of all currently identified species and circumscribed in a living environment which is equal to just 0 01 of the total terrestrial surface balian et al 2008 nevertheless 65 of continental waters are moderately or highly threatened by anthropogenic disturbance and climate change with the prospect of an intensification of anthropic pressure on ecosystems and an increase in freshwater needs driven by population growth a lot of efforts have been dedicated to the development of more sustainable water management strategies davis et al 2015 lapointe et al 2014 while these efforts have been capable of ensuring substantial improvement of water security for humans there is still a mismatch with what has been achieved in terms of biodiversity conservation partly because of the likelihood that the ways to meet water needs of humans and ecosystems can be substantially antagonistic vörösmarty et al 2010 in this context several life cycle impact assessment lcia models have been proposed to link freshwater consumptive use to biodiversity loss núñez et al 2016 and addressing specifically wetlands and surface water dependent ecosystems amores et al 2013 hanafiah et al 2011 tendall et al 2014 verones et al 2013b these approaches provide endpoint indicators built on cause effect pathways in which the impact characterization factor cf eq 1 results from the combination of three sub factors núñez et al 2018 the fate factor ff represents the environmental change e g change in m3 y river discharge or m2 wetland area due to water consumption defined as the withdrawn water that is not returned to the original drainage basin international organization for standardization iso tc 207 sc 5 2014 the exposure factor xf indicates how far this alteration can be offset e g in a river it can be approximated to 1 since most freshwater species have less mobility than terrestrial species to compensate the lack of water finally the effect factor ef describes the consequence on the ecosystem e g potentially disappeared fraction of species pdf 1 cf ff xf ef in the literature impact scores related to volumetric change in water availability and as a consequence indirectly linked to water quantity needs of affected taxa have been calculated based on species area relationships in wetlands sar verones et al 2017 2013a 2013b or species discharge relationships sdr in rivers hanafiah et al 2011 tendall et al 2014 xenopoulos et al 2005 despite the relative ease of applying sar and sdr to lcia such empirical approaches to relate species richness to water quantity involve some underlying necessary assumptions making these models less suitable to be used in certain circumstances regarding for instance the completeness of covered taxa sar based methods do not consider instream species namely fish species and invertebrates on the other hand sdr aim at quantifying species occurrence related to river discharge and therefore estimating species mortality of fish and macroinvertebrates derived from flow reduction tendall et al 2014 addressed some limits of sdr for instance by better regionalizing species discharge curves however building a mechanistic lcia model on sdr implies considering equal responses to stress for highly differentiated taxa concealing the complexity of the relationships between living organisms and their habitat it could also lead to interpreting discharge calculated at the catchment outlet as the direct cause of species richness in the catchment which remains unproven in particular more species can be found in large catchments due to larger available space and not only greater discharge and this is interestingly what an approach based on sar would suggest iwasaki et al 2012 mcgarvey and terra 2016 moreover at the local scale species traits and specific flow preferences which can be essential for shaping community structures are not taken into account these considerations are important to evaluate ecosystem response occurring below the extinction threshold which is determined usually by extreme or prolonged events lytle and poff 2004 rather than marginal water flow change what most of lcia models appraise in addition when evaluating long term effects of flow reduction on species such as in modeling global climate change scenarios fish communities background extinction rates natural extinction rates in undisturbed conditions and extinction time horizons when species committed to extinction go actually extinct are not defined leading to a potential overestimation of species loss induced by flow reduction tedesco et al 2013 xenopoulos et al 2005 while lcia methods based on sar and sdr have the advantage of taking into account biological aspects compared to stress based indicators berger and finkbeiner 2013 boulay et al 2018 exploring complementary options to model aspects of biological communities other than species richness number of species such as species abundance number of individuals per species or diversity tuomisto 2010 would therefore support a more comprehensive analysis of ecosystem quality curran et al 2011 damiani et al 2018 this raises the question on whether it would be possible to improve current lcia models to appropriate recent available knowledge concerning freshwater ecology and especially environmental flow management angus webb et al 2013 damiani et al 2018 poff and zimmerman 2010 the aim of this study is first to trace in detail the potential water consumption impact pathways on freshwater habitat and ecosystems the purpose is to identify the main relevant environmental mechanisms and develop a local bottom up mechanistic model addressing the limitations of current lcia models since lotic habitats are currently the most vulnerable to freshwater consumptive use vörösmarty et al 2010 a habitat based characterization factor linking marginal hydrological alteration to instream ecosystems effects is proposed based on existing literature on the applicability of ecohydrological methods in lcia damiani et al 2018 the model is subsequently applied on french streams and the feasibility ecological relevance and limitations of upscaling from the river to the watershed scale is discussed 2 impact pathway analysis the simplified diagram in fig 1 outlines the impact pathways of consumptive freshwater use linked to water dependent ecosystems damage the picture is built on existing environmental flow management and ecohydrology literature angus webb et al 2013 gillespie et al 2015 poff and zimmerman 2010 ecological bibliographical sources in appendix a section 1 provide additional details on the environmental mechanisms represented that would otherwise be difficult to include in the chart freshwater habitats are multifaceted and it is essential for mechanistic lcia to identify relevant impact pathways to be modeled separately particularly when these are linked to specific environmental interventions e g water withdrawal river damming as for the abiotic components of habitat e g morphodynamic and physicochemical aspects of groundwater and surface water bodies the whole ecosystem may undergo cascading effects triggered by water balance changes in fact while an ecological system remains relatively stable over time a human induced alteration may promote a directional shift to a new dynamic equilibrium resulting from the ecological response of the first affected ecosystem compartments and of all those subsequently connected these mechanisms involve all inter and intra specific relationships between different biota and are only generically represented in fig 1 as an example species loss and reduced riparian vegetation cover limits shading on rivers which in turn influences food and shelter availability for reproduction and juvenile growth in fish and macroinvertebrate species li and dudgeon 2008 mokany et al 2008 riley et al 2009 moreover under environmental stress trophic webs play an important role in determining the stability and evolution of an ecological community downing and leibold 2010 thompson et al 2012 it is also necessary to bear in mind that habitat characteristics can be influenced by multiple drivers for instance nutrient availability in water can be determined by flow regime thermal regime and the presence or absence of forest buffers kennen et al 2008 kløve et al 2014 this draws attention to the fact that habitats not only drive the establishment of a specific type of ecosystem but can also be shaped themselves by the biota they harbor these ecosystem mechanisms involve all abiotic and biotic changes induced or limited by the presence of certain ecological communities berke 2010 jones et al 2010 in addition habitat modifications can be further amplified by the proliferation of non native species crooks 2002 ward and ricciardi 2010 for these reasons lcia modeling would require describing ecological mechanisms at different scales from species response to community composition and with short to long term time horizons with this respect hydrologic alteration of flow regimes should be characterized in all its components magnitude timing duration frequency and rate of change as well as flow dependent habitat characteristics the brisbane declaration 2007 in the present article we analyze for the first time in lcia modeling the midpoint effect of flow magnitude alteration i e river discharge on physical habitat i e hydraulics for fish species and stream invertebrates this represents a bottom up approach aimed at determining the relations between freshwater species and their habitat at an early stage of the mechanistic impact pathway the alteration of river flow conditions and hydraulics can result from surface water consumption groundwater consumption and from water infrastructures building and operation the study focuses mainly on the development of an effect factor for marginal discharge alteration in the following section a simplified fate factor is adopted water balance between groundwater and surface water is not modeled and non marginal change in flow regime is not considered e g from river damming for these reasons at present the proposed model would perform better for assessing the impact of direct surface water withdrawal and release 3 materials and methods 3 1 freshwater habitat modeling in lcia water consumption may result in the alteration of river discharge and other related physical variables such as reach hydraulics velocities depths bed forces turbulence depending on the reach morphology and on their different habitat preferences species can be favored or disfavored by these changes in ecohydrology habitat preferences are modeled by means of habitat suitability equations for physical habitat variables such as microhabitat hydraulics width depth velocity the substrate composition and the turbulence bovee 1982 payne and jowett 2013 among ecohydrological habitat models generalized statistical habitat models have been developed based on the findings that species habitat suitability in reaches strongly depends on reach scale hydraulic geometries i e variations in reach average width and water depth with discharge lamouroux and capra 2002 lamouroux and jowett 2005 lamouroux and souchon 2002 this approach to freshwater habitat modeling is well suited for mechanistic lcia indicators evaluating habitat alteration effects on instream assemblages damiani et al 2018 in particular because reach hydraulic geometries can be modeled over the whole hydrographic networks lamouroux 2008 miguel et al 2016 snelder et al 2011 on this basis a habitat based midpoint characterization factor is proposed in eq 2 to quantify the change in habitat availability for freshwater fish species and stream invertebrates according to river discharge alteration 2 cf i ff i ef i d q i d c w u i hcp i cfi is the characterization factor for the river reach i the fate factor ffi represents the marginal change in discharge dq i m3 s for marginal change in consumptive water use dcwu i m3 s in the present study the fate factor is considered equal to 1 as done by hanafiah et al 2011 and tendall et al 2014 lcia models meaning that 1 m3 s of water withdrawn or released in the environment causes 1 m3 s discharge alteration this could be modeled more precisely using a mass balance multimedia fate modeling approach as proposed by núñez et al 2018 the effect factor efi is calculated as the habitat change potential hcpi in m2 s m3 of habitat surface derived from marginal discharge alteration eq 3 in the proposed approach the value of the characterization factor corresponds therefore to the value of the effect factor and will be indicated indifferently as cf or hcp hereafter 3 h c p ij h s ij j 1 n h s ij d w u a ij d q i hcpij is calculated from seventeen multivariate microhabitat suitability hs equations j developed empirically based on the abundance of eight fish species at different ontogenetic stages four fish guilds of species with similar habitat preference and the production of invertebrates biomass table 1 the definitions of the four guilds are adopted from lamouroux et al 2002 the pool guild includes species or size classes preferring deep and slow flowing habitats with fine substrate sediment the bank guild individuals are adapted to shallow and slow flowing waters with fine sediment shallow microhabitats harbor also riffle species if velocities are intermediate to high and with intermediate particle size midstream guild species are instead adapted to fast flowing and deep waters with coarse substrate composition more details on guilds composition are given in appendix a table a 2 most habitat suitability equations considered here are included in the generalized statistical habitat simulation model estimhab lamouroux and capra 2002 lamouroux and souchon 2002 souchon et al 2003 which constitutes the habitat modeling module of the modeling platform estimkart lamouroux et al 2010 the first term of the hcp equation therefore weights the species guild or invertebrates habitat suitability at a given reach against the overall habitat suitability of all fish species guilds or invertebrates respectively in the same river in short the weighted hs represents the reference habitat condition for the chosen species guild or for benthic macroinvertebrates in the second term of equation 3 wua is the weighted usable area in m2 calculated as hs ij w i 100 where w i is the width of the river reach i in meters which is multiplied by 100 m length wua represents therefore the surface of suitable habitat in a reach of a given width bovee 1982 and it is quantified on a constant 100 m length to allow comparability between river segments of different length e g 100 m of 0 4 hs and 50 m of 0 8 hs would give the same wua as a result of two completely different ecological conditions the derivative of wua in the second term is the change in habitat area to discharge change calculated through two different models lamouroux and capra 2002 lamouroux and jowett 2005 lamouroux and souchon 2002 depending on the wua equation attributed in literature to the different taxa based on the best fit to observed abundance data table 1 eqs 4 and 5 the different models correspond to different types of species response to flow model 1 4 w u a i a i r e i c exp k r e i w i 100 model 2 5 w u a i a i 1 c exp k r e i w i 100 in both models re i is the reynolds number representing specific river discharge and turbulence in the river reach i re i is defined as q i v w i where v is the kinematic viscosity of water considered equal to 10 6 m2 s 1 lamouroux et al 1999 the dimensionless parameter a i is a distinctive static descriptor of the reach it is based on its average characteristics at a median discharge level i e it is independent of discharge and its alteration appendix a section 3 conversely the constants c and k shared by all river segments determine the rate of change of wua i with re i within reaches as in estimhab viscosity is multiplied by 107 to run the calculation with low re i numbers since river width varies with discharge w i can in turn be written as a i q i bi where a i and b i are the hydraulic geometry coefficient and exponent of the width discharge power relation leopold and maddock 1953 miguel et al 2016 the analytical derivative of wua i on discharge q i can be calculated as model 1 6 d w u a i d q i a i q i v a i q i b i c exp k q i v a i q i b i a i q i b i 1 c c b i b i k v 1 b i 100 model 2 7 d w u a i d q i a i c exp k q i v a i q i b i k v 1 b i b i a i q i b i 1 b i a i q i b i 1 100 the values for k and c as well as the models used for the parameter a are included in appendix a table a 3 k c and a are applied indifferently to hs empirical equations and to the related wua derivatives since the parameter a represents the average river characteristics values lower or equal to 0 mean that the habitat is not suitable for a given species or group in such cases the terms of the hcp equation are therefore set to 0 in the calculation of the characterization factor the comparison of wua analytical and numerical derivatives confirmed the consistency of calculations see appendix a fig a1 to provide the reader with an overall interpretation of the characterization factor the proposed indicator hcp represents therefore the change in m2 habitat quantity wua from baseline habitat suitability conditions hs induced by river discharge alteration m3 s for instance consuming water in a river with hcp equal to 100 m2 s m3 means altering ten times more usable habitat surface than in a river with hcp equal to 10 m2 s m3 3 2 application of the characterization model and aggregation the characterization model based on hcp has been implemented in the software suite r r core team 2016 rstudio team 2016 using some elements developed by miguel et al 2016 and applied to the french hydrographic network rht pella et al 2012 which includes 114 332 river segments with the associated discharges and other topographical information e g altitude river length strahler order the mean river reach length is 24 7 km 20 4 km standard deviation reach length is generally sufficient for including the diversity of available aquatic habitats in order to appraise the sensitivity of habitat conditions to dry seasons the cf has been calculated for rht q50 median and q90 low flows which are the water discharges in cubic meters per second equaling or exceeding respectively the 50 and 90 percent of the time in the year q90 applies to dry seasons and q50 is the median discharge that is assumed to be characteristic for the rest of the year the reason of this choice is that most water abstraction works except reservoirs do not alter flows much higher than q50 and low flow quantiles q90 are good predictors of aquatic community characteristics lamouroux et al 1999 the ecological consequences of high flow pulses and temporal variability of flow events are out of the scope of the present study and the preference models used here are relevant for low to intermediate discharge rates only where q50 and q90 data were not available 3635 river segments which represent 3 of the overall rht river network database median and low flows have been calculated from the given inter annual average discharge qm based on the coefficients of the linear model fitted to available qm q50 and q90 data the regression has been carried out on rht reaches with qm between 0 001 and 950 m3 s since all missing values were for rivers within this range of discharge appendix a fig a2 in order to calculate the hcp model input variables that were not included in the rht namely the parameters of the width discharge relation and hydraulic geometry dependent variables the hydraulic geometry and the habitat simulation estimhab modules of estimkart have been used in this way it has been possible to calculate all model s input variables from the hydrological and topographical information provided by the rht database specifically discharges strahler order drainage area and river slope for each river segment the hcps indicated in table 1 have been calculated separately aggregated characterization factors are also provided to enable the applicability of habitat models in life cycle assessment lca which requires aggregation into coarser spatial resolutions in order to align with the resolution of life cycle inventory data quantifying elementary flows such as a water abstraction or discharge multi species aggregated indicators at the reach scale have been calculated one for species and one including guilds and invertebrates biomass production species and guilds hcps cannot be combined because some species are already counted in one or more guilds depending on the size class appendix a table a 2 hs functions are not monotonic and can increase or decrease depending on discharge therefore wua derivatives and hcp values can be positive or negative meaning that discharge alteration may respectively lead to habitat loss or habitat gain in order to test the results sensitivity to positive and negative hcps multi species aggregation has been carried out respectively with the individualist hierarchist and egalitarian cultural perspectives thompson et al 1990 the choice between different aggregated lcia indicators is based on the consideration of different perceptions of nature as previously addressed in environmental risk assessment steg and sievers 2000 and life cycle assessment goedkoop and spriensma 2001 huijbregts et al 2016 according to the individualist perspective eq 8 nature is in equilibrium and able to compensate for anthropogenic environmental alterations positive and negative hcps are therefore not weighted and habitat gain counterbalances habitat loss among species 8 i h c p i j 1 n h c p ij the hierarchist approach eq 9 assumes that nature can offset an impact within certain acceptable limits that can be defined and controlled by expert judgement in such a regulation oriented perspective only the most vulnerable species are considered and therefore habitat loss corresponding to positive hcp this approach is based on the same logic adopted in miguel et al 2016 where maximum percent habitat alteration is calculated 9 h h c p i j 1 n h c p ij 0 under the egalitarian perspective eq 10 nature is ephemeral every perturbation of its equilibrium is equally weighted and judged negatively according to the precautionary principle 10 e h c p i j 1 n h c p ij to allow for regionalized water consumption lcia of instream habitats each individual and multi species aggregated characterization factor for a given reach has been upscaled to the sub watershed and watershed scale the spatial aggregation has been performed based on the length of each river segment and thus on the related habitat quantity against the total habitat availability in the watershed eq 11 the latter being identified according to four hydrobasins pfafstetter levels lehner and grill 2013 pfafstetter codes have been merged to rht attributes using the quantum gis geographic information system quantum gis development team 2017 11 c f w f f w e f w d q w d c w u w i 1 n h c p i l i i 1 n l i in the above equation cf ff and ef are calculated at watershed w and l is the river reach length weighted to total length of river segments in the watershed 4 results characterization results in rht river segments are highly variable as represented in fig 2 for riffle species and in fig 3 for all four guilds depending on the observed biological group river reach hcps fall within three or four order of magnitude ranges the detail of riffle species hcp density distribution confirms that habitat sensitivity to water consumption is predictably greater in low flow periods q90 than in normal conditions q50 leptokurtic heavy tailed and right skewed distributions are highlighted in the graphs indicating a non normal distribution of the data sample confirmed by the q q plots in appendix a fig a3 suggesting a gamma distribution for this reason robust statistical measures have been used to analyze the characterization results namely the median absolute deviation mad for statistical dispersion and the medcouple measure of skewness to identify outliers hubert and vandervieren 2008 in order to limit the influence of extreme values in fig 2 compared to median yearly flow conditions the dry season moves the medians m toward higher hcps mq50 54 3 m2 s m3 mq90 137 7 m2 s m3 increases the number of extreme values maxq50 2519 3 m2 s m3 maxq90 4958 8 m2 s m3 and increments the statistical dispersion madq50 62 7 m2 s m3 madq90 139 2 m2 s m3 because of the large size of the sample and the nature of data distribution a relevant number of outliers were identified fig 3 the corresponding river segments were however kept unmodified in the resulting hcp database a check of outlying reaches was performed and no artifact was detected for this reason it was assumed the general validity of the modeled extreme habitat conditions based essentially on their morphological and hydrological characteristics low order streams small size and low discharge 4 1 hcp multi species aggregation at reach scale hcp frequency distributions of the other taxa included in this study follow a tendency akin to the one discussed above for riffle species however results demonstrate that for the same amount of water consumed some species are more sensitive to habitat change than others fig 3 shows that fish guilds adapted to shallow water habitats are in fact the most vulnerable to habitat loss from water consumption see appendix a figures a4 and a5 for all species and invertebrates hcps which is most likely accentuated by the adoption of the weighting factor in eq 3 it is also evident that positive effect factors meaning habitat loss are more frequent than negative ones fig 3 this is due to the fact that the wua derivatives are generally positive for the considered flows for these reasons the multi species aggregation of hcps at the reach scale is driven by a limited number of taxa most likely subject to habitat loss from water consumption the adopted individualist hierarchist and egalitarian aggregation approaches therefore do not show substantial differences and following the parsimony principle the individualist perspective should be used for aggregating lcia habitat indicators at the reach scale see example in appendix a fig a 8 for an application of the three perspectives at watershed scale as an illustrative example the application of habitat indicators aggregated at reach scale to the durance verdon river basin in france fig 4 shows the distribution of habitat sensitivity to water consumption in that watershed fish guilds and benthic invertebrates hcps result in a cumulative characterization factor to which riffle and bank species have a major contribution individualist hcp ihcpgi for guilds and invertebrates were chosen for the representation in fig 4 ihcp classes are defined by percentiles p10 p100 indicating the river segments amount that falls below or is equal to the upper bounds of each class for instance in 90 of river reaches ihcpgi 1711 m2 s m3 as a consequence of a gamma like distribution of hcp values 10 of rivers between p90 and p100 present the highest scores varying by almost a factor of five from the lower to the upper boundary of the same class physical habitat indicators at the river reach scale are highly correlated to the reynolds number as it is the discharge dependent input parameter of the habitat model spearman s ρ 0 99 indicates a non linear negative and monotonic correlation between ihcpgi and reynolds number following the definition of the reynolds number given in materials and methods it is straightforward that habitat change potentials depend largely on discharge q90 ρ 0 97 and river size width ρ 0 81 depth ρ 0 91 velocity ρ 0 86 these variables are in fact at the root of the differences represented in fig 4 between the north eastern and the south western area of the watershed it is also interesting to note that altitude is not significant for the habitat change potential definition ρ 0 05 while stream order strahler 1957 is a necessary condition but not sufficient to determine habitat sensitivity negative correlation ρ 0 67 with strahler order in other terms high strahler stream orders are generally less sensitive to water consumption while low order tributaries show high hcps unless certain conditions of discharge and size are satisfied fig a 6 in appendix a shows the difference between guilds and invertebrates ihcpgi in normal condition and dry season at the national scale median and average values at q50 increase in q90 periods median from 175 5 to 442 8 m2 s m3 average from 254 9 to 764 5 m2 s m3 results are consistent with species specific aggregated habitat change potentials in appendix a fig a 7 where aggregated hcp are driven by individual habitat indicators of brown trout juvenile gudgeon stone loach and minnow 4 2 hcp spatial aggregation at watershed scale in addition to the reach scale characterization factors have been aggregated at four different spatial scales based on the hydrobasins data base watershed boundaries fig 5 hcps at reach scale are weighted by the relative river length against the total length of watershed river segments weighted habitat surface represents therefore the habitat frequency in the watershed it is positively correlated to the probability of habitat alteration at watershed scale due to water consumption should site specific information is not available in fig 5 ihcpgi values are progressively averaged as the spatial resolution decreases maximum hcp is 2625 8 m2 s m3 and 759 2 m2 s m3 at hydrobasins levels 6 and 3 respectively for 1 m3 of water consumed in dry season the picture highlights how sub watersheds characterized by high habitat change potentials determine the overall watershed cf at each step of the spatial aggregation a valid alternative to the spatial aggregation formula proposed in the present study implies using weighted medians to limit the influence of extremes in aggregated cfs however since hcp distribution is the same in all watersheds the change in cf values would not be significant for comparative lca moreover moving the cf closer to the median value of the data sample would imply higher risk of underestimating the habitat change potential of the watershed considering potential model uncertainties for this reason and following a precautionary principle the proposed aggregation method should be used see appendix a section 8 for an example of the application of the weighted median spatial aggregation method 5 discussion river reach characterization factors may represent a useful instrument for site specific lca complementary to environmental risk assessment era and environmental impact assessment eia larrey lassalle et al 2017 multi species aggregation at the reach scale and spatial aggregation at the watershed scale represent a parsimonious approach to modeling habitat change potential which is necessary to respond to the need for large scale spatialized water management and lcia approaches advocated by iso 14044 international organization for standardization iso tc 207 sc 5 2006 and increasingly applied in lca loiseau et al 2014 nitschelm et al 2016 patouillard et al 2018 5 1 model uncertainty optimizing the spatial resolution is crucial to reduce the uncertainty of the impact assessment coming from neglecting spatial variability in principle the higher the spatial resolution of the cf the smaller the uncertainty contribution due to spatial variability however the choice of the most appropriate scale depends largely on the availability of regionalized inventory data henderson et al 2017 mutel et al 2012 in addition the characterization factor presented in this study is built on the hcp model which is essentially an effect factor the development of a regionalized fate factor at comparable spatial resolutions is therefore necessary for the optimization of the resolution of the characterization model section 8 of appendix a discusses another potential source of spatial uncertainty deriving from the aggregation formula of watershed cf the weighted average approach implies overestimating impact assessment results for certain river segments compared to the weighted median method which on the contrary shows higher risk of underestimation the choice of the best aggregation method can thus be subject to the need of more or less conservative approaches depending on the specific application scenario e g lca in critical regions where small stream habitats are endangered or exposed to multiple stressors the uncertainty deriving from model parameters should also be considered in the interpretation of characterization results for instance since discharge data for ungauged river reaches are hardly available in the rht database flow duration curves are modeled for the whole river network and therefore q50 and q90 values the same goes for hydraulic geometry variables modeled through estimkart uncertainty plays a relevant role especially for small catchments low discharges and mountainous areas on the contrary parameters estimates are more accurate in downstream river segments and bigger catchments where the boundaries are defined more precisely lamouroux et al 2014 contrary to spatial uncertainty model parameters uncertainty is therefore potentially higher at the river reach scale than it is for habitat alteration quantification over large areas and thus spatially aggregated cfs taking into account average watershed conditions in support of these considerations the uncertainty analysis by miguel et al 2016 demonstrated that habitat changes at the regional scale are generally robust despite high uncertainties at the reach scale for this reason a reliable cf for lcia should aim at minimizing spatial variability of inventory data ff and ef although ensuring reasonable modeling of average watershed characteristics 5 2 operationalization and model extension for a complete operationalization of the cf associating a multimedia fate factor to the hcp model would be needed núñez et al 2018 this implies including the consideration of different water sources groundwater and surface water the water flow transport between different compartments in the hydrological cycle e g lateral flow from irrigated land returning to the original water basin along with water withdrawal and discharge areas and therefore the spatialization of hydrologic alteration induced by water consumption e g direct and indirect alteration potential longitudinal cascade effects on surface waters as in loubet et al 2013 in the same way the characterization of different water uses in the life cycle inventory should justify the adoption of an effect factor for marginal water consumption i e life cycle impact of river damming would imply non marginal hydrologic alteration in river basins data availability could be the major constraint for refining and extending the model outside of france at present comprehensive databases including all necessary input data i e discharge substrate composition hydraulic geometry are not available globally although useful data sources are currently being developed e g lehner 2018 the model can be therefore applied using local data for an application on the larger scale the input variables should be modeled if not available such as in the french rht hydrographic network or the model should be simplified retaining the factors that most determine characterization results see section 4 1 for improving taxa coverage more habitat equations should be included when possible other freshwater fish species could be assigned to the four guilds based on habitat preference in addition using q50 and q90 at monthly resolution would improve the precision and temporal relevance of the cf since river flow regimes can be different in relation to topographical and climatic conditions 6 conclusions in this study a new mechanistic characterization factor based on freshwater physical habitat modeling has been presented the effect factor model has been applied at the river and watershed scale to assess marginal impact of water consumption on instream ecosystems hcp has been calculated at q50 and q90 as representative of the median discharge condition and dry season respectively a simplified fate factor has been associated to the hcp model limiting the cf applicability to surface water consumption lcia in addition the appropriate impact assessment spatial scale has to be evaluated considering spatialized life cycle inventory data availability and the resolution of the fate factor to limit the risk of hyper regionalization heijungs 2012 and the uncertainty of the model s parameters however modeling the potential midpoint impact on freshwater physical habitat availability can be considered as a first breakthrough from empirical towards mechanistic quantification of instream ecosystems damage due to marginal water consumption notwithstanding the environmental relevance of hcp as a proxy for ecosystem damage the development of large scale endpoint indicators for lcia against this background should consider actual community structures through species distribution data or probability of presence absence miguel et al 2016 this would allow moving from the characterization of potential habitat alteration to species damage developing for example vulnerability metrics for endangered specialists woods et al 2018 ultimately species density and abundance number of individuals per species linked to habitat change could be used to derive lcia density and abundance indicators contributing to a more comprehensive assessment of freshwater ecosystems quality through different complementary biodiversity indicators associated content appendix a and the hcp modeling dataset file are available with this article the rht database and the r script are available upon request to the authors acknowledgements the authors are grateful to arnaud hélias for its contribution on mathematical computation and to all other members of the elsa research group environmental life cycle and sustainability assessment http www elsa lca org for their advice the authors acknowledge anr the occitanie region onema and the industrial partners brl scp suez groupe vinadeis compagnie fruitière for financial support of the industrial chair for environmental and social sustainability assessment elsa pact grant no 13 chin 0005 01 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j watres 2019 114884 
