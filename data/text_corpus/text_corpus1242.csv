index,text
6210,streamflow scenario tree reduction based on conditional monte carlo sampling and regularized optimization jinshu li a feilin zhu a b bin xu b william w g yeh a a department of civil and environmental engineering university of california los angeles ca usa department of civil and environmental engineering university of california los angeles ca usa department of civil and environmental engineering university of california los angeles ca usa b college of hydrology and water resources hohai university nanjing china college of hydrology and water resources hohai university nanjing china college of hydrology and water resources hohai university nanjing china corresponding author this manuscript was handled by p kitanidis editor in chief with the assistance of jian luo associate editor streamflow scenario tree reduction is essential for alleviating the computational burden of a stochastic programming with recourse model this paper develops a new streamflow scenario tree reduction method aimed at preserving important statistical moment information and maintaining streamflow scenario probability specifically we first employ a neural gas algorithm for scenario tree generation then establish a stepwise conditional monte carlo sampling method for systemically reducing the number of scenarios from the full tree we then develop a regularized optimization model based on ridge regression and moment matching to determine the posterior scenario probability we apply the proposed method to the qingjiang cascade reservoir system in china the results show that the reduced tree with 35 reduction level can still maintain robust moment preservations including the mean variance lag one covariance cross site covariance and scenario probability additionally the stability test indicates that the proposed conditional monte carlo sampling method is stable and converges within a reasonable number of scenario combinations keywords streamflow scenario tree generation scenario tree reduction regularized optimization monte carlo sampling reservoir operation 1 introduction handling streamflow uncertainty is a major issue in reservoir management and operation accordingly the stochasticity associated with long term streamflow prediction must be considered in an analysis however methods based on deterministic streamflow prediction for reservoir operations suffer from the risk of making inappropriate release decisions which may incur heavy losses of benefit li et al 2009 zhao et al 2011 zhu et al 2018 to account for streamflow uncertainty the multistage stochastic programming with recourse model has been developed and applied to solve water resources management problems in which streamflow is modeled as a stochastic process birge and louveaux 2011 yeh 1985 zhu et al 2017 this model requires an input of a discretized streamflow scenario tree that can most represent the future random streamflow and the occurrence probability of each scenario the goal of the model is then to minimize the expected loss or maximize the expected benefit existing scenario tree generation methods can be categorized in three broad ways those based on simulation on clustering and on optimization kaut and wallace 2007 discussed methods based on conditional simulation that sample the nodal values or scenarios from known distributions of random variables these methods are implemented easily and able to preserve transition probability but they only can generate uncorrelated random vectors and generally suffer from a problem of dimensionality because of the size of the random vector methods based on clustering however do not require random variable distributions instead these methods pursue the most representative scenarios i e centroids of observation data by clustering observation data hansen and jaumard 1997 šutiene et al 2010 a unique feature of the clustering methods is that the mean value of the generated scenario tree always equals the mean of the observation data the neural gas clustering method is a neural network algorithm originally used for vector quantization in topology martinetz and schulten 1991 martinetz et al 1993 it requires a fixed pre specified tree structure and uses a distance based iterative method to update nodal values to centroids latorre et al 2007 compared four popular clustering techniques including the conditional clustering method neural gas method node clustering method and progressive clustering method and conducted numerical experiments to test their performance by calculating the quantization error for each method the error measures how well the resulting scenario tree fits the original distribution based on the results from the numerical experiments they concluded that the neural gas method performs best in the application to hydro inflow in general a key advantage of clustering based methods is that the sample mean can be preserved very well and the computational cost is minimum however clustering based methods usually fail to preserve other statistical moments and co moments other than the mean this introduces a significant loss of important sample information from the generated scenario tree such as high and low flows xu et al 2015 methods based on optimization mostly refer to moment matching which aims to match different statistical moments and co moments between the scenario tree and historical data series høyland and wallace 2001 in general there are two approaches to implementing moment matching the sequential approach that generates the tree node by node and the overall approach that generates the entire tree in a single optimization however both approaches have drawbacks høyland and wallace 2001 the sequential approach requires that the first stage tree satisfy the first stage statistical properties this may result in conditional second stage properties that are impossible to match and consequently it may produce a suboptimal tree also since the sequential approach updates the nodal values and probability at each stage for a large multi stage scenario tree generation problem it will require solving a large number of optimization problems and is computationally expensive in contrast the overall approach accounts for the entire tree s statistical property and only requires one single optimization for multi stage problems however the degree of non convexity increases significantly with the scale of the optimization problem and it is very difficult to obtain a good match for large scale problems in addition hochreiter and pflug 2007 provided an example of four distributions that coincide with the first four moments the results show that the moment matching method may not be able to match the target distribution despite the shortcomings of the original moment matching method the method has been widely applied to streamflow scenario tree generation vitoriano et al 2000 and portfolio analysis boender 1997 gülpinar et al 2004a furthermore some variants of the moment matching method have been proposed to reduce the number of random variables and expedite the algorithm høyland et al 2003 developed a heuristic algorithm for accelerating the original moment matching but convergence cannot be guaranteed rubasheuski et al 2014 described a method that combines moment matching with a forward selection reduction technique to expedite the tree construction process gülpinar et al 2004b introduced a hybrid method for price assessment scenario tree generation which combines the main ideas of clustering and moment matching in this hybrid method prices the scenario nodal values are obtained from clustering and the scenario probabilities are determined by solving a moment matching problem however in the field of streamflow prediction the probability of a fixed nodal values scenario is required to be fixed so this hybrid approach may not be appropriate this is because the probability of a fixed nodal values scenario under such a hybrid method might be subject to change due to the non uniqueness of the optimization problem if only scenario probabilities are used as decision variables moreover changes of preference coefficients for different moments also can lead to a large probability change of the fixed nodal values scenario thus the result of this hybrid method may yield good model parameters but they cannot be regarded as the scenario probabilities note that the original moment matching method does not suffer from this drawback since both the nodal value and probability are used as decision variables in addition to those three categories there are other scenario tree generation methods pflug 2001 presented a method that can be used to approximate a scenario tree for a given stochastic process based on minimizing the wasserstein distance between the original stochastic process and the generated scenario tree da costa et al 2006 presented a method for producing a parsimonious multivariate scenario tree by using principal component analysis pca but it may encounter computational difficulties when generating a scenario tree with many stages heitsch and römisch 2009a developed a theory based heuristic method that generates scenario trees out of an initial set of scenarios based on forward or backward algorithms for tree generation including recursive scenario reduction and bundling steps pflug and pichler 2015a b presented a method for the dynamic generation of a scenario tree based on random vectors which are drawn from conditional distributions given the past and on sample trajectories these two methods are both capable of generating scenario trees that are good approximations of the historical series however the structure of the generated scenario tree for both methods cannot be determined beforehand it is dynamically adjusted with respect to a distance criterion dupacová et al 2000 examined different scenario tree generation methods and concluded that the choice among these methods should depend on specific problems and the amount of information available since the generated scenario tree is used as the input to a stochastic programming with recourse model the size of the tree directly impacts the dimensionality of the optimization model casey and sen 2005 to avoid high computation cost the number of scenarios size of the tree must be reduced properly pan et al 2015 séguin et al 2017 the most important issue when conducting scenario tree reduction is the tradeoff between information loss from the full tree and the computational burden høyland and wallace 2001 king and wallace 2012 compared with scenario tree generation scenario tree reduction techniques have received less attention in the literature in fact scenario tree generation and scenario tree reduction are often intertwined there are some common features in scenario tree generation and scenario tree reduction and some scenario tree generation techniques also can be used for tree reduction however directly applying tree generation methods to scenario tree reduction is not always the best choice since the reduced tree will inherit the drawbacks of the selected tree generation method for instance xu et al 2015 applied a clustering method to reduce the scenario tree by adopting new pre specified tree structures with smaller sizes this is very easy to implement and the mean of the full tree can be preserved very well however the reduced tree cannot preserve any higher moments due to the inherent limitations of the clustering method moreover applying tree generation methods to tree reduction typically involves generating a new tree of smaller size which cannot be classified as reduction from a given scenario tree if the strict definition of our objective of scenario tree reduction is to determine a subset of the initial scenario tree and assign new probability to the reduced scenarios growe kuska et al 2003 most existing scenario tree reduction methods generate the reduced tree by minimizing the probability metric between the reduced tree s distribution and the full tree s distribution and then calculating the new scenario probability by adding the deleted scenarios probabilities according to some rules the goal of these methods can be summarized as follows 1 min distance distribution full t r e e distribution reduced t r e e which is equivalent to minimizing the following norm 2 min i moment full t r e e i th moment reduced t r e e i th dupacová et al 2003 first proposed a backward reduction and forward selection scenario reduction algorithm for distribution matching heitsch and römisch 2003 then improved the computational efficiency of the algorithm subsequently many studies were carried out focusing on either improving the algorithm or applying it to different fields heitsch et al 2005 heitsch and römisch 2007 heitsch and römisch 2009a heitsch and römisch 2009b de oliveira et al 2010 by minimizing the probability metric between the reduced tree s distribution and the full tree s distribution it is possible to generate a reduced tree with similar distribution as that of the full tree at some loss of information however for water resource planning and management the higher order moments i e the third order moment skewness the fourth order moment kurtosis etc are not as important as the first order moment mean and the second order moment variance particularly in reservoir management and operations yeh 1985 this is because the mean streamflow is closely related to power generation and fluctuation around the mean is important to flood control therefore we believe the methods based on matching the distribution may not be the best for tree reduction for reservoir operations because of the property of scalarization of vector optimization boyd and vandenberghe 2004 the pursuit of matching higher order moments i e the moment i th i 3 in eq 2 will be at the expense of matching the first order and second order moments moreover methods based on distribution matching are not guaranteed to match lag one i e serial and cross site i e spatial correlations these correlations are important in the operation of a cascade reservoir system hao and singh 2013 chen et al 2015 in our study we propose a new scenario tree reduction method based on a variant of the monte carlo sampling method and regularized optimization distinct from all previous studies our proposed method reduces the scenario tree in a systematic way based on the prior probability and conditional sampling it is different from the traditional distribution matching and is specifically designed for matching statistical moments that are important to the management and operation of hydro systems the objective of our proposed method is to minimize the following norm 3 min i r w i co moment historical i th co moment reduced t r e e i th λ β historical β reduced t r e e where r indicates the first r th moments selected for matching w i is the weighting parameter for the co moment i λ is a penalty parameter and β is the scenario probability vector comparing eq 3 with eq 2 it is evident that our method focuses on matching the first r th important moments it is worth noting that our method does not alter the nodal values of the full scenario tree this is more compatible with the definition of scenario tree reduction moreover we formulate a model to optimize a weighted multiple objective function where we can assess the tradeoff between probability matching and moment matching we also test and ensure the stability of our method we outline this paper as follows section 2 1 introduces the neural gas method and emphasizes the probability formula that we use later section 2 2 illustrates the developed stepwise conditional monte carlo random sampling procedure section 2 3 uses the established probability formula to find the historical probability for each reduced tree candidate section 2 4 introduces five important criteria related to moment matching section 2 5 applies regularized optimization to determine posterior probability section 2 6 selects the best reduced tree section 3 applies the proposed method to the qingjiang cascade system of reservoirs in china we provide final remarks and conclusions in section 4 2 methodology a flowchart of the proposed method is shown in fig 1 first we employ the neural gas clustering algorithm to generate a full scenario tree using historical inflow series then based on the prior probability of each scenario obtained from the neural gas procedure we apply stepwise conditional monte carlo random sampling to generate a tractable number of scenario combinations subsets of the full tree serving as the reduced tree candidates for further selection we calculate the historical scenario probability for each reduced tree candidate by reusing the neural gas probability formula then we formulate a regularized optimization model which is a combination of moment matching and modified ridge regression the model generates the posterior scenario probability for each reduced tree candidate lastly we evaluate all the reduced tree candidates and select the one that performs best under different subjective preferences each procedure will be elaborated in the following subsections 2 1 neural gas method for scenario tree generation there are many ways to represent a stochastic process and a scenario tree is widely used to model streamflow stochasticity a scenario tree consists of a finite number of outcomes and their corresponding probabilities at each stage each scenario is basically a path from the root outcome to the leaves outcome and its probability is the product of the outcomes probabilities on that path specifically a streamflow scenario tree is a tree generated from the historical observed streamflow series we use the notation s i i 1 p to define a streamflow scenario tree in which s i is the scenario i and p is the total number of scenarios β i is defined as the scenario probability of scenario i we let t be the stage and t be the total number of stages then we define a sequence of nodes n i t i 1 p t 1 t which consists of the scenario s i from stage 1 root node to stage t leaves node note that n i t n i t if scenario s i and scenario s i have a common node n i t or n i t at stage t in which case they are interchangeable moreover the nodal value can be a vector instead of a scalar which means n i t j n i t 1 n i t 2 n i t 3 n i t m t j 1 m and m is the length of the node vector which in the case of a streamflow scenario tree is the total number of reservoirs in a reservoir system fig 2 is an example of a three stage streamflow scenario tree with four scenarios in which n 1 2 n 2 2 and n 3 2 n 4 2 even though n 2 2 and n 3 2 are not shown in fig 2 the neural gas method a well known artificial neural network algorithm generates the representative vectors from the known vector sets martinetz and schulten 1991 martinetz et al 1993 it is a probabilistic generalization of the k means algorithm in which each vector is assigned a probability of being in each cluster that is proportional to its distance from that cluster s mean it has been shown that the neural gas method outperforms other clustering methods when generating a streamflow scenario tree for hydro inflows data latorre et al 2007 so we adopt the neural gas algorithm for streamflow scenario tree generation the neural gas method extracts several representative sequences from historical streamflow series as different scenarios to form a streamflow scenario tree the steps for performing the neural gas method are detailed in previous studies latorre et al 2007 melato et al 2007 xu et al 2015 however we outline some key steps below 2 1 1 pre specified scenario tree structure the neural gas method requires a pre specified scenario tree structure to determine a the number of scenarios in the scenario tree and b the nodal structure of the tree the tree structure can be described by a scenario tree nodal partition matrix the column vector of a partition matrix represents a scenario the element value of the partition matrix has no meaning but if two scenarios share one node the element values of that node in each scenario should be the same for example the partition matrix of the tree structure in fig 2 can be 1 1 1 1 2 2 3 3 4 5 6 7 because every scenario in the tree shares the common root node in the tree the first row elements of the partition matrix must be the same similarly the last row elements of the partition matrix must be different from each other since the leaves node cannot be the shared node latorre et al 2007 suggested that the initial tree structure should be wide enough not to limit how the scenario tree represents the series ignoring whether the resulting tree will be too large or not 2 1 2 nodal value initialization we initialize the nodal value of each scenario in the scenario tree by randomly selecting a historical streamflow series i e an observed sample vector 4 n i t h rand t i 1 p t 1 t where h υ t is the streamflow value of historical streamflow series υ at stage t υ 1 k rand is a random integer within 1 to k and k is the total number of historical streamflow series each series is a set period in this case a year and stage t corresponds to month then we average the values of the node shared by two or more scenarios i e the nodes with the same partition matrix elements this step is for accelerating the convergence 5 n i t average n i t n j t n z t 2 1 3 iteration to update nodal values altering nodal values to match the historical data series is an important step of the neural gas method which involves the following operations 1 select a series ξ randomly from the historical data series set before each iteration and calculate the euclidean distance between this series and every initialized scenario in the tree then sort the distances in array d in an ascending order and generate an array o to record the distance rank of each scenario 6 d i ξ t t m m h ξ t m n i t m 2 i 1 p 7 o o r d e r d where d i ξ is the euclidean distance between scenario i and randomly picked series ξ h ξ t m is the nodal value of randomly picked series ξ at stage t for reservoir m and n i t m is the nodal value of scenario s i at stage t for reservoir m 2 define the iteration step size functions ε j λ j and adaptation function h o i λ j 8 ε j ε 0 ε f ε 0 j j m 9 λ j λ 0 λ f λ 0 j j m 10 h o i λ j e o i λ j i 1 p where ε 0 and ε f are step size parameters to reduce from ε 0 to ε f after each iteration j represents the iteration time from 0 to the maximum iteration time j m λ 0 and λ f are the adaptation parameters o i is the distance rank of scenario s i ε j λ j and h o i λ j are used to change the nodal values at each iteration as described in step 3 ε j is the step size function a k a learning rate that decides the general step size for every scenario s change and λ j is another step size function a k a adaptation radius that determines the step size of change for each individual scenario both step size functions decrease as the iterations proceed to convergence as a learning algorithm the convergence parameters ε 0 ε f λ 0 λ f and j m can be determined by fine tuning for each case however in this study we adopt the values suggested by latorre et al 2007 and xu et al 2015 as follows maximum iteration time j m 3000 step size parameters ε 0 0 5 and ε f 0 05 and adaptation parameters λ 0 10 and λ f 0 01 the adaptation function h o i λ j provides the adaption value for each scenario based on its distance order to the randomly picked series 3 reduce the distance between the historical data series and the scenarios by changing the node value of n i t by δ n i t where δ n i t is determined by eq 11 11 δ n i t ε j i 1 n i t s i p h o i λ j h ξ t n i t i 1 n i t s i p 1 i 1 p t 1 t note that the notation i 1 2 p n i t s i refers to every index i whose corresponding scenario s i consists of the node n i t update the nodal value 12 n i t j 1 n i t j δ n i t return to procedure 1 until j j m by iterating on nodal values the scenario tree is moved toward the selected series the closer a scenario is to the picked series the more significant change this scenario will experience specifically eq 11 indicates that the change of a given node is determined both by step size and the weighted average distance from every scenario that contains this node to the picked series the entire scenario tree gradually will move and finally converge to the centroid of the historical streamflow series set we provide an illustrative figure fig 5 in the case study section the centroid is a set of p scenarios within the pre specified tree structure i e a scenario tree with p scenarios in the higher dimensional space with minimum total distances to each historical streamflow series 2 1 4 determine the probability of each scenario the probability of scenario s i is proportional to the number of historical flows whose closest scenario is s i this is described mathematically by eq 13 13 β i c o u n t ζ 1 k ζ d i ζ min i 1 p d i ζ k i 1 p where β i is the probability of scenario s i d i ζ is the euclidean distance between scenario s i and historical series ζ and count is the counting function the probability calculated by eq 13 is based on the likelihood of occurrence in the history 2 2 search for scenario subsets based on conditional monte carlo sampling after applying the neural gas algorithm we obtain p scenarios with corresponding probabilities in the full scenario tree the full scenario tree refers to the scenario tree without reduction by sampling the scenario from the full tree c scenario combinations i e subsets are generated and each combination contains r scenarios r p this sampling is based on the full tree scenario probability obtained from the neural gas algorithm 2 2 1 define the reduction level combination size r and total combination number c suppose the full tree contains p scenarios and the associated probabilities are denoted by β i i 1 p the reduction level for the reduced tree is defined as p r p 100 where r is the number of scenarios in the reduced tree a scenario combination is the combination of the sampled r scenarios therefore a scenario combination also can be regarded as a reduced tree candidate with r scenarios the number of combinations is represented by the total combination number c the reason that we specify this number c instead of going through all possible combinations is that the number of all possible combinations can be extremely large for example if we sample 24 scenarios from a full tree with 48 scenarios then the number of all possible combinations is 48 24 3 22 10 13 which is intractable this motivates us to find an efficient sampling strategy to form scenario combinations that are most likely to occur namely c combinations and c p r 2 2 2 sample and formulate scenario combinations we use the prior scenario probability obtained from the neural gas algorithm to formulate scenario subsets by multi steps conditional monte carlo sampling 1 prior probability distribution d 0 before sampling a scenario from the full scenario tree the prior probability distribution is known since the neural gas algorithm calculates the probability of each scenario in the full tree let d 0 be a discrete probability distribution that dictates the probability for each scenario in the full tree the probability mass function pmf of d 0 can be written as 14 pr d 0 s s i β i i 1 p where β i are the prior probabilities obtained from the neural gas algorithm 2 the first step sample and the first conditional probability distribution d 1 under the d 0 distribution we sample one scenario from the full tree sample 1 d 0 so the probability distribution from which it is selected needs to be updated from d 0 to d 1 suppose scenario s u is selected then the pmf of d 1 can be written as 15 pr d 1 s s i β i 1 β u β i i p u β i i 1 p i u where p u indicates all scenarios except the selected scenario s u this equation basically indicates that when s u is picked out we delete β u from d 0 and adjust other scenarios probabilities to form d 1 3 the second step sample and the second conditional probability distribution d 2 under the d 1 distribution we sample the second scenario from the remaining scenarios i e the full tree scenarios with s u deleted sample 2 d 1 suppose scenario s l is selected then the second conditional probability distribution d 2 can be expressed as 16 pr d 2 s s i β i 1 β u β l β i i p u l β i i 1 p i u l 4 the r step sample and r step conditioned probability distribution d r follow the same procedure and keep sampling scenarios from the remaining scenarios until sampling out the r scenario under the r 1 conditioned probability distribution d r 1 sample r d r 1 the r conditional probability distribution d r can be written as 17 pr d r s s i β i i p u l v β i i 1 p i u l v where count u l v r with r the combination size now the sampling procedure over r stages for this combination is terminated resulting in one scenario combination comb 1 s u s l s v repeat the procedures c times to obtain c combinations back to the original prior probability distribution d 0 we implement the same method to sample r scenarios for one combination repeat c times to form c combinations these scenario combinations serve as candidates for the final reduced tree s scenarios and new probabilities for each scenario in each combination will be determined as demonstrated above this proposed stepwise sampling method is developed to find a tractable number of scenario combinations that are most likely to occur since the sampled combination will serve as the reduced tree candidate so it is required that the sampled combination only contains different scenarios the proposed sampling method ensures that the sampled scenarios are different and the sampling process is always based on the prior scenario probability distribution additionally the proposed sampling procedure is guaranteed to terminate in r steps 2 3 the historical probability of the sampled scenario for each combination after sampling r scenarios from the full tree the original probabilities of the sampled scenarios obtained from the neural gas algorithm no longer can be used it is necessary to determine the scenario probability for each of scenario combinations reduced tree candidates in this step we still use the probability formula of the neural gas method to re compute the probability of each scenario in each reduced tree candidate in other words we re calculate the distance array d in eq 6 and then reuse probability eq 13 to determine the probabilities of the scenarios for each combination this probability calculated by reusing the neural gas probability formula eq 13 is named historical probability or β hist note that the probability is calculated based on the distance to the historical data series so it represents the likelihood of occurrence in the history this β hist should be preserved for each scenario in each reduced tree candidate however since we take some scenarios out of the full tree if this β hist is used as the scenario probability in the reduced tree candidates the statistical moments of the reduced tree candidates will be different from the full tree the extent of moment deviation depends on the scenario combination in each reduced tree candidate therefore for the reduced tree candidate k it is preferable to find another posterior scenario probability β k which is sufficiently close to β k h i s t and at the same time minimizes the deviation of statistical moments between the reduced tree and full tree we then assign this posterior probability as the scenario probability of the reduced tree k 2 4 five criteria to evaluate the statistical moment deviation we define five criteria to measure the statistical moment deviation total mean deviation squared tmds total variance deviation tvd total lag one co variance deviation tlcvd total cross site co variance deviation tccvd and mean squared error mse tmds tvd tlcvd and tccvd are used in the optimization step but mse serves as a reference these criteria are explained below 2 4 1 total mean deviation squared tmds this subsection describes the calculation of the mean values for different scenario trees and the historical data series the definition of tmds also is provided mean value of the reduced tree k for reservoir m at stage t 18 μ m t k i 1 r n i m t k β i k mean value of the historical data series for reservoir m at stage t 19 μ m t historical i 1 k h i m t k mean value of the full tree for reservoir m at stage t 20 μ m t fulltree i 1 p n i m t fulltree β i fulltree where β i fulltree is the probability of scenario i of the full tree β i k is the probability of scenario i of the reduced tree k n i m t k is the node value of scenario i in the reduced tree k for reservoir m at stage t h i m t is the value of historical data series i for reservoir m at stage t n i m t fulltree is the node value of scenario i in the full tree for reservoir m at stage t r is the combination size i e the number of scenarios in a reduced tree k is the total number of historical data series p is the number of scenarios in the full tree c is the total number of reduced tree candidates m is the length of the node vector i e the total number of reservoirs in a reservoir system and m 1 m t 1 t k 1 c total mean deviation squared tmds of reduced tree k the term tmds is used to evaluate the deviation from the reduced tree s mean from the historical series mean across all reservoirs and stages a low tmds value for a reduced tree indicates a good match between a reduced tree s mean and the historical mean which is desirable tmds can be summarized as 21 tmd s k m 1 m γ m t 1 t μ m t historical μ m t k 2 where γ m is the weight assigned to reservoir m 2 4 2 total variance deviation tvd this subsection describes the calculation of the variance values for different scenario trees and the historical data series the definition of tvd also is provided variance of the reduced tree k for reservoir m at stage t 22 σ m t 2 k i 1 r n i m t k μ m t k 2 β i k variance of the historical data series for reservoir m at stage t 23 σ m t 2 h i s t o r i c a l i 1 k h i m t μ m t historical 2 k 1 variance of the full tree for reservoir m at stage t 24 σ m t 2 f u l l t r e e i 1 p n i m t fulltree μ m t fulltree 2 β i fulltree total variance deviation tvd of reduced tree k the term tvd represents a criterion that estimates the deviation of the reduced tree s variance from the historical series variance across all reservoirs and stages a low tvd value for a reduced tree implies that the reduced tree reproduces the historical variance well and is able to capture the historical extreme values tvd can be written as 25 tv d k m 1 m γ m t 1 t σ m t 2 h i s t o r i c a l σ m t 2 k 2 4 3 total lag one co variance deviation tlcvd the serial correlation between two consecutive stages in the historical streamflow series can be represented by the lag one covariance lag one co variance of the reduced tree k for reservoir m at stage t 26 co v m t k i 1 r n i m t k μ m t k n i m t 1 k μ m t 1 k β i k lag one co variance of the historical data series for reservoir m at stage t 27 co v m t historical i 1 k h i m t μ m t historical h i m t 1 μ m t 1 historical k 1 lag one co variance of the full tree for reservoir m at stage t 28 co v m t fulltree i 1 p n i m t fulltree μ m t fulltree n i m t 1 fulltree μ m t 1 fulltree β i fulltree equations 26 and 28 are valid for t 2 for historical data we assume co v m 1 historical 0 total lag one co variance deviation tlcvd of the reduced tree k the term tlcvd represents the similarity of the serial correlation between the reduced tree and the historical data series generally the smaller the tlcvd the better since it means the reduced tree has a similar serial correlation as the historical data series this criterion can be defined as 29 tlcv d k m 1 m γ m t 1 t co v m t historical c o v m t k 2 4 4 total cross site co variance deviation tccvd the spatial correlation between two reservoirs in the historical streamflow series can be represented by the cross site covariance cross reservoir co variance of the reduced tree k between reservoir m and n m n at stage t 30 co v m n t k i 1 r n i m t k μ m t k n i n t k μ n t k β i k cross reservoir co variance of the historical data series between reservoir m and n m n at stage t 31 co v m n t historical i 1 k h i m t μ m t historical h i n t μ n t historical k 1 cross reservoir co variance of the full tree between reservoir m and n m n at stage t 32 co v m n t fulltree i 1 p n i m t fulltree μ m t fulltree n i n t fulltree μ n t fulltree β i fulltree total cross reservoir co variance deviation tccvd of the reduced tree k the term tccvd represents the similarity of the cross site correlation between the reduced tree and the historical data series like tlcvd the lower the tccvd the better since it indicates the reduced tree has a similar cross site correlation as the historical data series this criterion can be defined as 33 tccv d k m n m 2 t 1 t co v m n t historical c o v m n t k 2 4 5 mean squared error mse mse is a commonly used index to evaluate the euclidian distance between two data series here it is used to describe the distance between the reduced tree and the historical data series the mse of the reduced tree k is 34 ms e k j 1 k m 1 m γ m t 1 t h j m t i 1 r β i k n i m t k 2 k 2 5 regularized optimization for determining posterior scenario probability as discussed in section 2 3 it is desirable to keep the smallest deviation of statistical moments between the reduced tree and full tree while making each scenario probability stable in this section we apply the technique of regularized optimization which combines the modified ridge regression and moment matching 2 5 1 ridge regression and the modified assumption ridge regression is a widely used biased estimating technique in statistics for multiple linear regression that suffers from multicollinearity hoerl and kennard 1970 it is similar in form to the ordinary least square ols estimator but adds another regularization term thus a traditional ridge linear regression can be generalized as the following optimization problem 35 min ω j i 1 ω y i j 1 ψ x ij ω j 2 λ j 1 ψ ω j 2 where y i are dependent variables x ij are independent variables ω j are regression coefficients λ is the ridge parameter and ω and ψ are the total number of observations and regressors respectively from the bayesian point of view the ols estimator is identical to the maximum likelihood estimator mle under the normality assumption for the error terms in contrast the ridge linear regression ridge lr is identical to the maximum a posteriori map under the normality assumption for the error terms and coefficients terms this condition can be expressed as 36 y j 1 ψ ω j x j ε x ω t ε ε i n 0 σ 2 ω j n 0 τ 2 where ε i is the error term and ω j is the coefficient term both of which have a gaussian distribution with zero mean compared with the ols estimator the ridge lr utilizes the prior information that the coefficient term ω has a gaussian distribution with zero mean based on these assumptions the optimized coefficient ω will be close to zero if a very large value is assumed for the ridge parameter λ however as illustrated in section 2 3 it is desirable to make the scenario probability as close to β hist as possible therefore instead of assuming all the coefficient terms in gaussian distributions with zero mean we assume that the coefficient terms have gaussian distributions with mean values of β hist that is ε i n 0 σ 2 ω j n β j hist τ 2 where β hist is obtained as in section 2 3 based on the assumption and the map principle the modified ridge lr can be expressed as 37 minimize ω j i 1 ω y i j 1 ψ x ij ω j 2 λ j 1 ψ ω j β j hist 2 2 5 2 regularized optimization for scenario tree reduction since matching the statistical moments between observed data i e historical data series and the reduced tree is very important we conduct regularized optimization that combines the modified ridge regression and moment matching in this study we consider matching the first order moment mean the second order moment variance lag one covariance and cross site covariance thus the general form of the optimization model can be expressed as 38 minimize β i w 1 t m d s w 2 t v d w 3 t l c v d w 4 t c c v d λ i 1 r β i β i hist 2 where w 1 w 2 w 3 and w 4 are the weighting parameters for tmds tvd tlcvd and tccvd respectively β i hist is the historical probability of scenario i in a reduced tree candidate and β i the decision variable is the probability of scenario i in the final reduced tree the weighting parameter values sum to one and should be specified by decision makers depending on their preference after substitution the optimization model can be written as 39 minimize β i k w 1 m 1 m γ m t 1 t μ m t historical μ m t k 2 w 2 m 1 m γ m t 1 t σ m t 2 h i s t o r i c a l σ m t 2 k 2 w 3 m 1 m γ m t 1 t c o v m t historical c o v m t k 2 w 4 m n m 2 t 1 t c o v m n t historical c o v m n t k 2 λ i 1 r β i k β i k h i s t 2 subject to 40 i 1 r β i k 1 and β i k 1 k i 1 p k 1 c where k is the total number of historical streamflow series and λ is the ridge parameter the decision variable in the optimization model is the reduced tree probability β i k for the reduced tree k note that eq 39 subject to constraint 40 is a nonlinear and constrained optimization problem we solve it with the interior point method via the matlab optimization toolbox byrd et al 1999 2000 2 5 3 determine the optimal value of ridge parameter λ ridge parameter λ controls the magnitude of preservation of β hist a larger λ value encourages the solution of β i to be closer to β i hist the value of λ determines the tradeoff between matching the statistical moments and matching the historical probabilities we use the ridge trace method suggested by hoerl and kennard 1970 to determine the optimal λ value λ optimal 1 plot a figure that shows the optimal β i values as a function of λ these functions in the plot are considered ridge traces for each β i 2 view the ridge traces and pick values of λ for which all β i values have stabilized it is common that β i will fluctuate very widely for small values of λ and then become stable 3 choose the smallest λ value after all β i have approached constants the smallest value is the optimal λ value namely λ optimal we provide an illustrative figure fig 6 of this method in the case study section 2 5 4 measure total probability deviation by total probability error tpe tpe measures the total probability deviation from the historical scenario probability β hist it is apparent that tpe decreases as the ridge parameter λ increases the acceptable tolerance of tpe is subject to the decision maker the tpe of the reduced tree k is 41 tp e k j 1 r β j k β j k h i s t 2 6 selection of the final reduced tree since we have c different reduced tree candidates we select the reduced tree with the smallest objective function value in eq 39 as our final reduced tree under the specified value of ridge parameter λ 3 case study our study area is the qingjiang cascade reservoir system located in the middle part of the yangtze river china the system produces hydropower to supplement the energy demand in hubei province it consists of three reservoirs shuibuya the most upstream reservoir geheyan the middle reservoir and gaobazhou the most downstream reservoir respectively as shown in fig 3 in fig 3 q 1 is the streamflow into shuibuya reservoir q 2 is the lateral streamflow between shuibuya and geheyan and q 3 is the lateral streamflow between geheyan and gaobazhou for this case study we assume equal weights for all three reservoirs i e γ 1 γ 2 γ 3 1 we have 59 years of monthly historical streamflow data series available from jan 1951 to dec 2009 each set of historical streamflow series consists of 12 stages i e 12 months and each stage has three streamflow values which can be vectorized as q 1 q 2 q 3 t 3 1 generation of the full streamflow scenario tree we use historical data and the neural gas algorithm to generate the full streamflow scenario tree the step size and adaptation parameter values in section 2 1 3 are used the pre specified structure for the full tree is shown in fig 4 a and contains 48 scenarios we also provide fig 4 b to show an example of the reduced scenario tree structure with a 50 reduction level the results from the neural gas algorithm are the 48 representative scenarios including their nodal values and probabilities also these nodal values at different stages can be interpreted as the centroids of the historical data series these results are shown in fig 5 using the shuibuya reservoir as an example as computed by the neural gas method 11 scenarios of the full scenario tree have a zero probability because of the limited historical series sets 59 and relatively large number of scenarios in the full tree 48 it is highly possible that for a specific scenario s i there is no historical series whose closest scenario is s i which leads to zero probability of that scenario in our analysis the zero probability scenarios are discarded hence our full tree is a scenario tree with 37 i e 48 11 37 non zero probability scenarios note that our study focuses on scenario tree reduction for a given a full tree therefore any method can be used to generate the full tree we use the neural gas algorithm because it has been shown that the generated scenario tree performs best for hydro inflow applications latorre et al 2007 3 2 test ridge parameter λ ridge parameter λ will impact the optimization problem its optimum value should be determined prior to optimization in this section we use a 35 reduction level as an example i e the 37 scenario full tree is reduced to 24 scenarios and the weighting parameters are assumed to be as follows w 1 0 98 w 2 0 02 and w 3 w 4 0 i e mean variance optimization with the total combination number c 100 i e the number of reduced tree candidates generated we first employ the ridge trace method to determine the optimal value of ridge parameter λ then conduct a tradeoff analysis to validate the value of λ optimal 3 2 1 ridge trace method for determining λ optimal using the specified parameter values we solve the optimization model then we randomly select reduced tree 15 and its associated scenario probabilities i e optimized coefficients and plot the ridge trace diagram shown in fig 6 the results show that all the coefficient values remain almost constant when λ 10 6 therefore we adopt λ optimal 10 6 3 2 2 validate λ optimal value based on a tradeoff between weighted moment deviation and tpe in eq 38 the two competing objectives are the weighted moment deviation w 1 tmds w 2 tvd w 3 tlcvd w 4 tccvd and tpe by varying the λ value a tradeoff between these two objectives can be determined as shown in fig 7 this makes it possible to select a compromise solution fig 7 shows that λ 10 6 yields the best compromise solution in that both optimized values are low this validates the ridge trace method as suggested by hoerl and kennard 1970 3 3 the mean variance preservation of the reduced trees in this section we demonstrate the mean variance preservation of the reduced tree using the weighting parameters w 1 0 98 w 2 0 02 and w 3 w 4 0 total combination number c 100 optimal ridge parameter λ optimal 10 6 and 35 reduction level additionally we test four different reduction levels 10 35 50 and 70 to evaluate the effect of the reduction level on the reduced tree s moment matching before the optimization process the scenario probabilities for each scenario combination reduced tree candidate are β hist i e historical probability as the scenario combinations are generated by conditional monte carlo sampling some scenario combinations may be able to capture the extreme scenarios in the full tree extreme scenarios are those scenarios containing extreme nodal values thus typically with low probabilities if extreme scenarios are selected in some reduced tree candidates then those candidates may be good candidates for the final reduced tree after the optimization process the posterior scenario probabilities β i are calculated based on the reduced tree selection rule the best reduced tree is selected this best reduced tree is defined as the reduced tree in short the historical probability tree is then defined as the reduced tree candidate that has the same scenarios as the reduced tree the difference between the historical probability tree and the reduced tree is their scenario probabilities which are β i hist and β i respectively fig 8 shows the mean and variance for the historical data series and different scenario trees using shuibuya reservoir as the example also the total moment deviations and mse for each tree are calculated and summarized in table 1 the results reveal that the mean value of the full tree is preserved because of the nature of the neural gas algorithm and the mean value of the reduced tree is also very close to the historical data mean due to the optimization however the historical probability tree has almost more than two times tmds than both the full tree and the reduced tree since it is not optimized as for the variance although the variances of both the full tree and reduced tree are not preserved when compared with the historical variance the variance of the reduced tree deviates less than the full tree which implies that the reduced tree matches the variance better than the full tree when using historical variance as the reference this is because the full tree is generated by the neural gas method which does not preserve higher order moments while the reduced tree is based on the optimization where the higher moments are optimized in addition the mse value between the reduced tree and the historical series is very close to that between the full tree and the historical series which indicates that the absolute nodal value deviation from the historical series also is maintained after the reduction we then test four reduction levels 10 35 50 and 70 based on the full tree with 37 non zero probability scenarios i e p 37 also using λ optimal 10 6 and c 100 their corresponding combination sizes are r 33 r 24 r 18 and r 11 and these numbers are also the number of scenarios left in the reduced trees generally the higher the reduction level the larger the information loss fig 9 shows the mean and variance values for trees with different reduction levels using the shuibuya reservoir as an example note that results obtained for other reservoirs are similar also the total moment deviations and tpe of those trees are summarized in table 2 we arrive at several results first under λ optimal 10 6 the tpe of all reduced trees is acceptable less than 0 1 second the variance deviations of all reduced trees are smaller than the full tree as indicated by tvd in table 2 third the trees with 10 and 35 reduction levels perform better than the full tree regarding variance and their mean deviations are both quite close to the full tree however for the 50 and 70 reduction level cases the means deviate significantly from historical series because of the large information loss and the variance deviations also increase as the reduction level goes up among the four different reduction levels the 35 reduction level is the most ideal at this level of reduction a considerable number of scenarios are eliminated and the mean and variance are well preserved 3 4 preservation of covariances and sensitivity analysis of weighting parameters in addition to mean variance optimization in this section we also account for covariances accordingly we first use weighting coefficients of w 1 0 96 w 2 0 02 w 3 0 02 and w 4 0 total combination number c 100 optimal ridge parameter λ optimal 10 6 and a 35 reduction level to include lag one covariance in the analysis fig 10 demonstrates the comparisons of mean a variance b and lag one covariance c for the two test cases compared with the no covariance optimized case w 1 0 98 w 2 0 02 w 3 w 4 0 the lag one covariance optimized case w 1 0 96 w 2 0 02 w 3 0 02 w 4 0 produces a better match of the lag one covariance but worsens the match of variance as shown in fig 10 to avoid the random error 50 experiments are conducted for both cases and their means are computed table 3 and fig 10 show the similar results table 3 also indicates that the weighted tmds and tvd defined as w 1 tmds w 2 tvd of the lag one covariance optimized case is greater than that of the no covariance optimized case we conduct another experiment named cross site covariance optimized to include the cross site covariance in the optimization we use the following weighting coefficients w 1 0 94 w 2 0 02 w 3 0 02 and w 4 0 02 in eq 38 a comparison of moment preservations is shown in fig 11 for both the lag one covariance optimized case and the cross site covariance optimized case the results show that the former case can better preserve the mean and lag one covariance however the variance and cross site covariance are better matched in the latter case also we can see that unlike the mean value the reduced tree s variance and covariance are not very close to the historical variance and covariance in the first three stages this is caused by the pre determined tree structure that has very few nodes in the first few stages this problem can be corrected by pre specifying a different tree structure that has more nodes at the first few stages however the historical mean value can be preserved very well regardless of the tree structure since the nodal values obtained from the neural gas algorithm are the centroids of the historical series at each stage we conduct sensitivity analysis on the reduced tree s weighting coefficients w 1 w 2 w 3 w 4 under a 35 reduction level the results of moments matching based on the reduced trees with seven different sets of weighting coefficients are listed in table 4 as table 4 shows the full tree does not preserve the historical second order moments very well i e variance lag one covariance and cross site covariance since it is generated by a clustering method that considers only matching the mean value with the assigned weighting parameter values for case 1 the mean and variance are well preserved when compared with the full tree the lag one covariance is included in case 2 case 3 and case 4 are designed to find the best weighting parameter values for cross site covariance case 4 has a worse cross site preservation than case 3 although case 4 s w 4 value is larger this is because the w 2 value of case 4 is smaller than that of case 3 which indicates the w 2 value may have a higher influence on the optimization of cross site covariance than the w 4 value to test this hypothesis case 5 and case 6 are included comparing case 3 with case 5 we see that under the same w 2 values cross site covariance preservation can be improved by simply increasing the w 4 value comparing case 4 with case 5 the positive effect of w 2 on cross site covariance preservation can be detected case 6 shows that w 1 does not have much effect on covariances case 7 tests the range of the w 1 value for obtaining an acceptable tmds value and indicates that w 1 should be at least 0 9 the weights represent the preference of the decision maker however sensitivity analysis can be used to determine the influence of the weights on the objective function and help select the appropriate values it is also important to know the difference in magnitude of each moment in choosing weights in table 4 the weights are varied systematically and the corresponding objective function values calculated the results should help the decision maker to select the appropriate values for the weights for the qingjiang reservoir case study we suggest the following for mean variance preservation use case 1 parameter values for including lag one covariance adopt case 2 parameter values for also including cross site covariance preservation select case 3 or case 5 parameter values admittedly for a different case study the best parameter values may be different 3 5 stability test of the total combination number c and computation time report in the above tests we conducted all the experiments under total combination number c 100 in this section we test the stability of the reduction method under various total combination number c and find a number of c at which the solutions of this method converge we conduct a numerical experiment to test the stability of the conditional monte carlo sampling method by systematically increasing the c value c 10 50 100 200 300 400 500 600 700 800 900 1000 we use the following parameter values w 1 0 98 w 2 0 02 and w 3 w 4 0 optimal ridge parameter λ optimal 10 6 and a 35 reduction level fifty numerical experiments are conducted for each c number the mean of the results for the weighted moment deviation w 1 tmds w 2 tvd w 3 tlcvd w 4 tccvd is plotted in fig 12 additionally we report the computational time for the reduction method under each c number fig 12 reveals that generally the weighted moment deviation stops decreasing at c 700 we conduct the significance test showing that after c 700 the weighted moment deviation has no significant linear relationship with the total combination number under the 95 confidence interval therefore at c 700 the proposed method converges and the small fluctuation after 700 is due to random errors on the other hand the computational time increases almost linearly as the total combination number rises therefore if we trade off between the weighted moment deviation and the computational time a total combination number c 400 would be considered ideal this is because at c 400 both weighted moment deviation and computational times are low 4 conclusion in this paper we presented a new scenario tree reduction method based on stepwise conditional monte carlo sampling and regularized optimization the proposed method is particularly suited for reducing a streamflow scenario tree that consists of many stages for three reasons 1 this method does not require updating nodal values at each stage it is consistent with the definition of scenario tree reduction i e to determine a subset of the initial scenario tree and assign new probability to the reduced scenarios growe kuska et al 2003 2 this method stabilizes the reduced tree scenario s probability and the physical meaning of the streamflow scenario probability is interpreted easily 3 this method does not rely on the probability metric but takes advantage of the basic moment matching technique to provide a direct moment matching between the historical data series and the reduced tree in the qingjiang reservoir case study we determined the optimal ridge parameter λ optimal using the ridge trace method and confirmed this value through the tradeoff between the weighted moment deviation and tpe we tested the moment preservation of the reduced tree with different reduction levels and under mean variance optimization finding that even with a high level of reduction such as 35 the reduced tree still can moment match well with historical series we also examined the covariance showing that it can be preserved well while other moments are not significantly impacted also the stability test showed that the proposed method is stable and approximately converges when the total combination number c 700 which is a small sample size compared with all possible combinations of sampling acknowledgments this work was supported by an aecom endowment we would like to thank three anonymous reviewers for their in depth reviews and constructive comments the remarks and summary of reviewer comments provided by the editor and associate editor also are greatly appreciated data used in this paper are openly available at http doi org 10 5281 zenodo 1194021 
6210,streamflow scenario tree reduction based on conditional monte carlo sampling and regularized optimization jinshu li a feilin zhu a b bin xu b william w g yeh a a department of civil and environmental engineering university of california los angeles ca usa department of civil and environmental engineering university of california los angeles ca usa department of civil and environmental engineering university of california los angeles ca usa b college of hydrology and water resources hohai university nanjing china college of hydrology and water resources hohai university nanjing china college of hydrology and water resources hohai university nanjing china corresponding author this manuscript was handled by p kitanidis editor in chief with the assistance of jian luo associate editor streamflow scenario tree reduction is essential for alleviating the computational burden of a stochastic programming with recourse model this paper develops a new streamflow scenario tree reduction method aimed at preserving important statistical moment information and maintaining streamflow scenario probability specifically we first employ a neural gas algorithm for scenario tree generation then establish a stepwise conditional monte carlo sampling method for systemically reducing the number of scenarios from the full tree we then develop a regularized optimization model based on ridge regression and moment matching to determine the posterior scenario probability we apply the proposed method to the qingjiang cascade reservoir system in china the results show that the reduced tree with 35 reduction level can still maintain robust moment preservations including the mean variance lag one covariance cross site covariance and scenario probability additionally the stability test indicates that the proposed conditional monte carlo sampling method is stable and converges within a reasonable number of scenario combinations keywords streamflow scenario tree generation scenario tree reduction regularized optimization monte carlo sampling reservoir operation 1 introduction handling streamflow uncertainty is a major issue in reservoir management and operation accordingly the stochasticity associated with long term streamflow prediction must be considered in an analysis however methods based on deterministic streamflow prediction for reservoir operations suffer from the risk of making inappropriate release decisions which may incur heavy losses of benefit li et al 2009 zhao et al 2011 zhu et al 2018 to account for streamflow uncertainty the multistage stochastic programming with recourse model has been developed and applied to solve water resources management problems in which streamflow is modeled as a stochastic process birge and louveaux 2011 yeh 1985 zhu et al 2017 this model requires an input of a discretized streamflow scenario tree that can most represent the future random streamflow and the occurrence probability of each scenario the goal of the model is then to minimize the expected loss or maximize the expected benefit existing scenario tree generation methods can be categorized in three broad ways those based on simulation on clustering and on optimization kaut and wallace 2007 discussed methods based on conditional simulation that sample the nodal values or scenarios from known distributions of random variables these methods are implemented easily and able to preserve transition probability but they only can generate uncorrelated random vectors and generally suffer from a problem of dimensionality because of the size of the random vector methods based on clustering however do not require random variable distributions instead these methods pursue the most representative scenarios i e centroids of observation data by clustering observation data hansen and jaumard 1997 šutiene et al 2010 a unique feature of the clustering methods is that the mean value of the generated scenario tree always equals the mean of the observation data the neural gas clustering method is a neural network algorithm originally used for vector quantization in topology martinetz and schulten 1991 martinetz et al 1993 it requires a fixed pre specified tree structure and uses a distance based iterative method to update nodal values to centroids latorre et al 2007 compared four popular clustering techniques including the conditional clustering method neural gas method node clustering method and progressive clustering method and conducted numerical experiments to test their performance by calculating the quantization error for each method the error measures how well the resulting scenario tree fits the original distribution based on the results from the numerical experiments they concluded that the neural gas method performs best in the application to hydro inflow in general a key advantage of clustering based methods is that the sample mean can be preserved very well and the computational cost is minimum however clustering based methods usually fail to preserve other statistical moments and co moments other than the mean this introduces a significant loss of important sample information from the generated scenario tree such as high and low flows xu et al 2015 methods based on optimization mostly refer to moment matching which aims to match different statistical moments and co moments between the scenario tree and historical data series høyland and wallace 2001 in general there are two approaches to implementing moment matching the sequential approach that generates the tree node by node and the overall approach that generates the entire tree in a single optimization however both approaches have drawbacks høyland and wallace 2001 the sequential approach requires that the first stage tree satisfy the first stage statistical properties this may result in conditional second stage properties that are impossible to match and consequently it may produce a suboptimal tree also since the sequential approach updates the nodal values and probability at each stage for a large multi stage scenario tree generation problem it will require solving a large number of optimization problems and is computationally expensive in contrast the overall approach accounts for the entire tree s statistical property and only requires one single optimization for multi stage problems however the degree of non convexity increases significantly with the scale of the optimization problem and it is very difficult to obtain a good match for large scale problems in addition hochreiter and pflug 2007 provided an example of four distributions that coincide with the first four moments the results show that the moment matching method may not be able to match the target distribution despite the shortcomings of the original moment matching method the method has been widely applied to streamflow scenario tree generation vitoriano et al 2000 and portfolio analysis boender 1997 gülpinar et al 2004a furthermore some variants of the moment matching method have been proposed to reduce the number of random variables and expedite the algorithm høyland et al 2003 developed a heuristic algorithm for accelerating the original moment matching but convergence cannot be guaranteed rubasheuski et al 2014 described a method that combines moment matching with a forward selection reduction technique to expedite the tree construction process gülpinar et al 2004b introduced a hybrid method for price assessment scenario tree generation which combines the main ideas of clustering and moment matching in this hybrid method prices the scenario nodal values are obtained from clustering and the scenario probabilities are determined by solving a moment matching problem however in the field of streamflow prediction the probability of a fixed nodal values scenario is required to be fixed so this hybrid approach may not be appropriate this is because the probability of a fixed nodal values scenario under such a hybrid method might be subject to change due to the non uniqueness of the optimization problem if only scenario probabilities are used as decision variables moreover changes of preference coefficients for different moments also can lead to a large probability change of the fixed nodal values scenario thus the result of this hybrid method may yield good model parameters but they cannot be regarded as the scenario probabilities note that the original moment matching method does not suffer from this drawback since both the nodal value and probability are used as decision variables in addition to those three categories there are other scenario tree generation methods pflug 2001 presented a method that can be used to approximate a scenario tree for a given stochastic process based on minimizing the wasserstein distance between the original stochastic process and the generated scenario tree da costa et al 2006 presented a method for producing a parsimonious multivariate scenario tree by using principal component analysis pca but it may encounter computational difficulties when generating a scenario tree with many stages heitsch and römisch 2009a developed a theory based heuristic method that generates scenario trees out of an initial set of scenarios based on forward or backward algorithms for tree generation including recursive scenario reduction and bundling steps pflug and pichler 2015a b presented a method for the dynamic generation of a scenario tree based on random vectors which are drawn from conditional distributions given the past and on sample trajectories these two methods are both capable of generating scenario trees that are good approximations of the historical series however the structure of the generated scenario tree for both methods cannot be determined beforehand it is dynamically adjusted with respect to a distance criterion dupacová et al 2000 examined different scenario tree generation methods and concluded that the choice among these methods should depend on specific problems and the amount of information available since the generated scenario tree is used as the input to a stochastic programming with recourse model the size of the tree directly impacts the dimensionality of the optimization model casey and sen 2005 to avoid high computation cost the number of scenarios size of the tree must be reduced properly pan et al 2015 séguin et al 2017 the most important issue when conducting scenario tree reduction is the tradeoff between information loss from the full tree and the computational burden høyland and wallace 2001 king and wallace 2012 compared with scenario tree generation scenario tree reduction techniques have received less attention in the literature in fact scenario tree generation and scenario tree reduction are often intertwined there are some common features in scenario tree generation and scenario tree reduction and some scenario tree generation techniques also can be used for tree reduction however directly applying tree generation methods to scenario tree reduction is not always the best choice since the reduced tree will inherit the drawbacks of the selected tree generation method for instance xu et al 2015 applied a clustering method to reduce the scenario tree by adopting new pre specified tree structures with smaller sizes this is very easy to implement and the mean of the full tree can be preserved very well however the reduced tree cannot preserve any higher moments due to the inherent limitations of the clustering method moreover applying tree generation methods to tree reduction typically involves generating a new tree of smaller size which cannot be classified as reduction from a given scenario tree if the strict definition of our objective of scenario tree reduction is to determine a subset of the initial scenario tree and assign new probability to the reduced scenarios growe kuska et al 2003 most existing scenario tree reduction methods generate the reduced tree by minimizing the probability metric between the reduced tree s distribution and the full tree s distribution and then calculating the new scenario probability by adding the deleted scenarios probabilities according to some rules the goal of these methods can be summarized as follows 1 min distance distribution full t r e e distribution reduced t r e e which is equivalent to minimizing the following norm 2 min i moment full t r e e i th moment reduced t r e e i th dupacová et al 2003 first proposed a backward reduction and forward selection scenario reduction algorithm for distribution matching heitsch and römisch 2003 then improved the computational efficiency of the algorithm subsequently many studies were carried out focusing on either improving the algorithm or applying it to different fields heitsch et al 2005 heitsch and römisch 2007 heitsch and römisch 2009a heitsch and römisch 2009b de oliveira et al 2010 by minimizing the probability metric between the reduced tree s distribution and the full tree s distribution it is possible to generate a reduced tree with similar distribution as that of the full tree at some loss of information however for water resource planning and management the higher order moments i e the third order moment skewness the fourth order moment kurtosis etc are not as important as the first order moment mean and the second order moment variance particularly in reservoir management and operations yeh 1985 this is because the mean streamflow is closely related to power generation and fluctuation around the mean is important to flood control therefore we believe the methods based on matching the distribution may not be the best for tree reduction for reservoir operations because of the property of scalarization of vector optimization boyd and vandenberghe 2004 the pursuit of matching higher order moments i e the moment i th i 3 in eq 2 will be at the expense of matching the first order and second order moments moreover methods based on distribution matching are not guaranteed to match lag one i e serial and cross site i e spatial correlations these correlations are important in the operation of a cascade reservoir system hao and singh 2013 chen et al 2015 in our study we propose a new scenario tree reduction method based on a variant of the monte carlo sampling method and regularized optimization distinct from all previous studies our proposed method reduces the scenario tree in a systematic way based on the prior probability and conditional sampling it is different from the traditional distribution matching and is specifically designed for matching statistical moments that are important to the management and operation of hydro systems the objective of our proposed method is to minimize the following norm 3 min i r w i co moment historical i th co moment reduced t r e e i th λ β historical β reduced t r e e where r indicates the first r th moments selected for matching w i is the weighting parameter for the co moment i λ is a penalty parameter and β is the scenario probability vector comparing eq 3 with eq 2 it is evident that our method focuses on matching the first r th important moments it is worth noting that our method does not alter the nodal values of the full scenario tree this is more compatible with the definition of scenario tree reduction moreover we formulate a model to optimize a weighted multiple objective function where we can assess the tradeoff between probability matching and moment matching we also test and ensure the stability of our method we outline this paper as follows section 2 1 introduces the neural gas method and emphasizes the probability formula that we use later section 2 2 illustrates the developed stepwise conditional monte carlo random sampling procedure section 2 3 uses the established probability formula to find the historical probability for each reduced tree candidate section 2 4 introduces five important criteria related to moment matching section 2 5 applies regularized optimization to determine posterior probability section 2 6 selects the best reduced tree section 3 applies the proposed method to the qingjiang cascade system of reservoirs in china we provide final remarks and conclusions in section 4 2 methodology a flowchart of the proposed method is shown in fig 1 first we employ the neural gas clustering algorithm to generate a full scenario tree using historical inflow series then based on the prior probability of each scenario obtained from the neural gas procedure we apply stepwise conditional monte carlo random sampling to generate a tractable number of scenario combinations subsets of the full tree serving as the reduced tree candidates for further selection we calculate the historical scenario probability for each reduced tree candidate by reusing the neural gas probability formula then we formulate a regularized optimization model which is a combination of moment matching and modified ridge regression the model generates the posterior scenario probability for each reduced tree candidate lastly we evaluate all the reduced tree candidates and select the one that performs best under different subjective preferences each procedure will be elaborated in the following subsections 2 1 neural gas method for scenario tree generation there are many ways to represent a stochastic process and a scenario tree is widely used to model streamflow stochasticity a scenario tree consists of a finite number of outcomes and their corresponding probabilities at each stage each scenario is basically a path from the root outcome to the leaves outcome and its probability is the product of the outcomes probabilities on that path specifically a streamflow scenario tree is a tree generated from the historical observed streamflow series we use the notation s i i 1 p to define a streamflow scenario tree in which s i is the scenario i and p is the total number of scenarios β i is defined as the scenario probability of scenario i we let t be the stage and t be the total number of stages then we define a sequence of nodes n i t i 1 p t 1 t which consists of the scenario s i from stage 1 root node to stage t leaves node note that n i t n i t if scenario s i and scenario s i have a common node n i t or n i t at stage t in which case they are interchangeable moreover the nodal value can be a vector instead of a scalar which means n i t j n i t 1 n i t 2 n i t 3 n i t m t j 1 m and m is the length of the node vector which in the case of a streamflow scenario tree is the total number of reservoirs in a reservoir system fig 2 is an example of a three stage streamflow scenario tree with four scenarios in which n 1 2 n 2 2 and n 3 2 n 4 2 even though n 2 2 and n 3 2 are not shown in fig 2 the neural gas method a well known artificial neural network algorithm generates the representative vectors from the known vector sets martinetz and schulten 1991 martinetz et al 1993 it is a probabilistic generalization of the k means algorithm in which each vector is assigned a probability of being in each cluster that is proportional to its distance from that cluster s mean it has been shown that the neural gas method outperforms other clustering methods when generating a streamflow scenario tree for hydro inflows data latorre et al 2007 so we adopt the neural gas algorithm for streamflow scenario tree generation the neural gas method extracts several representative sequences from historical streamflow series as different scenarios to form a streamflow scenario tree the steps for performing the neural gas method are detailed in previous studies latorre et al 2007 melato et al 2007 xu et al 2015 however we outline some key steps below 2 1 1 pre specified scenario tree structure the neural gas method requires a pre specified scenario tree structure to determine a the number of scenarios in the scenario tree and b the nodal structure of the tree the tree structure can be described by a scenario tree nodal partition matrix the column vector of a partition matrix represents a scenario the element value of the partition matrix has no meaning but if two scenarios share one node the element values of that node in each scenario should be the same for example the partition matrix of the tree structure in fig 2 can be 1 1 1 1 2 2 3 3 4 5 6 7 because every scenario in the tree shares the common root node in the tree the first row elements of the partition matrix must be the same similarly the last row elements of the partition matrix must be different from each other since the leaves node cannot be the shared node latorre et al 2007 suggested that the initial tree structure should be wide enough not to limit how the scenario tree represents the series ignoring whether the resulting tree will be too large or not 2 1 2 nodal value initialization we initialize the nodal value of each scenario in the scenario tree by randomly selecting a historical streamflow series i e an observed sample vector 4 n i t h rand t i 1 p t 1 t where h υ t is the streamflow value of historical streamflow series υ at stage t υ 1 k rand is a random integer within 1 to k and k is the total number of historical streamflow series each series is a set period in this case a year and stage t corresponds to month then we average the values of the node shared by two or more scenarios i e the nodes with the same partition matrix elements this step is for accelerating the convergence 5 n i t average n i t n j t n z t 2 1 3 iteration to update nodal values altering nodal values to match the historical data series is an important step of the neural gas method which involves the following operations 1 select a series ξ randomly from the historical data series set before each iteration and calculate the euclidean distance between this series and every initialized scenario in the tree then sort the distances in array d in an ascending order and generate an array o to record the distance rank of each scenario 6 d i ξ t t m m h ξ t m n i t m 2 i 1 p 7 o o r d e r d where d i ξ is the euclidean distance between scenario i and randomly picked series ξ h ξ t m is the nodal value of randomly picked series ξ at stage t for reservoir m and n i t m is the nodal value of scenario s i at stage t for reservoir m 2 define the iteration step size functions ε j λ j and adaptation function h o i λ j 8 ε j ε 0 ε f ε 0 j j m 9 λ j λ 0 λ f λ 0 j j m 10 h o i λ j e o i λ j i 1 p where ε 0 and ε f are step size parameters to reduce from ε 0 to ε f after each iteration j represents the iteration time from 0 to the maximum iteration time j m λ 0 and λ f are the adaptation parameters o i is the distance rank of scenario s i ε j λ j and h o i λ j are used to change the nodal values at each iteration as described in step 3 ε j is the step size function a k a learning rate that decides the general step size for every scenario s change and λ j is another step size function a k a adaptation radius that determines the step size of change for each individual scenario both step size functions decrease as the iterations proceed to convergence as a learning algorithm the convergence parameters ε 0 ε f λ 0 λ f and j m can be determined by fine tuning for each case however in this study we adopt the values suggested by latorre et al 2007 and xu et al 2015 as follows maximum iteration time j m 3000 step size parameters ε 0 0 5 and ε f 0 05 and adaptation parameters λ 0 10 and λ f 0 01 the adaptation function h o i λ j provides the adaption value for each scenario based on its distance order to the randomly picked series 3 reduce the distance between the historical data series and the scenarios by changing the node value of n i t by δ n i t where δ n i t is determined by eq 11 11 δ n i t ε j i 1 n i t s i p h o i λ j h ξ t n i t i 1 n i t s i p 1 i 1 p t 1 t note that the notation i 1 2 p n i t s i refers to every index i whose corresponding scenario s i consists of the node n i t update the nodal value 12 n i t j 1 n i t j δ n i t return to procedure 1 until j j m by iterating on nodal values the scenario tree is moved toward the selected series the closer a scenario is to the picked series the more significant change this scenario will experience specifically eq 11 indicates that the change of a given node is determined both by step size and the weighted average distance from every scenario that contains this node to the picked series the entire scenario tree gradually will move and finally converge to the centroid of the historical streamflow series set we provide an illustrative figure fig 5 in the case study section the centroid is a set of p scenarios within the pre specified tree structure i e a scenario tree with p scenarios in the higher dimensional space with minimum total distances to each historical streamflow series 2 1 4 determine the probability of each scenario the probability of scenario s i is proportional to the number of historical flows whose closest scenario is s i this is described mathematically by eq 13 13 β i c o u n t ζ 1 k ζ d i ζ min i 1 p d i ζ k i 1 p where β i is the probability of scenario s i d i ζ is the euclidean distance between scenario s i and historical series ζ and count is the counting function the probability calculated by eq 13 is based on the likelihood of occurrence in the history 2 2 search for scenario subsets based on conditional monte carlo sampling after applying the neural gas algorithm we obtain p scenarios with corresponding probabilities in the full scenario tree the full scenario tree refers to the scenario tree without reduction by sampling the scenario from the full tree c scenario combinations i e subsets are generated and each combination contains r scenarios r p this sampling is based on the full tree scenario probability obtained from the neural gas algorithm 2 2 1 define the reduction level combination size r and total combination number c suppose the full tree contains p scenarios and the associated probabilities are denoted by β i i 1 p the reduction level for the reduced tree is defined as p r p 100 where r is the number of scenarios in the reduced tree a scenario combination is the combination of the sampled r scenarios therefore a scenario combination also can be regarded as a reduced tree candidate with r scenarios the number of combinations is represented by the total combination number c the reason that we specify this number c instead of going through all possible combinations is that the number of all possible combinations can be extremely large for example if we sample 24 scenarios from a full tree with 48 scenarios then the number of all possible combinations is 48 24 3 22 10 13 which is intractable this motivates us to find an efficient sampling strategy to form scenario combinations that are most likely to occur namely c combinations and c p r 2 2 2 sample and formulate scenario combinations we use the prior scenario probability obtained from the neural gas algorithm to formulate scenario subsets by multi steps conditional monte carlo sampling 1 prior probability distribution d 0 before sampling a scenario from the full scenario tree the prior probability distribution is known since the neural gas algorithm calculates the probability of each scenario in the full tree let d 0 be a discrete probability distribution that dictates the probability for each scenario in the full tree the probability mass function pmf of d 0 can be written as 14 pr d 0 s s i β i i 1 p where β i are the prior probabilities obtained from the neural gas algorithm 2 the first step sample and the first conditional probability distribution d 1 under the d 0 distribution we sample one scenario from the full tree sample 1 d 0 so the probability distribution from which it is selected needs to be updated from d 0 to d 1 suppose scenario s u is selected then the pmf of d 1 can be written as 15 pr d 1 s s i β i 1 β u β i i p u β i i 1 p i u where p u indicates all scenarios except the selected scenario s u this equation basically indicates that when s u is picked out we delete β u from d 0 and adjust other scenarios probabilities to form d 1 3 the second step sample and the second conditional probability distribution d 2 under the d 1 distribution we sample the second scenario from the remaining scenarios i e the full tree scenarios with s u deleted sample 2 d 1 suppose scenario s l is selected then the second conditional probability distribution d 2 can be expressed as 16 pr d 2 s s i β i 1 β u β l β i i p u l β i i 1 p i u l 4 the r step sample and r step conditioned probability distribution d r follow the same procedure and keep sampling scenarios from the remaining scenarios until sampling out the r scenario under the r 1 conditioned probability distribution d r 1 sample r d r 1 the r conditional probability distribution d r can be written as 17 pr d r s s i β i i p u l v β i i 1 p i u l v where count u l v r with r the combination size now the sampling procedure over r stages for this combination is terminated resulting in one scenario combination comb 1 s u s l s v repeat the procedures c times to obtain c combinations back to the original prior probability distribution d 0 we implement the same method to sample r scenarios for one combination repeat c times to form c combinations these scenario combinations serve as candidates for the final reduced tree s scenarios and new probabilities for each scenario in each combination will be determined as demonstrated above this proposed stepwise sampling method is developed to find a tractable number of scenario combinations that are most likely to occur since the sampled combination will serve as the reduced tree candidate so it is required that the sampled combination only contains different scenarios the proposed sampling method ensures that the sampled scenarios are different and the sampling process is always based on the prior scenario probability distribution additionally the proposed sampling procedure is guaranteed to terminate in r steps 2 3 the historical probability of the sampled scenario for each combination after sampling r scenarios from the full tree the original probabilities of the sampled scenarios obtained from the neural gas algorithm no longer can be used it is necessary to determine the scenario probability for each of scenario combinations reduced tree candidates in this step we still use the probability formula of the neural gas method to re compute the probability of each scenario in each reduced tree candidate in other words we re calculate the distance array d in eq 6 and then reuse probability eq 13 to determine the probabilities of the scenarios for each combination this probability calculated by reusing the neural gas probability formula eq 13 is named historical probability or β hist note that the probability is calculated based on the distance to the historical data series so it represents the likelihood of occurrence in the history this β hist should be preserved for each scenario in each reduced tree candidate however since we take some scenarios out of the full tree if this β hist is used as the scenario probability in the reduced tree candidates the statistical moments of the reduced tree candidates will be different from the full tree the extent of moment deviation depends on the scenario combination in each reduced tree candidate therefore for the reduced tree candidate k it is preferable to find another posterior scenario probability β k which is sufficiently close to β k h i s t and at the same time minimizes the deviation of statistical moments between the reduced tree and full tree we then assign this posterior probability as the scenario probability of the reduced tree k 2 4 five criteria to evaluate the statistical moment deviation we define five criteria to measure the statistical moment deviation total mean deviation squared tmds total variance deviation tvd total lag one co variance deviation tlcvd total cross site co variance deviation tccvd and mean squared error mse tmds tvd tlcvd and tccvd are used in the optimization step but mse serves as a reference these criteria are explained below 2 4 1 total mean deviation squared tmds this subsection describes the calculation of the mean values for different scenario trees and the historical data series the definition of tmds also is provided mean value of the reduced tree k for reservoir m at stage t 18 μ m t k i 1 r n i m t k β i k mean value of the historical data series for reservoir m at stage t 19 μ m t historical i 1 k h i m t k mean value of the full tree for reservoir m at stage t 20 μ m t fulltree i 1 p n i m t fulltree β i fulltree where β i fulltree is the probability of scenario i of the full tree β i k is the probability of scenario i of the reduced tree k n i m t k is the node value of scenario i in the reduced tree k for reservoir m at stage t h i m t is the value of historical data series i for reservoir m at stage t n i m t fulltree is the node value of scenario i in the full tree for reservoir m at stage t r is the combination size i e the number of scenarios in a reduced tree k is the total number of historical data series p is the number of scenarios in the full tree c is the total number of reduced tree candidates m is the length of the node vector i e the total number of reservoirs in a reservoir system and m 1 m t 1 t k 1 c total mean deviation squared tmds of reduced tree k the term tmds is used to evaluate the deviation from the reduced tree s mean from the historical series mean across all reservoirs and stages a low tmds value for a reduced tree indicates a good match between a reduced tree s mean and the historical mean which is desirable tmds can be summarized as 21 tmd s k m 1 m γ m t 1 t μ m t historical μ m t k 2 where γ m is the weight assigned to reservoir m 2 4 2 total variance deviation tvd this subsection describes the calculation of the variance values for different scenario trees and the historical data series the definition of tvd also is provided variance of the reduced tree k for reservoir m at stage t 22 σ m t 2 k i 1 r n i m t k μ m t k 2 β i k variance of the historical data series for reservoir m at stage t 23 σ m t 2 h i s t o r i c a l i 1 k h i m t μ m t historical 2 k 1 variance of the full tree for reservoir m at stage t 24 σ m t 2 f u l l t r e e i 1 p n i m t fulltree μ m t fulltree 2 β i fulltree total variance deviation tvd of reduced tree k the term tvd represents a criterion that estimates the deviation of the reduced tree s variance from the historical series variance across all reservoirs and stages a low tvd value for a reduced tree implies that the reduced tree reproduces the historical variance well and is able to capture the historical extreme values tvd can be written as 25 tv d k m 1 m γ m t 1 t σ m t 2 h i s t o r i c a l σ m t 2 k 2 4 3 total lag one co variance deviation tlcvd the serial correlation between two consecutive stages in the historical streamflow series can be represented by the lag one covariance lag one co variance of the reduced tree k for reservoir m at stage t 26 co v m t k i 1 r n i m t k μ m t k n i m t 1 k μ m t 1 k β i k lag one co variance of the historical data series for reservoir m at stage t 27 co v m t historical i 1 k h i m t μ m t historical h i m t 1 μ m t 1 historical k 1 lag one co variance of the full tree for reservoir m at stage t 28 co v m t fulltree i 1 p n i m t fulltree μ m t fulltree n i m t 1 fulltree μ m t 1 fulltree β i fulltree equations 26 and 28 are valid for t 2 for historical data we assume co v m 1 historical 0 total lag one co variance deviation tlcvd of the reduced tree k the term tlcvd represents the similarity of the serial correlation between the reduced tree and the historical data series generally the smaller the tlcvd the better since it means the reduced tree has a similar serial correlation as the historical data series this criterion can be defined as 29 tlcv d k m 1 m γ m t 1 t co v m t historical c o v m t k 2 4 4 total cross site co variance deviation tccvd the spatial correlation between two reservoirs in the historical streamflow series can be represented by the cross site covariance cross reservoir co variance of the reduced tree k between reservoir m and n m n at stage t 30 co v m n t k i 1 r n i m t k μ m t k n i n t k μ n t k β i k cross reservoir co variance of the historical data series between reservoir m and n m n at stage t 31 co v m n t historical i 1 k h i m t μ m t historical h i n t μ n t historical k 1 cross reservoir co variance of the full tree between reservoir m and n m n at stage t 32 co v m n t fulltree i 1 p n i m t fulltree μ m t fulltree n i n t fulltree μ n t fulltree β i fulltree total cross reservoir co variance deviation tccvd of the reduced tree k the term tccvd represents the similarity of the cross site correlation between the reduced tree and the historical data series like tlcvd the lower the tccvd the better since it indicates the reduced tree has a similar cross site correlation as the historical data series this criterion can be defined as 33 tccv d k m n m 2 t 1 t co v m n t historical c o v m n t k 2 4 5 mean squared error mse mse is a commonly used index to evaluate the euclidian distance between two data series here it is used to describe the distance between the reduced tree and the historical data series the mse of the reduced tree k is 34 ms e k j 1 k m 1 m γ m t 1 t h j m t i 1 r β i k n i m t k 2 k 2 5 regularized optimization for determining posterior scenario probability as discussed in section 2 3 it is desirable to keep the smallest deviation of statistical moments between the reduced tree and full tree while making each scenario probability stable in this section we apply the technique of regularized optimization which combines the modified ridge regression and moment matching 2 5 1 ridge regression and the modified assumption ridge regression is a widely used biased estimating technique in statistics for multiple linear regression that suffers from multicollinearity hoerl and kennard 1970 it is similar in form to the ordinary least square ols estimator but adds another regularization term thus a traditional ridge linear regression can be generalized as the following optimization problem 35 min ω j i 1 ω y i j 1 ψ x ij ω j 2 λ j 1 ψ ω j 2 where y i are dependent variables x ij are independent variables ω j are regression coefficients λ is the ridge parameter and ω and ψ are the total number of observations and regressors respectively from the bayesian point of view the ols estimator is identical to the maximum likelihood estimator mle under the normality assumption for the error terms in contrast the ridge linear regression ridge lr is identical to the maximum a posteriori map under the normality assumption for the error terms and coefficients terms this condition can be expressed as 36 y j 1 ψ ω j x j ε x ω t ε ε i n 0 σ 2 ω j n 0 τ 2 where ε i is the error term and ω j is the coefficient term both of which have a gaussian distribution with zero mean compared with the ols estimator the ridge lr utilizes the prior information that the coefficient term ω has a gaussian distribution with zero mean based on these assumptions the optimized coefficient ω will be close to zero if a very large value is assumed for the ridge parameter λ however as illustrated in section 2 3 it is desirable to make the scenario probability as close to β hist as possible therefore instead of assuming all the coefficient terms in gaussian distributions with zero mean we assume that the coefficient terms have gaussian distributions with mean values of β hist that is ε i n 0 σ 2 ω j n β j hist τ 2 where β hist is obtained as in section 2 3 based on the assumption and the map principle the modified ridge lr can be expressed as 37 minimize ω j i 1 ω y i j 1 ψ x ij ω j 2 λ j 1 ψ ω j β j hist 2 2 5 2 regularized optimization for scenario tree reduction since matching the statistical moments between observed data i e historical data series and the reduced tree is very important we conduct regularized optimization that combines the modified ridge regression and moment matching in this study we consider matching the first order moment mean the second order moment variance lag one covariance and cross site covariance thus the general form of the optimization model can be expressed as 38 minimize β i w 1 t m d s w 2 t v d w 3 t l c v d w 4 t c c v d λ i 1 r β i β i hist 2 where w 1 w 2 w 3 and w 4 are the weighting parameters for tmds tvd tlcvd and tccvd respectively β i hist is the historical probability of scenario i in a reduced tree candidate and β i the decision variable is the probability of scenario i in the final reduced tree the weighting parameter values sum to one and should be specified by decision makers depending on their preference after substitution the optimization model can be written as 39 minimize β i k w 1 m 1 m γ m t 1 t μ m t historical μ m t k 2 w 2 m 1 m γ m t 1 t σ m t 2 h i s t o r i c a l σ m t 2 k 2 w 3 m 1 m γ m t 1 t c o v m t historical c o v m t k 2 w 4 m n m 2 t 1 t c o v m n t historical c o v m n t k 2 λ i 1 r β i k β i k h i s t 2 subject to 40 i 1 r β i k 1 and β i k 1 k i 1 p k 1 c where k is the total number of historical streamflow series and λ is the ridge parameter the decision variable in the optimization model is the reduced tree probability β i k for the reduced tree k note that eq 39 subject to constraint 40 is a nonlinear and constrained optimization problem we solve it with the interior point method via the matlab optimization toolbox byrd et al 1999 2000 2 5 3 determine the optimal value of ridge parameter λ ridge parameter λ controls the magnitude of preservation of β hist a larger λ value encourages the solution of β i to be closer to β i hist the value of λ determines the tradeoff between matching the statistical moments and matching the historical probabilities we use the ridge trace method suggested by hoerl and kennard 1970 to determine the optimal λ value λ optimal 1 plot a figure that shows the optimal β i values as a function of λ these functions in the plot are considered ridge traces for each β i 2 view the ridge traces and pick values of λ for which all β i values have stabilized it is common that β i will fluctuate very widely for small values of λ and then become stable 3 choose the smallest λ value after all β i have approached constants the smallest value is the optimal λ value namely λ optimal we provide an illustrative figure fig 6 of this method in the case study section 2 5 4 measure total probability deviation by total probability error tpe tpe measures the total probability deviation from the historical scenario probability β hist it is apparent that tpe decreases as the ridge parameter λ increases the acceptable tolerance of tpe is subject to the decision maker the tpe of the reduced tree k is 41 tp e k j 1 r β j k β j k h i s t 2 6 selection of the final reduced tree since we have c different reduced tree candidates we select the reduced tree with the smallest objective function value in eq 39 as our final reduced tree under the specified value of ridge parameter λ 3 case study our study area is the qingjiang cascade reservoir system located in the middle part of the yangtze river china the system produces hydropower to supplement the energy demand in hubei province it consists of three reservoirs shuibuya the most upstream reservoir geheyan the middle reservoir and gaobazhou the most downstream reservoir respectively as shown in fig 3 in fig 3 q 1 is the streamflow into shuibuya reservoir q 2 is the lateral streamflow between shuibuya and geheyan and q 3 is the lateral streamflow between geheyan and gaobazhou for this case study we assume equal weights for all three reservoirs i e γ 1 γ 2 γ 3 1 we have 59 years of monthly historical streamflow data series available from jan 1951 to dec 2009 each set of historical streamflow series consists of 12 stages i e 12 months and each stage has three streamflow values which can be vectorized as q 1 q 2 q 3 t 3 1 generation of the full streamflow scenario tree we use historical data and the neural gas algorithm to generate the full streamflow scenario tree the step size and adaptation parameter values in section 2 1 3 are used the pre specified structure for the full tree is shown in fig 4 a and contains 48 scenarios we also provide fig 4 b to show an example of the reduced scenario tree structure with a 50 reduction level the results from the neural gas algorithm are the 48 representative scenarios including their nodal values and probabilities also these nodal values at different stages can be interpreted as the centroids of the historical data series these results are shown in fig 5 using the shuibuya reservoir as an example as computed by the neural gas method 11 scenarios of the full scenario tree have a zero probability because of the limited historical series sets 59 and relatively large number of scenarios in the full tree 48 it is highly possible that for a specific scenario s i there is no historical series whose closest scenario is s i which leads to zero probability of that scenario in our analysis the zero probability scenarios are discarded hence our full tree is a scenario tree with 37 i e 48 11 37 non zero probability scenarios note that our study focuses on scenario tree reduction for a given a full tree therefore any method can be used to generate the full tree we use the neural gas algorithm because it has been shown that the generated scenario tree performs best for hydro inflow applications latorre et al 2007 3 2 test ridge parameter λ ridge parameter λ will impact the optimization problem its optimum value should be determined prior to optimization in this section we use a 35 reduction level as an example i e the 37 scenario full tree is reduced to 24 scenarios and the weighting parameters are assumed to be as follows w 1 0 98 w 2 0 02 and w 3 w 4 0 i e mean variance optimization with the total combination number c 100 i e the number of reduced tree candidates generated we first employ the ridge trace method to determine the optimal value of ridge parameter λ then conduct a tradeoff analysis to validate the value of λ optimal 3 2 1 ridge trace method for determining λ optimal using the specified parameter values we solve the optimization model then we randomly select reduced tree 15 and its associated scenario probabilities i e optimized coefficients and plot the ridge trace diagram shown in fig 6 the results show that all the coefficient values remain almost constant when λ 10 6 therefore we adopt λ optimal 10 6 3 2 2 validate λ optimal value based on a tradeoff between weighted moment deviation and tpe in eq 38 the two competing objectives are the weighted moment deviation w 1 tmds w 2 tvd w 3 tlcvd w 4 tccvd and tpe by varying the λ value a tradeoff between these two objectives can be determined as shown in fig 7 this makes it possible to select a compromise solution fig 7 shows that λ 10 6 yields the best compromise solution in that both optimized values are low this validates the ridge trace method as suggested by hoerl and kennard 1970 3 3 the mean variance preservation of the reduced trees in this section we demonstrate the mean variance preservation of the reduced tree using the weighting parameters w 1 0 98 w 2 0 02 and w 3 w 4 0 total combination number c 100 optimal ridge parameter λ optimal 10 6 and 35 reduction level additionally we test four different reduction levels 10 35 50 and 70 to evaluate the effect of the reduction level on the reduced tree s moment matching before the optimization process the scenario probabilities for each scenario combination reduced tree candidate are β hist i e historical probability as the scenario combinations are generated by conditional monte carlo sampling some scenario combinations may be able to capture the extreme scenarios in the full tree extreme scenarios are those scenarios containing extreme nodal values thus typically with low probabilities if extreme scenarios are selected in some reduced tree candidates then those candidates may be good candidates for the final reduced tree after the optimization process the posterior scenario probabilities β i are calculated based on the reduced tree selection rule the best reduced tree is selected this best reduced tree is defined as the reduced tree in short the historical probability tree is then defined as the reduced tree candidate that has the same scenarios as the reduced tree the difference between the historical probability tree and the reduced tree is their scenario probabilities which are β i hist and β i respectively fig 8 shows the mean and variance for the historical data series and different scenario trees using shuibuya reservoir as the example also the total moment deviations and mse for each tree are calculated and summarized in table 1 the results reveal that the mean value of the full tree is preserved because of the nature of the neural gas algorithm and the mean value of the reduced tree is also very close to the historical data mean due to the optimization however the historical probability tree has almost more than two times tmds than both the full tree and the reduced tree since it is not optimized as for the variance although the variances of both the full tree and reduced tree are not preserved when compared with the historical variance the variance of the reduced tree deviates less than the full tree which implies that the reduced tree matches the variance better than the full tree when using historical variance as the reference this is because the full tree is generated by the neural gas method which does not preserve higher order moments while the reduced tree is based on the optimization where the higher moments are optimized in addition the mse value between the reduced tree and the historical series is very close to that between the full tree and the historical series which indicates that the absolute nodal value deviation from the historical series also is maintained after the reduction we then test four reduction levels 10 35 50 and 70 based on the full tree with 37 non zero probability scenarios i e p 37 also using λ optimal 10 6 and c 100 their corresponding combination sizes are r 33 r 24 r 18 and r 11 and these numbers are also the number of scenarios left in the reduced trees generally the higher the reduction level the larger the information loss fig 9 shows the mean and variance values for trees with different reduction levels using the shuibuya reservoir as an example note that results obtained for other reservoirs are similar also the total moment deviations and tpe of those trees are summarized in table 2 we arrive at several results first under λ optimal 10 6 the tpe of all reduced trees is acceptable less than 0 1 second the variance deviations of all reduced trees are smaller than the full tree as indicated by tvd in table 2 third the trees with 10 and 35 reduction levels perform better than the full tree regarding variance and their mean deviations are both quite close to the full tree however for the 50 and 70 reduction level cases the means deviate significantly from historical series because of the large information loss and the variance deviations also increase as the reduction level goes up among the four different reduction levels the 35 reduction level is the most ideal at this level of reduction a considerable number of scenarios are eliminated and the mean and variance are well preserved 3 4 preservation of covariances and sensitivity analysis of weighting parameters in addition to mean variance optimization in this section we also account for covariances accordingly we first use weighting coefficients of w 1 0 96 w 2 0 02 w 3 0 02 and w 4 0 total combination number c 100 optimal ridge parameter λ optimal 10 6 and a 35 reduction level to include lag one covariance in the analysis fig 10 demonstrates the comparisons of mean a variance b and lag one covariance c for the two test cases compared with the no covariance optimized case w 1 0 98 w 2 0 02 w 3 w 4 0 the lag one covariance optimized case w 1 0 96 w 2 0 02 w 3 0 02 w 4 0 produces a better match of the lag one covariance but worsens the match of variance as shown in fig 10 to avoid the random error 50 experiments are conducted for both cases and their means are computed table 3 and fig 10 show the similar results table 3 also indicates that the weighted tmds and tvd defined as w 1 tmds w 2 tvd of the lag one covariance optimized case is greater than that of the no covariance optimized case we conduct another experiment named cross site covariance optimized to include the cross site covariance in the optimization we use the following weighting coefficients w 1 0 94 w 2 0 02 w 3 0 02 and w 4 0 02 in eq 38 a comparison of moment preservations is shown in fig 11 for both the lag one covariance optimized case and the cross site covariance optimized case the results show that the former case can better preserve the mean and lag one covariance however the variance and cross site covariance are better matched in the latter case also we can see that unlike the mean value the reduced tree s variance and covariance are not very close to the historical variance and covariance in the first three stages this is caused by the pre determined tree structure that has very few nodes in the first few stages this problem can be corrected by pre specifying a different tree structure that has more nodes at the first few stages however the historical mean value can be preserved very well regardless of the tree structure since the nodal values obtained from the neural gas algorithm are the centroids of the historical series at each stage we conduct sensitivity analysis on the reduced tree s weighting coefficients w 1 w 2 w 3 w 4 under a 35 reduction level the results of moments matching based on the reduced trees with seven different sets of weighting coefficients are listed in table 4 as table 4 shows the full tree does not preserve the historical second order moments very well i e variance lag one covariance and cross site covariance since it is generated by a clustering method that considers only matching the mean value with the assigned weighting parameter values for case 1 the mean and variance are well preserved when compared with the full tree the lag one covariance is included in case 2 case 3 and case 4 are designed to find the best weighting parameter values for cross site covariance case 4 has a worse cross site preservation than case 3 although case 4 s w 4 value is larger this is because the w 2 value of case 4 is smaller than that of case 3 which indicates the w 2 value may have a higher influence on the optimization of cross site covariance than the w 4 value to test this hypothesis case 5 and case 6 are included comparing case 3 with case 5 we see that under the same w 2 values cross site covariance preservation can be improved by simply increasing the w 4 value comparing case 4 with case 5 the positive effect of w 2 on cross site covariance preservation can be detected case 6 shows that w 1 does not have much effect on covariances case 7 tests the range of the w 1 value for obtaining an acceptable tmds value and indicates that w 1 should be at least 0 9 the weights represent the preference of the decision maker however sensitivity analysis can be used to determine the influence of the weights on the objective function and help select the appropriate values it is also important to know the difference in magnitude of each moment in choosing weights in table 4 the weights are varied systematically and the corresponding objective function values calculated the results should help the decision maker to select the appropriate values for the weights for the qingjiang reservoir case study we suggest the following for mean variance preservation use case 1 parameter values for including lag one covariance adopt case 2 parameter values for also including cross site covariance preservation select case 3 or case 5 parameter values admittedly for a different case study the best parameter values may be different 3 5 stability test of the total combination number c and computation time report in the above tests we conducted all the experiments under total combination number c 100 in this section we test the stability of the reduction method under various total combination number c and find a number of c at which the solutions of this method converge we conduct a numerical experiment to test the stability of the conditional monte carlo sampling method by systematically increasing the c value c 10 50 100 200 300 400 500 600 700 800 900 1000 we use the following parameter values w 1 0 98 w 2 0 02 and w 3 w 4 0 optimal ridge parameter λ optimal 10 6 and a 35 reduction level fifty numerical experiments are conducted for each c number the mean of the results for the weighted moment deviation w 1 tmds w 2 tvd w 3 tlcvd w 4 tccvd is plotted in fig 12 additionally we report the computational time for the reduction method under each c number fig 12 reveals that generally the weighted moment deviation stops decreasing at c 700 we conduct the significance test showing that after c 700 the weighted moment deviation has no significant linear relationship with the total combination number under the 95 confidence interval therefore at c 700 the proposed method converges and the small fluctuation after 700 is due to random errors on the other hand the computational time increases almost linearly as the total combination number rises therefore if we trade off between the weighted moment deviation and the computational time a total combination number c 400 would be considered ideal this is because at c 400 both weighted moment deviation and computational times are low 4 conclusion in this paper we presented a new scenario tree reduction method based on stepwise conditional monte carlo sampling and regularized optimization the proposed method is particularly suited for reducing a streamflow scenario tree that consists of many stages for three reasons 1 this method does not require updating nodal values at each stage it is consistent with the definition of scenario tree reduction i e to determine a subset of the initial scenario tree and assign new probability to the reduced scenarios growe kuska et al 2003 2 this method stabilizes the reduced tree scenario s probability and the physical meaning of the streamflow scenario probability is interpreted easily 3 this method does not rely on the probability metric but takes advantage of the basic moment matching technique to provide a direct moment matching between the historical data series and the reduced tree in the qingjiang reservoir case study we determined the optimal ridge parameter λ optimal using the ridge trace method and confirmed this value through the tradeoff between the weighted moment deviation and tpe we tested the moment preservation of the reduced tree with different reduction levels and under mean variance optimization finding that even with a high level of reduction such as 35 the reduced tree still can moment match well with historical series we also examined the covariance showing that it can be preserved well while other moments are not significantly impacted also the stability test showed that the proposed method is stable and approximately converges when the total combination number c 700 which is a small sample size compared with all possible combinations of sampling acknowledgments this work was supported by an aecom endowment we would like to thank three anonymous reviewers for their in depth reviews and constructive comments the remarks and summary of reviewer comments provided by the editor and associate editor also are greatly appreciated data used in this paper are openly available at http doi org 10 5281 zenodo 1194021 
6211,sediment transport in open channels has complicated nature and finding the analytical models applicable for channel design in practice is a quite difficult task to this end behind theoretical consideration of the open channel sediment transport through incorporating of four fundamental characteristics of fluid flow sediment and channel recently machine learning techniques are used for modeling of sediment transport in open channels however most of the studies in the literature used limited number of data for model development neglecting some effective parameters involved which may affect their performances moreover most of this studies had not provided a comprehensive explicit equation for future use accordingly this study applied four machine learning techniques of gene expression programming gep extreme learning machine elm generalized structure group method of data handling gs gmdh and fuzzy c means based adaptive neuro fuzzy inference system fcm anfis to model sediment transport in open channels four existing data sets in the literature with wide ranges of pipe size sediment size sediment volumetric concentration channel bed slope and flow depth are used for the model development the recommended models are compared with their corresponding conventional regression models taken from the literature in terms of different statistical performance indices results indicate superiority of the machine leaning techniques to the conventional multiple non linear regression models although developed gep elm gs gmdh and fcm anfis models have almost same performances gs gmdh gives slightly better performance which can be linked to the generalized structure of this approach a matlab code is provided to calculate the sediment transport in open channel for practical engineering keywords extreme learning machine fuzzy c means based adaptive neuro fuzzy inference system gene expression programming generalized structure of group method of data handling rigid boundary channel sediment transport 1 introduction linked to the urban hydrology modeling of sediment transport in rigid boundary open channels is of importance for designing of urban drainage systems irrigation canals and generally all types of lined channels sediment transport in alluvial channels has been studied to a great extent fakhri et al 2014 dalezios et al 2018 and its basic technology may be found in the well known textbooks such as raudkivi 1990 and vanoni 2006 there are variety of sediment transport conditions which are essential in studying of sediment transport in rigid boundary channels such as incipient motion incipient deposition and non deposition among those non deposition condition is mostly considered for design of rigid boundary channels to prevent permanent deposition of sediment within the flow examples of non deposition sediment transport studies in rigid boundary open channels are given herein novak and nalluri 1975 evaluated the loose boundary sediment transport models on rigid boundary channel experimental data and found out that shear stress and velocity required to retain sediment particles in motion are lower for the case of rigid boundary channels macke 1982 and arora 1983 studied suspended sediment transport at non deposition condition considering more hydraulic parameters that their results were re analyzed by nalluri and spaliviero 1998 may et al 1989 and may 1993 studied bed load sediment transport considering shear stress acting on the bad layer through analyzing hydrodynamic forces acting on sediment particle at the channel bottom ackers 1984 modified ackers and white 1973 sediment transport model for applying in rigid boundary channels by considering effective bed width mayerle 1988 and mayerle et al 1991 conducted experiments in circular and rectangular channels and evaluated formerly collected experimental data of novak and nalluri 1984 ab ghani 1993 carried out comprehensive experimental study on the bed load sediment transport in rigid boundary open channels to find the effect of pipe size at non deposition condition it is reported that larger channels require higher design velocity may et al 1996 butler et al 2003 and de sutter et al 2003 evaluated the applicability of formerly developed models to use in practice for sewer pipe design ota 1999 and ota and perrusquia 2013 performed set of experiments and analyzed own data considering much more hydraulic parameters such as sediment particle shape factor and angle of repose at non deposition condition of sediment transport vongvisessomjai et al 2010 conducted experiments in two pipes to study the bed load and suspended load sediment transport and evaluated the conventional camp criterion for drainage system design and found out that it overestimates design flow velocity safari et al 2017a carried out experiments in a trapezoidal cross section channel and introduced channel cross section shape factor considering flow resistance in open channel flow a model was developed and its applicability on different cross section channels was evaluated using large number of experimental data it is worthy to mention that aforementioned studies suggested sediment transport models using multiple non linear regression analysis details of aforementioned studies are given in safari et al 2018 examples of non deposition bed load sediment transport models in rigid boundary channels are listed in table 1 as shown in table 1 most of the studies in the literature used particle froude number frp as dependent variable defined as 1 f r p v gd s 1 where v is flow mean velocity d sediment median size g gravitational acceleration and s relative specific mass of sediment ρs to fluid ρ as presented in safari et al 2015 2016 frp can be obtained by analyzing the hydrodynamic forces acting on the sediment particle and also through combination of the well known shields 1936 parameter with manning formula generally frp is selected as dependent variable and fluid flow sediment and channel characteristics are considered as independent variables through the modeling process recently machine learning techniques have been widely applied on variety of water resources engineering problems chau 2017 alizadeh et al 2018 shamshirband et al 2019 yaseen et al 2018 because of complicated nature of sediment transport in open channel flow machine learning techniques have attracted interest of many researchers olyaie et al 2015 chen and chau 2016 various machine leaning techniques were used for modeling of sediment transport in rigid boundary open channels ab ghani and azamathulla 2010 and azamathulla et al 2012 applied gene expression programming gep and adaptive neuro fuzzy inference systems anfis techniques and developed models similar to the conventional regression models available in the literature however the suggested model performance was found superior to their counterparts qasem et al 2017a used particle swarm optimization pso for determination of the radial basis function neural network rbf parameters and evaluated different combination of input parameters to find most effective input parameters in sediment transport in rigid boundary channels different data driven techniques have been used for modeling of sediment transport in rigid boundary channels such as wavelet support vector machines svm by ebtehaj et al 2015 neuro fuzzy based group method of data handling nf gmdh by najafzadeh and bonakdari 2016 decision tree artificial neural network dt ann by ebtehaj et al 2016a combination of svm with firefly algorithm by ebtehaj et al 2016b evolutionary polynomial regression epr by najafzadeh et al 2017 extreme learning machine elm and support vector machines svm by roushangar and ghasempour 2017a combination of evolutionary algorithm with ann by ebtehaj and bonakdari 2016 and roushangar and ghasempour 2017b and multigene genetic programming mggp by safari and danandeh mehr 2018 safari et al 2016 and wan mohtar et al 2018 evaluated applicability of different artificial neural network ann techniques for modeling sediment threshold in sewer pipes it was concluded that appropriately constructed ann models can compute flow mean velocity precisely safari 2019 modelled sediment transport in sewer pipes using decision tree dt and multivariate adaptive regression splines mars techniques dt found channel design velocity of 0 6 m s similar to sewer design standards in many countries it is concluded that mars outperforms dt generalized regression neural network gr and conventional multiple non linear regression models safari et al 2017b and safari and shirzad 2019 applied particle swarm optimization pso for studying sediment transport in large sewers safari et al 2017b made a modification on well known may 1993 model the conditional equations of may 1993 was unified in a single equation and its applicability was demonstrated through evaluation of the developed equation performance on different data sets safari and shirzad 2019 defined optimum deposited bed thickness for large sewer pipes and recommended design charts for variety of pipe size and flow depths the optimum deposited bed thickness of 1 of pipe diameter was suggested for large sewer design it has to be emphasized that three factors of utilized machine learning technique ranges of experimental data used and selection of effective parameters involved must be considered to develop a robust data driven model this study is the extension of the authors previous works to enhance the modeling of sediment transport in rigid boundary open channels through considering aforesaid factors as powerful machine learning techniques gep elm gs gmdh and fcm anfis are applied for modeling most of the studies in the literature utilized two experimental data sets for modeling while this study used four experimental data sets covering wide ranges of pipe size sediment size and concentration flow depth and channel bed slope several studies in the literature neglected some effective parameters to simplify and increase the model performance while this study considered four fundamental characteristics of flow fluid sediment and channel for modeling of sediment transport in rigid boundary open channels 2 materials and methods 2 1 experimental data in order to model sediment transport in rigid boundary channels four data sets of mayerle 1988 may 1993 ab ghani 1993 and vongvisessomjai et al 2010 are used in this study experimental conditions and procedures are explained briefly as follows mayerle 1988 conducted experiments in two different cross section channels as rectangular and circular two rectangular channels with glass walls one with a width of 311 mm and a length of 12 2 m and another with a width of 462 mm and 12 6 m length with a smooth and rough bed and a circular pvc channel with a diameter of 152 mm and a length of 20 5 m with a smooth bed were used in the experiments sediment particles with six different sizes 0 5 mm 1 05 mm 1 95 mm 2 56 mm 5 22 mm and 8 74 mm were used in the experiments an electromagnetic vibrator system was installed as a sediment feeder at the beginning of the channel mayerle 1988 defined non deposition without deposition condition as limit of deposition in which the flow conveys the maximum amount of sediment without deposition in such a situation the flow velocity in which sediment particles move under the influence of flow forces are defined as self cleansing velocity through the experiments firstly uniform flow was established then sediment deposition condition was reached by increasing the sediment feeder discharge sediment discharge was determined by measuring the weight of dried sediment before and after of each experiment the depth of the flow was measured at each meter of the channel before and during the test to determine the effect of sediment deposition on the flow may 1993 performed experiments in a concrete circular channel having 450 mm diameter and 21 3 m length sediment was supplied by mixing of water with sediment and then pumped into the channel the clean water flow discharge was measured by a weir installed at the upstream of the channel and channel discharge depth relationship was determined sand size of 0 73 mm was used in the experiments for ensuring the uniform flow condition flow depth was measured at five points of the channel behind achieving the equilibrium condition of sediment deposition hydraulic characteristics as flow depth and sediment discharge were recorded ab ghani 1993 carried out experiments in three circular channels two pvc channels with a diameter of 154 mm a length of 20 5 m and another with a diameter of 305 mm and a length of 20 5 m and a concrete channel with a diameter of 405 mm and a length of 21 3 m were used in the experiments measurements were done at seven points in the observation section of the channel a tailgate at the downstream of the channel was used to adjust the flow depth to establish a uniform flow a triangular weir was used at the downstream of the channel to measure the flow discharge through the experiments in channels with diameters of 154 mm and 305 mm six sediment sizes of 0 46 mm 0 97 mm 2 mm 4 2 mm 5 7 mm and 8 3 mm and in channel with diameter of 405 mm sediment size of 0 72 mm were used the amount of sediment entered into the channel was adjusted using an electromagnetic vibrator system after establishing a uniform flow deposition condition was reached by increasing the sediment feeder discharge the sediment discharge was measured by weighing the accumulated sediments over a given time period the sediment was collected in channel downstream dried and reused for the next experiments vongvisessomjai et al 2010 carried out experiments in two circular pvc channels with diameter of 150 mm and 100 mm and both in length of 16 m three types of uniform sand with sizes of 0 2 mm 0 3 mm and 0 43 mm were used in the experiments the depth of flow across the channel was measured at several points to create a uniform flow the tailgate at the channel downstream was adjusted and accordingly the flow depth remained constant a sediment feeder was used for supplying the sediment within the flow through increasing the sediment feeder discharge sediment deposition condition was achieved 2 2 model parameters it is reported by robinson and graf 1972 that a sediment transport model should incorporate four fundamental flow fluid sediment and channel characteristics the effective variables of sediment transport in rigid boundary channels respecting the physically based models available in the literature can be considered as flow mean velocity v hydraulic radius r gravitational acceleration g fluid specific mass ρ and kinematic viscosity ν sediment median size d sediment volumetric concentration cv sediment specific mass ρs and channel friction factor λ among those v r and g are flow characteristics ρ and ν as fluid d cv and ρs as sediment and λ as channel characteristic are considered regarding the sediment transport models in rigid boundary channels aforementioned variables can be written as group of dimensionless parameters as follows 5 v gd s 1 f c v d gr d r λ experimental data are re formed based on the above equation to prepare the inputs and output of the model it is seen that left hand side of eq 5 as output of the model is particle froude number and parameters given at the right hand side are inputs of the model the prepared data are split in two parts for training and testing data sets through the training stage the unknown relationship of input and output parameters is determined and in the testing stage the performance of the developed model is investigated on unseen data set to this extent among 375 data 300 data for training and 75 data are randomly selected to test the models before the modeling data are re scaled to values between 0 1 to behave equally in the modeling procedure 2 3 statistical performance indices the performances of the models must be investigated for understanding their credibility hence three statistical performance indices of coefficient of determination r 2 mean absolute percentage error mape and discrepancy ratio dr are used in this study the r 2 is a statistical measure of how the results are fitted to the best fit line and defined as 6 r 2 1 i 1 n f r p m i f r p c i 2 i 1 n f r p m i f r p m mean 2 where f r p m f r p c and f r p m mean respectively are measured calculated and mean of measured particle froude number and n is the number of data mape calculates model errors by comparison of the measured and calculated outputs defined as 7 mape 1 n i 1 n f r p c i f r p m i f r p m i 100 the discrepancy ratio dr computes the percentage of calculation within a particular range of measured values and defined as 8 dr f r p c f r p m the underestimation and overestimation of models can be determined when mean of dr is less than or higher than unity respectively in this study dr is clustered in three ranges of 80 120 70 130 and 50 150 it shows the model soundness with 20 30 and 50 unreliability respectively in other words the model computes outputs with less than 20 30 and 50 errors respectively 2 4 adaptive neuro fuzzy inference systems embedded fuzzy c means clustering fcm anfis creating a powerful fuzzy system requires try and error to achieve the best possible performance one of the most popular neuro fuzzy systems for function approximation is anfis jang 1993 the anfis structure includes fuzzy inference system capabilities and neural network adaptability anfis is a way to improve the fuzzy system rules with the help of training algorithms in neural networks compared to the artificial neural network anfis is faster to train due to the configurability of the fuzzy system parameters and also more efficient because it executes a sugeno s fuzzy system in a neural structure mendel 2001 the structure of anfis network with two inputs x and y two rules and one output is presented in fig 1 where the n and π are fixed and adaptive nodes respectively ai and bi are the fuzzy sets of the x and y respectively wi is the weight of the ith rule and wi is the firing strength of ith rule and f is the final output of the anfis the anfis structure includes of five layers as follows layer 1 input nodes in this layer the membership degree of the input nodes is determined by the membership function mf 9 q i 1 μ a i x 10 q i 1 μ b i z y where q is the mf of linguistic variables a and b x and y are the input of the node ai and bi respectively the factors of each node determine the shape of the mf node there are different types of mf e g trapezoidal triangular generalized bell gaussian and etc that could be considered in anfis models the form of triangular and trapezoidal mfs are simple and assembled by linear line indeed the corner of some parameters in these mfs are not adequately smooth similar to these mfs the structure of the gaussian and generalized bell are simple and the corner points in these mfs are sufficiently smooth the gaussian and generalized bell are widely used in different nonlinear complex problems khoshbin et al 2016 azimi et al 2018 gholami et al 2017 moeeni et al 2017 moradi et al 2019 especially sediment transport ebtehaj et al 2018a qasem et al 2017b the mf of the fuzzy sets used in the current study is gaussian mf due to lower parameter of this mf related generalized bell shape which is defined as 11 μ a i x e x c 2 2 σ 2 where x is the input if ith node and s1 c σ is the adaptive factors indeed c and σ are the parameters of the gaussian mf that results in different shape of mf for each input layer 2 rule nodes in this layer each node calculates the degree of activity of a rule 12 q i 2 μ a i x μ b i y i 1 2 where μ a i x and μ b i y are the membership degree of x and y in ai and bi sets respectively layer 3 the nodes in this layer is fix in this layer the ratio of the degree of activity of the ith rule to the activity set of all the rules is computed indeed the firing strength of each rule is calculated as follows 13 o i 3 w i w 1 i 1 n w i where w i is the normalized degree of activity related to ith rule layer 4 consequence rules the output of each rule is calculated in this layer 14 o i 4 w i f i w p i q i r i where s 2 p i q i r i denotes to consequent parameters set layer 5 output node the final output of the anfis network is calculated as 15 o i 5 i 1 n w i f i after presentation of the general structure of the network the training algorithms as well as the fis generation techniques should be determined the nonlinear premise and linear consequents parameters in anfis architecture are modifiable through training stage to find the optimum value of these parameters there is need a training algorithm the training algorithm in anfis network is a backpropagation bp and hybrid the hybrid algorithm is a combination of the bp and least square bp ls the main drawbacks of the bp algorithm in training anfis are tending to trap in local minima and slow convergence rate jang 1993 comparison of these algorithms in sediment transport application ebtehaj and bonakdari 2014 and other real world problems yaseen et al 2017 zhou et al 2019 proved the higher performance of hybrid algorithm in anfis training to generate fuzzy inference systems fis there are three well known techniques sub clustering sc grid partitioning gp and fuzzy c means clustering fcm due to higher performance of fcm in terms of accuracy and simplicity it is employed as fis generation techniques in this study singh and mahajan 2014 ebtehaj and bonakdari 2014 found that the gp is more accurate than sc in sediment transport predicting besides the use of fcm in comparison with gp leads to much less adjustable parameters through training process resulting less computational time and simpler model therefore the fcm is employed as fis generation techniques the number of mfs in this study is found three through trial and error procedure based on the minimum rmse considering higher value decreased the generalizability and enhanced the complexity of the developed model 2 5 extreme learning machine the elm technique huang et al 2004 2006a is a training algorithm for single layer feed forward neural network slffnn in this approach the matrices of input weights and biases are randomly assign while the output weight matrix is approximated analytically by solving a linear based equation this approach is able to model a real world problem with high accuracy and very low training speed before starting the modeling using this method the only parameter to be adjusted is the number of hidden layer neurons so in the modeling practical problems using this method there are not problems of classical training algorithms i e existing different adjustable parameters which is sometimes difficult to find the optimal values indeed the absence of a large number of adjustable parameters leads to a significant reduction in the complexity of modeling and a remarkable increase in the modeling speed the structure of elm model is presented in fig 2 where the x 1 to xn are the input variables aij is the input weight matrix βjk is the output weight matrix g is activation function and b is the bias the mathematical form of slffnn with l hidden neurons is provided as follows huang et al 2006b 16 f l x i 1 l β i g a i b i x x a i b i r n here β i is the weight matrix that connects the ith hidden node to the output node ai and bi are the learning parameters of the slffnn and g a i b i x denotes the output of the ith node due to x as input variable there are several type of activation function af to apply in elm including radial basis function triangular basis function hard limit function sine function and sigmoid function among those afs the sigmoid and sin are infinitely differentiable the comparison of the mentioned afs in different studies proved that the elm with sigmoid af outperforms elm with other afs therefore in the current study the sigmoid is applied as the af of the hidden neurons due to its satisfying performance in different real world problems like runoff prediction wu et al 2009 scour depth prediction ebtehaj et al 2018b and estimation of the darcy weisbach friction factor milukow et al 2019 the mathematical form of the sigmoid activation function is as follows huang et al 2006b 17 sigmoid g a b x 1 1 exp a x b for n arbitrary data pairs as x i t i r n r m so that the ti and xi are matrices with m 1 and n 1 dimensions respectively a slffnn with l hidden nodes that able to approximate n arbitrary samples with zero error is defined as follows 18 f l x j i 1 l β i g a i x j b i j 1 2 n the matrix form of the above mentioned equation is as 19 h β t 20 h a b x h 1 h k g a 1 b 1 x 1 g a l b l x 1 g a 1 b 1 x n g a l b l x n where x x 1 x 2 x l b b 1 b 2 b l a a 1 a 2 a l h is the matrix of hidden layer output so that the ith column of output matrix is the ith hidden node for the inputs x1 xn 21 β β 1 t β l t l m 22 t t 1 t t n t n m t 11 t 1 m t n 1 t nm the elm algorithm as a slffnn with l hidden neurons has the ability to learn l distinct samples with zero error huang et al 2004 2006a if the number of hidden neurons is less than the number of individual samples l n elm randomly allocates different values to hidden layer neurons and computes the output weights matrix by moore penrose generalized inverse mpgi of the matrix h h with a very small error ε 0 the hidden layer neurons ai bi are not adjusted through the training process and are randomly adjusted the following theories illustrate this issue liang et al 2006 theorem i consider a slffnn with l hidden neurons and a differentiable activation function g x therefore using probability distribution the output matrix of the hidden layer is differentiable with probability 1 and h β t 0 theorem ii due to a differentiable activation function g x a small positive value ε 0 and l n there is h n l β l m t n m ε for each generated a i b i i 1 l based on the continuous probability distribution with probability 1 the only thing that is calculated analytically during the training is the output weights which are calculated as follows huang et al 2006a 23 β h t where h is the mpgi of h singh and balasundaram 2007 the number of hidden layer neurons which is found through trial and error process is 30 2 6 gene expression programming gep gep is a component of circulating algorithms ca based on darwin s theory of evolution cas have defined a goal function in terms of quantitative criteria and then used the function to measure and compare different solving methods in a step by step process of data structure correction and eventually provide the appropriate solution method ferreira 2001 gep is developed based on a combination of the genetic algorithm ga and genetic programming gp in this method simple and linear chromosomes with fix length similar to those of ga and expression trees ets with different size and form similar to those of gp are combined together each et consists of terminals set problem variables and functions set main operators the step by step process of gep is as follows 1 an initial population representing prediction models is considered randomly 2 each individual of this population is evaluated using fitness functions 3 in each production stage the following steps are followed to select a new population 3 1 one of the crossover mutation and recombination operators is selected 3 2 the individuals with highest fitness in the current population are selected and send to next generation 3 3 the selected operator is used to generate offspring 3 4 the new child enters into a new population 3 5 the model is evaluated using fitness function 4 step three will be repeated until the maximum number of production is reached at the beginning process of gep technique no functional relationship is considered and this method is able to optimize the model structure and its components the gep flowchart is presented in fig 3 an example of et crossover and mutation in gep is provided in fig 4 in fig 4 by considering h 1 h 2 and h 3 as input variables an example of the expression tree et that indicates the results of the gep model in predicting of the particle froude number is presented as fig 4a the mathematical expression related to this program indicated in fig 4a is as h 2 h 3 0 5 3 h 1 given the four stages of the gep process the solving procedure of the program shown in fig 4a is as follows generation production a population of ets is initially formed and the genetic operators act on them so that individual of the population can be produced with the help of terminal set t h 1 h 2 h 3 3 and function set f mutation a sub et is randomly assigned to another sub et fig 4b in fig 4b the right side of the et was selected to mutate it indeed the use of the mutation operator had changed the 5 h 2 h 1 h 2 h 3 in the form of 2 h 1 h 2 h 3 crossover within the current program two random nodes parent are selected and then replaced by two sub et thus obtaining a new program fig 4c due to provided example in fig 4c a sub et in the right side of the parent 2 was changed with a sub et in the left side of the parent 1 by crossover operator indeed the parents 1 2 which are presented as 5 h 2 h 1 h 2 h 3 and 2 h 1 h 1 h 3 respectively are provided as child 1 h 3 h 2 h 3 and child 2 2 h 1 h 2 5 h 2 h 1 respectively reproduction this step means reprogramming the program and this is done if the termination criteria is not verified the gep method is a new version of genetic programming that deals with the inference of computer programs with different sizes and shapes one of the strengths of gep is that the genetic diversity criterion is very simple and therefore genetic operators act on the chromosomal level another strength of this method is the unique nature of multi genetic that provides a framework for evaluating complex models that include several sub models borelli et al 2006 in order to use gep based modeling to predict frp first the data must be divided into two categories of training and testing so that the training set is employed for modeling and testing one is considered to check the flexibility of the model for samples which did not have any role in the modeling the general setting in gep includes n umber of chromosomes head size number of genes and linking functions in order to determine the number of chromosomes this parameter was considered in the range of 30 100 ferreira 2001 considering different numbers in this rang as 30 35 40 100 showed that the best performance is achieved with the use of 100 chromosomes in addition the value of this parameter was increased to 150 and it was observed that the modeling results did not change significantly and only the modeling time would increase to select the head size with all other parameters remaining constant the value of this parameter was considered as 3 10 which was the best result in the use of 8 head size in addition the number of genes was evaluated as 1 2 3 4 5 6 7 and considering the model accuracy and complexity simultaneously the value of this parameter was selected as 5 the last parameter in the general setting is linking function which has four options including addition multiplication subtraction and division modeling results prove the higher performance of gep based model with considering addition as linking function in compared with other linking functions the individuals are assessed and the fitness function is computed as follows 24 f i 1000 1 e i the ei could be considered as different indices based on the absolute error relative error and correlation coefficient the absolute error based indices are including of mean squared error mse root mean squared error rmse mean absolute error mae relative squared error rse root relative squared error rrse and relative absolute error rae the relative based indices are including relative mse rmse relative rmse rrmse relative mae rmae relative rse rrse relative rrse rrrse and relative rae rrae besides the correlation based coefficient indices are including r square and correlation coefficient to find the best fitness function the performance of the gep was evaluated for each fitness function so that other adjustable variables such as functions general setting and generic operators were similar in all models the results indicated the gep with rrse as fitness function outperforms gep with other fitness functions moreover the recent hydrological and hydraulic studies khozani et al 2017 gholami et al 2018 milukow et al 2019 also proved the successful performance of the gep with rrse as fitness function the rrse is defined as follows 25 rrse e i j 1 n p ij t j 2 j 1 n t j t 2 where tj is the actual value for fitness case j t is the mean of tj and pij is the estimated value by individual chromosome i for fitness case j other things that need to be determined prior to modeling are the function set and the genetic operators the optimum values of these parameters obtained using trial and error are presented in table 2 2 7 generalized structure of group method of data handling group method of data handling gmdh is a self organized approach so that gradually produces more complex models during the performance evaluation of input and output data sets ivakhnenko 1968 in the gmdh method the relation between the input and output variables is expressed by the series of volterra functions which is similar to the kolmogorov gabor discrete polynomial ivakhnenko 1968 26 y w 0 i 1 n w i x i i 1 n j 1 n w ij x i x j i 1 n j 1 n k 1 n w ijk x i x j x k where y is the output variables w w 0 w 1 wm is weights vector and x x 1 x 2 xm is the vector of input variables the gmdh model has been developed based on heuristic self organization to address the complexity of multi dimensional problems this method first considers the variables as data pairs xi 1 xi r 1 2 n and then specifies a threshold value for evaluating the variables that cannot reach the level of performance this procedure describes a self organized algorithm in this research the gmdh model is made according to the following steps and shown in fig 5 step 1 divide the data into training and testing stages the main data is divided into two stages of training and testing so that training data is used to estimate some features of the nonlinear system and the test data set is used to validate the performance of the proposed model step 2 create a pairs of variables xi 1 xi r 1 2 n in each layer all of variables pairs from n input variables are constructed for the desired layer the number of these pairs is equal to 27 c n m n r m r where n is the number of input variables and r is the coefficient that is considered 2 at the classic gmdh method given the number of neurons in each layer there are the same number of z z 1 z 2 zm 28 z m w 0 w 1 x 1 w 2 x 2 w 3 x 1 2 w 4 x 2 2 w 4 x 1 x 2 the above equation known as the transfer function is a comprehensive equation for this method step 3 estimate the weights in each layer the least square error method is employed to estimate coefficient of the transfer function which are as follows 29 w i x i t x i 1 x i y obs where w w 0 w 1 w 5 is the weight vector yobs yobs1 yobs2 yobsm t denotes the outputs values of the observed samples and x is the input data pairs step 4 optimize the elements in each layer the partial optimal expression of a nonlinear system is done by regression analysis into training data in this step the best variables and their pairs are identified and selected on each layer after calculating the output of zi in order to determine the structure of the network it is necessary to use an external criterion for the selection of the neurons that describe the purpose this criterion is calculated as follows 30 δ p 2 i 1 n v p v p i 1 n v o v o where vo and vp denote observed and estimated respectively value and the v o and v p are the mean of the observed and estimated respectively value step 5 stop criteria to create multi layer structure by comparing the value of the current layer index with the next layer to be created it will stop generating subsequent layers if the index value is not improved or less than the default value otherwise the second to fourth steps repeat until the precision of the predicted values is reached after the steps are completed a smart model with a multi layered structure is built that has ability to predict the complex process of sediment transport modeling although the gmdh method has different benefits it also has some limitations the limitations of modeling nonlinear problems with the classical gmdh method include the following i the maximum number of inputs in each neuron is two ii considering a polynomial with order 2 iii select the neurons only from adjacent layer taking into account the advantages offered by the classical gmdh method in order to overcome the expressed shortcomings a gmdh based method which has a different structure than the classical gmdh is presented as the generalized structure of gmdh gs gmdh in the proposed method based on gmdh the developed model structure can be a second order polynomial or three additionally input neurons can also be 2 or 3 in addition to changing the order and number of neurons in order to avoid the significant complexity of the model in the classical gmdh method the inputs of each neuron can also be selected from non adjacent layers to select the optimal structure in the gs gmdh model the akaike information criterion aic ebtehaj et al 2014 is used the aic consists of two terms so that the first term n log 1 nσi 1 n yi yi 2 is provided to evaluate the performance of the proposed model in term of accuracy while the second one 2 k is provided to control the complexity of the proposed model by gs gmdh failure to consider the second term to complexity control of the proposed model leads to a model with a large number of neurons in different layers therefore the uncertainty of the model may be significantly increased and the use of this model in the practical application will be difficult in addition to the precision of a model a practical tool should be as simple as possible consequently aic could be a good choice to apply in gs gmdh model as a fitness function fig 6 shows the structure of the proposed model for predicting of frp using the gs gmdh model it is observed that the neurons used in the latter layer are taken from non adjacent layer 31 aic n log 1 n i 1 n y i y i 2 2 k where y and y are the predicted and observed samples n denotes to number of samples and k is the number adjustable parameters in developed model the flowchart of the gs gmdh is provided in fig 5 where the cv is the sediment volumetric concentration r is the hydraulic radius d is the sediment median size dgr d g s 1 ν 2 1 3 is the dimensionless grain size parameter and λ is the channel friction factor the four mentioned dimensionless variables cv d r dgr λ are the independent input variables 3 discussion performance of gep elm gs gmdh and anfis fcm machine learning models developed in this study are compared with three well known conventional regression models of mayerle et al 1991 ab ghani 1993 and vongvisessomjai et al 2010 because of their validity as reported in safari et al 2018 the performance of models are investigated in terms of three statistical indices of coefficient of determination r 2 mean absolute percentage error mape and discrepancy ratio dr in tables 3 and 4 comparison of measured and calculated particle froude numbers frp is shown in fig 7 it is seen from table 3 that machine learning models provide better performance than regression models although gep elm gs gmdh and anfis fcm models have almost similar performances gs gmdh is found superior to the other models with r 2 and mape of 0 86 and 14 88 respectively it is found from table 3 that hybrid models of gs gmdh and anfis fcm outperform individual models of gep and elm despite ab ghani 1993 and vongvisessomjai et al 2010 models provide better results than mayerle et al 1991 model their computation accuracy are not as high as machine leaning techniques comparison of models in table 4 in terms of dr indicates that machine learning models highly perform in computing of the particle froude number with mean of the dr close to unity indicating that the gep elm gs gmdh and anfis fcm models outputs fits well to the measured data although a slight overestimation is observed for all machine learning models a significant overestimation is seen for mayerle et al 1991 regression model in the percentage of data within the range of 0 80 1 20 of measured data ab ghani 1993 and vongvisessomjai et al 2010 conventional regression models give results close to the machine learning techniques however for wide ranges of dr higher accuracy of machine leaning techniques should be noticed table 4 shows that hybrid models of gs gmdh and anfis fcm generate much better results in comparison with gep elm and regression models in terms of dr the goodness of fit is studied by comparison of measured and computed particle froude numbers in fig 7 particle froude numbers computed by machine learning techniques match their measured counterparts although a small scatter is observed for elm result conventional regression models considerably overestimate particle froude number with wide scatter which is more tangible for mayerle et al 1991 model among conventional regression models ab ghani 1993 and vongvisessomjai et al 2010 give almost acceptable results however their accuracy are not competitive to the machine leaning techniques it should be noticed that over fitting is an important problem in machine learning modeling safari et al 2016 fathian et al 2019 it is known that well organized models through using lower number of weights or input parameters prevent over fitting problem chang et al 2010 over fitted models fit training events too exactly and generates poor performance on unseen data set at testing step bishop 1995 however the models developed in this study are not expected to have over fitting problem as they generate acceptable results at testing stage moreover they are established on large number of data and input parameters are selected based on the sediment transport hydraulics in the modeling procedure the analysis given above indicated that developed gep elm gs gmdh and anfis fcm models in this study outperform selected conventional regression models in the literature additional to the application of the powerful machine learning techniques for sediment transport modeling in rigid boundary open channels ranges of experimental data used for the modeling and selection of the most effective parameters are of importance as extension of the authors previous works this study works out the existing deficiencies in the machine learning modeling of sediment transport in rigid boundary channels in order to manage the results several studies in the literature omitted some effective parameters in the modeling procedure while in this study four fundamental characteristics of flow fluid sediment and channel are incorporated in the model performance of data driven models is related to the ranges of experimental data most of the studies in the literature used limited number of experimental data for establishing of sediment transport models in rigid boundary open channels however this study utilizes data from four different sources with wide ranges of pipe and sediment size consequently in this study it seems that sediment transport is successfully modeled using machine learning techniques the matlab code for calculating of frp using developed model fcm anfis gep elm and gs gmdh is presented in table 5 4 conclusions machine learning techniques of gep elm gs gmdh and anfis fcm are used for modeling of sediment transport in rigid boundary open channels laboratory experimental data with wide ranges of pipes size sediment size and sediment volumetric concentration are used to establish models for wide ranges of applicability similar to the conventional regression models in the literature this study considered well known dimensionless parameters to show effective parameters in sediment transport in open channel flow to this end fluid flow sediment and channel characteristics are incorporated to develop machine learning models according to the results obtained in this study it is found that gep elm gs gmdh and anfis fcm outperform conventional regression models in particle froude number estimation hybrid models of gs gmdh and anfis fcm perform better than gep and elm where gs gmdh model is found superior to all other models in terms of different statistical performance indices limitations of this study and suggested improvements can be linked to the use of experimental data in the modeling as the ranges of experimental data has great effect on the credibility of machine learning models to this end collecting field data of rigid boundary open channels from sewers and urban drainage systems would be quite valuable to establish practical tool for designing rigid boundary channels such as sewers and drainage systems on the other hand available non deposition experimental data were mostly collected from circular channels therefore performing new set of experiments in variety of channel cross section shapes will be helpful to incorporate channel cross section shape factor in modeling of sediment transport in rigid boundary open channels using machine learning techniques declaration of competing interest the authors declare that they have no known competing financial interests acknowledgments the authors would like to express sincerest appreciation to prof geoff syme editor in chief associate editor and two anonymous reviewers for their highly insightful comments that improved the quality of this manuscript 
6211,sediment transport in open channels has complicated nature and finding the analytical models applicable for channel design in practice is a quite difficult task to this end behind theoretical consideration of the open channel sediment transport through incorporating of four fundamental characteristics of fluid flow sediment and channel recently machine learning techniques are used for modeling of sediment transport in open channels however most of the studies in the literature used limited number of data for model development neglecting some effective parameters involved which may affect their performances moreover most of this studies had not provided a comprehensive explicit equation for future use accordingly this study applied four machine learning techniques of gene expression programming gep extreme learning machine elm generalized structure group method of data handling gs gmdh and fuzzy c means based adaptive neuro fuzzy inference system fcm anfis to model sediment transport in open channels four existing data sets in the literature with wide ranges of pipe size sediment size sediment volumetric concentration channel bed slope and flow depth are used for the model development the recommended models are compared with their corresponding conventional regression models taken from the literature in terms of different statistical performance indices results indicate superiority of the machine leaning techniques to the conventional multiple non linear regression models although developed gep elm gs gmdh and fcm anfis models have almost same performances gs gmdh gives slightly better performance which can be linked to the generalized structure of this approach a matlab code is provided to calculate the sediment transport in open channel for practical engineering keywords extreme learning machine fuzzy c means based adaptive neuro fuzzy inference system gene expression programming generalized structure of group method of data handling rigid boundary channel sediment transport 1 introduction linked to the urban hydrology modeling of sediment transport in rigid boundary open channels is of importance for designing of urban drainage systems irrigation canals and generally all types of lined channels sediment transport in alluvial channels has been studied to a great extent fakhri et al 2014 dalezios et al 2018 and its basic technology may be found in the well known textbooks such as raudkivi 1990 and vanoni 2006 there are variety of sediment transport conditions which are essential in studying of sediment transport in rigid boundary channels such as incipient motion incipient deposition and non deposition among those non deposition condition is mostly considered for design of rigid boundary channels to prevent permanent deposition of sediment within the flow examples of non deposition sediment transport studies in rigid boundary open channels are given herein novak and nalluri 1975 evaluated the loose boundary sediment transport models on rigid boundary channel experimental data and found out that shear stress and velocity required to retain sediment particles in motion are lower for the case of rigid boundary channels macke 1982 and arora 1983 studied suspended sediment transport at non deposition condition considering more hydraulic parameters that their results were re analyzed by nalluri and spaliviero 1998 may et al 1989 and may 1993 studied bed load sediment transport considering shear stress acting on the bad layer through analyzing hydrodynamic forces acting on sediment particle at the channel bottom ackers 1984 modified ackers and white 1973 sediment transport model for applying in rigid boundary channels by considering effective bed width mayerle 1988 and mayerle et al 1991 conducted experiments in circular and rectangular channels and evaluated formerly collected experimental data of novak and nalluri 1984 ab ghani 1993 carried out comprehensive experimental study on the bed load sediment transport in rigid boundary open channels to find the effect of pipe size at non deposition condition it is reported that larger channels require higher design velocity may et al 1996 butler et al 2003 and de sutter et al 2003 evaluated the applicability of formerly developed models to use in practice for sewer pipe design ota 1999 and ota and perrusquia 2013 performed set of experiments and analyzed own data considering much more hydraulic parameters such as sediment particle shape factor and angle of repose at non deposition condition of sediment transport vongvisessomjai et al 2010 conducted experiments in two pipes to study the bed load and suspended load sediment transport and evaluated the conventional camp criterion for drainage system design and found out that it overestimates design flow velocity safari et al 2017a carried out experiments in a trapezoidal cross section channel and introduced channel cross section shape factor considering flow resistance in open channel flow a model was developed and its applicability on different cross section channels was evaluated using large number of experimental data it is worthy to mention that aforementioned studies suggested sediment transport models using multiple non linear regression analysis details of aforementioned studies are given in safari et al 2018 examples of non deposition bed load sediment transport models in rigid boundary channels are listed in table 1 as shown in table 1 most of the studies in the literature used particle froude number frp as dependent variable defined as 1 f r p v gd s 1 where v is flow mean velocity d sediment median size g gravitational acceleration and s relative specific mass of sediment ρs to fluid ρ as presented in safari et al 2015 2016 frp can be obtained by analyzing the hydrodynamic forces acting on the sediment particle and also through combination of the well known shields 1936 parameter with manning formula generally frp is selected as dependent variable and fluid flow sediment and channel characteristics are considered as independent variables through the modeling process recently machine learning techniques have been widely applied on variety of water resources engineering problems chau 2017 alizadeh et al 2018 shamshirband et al 2019 yaseen et al 2018 because of complicated nature of sediment transport in open channel flow machine learning techniques have attracted interest of many researchers olyaie et al 2015 chen and chau 2016 various machine leaning techniques were used for modeling of sediment transport in rigid boundary open channels ab ghani and azamathulla 2010 and azamathulla et al 2012 applied gene expression programming gep and adaptive neuro fuzzy inference systems anfis techniques and developed models similar to the conventional regression models available in the literature however the suggested model performance was found superior to their counterparts qasem et al 2017a used particle swarm optimization pso for determination of the radial basis function neural network rbf parameters and evaluated different combination of input parameters to find most effective input parameters in sediment transport in rigid boundary channels different data driven techniques have been used for modeling of sediment transport in rigid boundary channels such as wavelet support vector machines svm by ebtehaj et al 2015 neuro fuzzy based group method of data handling nf gmdh by najafzadeh and bonakdari 2016 decision tree artificial neural network dt ann by ebtehaj et al 2016a combination of svm with firefly algorithm by ebtehaj et al 2016b evolutionary polynomial regression epr by najafzadeh et al 2017 extreme learning machine elm and support vector machines svm by roushangar and ghasempour 2017a combination of evolutionary algorithm with ann by ebtehaj and bonakdari 2016 and roushangar and ghasempour 2017b and multigene genetic programming mggp by safari and danandeh mehr 2018 safari et al 2016 and wan mohtar et al 2018 evaluated applicability of different artificial neural network ann techniques for modeling sediment threshold in sewer pipes it was concluded that appropriately constructed ann models can compute flow mean velocity precisely safari 2019 modelled sediment transport in sewer pipes using decision tree dt and multivariate adaptive regression splines mars techniques dt found channel design velocity of 0 6 m s similar to sewer design standards in many countries it is concluded that mars outperforms dt generalized regression neural network gr and conventional multiple non linear regression models safari et al 2017b and safari and shirzad 2019 applied particle swarm optimization pso for studying sediment transport in large sewers safari et al 2017b made a modification on well known may 1993 model the conditional equations of may 1993 was unified in a single equation and its applicability was demonstrated through evaluation of the developed equation performance on different data sets safari and shirzad 2019 defined optimum deposited bed thickness for large sewer pipes and recommended design charts for variety of pipe size and flow depths the optimum deposited bed thickness of 1 of pipe diameter was suggested for large sewer design it has to be emphasized that three factors of utilized machine learning technique ranges of experimental data used and selection of effective parameters involved must be considered to develop a robust data driven model this study is the extension of the authors previous works to enhance the modeling of sediment transport in rigid boundary open channels through considering aforesaid factors as powerful machine learning techniques gep elm gs gmdh and fcm anfis are applied for modeling most of the studies in the literature utilized two experimental data sets for modeling while this study used four experimental data sets covering wide ranges of pipe size sediment size and concentration flow depth and channel bed slope several studies in the literature neglected some effective parameters to simplify and increase the model performance while this study considered four fundamental characteristics of flow fluid sediment and channel for modeling of sediment transport in rigid boundary open channels 2 materials and methods 2 1 experimental data in order to model sediment transport in rigid boundary channels four data sets of mayerle 1988 may 1993 ab ghani 1993 and vongvisessomjai et al 2010 are used in this study experimental conditions and procedures are explained briefly as follows mayerle 1988 conducted experiments in two different cross section channels as rectangular and circular two rectangular channels with glass walls one with a width of 311 mm and a length of 12 2 m and another with a width of 462 mm and 12 6 m length with a smooth and rough bed and a circular pvc channel with a diameter of 152 mm and a length of 20 5 m with a smooth bed were used in the experiments sediment particles with six different sizes 0 5 mm 1 05 mm 1 95 mm 2 56 mm 5 22 mm and 8 74 mm were used in the experiments an electromagnetic vibrator system was installed as a sediment feeder at the beginning of the channel mayerle 1988 defined non deposition without deposition condition as limit of deposition in which the flow conveys the maximum amount of sediment without deposition in such a situation the flow velocity in which sediment particles move under the influence of flow forces are defined as self cleansing velocity through the experiments firstly uniform flow was established then sediment deposition condition was reached by increasing the sediment feeder discharge sediment discharge was determined by measuring the weight of dried sediment before and after of each experiment the depth of the flow was measured at each meter of the channel before and during the test to determine the effect of sediment deposition on the flow may 1993 performed experiments in a concrete circular channel having 450 mm diameter and 21 3 m length sediment was supplied by mixing of water with sediment and then pumped into the channel the clean water flow discharge was measured by a weir installed at the upstream of the channel and channel discharge depth relationship was determined sand size of 0 73 mm was used in the experiments for ensuring the uniform flow condition flow depth was measured at five points of the channel behind achieving the equilibrium condition of sediment deposition hydraulic characteristics as flow depth and sediment discharge were recorded ab ghani 1993 carried out experiments in three circular channels two pvc channels with a diameter of 154 mm a length of 20 5 m and another with a diameter of 305 mm and a length of 20 5 m and a concrete channel with a diameter of 405 mm and a length of 21 3 m were used in the experiments measurements were done at seven points in the observation section of the channel a tailgate at the downstream of the channel was used to adjust the flow depth to establish a uniform flow a triangular weir was used at the downstream of the channel to measure the flow discharge through the experiments in channels with diameters of 154 mm and 305 mm six sediment sizes of 0 46 mm 0 97 mm 2 mm 4 2 mm 5 7 mm and 8 3 mm and in channel with diameter of 405 mm sediment size of 0 72 mm were used the amount of sediment entered into the channel was adjusted using an electromagnetic vibrator system after establishing a uniform flow deposition condition was reached by increasing the sediment feeder discharge the sediment discharge was measured by weighing the accumulated sediments over a given time period the sediment was collected in channel downstream dried and reused for the next experiments vongvisessomjai et al 2010 carried out experiments in two circular pvc channels with diameter of 150 mm and 100 mm and both in length of 16 m three types of uniform sand with sizes of 0 2 mm 0 3 mm and 0 43 mm were used in the experiments the depth of flow across the channel was measured at several points to create a uniform flow the tailgate at the channel downstream was adjusted and accordingly the flow depth remained constant a sediment feeder was used for supplying the sediment within the flow through increasing the sediment feeder discharge sediment deposition condition was achieved 2 2 model parameters it is reported by robinson and graf 1972 that a sediment transport model should incorporate four fundamental flow fluid sediment and channel characteristics the effective variables of sediment transport in rigid boundary channels respecting the physically based models available in the literature can be considered as flow mean velocity v hydraulic radius r gravitational acceleration g fluid specific mass ρ and kinematic viscosity ν sediment median size d sediment volumetric concentration cv sediment specific mass ρs and channel friction factor λ among those v r and g are flow characteristics ρ and ν as fluid d cv and ρs as sediment and λ as channel characteristic are considered regarding the sediment transport models in rigid boundary channels aforementioned variables can be written as group of dimensionless parameters as follows 5 v gd s 1 f c v d gr d r λ experimental data are re formed based on the above equation to prepare the inputs and output of the model it is seen that left hand side of eq 5 as output of the model is particle froude number and parameters given at the right hand side are inputs of the model the prepared data are split in two parts for training and testing data sets through the training stage the unknown relationship of input and output parameters is determined and in the testing stage the performance of the developed model is investigated on unseen data set to this extent among 375 data 300 data for training and 75 data are randomly selected to test the models before the modeling data are re scaled to values between 0 1 to behave equally in the modeling procedure 2 3 statistical performance indices the performances of the models must be investigated for understanding their credibility hence three statistical performance indices of coefficient of determination r 2 mean absolute percentage error mape and discrepancy ratio dr are used in this study the r 2 is a statistical measure of how the results are fitted to the best fit line and defined as 6 r 2 1 i 1 n f r p m i f r p c i 2 i 1 n f r p m i f r p m mean 2 where f r p m f r p c and f r p m mean respectively are measured calculated and mean of measured particle froude number and n is the number of data mape calculates model errors by comparison of the measured and calculated outputs defined as 7 mape 1 n i 1 n f r p c i f r p m i f r p m i 100 the discrepancy ratio dr computes the percentage of calculation within a particular range of measured values and defined as 8 dr f r p c f r p m the underestimation and overestimation of models can be determined when mean of dr is less than or higher than unity respectively in this study dr is clustered in three ranges of 80 120 70 130 and 50 150 it shows the model soundness with 20 30 and 50 unreliability respectively in other words the model computes outputs with less than 20 30 and 50 errors respectively 2 4 adaptive neuro fuzzy inference systems embedded fuzzy c means clustering fcm anfis creating a powerful fuzzy system requires try and error to achieve the best possible performance one of the most popular neuro fuzzy systems for function approximation is anfis jang 1993 the anfis structure includes fuzzy inference system capabilities and neural network adaptability anfis is a way to improve the fuzzy system rules with the help of training algorithms in neural networks compared to the artificial neural network anfis is faster to train due to the configurability of the fuzzy system parameters and also more efficient because it executes a sugeno s fuzzy system in a neural structure mendel 2001 the structure of anfis network with two inputs x and y two rules and one output is presented in fig 1 where the n and π are fixed and adaptive nodes respectively ai and bi are the fuzzy sets of the x and y respectively wi is the weight of the ith rule and wi is the firing strength of ith rule and f is the final output of the anfis the anfis structure includes of five layers as follows layer 1 input nodes in this layer the membership degree of the input nodes is determined by the membership function mf 9 q i 1 μ a i x 10 q i 1 μ b i z y where q is the mf of linguistic variables a and b x and y are the input of the node ai and bi respectively the factors of each node determine the shape of the mf node there are different types of mf e g trapezoidal triangular generalized bell gaussian and etc that could be considered in anfis models the form of triangular and trapezoidal mfs are simple and assembled by linear line indeed the corner of some parameters in these mfs are not adequately smooth similar to these mfs the structure of the gaussian and generalized bell are simple and the corner points in these mfs are sufficiently smooth the gaussian and generalized bell are widely used in different nonlinear complex problems khoshbin et al 2016 azimi et al 2018 gholami et al 2017 moeeni et al 2017 moradi et al 2019 especially sediment transport ebtehaj et al 2018a qasem et al 2017b the mf of the fuzzy sets used in the current study is gaussian mf due to lower parameter of this mf related generalized bell shape which is defined as 11 μ a i x e x c 2 2 σ 2 where x is the input if ith node and s1 c σ is the adaptive factors indeed c and σ are the parameters of the gaussian mf that results in different shape of mf for each input layer 2 rule nodes in this layer each node calculates the degree of activity of a rule 12 q i 2 μ a i x μ b i y i 1 2 where μ a i x and μ b i y are the membership degree of x and y in ai and bi sets respectively layer 3 the nodes in this layer is fix in this layer the ratio of the degree of activity of the ith rule to the activity set of all the rules is computed indeed the firing strength of each rule is calculated as follows 13 o i 3 w i w 1 i 1 n w i where w i is the normalized degree of activity related to ith rule layer 4 consequence rules the output of each rule is calculated in this layer 14 o i 4 w i f i w p i q i r i where s 2 p i q i r i denotes to consequent parameters set layer 5 output node the final output of the anfis network is calculated as 15 o i 5 i 1 n w i f i after presentation of the general structure of the network the training algorithms as well as the fis generation techniques should be determined the nonlinear premise and linear consequents parameters in anfis architecture are modifiable through training stage to find the optimum value of these parameters there is need a training algorithm the training algorithm in anfis network is a backpropagation bp and hybrid the hybrid algorithm is a combination of the bp and least square bp ls the main drawbacks of the bp algorithm in training anfis are tending to trap in local minima and slow convergence rate jang 1993 comparison of these algorithms in sediment transport application ebtehaj and bonakdari 2014 and other real world problems yaseen et al 2017 zhou et al 2019 proved the higher performance of hybrid algorithm in anfis training to generate fuzzy inference systems fis there are three well known techniques sub clustering sc grid partitioning gp and fuzzy c means clustering fcm due to higher performance of fcm in terms of accuracy and simplicity it is employed as fis generation techniques in this study singh and mahajan 2014 ebtehaj and bonakdari 2014 found that the gp is more accurate than sc in sediment transport predicting besides the use of fcm in comparison with gp leads to much less adjustable parameters through training process resulting less computational time and simpler model therefore the fcm is employed as fis generation techniques the number of mfs in this study is found three through trial and error procedure based on the minimum rmse considering higher value decreased the generalizability and enhanced the complexity of the developed model 2 5 extreme learning machine the elm technique huang et al 2004 2006a is a training algorithm for single layer feed forward neural network slffnn in this approach the matrices of input weights and biases are randomly assign while the output weight matrix is approximated analytically by solving a linear based equation this approach is able to model a real world problem with high accuracy and very low training speed before starting the modeling using this method the only parameter to be adjusted is the number of hidden layer neurons so in the modeling practical problems using this method there are not problems of classical training algorithms i e existing different adjustable parameters which is sometimes difficult to find the optimal values indeed the absence of a large number of adjustable parameters leads to a significant reduction in the complexity of modeling and a remarkable increase in the modeling speed the structure of elm model is presented in fig 2 where the x 1 to xn are the input variables aij is the input weight matrix βjk is the output weight matrix g is activation function and b is the bias the mathematical form of slffnn with l hidden neurons is provided as follows huang et al 2006b 16 f l x i 1 l β i g a i b i x x a i b i r n here β i is the weight matrix that connects the ith hidden node to the output node ai and bi are the learning parameters of the slffnn and g a i b i x denotes the output of the ith node due to x as input variable there are several type of activation function af to apply in elm including radial basis function triangular basis function hard limit function sine function and sigmoid function among those afs the sigmoid and sin are infinitely differentiable the comparison of the mentioned afs in different studies proved that the elm with sigmoid af outperforms elm with other afs therefore in the current study the sigmoid is applied as the af of the hidden neurons due to its satisfying performance in different real world problems like runoff prediction wu et al 2009 scour depth prediction ebtehaj et al 2018b and estimation of the darcy weisbach friction factor milukow et al 2019 the mathematical form of the sigmoid activation function is as follows huang et al 2006b 17 sigmoid g a b x 1 1 exp a x b for n arbitrary data pairs as x i t i r n r m so that the ti and xi are matrices with m 1 and n 1 dimensions respectively a slffnn with l hidden nodes that able to approximate n arbitrary samples with zero error is defined as follows 18 f l x j i 1 l β i g a i x j b i j 1 2 n the matrix form of the above mentioned equation is as 19 h β t 20 h a b x h 1 h k g a 1 b 1 x 1 g a l b l x 1 g a 1 b 1 x n g a l b l x n where x x 1 x 2 x l b b 1 b 2 b l a a 1 a 2 a l h is the matrix of hidden layer output so that the ith column of output matrix is the ith hidden node for the inputs x1 xn 21 β β 1 t β l t l m 22 t t 1 t t n t n m t 11 t 1 m t n 1 t nm the elm algorithm as a slffnn with l hidden neurons has the ability to learn l distinct samples with zero error huang et al 2004 2006a if the number of hidden neurons is less than the number of individual samples l n elm randomly allocates different values to hidden layer neurons and computes the output weights matrix by moore penrose generalized inverse mpgi of the matrix h h with a very small error ε 0 the hidden layer neurons ai bi are not adjusted through the training process and are randomly adjusted the following theories illustrate this issue liang et al 2006 theorem i consider a slffnn with l hidden neurons and a differentiable activation function g x therefore using probability distribution the output matrix of the hidden layer is differentiable with probability 1 and h β t 0 theorem ii due to a differentiable activation function g x a small positive value ε 0 and l n there is h n l β l m t n m ε for each generated a i b i i 1 l based on the continuous probability distribution with probability 1 the only thing that is calculated analytically during the training is the output weights which are calculated as follows huang et al 2006a 23 β h t where h is the mpgi of h singh and balasundaram 2007 the number of hidden layer neurons which is found through trial and error process is 30 2 6 gene expression programming gep gep is a component of circulating algorithms ca based on darwin s theory of evolution cas have defined a goal function in terms of quantitative criteria and then used the function to measure and compare different solving methods in a step by step process of data structure correction and eventually provide the appropriate solution method ferreira 2001 gep is developed based on a combination of the genetic algorithm ga and genetic programming gp in this method simple and linear chromosomes with fix length similar to those of ga and expression trees ets with different size and form similar to those of gp are combined together each et consists of terminals set problem variables and functions set main operators the step by step process of gep is as follows 1 an initial population representing prediction models is considered randomly 2 each individual of this population is evaluated using fitness functions 3 in each production stage the following steps are followed to select a new population 3 1 one of the crossover mutation and recombination operators is selected 3 2 the individuals with highest fitness in the current population are selected and send to next generation 3 3 the selected operator is used to generate offspring 3 4 the new child enters into a new population 3 5 the model is evaluated using fitness function 4 step three will be repeated until the maximum number of production is reached at the beginning process of gep technique no functional relationship is considered and this method is able to optimize the model structure and its components the gep flowchart is presented in fig 3 an example of et crossover and mutation in gep is provided in fig 4 in fig 4 by considering h 1 h 2 and h 3 as input variables an example of the expression tree et that indicates the results of the gep model in predicting of the particle froude number is presented as fig 4a the mathematical expression related to this program indicated in fig 4a is as h 2 h 3 0 5 3 h 1 given the four stages of the gep process the solving procedure of the program shown in fig 4a is as follows generation production a population of ets is initially formed and the genetic operators act on them so that individual of the population can be produced with the help of terminal set t h 1 h 2 h 3 3 and function set f mutation a sub et is randomly assigned to another sub et fig 4b in fig 4b the right side of the et was selected to mutate it indeed the use of the mutation operator had changed the 5 h 2 h 1 h 2 h 3 in the form of 2 h 1 h 2 h 3 crossover within the current program two random nodes parent are selected and then replaced by two sub et thus obtaining a new program fig 4c due to provided example in fig 4c a sub et in the right side of the parent 2 was changed with a sub et in the left side of the parent 1 by crossover operator indeed the parents 1 2 which are presented as 5 h 2 h 1 h 2 h 3 and 2 h 1 h 1 h 3 respectively are provided as child 1 h 3 h 2 h 3 and child 2 2 h 1 h 2 5 h 2 h 1 respectively reproduction this step means reprogramming the program and this is done if the termination criteria is not verified the gep method is a new version of genetic programming that deals with the inference of computer programs with different sizes and shapes one of the strengths of gep is that the genetic diversity criterion is very simple and therefore genetic operators act on the chromosomal level another strength of this method is the unique nature of multi genetic that provides a framework for evaluating complex models that include several sub models borelli et al 2006 in order to use gep based modeling to predict frp first the data must be divided into two categories of training and testing so that the training set is employed for modeling and testing one is considered to check the flexibility of the model for samples which did not have any role in the modeling the general setting in gep includes n umber of chromosomes head size number of genes and linking functions in order to determine the number of chromosomes this parameter was considered in the range of 30 100 ferreira 2001 considering different numbers in this rang as 30 35 40 100 showed that the best performance is achieved with the use of 100 chromosomes in addition the value of this parameter was increased to 150 and it was observed that the modeling results did not change significantly and only the modeling time would increase to select the head size with all other parameters remaining constant the value of this parameter was considered as 3 10 which was the best result in the use of 8 head size in addition the number of genes was evaluated as 1 2 3 4 5 6 7 and considering the model accuracy and complexity simultaneously the value of this parameter was selected as 5 the last parameter in the general setting is linking function which has four options including addition multiplication subtraction and division modeling results prove the higher performance of gep based model with considering addition as linking function in compared with other linking functions the individuals are assessed and the fitness function is computed as follows 24 f i 1000 1 e i the ei could be considered as different indices based on the absolute error relative error and correlation coefficient the absolute error based indices are including of mean squared error mse root mean squared error rmse mean absolute error mae relative squared error rse root relative squared error rrse and relative absolute error rae the relative based indices are including relative mse rmse relative rmse rrmse relative mae rmae relative rse rrse relative rrse rrrse and relative rae rrae besides the correlation based coefficient indices are including r square and correlation coefficient to find the best fitness function the performance of the gep was evaluated for each fitness function so that other adjustable variables such as functions general setting and generic operators were similar in all models the results indicated the gep with rrse as fitness function outperforms gep with other fitness functions moreover the recent hydrological and hydraulic studies khozani et al 2017 gholami et al 2018 milukow et al 2019 also proved the successful performance of the gep with rrse as fitness function the rrse is defined as follows 25 rrse e i j 1 n p ij t j 2 j 1 n t j t 2 where tj is the actual value for fitness case j t is the mean of tj and pij is the estimated value by individual chromosome i for fitness case j other things that need to be determined prior to modeling are the function set and the genetic operators the optimum values of these parameters obtained using trial and error are presented in table 2 2 7 generalized structure of group method of data handling group method of data handling gmdh is a self organized approach so that gradually produces more complex models during the performance evaluation of input and output data sets ivakhnenko 1968 in the gmdh method the relation between the input and output variables is expressed by the series of volterra functions which is similar to the kolmogorov gabor discrete polynomial ivakhnenko 1968 26 y w 0 i 1 n w i x i i 1 n j 1 n w ij x i x j i 1 n j 1 n k 1 n w ijk x i x j x k where y is the output variables w w 0 w 1 wm is weights vector and x x 1 x 2 xm is the vector of input variables the gmdh model has been developed based on heuristic self organization to address the complexity of multi dimensional problems this method first considers the variables as data pairs xi 1 xi r 1 2 n and then specifies a threshold value for evaluating the variables that cannot reach the level of performance this procedure describes a self organized algorithm in this research the gmdh model is made according to the following steps and shown in fig 5 step 1 divide the data into training and testing stages the main data is divided into two stages of training and testing so that training data is used to estimate some features of the nonlinear system and the test data set is used to validate the performance of the proposed model step 2 create a pairs of variables xi 1 xi r 1 2 n in each layer all of variables pairs from n input variables are constructed for the desired layer the number of these pairs is equal to 27 c n m n r m r where n is the number of input variables and r is the coefficient that is considered 2 at the classic gmdh method given the number of neurons in each layer there are the same number of z z 1 z 2 zm 28 z m w 0 w 1 x 1 w 2 x 2 w 3 x 1 2 w 4 x 2 2 w 4 x 1 x 2 the above equation known as the transfer function is a comprehensive equation for this method step 3 estimate the weights in each layer the least square error method is employed to estimate coefficient of the transfer function which are as follows 29 w i x i t x i 1 x i y obs where w w 0 w 1 w 5 is the weight vector yobs yobs1 yobs2 yobsm t denotes the outputs values of the observed samples and x is the input data pairs step 4 optimize the elements in each layer the partial optimal expression of a nonlinear system is done by regression analysis into training data in this step the best variables and their pairs are identified and selected on each layer after calculating the output of zi in order to determine the structure of the network it is necessary to use an external criterion for the selection of the neurons that describe the purpose this criterion is calculated as follows 30 δ p 2 i 1 n v p v p i 1 n v o v o where vo and vp denote observed and estimated respectively value and the v o and v p are the mean of the observed and estimated respectively value step 5 stop criteria to create multi layer structure by comparing the value of the current layer index with the next layer to be created it will stop generating subsequent layers if the index value is not improved or less than the default value otherwise the second to fourth steps repeat until the precision of the predicted values is reached after the steps are completed a smart model with a multi layered structure is built that has ability to predict the complex process of sediment transport modeling although the gmdh method has different benefits it also has some limitations the limitations of modeling nonlinear problems with the classical gmdh method include the following i the maximum number of inputs in each neuron is two ii considering a polynomial with order 2 iii select the neurons only from adjacent layer taking into account the advantages offered by the classical gmdh method in order to overcome the expressed shortcomings a gmdh based method which has a different structure than the classical gmdh is presented as the generalized structure of gmdh gs gmdh in the proposed method based on gmdh the developed model structure can be a second order polynomial or three additionally input neurons can also be 2 or 3 in addition to changing the order and number of neurons in order to avoid the significant complexity of the model in the classical gmdh method the inputs of each neuron can also be selected from non adjacent layers to select the optimal structure in the gs gmdh model the akaike information criterion aic ebtehaj et al 2014 is used the aic consists of two terms so that the first term n log 1 nσi 1 n yi yi 2 is provided to evaluate the performance of the proposed model in term of accuracy while the second one 2 k is provided to control the complexity of the proposed model by gs gmdh failure to consider the second term to complexity control of the proposed model leads to a model with a large number of neurons in different layers therefore the uncertainty of the model may be significantly increased and the use of this model in the practical application will be difficult in addition to the precision of a model a practical tool should be as simple as possible consequently aic could be a good choice to apply in gs gmdh model as a fitness function fig 6 shows the structure of the proposed model for predicting of frp using the gs gmdh model it is observed that the neurons used in the latter layer are taken from non adjacent layer 31 aic n log 1 n i 1 n y i y i 2 2 k where y and y are the predicted and observed samples n denotes to number of samples and k is the number adjustable parameters in developed model the flowchart of the gs gmdh is provided in fig 5 where the cv is the sediment volumetric concentration r is the hydraulic radius d is the sediment median size dgr d g s 1 ν 2 1 3 is the dimensionless grain size parameter and λ is the channel friction factor the four mentioned dimensionless variables cv d r dgr λ are the independent input variables 3 discussion performance of gep elm gs gmdh and anfis fcm machine learning models developed in this study are compared with three well known conventional regression models of mayerle et al 1991 ab ghani 1993 and vongvisessomjai et al 2010 because of their validity as reported in safari et al 2018 the performance of models are investigated in terms of three statistical indices of coefficient of determination r 2 mean absolute percentage error mape and discrepancy ratio dr in tables 3 and 4 comparison of measured and calculated particle froude numbers frp is shown in fig 7 it is seen from table 3 that machine learning models provide better performance than regression models although gep elm gs gmdh and anfis fcm models have almost similar performances gs gmdh is found superior to the other models with r 2 and mape of 0 86 and 14 88 respectively it is found from table 3 that hybrid models of gs gmdh and anfis fcm outperform individual models of gep and elm despite ab ghani 1993 and vongvisessomjai et al 2010 models provide better results than mayerle et al 1991 model their computation accuracy are not as high as machine leaning techniques comparison of models in table 4 in terms of dr indicates that machine learning models highly perform in computing of the particle froude number with mean of the dr close to unity indicating that the gep elm gs gmdh and anfis fcm models outputs fits well to the measured data although a slight overestimation is observed for all machine learning models a significant overestimation is seen for mayerle et al 1991 regression model in the percentage of data within the range of 0 80 1 20 of measured data ab ghani 1993 and vongvisessomjai et al 2010 conventional regression models give results close to the machine learning techniques however for wide ranges of dr higher accuracy of machine leaning techniques should be noticed table 4 shows that hybrid models of gs gmdh and anfis fcm generate much better results in comparison with gep elm and regression models in terms of dr the goodness of fit is studied by comparison of measured and computed particle froude numbers in fig 7 particle froude numbers computed by machine learning techniques match their measured counterparts although a small scatter is observed for elm result conventional regression models considerably overestimate particle froude number with wide scatter which is more tangible for mayerle et al 1991 model among conventional regression models ab ghani 1993 and vongvisessomjai et al 2010 give almost acceptable results however their accuracy are not competitive to the machine leaning techniques it should be noticed that over fitting is an important problem in machine learning modeling safari et al 2016 fathian et al 2019 it is known that well organized models through using lower number of weights or input parameters prevent over fitting problem chang et al 2010 over fitted models fit training events too exactly and generates poor performance on unseen data set at testing step bishop 1995 however the models developed in this study are not expected to have over fitting problem as they generate acceptable results at testing stage moreover they are established on large number of data and input parameters are selected based on the sediment transport hydraulics in the modeling procedure the analysis given above indicated that developed gep elm gs gmdh and anfis fcm models in this study outperform selected conventional regression models in the literature additional to the application of the powerful machine learning techniques for sediment transport modeling in rigid boundary open channels ranges of experimental data used for the modeling and selection of the most effective parameters are of importance as extension of the authors previous works this study works out the existing deficiencies in the machine learning modeling of sediment transport in rigid boundary channels in order to manage the results several studies in the literature omitted some effective parameters in the modeling procedure while in this study four fundamental characteristics of flow fluid sediment and channel are incorporated in the model performance of data driven models is related to the ranges of experimental data most of the studies in the literature used limited number of experimental data for establishing of sediment transport models in rigid boundary open channels however this study utilizes data from four different sources with wide ranges of pipe and sediment size consequently in this study it seems that sediment transport is successfully modeled using machine learning techniques the matlab code for calculating of frp using developed model fcm anfis gep elm and gs gmdh is presented in table 5 4 conclusions machine learning techniques of gep elm gs gmdh and anfis fcm are used for modeling of sediment transport in rigid boundary open channels laboratory experimental data with wide ranges of pipes size sediment size and sediment volumetric concentration are used to establish models for wide ranges of applicability similar to the conventional regression models in the literature this study considered well known dimensionless parameters to show effective parameters in sediment transport in open channel flow to this end fluid flow sediment and channel characteristics are incorporated to develop machine learning models according to the results obtained in this study it is found that gep elm gs gmdh and anfis fcm outperform conventional regression models in particle froude number estimation hybrid models of gs gmdh and anfis fcm perform better than gep and elm where gs gmdh model is found superior to all other models in terms of different statistical performance indices limitations of this study and suggested improvements can be linked to the use of experimental data in the modeling as the ranges of experimental data has great effect on the credibility of machine learning models to this end collecting field data of rigid boundary open channels from sewers and urban drainage systems would be quite valuable to establish practical tool for designing rigid boundary channels such as sewers and drainage systems on the other hand available non deposition experimental data were mostly collected from circular channels therefore performing new set of experiments in variety of channel cross section shapes will be helpful to incorporate channel cross section shape factor in modeling of sediment transport in rigid boundary open channels using machine learning techniques declaration of competing interest the authors declare that they have no known competing financial interests acknowledgments the authors would like to express sincerest appreciation to prof geoff syme editor in chief associate editor and two anonymous reviewers for their highly insightful comments that improved the quality of this manuscript 
6212,biochar has been increasingly used as an amendment to enhance soil structure and improve soil hydraulic properties nevertheless there are very limited physically based studies to investigate solute transport in biochar amended soils at pore scale in this study for the first time synchrotron based x ray micro computed tomography sr μct was used to obtain high resolution pore geometries of two clayey soils and their biochar amended samples then the three dimensional lattice boltzmann lb method was implemented to simulate solute transport using the pore structure information by using the innovative method of combining sr μct and lb simulation we found that biochar amendment reduced the spatial variability of pore water velocity and increased the dispersion coefficient by one order of magnitude in addition we observed that anomalous dispersion was more likely to occur in soils with biochar amendment furthermore soils after biochar amendment had relatively higher thresholds of both the transition zone and advection dominated zone for the dispersion coefficients these results are crucial in understanding nutrient transport processes and contaminant migration occurring at pore scale keywords pore scale modeling lattice boltzmann method sr μct image biochar solute transport dispersion 1 introduction agronomic management of clayey soils is quite challenging because of their poor structure and low hydraulic conductivity to ameliorate the physicochemical properties of clayey soils biochar has been increasingly used as a soil amendment as a porous low density and carbon rich material biochar amendment may improve soil physical and hydraulic properties baveye 2014 promote plant growth biederman and harpole 2013 laird et al 2010 liu et al 2016 schulz and glaser 2012 increase cation exchange capacity total n organic c and microbial biomass jindo et al 2016 quilliam et al 2012 zhou et al 2017 and improve nutrient agrochemical and water use efficiency castellini et al 2015 although the benefits of biochar application are still controversial bach et al 2016 baveye 2014 2018 a considerable amount of biochar has been applied to soils in the last two decades for a possible win win solution to energy problems carbon storage and ecosystem service delivery biochar as an amendment can change the pore size distribution of clayey soils and subsequently impact water and nutrient transport field scale and column scale experiments of biochar amended soils have been extensively studied in recent years for example jin et al 2016 conducted a column experiment to study nitrate leaching in biochar amended soil directly taken from fields in another column study kumari et al 2014 investigated the phosphorus leaching in biochar amended soils in contrast to the widely implemented studies at macro scale there are very limited studies to evaluate the effect of biochar amendment on solute transport at micro scale soil aggregates are the basic units of soil structure which are typically present in soils rich in clay and organic matter they are agglomerates of clay silt and sand particles organic substance fungi and bacterial biomass carminati et al 2008 soil aggregation is crucial for the movement of water gas and solute it can improve soil physical and hydraulic properties and consequently enhance infiltration and water storage and reduce run off however aggregates are easily deformed and spatially rearranged when they are subjected to hydraulic and mechanical loads as the consequence large inter and intra aggregate pores diminish or reduce in size and bulk density on the other hand increases since it is hard to experimentally investigate hydraulic behaviors at the micro scale alternatively the pore scale modeling technique provides a promising approach blunt et al 2013 li et al 2018 willmann et al 2010 zhang et al 2016b real soil pore structures are highly heterogeneous with complex solid boundaries and it is well recognized that pore scale processes are generally difficult to monitor and cannot be quantified by macroscopic models blunt et al 2013 synchrotron based x ray micro computed tomography sr μct offers more detailed soil pore structure information than industry ct scanners houston et al 2013 as an emerging technology the image based pore scale modeling approach can provide a powerful tool to gain insights of pore scale processes bultreys et al 2016 hu et al 2018 roose et al 2016 tian and wang 2018 differing from the traditional macro scale models the lattice boltzmann lb method provides a convenient tool for the computation of advective and diffusive transport of solute in complex porous media bijeljic et al 2013 cardenas 2008 pot et al 2015 vogel et al 2005 yong et al 2014 zhang et al 2016a the method is based on the boltzmann equation and cellular automaton which can recover the navier stokes equation for fluid flow and convective dispersive equation for solute transport using the intricacies of the geometry of a porous medium kang et al 2014 pal et al 2015 richefeu et al 2016 thorne and michael 2006 another advantage of the lb method is its good parallelism nature which is suitable for conducting high performance parallel computation to accelerate simulations chen et al 2016 in our previous work zhou et al 2018 the lb method was combined with sr μct to simulate water flow and to evaluate the effects of biochar amendment on hydraulic properties e g permeability and tortuosity the current study expands the application of the lb method to further investigate the effects of biochar amendment on solute transport in soils under saturated condition that mimics paddy soils since biochar has been increasingly used in rice field for soil remediation bian et al 2014 cui et al 2011 o connor et al 2018 2 experimental setups and sr μct data acquisition two clayey soil samples ultisol and vertisol were collected from two typical rice production regions in china the vertisol was obtained from jiangsu province in northern china 34 17 39 4 n 118 25 32 1 e locally known as shajiang black soil the ultisol was obtained from zhejiang province in southern china 30 8 24 n 119 58 25 e locally known as red soil both soils were collected from the topsoil layer 0 20 cm then air dried at room temperature grounded and passed through a 2 mm sieve before biochar amendment representative physical and chemical properties of the two soils are shown in table 1 the woodchip biochar wcb used in this study was prepared by pyrolyzing organic woodchip in a reactor at 500 c for 2 h the morphological characteristics and elementary composition of the wcb was investigated by field emission scanning electron microscopy fesem and x ray energy dispersive spectroscopy eds wcb is a porous material which has typical xylem pores its pores are mostly elliptic with thick walls and smooth inner walls in the fesem images wcb is similar to the honeycomb fig s 1 the elliptical pores are usually 5 15 μm long and 2 5 μm wide and the thickness of pore walls is 1 2 μm the eds analytical result showed that wcb is constituted by c o si ca and the atomic ratio of c is 93 the total organic carbon toc of wcb is 642 and ph is 8 4 alkalescency the biochar 4 by dry weight was added to soil samples mixed thoroughly to create the amended soils each treatment was settled in a pot with 1 5 kg soil both the amended and unamended control soil samples were then incubated in a glasshouse at 25 c to 28 c for 180 days during the incubation deionized water was added at the top of the soils every week for maintaining the soil moisture at about 70 of their respective field capacities measured at the matric potential of 33 kpa assouline and or 2014 after incubation the soil samples were air dried gently passed through a 5 mm sieve and macroaggregates 5 to 7 mm diameter were randomly selected from the amended and unamended soil optical microscopy was used to select two macroaggregates from each treatment to minimize the differences in size and shape of the duplicates for sr μct analysis the selected macroaggregates from each of the four treatments ultisol without amendment u ultisol with biochar amendment ub vertisol without amendment v and vertisol with biochar amendment vb were scanned using a sr μct at the shanghai synchrotron radiation facility ssrf the scanning was conducted by using a pco2000 charge coupled device ccd camera with 3 7 µm pixel resolution for each aggregate 720 projections of 2048 by 2048 pixels were obtained for ct reconstruction then the filtered back projection fbp algorithm was applied to the projection images for reconstruction sagara et al 2010 ring artifacts in all samples were removed before image segmentation zhou et al 2012 because the woodchip biochar is composed of elements with a low atomic number a low z material and has small x ray absorption coefficient the value i e color depth of pixels in a digital image of the biochar particles in the ct images is only slightly greater than that of the soil pores thus the biochar particles may be wrongly identified as soil pores in image segmentation to address this issue an accumulated histogram method was used to improve the accuracy of image recognition between biochar particles and soil pores yu et al 2015 after using accumulated histogram method to identify the biochar particles from pores the image could be divided into soil biochar and pores by selecting proper threshold values the image segmentations were performed with the modes method provided by image j software in this procedure two local maxima were found interactively in the grey value histogram and the midpoint between the local maxima was selected as the threshold value to avoid edge effects the images were further cropped to 400 by 400 pixels the resolution of the microtomography image is 3 7 µm with 400 400 400 voxels 1 48 1 48 1 48 mm in x y z directions 3 simulations for fluid dynamics and solute transport the sr μct images were converted to binary files and directly used as the internal boundaries in our 3d lb model for pore scale water and solute transport simulations initially the pore space was fully saturated with water an external pressure gradient was applied to induce the water flow in the lb method a fluid is treated as mesoscale particles moving in a lattice domain with discrete lattice velocities at discrete time steps the fluid variables such as density and velocity can be obtained through the distribution functions over all discrete lattices in this study we adopted a d3q19 lattice boltzmann model 19 velocity directions in 3d space to simulate water flow thorne and michael 2006 the model was validated by a comparison to analytical solutions and laboratory measurements chen et al 2008 compared with other scheme of 3d space lattice boltzmann method d3q13 d3q15 d3q17 d3q19 is more accurate in simulating complex pore structures of soil aggregates for the fluid flow part the macroscopic fluid density ρ and velocity u are defined as 1 ρ i 0 18 f i 2 u 1 ρ i 0 18 f i e i where f is the distribution function and e i is the microscopic velocity along the i th direction the fluid particles experience collision and propagation i e 3 f i x e i δ t t δ t f i x t streaming 1 τ f i x t f i eq ρ u collision 0 1 2 18 4 f i eq ρ u w i ρ x 1 3 e i u c 2 9 e i u 2 2 c 4 3 u 2 2 c 2 where f i eq ρ u is the equilibrium distribution function at location x and time t in the i th direction here e i is defined as 0 0 0 c for i 0 1 0 0 c for i 1 2 0 1 0 c for i 3 4 0 0 1 c for i 5 6 1 1 0 c for i 7 10 0 1 1 c for i 11 14 and 1 0 1 c for i 15 18 the parameter c is defined as δ x δ t where δ x is the lattice spacing δ t the time step τ is the dimensionless relaxation time that relates to the kinematic viscosity by 5 v 1 3 τ 1 2 δ x 2 δ t where w i is the weight coefficient for the d3q19 model w i 1 3 for i 0 w i 1 18 for i 1 6 and w i 1 36 for i 7 18 in the lb method δ x and δ t are used as the spatial and temporal units respectively complex boundaries geometries of porous media can be easily handled in the lb scheme a no slip boundary condition at the solid surface can be realized through the bounce back condition which mimics the phenomenon that a particle reflects its momentum when colliding with a solid surface thorne and michael 2006 the lb equation approximates the following macroscopic navier stokes equations 6 ρ t ρ u 0 7 ρ u t ρ uu p ρ v u u to simulate solute transport another distribution function g i is defined as follows 8 g i x e i δ t t δ t g i x t g i x t g i eq c u τ s i 0 1 2 18 where τ s is the dimensionless relaxation time g i eq c u is the equilibrium distribution function at location x and time t along the i th direction 9 g i eq c u w i c 1 3 e i u c 2 9 e i u 2 2 c 4 3 u 2 2 c 2 where c is the macroscopic solute concentration 10 c i 0 18 g i eq 10 recovers the macroscopic advection diffusion equation 11 c t u c d c the molecular diffusion coefficient d can be calculated by d τ s 0 5 δ x 2 3 δ t in the water flow simulation extra layers were added to the inlet and outlet for buffering chen et al 2009 the other four sides were defined as no flow boundary the reynolds number was kept much lower than 1 so that the macroscopic flow obeys the darcy regime after the flow field has reached the steady state i e when the variation of mean velocity of fluid flow u t meets the criterion u t u t δ t u t 10 4 an instantaneous plane source of solute was injected at the inlet the moment methods were employed to calculate the longitudinal dispersion tensor of the solute plume 12 d l d z d σ z 2 t 2 d t where the tensor σ z 2 t is the second spatial moment of the solute plume in z direction the value of d l is determined by two physical mechanisms molecular diffusion and mechanical dispersion as a quantitative metric the péclet number is defined as the ratio between the time needed for particles in the fluid to traverse a characteristic length l by advection and the time needed for particles to travel the same length by diffusion i e pe ul d m d m stands for the molecular diffusion coefficient bear 2013 identified various distinct zones for variation of the longitudinal dispersion coefficient with pe number in unconsolidated porous media in our simulation different pe numbers were obtained by adjusting the molecular diffusion coefficient d in the lb model zhang and lv 2007 ovaysi and piri 2011 the total number of grids for each sample is as high as 64 000 000 in this study which makes the lb simulations rather computationally expensive to accelerate the simulation gpu parallel computing was used to perform the simulations with combining the technology of compute unified device architecture cuda developed by nvidia and message passing interface mpi each simulation was implemented with 2 nodes and each node with 4 nividia tesla gpu accelerator k20m 2496 cores 5 gb memory size 706 mhz gpu clock 4 results and discussion fig 1 shows the 3d tomographic reconstructions of the pore geometries from sr μct scans for the two clayey soils ultisol and vertisol with and without woodchip biochar amendment as indicated by previous studies biochar can increase the total porosity and the fraction of large pore volume in the amended soils ajayi and horn 2017 liu et al 2017 consequently influence both water and solute transport the porosity of ultisol and vertisol calculated from the associated x ray images were 19 08 and 16 65 and the porosity of biochar amended ultisol and vertisol were 44 66 and 31 12 respectively in this study the effect of biochar amendment on solute transport were assessed by using pore scale lb simulations solute concentration profiles in different simulation scenarios are shown in fig 2 as the solute moved downwards hydraulic dispersion took place the solute plume was diluted and the solute concentration front became gradually smoothed out spatial variability in pore water flow was evaluated in terms of the first moment mean and second moment variance of the velocity distribution the coefficient of variation cv the ratio of standard deviation to mean of the longitudinal velocity u l is presented in fig 3 overall wcb amendment decreased the cv of longitudinal pore water velocity which was attributed to the lower spatial variabilities in tortuosity and pore water velocity in the biochar amended soils the pore cluster indicates that there were more newly formed pores in the wcb amended soils and wcb particles might act as the skeleton to stabilize the larger pores within the aggregates similar results were observed for the magnitudes of velocities u l and u t but here only the results for ul are shown in order to focus on the longitudinal transport behavior according to our previous study on the pore geometries zhou et al 2018 biochar addition decreased the tortuosity of the flow path and reduced the number of dead pores in other words with the same pressure gradient biochar amendment made the pore water velocity more uniform and increased the mean component to characterize the solute movement in porous media the transit time distribution φ t was defined as the probability of the time that a solute particle takes to move across one pixel zhang and lv 2007 we estimated the transit time distribution φ t by the velocity field calculated from the lb simulations fig 4 it has a power law tail decaying with time in φ t t β the values of β for u ub v vb were 0 97 1 41 1 16 1 34 respectively in the absence of molecular diffusion the macroscopic dispersion coefficients of the wcb amended soils were more diverged mechanical or hydrodynamic dispersion phenomenon originates from the fact that the fluid moves faster in larger pores than it does in smaller ones and faster at the centers of the pores than along the walls laminar flow as well as that flow pathways are with different lengths bear and cheng 2010 when the same pressure gradient was applied to the four treatments in this study the normalized longitudinal dispersion coefficients d l dm were always larger in the wcb amended soils table 2 this was because soils after biochar amendment tended to have higher pore water velocity with the same pressure gradient leading to significantly increased mechanical dispersion the temporal evolutions of the normalized longitudinal dispersion coefficients d l dm of the ultisol before and after biochar amendment are shown in fig 5 the longitudinal dispersion coefficients of the untreated soil were stable and nearly constant over time fig 5 a while for the biochar amended ultisol the longitudinal dispersion coefficients increased with time fig 5 b overall the plumes in the unamended soil samples were gaussian i e the asymptotic dispersion coefficients were reached fig 5 a in contrast the computation domain 1 48 mm is too small for the plume in the wcb amended samples to reach a gaussian state fig 5 b significant anomalous non fickian transport behavior e g the long tails were observed after traveling approximately 1 mm in the wcb amended aggregates non fickian transport of solute has been observed at field and laboratory scales in a wide variety of porous and fractured geological formations berkowitz and scher 2009 levy and berkowitz 2003 although it was shown that the interplay between pore scale velocity correlation and velocity heterogeneity might be responsible for the observed anomalous behavior the physical mechanism behind anomalous transport has not been fully understood bijeljic et al 2004 kang 2014 zhang and lv 2007 and the error associated with the use of fick s law alone may increase to further investigate the behavior of the solute plume the normalized longitudinal dispersion coefficients d l d m were obtained with different pe numbers it was observed that d l d m values always increased with pe number for all the test soils inspection of fig 6 shows there existed three zones on the longitudinal dispersion coefficient curves in zone i the water velocity was very low and molecular diffusion dominated and consequently the longitudinal dispersion coefficients increased very slowly with pe number zone ii was the transition zone where both molecular diffusion and mechanical dispersion played roles in solute transport as velocity further increased zone iii the longitudinal dispersion coefficient increased with pe number following a power law d l p e δ the δ values for treatments u ub v vb were 0 63 0 80 1 07 1 09 respectively indicating that advection which transversal diffusion dominated solute transport in this zone it is also noted that soil samples after wcb amendment had relatively higher thresholds for both zones ii and iii for example the threshold value of zone ii for the ultisol was 0 009 while it increased to 0 067 for the wcb amended ultisol for the vertisol it was 0 005 while for the wcb amended vertisol the threshold was 10 fold greater 0 05 soil structure is indicative of a soil s ability to hold and transmit water and nutrient that are required for plant and microbial growth although some possible explanations have been proposed the mechanisms for improvements in soil fertility after biochar amendment are not well understood burns 2014 especially at pore scale level our results showed that the addition of woodchip biochar is beneficial to improve soil aggregation in the clayey soils by increasing porosity changing the pore size distribution and decreasing tortuosity furthermore both the hydraulic conductivity and solute dispersion coefficient increased after the biochar amendment which led to the improved ability of transmitting water and nutrient 5 conclusions and implications in this study selected aggregates from two clayey soils an ultisol and a vertisol with and without biochar amendment were scanned by sr μct and the digitized image data were then used as internal boundaries in the high performance lb method to simulate water and solute transport at pore scale our results showed that woodchip biochar amendment decreased the spatial variability in pore water velocities and increased the dispersion coefficients in the aggregates by nearly one order of magnitude furthermore anomalous dispersion was more likely to occur in soils with biochar amendment and soil samples after biochar amendment had relatively higher pe thresholds for both the transition zone and advection dominated zone the sr μct is a non destructive method to obtain high resolution real pore architectures based on which the lb method can be directly implemented this approach of combining sr μct and lb simulation to study solute transport provides a new way to observe the poorly understood contaminant and plant nutrient transport processes at pore scale our current study is limited to water and solute transport in saturated condition and future investigations should be conducted to characterize the transport in unsaturated soil acknowledgements this work is supported by the national natural science foundation of china grants 41571215 and 41571450 the data presented in this paper will be made freely available once this manuscript is accepted for publication appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 123933 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6212,biochar has been increasingly used as an amendment to enhance soil structure and improve soil hydraulic properties nevertheless there are very limited physically based studies to investigate solute transport in biochar amended soils at pore scale in this study for the first time synchrotron based x ray micro computed tomography sr μct was used to obtain high resolution pore geometries of two clayey soils and their biochar amended samples then the three dimensional lattice boltzmann lb method was implemented to simulate solute transport using the pore structure information by using the innovative method of combining sr μct and lb simulation we found that biochar amendment reduced the spatial variability of pore water velocity and increased the dispersion coefficient by one order of magnitude in addition we observed that anomalous dispersion was more likely to occur in soils with biochar amendment furthermore soils after biochar amendment had relatively higher thresholds of both the transition zone and advection dominated zone for the dispersion coefficients these results are crucial in understanding nutrient transport processes and contaminant migration occurring at pore scale keywords pore scale modeling lattice boltzmann method sr μct image biochar solute transport dispersion 1 introduction agronomic management of clayey soils is quite challenging because of their poor structure and low hydraulic conductivity to ameliorate the physicochemical properties of clayey soils biochar has been increasingly used as a soil amendment as a porous low density and carbon rich material biochar amendment may improve soil physical and hydraulic properties baveye 2014 promote plant growth biederman and harpole 2013 laird et al 2010 liu et al 2016 schulz and glaser 2012 increase cation exchange capacity total n organic c and microbial biomass jindo et al 2016 quilliam et al 2012 zhou et al 2017 and improve nutrient agrochemical and water use efficiency castellini et al 2015 although the benefits of biochar application are still controversial bach et al 2016 baveye 2014 2018 a considerable amount of biochar has been applied to soils in the last two decades for a possible win win solution to energy problems carbon storage and ecosystem service delivery biochar as an amendment can change the pore size distribution of clayey soils and subsequently impact water and nutrient transport field scale and column scale experiments of biochar amended soils have been extensively studied in recent years for example jin et al 2016 conducted a column experiment to study nitrate leaching in biochar amended soil directly taken from fields in another column study kumari et al 2014 investigated the phosphorus leaching in biochar amended soils in contrast to the widely implemented studies at macro scale there are very limited studies to evaluate the effect of biochar amendment on solute transport at micro scale soil aggregates are the basic units of soil structure which are typically present in soils rich in clay and organic matter they are agglomerates of clay silt and sand particles organic substance fungi and bacterial biomass carminati et al 2008 soil aggregation is crucial for the movement of water gas and solute it can improve soil physical and hydraulic properties and consequently enhance infiltration and water storage and reduce run off however aggregates are easily deformed and spatially rearranged when they are subjected to hydraulic and mechanical loads as the consequence large inter and intra aggregate pores diminish or reduce in size and bulk density on the other hand increases since it is hard to experimentally investigate hydraulic behaviors at the micro scale alternatively the pore scale modeling technique provides a promising approach blunt et al 2013 li et al 2018 willmann et al 2010 zhang et al 2016b real soil pore structures are highly heterogeneous with complex solid boundaries and it is well recognized that pore scale processes are generally difficult to monitor and cannot be quantified by macroscopic models blunt et al 2013 synchrotron based x ray micro computed tomography sr μct offers more detailed soil pore structure information than industry ct scanners houston et al 2013 as an emerging technology the image based pore scale modeling approach can provide a powerful tool to gain insights of pore scale processes bultreys et al 2016 hu et al 2018 roose et al 2016 tian and wang 2018 differing from the traditional macro scale models the lattice boltzmann lb method provides a convenient tool for the computation of advective and diffusive transport of solute in complex porous media bijeljic et al 2013 cardenas 2008 pot et al 2015 vogel et al 2005 yong et al 2014 zhang et al 2016a the method is based on the boltzmann equation and cellular automaton which can recover the navier stokes equation for fluid flow and convective dispersive equation for solute transport using the intricacies of the geometry of a porous medium kang et al 2014 pal et al 2015 richefeu et al 2016 thorne and michael 2006 another advantage of the lb method is its good parallelism nature which is suitable for conducting high performance parallel computation to accelerate simulations chen et al 2016 in our previous work zhou et al 2018 the lb method was combined with sr μct to simulate water flow and to evaluate the effects of biochar amendment on hydraulic properties e g permeability and tortuosity the current study expands the application of the lb method to further investigate the effects of biochar amendment on solute transport in soils under saturated condition that mimics paddy soils since biochar has been increasingly used in rice field for soil remediation bian et al 2014 cui et al 2011 o connor et al 2018 2 experimental setups and sr μct data acquisition two clayey soil samples ultisol and vertisol were collected from two typical rice production regions in china the vertisol was obtained from jiangsu province in northern china 34 17 39 4 n 118 25 32 1 e locally known as shajiang black soil the ultisol was obtained from zhejiang province in southern china 30 8 24 n 119 58 25 e locally known as red soil both soils were collected from the topsoil layer 0 20 cm then air dried at room temperature grounded and passed through a 2 mm sieve before biochar amendment representative physical and chemical properties of the two soils are shown in table 1 the woodchip biochar wcb used in this study was prepared by pyrolyzing organic woodchip in a reactor at 500 c for 2 h the morphological characteristics and elementary composition of the wcb was investigated by field emission scanning electron microscopy fesem and x ray energy dispersive spectroscopy eds wcb is a porous material which has typical xylem pores its pores are mostly elliptic with thick walls and smooth inner walls in the fesem images wcb is similar to the honeycomb fig s 1 the elliptical pores are usually 5 15 μm long and 2 5 μm wide and the thickness of pore walls is 1 2 μm the eds analytical result showed that wcb is constituted by c o si ca and the atomic ratio of c is 93 the total organic carbon toc of wcb is 642 and ph is 8 4 alkalescency the biochar 4 by dry weight was added to soil samples mixed thoroughly to create the amended soils each treatment was settled in a pot with 1 5 kg soil both the amended and unamended control soil samples were then incubated in a glasshouse at 25 c to 28 c for 180 days during the incubation deionized water was added at the top of the soils every week for maintaining the soil moisture at about 70 of their respective field capacities measured at the matric potential of 33 kpa assouline and or 2014 after incubation the soil samples were air dried gently passed through a 5 mm sieve and macroaggregates 5 to 7 mm diameter were randomly selected from the amended and unamended soil optical microscopy was used to select two macroaggregates from each treatment to minimize the differences in size and shape of the duplicates for sr μct analysis the selected macroaggregates from each of the four treatments ultisol without amendment u ultisol with biochar amendment ub vertisol without amendment v and vertisol with biochar amendment vb were scanned using a sr μct at the shanghai synchrotron radiation facility ssrf the scanning was conducted by using a pco2000 charge coupled device ccd camera with 3 7 µm pixel resolution for each aggregate 720 projections of 2048 by 2048 pixels were obtained for ct reconstruction then the filtered back projection fbp algorithm was applied to the projection images for reconstruction sagara et al 2010 ring artifacts in all samples were removed before image segmentation zhou et al 2012 because the woodchip biochar is composed of elements with a low atomic number a low z material and has small x ray absorption coefficient the value i e color depth of pixels in a digital image of the biochar particles in the ct images is only slightly greater than that of the soil pores thus the biochar particles may be wrongly identified as soil pores in image segmentation to address this issue an accumulated histogram method was used to improve the accuracy of image recognition between biochar particles and soil pores yu et al 2015 after using accumulated histogram method to identify the biochar particles from pores the image could be divided into soil biochar and pores by selecting proper threshold values the image segmentations were performed with the modes method provided by image j software in this procedure two local maxima were found interactively in the grey value histogram and the midpoint between the local maxima was selected as the threshold value to avoid edge effects the images were further cropped to 400 by 400 pixels the resolution of the microtomography image is 3 7 µm with 400 400 400 voxels 1 48 1 48 1 48 mm in x y z directions 3 simulations for fluid dynamics and solute transport the sr μct images were converted to binary files and directly used as the internal boundaries in our 3d lb model for pore scale water and solute transport simulations initially the pore space was fully saturated with water an external pressure gradient was applied to induce the water flow in the lb method a fluid is treated as mesoscale particles moving in a lattice domain with discrete lattice velocities at discrete time steps the fluid variables such as density and velocity can be obtained through the distribution functions over all discrete lattices in this study we adopted a d3q19 lattice boltzmann model 19 velocity directions in 3d space to simulate water flow thorne and michael 2006 the model was validated by a comparison to analytical solutions and laboratory measurements chen et al 2008 compared with other scheme of 3d space lattice boltzmann method d3q13 d3q15 d3q17 d3q19 is more accurate in simulating complex pore structures of soil aggregates for the fluid flow part the macroscopic fluid density ρ and velocity u are defined as 1 ρ i 0 18 f i 2 u 1 ρ i 0 18 f i e i where f is the distribution function and e i is the microscopic velocity along the i th direction the fluid particles experience collision and propagation i e 3 f i x e i δ t t δ t f i x t streaming 1 τ f i x t f i eq ρ u collision 0 1 2 18 4 f i eq ρ u w i ρ x 1 3 e i u c 2 9 e i u 2 2 c 4 3 u 2 2 c 2 where f i eq ρ u is the equilibrium distribution function at location x and time t in the i th direction here e i is defined as 0 0 0 c for i 0 1 0 0 c for i 1 2 0 1 0 c for i 3 4 0 0 1 c for i 5 6 1 1 0 c for i 7 10 0 1 1 c for i 11 14 and 1 0 1 c for i 15 18 the parameter c is defined as δ x δ t where δ x is the lattice spacing δ t the time step τ is the dimensionless relaxation time that relates to the kinematic viscosity by 5 v 1 3 τ 1 2 δ x 2 δ t where w i is the weight coefficient for the d3q19 model w i 1 3 for i 0 w i 1 18 for i 1 6 and w i 1 36 for i 7 18 in the lb method δ x and δ t are used as the spatial and temporal units respectively complex boundaries geometries of porous media can be easily handled in the lb scheme a no slip boundary condition at the solid surface can be realized through the bounce back condition which mimics the phenomenon that a particle reflects its momentum when colliding with a solid surface thorne and michael 2006 the lb equation approximates the following macroscopic navier stokes equations 6 ρ t ρ u 0 7 ρ u t ρ uu p ρ v u u to simulate solute transport another distribution function g i is defined as follows 8 g i x e i δ t t δ t g i x t g i x t g i eq c u τ s i 0 1 2 18 where τ s is the dimensionless relaxation time g i eq c u is the equilibrium distribution function at location x and time t along the i th direction 9 g i eq c u w i c 1 3 e i u c 2 9 e i u 2 2 c 4 3 u 2 2 c 2 where c is the macroscopic solute concentration 10 c i 0 18 g i eq 10 recovers the macroscopic advection diffusion equation 11 c t u c d c the molecular diffusion coefficient d can be calculated by d τ s 0 5 δ x 2 3 δ t in the water flow simulation extra layers were added to the inlet and outlet for buffering chen et al 2009 the other four sides were defined as no flow boundary the reynolds number was kept much lower than 1 so that the macroscopic flow obeys the darcy regime after the flow field has reached the steady state i e when the variation of mean velocity of fluid flow u t meets the criterion u t u t δ t u t 10 4 an instantaneous plane source of solute was injected at the inlet the moment methods were employed to calculate the longitudinal dispersion tensor of the solute plume 12 d l d z d σ z 2 t 2 d t where the tensor σ z 2 t is the second spatial moment of the solute plume in z direction the value of d l is determined by two physical mechanisms molecular diffusion and mechanical dispersion as a quantitative metric the péclet number is defined as the ratio between the time needed for particles in the fluid to traverse a characteristic length l by advection and the time needed for particles to travel the same length by diffusion i e pe ul d m d m stands for the molecular diffusion coefficient bear 2013 identified various distinct zones for variation of the longitudinal dispersion coefficient with pe number in unconsolidated porous media in our simulation different pe numbers were obtained by adjusting the molecular diffusion coefficient d in the lb model zhang and lv 2007 ovaysi and piri 2011 the total number of grids for each sample is as high as 64 000 000 in this study which makes the lb simulations rather computationally expensive to accelerate the simulation gpu parallel computing was used to perform the simulations with combining the technology of compute unified device architecture cuda developed by nvidia and message passing interface mpi each simulation was implemented with 2 nodes and each node with 4 nividia tesla gpu accelerator k20m 2496 cores 5 gb memory size 706 mhz gpu clock 4 results and discussion fig 1 shows the 3d tomographic reconstructions of the pore geometries from sr μct scans for the two clayey soils ultisol and vertisol with and without woodchip biochar amendment as indicated by previous studies biochar can increase the total porosity and the fraction of large pore volume in the amended soils ajayi and horn 2017 liu et al 2017 consequently influence both water and solute transport the porosity of ultisol and vertisol calculated from the associated x ray images were 19 08 and 16 65 and the porosity of biochar amended ultisol and vertisol were 44 66 and 31 12 respectively in this study the effect of biochar amendment on solute transport were assessed by using pore scale lb simulations solute concentration profiles in different simulation scenarios are shown in fig 2 as the solute moved downwards hydraulic dispersion took place the solute plume was diluted and the solute concentration front became gradually smoothed out spatial variability in pore water flow was evaluated in terms of the first moment mean and second moment variance of the velocity distribution the coefficient of variation cv the ratio of standard deviation to mean of the longitudinal velocity u l is presented in fig 3 overall wcb amendment decreased the cv of longitudinal pore water velocity which was attributed to the lower spatial variabilities in tortuosity and pore water velocity in the biochar amended soils the pore cluster indicates that there were more newly formed pores in the wcb amended soils and wcb particles might act as the skeleton to stabilize the larger pores within the aggregates similar results were observed for the magnitudes of velocities u l and u t but here only the results for ul are shown in order to focus on the longitudinal transport behavior according to our previous study on the pore geometries zhou et al 2018 biochar addition decreased the tortuosity of the flow path and reduced the number of dead pores in other words with the same pressure gradient biochar amendment made the pore water velocity more uniform and increased the mean component to characterize the solute movement in porous media the transit time distribution φ t was defined as the probability of the time that a solute particle takes to move across one pixel zhang and lv 2007 we estimated the transit time distribution φ t by the velocity field calculated from the lb simulations fig 4 it has a power law tail decaying with time in φ t t β the values of β for u ub v vb were 0 97 1 41 1 16 1 34 respectively in the absence of molecular diffusion the macroscopic dispersion coefficients of the wcb amended soils were more diverged mechanical or hydrodynamic dispersion phenomenon originates from the fact that the fluid moves faster in larger pores than it does in smaller ones and faster at the centers of the pores than along the walls laminar flow as well as that flow pathways are with different lengths bear and cheng 2010 when the same pressure gradient was applied to the four treatments in this study the normalized longitudinal dispersion coefficients d l dm were always larger in the wcb amended soils table 2 this was because soils after biochar amendment tended to have higher pore water velocity with the same pressure gradient leading to significantly increased mechanical dispersion the temporal evolutions of the normalized longitudinal dispersion coefficients d l dm of the ultisol before and after biochar amendment are shown in fig 5 the longitudinal dispersion coefficients of the untreated soil were stable and nearly constant over time fig 5 a while for the biochar amended ultisol the longitudinal dispersion coefficients increased with time fig 5 b overall the plumes in the unamended soil samples were gaussian i e the asymptotic dispersion coefficients were reached fig 5 a in contrast the computation domain 1 48 mm is too small for the plume in the wcb amended samples to reach a gaussian state fig 5 b significant anomalous non fickian transport behavior e g the long tails were observed after traveling approximately 1 mm in the wcb amended aggregates non fickian transport of solute has been observed at field and laboratory scales in a wide variety of porous and fractured geological formations berkowitz and scher 2009 levy and berkowitz 2003 although it was shown that the interplay between pore scale velocity correlation and velocity heterogeneity might be responsible for the observed anomalous behavior the physical mechanism behind anomalous transport has not been fully understood bijeljic et al 2004 kang 2014 zhang and lv 2007 and the error associated with the use of fick s law alone may increase to further investigate the behavior of the solute plume the normalized longitudinal dispersion coefficients d l d m were obtained with different pe numbers it was observed that d l d m values always increased with pe number for all the test soils inspection of fig 6 shows there existed three zones on the longitudinal dispersion coefficient curves in zone i the water velocity was very low and molecular diffusion dominated and consequently the longitudinal dispersion coefficients increased very slowly with pe number zone ii was the transition zone where both molecular diffusion and mechanical dispersion played roles in solute transport as velocity further increased zone iii the longitudinal dispersion coefficient increased with pe number following a power law d l p e δ the δ values for treatments u ub v vb were 0 63 0 80 1 07 1 09 respectively indicating that advection which transversal diffusion dominated solute transport in this zone it is also noted that soil samples after wcb amendment had relatively higher thresholds for both zones ii and iii for example the threshold value of zone ii for the ultisol was 0 009 while it increased to 0 067 for the wcb amended ultisol for the vertisol it was 0 005 while for the wcb amended vertisol the threshold was 10 fold greater 0 05 soil structure is indicative of a soil s ability to hold and transmit water and nutrient that are required for plant and microbial growth although some possible explanations have been proposed the mechanisms for improvements in soil fertility after biochar amendment are not well understood burns 2014 especially at pore scale level our results showed that the addition of woodchip biochar is beneficial to improve soil aggregation in the clayey soils by increasing porosity changing the pore size distribution and decreasing tortuosity furthermore both the hydraulic conductivity and solute dispersion coefficient increased after the biochar amendment which led to the improved ability of transmitting water and nutrient 5 conclusions and implications in this study selected aggregates from two clayey soils an ultisol and a vertisol with and without biochar amendment were scanned by sr μct and the digitized image data were then used as internal boundaries in the high performance lb method to simulate water and solute transport at pore scale our results showed that woodchip biochar amendment decreased the spatial variability in pore water velocities and increased the dispersion coefficients in the aggregates by nearly one order of magnitude furthermore anomalous dispersion was more likely to occur in soils with biochar amendment and soil samples after biochar amendment had relatively higher pe thresholds for both the transition zone and advection dominated zone the sr μct is a non destructive method to obtain high resolution real pore architectures based on which the lb method can be directly implemented this approach of combining sr μct and lb simulation to study solute transport provides a new way to observe the poorly understood contaminant and plant nutrient transport processes at pore scale our current study is limited to water and solute transport in saturated condition and future investigations should be conducted to characterize the transport in unsaturated soil acknowledgements this work is supported by the national natural science foundation of china grants 41571215 and 41571450 the data presented in this paper will be made freely available once this manuscript is accepted for publication appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 123933 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6213,in this study different artificial intelligence ai techniques including feed forward neural network ffnn adaptive neuro fuzzy inference system anfis support vector regression svr empirical models including hargreaves and samani hs modified hargreaves and samani mhs makkink mk ritchie rt and conventional multilinear regression mlr were employed to model reference evapotranspiration et0 in fourteen stations from several climatic regions in turkey cyprus iraq iran and libya for this purpose 12 parameters of monthly climate data were collected and used as input parameters to the models the parameters were subjected to quality assurance tests to ensure their validity and acceptability the study was conducted in three sections i sensitivity analysis was conducted to determine the dominant inputs ii single models were trained and their performances were accessed on the basis of et0 derived from pan evaporation method iii finally three ensemble methods of simple averaging weighted averaging and neural ensemble were applied in strategy 1 for ai models and strategy 2 for empirical models to improve the predicting performance the results revealed that depending on the climate of the regions pan evaporation solar radiation temperatures and surface pressure are the most dominant parameters empirical and mlr models could be employed to achieve the valuable results ai based models are superior in performance to the other models also promising improvement in et0 modeling could be achieved by ensemble modeling the results of this study affirmed that the provided ensemble approaches can increase the performance of single models in the verification phase up to 22 for strategy 1 and 55 for strategy 2 also the results demonstrated that ai based ensemble modeling is preferable to empirical ensemble modeling keywords artificial intelligence pan evaporation sensitivity analysis feed forward neural network ensemble models reference evapotranspiration 1 introduction evapotranspiration et refers to the transfer of water from the surface of the earth to the atmosphere through evaporation from surfaces of wet plant water and soil and through plant stomata by transpiration odhiambo et al 2001 the reference surface of reference evapotranspiration or reference crop evapotranspiration is a hypothetical grass reference crop having an estimated crop depth of 0 12 m 70 sm 1 of a fixed surface resistant and 0 23 albedo allen et al 1998 water deficiency is becoming a major problem especially in arid and semi arid climatic regions this negatively affects irrigation scheduling which in turn results in a very low or no crop yields for multiannual and seasonal crops papadavid and diofantos 2010 however according to oladipo 1993 after soil fertility one of the main factors that limit agricultural production is water supply deficiencies therefore in order to have effective water resources management and soil water balance for crop productions in such regions estimation of reference evapotranspiration et0 with acceptable accuracy is important the history relating et to meteorological variables can be traced to early 19th century see brutsaert 1982 chen et al 2005 from then many methods categorized into 6 groups have been developed chen et al 2005 1 temperature based methods blaney 1952 hargreaves and samani 1985 2 radiation based methods e g makkink 1957 jones and ritchie 1990 3 combination methods e g penman 1948 monteith 1965 4 mass transfer methods e g harbeck 1962 5 water budget methods e g guitjens 1982 and 6 pan evaporation methods e g allen et al 1998 the practical value of pan evaporation ep has been proven and as such its application with empirical coefficients relating ep to et0 has been widely used for 10 or longer days periods allen et al 1998 measurements from ep provide combined effects of solar radiation wind speed temperature and humidity on the et0 a reasonable accuracy could successfully be achieved by these measurements for et0 estimations irmak et al 2002 as shown by numerous studies when ep is properly maintained it is expected to see higher correlation between ep and et0 khoob 2008 although physical based and conceptual models are reliable tools for investigating the actual physics of a phenomenon they have practical limitations and when accurate predictions are more important than the physical understanding employing black box models can be more useful multilinear regression mlr is a conventional method for modeling linear relationship between one or more independent and dependent variables tabari et al 2012 these kind of models which are basically linear lose their merit toward modeling processes in many fields that are embedded with high complexity dynamism and nonlinearity in both spatial and temporal scales in recent years application of artificial intelligence ai for examples artificial neural network ann adaptive neuro fuzzy inference system anfis and support vector regression svr are widely adopted through which several research papers were published by offering a new approach for lagged et0 data based modeling mehdizadeh 2018 applied ai methods to estimate daily et0 for urmia and tabriz semi arid isfahan and shiraz arid and yazd and zahedan hyper arid climates of iran sanikhani et al 2019 performed temperature based modeling of et0 for some selected stations in the mediterranean region of turkey using several ai models in different modeling scenarios for comparison ann model was developed by kumar et al 2002 to calculate et0 using limited number of climate data sudheer et al 2003 applied ann to model et0 kisi 2006 estimated daily et0 by applying both feed forward and generalized regression neural networks ffnn and grnn ekhmaj 2012 applied ann to predict et0 in two stations located in coastal area of western libya kisi and kilic 2016 investigated the generalization ability of ann and m5 model tree in modeling et0 antonopoulos and antonopoulos 2017 employed ann and empirical equations for daily et0 estimation using limited input climate variables also some researches were conducted to model et0 using anfis including kisi and oztürk 2007 study potentials of modeling daily et0 from limited climate data under arid conditions were investigated using anfis by karimaldini et al 2011 dou and yang 2018 performed a study to investigate the feasibility and effectiveness of anfis and extreme learning machine elm to estimate daily et with flux tower observation mosavi and edalatifar 2018 applied anfis to predict et0 in two cities of iran mohammadrezapour et al 2019 compared the performances of svm anfis and gene expression programming gep for monthly potential evapotranspiration modeling in arid region case study sistan and baluchestan province iran amiri et al 2019 investigated the capability of fuzzy regression method to estimate et0 under controlled environment in iran for the svr model based on the support vector machine svm concept as other type of ai method few studies might be found in technical literature tabari et al 2012 applied svm anfis regression and climate based models for et0 modeling using limited climatic data in a semi arid highland region wen et al 2015 evaluated the application of svm using combination of solar radiation wind speed minimum and maximum air temperature to model daily et0 in the ejina basin an extremely arid region in china mehdizadeh et al 2017 used svm gep multivariate adaptive regression splines mars and empirical equations for estimation of monthly mean et0 in iran yin et al 2017 modeled daily et0 in a semi arid mountain area by integrating genetic algorithm and svm using limited meteorological data fan et al 2018 evaluated the performances of svm elm and four tree based ensemble models for predicting daily et0 ferreira et al 2019 devised a new approach using ann and svm with limited meteorological data to estimate et0 although such black box models e g ann anfis svr and mlr may lead to quite reliable results it is well known issue that for a particular problem different models may lead to different outcomes therefore as indicated by bates and granger 1969 seminal work an ensemble of different approaches would result in minimal error variance compared to individual technique or approach working in solitary mode also makridakis et al 1982 disclosed that it has become a common practice to combine several single models for the improvement of forecasting accuracy the notion behind ensemble modeling is to accomplish unique features for the constituent models to bring about different patterns presented in the dataset kiran and ravi 2008 sharghi et al 2018 in the context of et0 modeling in particular and hydro climatic processes in general based on the knowledge of the authors so far no study was conducted utilizing ai based techniques and empirical models to perform ensemble modeling hence the main objective of this study is to apply the ensemble concept to model et0 in four stations in turkey adana ankara izmir and samsun four stations in north cyprus nc famagusta kyrenia morphou and nicosia two stations in iraq erbil and salahaddin two stations in iran tabriz and urmia and two stations in libya sabha and tripoli and to investigate the effect of different climate conditions on et0 modeling this was done in three steps firstly conducting input parameter analysis to determine the influence of each meteorological parameter to model et0 secondly ffnn an ai model that is commonly used anfis an ai model that is capable of handling uncertainty of process through fuzzy concept svr an ai model that is more recently used hargreaves and samani hs modified hargreaves and samani mhs makkink mk ritchie rt and mlr a linear conventional method models were applied for the modeling and compared to et0 computed by ep method finally for the improvement of the modeling performance outputs of the single models were used as inputs to obtain the final ensemble outputs ensemble models using strategy 1 for black box models ffnn anfis svr mlr and strategy 2 for empirical models hs mhs mk rt neural ensemble ne weighted averaging wa and simple averaging sa were used to form the ensemble models 2 materials and method 2 1 study location and data this study involves et0 modeling in 14 stations in different climatic regions across 5 countries including turkey nc iraq iran and libya hence the descriptions of the study locations are provided based on countries where the stations are situated 2 1 1 turkey stations adana station falls in the mediterranean med region which is humid and semi humid subtropical with a hot dry summer and heavy rainy and cool winter ankara station is in continental central anatolia ccan which is semi arid steppe that has light rainy and warm summer cold rainy winter and cool rainy spring izmir station shares same region with adana samsun station is in the black sea region bls a temperate climate which is uniformly rainy with its peak in autumn table a 1 shows data statistics location and coordinates of the study stations in turkey turkes 1996 as seen in table a 1 the maximum daily monthly average pr in adana and izmir is almost twice that of ankara and samsun this is because adana and izmir have climate of med region that is partly humid and semi humid ankara is located in ccan region which is semi arid climate while samsun has temperate bls climate however precipitation decreases with increase in aridity of a region that is why precipitation in samsun is a bit higher than in ankara meanwhile it can also be seen in table a 1 that tmax and tmin values in samsun station fall between the ranges of other stations under study which is because of the moderate nature of bls climate 2 1 2 north cyprus nc stations data from four stations in nc were used in this study including famagusta kyrenia morphou and nicosia see fig 1 the climate of famagusta is categorized as warm and temperate the average temperature is 19 3 c with a yearly average rainfall of 407 mm kyrenia has an average temperature of 29 c in the month of july hottest month and 10 c in january coldest month average rainfall in hilly areas ranges between 500 mm and 750 mm falling rarely in summer and particularly in winter owing to altitude kyrenia mountains witness the highest rainfall range as it ranges between 750 mm and 1110 mm the climate of morphou is classified as local steppe climate morphou experiences little rainfall throughout the year with precipitation falls of about 363 mm annually and an average annual temperature of 18 5 c nicosia is an inland city as such the effect of the mediterranean sea is less in nicosia than other coastal cities hence hotter summers and colder winters are experienced in nicosia than on the coastal areas also difference between night minimum and day maximum temperatures are larger for the hottest months july and august the difference in day time temperatures is about 4 c to 7 c between nicosia and other cities along the coastline in the coldest months january and february the daytime temperature difference is 2 c to 3 c in nicosia less than on the coast elkiran and ongul 2009 table a 2 shows data statistics location and coordinates of the study stations in nc being mediterranean climate which has relatively cool or mild winter and dry hot summer the temperatures were found to be as low high and medium as shown in table a 2 the minimum values of pan evaporation are 1 1 mm day for famagusta 0 9 mm day for kyrenia 1 11 mm day for morphou and 1 10 mm day for nicosia while its maximum value is as high as 12 6 mm day for nicosia relative humidity values are narrowly varied across all stations dew point temperatures range from lowest 2 86 c to the highest 22 48 c 2 1 3 iraq stations erbil is kurdistan s capital city located in northern region of iraq erbil is located within a semi arid and continental climate it experiences dry warm summers and rainy cool winters rasul et al 2015 salahaddin is located in the far north of iraq in the kurdistan region according to study by sarlak and agha 2018 salahaddin climate is semi arid table a 3 shows data statistics location and coordinates of the study stations in iraq with their vulnerability to semi arid climate condition erbil and salahaddin stations have almost similar statistics as seen in table a 3 salahaddin is more humid which gives rise to higher relative humidity than erbil the tmax in salahaddin station is also lower and can be as low as 0 c and as high as 39 9 c 2 1 4 iran stations tabriz city is situated in northwestern iran between latitudes 38 8 n and 46 15 e it is located at the junction of aji and quri rivers at an altitude of 1350 m tabriz has an annual precipitation of about 380 mm it enjoys dry and semi hot summer fine and mild climate in spring average temperature is around 13 c annually with potential evapotranspiration rate estimated around 1733 mm year urmia is a northwestern iran city located at latitude 37 34 n and longitude 44 58 e total days of freezing in urmia are about 120 days with a very scarce precipitation in the summer and heavy downfall in late autumn and winter the station is characterized by annual rainfall of around 300 mm year nourani and fard 2016 table a 4 shows the descriptive statistics of the data from tabriz and urmia stations higher altitude means faster reception of solar irradiance to the earth surface which causes rise in temperature and eventually increase in evapotranspiration this is why both maximum temperature and maximum pan evaporation are higher for tabriz station 35 15 c and 15 33 mm day than urmia 33 90 c and 10 96 mm day however presence of urmia lake increases the evaporation bodies of the station and consequently received more precipitation amount in maximum and average 3 74 mm day and 0 67 mm day than tabriz 2 91 mm day and 0 59 mm day 2 1 5 libya stations sabha is an inland station located in southern part of libya at 27 04 n latitude and 14 43 e longitude it has dry arid climate with annual average precipitation not exceeding 100 mm year 26 5 c annual average temperature and 52 mean annual relative humidity tripoli is mediterranean coastal city and capital of libya it has a semi arid climate which constitutes rainy winters and hot dry summers its annual rainfall ranges between 140 and 550 mm year 14 2 21 6 c annual air temperature and 70 mean annual relative humidity ageena 2013 the descriptive statistics of the study data are shown in table a 5 being distinct climate stations distinct characteristics of the meteorological parameters are observed as seen in table a 5 as arid climate possesses extreme weather condition sabha station has higher temperature less precipitation and higher evaporation 41 95 c 1 42 mm day and 24 70 mm day than tripoli station 36 21 c 4 79 mm day and 14 10 mm day fig 1 shows the regions and stations used in this study the data used in this study were obtained from turkish meteorological organization tmo for turkey stations north cyprus meteorological organization for cyprus stations kurdistan meteorological organization for iraq stations iran meteorological organization and libya national meteorological centre for model validation a stratified k fold cross validation method was used in the k fold cross validation method the original set is partitioned randomly into number of k subsamples of equal size of the k subsamples k 1 subsamples are used as training data while the remaining single subsample is used as validation data for testing the model the cross validation process is then repeated k times with the model been validated once by each of the k subsample single estimation result is then produced by averaging or otherwise combining the k results from the folds the k fold cross validation method has advantage that each observation is used for validation exactly once and the entire observations are used for both training and validation sharma et al 2018 in this study the data samples from the 14 stations were partitioned into k 4 random subsamples as shown in fig 2 each random subsample was obtained by dividing total sample size by k 4 folds as seen in table 1 the model was tested by single subsample while training was carried out by the k 1 4 1 random subsamples the process was repeated by the number of k fold subsamples 4 times for different single test subsamples and different k 1 training subsamples the schematic illustration of the 4 fold cross validation used in this study is shown in fig 2 while table 1 shows the duration of the used datasets usually in ai based modeling the data are normalized to ensure that equal attention is received by all variables and to eliminate their dimensions during model training nourani et al 2012 there are two primary advantages of data normalization before the application of ai based predictions the first is the avoidance of using attributes in bigger numeric ranges that overshadow those in smaller numeric ranges second advantage is the avoidance of numerical difficulties while calculation in this study the data were scaled within 0 to 1 to analyze and determine the performance and efficiency of the models proposed legates and mccabe 1999 research is endorsed which stated that determination coefficient dc or nash sutcliffe efficiency criterion eq 1 and root mean square error rmse eq 2 can sufficiently evaluate any hydro climatic model 1 dc 1 i 1 n r i r i 2 i 1 n r i r 2 2 rmse i 1 n r i r i 2 n where n r i r and r i are respectively the number of observations observed data mean of the observed values and predicted values the accuracy of the forecasted values is measured by rmse which gives positive value by squaring the error as divergence increasingly becomes large between observations and forecasts the rmse increases for perfect forecasts from zero through large positive values understandably dc dc 1 with high value up to 1 and rmse value close to 0 imply high model efficiency 2 2 proposed methodology in this study sensitivity analysis was applied to determine the most appropriate input parameters to the models ffnn anfis svr and mlr models were then created trained and validated separately for et0 modeling two temperature based hs and mhs and two radiation based mk and rt models were also applied to estimate et0 the proposed ensemble approaches were applied via strategies 1 and 2 for black box and empirical models using the outputs of the single models 2 2 1 strategy 1 in the first ensemble strategy the et0 is simulated as a function of the et0 outputs of the black box models ffnn anfis svr and mlr as 3 et 0 f et ffnn et anfis et svr et mlr 2 2 2 strategy 2 in the second ensemble strategy the outputs of the four empirical models are used as inputs to the ensemble approach as 4 et 0 f et hs et mhs et mk et rt the primary idea behind ensemble modeling is due to the following prospects sharghi et al 2018 i mostly it is difficult in practice to ascertain if the underlying process under study for a time series is generated by a non linear or linear phenomenon or whether one model or method in particular surpasses others therefore a difficult task before predictors is choosing the befitting method or technique for a unique issue hence application of ensemble strategy can handle the problem of selecting appropriate model ii for a real world process time series may involve both non linear and linear characteristics in such situation neither ai nor mlr could be sufficient for the time series prediction since errors of a linear pattern could be magnified by ai models and mlr model cannot cope with non linear relationship thus by combining ai and mlr models complex underlying nature of the data could be captured more accurately iii there is no particular or unique method that can perfectly investigate process as approved by previous studies including zhang 2003 sharghi et al 2018 this could be attributed largely to the complex nature of real world problem whereby a unique model may not be able to detect distinct patterns of process fig 3 shows the proposed methodology in this study the proposed methodology is applied to the data from all 14 stations to have an overall comparison regarding the computed et0 over different regions in turkey nc iraq iran and libya in fig 3 rh sp pr td tmax tmin tmean umin umax umean rs and ep are defined in tables a 1 a 5 etffnn etanfis etsvr etmlr eths etmhs etmk and etrt given in eqs 3 and 4 present predicted et0 by ffnn anfis svr mlr hs mhs mk and rt models respectively while etsa etwa and etne are ensemble et0 obtained by sa wa and ne techniques 2 3 artificial neural network ann ann presents convincing approach for handling huge amount of noisy nonlinear and dynamic data particularly when not fully understood the underlying physical relationships this makes ann appropriate to data driven nature of a time series modeling nourani et al 2015 a ffnn trained with back propagation bp algorithm are the most famous methods used among the neural networks applied in solving several engineering problems hornik et al 1989 among the several training methods levenberg marquardt lm training algorithm was chosen in this research in consideration of its ability to converge fast as stated by sahoo et al 2005 moreover tangent sigmoid tansig transfer function was applied for both hidden and output layers likewise the calibration epochs and hidden layer nodes were determined by trial and error process 2 4 adaptive neuro fuzzy inference system anfis neuro fuzzy simulation points to the techniques of applying different learning algorithm to fuzzy modeling in the neural network literature or fuzzy inference system fis akrami et al 2014 a distinctive approach in the development of neuro fuzzy is anfis which was first introduced by jang 1993 and utilize the learning algorithm of nn as a universal approximator anfis is capable of compacting set of accuracy to any degree for any real continuous function according to jang et al 1997 functionally anfis is equivalent to fis precisely the interest of the anfis system is functionally equivalent here to the first order sugeno fuzzy model jang et al 1997 the anfis general structure is given in b 2 as seen in b 2 it is considered that the anfis has x and y inputs and ƒ output according to aqil et al 2007 the sugeno first order fuzzy model has an ideal rule sets which are two fuzzy if then rules and are given by 5 r u l e 1 i f μ x i s a 1 a n d μ y i s b 1 t h e f 1 p 1 x q 1 y r 1 6 r u l e 2 i f μ x i s a 2 a n d μ y i s b 2 t h e f 2 p 2 x q 2 y r 2 in which a 1 and a 2 are x inputs mfs b 1 and b 2 are y inputs mfs respectively while the output function parameters are p 1 q 1 r 1 and p 2 q 2 r 2 2 5 support vector regression svr svr developed on the basis of svm concept is used for non linear regression issues unlike many other black box forecasting methods svm based methods such as svr instead of minimizing the error between the observed and computed values consider operational risk as the objective function to be minimized in svr first a linear regression is fitted on the data and then the outputs go through a non linear kernel to catch the non linear pattern of the data given a set of training data x i d i i n xi is the input vector di is the actual value and n is the total number of data patterns the general svr function is wang et al 2013 7 y f x w φ x i b where φ xi indicates feature spaces non linearly mapped from input vector x vapnik 1998 2 6 multi linear regression in general for multi linear regression mlr the dependent variable y and n regressor variables may be related by elkiran et al 2018 8 y b 0 b 1 x 1 b 2 x 2 b 3 x 3 b i x i ξ where x i is the value of the i th predictor b 0 is the regression constant and b i is the coefficient of the i th predictor and ξ is the error term detail information regarding the applied black box models can be found in sections b 1 b 4 in the appendices 2 7 empirical equations the empirical equations used in this study are shown in table 2 in table 2 k p is a pan coefficient which ranges between 0 3 and 1 1 and it is inversely proportional to wind speed and directly proportional to relative humidity heydari and heydari 2014 to compute kp cuenca approach cuenca 1989 was applied in this study owing to its performance and application in many studies snyder et al 2005 heydari and heydari 2014 as 9 k p 0 475 2 4 10 4 u 2 5 16 10 3 r h 1 18 10 3 f 1 6 10 5 r h 2 1 01 10 6 f 2 8 10 9 r h u 2 1 0 10 8 r h 2 f γ is the psychrometric constant kpa c 1 δ is the slope of the vapor pressure curve kpa c 1 r s u 2 r h t min t mean and t max previously defined in table a 1 f is fetch distance green crop windward side distance and α 1 have the following conditions 5 t max 35 c α 1 1 1 t max 35 c α 1 1 1 0 05 t max 35 t max 5 c α 1 0 01 exp 0 18 t max 20 2 8 ensemble modeling for a given set of data it is obvious that performance of one intelligent technique may surpass another and when different sets of data are used the results may entirely be opposite in order to benefit from the advantages of all techniques and also not to lose generality an ensemble model is developed which utilizes the single output of each technique with certain priority level assigned to each with the help of an arbitrator to provide the output kiran and ravi 2008 some ensemble techniques for prediction problems comprised of linear ensemble such as stack regression breiman 1996 weighted average perrone and cooper 1993 simple average benediktsson et al 1997 and nonlinear ensemble such as neural network based yu et al 2005 according to kiran and ravi 2008 there are two ensemble methods i linear ensemble method which includes linear ensemble by simple averaging linear ensemble by weighted averaging and linear ensemble by weighted median ii nonlinear ensemble method e g an ann is trained to obtain an ensemble output the ensemble modeling in this study was conducted via two linear simple and weighted averaging and one non linear ffnn ensemble methods simple averaging sa is done as 10 et 1 n i 1 n et i where et is output of simple ensemble model et i is the output of ith single model here outputs of ffnn anfis svr and mlr and n is the number of single models here n 4 the weighted averaging wa ensemble is expressed as 11 et i 1 n w i e t i where wi is the applied weight on ith model which can be determined based on the model performance as 12 w i d c i i 1 n d c i dci is the performance efficiency e g determination coefficient of ith single model the procedure applied for developing the ensemble models is given in fig 4 in the non linear ensemble ne the individual outputs of ffnn anfis svr and mlr are combined together as inputs to a new non linear model here an ffnn method to obtain the overall ensemble output similar to the case of single modeling for model validation the data samples were partitioned into 4 folds subsamples with k 1 used as training subsamples and the remaining subsample was used for model validation the process was repeated until each subsample was used once as validation dataset the general ensemble procedure is demonstrated in fig 4 2 9 validating the meteorological parameters quality control procedures were applied to determine from observations the erroneous and suspect data firstly verification is necessary to ensure that correct and complete record structure is obtained and all possible data have been collected gaps detected in the data files would be flagged as erroneous and should not be used as input variable in the estimations of et0 the methods used for quality assurance of the meteorological parameters include internal consistency test step test persistence test and range fixed or dynamic test estévez et al 2011 2016 in this study range fixed test procedure was applied because of its ability to detect erroneous data data outside acceptable fixed range table 3 shows the applied range test procedures for data quality control of the variables used 3 results and discussion the proposed methodology in this study contains 4 steps i application of classic empirical methods ii neural network based analysis focusing on the effect of each variable on et0 iii application of artificial intelligence ai based non linear and linear mlr models single modeling finally iv the results of ensemble models are presented in 2 strategies via two linear and one non linear approaches to appraise the improvement in performance that could be attained over the single models therefore the obtained results are also provided at different sub sections to ensure that the input variables to be used for the et0 simulations are valid and of accepted quality quality assurance procedures were applied to the input variables in order to identify erroneous values the results obtained from the fixed range test conducted showed no flagged erroneous data identified implying that the variables are within the fixed limits given in table 3 this can also be affirmed by the descriptive statistics of the data for all stations given in tables a 1 a 5 in the appendices section 3 1 results of the empirical models for performance evaluation of the climate based models et0 values computed by ep method were used as the basis of comparison with the et0 computed by the four climate based models the results of the empirical temperature based and radiation based models across all the 14 stations are shown in table 4 it should be noted that the results of the empirical models in this study were compared with those of ai based models as such in order to have accurate basis for comparison the former results were calibrated on the basis of 4 folds cross validation as the latter as seen in table 4 the performances of all models are appreciable in most of the stations meaning that reliable estimates of et0 could be achieved by empirical models in most of the study locations though for similar weather conditions the performances of the models are analogous nevertheless it is obvious that one model performance could be better than another mostly due to the condition climate under which the models are originally derived this brings about the best and least performing models for turkey stations the performances of the empirical models in adana and izmir stations are a bit better than in ankara and samsun stations in terms of higher dc and lower rmse this could be because the med climate receives higher precipitation and according to the concept of water cycle condensation and precipitation take place due to frequent evapotranspiration this indicates that with higher precipitation frequency evapotranspiration is easier to be estimated by empirical models the results provided by empirical models in ankara station also surpassed those obtained from samsun station because ankara station is located in ccan which is a semi arid climate environment while samsun is in bls temperate climate and most of the empirical models are developed on the basis of prediction in an extreme climate such as arid and semi arid in order to have proper management of the available water resources hence the empirical models may not have accurate predictions in the stations that are not subjected to such climate conditions comparing the models performances it can be said that temperature based models generally have better prediction capability than radiation based models and mhs models performed better than hs models the temperature based models increased prediction of radiation based models in the validation step up to 42 in adana station 14 in ankara station and 3 in izmir station whereas an increment of 16 is achieved in samsun region over temperature based models by radiation based models for cyprus stations mhs model was found to have better performance in famagusta station with dc 0 8245 rmse 0 1274 in training phase and dc 0 7941 and rmse 0 1232 in the validation phase hs model followed closely with dc 0 7727 and rmse 0 2083 in the validation phase in kyrenia station radiation based models were found to have better performances rt model was found to overestimate et0 with dc 0 8092 and rmse 0 1172 in the validation phase the better performances of the radiation based models could be due to the rocks surrounding the location which may result in easier evaporation and transpiration also atmospheric conditions such as extraterrestrial radiation and saturation vapor pressure slope could be more effective in this location kyrenia as they can easily reach to the earth surface unlike for a flat surrounding like nicosia in morphou however the correlation between et0 values computed by ep method and those computed by radiation based methods is slightly higher than when compared to et0 values computed by temperature based methods here rt produced the highest simulated values in the validation with dc 0 7391 rmse 0 1546 for nicosia station mhs has the best performance in the validation phase dc 0 8035 rmse 0 1152 based upon the results in table 4 temperature based models are superior in performance in the training phases for all stations perhaps due to the suitability of the indicators used in this study however in the validation phases better correlation is obtained for famagusta and nicosia by the temperature based models while radiation based models have better performances for kyrenia and morphou locations hence both temperature and radiation based models are capable of estimating et0 with certain degree of accuracy however as revealed in table 4 the performances of the models are identical across the four stations which could be because the stations share similar semi arid mediterranean climate of mild cold winter and hot dry summer the main difference between their climates is the station upon which they based that is for a coastal area the temperature will be little bit moderate than in inland area also hilly and valley areas may influence the climate and eventually prediction of et0 this could be why the radiation based models performances are higher in kyrenia than all other stations for iraq stations the models used in erbil station produced higher estimates of et0 values but lower estimations were achieved by the empirical models in salahaddin station although salahaddin has been classified as semi arid climate station study by sarlak and agha 2018 shows that the station s aridity varies with time and aridity index defined as the ratio of mean annual potential evaporation to precipitation ranjbar et al 2018 applied for its investigation for instance using unep 1992 aridity index the station was found to be sub humid between 1980 and 2011 sub humid between 1980 and 1997 and semi arid between 1998 and 2011 the inconsistency of the climate of the station makes et0 difficult to be estimated by empirical models as the long time data record contains elements of different climatic regions the empirical models fail to provide accurate estimations this is why the applied models showed the least performance for salahaddin station compared to the other stations table 4 also demonstrates the performances of the empirical models for tabriz and urmia stations of iran the results show that both temperature and radiation based empirical models could be employed successfully for et0 estimation in the stations this is because the temperatures tmax tmean and tmin and rs parameters used as inputs to the models do not have much deviation from the mean which means less data diversification and hence formed strong bond with the target output in both stations mhs shows promising performance over all other empirical models which has performance increment of about 5 2 and 3 for tabriz station and 2 2 and 3 for urmia station over hs mk and rt models in the validation phase this justifies its modification from original hs model for libya stations different outcomes can be seen in table 4 from two stations both temperature and radiation based models performed better for sabha than tripoli stations despite the arid nature of sabha climate this could be because tripoli is a densely populated city which contains about 1 million of the 6 million libyan population high population may lead to increase in industrialization and human activities which can cause environmental degradation and release of toxic gases to the atmosphere this can have a profound effect on et and thereby making et0 difficult to be estimated by physical methods the scatter plots for the best hs model erbil station mhs model adana station mk model izmir station and rt model izmir station in the validation phase are shown in fig 5 in general the temperature based empirical models produced better predictions than radiation based models any of the models could be applied to achieve valuable results in regions similar to that of this study and where only temperature data are available temperature based model can be employed successfully 3 2 sensitivity analysis results a non dimensional sensitivity analysis has been already applied to determine the effect of change of meteorological variables on the increase or decrease of et0 see beven 1979 estévez et al 2009 also some previous studies show that ai based single input single output sensitivity analysis may also be used to assess the influence of each meteorological variable on et0 for effective et0 modeling involving ai techniques for examples jain et al 2008 performed sensitivity analysis using ann for estimation of et0 doğan 2009 applied anfis to determine the effect of each meteorological variable on the benchmark et0 wang et al 2011 applied sensitivity of meteorological variables through ann to model et0 in the arid regions of africa eslamian et al 2012 used ann to determine the most effective parameters to model et0 petković et al 2015 determined the most influential weather parameters on et0 using anfis consequently a single input single output neural network based input sensitivity analysis was applied in this study in order to identify the key input parameters for the et0 modeling over the selected stations the et0 value is expressed as a function of each parameter and then the models were trained and verified the results are presented in table 5 for all the 12 parameters in this study table 5 shows the importance of the potential input parameters on the output et0 in the validation phase rmse was used in determining the effectiveness of the parameters while lowest error implying the most dominant parameter according to the results in table 5 for turkey stations ep rs and tmax ep tmean and rs ep rs and tmean and ep tmean and tmax are the most effective parameters for et0 modeling in adana ankara izmir and samsun stations respectively on the other hand for nc stations rs ep and sp are the 1st 2nd and 3rd most influential parameters in famagusta station for both training and validation phases temperatures tmean tmax and tmin are the 4th 5th and 6th most effective parameters for et0 in famagusta station due to the hot climate of the station and less precipitation the amount of water vapor in the air is minor which could be why td and pr are 7th and 8th in famagusta station rh would be more effective in the humid regions owing to higher moisture amount in air hence with increasing aridity index air moisture amount would be limited and rh effects will be reduced this might be the reason why rh is in the 9th position umin umean and umax are the least effective parameters to estimate et0 in the famagusta station for kyrenia being a coastal city the effects of parameters are similar to those of famagusta station but the influence of mountains surrounding the area for kyrenia city results in easier evaporation and precipitation which in turn results in higher precipitation thus pr is the 5th most dominant parameter in morphou station the effect of parameters on et0 estimation is in descending order of rs ep sp tmean tmax pr rh td tmin umax umin and umean for nicosia station with location similarity to morphou station the effects of the parameters on et0 are similar as well for the stations located in kurdistan s region of northern iraq the best 3 parameters that are most effective in et0 modeling for erbil are ep tmean and rs whereas for salahaddin station the dominant parameters are ep tmin and rs for tabriz and urmia stations of iran the 3 most dominant parameters are ep tmax and tmin with rmse of 0 0107 0 0515 and 0 0641 for tabriz station and ep rs and tmean with rmse of 0 0181 0 1104 and 1265 for urmia station for sabha station which is arid climate rise in temperature results in rise in et0 therefore tmean tmax tmin and ep are the 4 most influential parameters as a coastal station that receives more precipitation at a lower elevation above mean sea level rh and sp are among the 4 most effective parameters to et0 for tripoli station 3 3 results of the black box models ann anfis svr and mlr in this section the results of three ai based techniques ffnn anfis and svr and one conventional technique mlr are presented for et0 estimation for 14 different stations of turkey nc iraq iran and libya using different input combinations based upon the results of the input selection levenberg marquardt algorithm was used to train the ffnn with single hidden layer and varying number of hidden neurons for et0 simulation the hidden layer optimal node number was determined using trial and error procedure for each region accordingly the number of nodes in the hidden layer that provided the best results were found to be 7 9 6 11 10 9 12 11 8 10 9 7 14 and 12 for adana ankara izmir samsun famagusta kyrenia morphou nicosia erbil salahaddin tabriz urmia sabha and tripoli respectively anfis model which used sugeno type fuzzy inference algorithm was applied in this study where the membership function parameters were calibrated by a set of given input output data via hybrid optimization algorithm trial and error procedure was applied for the formulation of the structures of the anfis models in order to find the best anfis construction triangular trapezoidal and gaussian shaped membership functions mfs were found to be sufficient across all stations for the et0 simulation while modification of training epoch was examined to determine the number of mfs that will bring about the most optimum performance for svr models rbf kernel was used to create the models for all stations the tuning parameters of the rbf kernel are fewer than polynomial and two sigmoid kernels also considering smoothness assumptions the rbf kernel shows better performance in svr modeling sharghi et al 2018 to achieve the best performance efficiency and reliability for et0 estimations in each station the rbf kernel s parameters in svr were tuned finally mlr which expresses linearly the relationship between independent and dependent parameters was used in this study as well to ensure efficiency of performance each model is subjected to 3 4 and 5 different input combinations from the best 5 most effective parameters to et0 estimation as obtained from the input parameters analysis the results showed varied outcomes in some stations 3 inputs models have higher simulation reliability than models with 4 inputs while in some 4 inputs are better but generally 5 and 3 inputs models produced the highest number of best models performances hence the models were constructed according to the number of inputs parameters that provided the best performances therefore the input combinations with respect to the study stations for the et0 simulations are expressed as 13 et 0 ad f e p ad r s ad t max ad 14 et 0 an f e p an t mean an r s an t max an 15 et 0 iz f e p iz r s iz t mean iz 16 et 0 sa f e p sa t mean sa t max sa t min sa r s sa 17 et 0 fa f r s fa e p fa t mean fa t max fa s p fa 18 et 0 ky f r s ky e p ky t mean ky p r ky s p ky 19 et 0 mo f r s mo e p mo t mean mo t max mo s p mo 20 et 0 ni f r s ni e p ni t mean ni t min ni s p ni 21 et 0 er f e p er t mean er r s er 22 et 0 sl f e p sl t min sl r s sl t max sl t mean sl 23 et 0 ta f e p ta t max ta t min ta 24 et 0 ur f e p ur t mean ur r s ur 25 et 0 sh f e p sh t min sh t mean sh 26 et 0 tr f e p tr r h tr s p tr t max tr where et 0 ad et 0 an et 0 iz et 0 sa et 0 fa et 0 ky et 0 mo et 0 ni et 0 er et 0 sl et 0 ta et 0 ur et 0 sh and et 0 tr are et0 at adana ankara izmir samsun famagusta kyrenia morphou nicosia erbil salahaddin tabriz urmia sabha and tripoli stations respectively and r s e p t mean t max s p p r r h and t min defined as in table a 1 the results of the ffnn anfis svr and mlr models for all stations are presented in table 6 it should be noted that the results are given for the best output models only the ffnn structure numbering x y z implies the number of input parameters hidden layer neurons and number of output for anfis structure mf x represents membership function type and number of membership functions for svr rbf implies radial basis function used in the svr construction the mlr structure x y refers to number of inputs and output respectively table 6 depicts different performances of the models among all the applied models ai based models were found to have higher efficiency and reliability than the linear models for stations located in turkey both ai and mlr models provided good estimation of et0 with exception of samsun station where a fairly performance was obtained by mlr model with dc 0 5168 and rmse 0 1507 this shows that the bls temperate climate of samsun station negatively affects the performances of both empirical and mlr models but ai based models can overcome the threat caused by climate of the region as they produced reliable results due to their robustness and ability to deal with uncertain behavior of the climate however despite their ability to provide good results in bls climate region the ai based models performances in samsun station are inferior to their performances in other stations this implies that the ai based models can produce reliable results but not at the peak in such region it is apparent from the result shown in table 6 that all the performances of the applied models in adana and izmir stations med region humid and semi humid climate performed better than those in ankara station ccan region a semi arid climate by comparing tables 4 and 6 it can be seen that ai based models are superior in performances than the other models in all modeling phases training and validation the better performance of the black box models might be due to i inclusion of pan evaporation as their input which was also used as the benchmark output and ii their ability to deal with nonlinear phenomena empirical and mlr models performances differ across the stations with some empirical models having an upper hand over mlr models this is because the temperatures tmin tmax tmean and rs that are used as inputs to the empirical models are the dominant forefront parameters for the estimation of et0 in those stations which allowed empirical models to have accurate and successful et0 estimations than mlr model in most stations the overall results for turkey stations show that climate of the regions affects et0 modeling in the regions with best performance in med region followed by ccan region and lastly bls region for nc stations the mlr models in the training phases to estimate et0 led to acceptable values with respect to statistical indicators but mlr models could not give much reliable performances in the validation phase with the exception of kyrenia which got dc 0 7085 and rmse 0 1419 the better performance for kyrenia could be due to the effectiveness of the input combination used for the location which has an inclusion of precipitation because of its dominance for the et0 estimation in the area the inability of mlr models to produce good simulation beyond training steps could be as a result of their failure to deal with nonlinear complex process all the ai based models produced promising performances in the study stations because of their ability to handle complex phenomenon such as et0 process anfis achieved higher predictions in the validation phase for famagusta station with dc 0 8285 rmse 0 1146 svr performed better in training phase of kyrenia but was inferior to ffnn by 6 in the validation phase morphou and nicosia also witnessed higher anfis predictions efficiency over ffnn and svr the efficient performance of the anfis could be because of the shortcomings of ann to deal with ambiguous or less accurate data despite its robustness of dealing with various real world problems as such anfis might be a better option due to the fuzzy logic ability to handle the uncertainty of the process moghaddamnia et al 2009 in the case of stations in iraq the results of the models in table 6 show that the efficiency of the et0 modeling is higher in erbil than in salahaddin which is as a result of the distinct nature of the climate between the two stations the aridity of salahaddin station changes with time and the applied aridity index while that of erbil station remains constant irrespective of the time and indices applied see sarlak and agha 2018 the irregular behavior of the climate in the station reduces the accuracy of the models predictions for iran stations similar to results of the empirical models but in improved form the performances of the ai models are identical due to climate similarity of the study stations like most stations anfis has a standout performance among other ai models in the validation phase for both stations ffnn has superior performance in tabriz station and inferior performance in urmia station over svr mlr model despite been developed based on the dominant inputs that have higher correlation to the bench mark output does not produced better estimates than the empirical models table 6 this is because the inputs to the empirical models despite been fixed are of strong agreement with the et0 output the better performances of the applied models in tabriz station are due to better convergence of the meteorological parameters toward the mean as seen in the descriptive statistics of the data shown in table a 4 by st deviation of the input parameters for sabha and tripoli stations of libya all the models performances are convincing and with uncertainties and problems solving nature of ai models the deficiencies of the underperformance empirical models at tripoli station have been significantly dealt with performance improvements up to 26 26 28 and 33 were achieved in the validation phase by svr the least ai performing model over hs mhs mk and rt models respectively the scatter plots for the best ffnn model adana station anfis model nicosia station svr model adana station and mlr model adana station across all the study stations in the validation phase are shown in fig 6 comparisons were made between the empirical ai based and mlr models to ascertain the performance of one model over another for all stations the results are presented in fig 7 based on the dcs and rmses of the validation phase as depicted by fig 7 across all stations ai based models in terms of higher dc and lower rmse outperformed all other applied models in this study the reasons for the promising performance of ai models could be owing to one or all of the following reasons a ai models are capable of dealing with complex and nonlinear process b the input combinations of the ai models were used based on merit implying that the input selection of the ai based models were due to their effectiveness in et0 estimation as derived from input selection step while the inputs for all other models with exception of mlr models were fixed regardless of their performances toward et0 modeling fig 8 a shows a time series in the validation phase of adana station for the black box models ffnn anfis svr and mlr in order to have proper visualization of the estimated et0 values by each model fig 8b is also plotted that contains only the 12 months period of the year 2014 january 2014 december 2014 as revealed in fig 8b four different points are randomly selected and numbered 1 2 3 and 4 which correspond to the months of february june august and october respectively considering point 1 observed 1 4 mm ffnn 1 5 mm anfis 1 6 mm svr 1 6 mm and mlr 2 4 mm this shows that ffnn is more close to the target than the rest models at point 2 observed 5 0 mm ffnn 5 3 mm anfis 5 0 mm svr 5 1 mm and mlr 5 6 mm this indicates that the agreement between observed and predicted values is higher using anfis model ffnn model which was the best in the month of february is at 3rd rank in june at point 3 observed 4 8 mm ffnn 5 5 mm anfis 5 5 mm svr 5 4 mm and mlr 4 4 mm this shows that mlr is less deviated from the observed value than the other models and svr is 2nd performing model at point 3 at point 4 almost all models performances converged together observed 2 6 mm ffnn 2 6 mm anfis 2 7 mm svr 2 6 mm and mlr 2 6 mm from the models outputs at these four points it can be deduced that different data aspects can be captured by different models in different ways at different time points hence amalgamation of models via ensemble approaches could enhance the capability of the model in more precisely capturing the target to this end two linear sa wa and one nonlinear ne ensemble techniques are applied in strategy 1 for black box models and strategy 2 for empirical models in the next section it should be noted that the ensemble modeling was performed in strategies 1 and 2 instead of combining all the models ffnn anfis svr mlr hs mhs mk and rt for the following reasons i we tried to assess the responses of both ai and empirical models to ensemble modeling if we combine the models all together we cannot know the influence of each technique towards ensemble performance ii to see if it s possible to use empirical models for performance improvement in case of data lack for ai based modeling and iii to see the difference in performance improvement between ensemble models derived from low performance single models and the improvement achieved from higher performance single models 3 4 results of the ensemble techniques 3 4 1 results of strategy 1 in strategy 1 the outputs of 3 ai based ffnn anfis svr and mlr models were used as inputs to the ensemble models to ensure that the higher performance is achieved three methods were applied in obtaining the weights for wa ensembles i using only dc of the training phase ii using only dc of the validation phase iii using both dcs of training and validation phases of the single models the weights generated by the 3rd method provided better simulation and hence applied in both strategies 1 and 2 similar to the single ffnn models the ne models were developed using lm training algorithm with tangent sigmoid as activation functions for both hidden and output layers the schematic diagram of the developed procedure for ne modeling is given in fig 9 trial and error procedure was applied to determine the best epoch number and structure number of hidden neurons of the ensemble network the choice of ann in this study as the nonlinear ensemble technique over other ai models was made due to its popularity compatibility and above all high reported performance by many ensemble modeling studies including yu et al 2005 kiran and ravi 2008 sharghi et al 2018 while other ai models may also be employed as the kernel of ensembling the obtained results for the ensemble models in strategy 1 are presented in table 7 for model structure a b represent number of inputs and output for sa w x y z imply the generated weights by ffnn anfis svr and mlr that were applied for wa and finally ffnn ensemble structure is same as explained previously for single ffnn models the results in table 7 indicate that ensemble modeling certainly improved accuracy of performance over single models the performances of the models improved up to 20 31 22 34 13 11 7 10 15 29 4 11 8 and 14 over mlr models in the training phases and 4 13 12 8 13 7 14 2 9 6 10 7 22 and 15 over svr models in the validation phases for adana ankara izmir samsun famagusta kyrenia morphou nicosia erbil salahaddin tabriz urmia sabha and tripoli stations respectively it is observed that not much improvement in dcs was attained in the training phase over ai models but remarkably higher performances were achieved over all models in the validation phase which was the primary focused area in this study as explained earlier at different points in time different behaviors of the data could be captured with underestimation and overestimation of et0 by different models and with unique capability of each model the underlying process could be simulated better than in case of single models the deduced ensemble results table 7 in this study show that the performances of the sa and wa are almost equal in most cases across the study stations this could be because of the linear direct relationship they shared with the single models the performances of the nonlinear ffnn ensemble ne models are far better than sa and wa ensemble models this could be because i ffnn uses nonlinear kernel to simulate the behavior of a system hence simulation by ffnn would yield better results than the other linear sa and wa methods ii the performances of single models may influence overall results of sa and wa models implying that poor performing models may result in lesser ensemble performances because of the direct relationship that the methods sa and wa share with the single models iii the errors generated by single models might be propagated and incorporated by sa and wa ensemble techniques due to the direct amalgamation of the single models it is observed from table 7 that for turkey stations the results for adana and izmir stations are better than those for ankara and samsun stations also ensemble results for ankara are superior to the results obtained for samsun station similarly for iraq stations ensemble model result for erbil station surpassed that obtained for salahaddin station this clarifies that the climate conditions at these stations not only affect the performance of the single models but also accuracy and efficiency of the ensemble models that are generated from the single models to compare the performances of the ensemble models for the 4 study stations in nc the dc values of the best performance single models were subtracted from the dc values of the strategy 1 ensemble models tables 6 and 7 the differences are 0 0912 0 0136 0 0073 and 0 0026 for famagusta kyrenia morphou and nicosia respectively this shows that the differences which indicate the best performing ensemble models are higher in the first 2 stations than in the last 2 stations this could be because the heat capacity of soil for inland stations is lower than that of water ocean for coastal stations implying that the ocean cools down and heats up relatively slowly and in contrast the land heats faster and cools faster the sudden cooling and heating of inland areas make et0 phenomenon difficult to predict thus ensemble modeling predicts et0 better in coastal stations famagusta and kyrenia than inland stations morphou and nicosia considering the improvement in performance achieved by ensemble model in the validation phase of tabriz and urmia 10 and 7 stations by examining the difference in performance between the best performed single models anfis of two stations it could be seen that anfis model for tabriz station is a bit superior to anfis model for urmia station this signifies that ensemble models increase prediction of single models according to single model performance by same amount for station under same climatological condition such as tabriz and urmia of iran s semi arid region for libya stations the 22 increment in performance for arid sabha station compared to 15 increment for semi arid mediterranean tripoli station means ensemble techniques are capable of delivering superb performance in an extreme climate 3 4 2 results of strategy 2 the strategy 2 of ensemble simulation was applied to the four empirical models hs mhs mk rt in this study to see how ensemble approaches could cope with less performing models when compared to ai models the modeling was performed via three ensemble approaches sa wa and ne same as those used in strategy 1 table 8 shows the results of strategy 2 of ensemble models table 8 demonstrates the capability of ensemble approaches to improve the performances of all models irrespective of their applied simulation method the performances of the models were improved up to 5 9 10 18 7 8 6 1 8 29 12 11 8 and 33 over mhs models in the training phases and 8 15 11 37 1 12 7 7 14 31 17 19 13 and 55 over same mhs models of validation phases for adana ankara izmir samsun famagusta kyrenia morphou nicosia erbil salahaddin tabriz urmia sabha and tripoli stations respectively it is observed that almost all the features of strategy 2 resemble those of strategy 1 ensemble techniques including the closeness of the results obtained by sa and wa models higher performance of ne over sa and wa ensembles etc which their reasons are discussed in strategy 1 this shows that these features are peculiar to ensemble models irrespective of the single models used because of the processes and methodologies followed in carrying out the three ensemble simulations it is observed that there is huge gap in performance between the two linear ensemble models sa and wa and ne model in the second ensemble strategy than in the first ensemble strategy this is because empirical models provided lower estimates of et0 in the single modeling ne model in the second ensemble modeling overcomes the obstacle posed by the lower estimations whereas the other ensemble models due to their linear behavior could only adapt the nature of the single models the result shows that the improvement in et0 modeling is higher in strategy 2 ensemble modeling up to 55 than in the strategy 1 maximum 22 in the verification phase but owing to the capability of ai based models to handle uncertainty of system strategy 1 ensemble remains superior to the strategy 2 this means that with low performance single models more room for improvement will be left to be filled by ensemble models but with high performance single models higher predictions would be achieved by ensemble models it is worth mentioning that by comparing the two ensemble strategies tables 7 and 8 strategy 1 outperformed strategy 2 in terms of higher dc and lower rmse in the similar manner ai models outperformed empirical models see tables 4 and 6 this implies that the ensemble model performance follows the trend and direction of the performance of single models that is to say with high performance of single models ensemble modeling yields better simulations while in contrast less accurate but improved performance ensemble modeling could be achieved from poor performance single models therefore for more efficient and accurate estimation of et0 ai based models are preferable over empirical models however obviously the ensemble of empirical models may be used in the case of data lack for ai based modeling fig 10 compares the performances of strategies 1 and 2 ensemble techniques in form of computed versus observed et0 time series for stations in turkey nc iraq iran and libya according to fig 10 it is apparent that ai based ensemble models are more accurate than empirical based ensemble models the sa wa and ne follow closely the fluctuations of the observed data whereas sa2 wa2 and ne2 are unable to have close correlation with the observed data 4 conclusions in this study et0 was simulated via several ai models ffnn anfis svr conventional mlr and empirical models hs mhs mk and rt for adana ankara izmir samsun famagusta kyrenia morphou nicosia erbil salahaddin tabriz urmia sabha and tripoli stations afterwards ensemble methods which combine the outputs of the single models were developed in strategy 1 for black box models and in strategy 2 for empirical models via simple average weighted average and neural ensemble techniques in the absence of lysemeter pan evaporation method of et0 is widely accepted in practical problems therefore the method was used as the basis upon which the performance of the models were accessed the input selection results showed that ep temperatures tmin tmax tmean and rs for turkey iraq and iran stations rs ep and sp for nc and tripoli stations and tmin tmax tmean and ep for sabha station are the most dominant parameters to estimate et0 the results showed that the performance of the empirical models differs with stations and climate of the region upon which the stations are based but generally the empirical models results could be acceptable for et0 estimations despite the presence of data nonlinearity that mlr model could not cope with the selection of the best inputs for the mlr models from input selection step could help in ensuring reliable results could be achieved by mlr models for et0 estimations in both training and validation phases among the ai models anfis showed better performance in most stations which could be due to the serve of both neural network learning ability and fuzzy concept in a unique framework the results demonstrated that better approximations in et0 modeling could be achieved by ensemble models and combination of different models for strategy 1 ensemble modeling improvements over single models were accomplished up to 4 13 12 8 13 7 14 2 9 6 10 7 22 15 and for strategy 2 up to 8 15 11 37 1 12 7 7 14 31 17 19 13 and 55 in the validation phase of modeling for adana ankara izmir samsun famagusta kyrenia morphou nicosia erbil salahaddin tabriz urmia sabha and tripoli stations respectively comparing strategies 1 and 2 ensemble models higher efficient performance was achieved by the former in similar manner that ai based models outperformed empirical models this implies that higher performing single models could yield better ensemble simulations though with lower performance single models more space for improvement could be created for ensemble models to fill but higher performance single models would lead to most accurate efficient and reliable ensemble modeling despite the fact that ai based ensemble results could outperform empirical based ensemble results still empirical models may be used according to the data availability and also their easy implementation without need to training compared to ai models but of course results with a little more error the general results of this study demonstrated promising impact of combining models for et0 estimation the obtained results from the ensemble methods more especially neural ensemble method implied that better accuracy in et0 simulation can be achieved by combined outputs than individual models therefore it is suggested to apply the proposed ensemble method for modeling other hydro climatic parameters and also to include other black box process based methods in the ensembling unit declaration of competing interest there is not any conflict of interest regarding this submission appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 123958 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6213,in this study different artificial intelligence ai techniques including feed forward neural network ffnn adaptive neuro fuzzy inference system anfis support vector regression svr empirical models including hargreaves and samani hs modified hargreaves and samani mhs makkink mk ritchie rt and conventional multilinear regression mlr were employed to model reference evapotranspiration et0 in fourteen stations from several climatic regions in turkey cyprus iraq iran and libya for this purpose 12 parameters of monthly climate data were collected and used as input parameters to the models the parameters were subjected to quality assurance tests to ensure their validity and acceptability the study was conducted in three sections i sensitivity analysis was conducted to determine the dominant inputs ii single models were trained and their performances were accessed on the basis of et0 derived from pan evaporation method iii finally three ensemble methods of simple averaging weighted averaging and neural ensemble were applied in strategy 1 for ai models and strategy 2 for empirical models to improve the predicting performance the results revealed that depending on the climate of the regions pan evaporation solar radiation temperatures and surface pressure are the most dominant parameters empirical and mlr models could be employed to achieve the valuable results ai based models are superior in performance to the other models also promising improvement in et0 modeling could be achieved by ensemble modeling the results of this study affirmed that the provided ensemble approaches can increase the performance of single models in the verification phase up to 22 for strategy 1 and 55 for strategy 2 also the results demonstrated that ai based ensemble modeling is preferable to empirical ensemble modeling keywords artificial intelligence pan evaporation sensitivity analysis feed forward neural network ensemble models reference evapotranspiration 1 introduction evapotranspiration et refers to the transfer of water from the surface of the earth to the atmosphere through evaporation from surfaces of wet plant water and soil and through plant stomata by transpiration odhiambo et al 2001 the reference surface of reference evapotranspiration or reference crop evapotranspiration is a hypothetical grass reference crop having an estimated crop depth of 0 12 m 70 sm 1 of a fixed surface resistant and 0 23 albedo allen et al 1998 water deficiency is becoming a major problem especially in arid and semi arid climatic regions this negatively affects irrigation scheduling which in turn results in a very low or no crop yields for multiannual and seasonal crops papadavid and diofantos 2010 however according to oladipo 1993 after soil fertility one of the main factors that limit agricultural production is water supply deficiencies therefore in order to have effective water resources management and soil water balance for crop productions in such regions estimation of reference evapotranspiration et0 with acceptable accuracy is important the history relating et to meteorological variables can be traced to early 19th century see brutsaert 1982 chen et al 2005 from then many methods categorized into 6 groups have been developed chen et al 2005 1 temperature based methods blaney 1952 hargreaves and samani 1985 2 radiation based methods e g makkink 1957 jones and ritchie 1990 3 combination methods e g penman 1948 monteith 1965 4 mass transfer methods e g harbeck 1962 5 water budget methods e g guitjens 1982 and 6 pan evaporation methods e g allen et al 1998 the practical value of pan evaporation ep has been proven and as such its application with empirical coefficients relating ep to et0 has been widely used for 10 or longer days periods allen et al 1998 measurements from ep provide combined effects of solar radiation wind speed temperature and humidity on the et0 a reasonable accuracy could successfully be achieved by these measurements for et0 estimations irmak et al 2002 as shown by numerous studies when ep is properly maintained it is expected to see higher correlation between ep and et0 khoob 2008 although physical based and conceptual models are reliable tools for investigating the actual physics of a phenomenon they have practical limitations and when accurate predictions are more important than the physical understanding employing black box models can be more useful multilinear regression mlr is a conventional method for modeling linear relationship between one or more independent and dependent variables tabari et al 2012 these kind of models which are basically linear lose their merit toward modeling processes in many fields that are embedded with high complexity dynamism and nonlinearity in both spatial and temporal scales in recent years application of artificial intelligence ai for examples artificial neural network ann adaptive neuro fuzzy inference system anfis and support vector regression svr are widely adopted through which several research papers were published by offering a new approach for lagged et0 data based modeling mehdizadeh 2018 applied ai methods to estimate daily et0 for urmia and tabriz semi arid isfahan and shiraz arid and yazd and zahedan hyper arid climates of iran sanikhani et al 2019 performed temperature based modeling of et0 for some selected stations in the mediterranean region of turkey using several ai models in different modeling scenarios for comparison ann model was developed by kumar et al 2002 to calculate et0 using limited number of climate data sudheer et al 2003 applied ann to model et0 kisi 2006 estimated daily et0 by applying both feed forward and generalized regression neural networks ffnn and grnn ekhmaj 2012 applied ann to predict et0 in two stations located in coastal area of western libya kisi and kilic 2016 investigated the generalization ability of ann and m5 model tree in modeling et0 antonopoulos and antonopoulos 2017 employed ann and empirical equations for daily et0 estimation using limited input climate variables also some researches were conducted to model et0 using anfis including kisi and oztürk 2007 study potentials of modeling daily et0 from limited climate data under arid conditions were investigated using anfis by karimaldini et al 2011 dou and yang 2018 performed a study to investigate the feasibility and effectiveness of anfis and extreme learning machine elm to estimate daily et with flux tower observation mosavi and edalatifar 2018 applied anfis to predict et0 in two cities of iran mohammadrezapour et al 2019 compared the performances of svm anfis and gene expression programming gep for monthly potential evapotranspiration modeling in arid region case study sistan and baluchestan province iran amiri et al 2019 investigated the capability of fuzzy regression method to estimate et0 under controlled environment in iran for the svr model based on the support vector machine svm concept as other type of ai method few studies might be found in technical literature tabari et al 2012 applied svm anfis regression and climate based models for et0 modeling using limited climatic data in a semi arid highland region wen et al 2015 evaluated the application of svm using combination of solar radiation wind speed minimum and maximum air temperature to model daily et0 in the ejina basin an extremely arid region in china mehdizadeh et al 2017 used svm gep multivariate adaptive regression splines mars and empirical equations for estimation of monthly mean et0 in iran yin et al 2017 modeled daily et0 in a semi arid mountain area by integrating genetic algorithm and svm using limited meteorological data fan et al 2018 evaluated the performances of svm elm and four tree based ensemble models for predicting daily et0 ferreira et al 2019 devised a new approach using ann and svm with limited meteorological data to estimate et0 although such black box models e g ann anfis svr and mlr may lead to quite reliable results it is well known issue that for a particular problem different models may lead to different outcomes therefore as indicated by bates and granger 1969 seminal work an ensemble of different approaches would result in minimal error variance compared to individual technique or approach working in solitary mode also makridakis et al 1982 disclosed that it has become a common practice to combine several single models for the improvement of forecasting accuracy the notion behind ensemble modeling is to accomplish unique features for the constituent models to bring about different patterns presented in the dataset kiran and ravi 2008 sharghi et al 2018 in the context of et0 modeling in particular and hydro climatic processes in general based on the knowledge of the authors so far no study was conducted utilizing ai based techniques and empirical models to perform ensemble modeling hence the main objective of this study is to apply the ensemble concept to model et0 in four stations in turkey adana ankara izmir and samsun four stations in north cyprus nc famagusta kyrenia morphou and nicosia two stations in iraq erbil and salahaddin two stations in iran tabriz and urmia and two stations in libya sabha and tripoli and to investigate the effect of different climate conditions on et0 modeling this was done in three steps firstly conducting input parameter analysis to determine the influence of each meteorological parameter to model et0 secondly ffnn an ai model that is commonly used anfis an ai model that is capable of handling uncertainty of process through fuzzy concept svr an ai model that is more recently used hargreaves and samani hs modified hargreaves and samani mhs makkink mk ritchie rt and mlr a linear conventional method models were applied for the modeling and compared to et0 computed by ep method finally for the improvement of the modeling performance outputs of the single models were used as inputs to obtain the final ensemble outputs ensemble models using strategy 1 for black box models ffnn anfis svr mlr and strategy 2 for empirical models hs mhs mk rt neural ensemble ne weighted averaging wa and simple averaging sa were used to form the ensemble models 2 materials and method 2 1 study location and data this study involves et0 modeling in 14 stations in different climatic regions across 5 countries including turkey nc iraq iran and libya hence the descriptions of the study locations are provided based on countries where the stations are situated 2 1 1 turkey stations adana station falls in the mediterranean med region which is humid and semi humid subtropical with a hot dry summer and heavy rainy and cool winter ankara station is in continental central anatolia ccan which is semi arid steppe that has light rainy and warm summer cold rainy winter and cool rainy spring izmir station shares same region with adana samsun station is in the black sea region bls a temperate climate which is uniformly rainy with its peak in autumn table a 1 shows data statistics location and coordinates of the study stations in turkey turkes 1996 as seen in table a 1 the maximum daily monthly average pr in adana and izmir is almost twice that of ankara and samsun this is because adana and izmir have climate of med region that is partly humid and semi humid ankara is located in ccan region which is semi arid climate while samsun has temperate bls climate however precipitation decreases with increase in aridity of a region that is why precipitation in samsun is a bit higher than in ankara meanwhile it can also be seen in table a 1 that tmax and tmin values in samsun station fall between the ranges of other stations under study which is because of the moderate nature of bls climate 2 1 2 north cyprus nc stations data from four stations in nc were used in this study including famagusta kyrenia morphou and nicosia see fig 1 the climate of famagusta is categorized as warm and temperate the average temperature is 19 3 c with a yearly average rainfall of 407 mm kyrenia has an average temperature of 29 c in the month of july hottest month and 10 c in january coldest month average rainfall in hilly areas ranges between 500 mm and 750 mm falling rarely in summer and particularly in winter owing to altitude kyrenia mountains witness the highest rainfall range as it ranges between 750 mm and 1110 mm the climate of morphou is classified as local steppe climate morphou experiences little rainfall throughout the year with precipitation falls of about 363 mm annually and an average annual temperature of 18 5 c nicosia is an inland city as such the effect of the mediterranean sea is less in nicosia than other coastal cities hence hotter summers and colder winters are experienced in nicosia than on the coastal areas also difference between night minimum and day maximum temperatures are larger for the hottest months july and august the difference in day time temperatures is about 4 c to 7 c between nicosia and other cities along the coastline in the coldest months january and february the daytime temperature difference is 2 c to 3 c in nicosia less than on the coast elkiran and ongul 2009 table a 2 shows data statistics location and coordinates of the study stations in nc being mediterranean climate which has relatively cool or mild winter and dry hot summer the temperatures were found to be as low high and medium as shown in table a 2 the minimum values of pan evaporation are 1 1 mm day for famagusta 0 9 mm day for kyrenia 1 11 mm day for morphou and 1 10 mm day for nicosia while its maximum value is as high as 12 6 mm day for nicosia relative humidity values are narrowly varied across all stations dew point temperatures range from lowest 2 86 c to the highest 22 48 c 2 1 3 iraq stations erbil is kurdistan s capital city located in northern region of iraq erbil is located within a semi arid and continental climate it experiences dry warm summers and rainy cool winters rasul et al 2015 salahaddin is located in the far north of iraq in the kurdistan region according to study by sarlak and agha 2018 salahaddin climate is semi arid table a 3 shows data statistics location and coordinates of the study stations in iraq with their vulnerability to semi arid climate condition erbil and salahaddin stations have almost similar statistics as seen in table a 3 salahaddin is more humid which gives rise to higher relative humidity than erbil the tmax in salahaddin station is also lower and can be as low as 0 c and as high as 39 9 c 2 1 4 iran stations tabriz city is situated in northwestern iran between latitudes 38 8 n and 46 15 e it is located at the junction of aji and quri rivers at an altitude of 1350 m tabriz has an annual precipitation of about 380 mm it enjoys dry and semi hot summer fine and mild climate in spring average temperature is around 13 c annually with potential evapotranspiration rate estimated around 1733 mm year urmia is a northwestern iran city located at latitude 37 34 n and longitude 44 58 e total days of freezing in urmia are about 120 days with a very scarce precipitation in the summer and heavy downfall in late autumn and winter the station is characterized by annual rainfall of around 300 mm year nourani and fard 2016 table a 4 shows the descriptive statistics of the data from tabriz and urmia stations higher altitude means faster reception of solar irradiance to the earth surface which causes rise in temperature and eventually increase in evapotranspiration this is why both maximum temperature and maximum pan evaporation are higher for tabriz station 35 15 c and 15 33 mm day than urmia 33 90 c and 10 96 mm day however presence of urmia lake increases the evaporation bodies of the station and consequently received more precipitation amount in maximum and average 3 74 mm day and 0 67 mm day than tabriz 2 91 mm day and 0 59 mm day 2 1 5 libya stations sabha is an inland station located in southern part of libya at 27 04 n latitude and 14 43 e longitude it has dry arid climate with annual average precipitation not exceeding 100 mm year 26 5 c annual average temperature and 52 mean annual relative humidity tripoli is mediterranean coastal city and capital of libya it has a semi arid climate which constitutes rainy winters and hot dry summers its annual rainfall ranges between 140 and 550 mm year 14 2 21 6 c annual air temperature and 70 mean annual relative humidity ageena 2013 the descriptive statistics of the study data are shown in table a 5 being distinct climate stations distinct characteristics of the meteorological parameters are observed as seen in table a 5 as arid climate possesses extreme weather condition sabha station has higher temperature less precipitation and higher evaporation 41 95 c 1 42 mm day and 24 70 mm day than tripoli station 36 21 c 4 79 mm day and 14 10 mm day fig 1 shows the regions and stations used in this study the data used in this study were obtained from turkish meteorological organization tmo for turkey stations north cyprus meteorological organization for cyprus stations kurdistan meteorological organization for iraq stations iran meteorological organization and libya national meteorological centre for model validation a stratified k fold cross validation method was used in the k fold cross validation method the original set is partitioned randomly into number of k subsamples of equal size of the k subsamples k 1 subsamples are used as training data while the remaining single subsample is used as validation data for testing the model the cross validation process is then repeated k times with the model been validated once by each of the k subsample single estimation result is then produced by averaging or otherwise combining the k results from the folds the k fold cross validation method has advantage that each observation is used for validation exactly once and the entire observations are used for both training and validation sharma et al 2018 in this study the data samples from the 14 stations were partitioned into k 4 random subsamples as shown in fig 2 each random subsample was obtained by dividing total sample size by k 4 folds as seen in table 1 the model was tested by single subsample while training was carried out by the k 1 4 1 random subsamples the process was repeated by the number of k fold subsamples 4 times for different single test subsamples and different k 1 training subsamples the schematic illustration of the 4 fold cross validation used in this study is shown in fig 2 while table 1 shows the duration of the used datasets usually in ai based modeling the data are normalized to ensure that equal attention is received by all variables and to eliminate their dimensions during model training nourani et al 2012 there are two primary advantages of data normalization before the application of ai based predictions the first is the avoidance of using attributes in bigger numeric ranges that overshadow those in smaller numeric ranges second advantage is the avoidance of numerical difficulties while calculation in this study the data were scaled within 0 to 1 to analyze and determine the performance and efficiency of the models proposed legates and mccabe 1999 research is endorsed which stated that determination coefficient dc or nash sutcliffe efficiency criterion eq 1 and root mean square error rmse eq 2 can sufficiently evaluate any hydro climatic model 1 dc 1 i 1 n r i r i 2 i 1 n r i r 2 2 rmse i 1 n r i r i 2 n where n r i r and r i are respectively the number of observations observed data mean of the observed values and predicted values the accuracy of the forecasted values is measured by rmse which gives positive value by squaring the error as divergence increasingly becomes large between observations and forecasts the rmse increases for perfect forecasts from zero through large positive values understandably dc dc 1 with high value up to 1 and rmse value close to 0 imply high model efficiency 2 2 proposed methodology in this study sensitivity analysis was applied to determine the most appropriate input parameters to the models ffnn anfis svr and mlr models were then created trained and validated separately for et0 modeling two temperature based hs and mhs and two radiation based mk and rt models were also applied to estimate et0 the proposed ensemble approaches were applied via strategies 1 and 2 for black box and empirical models using the outputs of the single models 2 2 1 strategy 1 in the first ensemble strategy the et0 is simulated as a function of the et0 outputs of the black box models ffnn anfis svr and mlr as 3 et 0 f et ffnn et anfis et svr et mlr 2 2 2 strategy 2 in the second ensemble strategy the outputs of the four empirical models are used as inputs to the ensemble approach as 4 et 0 f et hs et mhs et mk et rt the primary idea behind ensemble modeling is due to the following prospects sharghi et al 2018 i mostly it is difficult in practice to ascertain if the underlying process under study for a time series is generated by a non linear or linear phenomenon or whether one model or method in particular surpasses others therefore a difficult task before predictors is choosing the befitting method or technique for a unique issue hence application of ensemble strategy can handle the problem of selecting appropriate model ii for a real world process time series may involve both non linear and linear characteristics in such situation neither ai nor mlr could be sufficient for the time series prediction since errors of a linear pattern could be magnified by ai models and mlr model cannot cope with non linear relationship thus by combining ai and mlr models complex underlying nature of the data could be captured more accurately iii there is no particular or unique method that can perfectly investigate process as approved by previous studies including zhang 2003 sharghi et al 2018 this could be attributed largely to the complex nature of real world problem whereby a unique model may not be able to detect distinct patterns of process fig 3 shows the proposed methodology in this study the proposed methodology is applied to the data from all 14 stations to have an overall comparison regarding the computed et0 over different regions in turkey nc iraq iran and libya in fig 3 rh sp pr td tmax tmin tmean umin umax umean rs and ep are defined in tables a 1 a 5 etffnn etanfis etsvr etmlr eths etmhs etmk and etrt given in eqs 3 and 4 present predicted et0 by ffnn anfis svr mlr hs mhs mk and rt models respectively while etsa etwa and etne are ensemble et0 obtained by sa wa and ne techniques 2 3 artificial neural network ann ann presents convincing approach for handling huge amount of noisy nonlinear and dynamic data particularly when not fully understood the underlying physical relationships this makes ann appropriate to data driven nature of a time series modeling nourani et al 2015 a ffnn trained with back propagation bp algorithm are the most famous methods used among the neural networks applied in solving several engineering problems hornik et al 1989 among the several training methods levenberg marquardt lm training algorithm was chosen in this research in consideration of its ability to converge fast as stated by sahoo et al 2005 moreover tangent sigmoid tansig transfer function was applied for both hidden and output layers likewise the calibration epochs and hidden layer nodes were determined by trial and error process 2 4 adaptive neuro fuzzy inference system anfis neuro fuzzy simulation points to the techniques of applying different learning algorithm to fuzzy modeling in the neural network literature or fuzzy inference system fis akrami et al 2014 a distinctive approach in the development of neuro fuzzy is anfis which was first introduced by jang 1993 and utilize the learning algorithm of nn as a universal approximator anfis is capable of compacting set of accuracy to any degree for any real continuous function according to jang et al 1997 functionally anfis is equivalent to fis precisely the interest of the anfis system is functionally equivalent here to the first order sugeno fuzzy model jang et al 1997 the anfis general structure is given in b 2 as seen in b 2 it is considered that the anfis has x and y inputs and ƒ output according to aqil et al 2007 the sugeno first order fuzzy model has an ideal rule sets which are two fuzzy if then rules and are given by 5 r u l e 1 i f μ x i s a 1 a n d μ y i s b 1 t h e f 1 p 1 x q 1 y r 1 6 r u l e 2 i f μ x i s a 2 a n d μ y i s b 2 t h e f 2 p 2 x q 2 y r 2 in which a 1 and a 2 are x inputs mfs b 1 and b 2 are y inputs mfs respectively while the output function parameters are p 1 q 1 r 1 and p 2 q 2 r 2 2 5 support vector regression svr svr developed on the basis of svm concept is used for non linear regression issues unlike many other black box forecasting methods svm based methods such as svr instead of minimizing the error between the observed and computed values consider operational risk as the objective function to be minimized in svr first a linear regression is fitted on the data and then the outputs go through a non linear kernel to catch the non linear pattern of the data given a set of training data x i d i i n xi is the input vector di is the actual value and n is the total number of data patterns the general svr function is wang et al 2013 7 y f x w φ x i b where φ xi indicates feature spaces non linearly mapped from input vector x vapnik 1998 2 6 multi linear regression in general for multi linear regression mlr the dependent variable y and n regressor variables may be related by elkiran et al 2018 8 y b 0 b 1 x 1 b 2 x 2 b 3 x 3 b i x i ξ where x i is the value of the i th predictor b 0 is the regression constant and b i is the coefficient of the i th predictor and ξ is the error term detail information regarding the applied black box models can be found in sections b 1 b 4 in the appendices 2 7 empirical equations the empirical equations used in this study are shown in table 2 in table 2 k p is a pan coefficient which ranges between 0 3 and 1 1 and it is inversely proportional to wind speed and directly proportional to relative humidity heydari and heydari 2014 to compute kp cuenca approach cuenca 1989 was applied in this study owing to its performance and application in many studies snyder et al 2005 heydari and heydari 2014 as 9 k p 0 475 2 4 10 4 u 2 5 16 10 3 r h 1 18 10 3 f 1 6 10 5 r h 2 1 01 10 6 f 2 8 10 9 r h u 2 1 0 10 8 r h 2 f γ is the psychrometric constant kpa c 1 δ is the slope of the vapor pressure curve kpa c 1 r s u 2 r h t min t mean and t max previously defined in table a 1 f is fetch distance green crop windward side distance and α 1 have the following conditions 5 t max 35 c α 1 1 1 t max 35 c α 1 1 1 0 05 t max 35 t max 5 c α 1 0 01 exp 0 18 t max 20 2 8 ensemble modeling for a given set of data it is obvious that performance of one intelligent technique may surpass another and when different sets of data are used the results may entirely be opposite in order to benefit from the advantages of all techniques and also not to lose generality an ensemble model is developed which utilizes the single output of each technique with certain priority level assigned to each with the help of an arbitrator to provide the output kiran and ravi 2008 some ensemble techniques for prediction problems comprised of linear ensemble such as stack regression breiman 1996 weighted average perrone and cooper 1993 simple average benediktsson et al 1997 and nonlinear ensemble such as neural network based yu et al 2005 according to kiran and ravi 2008 there are two ensemble methods i linear ensemble method which includes linear ensemble by simple averaging linear ensemble by weighted averaging and linear ensemble by weighted median ii nonlinear ensemble method e g an ann is trained to obtain an ensemble output the ensemble modeling in this study was conducted via two linear simple and weighted averaging and one non linear ffnn ensemble methods simple averaging sa is done as 10 et 1 n i 1 n et i where et is output of simple ensemble model et i is the output of ith single model here outputs of ffnn anfis svr and mlr and n is the number of single models here n 4 the weighted averaging wa ensemble is expressed as 11 et i 1 n w i e t i where wi is the applied weight on ith model which can be determined based on the model performance as 12 w i d c i i 1 n d c i dci is the performance efficiency e g determination coefficient of ith single model the procedure applied for developing the ensemble models is given in fig 4 in the non linear ensemble ne the individual outputs of ffnn anfis svr and mlr are combined together as inputs to a new non linear model here an ffnn method to obtain the overall ensemble output similar to the case of single modeling for model validation the data samples were partitioned into 4 folds subsamples with k 1 used as training subsamples and the remaining subsample was used for model validation the process was repeated until each subsample was used once as validation dataset the general ensemble procedure is demonstrated in fig 4 2 9 validating the meteorological parameters quality control procedures were applied to determine from observations the erroneous and suspect data firstly verification is necessary to ensure that correct and complete record structure is obtained and all possible data have been collected gaps detected in the data files would be flagged as erroneous and should not be used as input variable in the estimations of et0 the methods used for quality assurance of the meteorological parameters include internal consistency test step test persistence test and range fixed or dynamic test estévez et al 2011 2016 in this study range fixed test procedure was applied because of its ability to detect erroneous data data outside acceptable fixed range table 3 shows the applied range test procedures for data quality control of the variables used 3 results and discussion the proposed methodology in this study contains 4 steps i application of classic empirical methods ii neural network based analysis focusing on the effect of each variable on et0 iii application of artificial intelligence ai based non linear and linear mlr models single modeling finally iv the results of ensemble models are presented in 2 strategies via two linear and one non linear approaches to appraise the improvement in performance that could be attained over the single models therefore the obtained results are also provided at different sub sections to ensure that the input variables to be used for the et0 simulations are valid and of accepted quality quality assurance procedures were applied to the input variables in order to identify erroneous values the results obtained from the fixed range test conducted showed no flagged erroneous data identified implying that the variables are within the fixed limits given in table 3 this can also be affirmed by the descriptive statistics of the data for all stations given in tables a 1 a 5 in the appendices section 3 1 results of the empirical models for performance evaluation of the climate based models et0 values computed by ep method were used as the basis of comparison with the et0 computed by the four climate based models the results of the empirical temperature based and radiation based models across all the 14 stations are shown in table 4 it should be noted that the results of the empirical models in this study were compared with those of ai based models as such in order to have accurate basis for comparison the former results were calibrated on the basis of 4 folds cross validation as the latter as seen in table 4 the performances of all models are appreciable in most of the stations meaning that reliable estimates of et0 could be achieved by empirical models in most of the study locations though for similar weather conditions the performances of the models are analogous nevertheless it is obvious that one model performance could be better than another mostly due to the condition climate under which the models are originally derived this brings about the best and least performing models for turkey stations the performances of the empirical models in adana and izmir stations are a bit better than in ankara and samsun stations in terms of higher dc and lower rmse this could be because the med climate receives higher precipitation and according to the concept of water cycle condensation and precipitation take place due to frequent evapotranspiration this indicates that with higher precipitation frequency evapotranspiration is easier to be estimated by empirical models the results provided by empirical models in ankara station also surpassed those obtained from samsun station because ankara station is located in ccan which is a semi arid climate environment while samsun is in bls temperate climate and most of the empirical models are developed on the basis of prediction in an extreme climate such as arid and semi arid in order to have proper management of the available water resources hence the empirical models may not have accurate predictions in the stations that are not subjected to such climate conditions comparing the models performances it can be said that temperature based models generally have better prediction capability than radiation based models and mhs models performed better than hs models the temperature based models increased prediction of radiation based models in the validation step up to 42 in adana station 14 in ankara station and 3 in izmir station whereas an increment of 16 is achieved in samsun region over temperature based models by radiation based models for cyprus stations mhs model was found to have better performance in famagusta station with dc 0 8245 rmse 0 1274 in training phase and dc 0 7941 and rmse 0 1232 in the validation phase hs model followed closely with dc 0 7727 and rmse 0 2083 in the validation phase in kyrenia station radiation based models were found to have better performances rt model was found to overestimate et0 with dc 0 8092 and rmse 0 1172 in the validation phase the better performances of the radiation based models could be due to the rocks surrounding the location which may result in easier evaporation and transpiration also atmospheric conditions such as extraterrestrial radiation and saturation vapor pressure slope could be more effective in this location kyrenia as they can easily reach to the earth surface unlike for a flat surrounding like nicosia in morphou however the correlation between et0 values computed by ep method and those computed by radiation based methods is slightly higher than when compared to et0 values computed by temperature based methods here rt produced the highest simulated values in the validation with dc 0 7391 rmse 0 1546 for nicosia station mhs has the best performance in the validation phase dc 0 8035 rmse 0 1152 based upon the results in table 4 temperature based models are superior in performance in the training phases for all stations perhaps due to the suitability of the indicators used in this study however in the validation phases better correlation is obtained for famagusta and nicosia by the temperature based models while radiation based models have better performances for kyrenia and morphou locations hence both temperature and radiation based models are capable of estimating et0 with certain degree of accuracy however as revealed in table 4 the performances of the models are identical across the four stations which could be because the stations share similar semi arid mediterranean climate of mild cold winter and hot dry summer the main difference between their climates is the station upon which they based that is for a coastal area the temperature will be little bit moderate than in inland area also hilly and valley areas may influence the climate and eventually prediction of et0 this could be why the radiation based models performances are higher in kyrenia than all other stations for iraq stations the models used in erbil station produced higher estimates of et0 values but lower estimations were achieved by the empirical models in salahaddin station although salahaddin has been classified as semi arid climate station study by sarlak and agha 2018 shows that the station s aridity varies with time and aridity index defined as the ratio of mean annual potential evaporation to precipitation ranjbar et al 2018 applied for its investigation for instance using unep 1992 aridity index the station was found to be sub humid between 1980 and 2011 sub humid between 1980 and 1997 and semi arid between 1998 and 2011 the inconsistency of the climate of the station makes et0 difficult to be estimated by empirical models as the long time data record contains elements of different climatic regions the empirical models fail to provide accurate estimations this is why the applied models showed the least performance for salahaddin station compared to the other stations table 4 also demonstrates the performances of the empirical models for tabriz and urmia stations of iran the results show that both temperature and radiation based empirical models could be employed successfully for et0 estimation in the stations this is because the temperatures tmax tmean and tmin and rs parameters used as inputs to the models do not have much deviation from the mean which means less data diversification and hence formed strong bond with the target output in both stations mhs shows promising performance over all other empirical models which has performance increment of about 5 2 and 3 for tabriz station and 2 2 and 3 for urmia station over hs mk and rt models in the validation phase this justifies its modification from original hs model for libya stations different outcomes can be seen in table 4 from two stations both temperature and radiation based models performed better for sabha than tripoli stations despite the arid nature of sabha climate this could be because tripoli is a densely populated city which contains about 1 million of the 6 million libyan population high population may lead to increase in industrialization and human activities which can cause environmental degradation and release of toxic gases to the atmosphere this can have a profound effect on et and thereby making et0 difficult to be estimated by physical methods the scatter plots for the best hs model erbil station mhs model adana station mk model izmir station and rt model izmir station in the validation phase are shown in fig 5 in general the temperature based empirical models produced better predictions than radiation based models any of the models could be applied to achieve valuable results in regions similar to that of this study and where only temperature data are available temperature based model can be employed successfully 3 2 sensitivity analysis results a non dimensional sensitivity analysis has been already applied to determine the effect of change of meteorological variables on the increase or decrease of et0 see beven 1979 estévez et al 2009 also some previous studies show that ai based single input single output sensitivity analysis may also be used to assess the influence of each meteorological variable on et0 for effective et0 modeling involving ai techniques for examples jain et al 2008 performed sensitivity analysis using ann for estimation of et0 doğan 2009 applied anfis to determine the effect of each meteorological variable on the benchmark et0 wang et al 2011 applied sensitivity of meteorological variables through ann to model et0 in the arid regions of africa eslamian et al 2012 used ann to determine the most effective parameters to model et0 petković et al 2015 determined the most influential weather parameters on et0 using anfis consequently a single input single output neural network based input sensitivity analysis was applied in this study in order to identify the key input parameters for the et0 modeling over the selected stations the et0 value is expressed as a function of each parameter and then the models were trained and verified the results are presented in table 5 for all the 12 parameters in this study table 5 shows the importance of the potential input parameters on the output et0 in the validation phase rmse was used in determining the effectiveness of the parameters while lowest error implying the most dominant parameter according to the results in table 5 for turkey stations ep rs and tmax ep tmean and rs ep rs and tmean and ep tmean and tmax are the most effective parameters for et0 modeling in adana ankara izmir and samsun stations respectively on the other hand for nc stations rs ep and sp are the 1st 2nd and 3rd most influential parameters in famagusta station for both training and validation phases temperatures tmean tmax and tmin are the 4th 5th and 6th most effective parameters for et0 in famagusta station due to the hot climate of the station and less precipitation the amount of water vapor in the air is minor which could be why td and pr are 7th and 8th in famagusta station rh would be more effective in the humid regions owing to higher moisture amount in air hence with increasing aridity index air moisture amount would be limited and rh effects will be reduced this might be the reason why rh is in the 9th position umin umean and umax are the least effective parameters to estimate et0 in the famagusta station for kyrenia being a coastal city the effects of parameters are similar to those of famagusta station but the influence of mountains surrounding the area for kyrenia city results in easier evaporation and precipitation which in turn results in higher precipitation thus pr is the 5th most dominant parameter in morphou station the effect of parameters on et0 estimation is in descending order of rs ep sp tmean tmax pr rh td tmin umax umin and umean for nicosia station with location similarity to morphou station the effects of the parameters on et0 are similar as well for the stations located in kurdistan s region of northern iraq the best 3 parameters that are most effective in et0 modeling for erbil are ep tmean and rs whereas for salahaddin station the dominant parameters are ep tmin and rs for tabriz and urmia stations of iran the 3 most dominant parameters are ep tmax and tmin with rmse of 0 0107 0 0515 and 0 0641 for tabriz station and ep rs and tmean with rmse of 0 0181 0 1104 and 1265 for urmia station for sabha station which is arid climate rise in temperature results in rise in et0 therefore tmean tmax tmin and ep are the 4 most influential parameters as a coastal station that receives more precipitation at a lower elevation above mean sea level rh and sp are among the 4 most effective parameters to et0 for tripoli station 3 3 results of the black box models ann anfis svr and mlr in this section the results of three ai based techniques ffnn anfis and svr and one conventional technique mlr are presented for et0 estimation for 14 different stations of turkey nc iraq iran and libya using different input combinations based upon the results of the input selection levenberg marquardt algorithm was used to train the ffnn with single hidden layer and varying number of hidden neurons for et0 simulation the hidden layer optimal node number was determined using trial and error procedure for each region accordingly the number of nodes in the hidden layer that provided the best results were found to be 7 9 6 11 10 9 12 11 8 10 9 7 14 and 12 for adana ankara izmir samsun famagusta kyrenia morphou nicosia erbil salahaddin tabriz urmia sabha and tripoli respectively anfis model which used sugeno type fuzzy inference algorithm was applied in this study where the membership function parameters were calibrated by a set of given input output data via hybrid optimization algorithm trial and error procedure was applied for the formulation of the structures of the anfis models in order to find the best anfis construction triangular trapezoidal and gaussian shaped membership functions mfs were found to be sufficient across all stations for the et0 simulation while modification of training epoch was examined to determine the number of mfs that will bring about the most optimum performance for svr models rbf kernel was used to create the models for all stations the tuning parameters of the rbf kernel are fewer than polynomial and two sigmoid kernels also considering smoothness assumptions the rbf kernel shows better performance in svr modeling sharghi et al 2018 to achieve the best performance efficiency and reliability for et0 estimations in each station the rbf kernel s parameters in svr were tuned finally mlr which expresses linearly the relationship between independent and dependent parameters was used in this study as well to ensure efficiency of performance each model is subjected to 3 4 and 5 different input combinations from the best 5 most effective parameters to et0 estimation as obtained from the input parameters analysis the results showed varied outcomes in some stations 3 inputs models have higher simulation reliability than models with 4 inputs while in some 4 inputs are better but generally 5 and 3 inputs models produced the highest number of best models performances hence the models were constructed according to the number of inputs parameters that provided the best performances therefore the input combinations with respect to the study stations for the et0 simulations are expressed as 13 et 0 ad f e p ad r s ad t max ad 14 et 0 an f e p an t mean an r s an t max an 15 et 0 iz f e p iz r s iz t mean iz 16 et 0 sa f e p sa t mean sa t max sa t min sa r s sa 17 et 0 fa f r s fa e p fa t mean fa t max fa s p fa 18 et 0 ky f r s ky e p ky t mean ky p r ky s p ky 19 et 0 mo f r s mo e p mo t mean mo t max mo s p mo 20 et 0 ni f r s ni e p ni t mean ni t min ni s p ni 21 et 0 er f e p er t mean er r s er 22 et 0 sl f e p sl t min sl r s sl t max sl t mean sl 23 et 0 ta f e p ta t max ta t min ta 24 et 0 ur f e p ur t mean ur r s ur 25 et 0 sh f e p sh t min sh t mean sh 26 et 0 tr f e p tr r h tr s p tr t max tr where et 0 ad et 0 an et 0 iz et 0 sa et 0 fa et 0 ky et 0 mo et 0 ni et 0 er et 0 sl et 0 ta et 0 ur et 0 sh and et 0 tr are et0 at adana ankara izmir samsun famagusta kyrenia morphou nicosia erbil salahaddin tabriz urmia sabha and tripoli stations respectively and r s e p t mean t max s p p r r h and t min defined as in table a 1 the results of the ffnn anfis svr and mlr models for all stations are presented in table 6 it should be noted that the results are given for the best output models only the ffnn structure numbering x y z implies the number of input parameters hidden layer neurons and number of output for anfis structure mf x represents membership function type and number of membership functions for svr rbf implies radial basis function used in the svr construction the mlr structure x y refers to number of inputs and output respectively table 6 depicts different performances of the models among all the applied models ai based models were found to have higher efficiency and reliability than the linear models for stations located in turkey both ai and mlr models provided good estimation of et0 with exception of samsun station where a fairly performance was obtained by mlr model with dc 0 5168 and rmse 0 1507 this shows that the bls temperate climate of samsun station negatively affects the performances of both empirical and mlr models but ai based models can overcome the threat caused by climate of the region as they produced reliable results due to their robustness and ability to deal with uncertain behavior of the climate however despite their ability to provide good results in bls climate region the ai based models performances in samsun station are inferior to their performances in other stations this implies that the ai based models can produce reliable results but not at the peak in such region it is apparent from the result shown in table 6 that all the performances of the applied models in adana and izmir stations med region humid and semi humid climate performed better than those in ankara station ccan region a semi arid climate by comparing tables 4 and 6 it can be seen that ai based models are superior in performances than the other models in all modeling phases training and validation the better performance of the black box models might be due to i inclusion of pan evaporation as their input which was also used as the benchmark output and ii their ability to deal with nonlinear phenomena empirical and mlr models performances differ across the stations with some empirical models having an upper hand over mlr models this is because the temperatures tmin tmax tmean and rs that are used as inputs to the empirical models are the dominant forefront parameters for the estimation of et0 in those stations which allowed empirical models to have accurate and successful et0 estimations than mlr model in most stations the overall results for turkey stations show that climate of the regions affects et0 modeling in the regions with best performance in med region followed by ccan region and lastly bls region for nc stations the mlr models in the training phases to estimate et0 led to acceptable values with respect to statistical indicators but mlr models could not give much reliable performances in the validation phase with the exception of kyrenia which got dc 0 7085 and rmse 0 1419 the better performance for kyrenia could be due to the effectiveness of the input combination used for the location which has an inclusion of precipitation because of its dominance for the et0 estimation in the area the inability of mlr models to produce good simulation beyond training steps could be as a result of their failure to deal with nonlinear complex process all the ai based models produced promising performances in the study stations because of their ability to handle complex phenomenon such as et0 process anfis achieved higher predictions in the validation phase for famagusta station with dc 0 8285 rmse 0 1146 svr performed better in training phase of kyrenia but was inferior to ffnn by 6 in the validation phase morphou and nicosia also witnessed higher anfis predictions efficiency over ffnn and svr the efficient performance of the anfis could be because of the shortcomings of ann to deal with ambiguous or less accurate data despite its robustness of dealing with various real world problems as such anfis might be a better option due to the fuzzy logic ability to handle the uncertainty of the process moghaddamnia et al 2009 in the case of stations in iraq the results of the models in table 6 show that the efficiency of the et0 modeling is higher in erbil than in salahaddin which is as a result of the distinct nature of the climate between the two stations the aridity of salahaddin station changes with time and the applied aridity index while that of erbil station remains constant irrespective of the time and indices applied see sarlak and agha 2018 the irregular behavior of the climate in the station reduces the accuracy of the models predictions for iran stations similar to results of the empirical models but in improved form the performances of the ai models are identical due to climate similarity of the study stations like most stations anfis has a standout performance among other ai models in the validation phase for both stations ffnn has superior performance in tabriz station and inferior performance in urmia station over svr mlr model despite been developed based on the dominant inputs that have higher correlation to the bench mark output does not produced better estimates than the empirical models table 6 this is because the inputs to the empirical models despite been fixed are of strong agreement with the et0 output the better performances of the applied models in tabriz station are due to better convergence of the meteorological parameters toward the mean as seen in the descriptive statistics of the data shown in table a 4 by st deviation of the input parameters for sabha and tripoli stations of libya all the models performances are convincing and with uncertainties and problems solving nature of ai models the deficiencies of the underperformance empirical models at tripoli station have been significantly dealt with performance improvements up to 26 26 28 and 33 were achieved in the validation phase by svr the least ai performing model over hs mhs mk and rt models respectively the scatter plots for the best ffnn model adana station anfis model nicosia station svr model adana station and mlr model adana station across all the study stations in the validation phase are shown in fig 6 comparisons were made between the empirical ai based and mlr models to ascertain the performance of one model over another for all stations the results are presented in fig 7 based on the dcs and rmses of the validation phase as depicted by fig 7 across all stations ai based models in terms of higher dc and lower rmse outperformed all other applied models in this study the reasons for the promising performance of ai models could be owing to one or all of the following reasons a ai models are capable of dealing with complex and nonlinear process b the input combinations of the ai models were used based on merit implying that the input selection of the ai based models were due to their effectiveness in et0 estimation as derived from input selection step while the inputs for all other models with exception of mlr models were fixed regardless of their performances toward et0 modeling fig 8 a shows a time series in the validation phase of adana station for the black box models ffnn anfis svr and mlr in order to have proper visualization of the estimated et0 values by each model fig 8b is also plotted that contains only the 12 months period of the year 2014 january 2014 december 2014 as revealed in fig 8b four different points are randomly selected and numbered 1 2 3 and 4 which correspond to the months of february june august and october respectively considering point 1 observed 1 4 mm ffnn 1 5 mm anfis 1 6 mm svr 1 6 mm and mlr 2 4 mm this shows that ffnn is more close to the target than the rest models at point 2 observed 5 0 mm ffnn 5 3 mm anfis 5 0 mm svr 5 1 mm and mlr 5 6 mm this indicates that the agreement between observed and predicted values is higher using anfis model ffnn model which was the best in the month of february is at 3rd rank in june at point 3 observed 4 8 mm ffnn 5 5 mm anfis 5 5 mm svr 5 4 mm and mlr 4 4 mm this shows that mlr is less deviated from the observed value than the other models and svr is 2nd performing model at point 3 at point 4 almost all models performances converged together observed 2 6 mm ffnn 2 6 mm anfis 2 7 mm svr 2 6 mm and mlr 2 6 mm from the models outputs at these four points it can be deduced that different data aspects can be captured by different models in different ways at different time points hence amalgamation of models via ensemble approaches could enhance the capability of the model in more precisely capturing the target to this end two linear sa wa and one nonlinear ne ensemble techniques are applied in strategy 1 for black box models and strategy 2 for empirical models in the next section it should be noted that the ensemble modeling was performed in strategies 1 and 2 instead of combining all the models ffnn anfis svr mlr hs mhs mk and rt for the following reasons i we tried to assess the responses of both ai and empirical models to ensemble modeling if we combine the models all together we cannot know the influence of each technique towards ensemble performance ii to see if it s possible to use empirical models for performance improvement in case of data lack for ai based modeling and iii to see the difference in performance improvement between ensemble models derived from low performance single models and the improvement achieved from higher performance single models 3 4 results of the ensemble techniques 3 4 1 results of strategy 1 in strategy 1 the outputs of 3 ai based ffnn anfis svr and mlr models were used as inputs to the ensemble models to ensure that the higher performance is achieved three methods were applied in obtaining the weights for wa ensembles i using only dc of the training phase ii using only dc of the validation phase iii using both dcs of training and validation phases of the single models the weights generated by the 3rd method provided better simulation and hence applied in both strategies 1 and 2 similar to the single ffnn models the ne models were developed using lm training algorithm with tangent sigmoid as activation functions for both hidden and output layers the schematic diagram of the developed procedure for ne modeling is given in fig 9 trial and error procedure was applied to determine the best epoch number and structure number of hidden neurons of the ensemble network the choice of ann in this study as the nonlinear ensemble technique over other ai models was made due to its popularity compatibility and above all high reported performance by many ensemble modeling studies including yu et al 2005 kiran and ravi 2008 sharghi et al 2018 while other ai models may also be employed as the kernel of ensembling the obtained results for the ensemble models in strategy 1 are presented in table 7 for model structure a b represent number of inputs and output for sa w x y z imply the generated weights by ffnn anfis svr and mlr that were applied for wa and finally ffnn ensemble structure is same as explained previously for single ffnn models the results in table 7 indicate that ensemble modeling certainly improved accuracy of performance over single models the performances of the models improved up to 20 31 22 34 13 11 7 10 15 29 4 11 8 and 14 over mlr models in the training phases and 4 13 12 8 13 7 14 2 9 6 10 7 22 and 15 over svr models in the validation phases for adana ankara izmir samsun famagusta kyrenia morphou nicosia erbil salahaddin tabriz urmia sabha and tripoli stations respectively it is observed that not much improvement in dcs was attained in the training phase over ai models but remarkably higher performances were achieved over all models in the validation phase which was the primary focused area in this study as explained earlier at different points in time different behaviors of the data could be captured with underestimation and overestimation of et0 by different models and with unique capability of each model the underlying process could be simulated better than in case of single models the deduced ensemble results table 7 in this study show that the performances of the sa and wa are almost equal in most cases across the study stations this could be because of the linear direct relationship they shared with the single models the performances of the nonlinear ffnn ensemble ne models are far better than sa and wa ensemble models this could be because i ffnn uses nonlinear kernel to simulate the behavior of a system hence simulation by ffnn would yield better results than the other linear sa and wa methods ii the performances of single models may influence overall results of sa and wa models implying that poor performing models may result in lesser ensemble performances because of the direct relationship that the methods sa and wa share with the single models iii the errors generated by single models might be propagated and incorporated by sa and wa ensemble techniques due to the direct amalgamation of the single models it is observed from table 7 that for turkey stations the results for adana and izmir stations are better than those for ankara and samsun stations also ensemble results for ankara are superior to the results obtained for samsun station similarly for iraq stations ensemble model result for erbil station surpassed that obtained for salahaddin station this clarifies that the climate conditions at these stations not only affect the performance of the single models but also accuracy and efficiency of the ensemble models that are generated from the single models to compare the performances of the ensemble models for the 4 study stations in nc the dc values of the best performance single models were subtracted from the dc values of the strategy 1 ensemble models tables 6 and 7 the differences are 0 0912 0 0136 0 0073 and 0 0026 for famagusta kyrenia morphou and nicosia respectively this shows that the differences which indicate the best performing ensemble models are higher in the first 2 stations than in the last 2 stations this could be because the heat capacity of soil for inland stations is lower than that of water ocean for coastal stations implying that the ocean cools down and heats up relatively slowly and in contrast the land heats faster and cools faster the sudden cooling and heating of inland areas make et0 phenomenon difficult to predict thus ensemble modeling predicts et0 better in coastal stations famagusta and kyrenia than inland stations morphou and nicosia considering the improvement in performance achieved by ensemble model in the validation phase of tabriz and urmia 10 and 7 stations by examining the difference in performance between the best performed single models anfis of two stations it could be seen that anfis model for tabriz station is a bit superior to anfis model for urmia station this signifies that ensemble models increase prediction of single models according to single model performance by same amount for station under same climatological condition such as tabriz and urmia of iran s semi arid region for libya stations the 22 increment in performance for arid sabha station compared to 15 increment for semi arid mediterranean tripoli station means ensemble techniques are capable of delivering superb performance in an extreme climate 3 4 2 results of strategy 2 the strategy 2 of ensemble simulation was applied to the four empirical models hs mhs mk rt in this study to see how ensemble approaches could cope with less performing models when compared to ai models the modeling was performed via three ensemble approaches sa wa and ne same as those used in strategy 1 table 8 shows the results of strategy 2 of ensemble models table 8 demonstrates the capability of ensemble approaches to improve the performances of all models irrespective of their applied simulation method the performances of the models were improved up to 5 9 10 18 7 8 6 1 8 29 12 11 8 and 33 over mhs models in the training phases and 8 15 11 37 1 12 7 7 14 31 17 19 13 and 55 over same mhs models of validation phases for adana ankara izmir samsun famagusta kyrenia morphou nicosia erbil salahaddin tabriz urmia sabha and tripoli stations respectively it is observed that almost all the features of strategy 2 resemble those of strategy 1 ensemble techniques including the closeness of the results obtained by sa and wa models higher performance of ne over sa and wa ensembles etc which their reasons are discussed in strategy 1 this shows that these features are peculiar to ensemble models irrespective of the single models used because of the processes and methodologies followed in carrying out the three ensemble simulations it is observed that there is huge gap in performance between the two linear ensemble models sa and wa and ne model in the second ensemble strategy than in the first ensemble strategy this is because empirical models provided lower estimates of et0 in the single modeling ne model in the second ensemble modeling overcomes the obstacle posed by the lower estimations whereas the other ensemble models due to their linear behavior could only adapt the nature of the single models the result shows that the improvement in et0 modeling is higher in strategy 2 ensemble modeling up to 55 than in the strategy 1 maximum 22 in the verification phase but owing to the capability of ai based models to handle uncertainty of system strategy 1 ensemble remains superior to the strategy 2 this means that with low performance single models more room for improvement will be left to be filled by ensemble models but with high performance single models higher predictions would be achieved by ensemble models it is worth mentioning that by comparing the two ensemble strategies tables 7 and 8 strategy 1 outperformed strategy 2 in terms of higher dc and lower rmse in the similar manner ai models outperformed empirical models see tables 4 and 6 this implies that the ensemble model performance follows the trend and direction of the performance of single models that is to say with high performance of single models ensemble modeling yields better simulations while in contrast less accurate but improved performance ensemble modeling could be achieved from poor performance single models therefore for more efficient and accurate estimation of et0 ai based models are preferable over empirical models however obviously the ensemble of empirical models may be used in the case of data lack for ai based modeling fig 10 compares the performances of strategies 1 and 2 ensemble techniques in form of computed versus observed et0 time series for stations in turkey nc iraq iran and libya according to fig 10 it is apparent that ai based ensemble models are more accurate than empirical based ensemble models the sa wa and ne follow closely the fluctuations of the observed data whereas sa2 wa2 and ne2 are unable to have close correlation with the observed data 4 conclusions in this study et0 was simulated via several ai models ffnn anfis svr conventional mlr and empirical models hs mhs mk and rt for adana ankara izmir samsun famagusta kyrenia morphou nicosia erbil salahaddin tabriz urmia sabha and tripoli stations afterwards ensemble methods which combine the outputs of the single models were developed in strategy 1 for black box models and in strategy 2 for empirical models via simple average weighted average and neural ensemble techniques in the absence of lysemeter pan evaporation method of et0 is widely accepted in practical problems therefore the method was used as the basis upon which the performance of the models were accessed the input selection results showed that ep temperatures tmin tmax tmean and rs for turkey iraq and iran stations rs ep and sp for nc and tripoli stations and tmin tmax tmean and ep for sabha station are the most dominant parameters to estimate et0 the results showed that the performance of the empirical models differs with stations and climate of the region upon which the stations are based but generally the empirical models results could be acceptable for et0 estimations despite the presence of data nonlinearity that mlr model could not cope with the selection of the best inputs for the mlr models from input selection step could help in ensuring reliable results could be achieved by mlr models for et0 estimations in both training and validation phases among the ai models anfis showed better performance in most stations which could be due to the serve of both neural network learning ability and fuzzy concept in a unique framework the results demonstrated that better approximations in et0 modeling could be achieved by ensemble models and combination of different models for strategy 1 ensemble modeling improvements over single models were accomplished up to 4 13 12 8 13 7 14 2 9 6 10 7 22 15 and for strategy 2 up to 8 15 11 37 1 12 7 7 14 31 17 19 13 and 55 in the validation phase of modeling for adana ankara izmir samsun famagusta kyrenia morphou nicosia erbil salahaddin tabriz urmia sabha and tripoli stations respectively comparing strategies 1 and 2 ensemble models higher efficient performance was achieved by the former in similar manner that ai based models outperformed empirical models this implies that higher performing single models could yield better ensemble simulations though with lower performance single models more space for improvement could be created for ensemble models to fill but higher performance single models would lead to most accurate efficient and reliable ensemble modeling despite the fact that ai based ensemble results could outperform empirical based ensemble results still empirical models may be used according to the data availability and also their easy implementation without need to training compared to ai models but of course results with a little more error the general results of this study demonstrated promising impact of combining models for et0 estimation the obtained results from the ensemble methods more especially neural ensemble method implied that better accuracy in et0 simulation can be achieved by combined outputs than individual models therefore it is suggested to apply the proposed ensemble method for modeling other hydro climatic parameters and also to include other black box process based methods in the ensembling unit declaration of competing interest there is not any conflict of interest regarding this submission appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 123958 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6214,this study focused on the sensitivity of flood dynamics to soil hydraulic properties derived from three different soil databases in urbanized area 1 upscaled locally observed soil texture data based on the soil landscape relationships smls 2 the soilgrids250m open data source smsg and 3 the fao soil map smfao however soil in the urban areas has two major conditions related to land cover and soil physical structure for which the information is not available in soil databases the effect of fragmented vegetation cover and the effect of compaction of bare soil non built up areas can be covered by fragmented vegetation grass and shrubs which generally has high infiltration rates in contrast bare areas such as dirt roads and footpath can be heavily compacted and have typically low infiltration rates we used pedotransfer functions ptfs with satellite derived vegetation cover and bare soil to predict soil hydraulic properties related to the uncompacted and compacted scenarios infiltration dynamics was derived from the predicted soil hydraulic properties for these soil information sources which then determined the flood dynamics in the catchment the flash flood modeling was done using the integrated flood modeling system using openlisem bout and jetten 2018 for the whole of kampala uganda using the 25th of june 2012 flood event in the distributed openlisem hydrological model these two urban soil conditions have been treated separately we have evaluated the sensitivity of flood dynamics to three different soil databases under both uncompacted and compacted urban soil conditions by using different flood indicators such as catchment water balance infiltration rate flood depth and duration flooded area and flood volume and the average number of structures affected the result of the study indicates that soil hydraulic properties needed for the distributed hydrological model are better predicted when using the smsg and smls which resulted in better infiltration simulation consequently compared to an earlier simulation that was verified with stakeholders and accepted for drainage system design the accuracy of the simulated flood extent map was better when using smsg and smls moreover soil compaction significantly reduces infiltration and consequently increases the flood depth and duration and therefore must be included in the urban flash flood modeling study keywords flash floods fragmented vegetation cover openlisem ptfs soil compaction soil hydraulic properties 1 introduction population growth and migration are the leading causes of urban expansion particularly in developing countries sub saharan african cities are predicted to increase in total urban extent by nearly 20 fold in 2030 compared to the urban extent in 2000 which would intensify pressure on the natural environment güneralp et al 2017 from a hydrological point of view urbanization is a process by which natural vegetation and agriculture are replaced by compacted and constructed surfaces such as buildings and tarred roads chen et al 2014 also the surface and subsurface drainage systems are usually altered and topsoil may be replaced by building materials such changes in urban morphology can have a significant impact on hydrological processes such as changing infiltration rate and consequently increase storm runoff redfern et al 2016 urban expansion is not always through planned high rise buildings but often a process of releasing building permits for small scale private contractors that lead to vast sprawling areas with single story houses built on the natural surface these areas have many footpaths and dirt roads that appear semi naturally unplanned many studies have investigated the role of urban soil hydrology in urban storm runoff berthier et al 2004 miller and hess 2017 ossola et al 2015 highlighting the importance of urban soil heterogeneity in several cities with alternative sealed and infiltrating surfaces that significantly affected soil water storage and therefore runoff one such example is the city of kampala uganda which has experienced a tremendous expansion over the last two decades a previous study by abebe 2013 analyzed the development of the city from 1989 to 2010 and shown that the city expands the urban footprint from 72 9 km2 in 1989 to 325 km2 in 2010 where the majority of the expansions were to the northern part of the city topographically the city is characterized by rounded hills and wetlands with residential density increasing on the hillslopes and the wetlands gradually being filled with informal settlements in spite of laws protecting the wetlands these areas have experienced intermittent flooding but urbanization of the hillslopes combined with heavy rainfall events have increased their frequency kcca 2012 the risk of flash flooding in kampala has generally been associated with heavy rainfall occurring during the rainy season douglas et al 2008 a similar study by sliuzas et al 2013 concluded that the city s flooding had been greatly influenced by the formation of informal settlements in the flood prone areas and by the city s lack of manpower to enforce spatial planning with increasing population growth and urban expansion the city of kampala is in danger of significant risk in flash flooding in 2002 a master plan was designed to upgrade and enlarge the drainage systems which is currently being implemented and in the absence of measured water level it s used as a baseline for this study in spite of the drainage masterplan as housing density on slopes continues to increase in place of natural vegetation flood problems may continue to persist urban soil properties are exposed to three different systems 1 the surface is sealed with impervious structures tarred roads houses concrete channels and 2 non sealed surfaces such as dirt roads and footpaths that are usually heavily compacted and 3 surfaces that are vegetated such as grassed parklands have a positive effect on infiltration it is important to note that in kampala most of the houses are built directly on the natural soil surface maybe removing the organic topsoil and thus the original soil information available is valid for most of the kampala area on locations where larger buildings are constructed sand and gravel are used as a foundation unfortunately there was no detailed information available as to their exact location which prompted the need to assume that all buildings have been constructed using the original soil material for this study in other words the growth causes a sprawling development outward as well as densification of buildings which negatively influences the infiltration capacity of the soil hence the flash flood problem may continue in spite of the drainage master plan it is therefore important to implement an integrated flood model that analyzes the hydrological processes for the entire catchment which is needed to mitigate flood risks and help in urban planning for this study we used the openlisem model bout and jetten 2018 since it simulates the above ground and soil hydrology in detail and uses a high resolution representation of topography with a 2d dynamic wave for all surface flow the model does not consider subsurface storm drains but in kampala sub surface storms drains are only present in the city center and often not functioning well the infiltration process of an integrated catchment model needs information about soil hydraulic properties shp such as saturated hydraulic conductivity ksat porosity and field capacity as well as the influence of compaction on these properties direct measurements of these properties are scarce and the main information available is soil texture and organic matter sometimes as percentages sometimes only as texture classes nrcs 1993 while soil structure information is not available this therefore means the exclusion of models that do not simulate infiltration e g based on the scs curve number method soil texture organic matter and bulk density in this study called soil physical properties sp can be translated to soil hydraulic properties using either tabulated guide values per texture class see e g cosby et al 1984 or pedotransfer functions see e g saxton and rawls 2006 and rawls and brakensiek 1989 saxton and rawls 2006 investigated the effect of changing bulk density on shp which makes the functions useful to simulate the effect of compaction on infiltration and soil water storage there are three sources of sp and shp available for kampala i the global soil database soilgrids250m developed by isric hengl et al 2017 ii the fao unesco soil map of the world fao 1991 1974 at 1 5 000 000 scale and iii an analysis specific to kampala that related texture classes and shp directly to landforms following the catena concept soilgrids250m is a global database of soil physical properties for seven soil layers 5 200 cm and is based on geostatistical interpolation of field soil data using machine learning algorithms that include correlated terrain and land use variables the interpolation and correlation are done on a continental basis to have a large enough dataset hengl et al 2017 warn against the unconditional use in detailed applications but as a source of information it is well worth investigating if this database can be used in the context of flood modeling in kampala the fao unesco soil map has information limited to texture classes the soil landscape relations were coupled to texture classes and measured shps for a watershed in kampala as part of a un habitat project sliuzas et al 2013 rossiter 2014 it recognizes that the geological history of kampala has resulted in hard lateritic soils capping the hills reddish sandy clay loams on the hills of weathered sandstone and slate and heavy clays in the wetlands where sediment accumulated they deduced the soil landscape relationship from a landscape analysis using the dem to prepare a concept map of soil landscape units from dem first the probable landscape segments number and positions were obtained mainly from the literature second landscape segmentation was carried out by numerical landform analysis using a numerical programing 1 valley floor 2 bottom slope 3 mid slope and 4 hilltop the segmentation was carried out by fuzzy means clustering from elevation slope and profile curvature and finally the soil texture classes from field experiment assigned to each landscape units in this study since the pattern of the landscape of kampala is consistent which is ironstone capped hills and swampy inter hill valleys we followed the same procedure and deduced four different landscape units for the whole of kampala finally we assigned the field experiment soil texture classes to the created landscape units as previously explained soil hydrological information is vital in flood modeling and can influence a city s flood hazard assessment the main objective of this research is to determine how sensitive the flood dynamics are to soil hydrological properties derived from soil information based on the three different soil databases soilgrids250m fao soil map and mapped soil landscape relationships the sensitivity of the flood model to these databases will be compared all datasets were translated to shp using saxton s pedo transfer functions since sealing and compaction play an essential role all three data sources were used with and without compaction to see if the effect of sealing and compaction overrides the differences caused by the soil information sources also the differences in the number of affected buildings by the simulated floods will be shown finally if there are clear effects we aim to advise on the best strategy in case there is very little local soil data available it is important to note that there are no discharge or flood measurements in kampala the only independent information available is the simulated flood extent form the kampala drainage master plan for a design rainfall event with a 10 year return period calculated with the hec ras model the flood maps were verified with stakeholders 2 methodology 2 1 conceptual framework of the study the underlying hypothesis of this research is that the freely available global soil database does not explicitly consider the effects of changes in soil structure caused by the presence of vegetation cover or by compaction actual soil databases derived directly from global soil maps e g isric and fao are assumed to have a normal soil density as shown in table 1 in the ptfs of saxton and rawls 2006 this is indicated with a relative density factor of 1 0 we assume in this study that areas covered by vegetation have a density factor less than one while the compacted areas have a density factor higher than one the vegetation consists of grass and occasional trees and shrubs while we observed that all bare soil consists of footpaths and dirt roads and will be compacted thus the combined effects of vegetation cover and compaction on estimated soil hydraulic properties can be accounted for by incorporating vegetation cover and compaction effects into ptfs to produce soil hydraulic properties under uncompacted and compacted conditions the openlisem model has for each grid cell information about the fraction of the surface covered with vegetation bare surface and impermeable surface buildings tarred roads etc this leads to the following simulation scenarios scenario 1 the uncompacted urban soil of which the density factor is linearly adjusted to the vegetation cover fraction and varies between 0 98 almost no vegetation to 0 90 full cover scenario 2 the compacted urban soils with a density factor varying between 1 0 and 1 2 linearly related to the fraction of bare soil in a grid cell the exact procedure is explained below fig 1 shows the methodological setup used in this study 1 local soil texture classes scaled up through soil landscape relationships hereafter referred to as soil map landscape smls 2 soil texture data derived from the global soilgrids250m database hereafter referred to as soil map soilgrids smsg and 3 the soil texture class map derived from the fao 1 5 000 000 soil map hereafter referred to as soil map fao smfao 2 2 pedo transfer functions ptfs pedo transfer functions ptfs are non linear multiple regression of the combinations of soil physical properties sp to reproduce the soil hydraulic properties shp cosby et al 1984 the main sp used in ptfs are sand and clay content organic matter content om gravel content and bulk density numerous examples of ptfs are available in the literature for example cosby et al 1984 rawls and brakensiek 1989 and saxton and rawls 2006 provide ptfs built from a collection of usa soil samples and wösten et al 1999 provide ptfs built based on european soil samples these ptfs differ from each other in their required input parameters and underlying equations abdelbaki et al 2009 harrison et al 2012 a study by gijsman et al 2002 has indicated that the saxton and rawls 2006 method was the most accurate based on rmse root mean square error of 0 009 compared with a 0 25 average for all methods in this regard various studies e g abdelbaki et al 2009 have suggested that saxton and rawls s approach is suitable for deriving soil hydraulic properties needed for hydrological modeling the predictive equations reported by saxton and rawls are mainly based on mean texture class data in the usda texture class triangle which is the dominant effect for the soil hydraulic properties in addition to texture classes variables such as om and density can play a crucial role in the estimation methods om effects the values and distribution of the predicted soil hydraulic properties for example increases in om cause an increase in saturated hydraulic conductivity mainly because of its influence on soil aggregation and associated pore distribution the value of om is introduced in the predictive equations as percentage volume weight wt and in this study we used the constant value of 2 5 wt for all cases 2 3 relating land cover to soil compaction soil bulk density varies across the space based on the underlying surfaces loose organic soils and uncompacted urban soil have a bulk density ranges between 1 0 and 1 5 g cm3 while compacted soils have a bulk density varying between 1 5 and 1 8 g cm3 depending on the texture as shown in table 1 increases in soil compaction in general increases bulk density and consequently decreases in porosity and saturated hydraulic conductivity fig 2 saxton and rawls 2006 introduced a relative compaction factor to ptfs to provide a compacted density which is higher than normal density condition the compaction factors are categorized as 0 9 loose 1 0 normal 1 1 dense 1 20 hard and 1 3 severe saxton and rawls 2006 predicted moisture content at the wilting point field capacity and porosity among other properties from a series of multilinear regression equations from sand clay and organic matter contents using different combinations of these variables they used selections of approximately three thousand samples from the soils in the us the baseline porosity is then adjusted in case of compaction and the presence of gravel porosity and bulk density are simply related according to 1 ϕ 1 cf ρ bulk 2 65 where ϕ is porosity cm3 cm3 ρ bulk is normal bulk density g cm3 cf is the relative compaction factor scenario 1 and 2 65 g cm3 is the particle density saturated hydraulic conductivity is derived using the equation by brooks and corey 1964 as 2 ksat a ϕ θ fc 3 λ r 3 λ l n θ fc l n θ wp l n 1500 l n 33 4 r 1 g r a v e l 1 g r a v e l 1 1 5 ρ bulk 2 65 where r is a reduction factor caused by gravel content a is a regression constant set to 1930 and lambda λ is the pore size distribution index brooks and corey 1964 and ï fc ï wp are moisture content at field capacity and wilting point and 33 and 1500 is the matric potential in kpa from field capacity and wilting point the reduction factor is estimated based on the applied gravel content which is assumed to be zero in this study in the absence of information a gravel content of zero results in an r of 1 so that it does not affect the pore size distribution in eq 2 in this case equations 1 to 4 allows us to combine the fraction vegetation cover and bare soil with density factor and therefore have an effect on ksat and porosity for example a grid cell that has a cover fraction of 1 3 for a building grass and bare soil will have a ksat and porosity which is a weighted combination of 1 3 building is sealed ksat 0 porosity 0 1 3 is compacted with a cf of 1 07 related to bare surface and 1 3 has a cf of 0 96 related to the vegetation fraction fig 3 b c d presents vegetation cover built up and bare soil cover fractions of kampala which were derived from the landsat image of 2010 fura 2013 2 4 openlisem flood model the open access limburg soil erosion model openlisem is an integrated erosion and flood model simulating all surface hydraulics and if needed erosion and sediment dynamics in all flows bout and jetten 2018 it usually operates on a spatial resolution between 5 and 50 m and time steps between 0 1 and 30 sec and simulates flash floods for single rainfall events the model combines a surface and soil water balance including interception by vegetation and buildings surface storage and infiltration to calculate the surface runoff infiltration is done with the green and ampt equation for two soil layers using a solution of kutílek and nielsen 1994 which uses directly input maps for ksat porosity and suction head at the wetting front the simulations were done with an initial moisture content of 0 8 of the porosity and the corresponding matric suctions flow routing is done with a 2d dynamic wave using a finite volume solution bout and jetten 2018 as such the model does not distinguish between surface runoff and flood from a hydraulic perspective a flood is assumed to be all water that is deeper than a user defined value in this case the arbitrary water height of 10 cm was used to separate floods from runoff based on discussions with stakeholders in flooded areas in the northern lubigi catchment different types of cover result in different flow resistance values manning s n and also buildings are assumed to increase the flow resistance water is routed by the dynamic wave to a channel system that can overflow if the water volume is larger than the channel dimensions thus adding to the flooding process channel flow is done with a 1d kinematic wave in the channel network stationary base flow can be assumed for the channel system where necessary all information on grid cell properties related to land cover is used in the form of fractions per grid cell note that in this study the 2 layer infiltration process assumes topsoil of 30 cm in thickness which is affected by land cover and compaction while the subsoil uses the ptfs directly based on the texture information without any land cover information and influence of compaction the model has been used for flash flood modeling in the upper lubigi urban sub catchment in kampala uganda e g habonimana 2014 mahmood et al 2016 pérez molina et al 2017 sliuzas et al 2013 and four caribbean islands van westen et al 2015 among other areas 3 data 3 1 drainage systems and rainfall the drainage system in the city is divided into nine central drainage systems following the natural catchments in kampala fig 3 the major drainage systems are further divided into semi natural primary drains partly canalized in the central valley with up to 20 m2 cross section with secondary concrete drains 1 2 m2 cross section concentrating water from the side valleys due to the formation of informal settlements in the wetlands impermeable surfaces are increasing in the lowest flood prone areas resulting in increased problems there has been very limited hydro meteorological data monitoring in the city to carry out in depth flood modeling fieldwork and in situ data collection was conducted in 2012 as part of a joint un habitat s cities and climate change ccc and kampala capital city authority kcca initiative an automatic weather station was installed to collect rainfall data fig 4 and other meteorological data such as temperature and wind fig 4 shows the rainfall intensity used for flash flood modeling in this study since only one rainfall station is available in the study area which situated in the upper lubigi sub catchment the study assumed a spatially homogeneous rainfall distribution applied to the entire catchment the rainfall event of 66 2 mm is measured by an automatic weather station which is equal to a 2 year return period event the return periods are based on the kampala drainage master plan kdmp 2002 3 2 texture information derived from the soil maps the three soil databases were translated to texture information as follow the soil landscape relation was created in the un habitat project sliuzas et al 2013 established a preliminary soil map of the upper lubigi catchment based on the gathered preliminary soil data from 30 ring samples a strong relationship between the texture classes and the landscape morphology appeared to be present soil samples were collected from the representative landscape units and the critical aspect was soil texture classes belong to four landscape units i e valley floor bottom slope mid slope and hilltop the dominant soil texture classes in the catchment were clay c for valley floor sand clay loam sacl and sandy clay sac for the bottom slope sandy clay loam sacl for mid slope and sacl and sandy loam sal for hilltop areas as reported by rossiter 2014 the catchment landscape was highly consistent regarding the pattern of laterite capped hills and swampy inter hill valleys therefore upscaling of lubigi catchment observed soil texture classes to the whole kampala city is reasonably possible thus for smls this study used the upscaled soil texture classes to predict the necessary shps by applying ptfs as shown in table 2 according to the 1 5 000 000 scale fao world soil data source 1991 the soil texture class of the kampala city is represented by sacl except for a few areas in the eastern part of the lake victoria which is covered by sal the soil texture classes from smfao database used for ptfs are shown in table 2 the soilgrids250m database spatially variable maps of sand and clay contents were used as the predictors for ptfs texture ranges are shown in table 2 other inputs for ptfs are the same as those of smfao and smls the soil texture data from smsg maps shows that non built up and green areas wetlands are characterized by clay soil texture data whereas built up areas are dominated by sand content as big as 64 in the city center in this regard the prediction of soil texture data by using the smsg framework consider the presence of urban areas to some extent as indicated by hengl et al 2017 4 results and discussion 4 1 bulk density fig 5 presents the influence of bulk density for uncompacted and compacted scenarios for the three soil databases when the uncompacted scenario is considered the soil becomes loose with a lower bulk density which was predicted in the wetlands and at the edge of the city where the area is well covered by vegetation for the smls the lowest value of bulk density was predicted in the lower slope and valley floor landscape units characterized by c and sac while the higher bulk density was predicted in the middle slope and hilltop landscape units represented by sal and sacl fig 5 a in the case of smfao fig 5 c since the majority of the catchment area is represented by sacl texture class the predicted bulk density was only different according to the incorporated vegetation cover fraction the result when using smsg indicates that in the built up areas where the soil texture data is sandy soil the predicted bulk density was higher while in the non built up areas where the soil texture data is clay the predicted bulk density was lower fig 5 b the predicted bulk density was higher when using smsg compared to the other two soil databases particularly in the city center which is primarily due to its recent inclusion of land use urbanization in the isric soil database prediction system hengl et al 2017 in general the predicted bulk density distribution follows the distribution of soil texture classes with the higher bulk density predicted in the sand content and sacl areas while the lower bulk density was predicted in the c and sal content areas when the compaction is introduced in the predictive equations the predicted bulk density was increased quite significantly for all soil databases fig 5 d e f however the increase is lower when using smsg because both sand and clay soil texture data are less affected by compaction compared to the other soil texture classes used in the case of smls and smfao as shown in the figure the increment in the predicted bulk density was varying between 9 and 22 according to the type of soil texture classes and the degree of compaction used in the prediction system bulk density is in general increases with the increase in the degree of compaction which runs from hard to dense as shown in table 1 and fig 2 4 2 prediction of soil hydraulic properties for flood modeling table 3 shows the ranges of the predicted soil hydraulic properties shps needed for openlisem flood modeling for both compacted and uncompacted scenarios the shps needed for the openlisem model i e ksat porosity initial soil moisture content and matric suction psi were predicted for both uncompacted and compacted scenarios based on the soil data derived from the three soil databases the values and the spatial variability of the predicted shps mainly follow the predicted bulk density distribution for instance the predicted initial soil moisture content was higher in the wetlands and vegetation cover areas following the distribution of porosity which is associated with lower bulk density although the effect of compaction scenario can be applied to all shps for the sake of reducing the complexity in openlisem software we used only ksat and porosity as compacted input while keeping wetting front and initial moisture content input as uncompacted ksat and porosity are the main shps affect the infiltration characteristics of the openlisem model and therefore we exclusively discussed their values and distribution under both compacted and uncompacted scenarios under section 4 3 4 3 spatial distribution of the predicted porosity and ksat 4 3 1 porosity the result of the predicted porosity for the uncompacted scenario and the percentage changes due to soil compaction is presented in fig 6 for the uncompacted scenario the predicted porosity when using smls was varying across the landscape units the valley floor bottom slope vs middle slope hilltop while in the cases of smsg and smfao the predicted porosity was varying across the built up and non built up areas in all cases the predicted values and distribution of porosity follow the distribution of predicted bulk density higher porosity was predicted in the low bulk density areas e g in the wetlands and the outskirt of the city covered by vegetation while low porosity was predicted in the city center and built up areas the predicted porosity in the wetlands and vegetation cover areas were similar for all soil databases although their values are different according to the predicted bulk density the higher values and heterogeneous distribution was well predicted when using smls and then followed by smfao and smsg the predicted porosity when using smsg was quite lower compared to the other two soil maps which is mainly due to the higher bulk density predicted in the city center when using this soil database 4 3 2 saturated hydraulic conductivity ksat the result of the predicted saturated hydraulic conductivity ksat for the uncompacted scenario and the percentage changes due to soil compaction is presented in fig 7 since ksat was predicted as a function of bulk density and porosity the variability of the predicted ksat follows similar distribution as the predicted bulk density and porosity when using smls the highest and lowest ksat predicted at hilltop middle slope and valley floor lower slope landscape units respectively fig 7 a the predicted ksat at the valley floor landscape unit is about five times smaller than the values predicted at the hilltop landscape unit which is associated with the extremely higher porosity in the swampy areas which are characterized by clay soil texture in the case of both smsg and smfao the distribution of the predicted ksat was varying across the built up and non built up with the highest ksat predicted at the non built up areas covered by vegetation while the lowest ksat predicted at the built up areas fig 7 b and c again similar to porosity fair heterogeneous distribution was well predicted when using smls and then followed by smsg when using smfao there was large inter difference between the values simulated in the vicinity of lake victoria and city center and the intra difference against the other two soil maps was also high in areas close to lake victoria huge differences predicted because these areas were represented by sal which is further enhanced by the introduced uncompacted scenario consequently higher ksat 4 3 3 the effect of compaction on porosity and ksat fig 6 d e f shows the effects of soil compaction on the predicted porosity in the catchment as shown in the figure compaction affects both values and the spatial distribution of the porosity across the catchment which is mainly due to the increased bulk density as a result of soil compaction under all soil databases wetlands and non built up areas are profoundly affected by compaction reducing the porosity up to 18 39 however the built up areas are least affected by compaction with porosity reduction varying between 6 and 14 mainly because the compacted areas are usually distributed outside the built up areas e g playing ground murrum roads and tarred roads the effect of compaction on the predicted ksat is also shown in fig 7 d e f as shown in the figure the predicted ksat was reduced quite significantly with the percentage changes reaching up to 99 in the wetland areas compaction increased bulk density consequently decreased both porosity and ksat values with the degree of compaction richard et al 2001 saxton and rawls 2006 the result of the compacted scenario indicated that the predicted porosity and ksat when using smsg was least affected compared to smls and smfao and one possible reason is due to the inclusion of urbanized areas in the soilgrids system which causes a relatively homogeneous sandy soil texture data in the city center area since ksat was predicted as a function of compacted porosity a decrease in porosity caused a significant reduction in the predicted ksat similar studies found that the increase in urban soil compaction has resulted in decreased in saturated hydraulic conductivity for instance gregory et al 2006 found that increased in soil compaction caused a reduction in saturated hydraulic conductivity by 75 similarly ossola et al 2015 found a reduction in porosity in the compacted parks which consequently caused a decrease in saturated hydraulic conductivity 4 3 4 comparison of the predicted shps using smls with observations the predicted ksat and porosity under uncompacted conditions when using smls was compared with the field data collected from upper lubigi catchment as shown in table 4 field data have shown that the valley floor landscape unit with heavy clay in the swampy shrubs areas has lower ksat varying between 1 and 20 mm hr but the predicted ksat at the same landscape unit shows higher values varying between 16 and 35 mm hr the observed values in the hilltop with sandy clay loam in the grass shrubs areas have high ksat values varying between 90 and 150 mm hr while the predicted ksat at the same landscape unit shows comparable values which are varying between 78 and 140 mm hr however the predicted porosity at the valley floor was underestimated compared to the field data as shown in table 4 possibly because of a strong effect of organic matter in the samples that do not appear in the ptfs both observed and predicted porosity at the hilltop landscape unit has a relatively low value varying between 0 51 0 61 and 0 46 0 52 cm3 cm3 respectively the variation in values from the ptfs is smaller than the reality but more samples would have to be taken to investigate this better a comparison of the observed and predicted ksat under the compacted scenario at the valley floor shows relatively a similar result as shown in table 4 which are mainly due to the high effect of compaction at the clay content area as shown in the table the predicted ksatcomp in the hilltop landscape unit was moderately overestimated compared to the observed ksatcomp in the other landscape units the predicted ksatcomp was within the range of the observed values in the case of porosity the predicted values at all landscape units were underpredicted compared to the observed values at the respective landscape units 4 4 the sensitivity of flood dynamics to different soil databases 4 4 1 flood simulations and catchment hydrological behaviour table 5 presents the results of the catchment surface water balance for six different model simulations and the percentage differences between compacted and uncompacted scenarios the usage of smls and smsg appears in much higher flood volumes than the smfao 14 7 106 and 16 0 106 m3 versus 6 2 106 m3 and flooded areas of 49 1 and 53 0 km2 versus 26 1 km2 this is caused by the infiltration as illustrated by the infiltration amounts and aggregated runoff percentages calculated as total discharge total rainfall as shown in the table with the compaction scenario considered the flooded area has grown by 6 for both smls and smsg and 12 for smfao the simulated total discharges were also increased by 11 10 and 10 for smls smsg and smfao respectively the use of soil compaction increased runoff percentage for all soil maps but the increment is higher when using smfao compared to the other two soil maps 4 4 2 infiltration dynamics fig 8 shows infiltration maps simulated by using the three soil databases under the uncompacted scenario when using smls fig 8 a the simulated infiltration dynamics has followed the distribution of predicted porosity and ksat at each landscape units however due to the sealing characteristics at the hilltop and middle slope landscape units the simulated infiltration at those landscape units was relatively lower except at the grid cells where it s covered by vegetation similarly in the case of smsg the higher infiltration was simulated where the higher ksat was predicted fig 8 b since the predicted porosity and ksat followed the distribution of built up and non built up areas the dynamics of the simulated infiltration was followed the same pattern with higher infiltration simulated in the non built up areas while the lower infiltration simulated in the built up areas the simulated infiltration when using smfao database shows a widespread distribution of high values of infiltration in the catchment fig 8 c as shown in the figure most of the area has values larger than 75 mm particularly in the low density urban areas and wetlands the result of this study indicates that the variability of the simulated infiltration dynamics is well associated with the values and the spatial variability of the shps in particular predicted ksat and porosity higher infiltration is predicted in the well vegetated and non built up areas 20 5 of the total catchment area counter intuitive is the generally high value of infiltration simulated in swampy areas due to the overpressure of flood water during the event which strongly affects the green and ampt model the surface sealing which accounts for 48 of the total area results clearly in much lower infiltration 4 4 3 flood dynamics the uncompacted scenario for flood depth shows widespread flooding along the main channels primary and secondary channels as simulated by using the three soil databases fig 9 a b c the result indicated that the spatial distribution of flood dynamics flood depth flooded area flood volume and duration follows the infiltration dynamics in the catchment and accumulation of flood water at the valley floor transported there by the secondary channels draining the hills in general the simulated flood depth along the primary channels when using smls and smsg 0 5 2 m was deeper than that of smfao 0 5 1 m which is associated with the lower infiltration simulated when using smls and smsg similarly the simulated flood duration when using smls and smsg is longer along both primary and secondary channels compared smfao fig 10 also the calculated flooded areas and flood volume e g at 0 5 m was 23 km2 and 11 281 million m3 when using smls and smsg which are almost double the result found when using smfao at the same water depth fig 11 since in the case of smfao the simulated infiltration was higher there is little water left for direct runoff which leads to shallow flood depth consequently less flooded area flood volume and duration 4 4 4 the effect of soil compaction on infiltration and flood dynamics table 6 shows the percentage reduction in infiltration and the increments in flood depth as a result of soil compaction as shown in the table the higher infiltration reduction was simulated at the lower infiltrating area 40 80 mm which is reduced by 13 4 17 9 and 14 2 for smls smsg and smfao respectively however in the higher infiltrated areas 160 200 mm mainly in the swamps the simulated total infiltration was increased instead of decreasing likely due to an overpressure of greater than 1 5 m of flood water produced due to compaction as a result of reduced infiltration flood depth at different levels was increased as shown in table 6 which is consistent with the infiltration reduction however the flood depth increment in the swampy areas was due to the combined effect of flow concentration from high elevation and compaction similarly the simulated flood duration not shown here and both the calculated flooded areas and flood volume fig 11 were all increased following the infiltration reduction and nearly overweight the difference caused as a result of using different soil databases the effect of compaction on infiltration was high because there is a double effect of compaction on green and ampt infiltration as both porosity and ksat are affected since the ksat was predicted as a function of porosity in the ptfs and the green and ampt infiltration equation uses both variables the combined effect reduced the predicted infiltration quite significantly consequently more water is converted into the overland flow and increased water depth flood areas flood volume and flood duration this may cause an overestimation of the effect of compaction in this methodology 4 4 5 flood hazard aspects the effects of flood depth and duration on buildings the total number of structures affected by flood depth and flood duration was calculated by using flood depth greater than 10 cm and flood duration greater than 30 min fig 11 the model does not have a concept of individual buildings but uses a fraction of building per grid cell and gives the total building surface affected by different flood levels the average structure size is 90 m2 which is used to convert the total built up area to individual buildings as shown in the figure the total number of structures affected was largest at lower water depths between 10 and 50 cm in all scenarios and highest for smls and smsg under both uncompacted and compacted conditions it is very different between the scenarios of compacted and uncompacted the main difference is between the smfao and the others nevertheless there are differences of several thousand buildings under the uncompacted scenario the number of structures affected by flood depth at lower flood depth of 10 and 50 cm was 91113 87686 and 67 307 for smsg smls and smfao respectively these numbers are increased to 93 585 for smsg 90 403 for smls and 71 333 for smfao under the compacted scenario the areas with deepest flood water in the wetlands are not yet used for habitation although also these areas are more and more used for settlement as shown in fig 11 the duration the structures stayed under water was also high at lower flood duration between 0 5 and 1 h and the effect was high when using smsg and smls compared to smfao under both uncompacted and compacted conditions the number of structures stayed under flood water at lower duration was mainly at the location of the secondary channels between 15 and 20 h the number of structures affected by flood duration was 34628 34221 and 25 350 under the compacted condition and 33022 32 737 and 23 986 under the uncompacted condition for smls smsg and smfao respectively the effect at 15 20 hr duration was mainly from the main channels in the wetlands 4 4 6 model verification with earlier calibrated simulations due to the lack of observed actual discharge data at the main outlets the openlisem model verification has only handled by comparing the model flood inundation map with earlier calibrated flood line developed by kdmp kcca 2002 the earlier flood lines in kampala were generated by using hec georas river analysis system of the hydrologic engineering center of the us army corps of engineers for return periods of 2 10 and 100 years the generated flood lines are simply the strip or areas along the sides of drainage channels that will be prone to flood inundation for the different return period fig 12 a shows the effect of 2 10 and 100 year return period on flood extent and it s reported that the flood extent for different return period is all similar around the major wetlands mainly due to the flat cross section used in the simulation when the capacity of the natural channel is exceeded in most cases during floods with return periods of two years or less floodwaters spread over the full extent of the floodplain up to the steeper regions that define the floodplain with the flood depth increases slightly for more extended return periods the un habitat project sliuzas et al 2013 has also found that the width of flooding along the primary and secondary channels of all drainage systems does not differ much for the different return periods which is primarily caused by the relatively small cross sectional areas of the natural channels along the floodplains the openlisem model simulated with compacted scenarios by using the storm event of 66 2 mm which is equivalent to an event of the 2 year return period was used to compare the simulated flood extent with earlier flood extent the model result as shown in fig 12 b c d indicates that flood water spread over the full extent of the plain around the main drainage channels but with different water depth distribution the f statistics goodness of fit was calculated by using eq 6 and the result is shown in table 7 the high value of f statistics indicates high goodness fit between areas of flood line and model simulation 6 f aos ao a s a o s 100 ao refers to the flooded area observed under flood lines as indicates flooded area simulated by openlisem model and aos represents the intersected flooded area between ao and as the result of the flood extent maps indicated that flood extent accuracy was lower when using the smfao for hydrological modeling in line with the effect of infiltration described above note that this is just a general comparison between two models that use entirely different principles and datasets however because the hec ras derived flood lines are used in the spatial planning of kampala there is some merit in comparing the openlisem simulations to the flood lines 5 conclusions this study applied three different soil databases to predict soil hydraulic properties used in an integrated flood model in order to determine how sensitive the flood dynamics are to the predicted shps and if possible to select the best data source for integrated flash flood modeling in an urbanized area where there is a scarcity on local soil physical data three soil data sources were used coupling soil texture to landscape form using the soilgrids250m database and deriving texture information from the small scale fao soil map in doing flood predictions one may choose to include compaction and the effect of vegetation as an additional factor apart from surface sealing by roads and buildings we assumed that bare areas would have a certain degree of compaction and investigated its impact for each of the three soil data sources the results indicate that the choice of the data source has a high influence on both the quantity and spatial variability of infiltration which naturally directly affects runoff and flooding on top of that the effect of sealing and compaction is equally essential and nearly outweighs the differences caused by the use of different soil databases this indicates that sufficient effort should be attributed to getting actual compaction information in an area for which a flood simulation is done to establish in how far this affects reality when looking at the indicators for flood hazard buildings affected the differences are less pronounced but still large in terms of the number of the building because the shape of the terrain causes a similar flood extent the use of the saxton and rawls 2006 pedotransfer functions have the added advantage of incorporating the effects of compaction and organic matter into their equations so that in the future simulations the urban fabric can be represented in detail using for instance high resolution earth observation we did not attempt an uncertainty analysis of the simulated scenarios using an estimated texture in multilinear regression ptfs which by themselves have a varying error ranges and subsequently using these in a complex spatial model may very well result in a large uncertainty this could be studied with a monte carlo analysis but is beyond the scope of this study the study found that soil databases with high variability of soil physical properties e g when using smls and smsg can better predict shps used for flood modeling comparison of flood inundation areas using the three different soil maps with earlier calibrated simulations indicates that smsg and smls are a better strategy although still different from the accepted flood extent in the kampala master drainage plan in the case of kampala city given the fact that the soilgrids database is available globally while soil landscape relations may not be available everywhere it seems to be an acceptable source to derive infiltration properties because soilgrids comes with a warning and an extensive error analysis and is not meant for detailed studies as is done here it is of course advisable to check the results against local measurements of texture and in combination with ptfs against resulting hydrological properties nevertheless the results seem promising for integrated flood modeling in those urbanized areas where most buildings are constructed directly on the original soil declaration of competing interest none acknowledgment the project on the flood modeling in kampala was initiated with funding from un habitat s cities and climate change and the current work is the continuation of this project with the financial support granted by the university of twente the netherlands the authors would like to thank numerous people who collected field data under un habitat s project we also thank bastian van den bout for his support on the openlisem model database preparation 
6214,this study focused on the sensitivity of flood dynamics to soil hydraulic properties derived from three different soil databases in urbanized area 1 upscaled locally observed soil texture data based on the soil landscape relationships smls 2 the soilgrids250m open data source smsg and 3 the fao soil map smfao however soil in the urban areas has two major conditions related to land cover and soil physical structure for which the information is not available in soil databases the effect of fragmented vegetation cover and the effect of compaction of bare soil non built up areas can be covered by fragmented vegetation grass and shrubs which generally has high infiltration rates in contrast bare areas such as dirt roads and footpath can be heavily compacted and have typically low infiltration rates we used pedotransfer functions ptfs with satellite derived vegetation cover and bare soil to predict soil hydraulic properties related to the uncompacted and compacted scenarios infiltration dynamics was derived from the predicted soil hydraulic properties for these soil information sources which then determined the flood dynamics in the catchment the flash flood modeling was done using the integrated flood modeling system using openlisem bout and jetten 2018 for the whole of kampala uganda using the 25th of june 2012 flood event in the distributed openlisem hydrological model these two urban soil conditions have been treated separately we have evaluated the sensitivity of flood dynamics to three different soil databases under both uncompacted and compacted urban soil conditions by using different flood indicators such as catchment water balance infiltration rate flood depth and duration flooded area and flood volume and the average number of structures affected the result of the study indicates that soil hydraulic properties needed for the distributed hydrological model are better predicted when using the smsg and smls which resulted in better infiltration simulation consequently compared to an earlier simulation that was verified with stakeholders and accepted for drainage system design the accuracy of the simulated flood extent map was better when using smsg and smls moreover soil compaction significantly reduces infiltration and consequently increases the flood depth and duration and therefore must be included in the urban flash flood modeling study keywords flash floods fragmented vegetation cover openlisem ptfs soil compaction soil hydraulic properties 1 introduction population growth and migration are the leading causes of urban expansion particularly in developing countries sub saharan african cities are predicted to increase in total urban extent by nearly 20 fold in 2030 compared to the urban extent in 2000 which would intensify pressure on the natural environment güneralp et al 2017 from a hydrological point of view urbanization is a process by which natural vegetation and agriculture are replaced by compacted and constructed surfaces such as buildings and tarred roads chen et al 2014 also the surface and subsurface drainage systems are usually altered and topsoil may be replaced by building materials such changes in urban morphology can have a significant impact on hydrological processes such as changing infiltration rate and consequently increase storm runoff redfern et al 2016 urban expansion is not always through planned high rise buildings but often a process of releasing building permits for small scale private contractors that lead to vast sprawling areas with single story houses built on the natural surface these areas have many footpaths and dirt roads that appear semi naturally unplanned many studies have investigated the role of urban soil hydrology in urban storm runoff berthier et al 2004 miller and hess 2017 ossola et al 2015 highlighting the importance of urban soil heterogeneity in several cities with alternative sealed and infiltrating surfaces that significantly affected soil water storage and therefore runoff one such example is the city of kampala uganda which has experienced a tremendous expansion over the last two decades a previous study by abebe 2013 analyzed the development of the city from 1989 to 2010 and shown that the city expands the urban footprint from 72 9 km2 in 1989 to 325 km2 in 2010 where the majority of the expansions were to the northern part of the city topographically the city is characterized by rounded hills and wetlands with residential density increasing on the hillslopes and the wetlands gradually being filled with informal settlements in spite of laws protecting the wetlands these areas have experienced intermittent flooding but urbanization of the hillslopes combined with heavy rainfall events have increased their frequency kcca 2012 the risk of flash flooding in kampala has generally been associated with heavy rainfall occurring during the rainy season douglas et al 2008 a similar study by sliuzas et al 2013 concluded that the city s flooding had been greatly influenced by the formation of informal settlements in the flood prone areas and by the city s lack of manpower to enforce spatial planning with increasing population growth and urban expansion the city of kampala is in danger of significant risk in flash flooding in 2002 a master plan was designed to upgrade and enlarge the drainage systems which is currently being implemented and in the absence of measured water level it s used as a baseline for this study in spite of the drainage masterplan as housing density on slopes continues to increase in place of natural vegetation flood problems may continue to persist urban soil properties are exposed to three different systems 1 the surface is sealed with impervious structures tarred roads houses concrete channels and 2 non sealed surfaces such as dirt roads and footpaths that are usually heavily compacted and 3 surfaces that are vegetated such as grassed parklands have a positive effect on infiltration it is important to note that in kampala most of the houses are built directly on the natural soil surface maybe removing the organic topsoil and thus the original soil information available is valid for most of the kampala area on locations where larger buildings are constructed sand and gravel are used as a foundation unfortunately there was no detailed information available as to their exact location which prompted the need to assume that all buildings have been constructed using the original soil material for this study in other words the growth causes a sprawling development outward as well as densification of buildings which negatively influences the infiltration capacity of the soil hence the flash flood problem may continue in spite of the drainage master plan it is therefore important to implement an integrated flood model that analyzes the hydrological processes for the entire catchment which is needed to mitigate flood risks and help in urban planning for this study we used the openlisem model bout and jetten 2018 since it simulates the above ground and soil hydrology in detail and uses a high resolution representation of topography with a 2d dynamic wave for all surface flow the model does not consider subsurface storm drains but in kampala sub surface storms drains are only present in the city center and often not functioning well the infiltration process of an integrated catchment model needs information about soil hydraulic properties shp such as saturated hydraulic conductivity ksat porosity and field capacity as well as the influence of compaction on these properties direct measurements of these properties are scarce and the main information available is soil texture and organic matter sometimes as percentages sometimes only as texture classes nrcs 1993 while soil structure information is not available this therefore means the exclusion of models that do not simulate infiltration e g based on the scs curve number method soil texture organic matter and bulk density in this study called soil physical properties sp can be translated to soil hydraulic properties using either tabulated guide values per texture class see e g cosby et al 1984 or pedotransfer functions see e g saxton and rawls 2006 and rawls and brakensiek 1989 saxton and rawls 2006 investigated the effect of changing bulk density on shp which makes the functions useful to simulate the effect of compaction on infiltration and soil water storage there are three sources of sp and shp available for kampala i the global soil database soilgrids250m developed by isric hengl et al 2017 ii the fao unesco soil map of the world fao 1991 1974 at 1 5 000 000 scale and iii an analysis specific to kampala that related texture classes and shp directly to landforms following the catena concept soilgrids250m is a global database of soil physical properties for seven soil layers 5 200 cm and is based on geostatistical interpolation of field soil data using machine learning algorithms that include correlated terrain and land use variables the interpolation and correlation are done on a continental basis to have a large enough dataset hengl et al 2017 warn against the unconditional use in detailed applications but as a source of information it is well worth investigating if this database can be used in the context of flood modeling in kampala the fao unesco soil map has information limited to texture classes the soil landscape relations were coupled to texture classes and measured shps for a watershed in kampala as part of a un habitat project sliuzas et al 2013 rossiter 2014 it recognizes that the geological history of kampala has resulted in hard lateritic soils capping the hills reddish sandy clay loams on the hills of weathered sandstone and slate and heavy clays in the wetlands where sediment accumulated they deduced the soil landscape relationship from a landscape analysis using the dem to prepare a concept map of soil landscape units from dem first the probable landscape segments number and positions were obtained mainly from the literature second landscape segmentation was carried out by numerical landform analysis using a numerical programing 1 valley floor 2 bottom slope 3 mid slope and 4 hilltop the segmentation was carried out by fuzzy means clustering from elevation slope and profile curvature and finally the soil texture classes from field experiment assigned to each landscape units in this study since the pattern of the landscape of kampala is consistent which is ironstone capped hills and swampy inter hill valleys we followed the same procedure and deduced four different landscape units for the whole of kampala finally we assigned the field experiment soil texture classes to the created landscape units as previously explained soil hydrological information is vital in flood modeling and can influence a city s flood hazard assessment the main objective of this research is to determine how sensitive the flood dynamics are to soil hydrological properties derived from soil information based on the three different soil databases soilgrids250m fao soil map and mapped soil landscape relationships the sensitivity of the flood model to these databases will be compared all datasets were translated to shp using saxton s pedo transfer functions since sealing and compaction play an essential role all three data sources were used with and without compaction to see if the effect of sealing and compaction overrides the differences caused by the soil information sources also the differences in the number of affected buildings by the simulated floods will be shown finally if there are clear effects we aim to advise on the best strategy in case there is very little local soil data available it is important to note that there are no discharge or flood measurements in kampala the only independent information available is the simulated flood extent form the kampala drainage master plan for a design rainfall event with a 10 year return period calculated with the hec ras model the flood maps were verified with stakeholders 2 methodology 2 1 conceptual framework of the study the underlying hypothesis of this research is that the freely available global soil database does not explicitly consider the effects of changes in soil structure caused by the presence of vegetation cover or by compaction actual soil databases derived directly from global soil maps e g isric and fao are assumed to have a normal soil density as shown in table 1 in the ptfs of saxton and rawls 2006 this is indicated with a relative density factor of 1 0 we assume in this study that areas covered by vegetation have a density factor less than one while the compacted areas have a density factor higher than one the vegetation consists of grass and occasional trees and shrubs while we observed that all bare soil consists of footpaths and dirt roads and will be compacted thus the combined effects of vegetation cover and compaction on estimated soil hydraulic properties can be accounted for by incorporating vegetation cover and compaction effects into ptfs to produce soil hydraulic properties under uncompacted and compacted conditions the openlisem model has for each grid cell information about the fraction of the surface covered with vegetation bare surface and impermeable surface buildings tarred roads etc this leads to the following simulation scenarios scenario 1 the uncompacted urban soil of which the density factor is linearly adjusted to the vegetation cover fraction and varies between 0 98 almost no vegetation to 0 90 full cover scenario 2 the compacted urban soils with a density factor varying between 1 0 and 1 2 linearly related to the fraction of bare soil in a grid cell the exact procedure is explained below fig 1 shows the methodological setup used in this study 1 local soil texture classes scaled up through soil landscape relationships hereafter referred to as soil map landscape smls 2 soil texture data derived from the global soilgrids250m database hereafter referred to as soil map soilgrids smsg and 3 the soil texture class map derived from the fao 1 5 000 000 soil map hereafter referred to as soil map fao smfao 2 2 pedo transfer functions ptfs pedo transfer functions ptfs are non linear multiple regression of the combinations of soil physical properties sp to reproduce the soil hydraulic properties shp cosby et al 1984 the main sp used in ptfs are sand and clay content organic matter content om gravel content and bulk density numerous examples of ptfs are available in the literature for example cosby et al 1984 rawls and brakensiek 1989 and saxton and rawls 2006 provide ptfs built from a collection of usa soil samples and wösten et al 1999 provide ptfs built based on european soil samples these ptfs differ from each other in their required input parameters and underlying equations abdelbaki et al 2009 harrison et al 2012 a study by gijsman et al 2002 has indicated that the saxton and rawls 2006 method was the most accurate based on rmse root mean square error of 0 009 compared with a 0 25 average for all methods in this regard various studies e g abdelbaki et al 2009 have suggested that saxton and rawls s approach is suitable for deriving soil hydraulic properties needed for hydrological modeling the predictive equations reported by saxton and rawls are mainly based on mean texture class data in the usda texture class triangle which is the dominant effect for the soil hydraulic properties in addition to texture classes variables such as om and density can play a crucial role in the estimation methods om effects the values and distribution of the predicted soil hydraulic properties for example increases in om cause an increase in saturated hydraulic conductivity mainly because of its influence on soil aggregation and associated pore distribution the value of om is introduced in the predictive equations as percentage volume weight wt and in this study we used the constant value of 2 5 wt for all cases 2 3 relating land cover to soil compaction soil bulk density varies across the space based on the underlying surfaces loose organic soils and uncompacted urban soil have a bulk density ranges between 1 0 and 1 5 g cm3 while compacted soils have a bulk density varying between 1 5 and 1 8 g cm3 depending on the texture as shown in table 1 increases in soil compaction in general increases bulk density and consequently decreases in porosity and saturated hydraulic conductivity fig 2 saxton and rawls 2006 introduced a relative compaction factor to ptfs to provide a compacted density which is higher than normal density condition the compaction factors are categorized as 0 9 loose 1 0 normal 1 1 dense 1 20 hard and 1 3 severe saxton and rawls 2006 predicted moisture content at the wilting point field capacity and porosity among other properties from a series of multilinear regression equations from sand clay and organic matter contents using different combinations of these variables they used selections of approximately three thousand samples from the soils in the us the baseline porosity is then adjusted in case of compaction and the presence of gravel porosity and bulk density are simply related according to 1 ϕ 1 cf ρ bulk 2 65 where ϕ is porosity cm3 cm3 ρ bulk is normal bulk density g cm3 cf is the relative compaction factor scenario 1 and 2 65 g cm3 is the particle density saturated hydraulic conductivity is derived using the equation by brooks and corey 1964 as 2 ksat a ϕ θ fc 3 λ r 3 λ l n θ fc l n θ wp l n 1500 l n 33 4 r 1 g r a v e l 1 g r a v e l 1 1 5 ρ bulk 2 65 where r is a reduction factor caused by gravel content a is a regression constant set to 1930 and lambda λ is the pore size distribution index brooks and corey 1964 and ï fc ï wp are moisture content at field capacity and wilting point and 33 and 1500 is the matric potential in kpa from field capacity and wilting point the reduction factor is estimated based on the applied gravel content which is assumed to be zero in this study in the absence of information a gravel content of zero results in an r of 1 so that it does not affect the pore size distribution in eq 2 in this case equations 1 to 4 allows us to combine the fraction vegetation cover and bare soil with density factor and therefore have an effect on ksat and porosity for example a grid cell that has a cover fraction of 1 3 for a building grass and bare soil will have a ksat and porosity which is a weighted combination of 1 3 building is sealed ksat 0 porosity 0 1 3 is compacted with a cf of 1 07 related to bare surface and 1 3 has a cf of 0 96 related to the vegetation fraction fig 3 b c d presents vegetation cover built up and bare soil cover fractions of kampala which were derived from the landsat image of 2010 fura 2013 2 4 openlisem flood model the open access limburg soil erosion model openlisem is an integrated erosion and flood model simulating all surface hydraulics and if needed erosion and sediment dynamics in all flows bout and jetten 2018 it usually operates on a spatial resolution between 5 and 50 m and time steps between 0 1 and 30 sec and simulates flash floods for single rainfall events the model combines a surface and soil water balance including interception by vegetation and buildings surface storage and infiltration to calculate the surface runoff infiltration is done with the green and ampt equation for two soil layers using a solution of kutílek and nielsen 1994 which uses directly input maps for ksat porosity and suction head at the wetting front the simulations were done with an initial moisture content of 0 8 of the porosity and the corresponding matric suctions flow routing is done with a 2d dynamic wave using a finite volume solution bout and jetten 2018 as such the model does not distinguish between surface runoff and flood from a hydraulic perspective a flood is assumed to be all water that is deeper than a user defined value in this case the arbitrary water height of 10 cm was used to separate floods from runoff based on discussions with stakeholders in flooded areas in the northern lubigi catchment different types of cover result in different flow resistance values manning s n and also buildings are assumed to increase the flow resistance water is routed by the dynamic wave to a channel system that can overflow if the water volume is larger than the channel dimensions thus adding to the flooding process channel flow is done with a 1d kinematic wave in the channel network stationary base flow can be assumed for the channel system where necessary all information on grid cell properties related to land cover is used in the form of fractions per grid cell note that in this study the 2 layer infiltration process assumes topsoil of 30 cm in thickness which is affected by land cover and compaction while the subsoil uses the ptfs directly based on the texture information without any land cover information and influence of compaction the model has been used for flash flood modeling in the upper lubigi urban sub catchment in kampala uganda e g habonimana 2014 mahmood et al 2016 pérez molina et al 2017 sliuzas et al 2013 and four caribbean islands van westen et al 2015 among other areas 3 data 3 1 drainage systems and rainfall the drainage system in the city is divided into nine central drainage systems following the natural catchments in kampala fig 3 the major drainage systems are further divided into semi natural primary drains partly canalized in the central valley with up to 20 m2 cross section with secondary concrete drains 1 2 m2 cross section concentrating water from the side valleys due to the formation of informal settlements in the wetlands impermeable surfaces are increasing in the lowest flood prone areas resulting in increased problems there has been very limited hydro meteorological data monitoring in the city to carry out in depth flood modeling fieldwork and in situ data collection was conducted in 2012 as part of a joint un habitat s cities and climate change ccc and kampala capital city authority kcca initiative an automatic weather station was installed to collect rainfall data fig 4 and other meteorological data such as temperature and wind fig 4 shows the rainfall intensity used for flash flood modeling in this study since only one rainfall station is available in the study area which situated in the upper lubigi sub catchment the study assumed a spatially homogeneous rainfall distribution applied to the entire catchment the rainfall event of 66 2 mm is measured by an automatic weather station which is equal to a 2 year return period event the return periods are based on the kampala drainage master plan kdmp 2002 3 2 texture information derived from the soil maps the three soil databases were translated to texture information as follow the soil landscape relation was created in the un habitat project sliuzas et al 2013 established a preliminary soil map of the upper lubigi catchment based on the gathered preliminary soil data from 30 ring samples a strong relationship between the texture classes and the landscape morphology appeared to be present soil samples were collected from the representative landscape units and the critical aspect was soil texture classes belong to four landscape units i e valley floor bottom slope mid slope and hilltop the dominant soil texture classes in the catchment were clay c for valley floor sand clay loam sacl and sandy clay sac for the bottom slope sandy clay loam sacl for mid slope and sacl and sandy loam sal for hilltop areas as reported by rossiter 2014 the catchment landscape was highly consistent regarding the pattern of laterite capped hills and swampy inter hill valleys therefore upscaling of lubigi catchment observed soil texture classes to the whole kampala city is reasonably possible thus for smls this study used the upscaled soil texture classes to predict the necessary shps by applying ptfs as shown in table 2 according to the 1 5 000 000 scale fao world soil data source 1991 the soil texture class of the kampala city is represented by sacl except for a few areas in the eastern part of the lake victoria which is covered by sal the soil texture classes from smfao database used for ptfs are shown in table 2 the soilgrids250m database spatially variable maps of sand and clay contents were used as the predictors for ptfs texture ranges are shown in table 2 other inputs for ptfs are the same as those of smfao and smls the soil texture data from smsg maps shows that non built up and green areas wetlands are characterized by clay soil texture data whereas built up areas are dominated by sand content as big as 64 in the city center in this regard the prediction of soil texture data by using the smsg framework consider the presence of urban areas to some extent as indicated by hengl et al 2017 4 results and discussion 4 1 bulk density fig 5 presents the influence of bulk density for uncompacted and compacted scenarios for the three soil databases when the uncompacted scenario is considered the soil becomes loose with a lower bulk density which was predicted in the wetlands and at the edge of the city where the area is well covered by vegetation for the smls the lowest value of bulk density was predicted in the lower slope and valley floor landscape units characterized by c and sac while the higher bulk density was predicted in the middle slope and hilltop landscape units represented by sal and sacl fig 5 a in the case of smfao fig 5 c since the majority of the catchment area is represented by sacl texture class the predicted bulk density was only different according to the incorporated vegetation cover fraction the result when using smsg indicates that in the built up areas where the soil texture data is sandy soil the predicted bulk density was higher while in the non built up areas where the soil texture data is clay the predicted bulk density was lower fig 5 b the predicted bulk density was higher when using smsg compared to the other two soil databases particularly in the city center which is primarily due to its recent inclusion of land use urbanization in the isric soil database prediction system hengl et al 2017 in general the predicted bulk density distribution follows the distribution of soil texture classes with the higher bulk density predicted in the sand content and sacl areas while the lower bulk density was predicted in the c and sal content areas when the compaction is introduced in the predictive equations the predicted bulk density was increased quite significantly for all soil databases fig 5 d e f however the increase is lower when using smsg because both sand and clay soil texture data are less affected by compaction compared to the other soil texture classes used in the case of smls and smfao as shown in the figure the increment in the predicted bulk density was varying between 9 and 22 according to the type of soil texture classes and the degree of compaction used in the prediction system bulk density is in general increases with the increase in the degree of compaction which runs from hard to dense as shown in table 1 and fig 2 4 2 prediction of soil hydraulic properties for flood modeling table 3 shows the ranges of the predicted soil hydraulic properties shps needed for openlisem flood modeling for both compacted and uncompacted scenarios the shps needed for the openlisem model i e ksat porosity initial soil moisture content and matric suction psi were predicted for both uncompacted and compacted scenarios based on the soil data derived from the three soil databases the values and the spatial variability of the predicted shps mainly follow the predicted bulk density distribution for instance the predicted initial soil moisture content was higher in the wetlands and vegetation cover areas following the distribution of porosity which is associated with lower bulk density although the effect of compaction scenario can be applied to all shps for the sake of reducing the complexity in openlisem software we used only ksat and porosity as compacted input while keeping wetting front and initial moisture content input as uncompacted ksat and porosity are the main shps affect the infiltration characteristics of the openlisem model and therefore we exclusively discussed their values and distribution under both compacted and uncompacted scenarios under section 4 3 4 3 spatial distribution of the predicted porosity and ksat 4 3 1 porosity the result of the predicted porosity for the uncompacted scenario and the percentage changes due to soil compaction is presented in fig 6 for the uncompacted scenario the predicted porosity when using smls was varying across the landscape units the valley floor bottom slope vs middle slope hilltop while in the cases of smsg and smfao the predicted porosity was varying across the built up and non built up areas in all cases the predicted values and distribution of porosity follow the distribution of predicted bulk density higher porosity was predicted in the low bulk density areas e g in the wetlands and the outskirt of the city covered by vegetation while low porosity was predicted in the city center and built up areas the predicted porosity in the wetlands and vegetation cover areas were similar for all soil databases although their values are different according to the predicted bulk density the higher values and heterogeneous distribution was well predicted when using smls and then followed by smfao and smsg the predicted porosity when using smsg was quite lower compared to the other two soil maps which is mainly due to the higher bulk density predicted in the city center when using this soil database 4 3 2 saturated hydraulic conductivity ksat the result of the predicted saturated hydraulic conductivity ksat for the uncompacted scenario and the percentage changes due to soil compaction is presented in fig 7 since ksat was predicted as a function of bulk density and porosity the variability of the predicted ksat follows similar distribution as the predicted bulk density and porosity when using smls the highest and lowest ksat predicted at hilltop middle slope and valley floor lower slope landscape units respectively fig 7 a the predicted ksat at the valley floor landscape unit is about five times smaller than the values predicted at the hilltop landscape unit which is associated with the extremely higher porosity in the swampy areas which are characterized by clay soil texture in the case of both smsg and smfao the distribution of the predicted ksat was varying across the built up and non built up with the highest ksat predicted at the non built up areas covered by vegetation while the lowest ksat predicted at the built up areas fig 7 b and c again similar to porosity fair heterogeneous distribution was well predicted when using smls and then followed by smsg when using smfao there was large inter difference between the values simulated in the vicinity of lake victoria and city center and the intra difference against the other two soil maps was also high in areas close to lake victoria huge differences predicted because these areas were represented by sal which is further enhanced by the introduced uncompacted scenario consequently higher ksat 4 3 3 the effect of compaction on porosity and ksat fig 6 d e f shows the effects of soil compaction on the predicted porosity in the catchment as shown in the figure compaction affects both values and the spatial distribution of the porosity across the catchment which is mainly due to the increased bulk density as a result of soil compaction under all soil databases wetlands and non built up areas are profoundly affected by compaction reducing the porosity up to 18 39 however the built up areas are least affected by compaction with porosity reduction varying between 6 and 14 mainly because the compacted areas are usually distributed outside the built up areas e g playing ground murrum roads and tarred roads the effect of compaction on the predicted ksat is also shown in fig 7 d e f as shown in the figure the predicted ksat was reduced quite significantly with the percentage changes reaching up to 99 in the wetland areas compaction increased bulk density consequently decreased both porosity and ksat values with the degree of compaction richard et al 2001 saxton and rawls 2006 the result of the compacted scenario indicated that the predicted porosity and ksat when using smsg was least affected compared to smls and smfao and one possible reason is due to the inclusion of urbanized areas in the soilgrids system which causes a relatively homogeneous sandy soil texture data in the city center area since ksat was predicted as a function of compacted porosity a decrease in porosity caused a significant reduction in the predicted ksat similar studies found that the increase in urban soil compaction has resulted in decreased in saturated hydraulic conductivity for instance gregory et al 2006 found that increased in soil compaction caused a reduction in saturated hydraulic conductivity by 75 similarly ossola et al 2015 found a reduction in porosity in the compacted parks which consequently caused a decrease in saturated hydraulic conductivity 4 3 4 comparison of the predicted shps using smls with observations the predicted ksat and porosity under uncompacted conditions when using smls was compared with the field data collected from upper lubigi catchment as shown in table 4 field data have shown that the valley floor landscape unit with heavy clay in the swampy shrubs areas has lower ksat varying between 1 and 20 mm hr but the predicted ksat at the same landscape unit shows higher values varying between 16 and 35 mm hr the observed values in the hilltop with sandy clay loam in the grass shrubs areas have high ksat values varying between 90 and 150 mm hr while the predicted ksat at the same landscape unit shows comparable values which are varying between 78 and 140 mm hr however the predicted porosity at the valley floor was underestimated compared to the field data as shown in table 4 possibly because of a strong effect of organic matter in the samples that do not appear in the ptfs both observed and predicted porosity at the hilltop landscape unit has a relatively low value varying between 0 51 0 61 and 0 46 0 52 cm3 cm3 respectively the variation in values from the ptfs is smaller than the reality but more samples would have to be taken to investigate this better a comparison of the observed and predicted ksat under the compacted scenario at the valley floor shows relatively a similar result as shown in table 4 which are mainly due to the high effect of compaction at the clay content area as shown in the table the predicted ksatcomp in the hilltop landscape unit was moderately overestimated compared to the observed ksatcomp in the other landscape units the predicted ksatcomp was within the range of the observed values in the case of porosity the predicted values at all landscape units were underpredicted compared to the observed values at the respective landscape units 4 4 the sensitivity of flood dynamics to different soil databases 4 4 1 flood simulations and catchment hydrological behaviour table 5 presents the results of the catchment surface water balance for six different model simulations and the percentage differences between compacted and uncompacted scenarios the usage of smls and smsg appears in much higher flood volumes than the smfao 14 7 106 and 16 0 106 m3 versus 6 2 106 m3 and flooded areas of 49 1 and 53 0 km2 versus 26 1 km2 this is caused by the infiltration as illustrated by the infiltration amounts and aggregated runoff percentages calculated as total discharge total rainfall as shown in the table with the compaction scenario considered the flooded area has grown by 6 for both smls and smsg and 12 for smfao the simulated total discharges were also increased by 11 10 and 10 for smls smsg and smfao respectively the use of soil compaction increased runoff percentage for all soil maps but the increment is higher when using smfao compared to the other two soil maps 4 4 2 infiltration dynamics fig 8 shows infiltration maps simulated by using the three soil databases under the uncompacted scenario when using smls fig 8 a the simulated infiltration dynamics has followed the distribution of predicted porosity and ksat at each landscape units however due to the sealing characteristics at the hilltop and middle slope landscape units the simulated infiltration at those landscape units was relatively lower except at the grid cells where it s covered by vegetation similarly in the case of smsg the higher infiltration was simulated where the higher ksat was predicted fig 8 b since the predicted porosity and ksat followed the distribution of built up and non built up areas the dynamics of the simulated infiltration was followed the same pattern with higher infiltration simulated in the non built up areas while the lower infiltration simulated in the built up areas the simulated infiltration when using smfao database shows a widespread distribution of high values of infiltration in the catchment fig 8 c as shown in the figure most of the area has values larger than 75 mm particularly in the low density urban areas and wetlands the result of this study indicates that the variability of the simulated infiltration dynamics is well associated with the values and the spatial variability of the shps in particular predicted ksat and porosity higher infiltration is predicted in the well vegetated and non built up areas 20 5 of the total catchment area counter intuitive is the generally high value of infiltration simulated in swampy areas due to the overpressure of flood water during the event which strongly affects the green and ampt model the surface sealing which accounts for 48 of the total area results clearly in much lower infiltration 4 4 3 flood dynamics the uncompacted scenario for flood depth shows widespread flooding along the main channels primary and secondary channels as simulated by using the three soil databases fig 9 a b c the result indicated that the spatial distribution of flood dynamics flood depth flooded area flood volume and duration follows the infiltration dynamics in the catchment and accumulation of flood water at the valley floor transported there by the secondary channels draining the hills in general the simulated flood depth along the primary channels when using smls and smsg 0 5 2 m was deeper than that of smfao 0 5 1 m which is associated with the lower infiltration simulated when using smls and smsg similarly the simulated flood duration when using smls and smsg is longer along both primary and secondary channels compared smfao fig 10 also the calculated flooded areas and flood volume e g at 0 5 m was 23 km2 and 11 281 million m3 when using smls and smsg which are almost double the result found when using smfao at the same water depth fig 11 since in the case of smfao the simulated infiltration was higher there is little water left for direct runoff which leads to shallow flood depth consequently less flooded area flood volume and duration 4 4 4 the effect of soil compaction on infiltration and flood dynamics table 6 shows the percentage reduction in infiltration and the increments in flood depth as a result of soil compaction as shown in the table the higher infiltration reduction was simulated at the lower infiltrating area 40 80 mm which is reduced by 13 4 17 9 and 14 2 for smls smsg and smfao respectively however in the higher infiltrated areas 160 200 mm mainly in the swamps the simulated total infiltration was increased instead of decreasing likely due to an overpressure of greater than 1 5 m of flood water produced due to compaction as a result of reduced infiltration flood depth at different levels was increased as shown in table 6 which is consistent with the infiltration reduction however the flood depth increment in the swampy areas was due to the combined effect of flow concentration from high elevation and compaction similarly the simulated flood duration not shown here and both the calculated flooded areas and flood volume fig 11 were all increased following the infiltration reduction and nearly overweight the difference caused as a result of using different soil databases the effect of compaction on infiltration was high because there is a double effect of compaction on green and ampt infiltration as both porosity and ksat are affected since the ksat was predicted as a function of porosity in the ptfs and the green and ampt infiltration equation uses both variables the combined effect reduced the predicted infiltration quite significantly consequently more water is converted into the overland flow and increased water depth flood areas flood volume and flood duration this may cause an overestimation of the effect of compaction in this methodology 4 4 5 flood hazard aspects the effects of flood depth and duration on buildings the total number of structures affected by flood depth and flood duration was calculated by using flood depth greater than 10 cm and flood duration greater than 30 min fig 11 the model does not have a concept of individual buildings but uses a fraction of building per grid cell and gives the total building surface affected by different flood levels the average structure size is 90 m2 which is used to convert the total built up area to individual buildings as shown in the figure the total number of structures affected was largest at lower water depths between 10 and 50 cm in all scenarios and highest for smls and smsg under both uncompacted and compacted conditions it is very different between the scenarios of compacted and uncompacted the main difference is between the smfao and the others nevertheless there are differences of several thousand buildings under the uncompacted scenario the number of structures affected by flood depth at lower flood depth of 10 and 50 cm was 91113 87686 and 67 307 for smsg smls and smfao respectively these numbers are increased to 93 585 for smsg 90 403 for smls and 71 333 for smfao under the compacted scenario the areas with deepest flood water in the wetlands are not yet used for habitation although also these areas are more and more used for settlement as shown in fig 11 the duration the structures stayed under water was also high at lower flood duration between 0 5 and 1 h and the effect was high when using smsg and smls compared to smfao under both uncompacted and compacted conditions the number of structures stayed under flood water at lower duration was mainly at the location of the secondary channels between 15 and 20 h the number of structures affected by flood duration was 34628 34221 and 25 350 under the compacted condition and 33022 32 737 and 23 986 under the uncompacted condition for smls smsg and smfao respectively the effect at 15 20 hr duration was mainly from the main channels in the wetlands 4 4 6 model verification with earlier calibrated simulations due to the lack of observed actual discharge data at the main outlets the openlisem model verification has only handled by comparing the model flood inundation map with earlier calibrated flood line developed by kdmp kcca 2002 the earlier flood lines in kampala were generated by using hec georas river analysis system of the hydrologic engineering center of the us army corps of engineers for return periods of 2 10 and 100 years the generated flood lines are simply the strip or areas along the sides of drainage channels that will be prone to flood inundation for the different return period fig 12 a shows the effect of 2 10 and 100 year return period on flood extent and it s reported that the flood extent for different return period is all similar around the major wetlands mainly due to the flat cross section used in the simulation when the capacity of the natural channel is exceeded in most cases during floods with return periods of two years or less floodwaters spread over the full extent of the floodplain up to the steeper regions that define the floodplain with the flood depth increases slightly for more extended return periods the un habitat project sliuzas et al 2013 has also found that the width of flooding along the primary and secondary channels of all drainage systems does not differ much for the different return periods which is primarily caused by the relatively small cross sectional areas of the natural channels along the floodplains the openlisem model simulated with compacted scenarios by using the storm event of 66 2 mm which is equivalent to an event of the 2 year return period was used to compare the simulated flood extent with earlier flood extent the model result as shown in fig 12 b c d indicates that flood water spread over the full extent of the plain around the main drainage channels but with different water depth distribution the f statistics goodness of fit was calculated by using eq 6 and the result is shown in table 7 the high value of f statistics indicates high goodness fit between areas of flood line and model simulation 6 f aos ao a s a o s 100 ao refers to the flooded area observed under flood lines as indicates flooded area simulated by openlisem model and aos represents the intersected flooded area between ao and as the result of the flood extent maps indicated that flood extent accuracy was lower when using the smfao for hydrological modeling in line with the effect of infiltration described above note that this is just a general comparison between two models that use entirely different principles and datasets however because the hec ras derived flood lines are used in the spatial planning of kampala there is some merit in comparing the openlisem simulations to the flood lines 5 conclusions this study applied three different soil databases to predict soil hydraulic properties used in an integrated flood model in order to determine how sensitive the flood dynamics are to the predicted shps and if possible to select the best data source for integrated flash flood modeling in an urbanized area where there is a scarcity on local soil physical data three soil data sources were used coupling soil texture to landscape form using the soilgrids250m database and deriving texture information from the small scale fao soil map in doing flood predictions one may choose to include compaction and the effect of vegetation as an additional factor apart from surface sealing by roads and buildings we assumed that bare areas would have a certain degree of compaction and investigated its impact for each of the three soil data sources the results indicate that the choice of the data source has a high influence on both the quantity and spatial variability of infiltration which naturally directly affects runoff and flooding on top of that the effect of sealing and compaction is equally essential and nearly outweighs the differences caused by the use of different soil databases this indicates that sufficient effort should be attributed to getting actual compaction information in an area for which a flood simulation is done to establish in how far this affects reality when looking at the indicators for flood hazard buildings affected the differences are less pronounced but still large in terms of the number of the building because the shape of the terrain causes a similar flood extent the use of the saxton and rawls 2006 pedotransfer functions have the added advantage of incorporating the effects of compaction and organic matter into their equations so that in the future simulations the urban fabric can be represented in detail using for instance high resolution earth observation we did not attempt an uncertainty analysis of the simulated scenarios using an estimated texture in multilinear regression ptfs which by themselves have a varying error ranges and subsequently using these in a complex spatial model may very well result in a large uncertainty this could be studied with a monte carlo analysis but is beyond the scope of this study the study found that soil databases with high variability of soil physical properties e g when using smls and smsg can better predict shps used for flood modeling comparison of flood inundation areas using the three different soil maps with earlier calibrated simulations indicates that smsg and smls are a better strategy although still different from the accepted flood extent in the kampala master drainage plan in the case of kampala city given the fact that the soilgrids database is available globally while soil landscape relations may not be available everywhere it seems to be an acceptable source to derive infiltration properties because soilgrids comes with a warning and an extensive error analysis and is not meant for detailed studies as is done here it is of course advisable to check the results against local measurements of texture and in combination with ptfs against resulting hydrological properties nevertheless the results seem promising for integrated flood modeling in those urbanized areas where most buildings are constructed directly on the original soil declaration of competing interest none acknowledgment the project on the flood modeling in kampala was initiated with funding from un habitat s cities and climate change and the current work is the continuation of this project with the financial support granted by the university of twente the netherlands the authors would like to thank numerous people who collected field data under un habitat s project we also thank bastian van den bout for his support on the openlisem model database preparation 
