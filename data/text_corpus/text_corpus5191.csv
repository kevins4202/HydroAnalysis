index,text
25955,we introduce the aurora software which performs continuous time random walk ctrw particle tracking pt of dissolved contaminants on macroscopic groundwater flow fields computed using modflow this is a natural and practical approach to modeling transport at field scale deterministic explicitly resolved flow information is used directly while unresolved small scale physical and chemical heterogeneity that may cause non fickian transport is captured by the ctrw existing particle trackers are limited to streamline tracing or to homogeneous velocity fields and no existing software handles subsurface non fickian transport so this represents a substantial technical advance fickian transport simulation can also benefit from aurora s pt approach which lacks numerical dispersion and other artifacts we outline the equations aurora implements and explain the technical and practical details of modflow interoperation two demonstrations are presented of pt simulations using aurora to model physics that would otherwise be difficult to treat analytically or numerically highlights aurora is the first full physics particle tracker for groundwater transport problems aurora is the first software to model general non fickian subsurface transport aurora debuts a hybrid large scale deterministic small scale stochastic approach aurora partners with modflow to obtain large scale groundwater flow information aurora does non fickian particle tracking on any model modflow 2005 can simulate keywords modflow particle tracking groundwater solute transport non fickian software software availability aurora is open source software developed in c and runs on any modern version of windows that supports net framework 4 6 or higher the aurora visualizer scripts can run on any platform with python 3 6 or higher installed and the ability to install packages via pip the source code for aurora and the visualizer the documentation and examples may be freely downloaded from a git repository and a zip archive containing compiled binaries is also available links to all resources may be found at the aurora homepage http aurora hub gitlab io any computer that runs windows 7 or later should be capable of running aurora 1 introduction classical eulerian numerical models that employ a discretized advection dispersion equation ade are often not sufficient to model groundwater solute transport with needed realism they may fail to capture important advective heterogeneity berkowitz et al 2008 at scales beneath their discretization scale or even above it due to a lack of constraining information and they neglect mobile immobile mass transfer mimt processes such as kinetic sorption and diffusion into secondary porosity non fickian transport models generalize classical models and have demonstrated success at capturing realistic chemical physics that cannot be modeled by classic approaches berkowitz et al 2006 frippiat and holeyman 2008 zheng et al 2011 in the case of the popular continuous time random walk ctrw approach the additional flexibility comes in the free selection of a waiting time probability distribution ψ t that provides another source of randomness on top of classical fickian dispersion there has typically been an all or nothing choice when modeling groundwater velocity heterogeneity either it all is modeled explicitly and an ade based transport model used or it all is implicitly combined with all other sources of non fickian behavior in selection of spatially homogeneous model parameters e g ψ t a consequence is that it is difficult to incorporate partial knowledge about large scale flow into non fickian solute transport models and it is generally difficult to predict correct model parameters a priori as these depend in non straightforward ways on the interaction of multiple physical and chemical sources of heterogeneity this limitation may have limited the penetration of advanced transport models from academic research into hydrogeologic practice but whatever the underlying reasons practitioners have typically favored explicitly heterogeneous models incorporating large scale flow information coupled with ade based transport whereas researchers have often chosen conversely employing homogeneous often quasi 1d flow alongside more exotic non fickian transport models ideally a hybrid approach to modeling would be available a allowing usage of explicitly delineated spatially heterogeneous groundwater velocity fields with non fickian transport modeling used only to capture small scale physics and b whose non fickian model parameters are straightforwardly determined by the various sources of heterogeneity allowing a priori prediction the aurora software enables such an approach usgs modflow harbaugh 2005 is a leading groundwater flow model in industry and academia and a large ecosystem of software has developed that integrates tightly with it to enhance its usability e g winston 2000 2009 and extend its numerical capabilities e g bailey et al 2020 bedekar et al 2016 clement 1997 pollock 2016 wei et al 2019 aurora is an addition to this ecosystem and is designed to integrate tightly with the modflow 2005 release and with related usgs codes currently there are many widely used numerical subsurface transport codes including min3p su et al 2017 mt3dms zheng 2010 pflotran lichtner et al 2015 phast parkhurst et al 2004 pht3d prommer and post 2010 appelo and rolle 2010 hydrus sejna et al 2014 rt3d clement 1997 and tough2 zhang et al 2008 general purpose multiphysics solvers such as opengeosys sachse et al 2015 comsol li et al 2009 nardi et al 2014 azad et al 2016 and gem little et al 2018 have also been used for subsurface transport against this background what does new software bring to the table the answer is that the aforementioned packages are all fundamentally eulerian discretizing the ade meaning that they are generally incapable of capturing non fickian transport physics and suffer from numerical dispersion and or oscillations neumann et al 2011 the only general purpose lagrangian subsurface codes we know of are pure streamline tracers that do not consider any dispersive or chemical physics except retardation modpath pollock 2016 and tough2path bonduà and bortolotti 2019 mt3dms is also semi lagrangian but still based on the ade and we are not aware of any software that models non fickian solute transport at field scale in its capacity as a fully general particle tracking package simulating non fickian and fickian transport physics we believe aurora is unique here we briefly summarize the theory underlying aurora discuss its capabilities and use and demonstrate its ability to capture non fickian transport in spatially and temporally non uniform flow fields that would be challenging to otherwise model 2 theory aurora uses a trajectory based variant of the ctrw which is sometimes referred to as a time domain random walk tdrw in which step size is kept constant in the direction of flow this approach has been suggested for a variety of subsurface transport scenarios bodin 2001 painter et al 2008 hansen and berkowitz 2014 cvetkovic et al 2016 and it has recently been shown hansen 2020 hansen and berkowitz 2020 that this ctrw formulation is sufficiently general to capture the major physics relevant to subsurface flow and transport the underlying theory is detailed in hansen and berkowitz 2020 and is only summarized here in a ctrw particle tracking scheme each particle is represented as a spatial location and a time both of which are updated iteratively at iteration n position x n and clock time t c n are updated according to the following equations 1 x n 1 x n v x n v x n d δ x n 2 t c n 1 t c n δ t c n x n is updated by traveling d units along the local streamline and then making a random jump δ x n in the plane orthogonal to the local pore water velocity vector v x n each of whose components is drawn from n 0 2 α t d where α t is the medium s transverse dispersivity the time increment δ t c n is specified via two functions and one parameter chosen by the user f r a probability density function pdf representing the ratio of δ t a n the actual time taken for advection from x n to x n 1 accounting for all advective heterogeneity to d v x n the time taken when accounting only for large scale heterogeneity for the large scale velocity field to be on average correct this distribution must be chosen so that e r 1 several distributions for f r are presently implemented in aurora shifted dirac i e pure streamline tracing inverse gaussian i e ade behavior lognormal and pareto λ the probability of immobilization of any given particle per unit time g t a pdf representing the amount of time a particular particle remains immobilized during a given immobilization event exponential and tempered power law models are available δ t a n is computed by drawing randomly from f r and multiplying by d v x n to compute δ t a n then a number of immobilization events n is selected by drawing randomly from poisson λ δ t a n for each step of each particle n draws from g t are performed summed and then added to arrive at δ t c n once the step is completed a random motion representing local scale fickian dispersion is added in a plane orthogonal to v x n the strength of this transverse dispersion is specified by a user defined dispersivity α t the particle tracking process is summarized in fig 1 all user specified parameters are assumed to apply uniformly in each layer but may differ between them in the above analysis it is taken for granted that v x is available for any valid real valued position vector x however in modflow 2005 velocity information is only reported indirectly as between cell fluxes which must be interpolated to estimate the velocity field see fig 2 additionally the thickness of cells may vary within a single layer even though cells are always rectangular prisms aligned with the global coordinate axes and modflow does not compute flows from a given cell to another cell that differs in more than one of row column and layer even if they share a boundary aurora interpolates velocities correctly regardless of layer geometry using a variant of the pollock method pollock 1988 with additional adjustment for the nonuniform layers that may be present in modflow 2005 models to start we must consider the position vector x as broken into its individual coordinates x x y z with z being the vertical coordinate and we index the cells from 1 in each direction with increasing i j k indexes corresponding respectively to increasing x y z coordinates we define x i to be the x coordinate of the largest x of the two planes defining column i and x 0 to represent the smallest x boundary of the model and likewise for the y and z directions because we wish to interpolate fluxes we define the normalized cell position ratio in the x direction 3 r x x x i 1 x i x i 1 with similar definitions for the y and z directions we define q i j k x to be the flux through and a i j k x the area of the largest x face of cell with index i j k and similarly for the y and z directions for these two quantities we allow all indexes to go to 0 to account for cell faces on the lowest x y and z planes of the domain finally we define θ i j k to be the porosity in cell i j k armed with all these definitions it is straightforward to write down the velocity interpreted from interfacial fluxes v flux of a particle located in cell i j k 4 v flux 1 θ i j k r x q i j k x a i j k x 1 r x q i 1 j k x a i 1 j k x r y q i j k y a i j k y 1 r y q i j 1 k y a i j 1 k y r z q i j k z a i j k z 1 r z q i j k 1 z a i j k 1 z a second consideration is that layers in modflow may be of uneven thicknesses even though cells are always rectangular prisms aligned with the global coordinate axes furthermore modflow does not compute flows from a given cell to another cell that differs in more than one of row column and layer even if they share a boundary see fig 3 consequently we must interpret the upper and lower layer boundaries as interpolating continuously between adjacent cells where layers rise or fall the necessity of this is best seen by considering a steeply inclined system modeled as a single layer because there is no flux through system outer boundaries in modflow flux interpolation would yield velocities that everywhere have no vertical component this would cause particles to travel out of bounds as they would never deviate from their initial injection height thus it is necessary to both model layer boundaries as continuous and to impose an additional vertical velocity to maintain relative position in the notional layer due to flux interpolated advection in the x and y directions obviously we do not want to neutralize flux interpolated injection in the z direction because vertical advection must be allowed when layer interfaces are no longer planar but vary with x and y we must slightly redefine some of our concepts let z k x y represent the upper bound surface of layer k 1 2 with z 0 x y representing the bottom surface of the model so a particle will be in layer k if and only if z k 1 x y x z k x y normalized cell position ratio in the z direction also becomes a function of x and y r z z z k 1 x y z k z k 1 x y it can be shown and should be intuitive that to maintain the same r z as x changes the following equation must hold 5 z x r z z k x 1 r z z k 1 x and similarly for changes in y we then may define an additional velocity v layer acting in the z direction only which serves to adjust the particle position for layer variability 6 v layer x v flux x r z z k x 1 r z z k 1 x r z z k y 1 r z z k 1 y 0 k where k is the z direction unit vector we finally define total velocity as v x v flux x v layer x for use in equation 1 we note that when layer interfaces are horizontal there is no difference between v and v flux 3 interoperation with modflow aurora partners with modflow 2005 by employing its discretization files dis and cell by cell budget files cbc to define the model domain and flow field it works with the same coordinate field employed by modelmuse in the event that software is used the cbc files must be first be processed by usgs gw chart winston 2000 into separate interfacial flux files the full workflow showing tight interface with usgs tools is shown in fig 4 aurora itself is a compiled standalone executable whose output is one or more human readable text files that list the locations of all the particles at given times or the times of arrival of particles at given locations to make sense of these results it ships with an assortment of python scripts collectively known as the aurora visualizer that plot these results to generate plumes particle swarms and breakthrough curves some visualization possibilities include isometric 3d plots of the particle locations at fixed times overlaid on the modflow grid fixed time 2d concentration plumes orthogonal to any of the coordinate axes produced by depth integration and kernel density estimation movies of plume evolution and breakthrough curves generated by kernel density estimation the visualizer scripts were used to generate a number of the figures in this paper a complete description of aurora s capabilities can be found in its included user s guide but here we note that aurora supports general modflow 2005 models with arbitrary row column and layer counts and thicknesses variable thickness layers inactive cells arbitrary boundary conditions and transient models with multiple stress periods and time steps per period releases of particles may occur from a variety of source geometries throughout the modeled aquifer may be flux or uniformly weighted and may be instantaneous or continuous the output of aurora consists of listings of particle locations recorded at user specified times and listings of times of particle arrivals recorded at user specified planes or cylinders by use of included visualization scripts these can be visualized as plumes and breakthrough curves and plume evolution movies can be made all of the additional physics defining the aurora particle tracking model are defined in a single marshal input file located in the directory containing the dis and other needed files and in which output files will be saved 4 specification of marshal file 4 1 overview as mentioned all that is necessary to define an aurora model is to correctly specify the contents of the marshal file is divided into blocks that begin with a capitalized keyword main domain layer breakthroughs profiles or source and end with the keyword end the marshal file must begin with exactly one main block describing the non physics details of the simulation the additional physics defining the parameters and pdf s mentioned in section 2 i e f r λ and g t must also be specified for each layer this may be done for all layers at once using a single domain block and or for individual layers with individual layer blocks after these initial blocks any number including zero of each of the other types of blocks may be listed in any order definitions of the contents of each of the several types of block are defined below the sample marshal files shown in tables 1 and 2 illustrate the structure and format of the various blocks 4 2 main block the main block defines global information about the simulation the name of the discretization file followed by the names of the respective right front and bottom flux files generated by gw chart from the modflow cbc file comprise the first four next the step length d is defined and finally the maximum time of the pt simulation is given 4 3 domain and layer blocks all the information about the additional small scale physics must be defined for each layer of the model this can be done for all layers at once using a domain block or for a specific layer using layer block where represents the index of the layer being defined with layer 0 being the bottom layer of the model an unlimited number of these blocks can be used with blocks later in the marshal file redefining parameters defined in earlier blocks where these conflict the syntax inside either type of block is the same the first two lines are respectively the porosity θ and the local scale transverse dispersivity α t next comes an advective heterogeneity sub block which contains the parameters defining f r which may be of five types none indicating pure tracing the large scale streamlines 7 f r δ r 1 ade indicating advective dispersive fickian small scale behavior defined by a single user defined parameter α l representing longitudinal dispersivity 8 f r 1 r 4 π a r exp r 1 2 4 a r where a α l d lognormal indicating mildly heavy tailed breakthrough where the pdf for r is defined by a single log variance σ 2 9 f r 1 r σ 2 π exp ln r σ 2 2 2 2 σ 2 the log normal distribution is a two parameter distribution but the requirement that mean flow velocity is unchanged by the small scale heterogeneity imposes the condition e r 1 and constrains the log mean pareto indicating a heavy tailed advective breakthrough with a pure power law tail that is defined by a single parameter β 10 f r β 1 β β 1 β r β 1 as elsewhere the constraint e r 1 removes one degree of freedom from the standard pareto distribution tpl standing for truncated or tempered power law where 11 f r r r 1 1 β e r r 2 unlike the other distributions aurora does not normalize this distribution to enforce the constraint e r 1 the user may choose any two of the parameters r 1 r 2 and β freely and must select the other so that the constraint is enforced finally comes the mimt sub block which defines λ and g t three options are available none indicating no immobilization at all so neither λ nor g t need to be specified exponential indicating an immobilization time pdf 12 g t μ e μ t where μ is an arbitrary user defined rate constant tpl indicating truncated power law behavior with a slightly different definition from that used for small scale advective heterogeneity 13 g t 1 t t 1 1 β e t t 2 the tpl definition here is based on the shifted pareto distribution and so has support on the whole half line 0 and any positive values of the exponent β and truncation time t 2 are permitted 4 4 source blocks any number of source blocks may be employed each defining a source of particles on the first line the total number of particles to be released from the source is specified next is a sub block representing the release regime which may be instant i e all particles released at once or continuous particles released at uniformly distributed random times in a specified interval on the next line a selection is made as to whether particles should be randomly spatially distributed throughout the source region uniformly weighted or if their spatial distribution within the region should be proportional to the groundwater speed flux weighted finally there is a sub block defining the source region itself options are box cylinder sphere and tube a 2d manifold representing only the curved surface of a finite height cylinder 4 5 breakthrough blocks any number of breakthroughs blocks may be employed although there is generally no reason for more than one to be used there may be any number of sub blocks in a breakthroughs block each defining the geometry of a 2d surface at which particle crossing times are to be recorded as well as a crossing direction required to trigger a breakthrough in out or either all surfaces are defined by some equation h x c where c is a scalar constant and an in transition is defined as one where h x n 1 c h x n where x n represents the particle s position after the n th step an out transition is defined conversely two types of breakthrough surface may be defined a tube or a plane of infinite extent 4 6 profile blocks any number of profiles blocks may be employed although there is generally no reason for more than one to be used there may be any number of lines in a given profile block each containing a time at which locations of all active particles those which have not previously reached a stagnation point should be recorded 5 demonstrations 5 1 rocky mountain arsenal aquifer as a demonstration of aurora s capabilities we apply it to modeling transport in the approximately 20 km 2 of unconfined aquifer shown in the rocky mountain arsenal tutorial that ships with the usgs modelmuse software this model exhibits a number of features including strongly nonuniform layer boundary elevations regions of inactive cells low conductivity inclusions and irregular segmented specified head boundaries the shore of a lake and the course of a river whose elevation changes significantly as it passes through the domain head broadly decreases from a high elevation lake in the north to the lower elevation river in the south however there is also a significant source of recharge at a pond in the center of the domain which represents a local maximum of hydraulic head a modelmuse screenshot showing the model domain is presented in fig 5 aurora was employed to model the fate of a one time accidental release of solute into the pond and to track the transport of the particles as they travel down gradient to the river modpath was also employed on the modflow results to generate path lines for comparison with the evolution of the plume in aurora for comparison two advective heterogeneity models were employed ade and pareto with all other parameters held constant the modpath computed path lines are shown superimposed in fig 5 and the aurora marshal file is shown in table 1 no mimt was incorporated examination of the plume snapshots in fig 6 shows broad agreement with the modpath path lines in both cases and a strikingly distinct plume tailing caused by the power law physics 5 2 push pull tracer test in layered aquifer we consider a push pull single well injection extraction test at a monitoring well penetrating a confined aquifer featuring three layers of differing properties one of whose hydraulic conductivities is two orders of magnitude higher than the others this scenario illustrates a number of features that complement the first demonstration mimt with exponentially distributed immobilization times multiple stress periods and time steps including full flow reversal independent definition of layer properties flux weighted injection on a tubular surface and uni directional breakthrough recording at that same tubular surface the marshal file for this simulation is presented in table 2 the swarm of injected particles at the end of the push phase is shown in fig 7 with the much larger spread of particles in the high conductivity layer notable the breakthrough curve at the well is shown in fig 8 as discussed in hansen et al 2016 the heterogeneity of the flow regime should not be visible in the breakthrough in the absence of any mimt there would be no scattering under the modeled conditions with all particles re converging at the well at twice the duration of the push phase this was confirmed by another simulation with mimt off not shown and in the shown breakthrough there is a smooth tail whose shape is determined by the immobilization parameter μ also note the presence of a secondary spike at twice the push phase duration reflecting those particles that were never immobilized it is a convenient feature of particle tracking simulation that these types of features not accessible to traditional push pull interpretation analytical solutions that model mimt with a retardation factor but generally present at early time hansen 2015 are captured automatically 6 summary we presented aurora a software that performs particle tracking on velocity fields it computes from modflow cell water budgets aurora enables a hybrid approach to modeling solute transport on velocity fields that are explicitly characterized at a coarse scale but which contain small scale heterogeneity that may lead to non fickian behavior the approach which we dubbed ctrw on a streamline hansen and berkowitz 2020 is a particle tracking technique that represents an advance of the current state of the art in which non fickian transport models or explicitly delineated heterogeneous velocity fields are employed but generally not both our approach separates the parameters characterizing macroscopic flow small scale advective heterogeneity and mimt each can be specified a priori from available data we summarized the underlying theory described how aurora works and offered two demonstrations showcasing different capabilities declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement skh holds the helen unger career development chair in desert hydrogeology bb holds the sam zuckerberg professorial chair in hydrology 
25955,we introduce the aurora software which performs continuous time random walk ctrw particle tracking pt of dissolved contaminants on macroscopic groundwater flow fields computed using modflow this is a natural and practical approach to modeling transport at field scale deterministic explicitly resolved flow information is used directly while unresolved small scale physical and chemical heterogeneity that may cause non fickian transport is captured by the ctrw existing particle trackers are limited to streamline tracing or to homogeneous velocity fields and no existing software handles subsurface non fickian transport so this represents a substantial technical advance fickian transport simulation can also benefit from aurora s pt approach which lacks numerical dispersion and other artifacts we outline the equations aurora implements and explain the technical and practical details of modflow interoperation two demonstrations are presented of pt simulations using aurora to model physics that would otherwise be difficult to treat analytically or numerically highlights aurora is the first full physics particle tracker for groundwater transport problems aurora is the first software to model general non fickian subsurface transport aurora debuts a hybrid large scale deterministic small scale stochastic approach aurora partners with modflow to obtain large scale groundwater flow information aurora does non fickian particle tracking on any model modflow 2005 can simulate keywords modflow particle tracking groundwater solute transport non fickian software software availability aurora is open source software developed in c and runs on any modern version of windows that supports net framework 4 6 or higher the aurora visualizer scripts can run on any platform with python 3 6 or higher installed and the ability to install packages via pip the source code for aurora and the visualizer the documentation and examples may be freely downloaded from a git repository and a zip archive containing compiled binaries is also available links to all resources may be found at the aurora homepage http aurora hub gitlab io any computer that runs windows 7 or later should be capable of running aurora 1 introduction classical eulerian numerical models that employ a discretized advection dispersion equation ade are often not sufficient to model groundwater solute transport with needed realism they may fail to capture important advective heterogeneity berkowitz et al 2008 at scales beneath their discretization scale or even above it due to a lack of constraining information and they neglect mobile immobile mass transfer mimt processes such as kinetic sorption and diffusion into secondary porosity non fickian transport models generalize classical models and have demonstrated success at capturing realistic chemical physics that cannot be modeled by classic approaches berkowitz et al 2006 frippiat and holeyman 2008 zheng et al 2011 in the case of the popular continuous time random walk ctrw approach the additional flexibility comes in the free selection of a waiting time probability distribution ψ t that provides another source of randomness on top of classical fickian dispersion there has typically been an all or nothing choice when modeling groundwater velocity heterogeneity either it all is modeled explicitly and an ade based transport model used or it all is implicitly combined with all other sources of non fickian behavior in selection of spatially homogeneous model parameters e g ψ t a consequence is that it is difficult to incorporate partial knowledge about large scale flow into non fickian solute transport models and it is generally difficult to predict correct model parameters a priori as these depend in non straightforward ways on the interaction of multiple physical and chemical sources of heterogeneity this limitation may have limited the penetration of advanced transport models from academic research into hydrogeologic practice but whatever the underlying reasons practitioners have typically favored explicitly heterogeneous models incorporating large scale flow information coupled with ade based transport whereas researchers have often chosen conversely employing homogeneous often quasi 1d flow alongside more exotic non fickian transport models ideally a hybrid approach to modeling would be available a allowing usage of explicitly delineated spatially heterogeneous groundwater velocity fields with non fickian transport modeling used only to capture small scale physics and b whose non fickian model parameters are straightforwardly determined by the various sources of heterogeneity allowing a priori prediction the aurora software enables such an approach usgs modflow harbaugh 2005 is a leading groundwater flow model in industry and academia and a large ecosystem of software has developed that integrates tightly with it to enhance its usability e g winston 2000 2009 and extend its numerical capabilities e g bailey et al 2020 bedekar et al 2016 clement 1997 pollock 2016 wei et al 2019 aurora is an addition to this ecosystem and is designed to integrate tightly with the modflow 2005 release and with related usgs codes currently there are many widely used numerical subsurface transport codes including min3p su et al 2017 mt3dms zheng 2010 pflotran lichtner et al 2015 phast parkhurst et al 2004 pht3d prommer and post 2010 appelo and rolle 2010 hydrus sejna et al 2014 rt3d clement 1997 and tough2 zhang et al 2008 general purpose multiphysics solvers such as opengeosys sachse et al 2015 comsol li et al 2009 nardi et al 2014 azad et al 2016 and gem little et al 2018 have also been used for subsurface transport against this background what does new software bring to the table the answer is that the aforementioned packages are all fundamentally eulerian discretizing the ade meaning that they are generally incapable of capturing non fickian transport physics and suffer from numerical dispersion and or oscillations neumann et al 2011 the only general purpose lagrangian subsurface codes we know of are pure streamline tracers that do not consider any dispersive or chemical physics except retardation modpath pollock 2016 and tough2path bonduà and bortolotti 2019 mt3dms is also semi lagrangian but still based on the ade and we are not aware of any software that models non fickian solute transport at field scale in its capacity as a fully general particle tracking package simulating non fickian and fickian transport physics we believe aurora is unique here we briefly summarize the theory underlying aurora discuss its capabilities and use and demonstrate its ability to capture non fickian transport in spatially and temporally non uniform flow fields that would be challenging to otherwise model 2 theory aurora uses a trajectory based variant of the ctrw which is sometimes referred to as a time domain random walk tdrw in which step size is kept constant in the direction of flow this approach has been suggested for a variety of subsurface transport scenarios bodin 2001 painter et al 2008 hansen and berkowitz 2014 cvetkovic et al 2016 and it has recently been shown hansen 2020 hansen and berkowitz 2020 that this ctrw formulation is sufficiently general to capture the major physics relevant to subsurface flow and transport the underlying theory is detailed in hansen and berkowitz 2020 and is only summarized here in a ctrw particle tracking scheme each particle is represented as a spatial location and a time both of which are updated iteratively at iteration n position x n and clock time t c n are updated according to the following equations 1 x n 1 x n v x n v x n d δ x n 2 t c n 1 t c n δ t c n x n is updated by traveling d units along the local streamline and then making a random jump δ x n in the plane orthogonal to the local pore water velocity vector v x n each of whose components is drawn from n 0 2 α t d where α t is the medium s transverse dispersivity the time increment δ t c n is specified via two functions and one parameter chosen by the user f r a probability density function pdf representing the ratio of δ t a n the actual time taken for advection from x n to x n 1 accounting for all advective heterogeneity to d v x n the time taken when accounting only for large scale heterogeneity for the large scale velocity field to be on average correct this distribution must be chosen so that e r 1 several distributions for f r are presently implemented in aurora shifted dirac i e pure streamline tracing inverse gaussian i e ade behavior lognormal and pareto λ the probability of immobilization of any given particle per unit time g t a pdf representing the amount of time a particular particle remains immobilized during a given immobilization event exponential and tempered power law models are available δ t a n is computed by drawing randomly from f r and multiplying by d v x n to compute δ t a n then a number of immobilization events n is selected by drawing randomly from poisson λ δ t a n for each step of each particle n draws from g t are performed summed and then added to arrive at δ t c n once the step is completed a random motion representing local scale fickian dispersion is added in a plane orthogonal to v x n the strength of this transverse dispersion is specified by a user defined dispersivity α t the particle tracking process is summarized in fig 1 all user specified parameters are assumed to apply uniformly in each layer but may differ between them in the above analysis it is taken for granted that v x is available for any valid real valued position vector x however in modflow 2005 velocity information is only reported indirectly as between cell fluxes which must be interpolated to estimate the velocity field see fig 2 additionally the thickness of cells may vary within a single layer even though cells are always rectangular prisms aligned with the global coordinate axes and modflow does not compute flows from a given cell to another cell that differs in more than one of row column and layer even if they share a boundary aurora interpolates velocities correctly regardless of layer geometry using a variant of the pollock method pollock 1988 with additional adjustment for the nonuniform layers that may be present in modflow 2005 models to start we must consider the position vector x as broken into its individual coordinates x x y z with z being the vertical coordinate and we index the cells from 1 in each direction with increasing i j k indexes corresponding respectively to increasing x y z coordinates we define x i to be the x coordinate of the largest x of the two planes defining column i and x 0 to represent the smallest x boundary of the model and likewise for the y and z directions because we wish to interpolate fluxes we define the normalized cell position ratio in the x direction 3 r x x x i 1 x i x i 1 with similar definitions for the y and z directions we define q i j k x to be the flux through and a i j k x the area of the largest x face of cell with index i j k and similarly for the y and z directions for these two quantities we allow all indexes to go to 0 to account for cell faces on the lowest x y and z planes of the domain finally we define θ i j k to be the porosity in cell i j k armed with all these definitions it is straightforward to write down the velocity interpreted from interfacial fluxes v flux of a particle located in cell i j k 4 v flux 1 θ i j k r x q i j k x a i j k x 1 r x q i 1 j k x a i 1 j k x r y q i j k y a i j k y 1 r y q i j 1 k y a i j 1 k y r z q i j k z a i j k z 1 r z q i j k 1 z a i j k 1 z a second consideration is that layers in modflow may be of uneven thicknesses even though cells are always rectangular prisms aligned with the global coordinate axes furthermore modflow does not compute flows from a given cell to another cell that differs in more than one of row column and layer even if they share a boundary see fig 3 consequently we must interpret the upper and lower layer boundaries as interpolating continuously between adjacent cells where layers rise or fall the necessity of this is best seen by considering a steeply inclined system modeled as a single layer because there is no flux through system outer boundaries in modflow flux interpolation would yield velocities that everywhere have no vertical component this would cause particles to travel out of bounds as they would never deviate from their initial injection height thus it is necessary to both model layer boundaries as continuous and to impose an additional vertical velocity to maintain relative position in the notional layer due to flux interpolated advection in the x and y directions obviously we do not want to neutralize flux interpolated injection in the z direction because vertical advection must be allowed when layer interfaces are no longer planar but vary with x and y we must slightly redefine some of our concepts let z k x y represent the upper bound surface of layer k 1 2 with z 0 x y representing the bottom surface of the model so a particle will be in layer k if and only if z k 1 x y x z k x y normalized cell position ratio in the z direction also becomes a function of x and y r z z z k 1 x y z k z k 1 x y it can be shown and should be intuitive that to maintain the same r z as x changes the following equation must hold 5 z x r z z k x 1 r z z k 1 x and similarly for changes in y we then may define an additional velocity v layer acting in the z direction only which serves to adjust the particle position for layer variability 6 v layer x v flux x r z z k x 1 r z z k 1 x r z z k y 1 r z z k 1 y 0 k where k is the z direction unit vector we finally define total velocity as v x v flux x v layer x for use in equation 1 we note that when layer interfaces are horizontal there is no difference between v and v flux 3 interoperation with modflow aurora partners with modflow 2005 by employing its discretization files dis and cell by cell budget files cbc to define the model domain and flow field it works with the same coordinate field employed by modelmuse in the event that software is used the cbc files must be first be processed by usgs gw chart winston 2000 into separate interfacial flux files the full workflow showing tight interface with usgs tools is shown in fig 4 aurora itself is a compiled standalone executable whose output is one or more human readable text files that list the locations of all the particles at given times or the times of arrival of particles at given locations to make sense of these results it ships with an assortment of python scripts collectively known as the aurora visualizer that plot these results to generate plumes particle swarms and breakthrough curves some visualization possibilities include isometric 3d plots of the particle locations at fixed times overlaid on the modflow grid fixed time 2d concentration plumes orthogonal to any of the coordinate axes produced by depth integration and kernel density estimation movies of plume evolution and breakthrough curves generated by kernel density estimation the visualizer scripts were used to generate a number of the figures in this paper a complete description of aurora s capabilities can be found in its included user s guide but here we note that aurora supports general modflow 2005 models with arbitrary row column and layer counts and thicknesses variable thickness layers inactive cells arbitrary boundary conditions and transient models with multiple stress periods and time steps per period releases of particles may occur from a variety of source geometries throughout the modeled aquifer may be flux or uniformly weighted and may be instantaneous or continuous the output of aurora consists of listings of particle locations recorded at user specified times and listings of times of particle arrivals recorded at user specified planes or cylinders by use of included visualization scripts these can be visualized as plumes and breakthrough curves and plume evolution movies can be made all of the additional physics defining the aurora particle tracking model are defined in a single marshal input file located in the directory containing the dis and other needed files and in which output files will be saved 4 specification of marshal file 4 1 overview as mentioned all that is necessary to define an aurora model is to correctly specify the contents of the marshal file is divided into blocks that begin with a capitalized keyword main domain layer breakthroughs profiles or source and end with the keyword end the marshal file must begin with exactly one main block describing the non physics details of the simulation the additional physics defining the parameters and pdf s mentioned in section 2 i e f r λ and g t must also be specified for each layer this may be done for all layers at once using a single domain block and or for individual layers with individual layer blocks after these initial blocks any number including zero of each of the other types of blocks may be listed in any order definitions of the contents of each of the several types of block are defined below the sample marshal files shown in tables 1 and 2 illustrate the structure and format of the various blocks 4 2 main block the main block defines global information about the simulation the name of the discretization file followed by the names of the respective right front and bottom flux files generated by gw chart from the modflow cbc file comprise the first four next the step length d is defined and finally the maximum time of the pt simulation is given 4 3 domain and layer blocks all the information about the additional small scale physics must be defined for each layer of the model this can be done for all layers at once using a domain block or for a specific layer using layer block where represents the index of the layer being defined with layer 0 being the bottom layer of the model an unlimited number of these blocks can be used with blocks later in the marshal file redefining parameters defined in earlier blocks where these conflict the syntax inside either type of block is the same the first two lines are respectively the porosity θ and the local scale transverse dispersivity α t next comes an advective heterogeneity sub block which contains the parameters defining f r which may be of five types none indicating pure tracing the large scale streamlines 7 f r δ r 1 ade indicating advective dispersive fickian small scale behavior defined by a single user defined parameter α l representing longitudinal dispersivity 8 f r 1 r 4 π a r exp r 1 2 4 a r where a α l d lognormal indicating mildly heavy tailed breakthrough where the pdf for r is defined by a single log variance σ 2 9 f r 1 r σ 2 π exp ln r σ 2 2 2 2 σ 2 the log normal distribution is a two parameter distribution but the requirement that mean flow velocity is unchanged by the small scale heterogeneity imposes the condition e r 1 and constrains the log mean pareto indicating a heavy tailed advective breakthrough with a pure power law tail that is defined by a single parameter β 10 f r β 1 β β 1 β r β 1 as elsewhere the constraint e r 1 removes one degree of freedom from the standard pareto distribution tpl standing for truncated or tempered power law where 11 f r r r 1 1 β e r r 2 unlike the other distributions aurora does not normalize this distribution to enforce the constraint e r 1 the user may choose any two of the parameters r 1 r 2 and β freely and must select the other so that the constraint is enforced finally comes the mimt sub block which defines λ and g t three options are available none indicating no immobilization at all so neither λ nor g t need to be specified exponential indicating an immobilization time pdf 12 g t μ e μ t where μ is an arbitrary user defined rate constant tpl indicating truncated power law behavior with a slightly different definition from that used for small scale advective heterogeneity 13 g t 1 t t 1 1 β e t t 2 the tpl definition here is based on the shifted pareto distribution and so has support on the whole half line 0 and any positive values of the exponent β and truncation time t 2 are permitted 4 4 source blocks any number of source blocks may be employed each defining a source of particles on the first line the total number of particles to be released from the source is specified next is a sub block representing the release regime which may be instant i e all particles released at once or continuous particles released at uniformly distributed random times in a specified interval on the next line a selection is made as to whether particles should be randomly spatially distributed throughout the source region uniformly weighted or if their spatial distribution within the region should be proportional to the groundwater speed flux weighted finally there is a sub block defining the source region itself options are box cylinder sphere and tube a 2d manifold representing only the curved surface of a finite height cylinder 4 5 breakthrough blocks any number of breakthroughs blocks may be employed although there is generally no reason for more than one to be used there may be any number of sub blocks in a breakthroughs block each defining the geometry of a 2d surface at which particle crossing times are to be recorded as well as a crossing direction required to trigger a breakthrough in out or either all surfaces are defined by some equation h x c where c is a scalar constant and an in transition is defined as one where h x n 1 c h x n where x n represents the particle s position after the n th step an out transition is defined conversely two types of breakthrough surface may be defined a tube or a plane of infinite extent 4 6 profile blocks any number of profiles blocks may be employed although there is generally no reason for more than one to be used there may be any number of lines in a given profile block each containing a time at which locations of all active particles those which have not previously reached a stagnation point should be recorded 5 demonstrations 5 1 rocky mountain arsenal aquifer as a demonstration of aurora s capabilities we apply it to modeling transport in the approximately 20 km 2 of unconfined aquifer shown in the rocky mountain arsenal tutorial that ships with the usgs modelmuse software this model exhibits a number of features including strongly nonuniform layer boundary elevations regions of inactive cells low conductivity inclusions and irregular segmented specified head boundaries the shore of a lake and the course of a river whose elevation changes significantly as it passes through the domain head broadly decreases from a high elevation lake in the north to the lower elevation river in the south however there is also a significant source of recharge at a pond in the center of the domain which represents a local maximum of hydraulic head a modelmuse screenshot showing the model domain is presented in fig 5 aurora was employed to model the fate of a one time accidental release of solute into the pond and to track the transport of the particles as they travel down gradient to the river modpath was also employed on the modflow results to generate path lines for comparison with the evolution of the plume in aurora for comparison two advective heterogeneity models were employed ade and pareto with all other parameters held constant the modpath computed path lines are shown superimposed in fig 5 and the aurora marshal file is shown in table 1 no mimt was incorporated examination of the plume snapshots in fig 6 shows broad agreement with the modpath path lines in both cases and a strikingly distinct plume tailing caused by the power law physics 5 2 push pull tracer test in layered aquifer we consider a push pull single well injection extraction test at a monitoring well penetrating a confined aquifer featuring three layers of differing properties one of whose hydraulic conductivities is two orders of magnitude higher than the others this scenario illustrates a number of features that complement the first demonstration mimt with exponentially distributed immobilization times multiple stress periods and time steps including full flow reversal independent definition of layer properties flux weighted injection on a tubular surface and uni directional breakthrough recording at that same tubular surface the marshal file for this simulation is presented in table 2 the swarm of injected particles at the end of the push phase is shown in fig 7 with the much larger spread of particles in the high conductivity layer notable the breakthrough curve at the well is shown in fig 8 as discussed in hansen et al 2016 the heterogeneity of the flow regime should not be visible in the breakthrough in the absence of any mimt there would be no scattering under the modeled conditions with all particles re converging at the well at twice the duration of the push phase this was confirmed by another simulation with mimt off not shown and in the shown breakthrough there is a smooth tail whose shape is determined by the immobilization parameter μ also note the presence of a secondary spike at twice the push phase duration reflecting those particles that were never immobilized it is a convenient feature of particle tracking simulation that these types of features not accessible to traditional push pull interpretation analytical solutions that model mimt with a retardation factor but generally present at early time hansen 2015 are captured automatically 6 summary we presented aurora a software that performs particle tracking on velocity fields it computes from modflow cell water budgets aurora enables a hybrid approach to modeling solute transport on velocity fields that are explicitly characterized at a coarse scale but which contain small scale heterogeneity that may lead to non fickian behavior the approach which we dubbed ctrw on a streamline hansen and berkowitz 2020 is a particle tracking technique that represents an advance of the current state of the art in which non fickian transport models or explicitly delineated heterogeneous velocity fields are employed but generally not both our approach separates the parameters characterizing macroscopic flow small scale advective heterogeneity and mimt each can be specified a priori from available data we summarized the underlying theory described how aurora works and offered two demonstrations showcasing different capabilities declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement skh holds the helen unger career development chair in desert hydrogeology bb holds the sam zuckerberg professorial chair in hydrology 
25956,sensitivity analysis sa has been used to evaluate the behavior and quality of environmental models by estimating the contributions of potential uncertainty sources to quantities of interest qoi in the model output although there is an increasing literature on applying sa in environmental modeling a pragmatic and specific framework for spatially distributed environmental models sd ems is lacking and remains a challenge this article reviews the sa literature for the purposes of providing a step by step pragmatic framework to guide sa with an emphasis on addressing potential uncertainty sources related to spatial datasets and the consequent impact on model predictive uncertainty in sd ems the framework includes identifying potential uncertainty sources selecting appropriate sa methods and qoi in prediction according to sa purposes and sd em properties propagating perturbations of the selected potential uncertainty sources by considering the spatial structure and verifying the sa measures based on post processing the proposed framework was applied to a swat soil and water assessment tool application to demonstrate the sensitivities of the selected qoi to spatial inputs including both raster and vector datasets for example dem and meteorological information and swat sub model parameters the framework should benefit sa users not only in environmental modeling areas but in other modeling domains such as those embraced by geographical information system communities keywords sensitivity analysis spatially distributed environmental model uncertainty swat environmental modeling 1 introduction sensitivity analysis sa and uncertainty analysis ua are important tools for investigating model behavior testing model hypotheses and exploring the potential for simplifying models wagener and pianosi 2019 uncertainty is intrinsic to all modeling work that involves representing natural processes and or human behavior sources of uncertainty that need to be considered in such exercises are model input datasets model structure and model parameters sa studies the influence of input factors e g parameters forcing initial value of model states model resolution and model structure such as different parameterization schemes of a model or submodel on model outputs it is considered a key practice in the assessment of environmental models chen et al 2019 gan et al 2014 jakeman et al 2006 matott et al 2009 oakley and o hagan 2004 pianosi et al 2016 yue et al 2020 in comparison ua quantifies the uncertainty of model outputs from input datasets and model parameters typically characterized by empirical probability distributions and or confidence bounds for the model parameters and outputs ua can be considered an extension of sa with the uncertainty distributions for the input factors being used as the perturbations therefore sa can be used to indicate when uncertainty in input factors matters in terms of the impact on the uncertainty in the outputs care must however be taken in the interpretation of sa results as sensitivities can be dependent on parameter ranges selected model structure assumed length of data period examined and its climatic forcing shin et al 2013 the use of sa and ua in environmental modeling has become of particular importance due to the highly complex nature of environmental systems and the attendant complexity of models typically invoked to represent them this is especially the case for spatially distributed environmental models referred to from hereon as sd ems where there tends to be a considerable number of model parameters due to their spatially variant nature and substantial uncertainty in the model and its predictions uncertainty and sensitivity related studies in environmental modeling are rising in popularity because of the growing awareness of the importance of models in supporting informed decision making douglas smith et al 2020 coupled with the fact that current process based environmental models are typically and perhaps necessarily deterministic in their representation farmer and vogel 2016 uusitalo et al 2015 this paper focuses on monte carlo simulation based sa of sd ems which is a valuable tool per se and one that can also inform uncertainty analyses monte carlo simulation based approaches are widely applied due to their ease of implementation yet there is a lack of a comprehensive pragmatic framework for conducting such approaches for sd ems yang et al 2018 an sd em is intrinsically tied to the spatial dimensions of producing and utilizing data that represent the spatially distributed nature of the modeling context grid based digital elevation models dems site specific point measurements and remotely sensed images are examples of such data however sas are rarely conducted for dem and dem derived parameters even though the inherent scale and errors of a spatial dataset and or of the whole environmental model can have a significant impact on model outputs tran et al 2018 a crucial issue to take into account regarding spatial datasets is the spatial structure of their uncertainty generally spatial datasets are characterized by spatial dependence i e spatial coherence and their uncertainties are also spatially autocorrelated oksanen and sarjakoski 2005a wechsler 2007 thus ignoring such characteristics can lead to erroneous estimation of sensitivity measurements moreover because spatial datasets often determine the uncertainty in model resolution and structures through their boundaries discretization and scale exploring uncertainty related to spatial datasets can partly account for model uncertainty in sd ems this article introduces a pragmatic framework for the application of sa to an sd em using a scenario simulation based approach to investigate the significance of potential uncertainties in the model inputs which can not only explore model and data assumptions transparently but also be an informative precursor to a more thorough ua the objectives of the framework are to provide sufficient information and background in order to guide the selection of more appropriate choices at each step of the sa process potential uncertainty source identification selection of sa method s and quantities of interest qoi perturbation propagation and sa evaluation and post processing the framework emphasizes the following aspects it attempts to address potential uncertainty sources related to spatial datasets and assists in propagating the potential uncertainty sources by considering their likely spatial structure therefore the framework helps to explore the impact of potential uncertainty of spatial datasets in an sd em and to compare their relative impacts with the usual factors in sa e g model parameters the framework is intended to benefit both non experts and sa users in environmental modeling and geographical information system gis communities the remainder of this article is organized as follows section 2 broadly introduces the pragmatic framework for applying sa to sd ems covering potential uncertainty source identification selection of sa method and qoi perturbation propagation and sa evaluation and post processing then from sections 3 to 6 the detailed steps and their corresponding considerations are discussed section 7 provides a concise example of the sa framework the article concludes in section 8 with a discussion of future needs and opportunities 2 a pragmatic sa framework for sd ems the presented framework prescribes sequential steps in which important considerations are highlighted to guide modelers towards the selection of appropriate choices for the pragmatic application of sa to uncertainty exploration in sd ems the overarching steps and the corresponding considerations are depicted in fig 1 the main purpose of the framework is to identify the contributions of potential uncertainty sources to the selected qoi this section introduces the pragmatic framework to provide a broad guideline for sa users while the following sections detail the considerations within each step the first step is to identify potential sources of uncertainty section 3 numerous studies have investigated uncertainty sources in the context of environmental modeling and classified them in their own schemes matott et al 2009 refsgaard et al 2007 understanding these general classification schemes and the uncertainty sources involved assists in identifying the sources of uncertainty related to a specific application in particular this article discusses potential uncertainty sources not only in model parameters and model uncertainty but also in spatial datasets which are used as direct input s and or to derive parameters to describe the underlying spatially distributed structure of sd ems e g dem the second step is the selection of sa methods and qoi section 4 this selection primarily depends on the purposes of sa e g screening and ranking and the characteristics of the sd em the characteristics can include the model complexity and or computational cost as this framework is intended for monte carlo simulation based sa applying sa methods that require a large number of model evaluations to determine sa measures might not be feasible for a computationally expensive model this article broadly categorizes the most frequently used sa methods for environmental modeling based on their purposes and characteristics and synthesizes them to assist sa users and communities in selecting appropriate ones for a pragmatic sa application here we provide a general description and several previous studies provide complementary explanations for further sa methods pianosi et al 2016 borgonovo and plischke 2016 sarrazin et al 2016 for selecting qoi only scalar outputs are generally utilized as qoi in sa of environmental models which often requires aggregating spatially and or temporally distributed outputs into a scalar function pianosi et al 2016 however since potential uncertainty sources in sd ems have different impacts on spatially and also temporally distributed scalar outputs pappenberger et al 2008 wang et al 2013 preserving a spatial distribution of scalar outputs might be useful to understand the underlying spatial processes within sd ems even if that requires managing vast computing power the third step in the framework is the perturbation propagation of the identified potential uncertainty sources section 5 generally local and global sa methods require different types of perturbation propagation methods thus local sa utilizes the neighborhood values of the nominal value and global sa generally requires the input variability space via a probability distribution borgonovo and plischke 2016 for all sa methods perturbation propagation with appropriate distributions is crucial because the representativeness of uncertainty sources primarily determines the sa results different types of uncertainties also need different types of perturbation propagations for example the propagation of perturbations in a model parameter that is represented as a scalar random variable could be performed using a probability distribution e g normal distribution while more complex perturbation propagation methods are necessary for characterizing the uncertainty of input datasets e g spatial datasets by simultaneously taking account of their characteristics e g spatial autocorrelation wechsler 2007 crosetto and tarantola 2001 nonetheless perturbation propagation of model uncertainty is still an ongoing research subject and remains a fruitful area of investigation matott et al 2009 uusitalo et al 2015 o hagan 2012 the final step of the framework is evaluating the results of the sa which includes post processing of analysis results section 6 in evaluating sa results reliability and convergence aspects should be assessed as a verification step sa measures or metrics vary with sample size thereby requiring a convergence test to check if the metric s of choice is converging and its confidence bounds acceptable for the purpose yang 2011 in addition different sa methods are based on different premises may produce different metrics and hence produce different outputs thus sa with different methods can increase confidence in the reliability or interpretation of sa outputs finally this step also includes credibility assessment for sa outputs if unexpected sa outputs are obtained these outputs can lead to new indications of uncertainty in model behavior pappenberger et al 2008 or indicate issues with the model implementation pianosi et al 2016 otherwise these outputs can assist with revising the sa steps such as identifying missing uncertainty sources and redefining the perturbation propagation approach because sa results are generally associated with large sets of potential uncertainty sources visualization methods of sa results are useful to identify critical uncertainty sources and to compare their importance thus this step includes the descriptions of specific visualization methods with their corresponding sa methods conventional scientific visualization techniques for sa outputs kelleher and wagener 2011 and geographical visualization and analysis for the representation of spatially distributed sa measures feick and hall 2004 3 identification of potential uncertainty sources 3 1 classification of uncertainty sources this initial step involves the identification of potential uncertainty sources associated with the model s input factors that influence the selected outputs of an environmental model or functions of those outputs i e qoi various types of uncertainty sources could influence the outputs of the models and numerous classification schemes for uncertainty sources have been introduced to categorize them matott et al 2009 refsgaard et al 2007 beck 1987 linkov and burmistrov 2003 because the classification schemes commonly share two fundamental uncertainty sources i e input and model uncertainties this article discusses the various uncertainty sources in these two broad categories 3 2 input uncertainty input uncertainty is associated with model parameters and input datasets among various sources of uncertainty model parameter uncertainty is the most commonly considered source setegn et al 2010 wu and chen 2015 wu and liu 2012 berzaghi et al 2019 porada et al 2018 and can be controlled to some extent through calibration processes zhao et al 2018 therefore uncertainty in model parameters is often considered to be reducible and it has been argued that model parameters can be carefully tailored to reduce that uncertainty related to model outputs and to improve model performance matott et al 2009 however if globally optimized parameters are obtained through a calibration process they would also be affected by other sources of uncertainty including input data uncertainty model uncertainty and also model parameter uncertainty and these might lead to equifinality zhao et al 2018 beven and freer 2001 input datasets are usually assumed to be accurate that is effectively without uncertainty however this is incorrect as all data have inherent uncertainties chrisman 1991 uncertainties related to input datasets can be irreducible matott et al 2009 and thus they are often ignored in uncertainty related studies moreover uncertainty in spatial datasets involves spatial autocorrelation griffith 2008 koo et al 2018c in the sd em context spatial datasets include maps and site specific measurements for example temperature and precipitation surfaces would have strong positive spatial autocorrelation and soil and land use land cover lulc datasets possess complex spatial autocorrelation legendre 1993 site specific measurements e g meteorological data have spatial autocorrelation as well as temporal autocorrelation therefore if an environmental modeling analysis does not address the independent inputs and relationships among them an incomplete understanding of the uncertainty in the model will result leading to a biased estimation of the confidence in the model outputs the uncertainty in spatial datasets is generally caused by five fundamental components lineage positional accuracy attribute accuracy logical consistency and completeness ansi 1998 koo et al 2020 briefly lineage relates to the description of spatial data sources e g dates and reference systems and logical consistency describes the fidelity of spatial data structure e g topology completeness refers to selection criteria of spatial entities for example geometric thresholds such as minimum width and area of spatial features positional and attribute accuracies literally refer to uncertainties respectively in position i e location and attribute information of spatial datasets among these components positional and attribute accuracies are closely related to uncertainties in sd ems and they show different aspects depending on the type of spatial datasets spatial datasets can be broadly divided into raster and vector datasets a typical example of a raster dataset is a dem where scale i e resolution random and systematic measurement uncertainties resulting from attribute accuracy are the major uncertainty sources hengl et al 2010 the scale issue is relatively well discussed as a source of uncertainty in dems chaubey et al 2005 dixon and earls 2009 lin et al 2013c shen et al 2013 together with scale measurement uncertainty is known to also have an impact on watershed delineation oksanen and sarjakoski 2005a wu et al 2008 stream network extraction hengl et al 2010 and derivation of other topographic parameters wechsler 2007 as systematic measurement uncertainty generally shows a fixed pattern stemming from dem generation processes e g blunders if the cause of the uncertainty is known it could be reduced wechsler 2007 however random measurement uncertainty still remains after reducing systematic uncertainty for example the shuttle radar topography mission srtm v 4 1 dataset has a vertical accuracy of 16 m at a 95 confidence level mukul et al 2017 other widely used raster datasets are lulc and soil datasets which possess uncertainty related to positional uncertainty and scale issues koo et al 2020 vector datasets are typically used to define the boundary of a study area and describe topographic and or environmental features such as stream networks in addition site specific measurements are handled as a type of vector dataset generally point features that have attributes on specific locations for example measurements of precipitation temperature wind speed humidity and solar radiance even though some raster datasets e g precipitation and temperature surfaces are converted from site specific measurements their uncertainty sources can mainly be explained by uncertainty in vector datasets vector datasets typically include two main uncertainty sources which are positional and attribute uncertainties koo et al 2018a positional uncertainty refers to the uncertainty of geographical features in vector datasets which often results from a global positioning system gps geocoding and digitizing errors attribute uncertainty describing non spatial properties of geographical features in vector datasets are generally estimated from their sampling processes aouissi et al 2013 strauch et al 2012 tasdighi et al 2018 bárdossy and das 2008 chaplot et al 2005 cho et al 2009 gong et al 2012 masih et al 2011 and measurements shen et al 2015 li 2014 in addition attribute uncertainty often includes spatial autocorrelation when attribute uncertainty contains temporally varying quantities they also need to consider the information lost in converting to discrete time littlewood and croke 2013 3 3 model uncertainty model uncertainty results from the inability of a model to mimic the real world yen et al 2014 model uncertainty might be subdivided into the effects of model structure model resolution and model integration uncertainties matott et al 2009 voinov and shugart 2013 first of all model structure uncertainty is caused by a model structure that imperfectly represents underlying environmental processes in a model yen et al 2014 numerous alternative model structures e g scientific hypotheses and equations might be proffered in a model which could adversely impact model outputs a related consideration is the issue of the identifiability of the model structure guillaume et al 2019 shin et al 2015 which largely means the data available are insufficiently informative to identify unique values of some of its parameters sa methods are often used to determine the insensitive non identifiable parameters so that focus for calibration and or uncertainty analysis can then be turned to the most sensitive ones second model resolution uncertainty is due to uncertainties in the spatio temporal discretization boundary specification and scale dependence of a model matott et al 2009 in an sd em spatial discretization boundary and scale are often determined by the available spatial datasets and model resolution uncertainty can then be partially explained through exploring uncertainty of the spatial datasets koo et al 2020 trusel et al 2015 fig 1 another aspect of model uncertainty arises from model integration processes chen et al 2020 currently environmental models become more complex by integrating multiple models lin et al 2013a 2013b lu et al 2019 the integration processes yield uncertainty from skewed space e g difference in spatial resolution mismatched measurement scales and confusion of linguistic representations voinov and shugart 2013 particularly in an sd em when sub models with different spatial and temporal scales are integrated without a solid design the uncertainty of an integrated model could become large and undetectable tscheikner gratl et al 2019 4 selection of sa method s and quantities of interest this step firstly provides guidance on sa method selection based on two main criteria the purposes of the sa and the characteristics of the sd em this guidance includes only two fundamental sa purposes i e ranking and screening but sa can have additional purposes such as factor mapping that provides further descriptions for the input space related to qoi saltelli et al 2008 sa methods for ranking generate the order of input factors based on their relevant influence on qoi and screening methods identify input factors with significant or negligible influence on model output pianosi et al 2016 sa methods for each purpose can be applied sequentially such that model results from screening methods are leveraged to reduce the number of input factors and are followed by sa for the purpose of ranking thus reducing the overall number of model evaluations saltelli et al 2004 sun et al 2012 secondly two major characteristics of an environmental model are necessary to consider in selecting appropriate sa methods model complexity and interdependency between input factors saltelli 2002 here we briefly discuss widely used sa methods including local sa the morris method correlation and regression and variance based sa methods according to the two major criteria fig 2 classifies these sa methods where the positions of each sa method relate to sa purpose and model complexity and their outlines represent interdependency in addition the advantages of an emulator and its consideration for dealing with spatially distributed outputs are also briefly discussed local sa is the simplest sa method and is often conducted through one at a time oat perturbation of input factors around their nominal values to determine the response of model outputs sun et al 2012 campolongo and saltelli 2000 a formal approach for local sa involves using partial derivatives helton 1993 partial derivatives can provide sa measures metrics for both ranking and screening however they should be rescaled and applied to several locations in factor space in order to reveal the global effects of input factors with different measurement units borgonovo and plischke 2016 campolongo et al 2011 purely oat analyses are however typically inappropriate for determining sensitivity estimates such analyses do not consider interactions among input factors borgonovo and plischke 2016 and while oat can investigate non linearities if input factors are independent newham et al 2003 typical applications are unable to do so due to the use of a single perturbation sun et al 2012 saltelli and annoni 2010 importantly because local sa evaluates sensitivity at a specific location of input factors rather than over their plausible ranges as global sa does it might provide a limited indication of model behavior sun et al 2012 sobol 2001 although pure oat analyses have the advantage of reduced computational time over a more substantive global analysis an initial indication of model behavior can be gained with just n 1 model evaluations for n input factors pianosi et al 2016 or fewer if groups of input factors are perturbed together through group sampling sobol 2001 improper model behavior caught at this stage indicates errors in the model implementation to be addressed before global analyses are applied a simple global extension of local sa is the morris method morris 1991 also known as the elementary effect test saltelli et al 2008 the morris method generally requires much lower numbers of model evaluations than other global sa methods for the purpose of screening and thus the morris method is appropriate for computationally complex models and or models with a large number of input factors campolongo et al 2007 herman et al 2013 a drawback of the morris method is that it gives a poor measure of the relative importance between factors and can be considered as offering qualitative sensitivity measures only brockmann and morgenroth 2007 besides the average elementary effects it does provide the standard deviations of the elementary effects which are beneficial for identifying interaction effects among input factors norton 2015 regional sensitivity analysis spear and hornberger 1980 typically divides input factors into two or more groups depending on a prescribed threshold of model output and then studies the difference in their empirical cumulative distribution functions cdf for each input factor the kolmogorov smirnov k s statistic quantifies the divergence between the cdf and serves as a common sensitivity measure thus if the k s statistic is high i e the cdf of one group differs from the other the input factor has a significant influence on model output pianosi and wagener 2015 k s statistics are mainly utilized for ranking input factors however the k s statistic is inappropriate for screening because it is only applicable to the same groups saltelli et al 2008 the advantage of regional sensitivity analysis is that it is applicable for any type of splittable model outputs e g futter et al 2007 whitehead et al 2015 whitehead and hornberger 1984 however if the grouping i e splitting criterion is not clear i e a model does not have meaningful model output values to describe model behavior regional sensitivity analysis would be inappropriate various correlation and regression methods are also extensively used to measure sensitivities these methods basically obtain sa measures based on different statistics i e correlation and regression coefficients between input factors and qoi generated from a monte carlo simulation pianosi et al 2016 helton et al 2006 specifically for correlation coefficient estimations various types of correlation coefficients are selected mainly based on the linearity between input factors and model outputs when they have a linear relationship pearson and partial correlation coefficients are appropriate methods saltelli and marivoet 1990 if the relationship is non linear spearman and partial rank correlation coefficients can be used as alternatives pastres et al 1999 furthermore if sa methods simultaneously are to take account of multiple relationships for multiple outputs a canonical correlation analysis provides an additional option minunno et al 2013 regression methods obtain sensitivity measures by estimating regression coefficients which are commonly standardized regression methods are often superior to correlation methods in deriving sensitivity measures especially when a large number of input factors are considered since regression methods can obtain sa measures of all input factors at once however while linear regression is the simplest and most widely used sa method iman and helton 1988 it is not suitable if there is a non linear or non monotonic relationship in the model response and a high level of interaction among factors also makes linear regression act poorly yang 2011 when a non linear relationship exists rank regression storlie et al 2009 and machine learning techniques such as decision trees singh et al 2014 are appropriate regression and correlation methods are commonly utilized for both screening and ranking purposes variance based methods produce sensitivities by decomposing the variance of a model output into the contributions from input factors the contributions can be defined according to different indices for example first order and total indices saltelli et al 2008 the first order index quantifies the contribution of a specific input factor to the variance of the selected qoi while the total index measures the total contribution of an input factor to the variance of the qoi including those due to its interactions with other input factors the first order index is usually used to rank input factors when interactions are not significant with total indices variance based sa methods are able to address non linearity of model responses to input factors factors with a total index close to zero can be considered negligible and screened out pianosi et al 2016 often these negligible factors are made constant a practice referred to as factor fixing variance based sa methods can be challenging for computationally intensive complex models because they take a relatively large number of factor samples and related model evaluations to obtain reasonably accurate and stable indices gan et al 2014 however several approaches such as the sobol method sobol 2001 fourier amplitude sensitivity test fast cukier et al 1973 and extended fast saltelli et al 1999 have been proposed to more efficiently estimate main and total effects paleari and confalonieri 2016 in practice however the computational requirement is still a burden for these sa methods moreover common variance based approaches operate on a number of assumptions including that the variance of model outputs resulting from the prior input distribution is indicative of input factor sensitivities inputs are independent saltelli and tarantola 2002 and the distribution of the sampled model outputs often estimated through kernel density estimation approaches are unimodal misleading sa results may be produced if model outputs do not conform to these assumptions pianosi et al 2016 for computationally intensive models sa e g variance based methods can be implemented using an emulator which is a statistical approximation of the output response surface of the original environmental model o hagan 2012 a simple approach for building an emulator is through use of gaussian processes oakley and o hagan 2004 though other options exist such as polynomial chaos expansions sudret 2008 statistical emulators young and ratto 2011 and machine learning based emulators e g random forest and gradient boosting storlie et al 2009 however an emulator might be inappropriate to evaluate a large number of input factors because it suffers from estimation inefficiency and inaccuracy due to the curse of dimensionality storlie et al 2009 li et al 2020 this can be resolved by screening out negligible input factors or by applying an emulator that includes a procedure for input factor selection yang 2011 the selection of qoi sometimes embodied in an objective function or loss function is also crucial to reflect the modeling purposes as different modeling purposes lead to different sensitivity measures in the input factors for example rainfall intensity yields more sensitivity to stream flow peak than to baseflow based on modeling purpose a large number of qoi have been used in hydrological models the most frequently used include the nash sutcliffe coefficient root mean square error rmse and differences in the flow duration curve e g between simulated and observed flows and total nitrate although sa tends to select only single scalar qoi sd ems often need to explore spatially distributed sensitivities of input factors on multiple and multi dimensional outputs gupta and razavi 2018 pappenberger et al 2008 which can involve a colossal computational burden emulators may be developed to circumvent the issue of computational cost a typical practice however is to build separate emulators for individual outputs ryan et al 2018 which may impose an additional computational cost implementing other types of sa methods may be useful to mitigate this computational burden including a separate generalized additive model mara and tarantola 2008 partial least squares sobie 2009 multi fidelity polynomial chaos expansions palar et al 2018 and global sensitivity matrix approaches razavi and gupta 2019 moreover decreasing the dimensionality of outputs by using a principal component analysis and grouping factors based on bootstrap based clustering sheikholeslami et al 2019 can provide another solution gómez dans et al 2016 5 perturbation propagation monte carlo simulation based sa needs to propagate the perturbations of input factors through the model to analyze the sensitivity of model outputs and their qoi to those input factors saltelli and tarantola 2002 proper selection of the perturbations within plausible ranges and distributional assumptions is a crucial step in sa because the perturbation attempts to reflect the degree of uncertainty in input factors this section introduces useful methods for perturbation propagation of the corresponding uncertainty sources including input parameters spatial and point datasets 5 1 model parameters model parameters in environmental models are typically represented as scalar variables and often treated as random variables with prior probability distributions borgonovo and plischke 2016 specifically samples of individual model parameters are obtained from their corresponding probability distributions and then sa evaluates model responses based on the samples the interactions between model parameters can be represented using a covariance matrix for example the cholesky decomposition xiu and karniadakis 2003 because defining appropriate probability distributions with plausible ranges are also crucial for evaluating model parameter sensitivities taking all available information on individual model parameters is necessary for the generation of those probability distributions e g expert opinion crosetto and tarantola 2001 5 2 raster datasets an sd em normally uses various types of spatial datasets including spatially distributed input datasets and site specific measurements because spatial datasets have various forms e g vector and raster datasets different types of perturbation propagation are required crosetto and tarantola 2001 furthermore the propagation for spatial datasets should consider the characteristics of those datasets especially spatial autocorrelation temme et al 2009 table 1 demonstrates applicable perturbation propagation methods for the various types of spatial datasets raster datasets are either generally subdivided into categorical e g lulc and soil datasets or quantitative e g dem temperature and precipitation surfaces rasters requiring different perturbation propagation methods heuvelink 1998 in quantitative rasters individual cell values can be treated as individual random variables with their own probability distributions which means an observed quantitative raster is just one rendering of all possible realizations however this assumption ignores the spatial structure of uncertainty in quantitative rasters random fields are widely used to represent uncertainty in quantitative rasters random fields comprise a surface of random values that estimates uncertainty magnitude variance and spatial variability where each value represents potential uncertainty at a specific location of the grid cell wechsler 2007 furthermore random fields are applicable for regularly discretized space time voxels in 3d rasters pebesma et al 2007 like other perturbation propagation methods random fields require a definition of the appropriate potential uncertainty level i e plausible ranges and their spatial structure if the information for the ranges and structures are not available random fields are often estimated using the accuracy statistics of a target raster dataset e g root mean square error if the information is obtained from a survey and other methods this can be used for estimating parameters of a random field generation the simplest method for random field generation is using a normal distribution with a mean of zero and standard deviation derived from the accuracy statistics of a quantitative raster dataset however potential uncertainty in spatial datasets has spatial structure including spatial autocorrelation wechsler 2007 oksanen and sarjakoski 2005b which should be considered in a random field generation the following methods are the typical methods accounting for spatial autocorrelation in random field generation the first method is the spatial moving average wechsler and kroll 2006 which applies a low pass filter to a random field generated from a simple probability distribution e g normal distribution without considering spatial autocorrelation the size of a low pass filter determines the level of spatial autocorrelation which often covers from 3 by 3 grids to the grid size that is computed from the range of a semi variogram in a target quantitative raster pixel swapping fisher 1991a goodchild and openshaw 1980 and a spatial autoregressive model hunter and goodchild 1996 koo et al 2019 anselin 1995 are other methods for random field generation that considers spatial autocorrelation they can be utilized for quantitative raster datasets and attribute uncertainty in vector datasets pixel swapping is developed based on the concept of simulated annealing where two cells in random fields are continuously swapped until spatial autocorrelation in the random fields achieves its threshold level derived from spatial autocorrelation level in a target raster dataset this method has the advantage that it is a simple process but it is difficult to implement for a large spatial dataset due to the slowness of the procedure oksanen and sarjakoski 2005a a spatial autoregressive model produces random fields based on the following equation y i ρ w 1 ε where i denotes an identity matrix and ε is a vector that is generated generally from a probability distribution by using an autoregressive parameter ρ this model effectively determines the level of spatial autocorrelation w is a spatial weights matrix which is also useful for specifying various types of neighborhood definitions e g contiguity and k nearest neighbors because it is an n by n matrix and tends to be sparse computing its inverse often requires a lot of resources for large datasets anselin 2005 sequential gaussian simulation is a widely applicable method for random field generation koo et al 2020 aerts et al 2003 which is developed based on a geostatistical approach with a normality assumption for potential uncertainty goovaerts 1997 the basic steps for sequential gaussian simulation are as follows first random paths are generated in a field and each node in the path is sequentially visited second at each node descriptive statistics for a local conditional probability function are estimated based on surrounding values using kriging and finally a random value is generated from the local conditional probability function if survey samples for uncertainty exist the values at the sample nodes are maintained with the original sample value which has been referred to as conditional gaussian simulation aerts et al 2003 a data transformation process will be required if a dataset does not follow a gaussian distribution because sequential gaussian simulation is only applicable to a dataset that follows a gaussian distribution deutsch and journel 1998 additionally if prior knowledge for the spatial structure of uncertainty in a quantitative raster dataset can be obtained from surveys or higher accuracy datasets the three parameter method ehlschlaeger et al 1997 is useful to reflect this prior knowledge whereas only one parameter is available in pixel swapping and a spatial autoregressive model this three parameter method is superior for generating random fields with the mean and standard deviation under gaussian distribution and spatial autocorrelation wechsler 2007 other studies hengl et al 2010 fisher 1998 holmes et al 2000 discuss further perturbation propagation for quantitative raster datasets and have implemented them in real world applications perturbation propagation for categorical raster datasets has received less attention than those for quantitative raster datasets crosetto and tarantola 2001 generally the selection of the method for categorical raster datasets relies on the method of dataset generation when a categorical raster dataset e g for lulc and soil data is generated using a fuzzy classification method of satellite images individual pixels include uncertainty information of the classification result such as probability or membership vectors lucieer and kraak 2004 this type of uncertainty information can directly be applied to monte carlo simulation based sa when a conventional classification method is used for raster data generation the confusion matrix i e error matrix is available for sa which is a cross tabulation of the classified raster against reference samples to estimate classification accuracy the confusion matrix based perturbation propagation is fully discussed in heuvelink 1998 fisher 1991b however the confusion matrix might be limited in representing the spatial structure of uncertainty comber et al 2012 in addition perturbation propagation for vector datasets can also be applied for categorical raster datasets mainly to represent their positional uncertainty kiiveri 1997 shi 1998 the details of perturbation propagation will be discussed with that for vector datasets in section 5 3 5 3 vector datasets vector datasets including the case of site specific measurements mainly have two major potential uncertainty sources attribute and positional uncertainties koo et al 2018a ansi 1998 attribute uncertainty results from sampling and measurement errors thus perturbation propagation can be simply a probability distribution function derived from the descriptive statistics of potential attribute uncertainty if the attributes of geographical features in vector datasets are spatially autocorrelated pixel swapping goodchild and openshaw 1980 and a spatial autoregressive model anselin 1995 are useful to describe the spatial structure of attribute uncertainty for example with a spatial autoregressive model a random vector i e ε is generated from the descriptive statistics of attribute uncertainty and spatial structures are described by a spatial weights matrix i e w of geographical features and predefined spatial autocorrelation level i e ρ as we described in the previous section 5 2 propagating perturbations for positional uncertainty in vector datasets has been one of the major focuses in spatial data quality research groups devillers et al 2010 the geometric features of vector datasets are generally classified as point line and polygon however in the context of modeling positional uncertainty a polygon is considered as a closed line shi 1998 according to the latter perturbation propagation is discussed in two general feature types point and line error ellipses constitute a common method for propagating perturbations of point features dutton 1992 stanislawski et al 1996 goodchild 1991 where x y coordinates follow a two dimensional extension of a probability distribution function on individual points for positional uncertainty representation a normal distribution can be simply used for the probability distribution goodchild 1991 wolf and ghilani 1997 but a log normal distribution a mixture of bivariate t distributions zimmerman et al 2007 and a chi square distribution griffith et al 2007 have also been suggested karimi et al 2004 koo et al 2018c positional uncertainties of point features are usually considered independent though possibly spatially autocorrelated if independent error ellipses can be directly applicable to their selected probability distributions however if positional uncertainties of individual point features are dependent and spatially autocorrelated additional stochastic techniques e g a spatial autoregressive model are required to represent their dependence structure in particular the source of point features e g gps geocoding and lidar is the important criterion to select a proper probability distribution and independence between positional uncertainty zandbergen 2008 for example positional inaccuracy and its empirical distribution of gps have been regularly reported by the federal aviation administration and its accuracy is generally reliable and independent regardless of study areas however the positional uncertainty of geocoded points varies and is dependent on the locations of geocoded points koo et al 2018c perturbation propagation for positional uncertainty of line features is more complex than that of point features because the former consists of a set of the propagations for individual point features shi 1998 shi et al 2014 similar to error ellipses for point features the epsilon band is a typical method to represent positional uncertainty of line features crosetto and tarantola 2001 shi 1998 which defines an error band of a boundary using a constant distance for both sides of lines however the epsilon band does not represent spatial structures of positional uncertainty between individual point features on a line feature specifically point features that are located relatively midway on a line segment have generally smaller positional uncertainty than that of endpoints shi 1998 and positional uncertainty of individual point features are also often spatially autocorrelated tong et al 2013 thus under the assumption that the positional uncertainties at both endpoints are independent and the amount of positional uncertainty varies by their location perturbation propagation is offered shi 1998 furthermore a stochastic process based model has been proposed for considering spatial autocorrelation between points shi and liu 2000 recently entropy based models gong and li 2011 and the statistical simulation error model tong et al 2013 have offered alternatives for perturbation propagation for positional uncertainty of line features 5 4 model uncertainty quantification of model uncertainty is challenging and is a subject of ongoing research o hagan 2012 additionally quantitative methods alone cannot address all aspects of model uncertainty as there are qualitative sources that cannot be quantified and arise from the subjective judgment and biases of the modelers and stakeholders chen et al 2007 thus model uncertainty can also be explained based on qualitative e g expert assessment rather than quantitative evaluation uusitalo et al 2015 that said a qualitative evaluation may be sufficient for the model purpose when its evaluators are well informed for instance several solver types are compared by adjusting convergence criteria to evaluate the impact of model solution precision ahlfeld and hoque 2008 similarly different model formulations resolutions and solvers are explored for model uncertainty investigation farthing et al 2012 evaluating the uncertainty caused by model integration is also difficult using quantitative methods due to a lack of proper perturbation propagation voinov and shugart 2013 tscheikner gratl et al 2019 voinov and cerco 2010 the resolution of spatial datasets has been used as one source to explore model uncertainty trusel et al 2015 an sd em generally uses predefined sources of spatial datasets for example srtm for dem and the impact of spatial dataset resolution is often investigated by comparing several spatial datasets with different resolutions thus the resolutions of dem lulc and soil datasets can show a significant impact on sd em outputs chaubey et al 2005 dixon and earls 2009 lin et al 2013c kumar and merwade 2009 generally finer resolutions of spatial datasets lead to more accurate model outputs shen et al 2013 but longer model evaluation time and furthermore the uncertainties arising from the resolutions of spatial datasets can be compensated for when different types of spatial datasets are utilized shen et al 2015 however monte carlo simulation based sa requires propagating perturbations for individual input factors with their plausible ranges and sometimes probability distribution assumptions adding a systematic or random model error at model runtime is suggested to assess perturbation propagation marin et al 2003 however the method could not differentiate the sources of uncertainties because the added errors include both input and model uncertainties adding systematic errors directly in the model structure might be an alternative to adding errors into model runtime for example general perturbation propagation is provided for state parameter estimation based on recursive and batch estimation beck 1987 another frequently used strategy for model uncertainty exploration is adjusting parameters that relate to model structures koo et al 2020 yen et al 2014 wagener et al 2003 however this strategy is not applicable when the model parameters are discrete and have limited options recently koo et al 2020 explored model structure uncertainty using sa by adjusting the level of spatial discretization which could provide another solution for taking model uncertainty into account in sa 6 sa evaluation and post processing 6 1 assessing convergence and credibility the convergence of sa measures needs to be assessed because sa measures sometimes are not constant and vary with sample sizes especially when they are obtained from smaller sample sizes than required sizes suggested in the literature sarrazin et al 2016 vanrolleghem et al 2015 two methods are generally utilized to evaluate the convergence of sa measures which are based on the central limit theorem clt and the bootstrapping technique yang 2011 according to the clt the sample mean of a distribution with mean μ and standard deviation σ approaches a normal distribution with mean μ and standard deviation σ n with increasing sample size n the clt based method calculates sa measures r times using different sets of sub samples and compares its mean and standard deviation using gradually increasing sizes of sub samples the convergence of an sa measure could be regarded as achieved when the coefficient of variation σ μ does not show a significant change bootstrapping uses sub samples from the original samples and then compares sa measures derived from the sub samples to the original sa measures the advantage of the bootstrapping technique is that there is no requirement for additional simulation but the convergence rate could be overestimated i e underestimation of uncertainty when the sub samples are strongly dependent on the original samples in addition a convergence test could be performed by analyzing sa measures obtained from different numbers of monte carlo simulations vanrolleghem et al 2015 the number of required samples for a convergence test differs according to the purpose type s of sa applied and the characteristics of the environmental model sarrazin et al 2016 sa for screening purposes generally requires a smaller sample size for a convergence test than that for ranking similarly the required number of samples for convergence is typically the largest in variance based sa and significantly smaller for the morris method and local sa methods campolongo et al 2007 in variance based sa methods sa measures for the main effect converge faster than those for the total effect sarrazin et al 2016 nossent et al 2011 the characteristics of the environmental models and its related study area processes and data also influence convergence rate so that there is no clear relationship between the number of input factors and the required sample size sarrazin et al 2016 finally the reliability and credibility of sa measures should be assessed the reliability and credibility are obtained by verifying that the underlying assumptions and conditions of the sa are satisfied in a target environmental model pianosi et al 2016 for example linear regression assumes a linear relationship in model response which might well be inappropriate for the model with a non linear response additionally the sa results obtained could be biased due to use of implausible perturbation propagation and missing input factors another check is conducting the sa with different sa methods the reliability of sa results could be regarded as being enhanced if there is a consensus among different methods if the sa results are contradictory it encourages further investigations to discover various aspects of model behaviors that are captured from different sa methods pappenberger et al 2008 paleari and confalonieri 2016 6 2 visualizing sa measures effective visualization methods help to increase understanding and interpretation of sa measures and their relationships to input factors being especially valuable when sa measures are associated with a large number of input factors specifically visualizing sa results helps in achieving the general purposes of sa by finding and ordering the critical input factors visualization also supports the discovery of counterintuitive sa results which could lead to unearthing new aspects of model behavior or revising our sa processes for example adding missing uncertainty sources and re propagating perturbation methods when revising sa processes the visualization helps to compare sa results under various conditions such as different ranges and theoretical assumptions of perturbation propagation furthermore in sd ems spatial and temporal patterns in sa measures could be revealed through visualization methods pianosi et al 2016 simply the relationship between input factors and their corresponding outputs are typically visualized using scatter plots colored scatter plots and parallel coordinate plots specifically scatter plots demonstrate the relationship between model output with one input fig 3 a while colored scatter plots typically show the relationship of model output with two input factors fig 3 b thus scatter plots and colored scatter plots are useful for screening and ranking input factors parallel coordinate plots show distributions of input factors and outputs fig 3 c which can highlight patterns using colors and or dynamic linking and brushing ge et al 2009 koo et al 2018b symanzik et al 2000 violin plots are also useful to visualize the distributions of input factors and outputs hintze and nelson 1998 whose relationship could be emphasized using dynamic linking and brushings further examples of general scientific visualization methods for sa are suggested in pianosi et al 2016 and kelleher and wagener 2011 some sa methods are effectively represented using their own specific visualization methods for instance morris method results are represented using scatter plots of the absolute means of elementary effects against their standard deviations for individual input factors fig 3 d where both the relative importance of each input factor and their interactions are highlighted local sa and also correlation and regression based sa methods utilize general scatter plots of input factors against outputs additionally a regression coefficient plot can be applicable for regression based sa results fig 3 e a regression coefficient plot is a scatter plot of an estimated coefficient with lines indicating standard errors which effectively show the relative importance of individual input factors and compare changes of regression coefficients in sa results under different conditions visualization of variance based sa methods is sometimes difficult because it requires a simultaneous representation of multiple sa measures for main and total effects a simple and general method for such visualization is using a stacked bar plot for individual input factors with main and total effects fig 3 f recently circos kelleher et al 2013 and radial convergence diagrams butler et al 2014 were developed to effectively visualize the main and total effects of multiple sa measures in an sd em because all the uncertainty sources could make different contributions to the spatially varying outputs geographical visualization of spatially variable sa measures and their spatial analysis can enhance understanding of spatial aspects in sa measures chen et al 2010 when sa measures are represented in a discrete object approach e g vector datasets they can be simply displayed by using pie chart data series in multivariate map compositions feick and hall 2004 and effectively visualize the relative sensitivities of all associated input factors at different locations fig 4 a additionally other multivariate mapping techniques for example bar chart ray glyphs and snowflake fig 4 b slocum et al 2009 can also be applicable for geographical visualization of sa measures if sa measures are associated with a continuous field approach e g raster datasets using multiple choropleth maps i e small multiple for multivariate of sensitivities for individual input factors is a useful approach chen et al 2010 xu and zhang 2013 fig 4 c however the classification schemes of multiple choropleth maps should be carefully selected to compare input factors on different choropleth maps slocum et al 2009 furthermore spatial analysis of sa measures for example applying univariate spatial autocorrelation measures and or semi variograms for individual input factors or multivariate spatial autocorrelation measures anselin 2019 may offer further elucidations for the spatial distribution of sa measures 7 an example of the framework with swat this section illustrates the sa framework applied to a widely used sd em the soil and water assessment tool swat neitsch et al 2011 zhang et al 2019 based on an extension of the sa applied in koo et al 2020 following the framework the first step is uncertainty source identification the application of swat can be divided into three sub models the watershed delineation model the hru hydrological response units generation model for preprocessing and the swat model for the prediction of water quantity and quality most studies focus on the model parameters related to the execution of swat as these are often considered the main sources of uncertainty yang et al 2015 there are however other parameters associated with the preprocessing of swat submodels to be considered which may have profound effects on the spatial discretization and resolution watershed and hrus used in the model ray 2018 these preprocessing parameters should be treated as input factors in an sa which then aids in identifying for example minimum percentages of lulc and soil classes in order to eliminate inappropriately small hrus exploring uncertainty in these parameters partially addresses uncertainties in model structure and resolution swat requires input raster datasets of dems lulc soil datasets and vector datasets of meteorological information on monitoring stations and optionally predefined stream networks these datasets provide fundamental information for describing the characteristics of an underlying watershed shen et al 2015 as discussed in section 3 2 dems involve both resolution and random and systematic measurement uncertainties lulc and soil datasets possess resolution and positional uncertainty meteorological information being site specific can have positional uncertainty as well as attribute uncertainty in its measurement importantly measurement uncertainty in both raster and vector datasets should take account of its spatial structure i e spatial autocorrelation although stream networks used in swat do not include attribute information and its related uncertainty the scale and positional uncertainty of a stream network would have significant impacts on the scale and shape of a watershed respectively and should be considered in sa koo et al 2020 the second step is selection of the sa method based on the purposes of the sa and the characteristics of the swat model the purpose of sa can be different for different users but generally swat applications include many uncertainty sources e g hundreds of swat model parameters thus reducing the number of input factors through screening methods is recommended prior to ranking related to the swat characteristics local sa pearson s correlation and linear regression based sa methods are inadequate because input factors in swat are interdependent and generally interact with one other variance based sa methods are often utilized in swat applications zadeh et al 2017 if holistic uncertainty sources in swat are to be evaluated through sa then variance based sa methods may be unsuitable due to their relatively high computational costs razavi and gupta 2015 use of the morris method and rank regression or implementing an emulator instead of swat yang et al 2015 would therefore be recommended the third step is perturbation propagation for individual input factors plausible ranges and assumed probability distributions for swat model parameters can be found in the swat model calibration literature yang et al 2018 abbaspour 2015 as the dem consists of a huge number of grids pixel swapping and a spatial autoregressive model might be undesirable due to their high computational costs thus spatial moving average and sequential gaussian simulation are suggested approaches for propagate perturbations for lulc and soil datasets if their confusion matrices exist using these matrices would provide credible values to propagate although they cannot represent the spatial structure of uncertainty if confusion matrices do not exist simple epsilon bands or other methods for a line feature type could be applicable meteorological information on monitoring stations and predefined stream networks could utilize perturbation propagation for vector datasets the perturbation of position in meteorological information can be propagated using error ellipses and its attributes can be propagated using a spatial autoregressive model positional uncertainty of stream networks can be propagated through simple epsilon bands or other methods for a line feature type if the precision of stream networks is subjected to sa a stream network can be generated directly from a dem using a gis operation and its impact can be evaluated by adjusting a model parameter for the gis operation to designate drainage to a stream network koo et al 2020 the final step is sa evaluation and post processing in this example the morris method and rank regression were the sa methods applied to the swat model in the second step thus the sa result of the morris method could be visualized via scatter plots for absolute means of elementary effects against their standard deviations and rank regression could be represented using scatter plots and regression coefficient plots if spatially varying sa results are of interest for example qois at the outlet of sub watersheds pie charts could be used to display the proportional influence of input factors at different locations see fig 4 a feick and hall 2004 the convergence of the sa measures should additionally be assessed as should any dependence of sensitivity results on the climatic forcing as it changes through the period on interest 8 conclusions this article presents a pragmatic framework for the application of sensitivity analysis sa to a spatially distributed environmental model sd em the suggested framework for sa consists of four general steps potential uncertainty source identification selection of sa method and predictive quantities of interest perturbation propagation and sa evaluation and post processing this framework also provides useful background and general guidance on applying sa to other areas of environmental modeling and related gis communities more specifically the framework can be used to assist in identifying potential uncertainty sources and their corresponding uncertainty classification scheme choosing an appropriate sa method according to the sa purpose s and model characteristics propagating perturbations with plausible ranges and assumptions and verifying sa measures using visualization methods convergence and reliability tests in particular it provides guidance on the assessment and treatment of uncertainty sources related to spatial datasets including positional and attribute uncertainty and on widely utilized perturbation propagation methods that take account of the spatial structure of potential uncertainty in those spatial datasets the framework includes guidance on methods for analyzing sa results involving multiple outputs and their visualization which could offer efficient ways to handle spatially distributed sa measures the framework should therefore be helpful in incorporating the uncertainty of spatial components in sd ems into a general sa process along with the usual model parameters and other input factors that sa commonly evaluates furthermore we expect that model structure uncertainty related to the scales boundaries and discretization of spatial datasets could be addressed through this framework and provide concrete support for further uncertainty analysis declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the primary idea of this manuscript was discussed during two workshops in nanjing may 2019 and canberra december 2019 and the manuscript was drafted through several zoom meetings this work was supported by the key project of nsf of china grant 41930648 the nsf for excellent young scholars of china grant 41622108 national key research and development program of china grant 2017yfb0503500 the australian government research training program agrtp scholarship and a top up scholarship from the anu hilda john endowment fund and the priority academic program development of jiangsu higher education institutions grant 164320h116 
25956,sensitivity analysis sa has been used to evaluate the behavior and quality of environmental models by estimating the contributions of potential uncertainty sources to quantities of interest qoi in the model output although there is an increasing literature on applying sa in environmental modeling a pragmatic and specific framework for spatially distributed environmental models sd ems is lacking and remains a challenge this article reviews the sa literature for the purposes of providing a step by step pragmatic framework to guide sa with an emphasis on addressing potential uncertainty sources related to spatial datasets and the consequent impact on model predictive uncertainty in sd ems the framework includes identifying potential uncertainty sources selecting appropriate sa methods and qoi in prediction according to sa purposes and sd em properties propagating perturbations of the selected potential uncertainty sources by considering the spatial structure and verifying the sa measures based on post processing the proposed framework was applied to a swat soil and water assessment tool application to demonstrate the sensitivities of the selected qoi to spatial inputs including both raster and vector datasets for example dem and meteorological information and swat sub model parameters the framework should benefit sa users not only in environmental modeling areas but in other modeling domains such as those embraced by geographical information system communities keywords sensitivity analysis spatially distributed environmental model uncertainty swat environmental modeling 1 introduction sensitivity analysis sa and uncertainty analysis ua are important tools for investigating model behavior testing model hypotheses and exploring the potential for simplifying models wagener and pianosi 2019 uncertainty is intrinsic to all modeling work that involves representing natural processes and or human behavior sources of uncertainty that need to be considered in such exercises are model input datasets model structure and model parameters sa studies the influence of input factors e g parameters forcing initial value of model states model resolution and model structure such as different parameterization schemes of a model or submodel on model outputs it is considered a key practice in the assessment of environmental models chen et al 2019 gan et al 2014 jakeman et al 2006 matott et al 2009 oakley and o hagan 2004 pianosi et al 2016 yue et al 2020 in comparison ua quantifies the uncertainty of model outputs from input datasets and model parameters typically characterized by empirical probability distributions and or confidence bounds for the model parameters and outputs ua can be considered an extension of sa with the uncertainty distributions for the input factors being used as the perturbations therefore sa can be used to indicate when uncertainty in input factors matters in terms of the impact on the uncertainty in the outputs care must however be taken in the interpretation of sa results as sensitivities can be dependent on parameter ranges selected model structure assumed length of data period examined and its climatic forcing shin et al 2013 the use of sa and ua in environmental modeling has become of particular importance due to the highly complex nature of environmental systems and the attendant complexity of models typically invoked to represent them this is especially the case for spatially distributed environmental models referred to from hereon as sd ems where there tends to be a considerable number of model parameters due to their spatially variant nature and substantial uncertainty in the model and its predictions uncertainty and sensitivity related studies in environmental modeling are rising in popularity because of the growing awareness of the importance of models in supporting informed decision making douglas smith et al 2020 coupled with the fact that current process based environmental models are typically and perhaps necessarily deterministic in their representation farmer and vogel 2016 uusitalo et al 2015 this paper focuses on monte carlo simulation based sa of sd ems which is a valuable tool per se and one that can also inform uncertainty analyses monte carlo simulation based approaches are widely applied due to their ease of implementation yet there is a lack of a comprehensive pragmatic framework for conducting such approaches for sd ems yang et al 2018 an sd em is intrinsically tied to the spatial dimensions of producing and utilizing data that represent the spatially distributed nature of the modeling context grid based digital elevation models dems site specific point measurements and remotely sensed images are examples of such data however sas are rarely conducted for dem and dem derived parameters even though the inherent scale and errors of a spatial dataset and or of the whole environmental model can have a significant impact on model outputs tran et al 2018 a crucial issue to take into account regarding spatial datasets is the spatial structure of their uncertainty generally spatial datasets are characterized by spatial dependence i e spatial coherence and their uncertainties are also spatially autocorrelated oksanen and sarjakoski 2005a wechsler 2007 thus ignoring such characteristics can lead to erroneous estimation of sensitivity measurements moreover because spatial datasets often determine the uncertainty in model resolution and structures through their boundaries discretization and scale exploring uncertainty related to spatial datasets can partly account for model uncertainty in sd ems this article introduces a pragmatic framework for the application of sa to an sd em using a scenario simulation based approach to investigate the significance of potential uncertainties in the model inputs which can not only explore model and data assumptions transparently but also be an informative precursor to a more thorough ua the objectives of the framework are to provide sufficient information and background in order to guide the selection of more appropriate choices at each step of the sa process potential uncertainty source identification selection of sa method s and quantities of interest qoi perturbation propagation and sa evaluation and post processing the framework emphasizes the following aspects it attempts to address potential uncertainty sources related to spatial datasets and assists in propagating the potential uncertainty sources by considering their likely spatial structure therefore the framework helps to explore the impact of potential uncertainty of spatial datasets in an sd em and to compare their relative impacts with the usual factors in sa e g model parameters the framework is intended to benefit both non experts and sa users in environmental modeling and geographical information system gis communities the remainder of this article is organized as follows section 2 broadly introduces the pragmatic framework for applying sa to sd ems covering potential uncertainty source identification selection of sa method and qoi perturbation propagation and sa evaluation and post processing then from sections 3 to 6 the detailed steps and their corresponding considerations are discussed section 7 provides a concise example of the sa framework the article concludes in section 8 with a discussion of future needs and opportunities 2 a pragmatic sa framework for sd ems the presented framework prescribes sequential steps in which important considerations are highlighted to guide modelers towards the selection of appropriate choices for the pragmatic application of sa to uncertainty exploration in sd ems the overarching steps and the corresponding considerations are depicted in fig 1 the main purpose of the framework is to identify the contributions of potential uncertainty sources to the selected qoi this section introduces the pragmatic framework to provide a broad guideline for sa users while the following sections detail the considerations within each step the first step is to identify potential sources of uncertainty section 3 numerous studies have investigated uncertainty sources in the context of environmental modeling and classified them in their own schemes matott et al 2009 refsgaard et al 2007 understanding these general classification schemes and the uncertainty sources involved assists in identifying the sources of uncertainty related to a specific application in particular this article discusses potential uncertainty sources not only in model parameters and model uncertainty but also in spatial datasets which are used as direct input s and or to derive parameters to describe the underlying spatially distributed structure of sd ems e g dem the second step is the selection of sa methods and qoi section 4 this selection primarily depends on the purposes of sa e g screening and ranking and the characteristics of the sd em the characteristics can include the model complexity and or computational cost as this framework is intended for monte carlo simulation based sa applying sa methods that require a large number of model evaluations to determine sa measures might not be feasible for a computationally expensive model this article broadly categorizes the most frequently used sa methods for environmental modeling based on their purposes and characteristics and synthesizes them to assist sa users and communities in selecting appropriate ones for a pragmatic sa application here we provide a general description and several previous studies provide complementary explanations for further sa methods pianosi et al 2016 borgonovo and plischke 2016 sarrazin et al 2016 for selecting qoi only scalar outputs are generally utilized as qoi in sa of environmental models which often requires aggregating spatially and or temporally distributed outputs into a scalar function pianosi et al 2016 however since potential uncertainty sources in sd ems have different impacts on spatially and also temporally distributed scalar outputs pappenberger et al 2008 wang et al 2013 preserving a spatial distribution of scalar outputs might be useful to understand the underlying spatial processes within sd ems even if that requires managing vast computing power the third step in the framework is the perturbation propagation of the identified potential uncertainty sources section 5 generally local and global sa methods require different types of perturbation propagation methods thus local sa utilizes the neighborhood values of the nominal value and global sa generally requires the input variability space via a probability distribution borgonovo and plischke 2016 for all sa methods perturbation propagation with appropriate distributions is crucial because the representativeness of uncertainty sources primarily determines the sa results different types of uncertainties also need different types of perturbation propagations for example the propagation of perturbations in a model parameter that is represented as a scalar random variable could be performed using a probability distribution e g normal distribution while more complex perturbation propagation methods are necessary for characterizing the uncertainty of input datasets e g spatial datasets by simultaneously taking account of their characteristics e g spatial autocorrelation wechsler 2007 crosetto and tarantola 2001 nonetheless perturbation propagation of model uncertainty is still an ongoing research subject and remains a fruitful area of investigation matott et al 2009 uusitalo et al 2015 o hagan 2012 the final step of the framework is evaluating the results of the sa which includes post processing of analysis results section 6 in evaluating sa results reliability and convergence aspects should be assessed as a verification step sa measures or metrics vary with sample size thereby requiring a convergence test to check if the metric s of choice is converging and its confidence bounds acceptable for the purpose yang 2011 in addition different sa methods are based on different premises may produce different metrics and hence produce different outputs thus sa with different methods can increase confidence in the reliability or interpretation of sa outputs finally this step also includes credibility assessment for sa outputs if unexpected sa outputs are obtained these outputs can lead to new indications of uncertainty in model behavior pappenberger et al 2008 or indicate issues with the model implementation pianosi et al 2016 otherwise these outputs can assist with revising the sa steps such as identifying missing uncertainty sources and redefining the perturbation propagation approach because sa results are generally associated with large sets of potential uncertainty sources visualization methods of sa results are useful to identify critical uncertainty sources and to compare their importance thus this step includes the descriptions of specific visualization methods with their corresponding sa methods conventional scientific visualization techniques for sa outputs kelleher and wagener 2011 and geographical visualization and analysis for the representation of spatially distributed sa measures feick and hall 2004 3 identification of potential uncertainty sources 3 1 classification of uncertainty sources this initial step involves the identification of potential uncertainty sources associated with the model s input factors that influence the selected outputs of an environmental model or functions of those outputs i e qoi various types of uncertainty sources could influence the outputs of the models and numerous classification schemes for uncertainty sources have been introduced to categorize them matott et al 2009 refsgaard et al 2007 beck 1987 linkov and burmistrov 2003 because the classification schemes commonly share two fundamental uncertainty sources i e input and model uncertainties this article discusses the various uncertainty sources in these two broad categories 3 2 input uncertainty input uncertainty is associated with model parameters and input datasets among various sources of uncertainty model parameter uncertainty is the most commonly considered source setegn et al 2010 wu and chen 2015 wu and liu 2012 berzaghi et al 2019 porada et al 2018 and can be controlled to some extent through calibration processes zhao et al 2018 therefore uncertainty in model parameters is often considered to be reducible and it has been argued that model parameters can be carefully tailored to reduce that uncertainty related to model outputs and to improve model performance matott et al 2009 however if globally optimized parameters are obtained through a calibration process they would also be affected by other sources of uncertainty including input data uncertainty model uncertainty and also model parameter uncertainty and these might lead to equifinality zhao et al 2018 beven and freer 2001 input datasets are usually assumed to be accurate that is effectively without uncertainty however this is incorrect as all data have inherent uncertainties chrisman 1991 uncertainties related to input datasets can be irreducible matott et al 2009 and thus they are often ignored in uncertainty related studies moreover uncertainty in spatial datasets involves spatial autocorrelation griffith 2008 koo et al 2018c in the sd em context spatial datasets include maps and site specific measurements for example temperature and precipitation surfaces would have strong positive spatial autocorrelation and soil and land use land cover lulc datasets possess complex spatial autocorrelation legendre 1993 site specific measurements e g meteorological data have spatial autocorrelation as well as temporal autocorrelation therefore if an environmental modeling analysis does not address the independent inputs and relationships among them an incomplete understanding of the uncertainty in the model will result leading to a biased estimation of the confidence in the model outputs the uncertainty in spatial datasets is generally caused by five fundamental components lineage positional accuracy attribute accuracy logical consistency and completeness ansi 1998 koo et al 2020 briefly lineage relates to the description of spatial data sources e g dates and reference systems and logical consistency describes the fidelity of spatial data structure e g topology completeness refers to selection criteria of spatial entities for example geometric thresholds such as minimum width and area of spatial features positional and attribute accuracies literally refer to uncertainties respectively in position i e location and attribute information of spatial datasets among these components positional and attribute accuracies are closely related to uncertainties in sd ems and they show different aspects depending on the type of spatial datasets spatial datasets can be broadly divided into raster and vector datasets a typical example of a raster dataset is a dem where scale i e resolution random and systematic measurement uncertainties resulting from attribute accuracy are the major uncertainty sources hengl et al 2010 the scale issue is relatively well discussed as a source of uncertainty in dems chaubey et al 2005 dixon and earls 2009 lin et al 2013c shen et al 2013 together with scale measurement uncertainty is known to also have an impact on watershed delineation oksanen and sarjakoski 2005a wu et al 2008 stream network extraction hengl et al 2010 and derivation of other topographic parameters wechsler 2007 as systematic measurement uncertainty generally shows a fixed pattern stemming from dem generation processes e g blunders if the cause of the uncertainty is known it could be reduced wechsler 2007 however random measurement uncertainty still remains after reducing systematic uncertainty for example the shuttle radar topography mission srtm v 4 1 dataset has a vertical accuracy of 16 m at a 95 confidence level mukul et al 2017 other widely used raster datasets are lulc and soil datasets which possess uncertainty related to positional uncertainty and scale issues koo et al 2020 vector datasets are typically used to define the boundary of a study area and describe topographic and or environmental features such as stream networks in addition site specific measurements are handled as a type of vector dataset generally point features that have attributes on specific locations for example measurements of precipitation temperature wind speed humidity and solar radiance even though some raster datasets e g precipitation and temperature surfaces are converted from site specific measurements their uncertainty sources can mainly be explained by uncertainty in vector datasets vector datasets typically include two main uncertainty sources which are positional and attribute uncertainties koo et al 2018a positional uncertainty refers to the uncertainty of geographical features in vector datasets which often results from a global positioning system gps geocoding and digitizing errors attribute uncertainty describing non spatial properties of geographical features in vector datasets are generally estimated from their sampling processes aouissi et al 2013 strauch et al 2012 tasdighi et al 2018 bárdossy and das 2008 chaplot et al 2005 cho et al 2009 gong et al 2012 masih et al 2011 and measurements shen et al 2015 li 2014 in addition attribute uncertainty often includes spatial autocorrelation when attribute uncertainty contains temporally varying quantities they also need to consider the information lost in converting to discrete time littlewood and croke 2013 3 3 model uncertainty model uncertainty results from the inability of a model to mimic the real world yen et al 2014 model uncertainty might be subdivided into the effects of model structure model resolution and model integration uncertainties matott et al 2009 voinov and shugart 2013 first of all model structure uncertainty is caused by a model structure that imperfectly represents underlying environmental processes in a model yen et al 2014 numerous alternative model structures e g scientific hypotheses and equations might be proffered in a model which could adversely impact model outputs a related consideration is the issue of the identifiability of the model structure guillaume et al 2019 shin et al 2015 which largely means the data available are insufficiently informative to identify unique values of some of its parameters sa methods are often used to determine the insensitive non identifiable parameters so that focus for calibration and or uncertainty analysis can then be turned to the most sensitive ones second model resolution uncertainty is due to uncertainties in the spatio temporal discretization boundary specification and scale dependence of a model matott et al 2009 in an sd em spatial discretization boundary and scale are often determined by the available spatial datasets and model resolution uncertainty can then be partially explained through exploring uncertainty of the spatial datasets koo et al 2020 trusel et al 2015 fig 1 another aspect of model uncertainty arises from model integration processes chen et al 2020 currently environmental models become more complex by integrating multiple models lin et al 2013a 2013b lu et al 2019 the integration processes yield uncertainty from skewed space e g difference in spatial resolution mismatched measurement scales and confusion of linguistic representations voinov and shugart 2013 particularly in an sd em when sub models with different spatial and temporal scales are integrated without a solid design the uncertainty of an integrated model could become large and undetectable tscheikner gratl et al 2019 4 selection of sa method s and quantities of interest this step firstly provides guidance on sa method selection based on two main criteria the purposes of the sa and the characteristics of the sd em this guidance includes only two fundamental sa purposes i e ranking and screening but sa can have additional purposes such as factor mapping that provides further descriptions for the input space related to qoi saltelli et al 2008 sa methods for ranking generate the order of input factors based on their relevant influence on qoi and screening methods identify input factors with significant or negligible influence on model output pianosi et al 2016 sa methods for each purpose can be applied sequentially such that model results from screening methods are leveraged to reduce the number of input factors and are followed by sa for the purpose of ranking thus reducing the overall number of model evaluations saltelli et al 2004 sun et al 2012 secondly two major characteristics of an environmental model are necessary to consider in selecting appropriate sa methods model complexity and interdependency between input factors saltelli 2002 here we briefly discuss widely used sa methods including local sa the morris method correlation and regression and variance based sa methods according to the two major criteria fig 2 classifies these sa methods where the positions of each sa method relate to sa purpose and model complexity and their outlines represent interdependency in addition the advantages of an emulator and its consideration for dealing with spatially distributed outputs are also briefly discussed local sa is the simplest sa method and is often conducted through one at a time oat perturbation of input factors around their nominal values to determine the response of model outputs sun et al 2012 campolongo and saltelli 2000 a formal approach for local sa involves using partial derivatives helton 1993 partial derivatives can provide sa measures metrics for both ranking and screening however they should be rescaled and applied to several locations in factor space in order to reveal the global effects of input factors with different measurement units borgonovo and plischke 2016 campolongo et al 2011 purely oat analyses are however typically inappropriate for determining sensitivity estimates such analyses do not consider interactions among input factors borgonovo and plischke 2016 and while oat can investigate non linearities if input factors are independent newham et al 2003 typical applications are unable to do so due to the use of a single perturbation sun et al 2012 saltelli and annoni 2010 importantly because local sa evaluates sensitivity at a specific location of input factors rather than over their plausible ranges as global sa does it might provide a limited indication of model behavior sun et al 2012 sobol 2001 although pure oat analyses have the advantage of reduced computational time over a more substantive global analysis an initial indication of model behavior can be gained with just n 1 model evaluations for n input factors pianosi et al 2016 or fewer if groups of input factors are perturbed together through group sampling sobol 2001 improper model behavior caught at this stage indicates errors in the model implementation to be addressed before global analyses are applied a simple global extension of local sa is the morris method morris 1991 also known as the elementary effect test saltelli et al 2008 the morris method generally requires much lower numbers of model evaluations than other global sa methods for the purpose of screening and thus the morris method is appropriate for computationally complex models and or models with a large number of input factors campolongo et al 2007 herman et al 2013 a drawback of the morris method is that it gives a poor measure of the relative importance between factors and can be considered as offering qualitative sensitivity measures only brockmann and morgenroth 2007 besides the average elementary effects it does provide the standard deviations of the elementary effects which are beneficial for identifying interaction effects among input factors norton 2015 regional sensitivity analysis spear and hornberger 1980 typically divides input factors into two or more groups depending on a prescribed threshold of model output and then studies the difference in their empirical cumulative distribution functions cdf for each input factor the kolmogorov smirnov k s statistic quantifies the divergence between the cdf and serves as a common sensitivity measure thus if the k s statistic is high i e the cdf of one group differs from the other the input factor has a significant influence on model output pianosi and wagener 2015 k s statistics are mainly utilized for ranking input factors however the k s statistic is inappropriate for screening because it is only applicable to the same groups saltelli et al 2008 the advantage of regional sensitivity analysis is that it is applicable for any type of splittable model outputs e g futter et al 2007 whitehead et al 2015 whitehead and hornberger 1984 however if the grouping i e splitting criterion is not clear i e a model does not have meaningful model output values to describe model behavior regional sensitivity analysis would be inappropriate various correlation and regression methods are also extensively used to measure sensitivities these methods basically obtain sa measures based on different statistics i e correlation and regression coefficients between input factors and qoi generated from a monte carlo simulation pianosi et al 2016 helton et al 2006 specifically for correlation coefficient estimations various types of correlation coefficients are selected mainly based on the linearity between input factors and model outputs when they have a linear relationship pearson and partial correlation coefficients are appropriate methods saltelli and marivoet 1990 if the relationship is non linear spearman and partial rank correlation coefficients can be used as alternatives pastres et al 1999 furthermore if sa methods simultaneously are to take account of multiple relationships for multiple outputs a canonical correlation analysis provides an additional option minunno et al 2013 regression methods obtain sensitivity measures by estimating regression coefficients which are commonly standardized regression methods are often superior to correlation methods in deriving sensitivity measures especially when a large number of input factors are considered since regression methods can obtain sa measures of all input factors at once however while linear regression is the simplest and most widely used sa method iman and helton 1988 it is not suitable if there is a non linear or non monotonic relationship in the model response and a high level of interaction among factors also makes linear regression act poorly yang 2011 when a non linear relationship exists rank regression storlie et al 2009 and machine learning techniques such as decision trees singh et al 2014 are appropriate regression and correlation methods are commonly utilized for both screening and ranking purposes variance based methods produce sensitivities by decomposing the variance of a model output into the contributions from input factors the contributions can be defined according to different indices for example first order and total indices saltelli et al 2008 the first order index quantifies the contribution of a specific input factor to the variance of the selected qoi while the total index measures the total contribution of an input factor to the variance of the qoi including those due to its interactions with other input factors the first order index is usually used to rank input factors when interactions are not significant with total indices variance based sa methods are able to address non linearity of model responses to input factors factors with a total index close to zero can be considered negligible and screened out pianosi et al 2016 often these negligible factors are made constant a practice referred to as factor fixing variance based sa methods can be challenging for computationally intensive complex models because they take a relatively large number of factor samples and related model evaluations to obtain reasonably accurate and stable indices gan et al 2014 however several approaches such as the sobol method sobol 2001 fourier amplitude sensitivity test fast cukier et al 1973 and extended fast saltelli et al 1999 have been proposed to more efficiently estimate main and total effects paleari and confalonieri 2016 in practice however the computational requirement is still a burden for these sa methods moreover common variance based approaches operate on a number of assumptions including that the variance of model outputs resulting from the prior input distribution is indicative of input factor sensitivities inputs are independent saltelli and tarantola 2002 and the distribution of the sampled model outputs often estimated through kernel density estimation approaches are unimodal misleading sa results may be produced if model outputs do not conform to these assumptions pianosi et al 2016 for computationally intensive models sa e g variance based methods can be implemented using an emulator which is a statistical approximation of the output response surface of the original environmental model o hagan 2012 a simple approach for building an emulator is through use of gaussian processes oakley and o hagan 2004 though other options exist such as polynomial chaos expansions sudret 2008 statistical emulators young and ratto 2011 and machine learning based emulators e g random forest and gradient boosting storlie et al 2009 however an emulator might be inappropriate to evaluate a large number of input factors because it suffers from estimation inefficiency and inaccuracy due to the curse of dimensionality storlie et al 2009 li et al 2020 this can be resolved by screening out negligible input factors or by applying an emulator that includes a procedure for input factor selection yang 2011 the selection of qoi sometimes embodied in an objective function or loss function is also crucial to reflect the modeling purposes as different modeling purposes lead to different sensitivity measures in the input factors for example rainfall intensity yields more sensitivity to stream flow peak than to baseflow based on modeling purpose a large number of qoi have been used in hydrological models the most frequently used include the nash sutcliffe coefficient root mean square error rmse and differences in the flow duration curve e g between simulated and observed flows and total nitrate although sa tends to select only single scalar qoi sd ems often need to explore spatially distributed sensitivities of input factors on multiple and multi dimensional outputs gupta and razavi 2018 pappenberger et al 2008 which can involve a colossal computational burden emulators may be developed to circumvent the issue of computational cost a typical practice however is to build separate emulators for individual outputs ryan et al 2018 which may impose an additional computational cost implementing other types of sa methods may be useful to mitigate this computational burden including a separate generalized additive model mara and tarantola 2008 partial least squares sobie 2009 multi fidelity polynomial chaos expansions palar et al 2018 and global sensitivity matrix approaches razavi and gupta 2019 moreover decreasing the dimensionality of outputs by using a principal component analysis and grouping factors based on bootstrap based clustering sheikholeslami et al 2019 can provide another solution gómez dans et al 2016 5 perturbation propagation monte carlo simulation based sa needs to propagate the perturbations of input factors through the model to analyze the sensitivity of model outputs and their qoi to those input factors saltelli and tarantola 2002 proper selection of the perturbations within plausible ranges and distributional assumptions is a crucial step in sa because the perturbation attempts to reflect the degree of uncertainty in input factors this section introduces useful methods for perturbation propagation of the corresponding uncertainty sources including input parameters spatial and point datasets 5 1 model parameters model parameters in environmental models are typically represented as scalar variables and often treated as random variables with prior probability distributions borgonovo and plischke 2016 specifically samples of individual model parameters are obtained from their corresponding probability distributions and then sa evaluates model responses based on the samples the interactions between model parameters can be represented using a covariance matrix for example the cholesky decomposition xiu and karniadakis 2003 because defining appropriate probability distributions with plausible ranges are also crucial for evaluating model parameter sensitivities taking all available information on individual model parameters is necessary for the generation of those probability distributions e g expert opinion crosetto and tarantola 2001 5 2 raster datasets an sd em normally uses various types of spatial datasets including spatially distributed input datasets and site specific measurements because spatial datasets have various forms e g vector and raster datasets different types of perturbation propagation are required crosetto and tarantola 2001 furthermore the propagation for spatial datasets should consider the characteristics of those datasets especially spatial autocorrelation temme et al 2009 table 1 demonstrates applicable perturbation propagation methods for the various types of spatial datasets raster datasets are either generally subdivided into categorical e g lulc and soil datasets or quantitative e g dem temperature and precipitation surfaces rasters requiring different perturbation propagation methods heuvelink 1998 in quantitative rasters individual cell values can be treated as individual random variables with their own probability distributions which means an observed quantitative raster is just one rendering of all possible realizations however this assumption ignores the spatial structure of uncertainty in quantitative rasters random fields are widely used to represent uncertainty in quantitative rasters random fields comprise a surface of random values that estimates uncertainty magnitude variance and spatial variability where each value represents potential uncertainty at a specific location of the grid cell wechsler 2007 furthermore random fields are applicable for regularly discretized space time voxels in 3d rasters pebesma et al 2007 like other perturbation propagation methods random fields require a definition of the appropriate potential uncertainty level i e plausible ranges and their spatial structure if the information for the ranges and structures are not available random fields are often estimated using the accuracy statistics of a target raster dataset e g root mean square error if the information is obtained from a survey and other methods this can be used for estimating parameters of a random field generation the simplest method for random field generation is using a normal distribution with a mean of zero and standard deviation derived from the accuracy statistics of a quantitative raster dataset however potential uncertainty in spatial datasets has spatial structure including spatial autocorrelation wechsler 2007 oksanen and sarjakoski 2005b which should be considered in a random field generation the following methods are the typical methods accounting for spatial autocorrelation in random field generation the first method is the spatial moving average wechsler and kroll 2006 which applies a low pass filter to a random field generated from a simple probability distribution e g normal distribution without considering spatial autocorrelation the size of a low pass filter determines the level of spatial autocorrelation which often covers from 3 by 3 grids to the grid size that is computed from the range of a semi variogram in a target quantitative raster pixel swapping fisher 1991a goodchild and openshaw 1980 and a spatial autoregressive model hunter and goodchild 1996 koo et al 2019 anselin 1995 are other methods for random field generation that considers spatial autocorrelation they can be utilized for quantitative raster datasets and attribute uncertainty in vector datasets pixel swapping is developed based on the concept of simulated annealing where two cells in random fields are continuously swapped until spatial autocorrelation in the random fields achieves its threshold level derived from spatial autocorrelation level in a target raster dataset this method has the advantage that it is a simple process but it is difficult to implement for a large spatial dataset due to the slowness of the procedure oksanen and sarjakoski 2005a a spatial autoregressive model produces random fields based on the following equation y i ρ w 1 ε where i denotes an identity matrix and ε is a vector that is generated generally from a probability distribution by using an autoregressive parameter ρ this model effectively determines the level of spatial autocorrelation w is a spatial weights matrix which is also useful for specifying various types of neighborhood definitions e g contiguity and k nearest neighbors because it is an n by n matrix and tends to be sparse computing its inverse often requires a lot of resources for large datasets anselin 2005 sequential gaussian simulation is a widely applicable method for random field generation koo et al 2020 aerts et al 2003 which is developed based on a geostatistical approach with a normality assumption for potential uncertainty goovaerts 1997 the basic steps for sequential gaussian simulation are as follows first random paths are generated in a field and each node in the path is sequentially visited second at each node descriptive statistics for a local conditional probability function are estimated based on surrounding values using kriging and finally a random value is generated from the local conditional probability function if survey samples for uncertainty exist the values at the sample nodes are maintained with the original sample value which has been referred to as conditional gaussian simulation aerts et al 2003 a data transformation process will be required if a dataset does not follow a gaussian distribution because sequential gaussian simulation is only applicable to a dataset that follows a gaussian distribution deutsch and journel 1998 additionally if prior knowledge for the spatial structure of uncertainty in a quantitative raster dataset can be obtained from surveys or higher accuracy datasets the three parameter method ehlschlaeger et al 1997 is useful to reflect this prior knowledge whereas only one parameter is available in pixel swapping and a spatial autoregressive model this three parameter method is superior for generating random fields with the mean and standard deviation under gaussian distribution and spatial autocorrelation wechsler 2007 other studies hengl et al 2010 fisher 1998 holmes et al 2000 discuss further perturbation propagation for quantitative raster datasets and have implemented them in real world applications perturbation propagation for categorical raster datasets has received less attention than those for quantitative raster datasets crosetto and tarantola 2001 generally the selection of the method for categorical raster datasets relies on the method of dataset generation when a categorical raster dataset e g for lulc and soil data is generated using a fuzzy classification method of satellite images individual pixels include uncertainty information of the classification result such as probability or membership vectors lucieer and kraak 2004 this type of uncertainty information can directly be applied to monte carlo simulation based sa when a conventional classification method is used for raster data generation the confusion matrix i e error matrix is available for sa which is a cross tabulation of the classified raster against reference samples to estimate classification accuracy the confusion matrix based perturbation propagation is fully discussed in heuvelink 1998 fisher 1991b however the confusion matrix might be limited in representing the spatial structure of uncertainty comber et al 2012 in addition perturbation propagation for vector datasets can also be applied for categorical raster datasets mainly to represent their positional uncertainty kiiveri 1997 shi 1998 the details of perturbation propagation will be discussed with that for vector datasets in section 5 3 5 3 vector datasets vector datasets including the case of site specific measurements mainly have two major potential uncertainty sources attribute and positional uncertainties koo et al 2018a ansi 1998 attribute uncertainty results from sampling and measurement errors thus perturbation propagation can be simply a probability distribution function derived from the descriptive statistics of potential attribute uncertainty if the attributes of geographical features in vector datasets are spatially autocorrelated pixel swapping goodchild and openshaw 1980 and a spatial autoregressive model anselin 1995 are useful to describe the spatial structure of attribute uncertainty for example with a spatial autoregressive model a random vector i e ε is generated from the descriptive statistics of attribute uncertainty and spatial structures are described by a spatial weights matrix i e w of geographical features and predefined spatial autocorrelation level i e ρ as we described in the previous section 5 2 propagating perturbations for positional uncertainty in vector datasets has been one of the major focuses in spatial data quality research groups devillers et al 2010 the geometric features of vector datasets are generally classified as point line and polygon however in the context of modeling positional uncertainty a polygon is considered as a closed line shi 1998 according to the latter perturbation propagation is discussed in two general feature types point and line error ellipses constitute a common method for propagating perturbations of point features dutton 1992 stanislawski et al 1996 goodchild 1991 where x y coordinates follow a two dimensional extension of a probability distribution function on individual points for positional uncertainty representation a normal distribution can be simply used for the probability distribution goodchild 1991 wolf and ghilani 1997 but a log normal distribution a mixture of bivariate t distributions zimmerman et al 2007 and a chi square distribution griffith et al 2007 have also been suggested karimi et al 2004 koo et al 2018c positional uncertainties of point features are usually considered independent though possibly spatially autocorrelated if independent error ellipses can be directly applicable to their selected probability distributions however if positional uncertainties of individual point features are dependent and spatially autocorrelated additional stochastic techniques e g a spatial autoregressive model are required to represent their dependence structure in particular the source of point features e g gps geocoding and lidar is the important criterion to select a proper probability distribution and independence between positional uncertainty zandbergen 2008 for example positional inaccuracy and its empirical distribution of gps have been regularly reported by the federal aviation administration and its accuracy is generally reliable and independent regardless of study areas however the positional uncertainty of geocoded points varies and is dependent on the locations of geocoded points koo et al 2018c perturbation propagation for positional uncertainty of line features is more complex than that of point features because the former consists of a set of the propagations for individual point features shi 1998 shi et al 2014 similar to error ellipses for point features the epsilon band is a typical method to represent positional uncertainty of line features crosetto and tarantola 2001 shi 1998 which defines an error band of a boundary using a constant distance for both sides of lines however the epsilon band does not represent spatial structures of positional uncertainty between individual point features on a line feature specifically point features that are located relatively midway on a line segment have generally smaller positional uncertainty than that of endpoints shi 1998 and positional uncertainty of individual point features are also often spatially autocorrelated tong et al 2013 thus under the assumption that the positional uncertainties at both endpoints are independent and the amount of positional uncertainty varies by their location perturbation propagation is offered shi 1998 furthermore a stochastic process based model has been proposed for considering spatial autocorrelation between points shi and liu 2000 recently entropy based models gong and li 2011 and the statistical simulation error model tong et al 2013 have offered alternatives for perturbation propagation for positional uncertainty of line features 5 4 model uncertainty quantification of model uncertainty is challenging and is a subject of ongoing research o hagan 2012 additionally quantitative methods alone cannot address all aspects of model uncertainty as there are qualitative sources that cannot be quantified and arise from the subjective judgment and biases of the modelers and stakeholders chen et al 2007 thus model uncertainty can also be explained based on qualitative e g expert assessment rather than quantitative evaluation uusitalo et al 2015 that said a qualitative evaluation may be sufficient for the model purpose when its evaluators are well informed for instance several solver types are compared by adjusting convergence criteria to evaluate the impact of model solution precision ahlfeld and hoque 2008 similarly different model formulations resolutions and solvers are explored for model uncertainty investigation farthing et al 2012 evaluating the uncertainty caused by model integration is also difficult using quantitative methods due to a lack of proper perturbation propagation voinov and shugart 2013 tscheikner gratl et al 2019 voinov and cerco 2010 the resolution of spatial datasets has been used as one source to explore model uncertainty trusel et al 2015 an sd em generally uses predefined sources of spatial datasets for example srtm for dem and the impact of spatial dataset resolution is often investigated by comparing several spatial datasets with different resolutions thus the resolutions of dem lulc and soil datasets can show a significant impact on sd em outputs chaubey et al 2005 dixon and earls 2009 lin et al 2013c kumar and merwade 2009 generally finer resolutions of spatial datasets lead to more accurate model outputs shen et al 2013 but longer model evaluation time and furthermore the uncertainties arising from the resolutions of spatial datasets can be compensated for when different types of spatial datasets are utilized shen et al 2015 however monte carlo simulation based sa requires propagating perturbations for individual input factors with their plausible ranges and sometimes probability distribution assumptions adding a systematic or random model error at model runtime is suggested to assess perturbation propagation marin et al 2003 however the method could not differentiate the sources of uncertainties because the added errors include both input and model uncertainties adding systematic errors directly in the model structure might be an alternative to adding errors into model runtime for example general perturbation propagation is provided for state parameter estimation based on recursive and batch estimation beck 1987 another frequently used strategy for model uncertainty exploration is adjusting parameters that relate to model structures koo et al 2020 yen et al 2014 wagener et al 2003 however this strategy is not applicable when the model parameters are discrete and have limited options recently koo et al 2020 explored model structure uncertainty using sa by adjusting the level of spatial discretization which could provide another solution for taking model uncertainty into account in sa 6 sa evaluation and post processing 6 1 assessing convergence and credibility the convergence of sa measures needs to be assessed because sa measures sometimes are not constant and vary with sample sizes especially when they are obtained from smaller sample sizes than required sizes suggested in the literature sarrazin et al 2016 vanrolleghem et al 2015 two methods are generally utilized to evaluate the convergence of sa measures which are based on the central limit theorem clt and the bootstrapping technique yang 2011 according to the clt the sample mean of a distribution with mean μ and standard deviation σ approaches a normal distribution with mean μ and standard deviation σ n with increasing sample size n the clt based method calculates sa measures r times using different sets of sub samples and compares its mean and standard deviation using gradually increasing sizes of sub samples the convergence of an sa measure could be regarded as achieved when the coefficient of variation σ μ does not show a significant change bootstrapping uses sub samples from the original samples and then compares sa measures derived from the sub samples to the original sa measures the advantage of the bootstrapping technique is that there is no requirement for additional simulation but the convergence rate could be overestimated i e underestimation of uncertainty when the sub samples are strongly dependent on the original samples in addition a convergence test could be performed by analyzing sa measures obtained from different numbers of monte carlo simulations vanrolleghem et al 2015 the number of required samples for a convergence test differs according to the purpose type s of sa applied and the characteristics of the environmental model sarrazin et al 2016 sa for screening purposes generally requires a smaller sample size for a convergence test than that for ranking similarly the required number of samples for convergence is typically the largest in variance based sa and significantly smaller for the morris method and local sa methods campolongo et al 2007 in variance based sa methods sa measures for the main effect converge faster than those for the total effect sarrazin et al 2016 nossent et al 2011 the characteristics of the environmental models and its related study area processes and data also influence convergence rate so that there is no clear relationship between the number of input factors and the required sample size sarrazin et al 2016 finally the reliability and credibility of sa measures should be assessed the reliability and credibility are obtained by verifying that the underlying assumptions and conditions of the sa are satisfied in a target environmental model pianosi et al 2016 for example linear regression assumes a linear relationship in model response which might well be inappropriate for the model with a non linear response additionally the sa results obtained could be biased due to use of implausible perturbation propagation and missing input factors another check is conducting the sa with different sa methods the reliability of sa results could be regarded as being enhanced if there is a consensus among different methods if the sa results are contradictory it encourages further investigations to discover various aspects of model behaviors that are captured from different sa methods pappenberger et al 2008 paleari and confalonieri 2016 6 2 visualizing sa measures effective visualization methods help to increase understanding and interpretation of sa measures and their relationships to input factors being especially valuable when sa measures are associated with a large number of input factors specifically visualizing sa results helps in achieving the general purposes of sa by finding and ordering the critical input factors visualization also supports the discovery of counterintuitive sa results which could lead to unearthing new aspects of model behavior or revising our sa processes for example adding missing uncertainty sources and re propagating perturbation methods when revising sa processes the visualization helps to compare sa results under various conditions such as different ranges and theoretical assumptions of perturbation propagation furthermore in sd ems spatial and temporal patterns in sa measures could be revealed through visualization methods pianosi et al 2016 simply the relationship between input factors and their corresponding outputs are typically visualized using scatter plots colored scatter plots and parallel coordinate plots specifically scatter plots demonstrate the relationship between model output with one input fig 3 a while colored scatter plots typically show the relationship of model output with two input factors fig 3 b thus scatter plots and colored scatter plots are useful for screening and ranking input factors parallel coordinate plots show distributions of input factors and outputs fig 3 c which can highlight patterns using colors and or dynamic linking and brushing ge et al 2009 koo et al 2018b symanzik et al 2000 violin plots are also useful to visualize the distributions of input factors and outputs hintze and nelson 1998 whose relationship could be emphasized using dynamic linking and brushings further examples of general scientific visualization methods for sa are suggested in pianosi et al 2016 and kelleher and wagener 2011 some sa methods are effectively represented using their own specific visualization methods for instance morris method results are represented using scatter plots of the absolute means of elementary effects against their standard deviations for individual input factors fig 3 d where both the relative importance of each input factor and their interactions are highlighted local sa and also correlation and regression based sa methods utilize general scatter plots of input factors against outputs additionally a regression coefficient plot can be applicable for regression based sa results fig 3 e a regression coefficient plot is a scatter plot of an estimated coefficient with lines indicating standard errors which effectively show the relative importance of individual input factors and compare changes of regression coefficients in sa results under different conditions visualization of variance based sa methods is sometimes difficult because it requires a simultaneous representation of multiple sa measures for main and total effects a simple and general method for such visualization is using a stacked bar plot for individual input factors with main and total effects fig 3 f recently circos kelleher et al 2013 and radial convergence diagrams butler et al 2014 were developed to effectively visualize the main and total effects of multiple sa measures in an sd em because all the uncertainty sources could make different contributions to the spatially varying outputs geographical visualization of spatially variable sa measures and their spatial analysis can enhance understanding of spatial aspects in sa measures chen et al 2010 when sa measures are represented in a discrete object approach e g vector datasets they can be simply displayed by using pie chart data series in multivariate map compositions feick and hall 2004 and effectively visualize the relative sensitivities of all associated input factors at different locations fig 4 a additionally other multivariate mapping techniques for example bar chart ray glyphs and snowflake fig 4 b slocum et al 2009 can also be applicable for geographical visualization of sa measures if sa measures are associated with a continuous field approach e g raster datasets using multiple choropleth maps i e small multiple for multivariate of sensitivities for individual input factors is a useful approach chen et al 2010 xu and zhang 2013 fig 4 c however the classification schemes of multiple choropleth maps should be carefully selected to compare input factors on different choropleth maps slocum et al 2009 furthermore spatial analysis of sa measures for example applying univariate spatial autocorrelation measures and or semi variograms for individual input factors or multivariate spatial autocorrelation measures anselin 2019 may offer further elucidations for the spatial distribution of sa measures 7 an example of the framework with swat this section illustrates the sa framework applied to a widely used sd em the soil and water assessment tool swat neitsch et al 2011 zhang et al 2019 based on an extension of the sa applied in koo et al 2020 following the framework the first step is uncertainty source identification the application of swat can be divided into three sub models the watershed delineation model the hru hydrological response units generation model for preprocessing and the swat model for the prediction of water quantity and quality most studies focus on the model parameters related to the execution of swat as these are often considered the main sources of uncertainty yang et al 2015 there are however other parameters associated with the preprocessing of swat submodels to be considered which may have profound effects on the spatial discretization and resolution watershed and hrus used in the model ray 2018 these preprocessing parameters should be treated as input factors in an sa which then aids in identifying for example minimum percentages of lulc and soil classes in order to eliminate inappropriately small hrus exploring uncertainty in these parameters partially addresses uncertainties in model structure and resolution swat requires input raster datasets of dems lulc soil datasets and vector datasets of meteorological information on monitoring stations and optionally predefined stream networks these datasets provide fundamental information for describing the characteristics of an underlying watershed shen et al 2015 as discussed in section 3 2 dems involve both resolution and random and systematic measurement uncertainties lulc and soil datasets possess resolution and positional uncertainty meteorological information being site specific can have positional uncertainty as well as attribute uncertainty in its measurement importantly measurement uncertainty in both raster and vector datasets should take account of its spatial structure i e spatial autocorrelation although stream networks used in swat do not include attribute information and its related uncertainty the scale and positional uncertainty of a stream network would have significant impacts on the scale and shape of a watershed respectively and should be considered in sa koo et al 2020 the second step is selection of the sa method based on the purposes of the sa and the characteristics of the swat model the purpose of sa can be different for different users but generally swat applications include many uncertainty sources e g hundreds of swat model parameters thus reducing the number of input factors through screening methods is recommended prior to ranking related to the swat characteristics local sa pearson s correlation and linear regression based sa methods are inadequate because input factors in swat are interdependent and generally interact with one other variance based sa methods are often utilized in swat applications zadeh et al 2017 if holistic uncertainty sources in swat are to be evaluated through sa then variance based sa methods may be unsuitable due to their relatively high computational costs razavi and gupta 2015 use of the morris method and rank regression or implementing an emulator instead of swat yang et al 2015 would therefore be recommended the third step is perturbation propagation for individual input factors plausible ranges and assumed probability distributions for swat model parameters can be found in the swat model calibration literature yang et al 2018 abbaspour 2015 as the dem consists of a huge number of grids pixel swapping and a spatial autoregressive model might be undesirable due to their high computational costs thus spatial moving average and sequential gaussian simulation are suggested approaches for propagate perturbations for lulc and soil datasets if their confusion matrices exist using these matrices would provide credible values to propagate although they cannot represent the spatial structure of uncertainty if confusion matrices do not exist simple epsilon bands or other methods for a line feature type could be applicable meteorological information on monitoring stations and predefined stream networks could utilize perturbation propagation for vector datasets the perturbation of position in meteorological information can be propagated using error ellipses and its attributes can be propagated using a spatial autoregressive model positional uncertainty of stream networks can be propagated through simple epsilon bands or other methods for a line feature type if the precision of stream networks is subjected to sa a stream network can be generated directly from a dem using a gis operation and its impact can be evaluated by adjusting a model parameter for the gis operation to designate drainage to a stream network koo et al 2020 the final step is sa evaluation and post processing in this example the morris method and rank regression were the sa methods applied to the swat model in the second step thus the sa result of the morris method could be visualized via scatter plots for absolute means of elementary effects against their standard deviations and rank regression could be represented using scatter plots and regression coefficient plots if spatially varying sa results are of interest for example qois at the outlet of sub watersheds pie charts could be used to display the proportional influence of input factors at different locations see fig 4 a feick and hall 2004 the convergence of the sa measures should additionally be assessed as should any dependence of sensitivity results on the climatic forcing as it changes through the period on interest 8 conclusions this article presents a pragmatic framework for the application of sensitivity analysis sa to a spatially distributed environmental model sd em the suggested framework for sa consists of four general steps potential uncertainty source identification selection of sa method and predictive quantities of interest perturbation propagation and sa evaluation and post processing this framework also provides useful background and general guidance on applying sa to other areas of environmental modeling and related gis communities more specifically the framework can be used to assist in identifying potential uncertainty sources and their corresponding uncertainty classification scheme choosing an appropriate sa method according to the sa purpose s and model characteristics propagating perturbations with plausible ranges and assumptions and verifying sa measures using visualization methods convergence and reliability tests in particular it provides guidance on the assessment and treatment of uncertainty sources related to spatial datasets including positional and attribute uncertainty and on widely utilized perturbation propagation methods that take account of the spatial structure of potential uncertainty in those spatial datasets the framework includes guidance on methods for analyzing sa results involving multiple outputs and their visualization which could offer efficient ways to handle spatially distributed sa measures the framework should therefore be helpful in incorporating the uncertainty of spatial components in sd ems into a general sa process along with the usual model parameters and other input factors that sa commonly evaluates furthermore we expect that model structure uncertainty related to the scales boundaries and discretization of spatial datasets could be addressed through this framework and provide concrete support for further uncertainty analysis declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the primary idea of this manuscript was discussed during two workshops in nanjing may 2019 and canberra december 2019 and the manuscript was drafted through several zoom meetings this work was supported by the key project of nsf of china grant 41930648 the nsf for excellent young scholars of china grant 41622108 national key research and development program of china grant 2017yfb0503500 the australian government research training program agrtp scholarship and a top up scholarship from the anu hilda john endowment fund and the priority academic program development of jiangsu higher education institutions grant 164320h116 
25957,spatio temporal simulations are becoming essential tools for decision makers when forecasting future conditions and evaluating effectiveness of alternative decision scenarios however lack of interactive steering capabilities limits the value of advanced stochastic simulations for research and practice to address this gap we identified conceptual challenges associated with steering stochastic spatio temporal simulations and developed solutions that better represent the realities of decision making by allowing both reactive and proactive spatially explicit interventions we present our approach in a participatory modeling case study engaging stakeholders in developing strategies to contain the spread of a tree disease in oregon usa using intuitive interfaces implemented through web based and tangible platforms stakeholders explored management options as the simulation progressed spatio temporal steering allowed them to combine currently used management practices into novel adaptive management strategies which were previously difficult to test and assess demonstrating the utility of interactive simulations for decision making graphical abstract image 1 keywords stochastic participatory modeling adaptive management computational steering forecasting disease spread modeling 1 introduction spatio temporal simulations provide a powerful way to study complex spatial phenomena develop spatial theories and even forecast the future especially when traditional experimental methods to reveal patterns and processes are difficult or impossible to implement sullivan and perry 2013 accordingly substantial research efforts have been devoted to developing dynamic spatio temporal models of large scale socio ecological phenomena such as biological invasions meentemeyer et al 2011 miller et al 2017 or sustainable urban growth meentemeyer et al 2013 these models are particularly useful for simulating the efficacy of interventions such as strategies to curb the spread of invasive species which may have delayed impacts cost too much or become controversial garner and hamilton 2011 given the complexity of socio environmental problems researchers increasingly use participatory methods to incorporate diverse stakeholder perspectives into problem solving participatory modeling has been shown to help researchers develop relevant questions construct better models and generate solutions that can be easily translated into decisions voinov and bousquet 2010 spatio temporal simulations have proven effective in participatory modeling studies dealing with land use lagabrielle et al 2010 flood hazards becu et al 2017 and disease spread hossard et al 2013 gaydos et al 2019 but there is still a need to better integrate these models into the decision making process vukomanovic et al 2019 gaydos et al 2019 decision support poses a new challenge to modelers requiring them to make models more interactive and reflective of the realities of decision making most spatio temporal simulations are not interactive i e they are initialized with a set of inputs that cannot be adjusted while the simulation is running such a non interactive workflow pairs well with monte carlo techniques that allow researchers to capture uncertainties associated with stochastic models and model ensembles and to run calibration or sensitivity analyses by simulating large numbers of model realizations yang 2011 rubinstein and kroese 2016 however a non interactive simulation can obscure cause effect relationships and is impossible to adjust in response to new information or to its own intermediate results moreover most spatio temporal models do not have interactive visual interfaces which are known to facilitate communication of results and their uncertainties as well as help elicit user input voinov et al 2016 given decision makers need to quickly explore interventions and their consequences across space and time these model limitations can exacerbate the knowledge practice gap a common challenge in modeling wherein model insights do not directly inform actionable on the ground decisions voinov et al 2016 cunniffe et al 2015 outside of a participatory modeling context interactive modeling has been studied in computer science and related disciplines for several decades mccormick et al 1987 computational steering refers to a mechanism for interactively controlling the variables of a simulation as the computation is in progress and is often used to better understand parameter space and simulation behavior mulder et al 1999 matkovic et al 2008 in addition to efficiency computational steering also improves communication and discussion by providing immediate visual representation of the model and results van wijk et al 1997 computational steering has been used to advance research in a variety of fields including atmospheric and weather science physics and medical research dynamics jean et al 1995 walker et al 2007 johnson and parker 1995 and has proved especially important in computational fluid dynamics simulations marshall et al 1990 wright and hargreaves 2013 additionally certain agent based modeling frameworks provide a form of computational steering for model exploration rossiter 2015 cordasco et al 2013 or simulation coupling jaxa rozen et al 2019 steering can open up new possibilities to explore geospatial what if questions collaboratively with stakeholders although the term steering can be used in participatory modeling literature to mean interactive adjustments of key input model variables niño ruiz et al 2013 voinov et al 2016 we are specifically concerned here with spatio temporal steering i e allowing users to spatially intervene at any step of the simulation this type of steering can be critical for strategizing the management of dynamic systems computational steering is one of several possible implementations of spatio temporal steering some researchers have demonstrated how with the help of interactive environments computational steering can help explore complex spatio temporal decision space the prime example is world lines waser et al 2010 ribičić et al 2013 which combines computational steering of a flooding simulation with versatile interactive scenario visualization waser et al 2010 demonstrated the approach with a levee breach scenario exploring possible methods for closing the breach by simulating the strategic positioning of sandbags in different spatial configurations another example of what if scenario modeling was presented by afzal et al 2011 in the context of infectious disease modeling these authors developed a decision support environment on top of a mathematical epidemiological spread model to interactively evaluate scenarios with different mitigating measures afzal et al 2011 despite general agreement about the advantages of computationally steering simulations this methodology is still the exception rather than the rule especially outside of computer science pickles et al 2005 because there are several barriers to its broader usage one is the increased technological complexity of model implementation leading to high code maintenance costs and possibly more error prone code another is a lack of user friendly interfaces that facilitate steering for users with different technical backgrounds furthermore high performance computing platforms typically associated with computational steering often lack the necessary visualization capabilities and interactivity technological advances such as gpu computing allowed researchers to make many simulations more interactive and accessible through desktop interfaces linxweiler et al 2010 afzal et al 2011 ko et al 2014 however the increased need to provide simulation steering capabilities to analysts and stakeholders has necessitated the use of web based solutions deodhar et al 2014 shashidharan et al 2017 and alternative interfaces offering more natural user interactions e g virtual reality environments mulder et al 1998 wenisch et al 2005 or touch table and tangible interfaces mittelstädt et al 2013 tonini et al 2017 spatio temporal steering also poses conceptual challenges when dealing with stochastic models given that there are multiple realizations of a simulation running at the same time for a stochastic model it is not obvious which realization to use to make steering decisions visualizing several stochastic runs using an aggregate representation such as a probability or an average of model results ribičić et al 2013 can inform users about the potential range of outcomes however real world decisions are based on observations best represented as a single stochastic run applying steering to stochastic spatio temporal simulations is therefore challenging to inform strategies used in adaptive management which bases decisions on evaluation of past actions current observations and future forecasting we encountered these challenges when designing a participatory modeling workshop focused on the spread of an invasive forest disease sudden oak death sod in oregon sod spread poses serious environmental and economic risks but because treatments are costly at large scales decision makers must strategically target treatments across time and space cunniffe et al 2016 during a prior participatory modeling workshop we conducted gaydos et al 2019 stakeholders expressed the need to explore yearly treatment interventions which led us to incorporate spatio temporal steering into our modeling framework in this paper we detail how we overcame several challenges associated with steering a stochastic simulation and identify three conceptual approaches to spatio temporal steering in a participatory modeling context we present a novel adaptive management approach that better represents the realities of decision making by allowing both reactive and proactive spatially explicit interventions we also suggest simpler alternative ways to design steerable simulations that do not require the implementation of computational steering to reduce associated technological complexity the paper is structured as follows section 2 identifies several conceptual and implementation challenges associated with steering of spatio temporal simulations and develops methods to address them in section 3 we apply the methods in an epidemiological simulation and describe our steering implementation and interfaces developed for our participatory modeling case study using this case study section 4 demonstrates how workshop participants applied the novel adaptive management approach to interact with the simulation and develop relevant management scenarios sections 5 and 6 highlight the importance of using the adaptive management approach during the workshop and discuss the limitations and future work 2 methods 2 1 steering stochastic simulations representational and conceptual challenges accompany any attempt to steer many stochastic model realizations or a model ensemble to condense the spatial information from all independent runs aggregate renderings are typically used ribičić et al 2013 spatial results are aggregated using an aggregation operator returning a single value for each spatial unit such as mean minimum maximum standard deviation or count in this way modelers can obtain for example a probability map of infection or maximum height of flooding such aggregate views however are not always suitable as they tend to hide the patterns and behavior of an individual simulation run for example an infection or fire can jump over the unaffected areas of a landscape but such rare events may not be captured in the aggregate similarly an urban growth simulation can create patches of new development with distinct sizes and shapes that are not represented in the aggregate in these cases the aggregate view can confound understanding of results and even distort expectations of future events fig 1 another challenge of steering stochastic simulations involves selecting the run s to use when exploring scenarios for real world decision making although each steering decision acts on all of the parallel stochastic runs in the same way it can result in vastly different consequences depending on the run for example in the case of a spread simulation disease fire etc performing a spatial intervention on a landscape e g treating infected areas or creating firebreaks might be effective only for certain stochastic runs and not have any effect on the rest of the runs based on the different questions one can answer by steering stochastic spatio temporal simulations we distinguish three different approaches to steering a the single realization approach b the aggregation approach and c the adaptive management approach fig 2 2 1 1 single realization approach the single realization approach is very similar to steering a deterministic model with no uncertainty in initial conditions as the user interacts with only one stochastic run the run can be selected from a pool of stochastic runs based on certain criteria e g average characteristics of the modeled phenomena once an intervention is made a new set of stochastic runs branches out of the current state while the runs that were not selected do not continue fig 2a this approach enables examination of model behavior and patterns without obscuring them by aggregation fig 1a at the same time it allows users to avoid runs with extreme or atypical characteristics that could confuse and hinder understanding of cause and effect relationships conversely it allows a user to purposefully select an extreme run to better understand worst case scenarios the limitation of run selection is that it needs to be done early during the simulation when all runs are very similar alternatively one stochastic run can be selected randomly while the rest of the runs are still computed and aggregated to provide statistical summaries of all the runs 2 1 2 aggregation approach with the aggregation approach the simulation starts from initial conditions and steering is performed based on aggregated views of simulation steps typically a probability map would be used to show the frequency of the modeled phenomenon across multiple stochastic runs but other statistically derived values could be used as well an intervention is applied at a selected simulation step to all stochastic runs this approach helps to identify how a set of interventions impacts the probabilistic distribution of a modeled phenomenon fig 2b however as mentioned earlier certain typically spatial interventions might not affect some of the individual simulation runs and therefore do not reflect real world decisions for example in the case of forecasting disease spread several years in the future a treatment applied in a later year of the simulation and selected based on the probability surface would not spatially overlap with the infected areas simulated by a portion of the stochastic runs real management decisions always take into account the latest available information about the observed infections and so the aggregate approach may not be helpful on the other hand this approach is suitable for making more immediate decisions about future interventions because it allows exploring which spatio temporal configurations of interventions are likely to be most effective given current data and known uncertainties for example in the context of urban planning one can ask questions such as which land should a rapidly growing city prioritize purchasing in order to build future park infrastructure given projected development patterns and a limited annual budget ultimately one needs to recognize the limitations of this approach for adaptive management and be cautious when applying it and interpreting the results 2 1 3 adaptive management approach the adaptive management approach combines the previous two approaches to make simulation steering more closely model real world decision making adaptive management iteratively takes into account past actions and future probabilities introducing the concept of past observations and future estimates to the modeling in an adaptive management approach we can pause the simulation at any time step and go back to a previous step representing our past visualized as a single stochastic realization or go forward into our future visualized as an aggregate of multiple runs based on the present conditions as we move forward in the simulation a single run representing the current reality is selected from the multiple runs representing the future the single run can be selected randomly or specifically chosen to represent average characteristics among the set of all runs going back to add or change an intervention makes a previous step become the current reality and subsequent steps represent future estimates that take the new interventions into account fig 2c rather than pinpoint the best time and location of future interventions the adaptive management approach allows testing different strategies for example in the context of plant disease spread one can answer questions such as are we able to eradicate or slow down a disease with a given yearly budget would front loading our budget i e spending a majority of the budget in the first year lead to eradication is it better to focus on disease hotspots or isolated outbreaks do we need to treat the same places every year what type of interventions would be most effective 2 2 steering implementation approaches instrumenting a simulation to enable computational steering can be a challenging task often leading to separate implementations of that simulation for batch processing and for steering typically the structure of the code needs to be adjusted to allow stopping and starting the simulation at any step additionally the simulation needs to understand a certain communication protocol that controls the progress and has to maintain consistency in internal data structures throughout the simulation as simulations are rarely developed with those needs in mind the necessary restructuring of the simulation code can be difficult and can introduce errors to previously well tested code moreover the technological complexity can increase when simulations need to be deployed into cloud environments due to these challenges we suggest considering alternative approaches to computational steering fig 3 a these approaches are less complex to implement but come at a cost of reduced computational efficiency one such alternative to computational steering is an approach in which the simulation can save its complete state terminate and restart from the same step initialized with the saved state an intervention is then incorporated by terminating the simulation modifying the saved state and restarting the simulation from the same step using the modified state fig 3b in this way already computed steps are not repeated but the user can still step back providing all of the simulations states are kept the advantage of this approach is that reading and writing data are usually already part of a simulation and are possible to implement for any additional variables moreover the structure of the code can remain intact the downside is that the simulation needs to repeatedly read data possibly large spatial data allocate resources and write outputs which can take significant time it can be particularly relevant for this approach to require a steering step be longer than a simulation step i e the simulation can be steered only every n simulation steps practically that reduces the necessary read and write operations and it can simplify the user interaction as long as the steering step is carefully selected to reflect user s needs for intervention in cases when the simulation state is defined by many variables or complex structures but still runs fast enough another alternative approach is to provide as input to the starting simulation a time series of interventions and the corresponding time steps fig 3c when a steering intervention happens the simulation is restarted from the beginning with different input while acting as if the simulation just continued from the intervention the obvious drawback is the need to recompute everything from the start despite the inefficient use of computational resources this approach can be practical when the number of steering steps is small 2 2 1 steering manager to accommodate the variability in models steering implementations and interfaces we employed a generalized steering architecture based on a client server model in which a steering interface communicates with the simulation indirectly through a steering manager the purpose of the steering manager a server is to relay instructions and data between a steering interface client and a simulation client without the interface or the simulation knowing any specific details about each other the advantage of this approach is reduced complexity of the interface and simulation code and greater flexibility in combining different components for example the steering manager can instruct multiple simulations at the same time each simulating a certain aspect of a modeled phenomenon alternatively a simulation can be steered by multiple interfaces e g steered through a tangible interface in a participatory workshop setting and at the same time through a web based interface by remote workshop participants fig 4 additionally this architecture allows for the alternative steering implementations described above the steering manager can hide the particular implementation approach so that the steering interface works the same way regardless of the steering implementation 3 application for epidemiological simulation we developed and applied the steering concepts described above by augmenting an existing epidemiological model for forecasting the spread of plant diseases based on stakeholders feedback from a participatory workshop we implemented the adaptive management approach allowing them to steer the simulation by managing the disease yearly instead of only at the beginning of the simulation we adapted an existing tangible user interface and a web interface to allow users to test realistic management strategies the following sections describe the epidemiological simulation and the interfaces including relevant technical details of the steering implementation 3 1 epidemiological simulation we used the pest or pathogen spread pops simulation jones et al 2019 to forecast sod dispersal in oregon gaydos et al 2019 pops is a stochastic spatially explicit susceptible infected simulation that uses host distribution seasonality and weather patterns to project the spread of biological invasions stochastic components include pest reproduction dispersal and invasion probability the open source pops library written in modern c language has both r and grass gis interfaces jones et al 2019 and can run several stochastic simulations in parallel to leverage current computing infrastructures in close collaboration with stakeholders we configured the pops model to reflect observed epidemiological processes and historical patterns of sod spread in oregon gaydos et al 2019 gaydos 2020 spread is simulated weekly and aggregated to yearly visual outputs representing a single stochastic iteration fig 1b and the probability of infection over multiple iterations fig 1a users develop management scenarios by specifying spatial treatment polygons that alter the density or susceptibility of hosts treatments are designated yearly to match the timescale of decision making 3 2 steering interfaces we developed two steering interfaces to facilitate stakeholder use of the pops simulation a web based interface pops forecasting platform and a tangible interface pops tangible landscape both interfaces allow users to intuitively run the forecast visualize disease outcomes both spatially and as summary statistics and apply yearly interventions while effectively hiding the technical complexities of steering however differences in system architecture require different steering implementations as described in sec 3 3 we present these two alternatives to demonstrate how steering approaches can be tailored to different system requirements while providing similar adaptive management capabilities for end users the pops forecasting platform fig 5 is an online interface that leverages cloud computing to streamline plant pest risk assessments jones et al 2020 on the backend a django framework links a centralized database with a dockerized cloud environment running the pops simulation using the r interface this framework allows the simulation to access new data inputs such as user generated interventions and store the resulting disease outcomes into the database from where they can be accessed and visualized via the forecasting platform front end in this way the forecasting platform also serves as a repository of scenarios to allow easy comparison of simulation outcomes using dynamic spread maps and summary charts the pops tangible landscape interface fig 6 was designed with similar interactive capabilities but a substantially different system architecture tangible landscape is a type of tangible interface that uses physical objects to interact with a simulation petrasova et al 2018 it is thought that tangible platforms may be more intuitive for less tech savvy users and for group exploration of simulations gaydos et al 2019 gaydos 2020 the main components of the system include the physical setup projection augmented physical model interaction controls and a steering dashboard fig 6 the physical setup consists of a projector an rgb d scanner and a computer with pops tangible landscape software built around grass gis an open source geospatial analysis and modeling platform here the communication between the python based tangible interface and the c grass gis interface to pops simulation is controlled by the steering manager component written in python despite differences the pops forecasting platform and pops tangible landscape interfaces provide complementary functionality for stakeholders the display year toggle in the forecasting platform and the clicker in tangible landscape allow users to step through simulation years to visualize disease projections and select when to apply treatments users design intervention strategies by arranging treatment polygons either free form or predefined shapes on the landscape in tangible landscape these treatments are applied by placing felt indicators on the 3d projection augmented physical model fig 6 while in the forecasting platform this same functionality is achieved by drawing treatment polygons on the dynamic web map fig 5 as treatments are placed users are given instant feedback on cost and area of management to help plan their strategies control buttons physical usb buttons in tangible landscape or widget buttons in the forecasting platform allow users to start and progress through the simulation tangible landscape is linked to forecasting platform in order to store the scenarios created with the tangible interface and compare them using different metrics 3 3 steering implementation in order to enable dynamic implementation of treatments in different years of the simulation we integrated spatio temporal steering capabilities into our modeling platform given that different technologies are used in the back end of the tangible and web based interfaces we implemented spatio temporal steering using approaches best suited to each prototype when customizing tangible landscape and the underlying grass gis interface to pops library we implemented the full computational steering approach fig 3a in this case the combination of programming languages used by tangible landscape grass gis and pops c c and python is well suited for computational steering implementation more specifically we used python and c implementation of sockets to build a custom communication protocol to control the progress of the simulation including pausing resuming going back or running a single step of the simulation through this communication protocol the simulation is also informed when new treatments need to be loaded into current data structures since disease treatments are conducted yearly we selected the steering step the step when interventions can be performed to be one year and thus different from the shorter simulation step one week another important aspect of instrumenting the model a checkpointing mechanism allows going back to previous steps of the simulation by saving the states of the simulation similarly to how a person can go back and forward when navigating websites in a browser thanks to this mechanism users can decide to go back in time in the simulation and test different treatments or the same ones at a different time finally to simulate adaptive management as described in section 2 1 3 the simulation can synchronize the infected and susceptible host data among the multiple runs so that after synchronization all of the stochastic simulation runs start from the same state but progress individually each with different random number generator seed the run to which all other runs are synchronized is the one with the median value of the sum of infected hosts this selection ensures that extreme cases aren t selected but this criterion can be easily adjusted if more extreme scenarios are actually desirable to maintain the sustainability of the model code we have a single code base allowing us to use the model with or without computational steering without computational steering the treatments can still be provided for specific years and thus used for scenario modeling without the need for a special steering interface given the higher complexity of the web technologies used in pops forecasting platform we chose a simpler steering approach that does not require direct communication with the simulation described in fig 3b after each step we end the simulation output the raster layers of infected and susceptible hosts and restart the simulation for the following step with the input consisting of the output from the previous step and any treatments for the following step this approach supports all of the steering mechanisms described above including controlling the progress checkpointing and synchronization they are simply implemented outside of the simulation the disadvantage of this solution is the extra time spent writing intermediate data to disk and loading it back to memory as apparent from fig 3b although a part of this output data is used for visualization of the intermediate results a large portion of it serves only to save the simulation steps and would not be exported in cases of computational steering given our need to compare the outcomes of different treatment strategies we run the simulation with each new set of treatments until the end step of the simulation and then it is paused using the checkpointing mechanism we can then go back and resume the simulation from any point having such a tree structure of end results for all decisions allows us to easily compare the outcomes and better understand the effect of individual treatments additionally users can see estimates of multiple alternative futures and use them to inform their decisions all of the results are saved in pops forecasting platform database and can be reloaded on the dashboard to review them 4 case study we applied our modeling framework with stakeholders in oregon who must make decisions regarding the management of sudden oak death sod an emerging forest disease that has killed millions of trees in california and oregon based on their feedback from a prior workshop gaydos et al 2019 we extended our epidemiological model to enable them to spatio temporally steer the simulation by managing the disease at yearly intervals rather than managing only at the beginning of the simulation during a full day participatory workshop we introduced stakeholders to this novel adaptive management modeling approach using pops tangible landscape and pops forecasting platform fig 7 stakeholders were to work in groups to collaboratively develop management strategies to make the strategies relevant for stakeholders and their organizations funding cycles we simulated the disease for 3 years starting with known 2019 infections to account for delays in detection and treatment of the disease participants designed their treatments in the beginning of each year but the treatments were not applied until the end of that year we applied the adaptive management approach to steering sec 2 1 3 and simulated 10 stochastic realizations which were sufficient to communicate uncertainty while avoiding time delays more details about the workshop itself can be found in gaydos 2020 4 1 adaptive management workflow to better convey how adaptive management was incorporated into our participatory modeling exercise we describe here the steering workflow using one example of how workshop participants progressed through the simulation fig 8 this particular scenario was created by participants using pops tangible landscape by switching among the displayed years participants explored the probability of infection which would rapidly increase within three years without any management fig 8a using pops forecasting platform they then created a new scenario by specifying various settings including budget constraint or treatment efficiency to reduce the forecasted infection probability participants decided to split their 2019 budget between treating outbreaks in northern parts of the landscape and treating some of the core infected areas in the south fig 8 b 2019 they designated the treatments fig 7 and adjusted them based on real time feedback on the treatments size and cost when participants pressed the usb button the treatment was registered in the database and the simulation ran from that year until the end year 2021 taking into consideration the new treatments at that point the participants moved within the scenario to the beginning of 2020 and a newly simulated infection layer representing one stochastic realization for that year was displayed despite the treatment of most 2019 infected areas the disease escaped and spread northward fig 8 b 2020 this spread occurred due to the delay between the treatment decision at the beginning of 2019 and actual management happening at the end of 2019 representing realistic delays between detection and taking action taking into account the future probability of spread extent fig 8 b 2021 and b 2022 and the ineffectiveness of small buffers to completely prevent outbreaks as in 2019 participants decided to treat the remaining infections and allocate half of their 2020 budget to treat larger areas in the north that were likely to become infected later fig 8 c 2020 at the beginning of 2021 all northward infection was successfully eradicated and the remaining infection was greatly reduced fig 8 c 2021 in the last simulated year participants were able to spend a much smaller budget to treat the few remaining infections fig 8 d 2021 although the beginning of year 2022 the end of the simulation showed a few remaining infections fig 8 d 2022 these could be easily treated by future management efforts this example demonstrates that allowing participants to manage throughout the simulation is essential for representing adaptive management decisions although a disease spread forecast based solely on initial treatment fig 8b is useful in planning there are limits to using it to explore strategies without the option to treat in subsequent years participants would not be able to test realistic management strategies such as how much budget to allocate in different years when and where to treat given the spatial distribution of the infection or whether it is even possible to eradicate the disease given a realistic budget 5 discussion given the large uncertainties associated with forecasting social ecological phenomena researchers have been advocating for management experiments that could both develop scientific knowledge and lead to improved management policies and practices serrouya et al 2019 adaptive management has been shown to accomplish both through continuous learning by doing that takes into account the outcomes of previous strategies and models system behavior using updated knowledge walters and holling 1990 given that successful implementation of this approach requires effective communication across science policy and management bosch et al 2003 participatory modeling has been used to engage stakeholders in learning model development and creating management strategies lynam et al 2010 fujitani et al 2017 smith et al 2007 however participatory modeling efforts have been lacking in methods that allow stakeholders to explore how different spatially and temporally explicit interventions may impact a complex social ecological system in our work we demonstrated how using simulation steering allows stakeholders to explore more realistic decisions through adopting an adaptive management approach as aforementioned this work was motivated by the needs of stakeholders voiced at an initial participatory modeling session gaydos et al 2019 at which stakeholders could manage the infection only in the initial year of the simulation that exercise helped them to learn about disease spread behavior and strategies to reduce spread but also revealed that the disease cannot be contained without continued yearly interventions leading us to integrate adaptive management capabilities into the modeling framework the new approach allowed stakeholders to ask more questions about their ability to eradicate the disease how much of a yearly budget to allocate and which areas should be treated first at the workshop described here participants used the new approach to for example experiment with front loading the budget to see whether that would be more effective to eradicate the disease fig 8 as an analysis of the workshop reports gaydos 2020 the adaptive management approach empowered stakeholders to generate novel intervention scenarios and participants combined several tactics and changed their approaches through time which may better reflect the realistic complexities of control workshop participants gave good usability scores to both interfaces pops tangible landscape and pops forecasting platform and perceived them as useful for targeting treatment areas gaydos 2020 these results show us that using sophisticated realistic steerable spatio temporal simulations does not have to be difficult for stakeholders without modeling expertise indeed studies developing interactive visual interfaces affirm that complexities of the model and the steering implementation can be hidden behind intuitive visual interfaces that expose only information and behavior that are meaningful and practical for the stakeholders afzal et al 2011 ribičić et al 2013 niño ruiz et al 2017 in our case both interfaces provided the stakeholders the same steering capabilities despite featuring different implementations of steering due to the practicalities of their different web vs desktop deployments although the behavior of the simulation is the same pops forecasting platform currently requires more time to progress through each steering step that is to be expected given the steering implementation fig 3b but we are currently working on minimizing the delay by using better web deployment strategies nevertheless this delay did not negatively impact the workshop as participants used that time for more discussion to help stakeholders develop and evaluate more scenarios in the allotted time we decided to limit their ability to go back in time and change past interventions although this prevented them from fine tuning individual scenarios they managed to test a wider range of strategies the option to change past treatments to fine tune approaches is likely more important for analysts working with pops forecasting platform to finalize treatment plans although considerable research has been devoted to overcoming the technological challenges of computational steering the conceptual challenges of steering stochastic simulations have been overlooked the common aggregation approach ribičić et al 2013 fig 1a is limited in its ability to represent spatial structure and patterns explicitly modeled by many urban growth disease or wildfire simulations moreover this approach is problematic when differences in the intermediate states of multiple stochastic realizations would result in significantly different steering decisions the developed steering approaches single realization aggregation and adaptive management approach provide a framework to address these challenges by better defining the role of each method in modeling and the associated implications for interpreting results since this work focused on the adaptive management approach future work could further investigate the applicability and limitation of each of the developed methods for different spatio temporal simulations such as urban growth flooding based on their specific behavior and questions researchers and decision makers are interested in answering additionally a possible avenue to mitigate the limitation associated with using single stochastic realization in the adaptive management approach can be using a more sophisticated mechanism for selecting the single run to mitigate any concerns about representing a likely reality the run could be selected by weighing several of its characteristics including e g its size pattern or value range while at the same time the more extreme runs can be visualized alongside to provide a more complete picture of the stochastic variations to facilitate further research we envision developing a geospatial library with a simple steering interface that would streamline integrating spatio temporal steering into current and new geospatial simulations e g peckham et al 2013 neteler and mitasova 2008 such a library would encourage more interactive modeling in geospatial research making the research process and results more accessible to domain experts and decision makers 6 conclusions in this study we addressed the often overlooked conceptual and implementation challenges of steering stochastic spatio temporal simulations our suggested approach combining aggregate views of future estimates with a single realization representing the past provides a novel solution to steering multiple stochastic realizations and one that is particularly applicable for testing adaptive management strategies the adaptive management modeling approach developed to help stakeholders design more realistic management scenarios can better represent the realities of decision making by allowing stakeholders to decide when and where they manage based on past actions current observations and future forecasting the participatory modeling case study demonstrated that the stakeholders were able to ask better more realistic questions about the feasibility of disease eradication budget allocations and management priorities given the large uncertainties typically associated with social ecological phenomena the adaptive management modeling approach focuses attention on developing strategies that decision makers can adopt based on the current on ground situation although our case study highlights the application and importance of spatio temporal steering for disease management the described approach is generally applicable and relevant for a variety of stochastic geospatial models 7 software and data availability this work is based on pops forecasting and control system an open source project composed of several software components under gnu gpl v2 and later c library pops r package rpops c grass gis addon r pops spread pops forecasting platform based django web framework links to github repositories are accessible from osf project osf io q32p9 additionally tangible landscape tangible geospatial modeling and visualization system integrated with grass gis licensed under gnu gpl v2 and later was customized for steering and the code is available under a separate branch on github github com tangible landscape grass tangible landscape tree pops steering data used for sod modeling are accessible from r pops spread tutorial page grasswiki osgeo org wiki sod spread tutorial declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this material was made possible in part by cooperative agreements from the united states department of agriculture s animal and plant health inspection service aphis it may not necessarily express aphis views the authors thank dr megan skrip from the center for geospatial analytics ncsu for her help with language editing and for her valuable comments that improved the manuscript author contributions a p and v p performed the research a p and c j wrote the steering code v p provided programming support d g prepared and led the workshop a p wrote the initial draft a p d g c j v p h m and r m edited the manuscript 
25957,spatio temporal simulations are becoming essential tools for decision makers when forecasting future conditions and evaluating effectiveness of alternative decision scenarios however lack of interactive steering capabilities limits the value of advanced stochastic simulations for research and practice to address this gap we identified conceptual challenges associated with steering stochastic spatio temporal simulations and developed solutions that better represent the realities of decision making by allowing both reactive and proactive spatially explicit interventions we present our approach in a participatory modeling case study engaging stakeholders in developing strategies to contain the spread of a tree disease in oregon usa using intuitive interfaces implemented through web based and tangible platforms stakeholders explored management options as the simulation progressed spatio temporal steering allowed them to combine currently used management practices into novel adaptive management strategies which were previously difficult to test and assess demonstrating the utility of interactive simulations for decision making graphical abstract image 1 keywords stochastic participatory modeling adaptive management computational steering forecasting disease spread modeling 1 introduction spatio temporal simulations provide a powerful way to study complex spatial phenomena develop spatial theories and even forecast the future especially when traditional experimental methods to reveal patterns and processes are difficult or impossible to implement sullivan and perry 2013 accordingly substantial research efforts have been devoted to developing dynamic spatio temporal models of large scale socio ecological phenomena such as biological invasions meentemeyer et al 2011 miller et al 2017 or sustainable urban growth meentemeyer et al 2013 these models are particularly useful for simulating the efficacy of interventions such as strategies to curb the spread of invasive species which may have delayed impacts cost too much or become controversial garner and hamilton 2011 given the complexity of socio environmental problems researchers increasingly use participatory methods to incorporate diverse stakeholder perspectives into problem solving participatory modeling has been shown to help researchers develop relevant questions construct better models and generate solutions that can be easily translated into decisions voinov and bousquet 2010 spatio temporal simulations have proven effective in participatory modeling studies dealing with land use lagabrielle et al 2010 flood hazards becu et al 2017 and disease spread hossard et al 2013 gaydos et al 2019 but there is still a need to better integrate these models into the decision making process vukomanovic et al 2019 gaydos et al 2019 decision support poses a new challenge to modelers requiring them to make models more interactive and reflective of the realities of decision making most spatio temporal simulations are not interactive i e they are initialized with a set of inputs that cannot be adjusted while the simulation is running such a non interactive workflow pairs well with monte carlo techniques that allow researchers to capture uncertainties associated with stochastic models and model ensembles and to run calibration or sensitivity analyses by simulating large numbers of model realizations yang 2011 rubinstein and kroese 2016 however a non interactive simulation can obscure cause effect relationships and is impossible to adjust in response to new information or to its own intermediate results moreover most spatio temporal models do not have interactive visual interfaces which are known to facilitate communication of results and their uncertainties as well as help elicit user input voinov et al 2016 given decision makers need to quickly explore interventions and their consequences across space and time these model limitations can exacerbate the knowledge practice gap a common challenge in modeling wherein model insights do not directly inform actionable on the ground decisions voinov et al 2016 cunniffe et al 2015 outside of a participatory modeling context interactive modeling has been studied in computer science and related disciplines for several decades mccormick et al 1987 computational steering refers to a mechanism for interactively controlling the variables of a simulation as the computation is in progress and is often used to better understand parameter space and simulation behavior mulder et al 1999 matkovic et al 2008 in addition to efficiency computational steering also improves communication and discussion by providing immediate visual representation of the model and results van wijk et al 1997 computational steering has been used to advance research in a variety of fields including atmospheric and weather science physics and medical research dynamics jean et al 1995 walker et al 2007 johnson and parker 1995 and has proved especially important in computational fluid dynamics simulations marshall et al 1990 wright and hargreaves 2013 additionally certain agent based modeling frameworks provide a form of computational steering for model exploration rossiter 2015 cordasco et al 2013 or simulation coupling jaxa rozen et al 2019 steering can open up new possibilities to explore geospatial what if questions collaboratively with stakeholders although the term steering can be used in participatory modeling literature to mean interactive adjustments of key input model variables niño ruiz et al 2013 voinov et al 2016 we are specifically concerned here with spatio temporal steering i e allowing users to spatially intervene at any step of the simulation this type of steering can be critical for strategizing the management of dynamic systems computational steering is one of several possible implementations of spatio temporal steering some researchers have demonstrated how with the help of interactive environments computational steering can help explore complex spatio temporal decision space the prime example is world lines waser et al 2010 ribičić et al 2013 which combines computational steering of a flooding simulation with versatile interactive scenario visualization waser et al 2010 demonstrated the approach with a levee breach scenario exploring possible methods for closing the breach by simulating the strategic positioning of sandbags in different spatial configurations another example of what if scenario modeling was presented by afzal et al 2011 in the context of infectious disease modeling these authors developed a decision support environment on top of a mathematical epidemiological spread model to interactively evaluate scenarios with different mitigating measures afzal et al 2011 despite general agreement about the advantages of computationally steering simulations this methodology is still the exception rather than the rule especially outside of computer science pickles et al 2005 because there are several barriers to its broader usage one is the increased technological complexity of model implementation leading to high code maintenance costs and possibly more error prone code another is a lack of user friendly interfaces that facilitate steering for users with different technical backgrounds furthermore high performance computing platforms typically associated with computational steering often lack the necessary visualization capabilities and interactivity technological advances such as gpu computing allowed researchers to make many simulations more interactive and accessible through desktop interfaces linxweiler et al 2010 afzal et al 2011 ko et al 2014 however the increased need to provide simulation steering capabilities to analysts and stakeholders has necessitated the use of web based solutions deodhar et al 2014 shashidharan et al 2017 and alternative interfaces offering more natural user interactions e g virtual reality environments mulder et al 1998 wenisch et al 2005 or touch table and tangible interfaces mittelstädt et al 2013 tonini et al 2017 spatio temporal steering also poses conceptual challenges when dealing with stochastic models given that there are multiple realizations of a simulation running at the same time for a stochastic model it is not obvious which realization to use to make steering decisions visualizing several stochastic runs using an aggregate representation such as a probability or an average of model results ribičić et al 2013 can inform users about the potential range of outcomes however real world decisions are based on observations best represented as a single stochastic run applying steering to stochastic spatio temporal simulations is therefore challenging to inform strategies used in adaptive management which bases decisions on evaluation of past actions current observations and future forecasting we encountered these challenges when designing a participatory modeling workshop focused on the spread of an invasive forest disease sudden oak death sod in oregon sod spread poses serious environmental and economic risks but because treatments are costly at large scales decision makers must strategically target treatments across time and space cunniffe et al 2016 during a prior participatory modeling workshop we conducted gaydos et al 2019 stakeholders expressed the need to explore yearly treatment interventions which led us to incorporate spatio temporal steering into our modeling framework in this paper we detail how we overcame several challenges associated with steering a stochastic simulation and identify three conceptual approaches to spatio temporal steering in a participatory modeling context we present a novel adaptive management approach that better represents the realities of decision making by allowing both reactive and proactive spatially explicit interventions we also suggest simpler alternative ways to design steerable simulations that do not require the implementation of computational steering to reduce associated technological complexity the paper is structured as follows section 2 identifies several conceptual and implementation challenges associated with steering of spatio temporal simulations and develops methods to address them in section 3 we apply the methods in an epidemiological simulation and describe our steering implementation and interfaces developed for our participatory modeling case study using this case study section 4 demonstrates how workshop participants applied the novel adaptive management approach to interact with the simulation and develop relevant management scenarios sections 5 and 6 highlight the importance of using the adaptive management approach during the workshop and discuss the limitations and future work 2 methods 2 1 steering stochastic simulations representational and conceptual challenges accompany any attempt to steer many stochastic model realizations or a model ensemble to condense the spatial information from all independent runs aggregate renderings are typically used ribičić et al 2013 spatial results are aggregated using an aggregation operator returning a single value for each spatial unit such as mean minimum maximum standard deviation or count in this way modelers can obtain for example a probability map of infection or maximum height of flooding such aggregate views however are not always suitable as they tend to hide the patterns and behavior of an individual simulation run for example an infection or fire can jump over the unaffected areas of a landscape but such rare events may not be captured in the aggregate similarly an urban growth simulation can create patches of new development with distinct sizes and shapes that are not represented in the aggregate in these cases the aggregate view can confound understanding of results and even distort expectations of future events fig 1 another challenge of steering stochastic simulations involves selecting the run s to use when exploring scenarios for real world decision making although each steering decision acts on all of the parallel stochastic runs in the same way it can result in vastly different consequences depending on the run for example in the case of a spread simulation disease fire etc performing a spatial intervention on a landscape e g treating infected areas or creating firebreaks might be effective only for certain stochastic runs and not have any effect on the rest of the runs based on the different questions one can answer by steering stochastic spatio temporal simulations we distinguish three different approaches to steering a the single realization approach b the aggregation approach and c the adaptive management approach fig 2 2 1 1 single realization approach the single realization approach is very similar to steering a deterministic model with no uncertainty in initial conditions as the user interacts with only one stochastic run the run can be selected from a pool of stochastic runs based on certain criteria e g average characteristics of the modeled phenomena once an intervention is made a new set of stochastic runs branches out of the current state while the runs that were not selected do not continue fig 2a this approach enables examination of model behavior and patterns without obscuring them by aggregation fig 1a at the same time it allows users to avoid runs with extreme or atypical characteristics that could confuse and hinder understanding of cause and effect relationships conversely it allows a user to purposefully select an extreme run to better understand worst case scenarios the limitation of run selection is that it needs to be done early during the simulation when all runs are very similar alternatively one stochastic run can be selected randomly while the rest of the runs are still computed and aggregated to provide statistical summaries of all the runs 2 1 2 aggregation approach with the aggregation approach the simulation starts from initial conditions and steering is performed based on aggregated views of simulation steps typically a probability map would be used to show the frequency of the modeled phenomenon across multiple stochastic runs but other statistically derived values could be used as well an intervention is applied at a selected simulation step to all stochastic runs this approach helps to identify how a set of interventions impacts the probabilistic distribution of a modeled phenomenon fig 2b however as mentioned earlier certain typically spatial interventions might not affect some of the individual simulation runs and therefore do not reflect real world decisions for example in the case of forecasting disease spread several years in the future a treatment applied in a later year of the simulation and selected based on the probability surface would not spatially overlap with the infected areas simulated by a portion of the stochastic runs real management decisions always take into account the latest available information about the observed infections and so the aggregate approach may not be helpful on the other hand this approach is suitable for making more immediate decisions about future interventions because it allows exploring which spatio temporal configurations of interventions are likely to be most effective given current data and known uncertainties for example in the context of urban planning one can ask questions such as which land should a rapidly growing city prioritize purchasing in order to build future park infrastructure given projected development patterns and a limited annual budget ultimately one needs to recognize the limitations of this approach for adaptive management and be cautious when applying it and interpreting the results 2 1 3 adaptive management approach the adaptive management approach combines the previous two approaches to make simulation steering more closely model real world decision making adaptive management iteratively takes into account past actions and future probabilities introducing the concept of past observations and future estimates to the modeling in an adaptive management approach we can pause the simulation at any time step and go back to a previous step representing our past visualized as a single stochastic realization or go forward into our future visualized as an aggregate of multiple runs based on the present conditions as we move forward in the simulation a single run representing the current reality is selected from the multiple runs representing the future the single run can be selected randomly or specifically chosen to represent average characteristics among the set of all runs going back to add or change an intervention makes a previous step become the current reality and subsequent steps represent future estimates that take the new interventions into account fig 2c rather than pinpoint the best time and location of future interventions the adaptive management approach allows testing different strategies for example in the context of plant disease spread one can answer questions such as are we able to eradicate or slow down a disease with a given yearly budget would front loading our budget i e spending a majority of the budget in the first year lead to eradication is it better to focus on disease hotspots or isolated outbreaks do we need to treat the same places every year what type of interventions would be most effective 2 2 steering implementation approaches instrumenting a simulation to enable computational steering can be a challenging task often leading to separate implementations of that simulation for batch processing and for steering typically the structure of the code needs to be adjusted to allow stopping and starting the simulation at any step additionally the simulation needs to understand a certain communication protocol that controls the progress and has to maintain consistency in internal data structures throughout the simulation as simulations are rarely developed with those needs in mind the necessary restructuring of the simulation code can be difficult and can introduce errors to previously well tested code moreover the technological complexity can increase when simulations need to be deployed into cloud environments due to these challenges we suggest considering alternative approaches to computational steering fig 3 a these approaches are less complex to implement but come at a cost of reduced computational efficiency one such alternative to computational steering is an approach in which the simulation can save its complete state terminate and restart from the same step initialized with the saved state an intervention is then incorporated by terminating the simulation modifying the saved state and restarting the simulation from the same step using the modified state fig 3b in this way already computed steps are not repeated but the user can still step back providing all of the simulations states are kept the advantage of this approach is that reading and writing data are usually already part of a simulation and are possible to implement for any additional variables moreover the structure of the code can remain intact the downside is that the simulation needs to repeatedly read data possibly large spatial data allocate resources and write outputs which can take significant time it can be particularly relevant for this approach to require a steering step be longer than a simulation step i e the simulation can be steered only every n simulation steps practically that reduces the necessary read and write operations and it can simplify the user interaction as long as the steering step is carefully selected to reflect user s needs for intervention in cases when the simulation state is defined by many variables or complex structures but still runs fast enough another alternative approach is to provide as input to the starting simulation a time series of interventions and the corresponding time steps fig 3c when a steering intervention happens the simulation is restarted from the beginning with different input while acting as if the simulation just continued from the intervention the obvious drawback is the need to recompute everything from the start despite the inefficient use of computational resources this approach can be practical when the number of steering steps is small 2 2 1 steering manager to accommodate the variability in models steering implementations and interfaces we employed a generalized steering architecture based on a client server model in which a steering interface communicates with the simulation indirectly through a steering manager the purpose of the steering manager a server is to relay instructions and data between a steering interface client and a simulation client without the interface or the simulation knowing any specific details about each other the advantage of this approach is reduced complexity of the interface and simulation code and greater flexibility in combining different components for example the steering manager can instruct multiple simulations at the same time each simulating a certain aspect of a modeled phenomenon alternatively a simulation can be steered by multiple interfaces e g steered through a tangible interface in a participatory workshop setting and at the same time through a web based interface by remote workshop participants fig 4 additionally this architecture allows for the alternative steering implementations described above the steering manager can hide the particular implementation approach so that the steering interface works the same way regardless of the steering implementation 3 application for epidemiological simulation we developed and applied the steering concepts described above by augmenting an existing epidemiological model for forecasting the spread of plant diseases based on stakeholders feedback from a participatory workshop we implemented the adaptive management approach allowing them to steer the simulation by managing the disease yearly instead of only at the beginning of the simulation we adapted an existing tangible user interface and a web interface to allow users to test realistic management strategies the following sections describe the epidemiological simulation and the interfaces including relevant technical details of the steering implementation 3 1 epidemiological simulation we used the pest or pathogen spread pops simulation jones et al 2019 to forecast sod dispersal in oregon gaydos et al 2019 pops is a stochastic spatially explicit susceptible infected simulation that uses host distribution seasonality and weather patterns to project the spread of biological invasions stochastic components include pest reproduction dispersal and invasion probability the open source pops library written in modern c language has both r and grass gis interfaces jones et al 2019 and can run several stochastic simulations in parallel to leverage current computing infrastructures in close collaboration with stakeholders we configured the pops model to reflect observed epidemiological processes and historical patterns of sod spread in oregon gaydos et al 2019 gaydos 2020 spread is simulated weekly and aggregated to yearly visual outputs representing a single stochastic iteration fig 1b and the probability of infection over multiple iterations fig 1a users develop management scenarios by specifying spatial treatment polygons that alter the density or susceptibility of hosts treatments are designated yearly to match the timescale of decision making 3 2 steering interfaces we developed two steering interfaces to facilitate stakeholder use of the pops simulation a web based interface pops forecasting platform and a tangible interface pops tangible landscape both interfaces allow users to intuitively run the forecast visualize disease outcomes both spatially and as summary statistics and apply yearly interventions while effectively hiding the technical complexities of steering however differences in system architecture require different steering implementations as described in sec 3 3 we present these two alternatives to demonstrate how steering approaches can be tailored to different system requirements while providing similar adaptive management capabilities for end users the pops forecasting platform fig 5 is an online interface that leverages cloud computing to streamline plant pest risk assessments jones et al 2020 on the backend a django framework links a centralized database with a dockerized cloud environment running the pops simulation using the r interface this framework allows the simulation to access new data inputs such as user generated interventions and store the resulting disease outcomes into the database from where they can be accessed and visualized via the forecasting platform front end in this way the forecasting platform also serves as a repository of scenarios to allow easy comparison of simulation outcomes using dynamic spread maps and summary charts the pops tangible landscape interface fig 6 was designed with similar interactive capabilities but a substantially different system architecture tangible landscape is a type of tangible interface that uses physical objects to interact with a simulation petrasova et al 2018 it is thought that tangible platforms may be more intuitive for less tech savvy users and for group exploration of simulations gaydos et al 2019 gaydos 2020 the main components of the system include the physical setup projection augmented physical model interaction controls and a steering dashboard fig 6 the physical setup consists of a projector an rgb d scanner and a computer with pops tangible landscape software built around grass gis an open source geospatial analysis and modeling platform here the communication between the python based tangible interface and the c grass gis interface to pops simulation is controlled by the steering manager component written in python despite differences the pops forecasting platform and pops tangible landscape interfaces provide complementary functionality for stakeholders the display year toggle in the forecasting platform and the clicker in tangible landscape allow users to step through simulation years to visualize disease projections and select when to apply treatments users design intervention strategies by arranging treatment polygons either free form or predefined shapes on the landscape in tangible landscape these treatments are applied by placing felt indicators on the 3d projection augmented physical model fig 6 while in the forecasting platform this same functionality is achieved by drawing treatment polygons on the dynamic web map fig 5 as treatments are placed users are given instant feedback on cost and area of management to help plan their strategies control buttons physical usb buttons in tangible landscape or widget buttons in the forecasting platform allow users to start and progress through the simulation tangible landscape is linked to forecasting platform in order to store the scenarios created with the tangible interface and compare them using different metrics 3 3 steering implementation in order to enable dynamic implementation of treatments in different years of the simulation we integrated spatio temporal steering capabilities into our modeling platform given that different technologies are used in the back end of the tangible and web based interfaces we implemented spatio temporal steering using approaches best suited to each prototype when customizing tangible landscape and the underlying grass gis interface to pops library we implemented the full computational steering approach fig 3a in this case the combination of programming languages used by tangible landscape grass gis and pops c c and python is well suited for computational steering implementation more specifically we used python and c implementation of sockets to build a custom communication protocol to control the progress of the simulation including pausing resuming going back or running a single step of the simulation through this communication protocol the simulation is also informed when new treatments need to be loaded into current data structures since disease treatments are conducted yearly we selected the steering step the step when interventions can be performed to be one year and thus different from the shorter simulation step one week another important aspect of instrumenting the model a checkpointing mechanism allows going back to previous steps of the simulation by saving the states of the simulation similarly to how a person can go back and forward when navigating websites in a browser thanks to this mechanism users can decide to go back in time in the simulation and test different treatments or the same ones at a different time finally to simulate adaptive management as described in section 2 1 3 the simulation can synchronize the infected and susceptible host data among the multiple runs so that after synchronization all of the stochastic simulation runs start from the same state but progress individually each with different random number generator seed the run to which all other runs are synchronized is the one with the median value of the sum of infected hosts this selection ensures that extreme cases aren t selected but this criterion can be easily adjusted if more extreme scenarios are actually desirable to maintain the sustainability of the model code we have a single code base allowing us to use the model with or without computational steering without computational steering the treatments can still be provided for specific years and thus used for scenario modeling without the need for a special steering interface given the higher complexity of the web technologies used in pops forecasting platform we chose a simpler steering approach that does not require direct communication with the simulation described in fig 3b after each step we end the simulation output the raster layers of infected and susceptible hosts and restart the simulation for the following step with the input consisting of the output from the previous step and any treatments for the following step this approach supports all of the steering mechanisms described above including controlling the progress checkpointing and synchronization they are simply implemented outside of the simulation the disadvantage of this solution is the extra time spent writing intermediate data to disk and loading it back to memory as apparent from fig 3b although a part of this output data is used for visualization of the intermediate results a large portion of it serves only to save the simulation steps and would not be exported in cases of computational steering given our need to compare the outcomes of different treatment strategies we run the simulation with each new set of treatments until the end step of the simulation and then it is paused using the checkpointing mechanism we can then go back and resume the simulation from any point having such a tree structure of end results for all decisions allows us to easily compare the outcomes and better understand the effect of individual treatments additionally users can see estimates of multiple alternative futures and use them to inform their decisions all of the results are saved in pops forecasting platform database and can be reloaded on the dashboard to review them 4 case study we applied our modeling framework with stakeholders in oregon who must make decisions regarding the management of sudden oak death sod an emerging forest disease that has killed millions of trees in california and oregon based on their feedback from a prior workshop gaydos et al 2019 we extended our epidemiological model to enable them to spatio temporally steer the simulation by managing the disease at yearly intervals rather than managing only at the beginning of the simulation during a full day participatory workshop we introduced stakeholders to this novel adaptive management modeling approach using pops tangible landscape and pops forecasting platform fig 7 stakeholders were to work in groups to collaboratively develop management strategies to make the strategies relevant for stakeholders and their organizations funding cycles we simulated the disease for 3 years starting with known 2019 infections to account for delays in detection and treatment of the disease participants designed their treatments in the beginning of each year but the treatments were not applied until the end of that year we applied the adaptive management approach to steering sec 2 1 3 and simulated 10 stochastic realizations which were sufficient to communicate uncertainty while avoiding time delays more details about the workshop itself can be found in gaydos 2020 4 1 adaptive management workflow to better convey how adaptive management was incorporated into our participatory modeling exercise we describe here the steering workflow using one example of how workshop participants progressed through the simulation fig 8 this particular scenario was created by participants using pops tangible landscape by switching among the displayed years participants explored the probability of infection which would rapidly increase within three years without any management fig 8a using pops forecasting platform they then created a new scenario by specifying various settings including budget constraint or treatment efficiency to reduce the forecasted infection probability participants decided to split their 2019 budget between treating outbreaks in northern parts of the landscape and treating some of the core infected areas in the south fig 8 b 2019 they designated the treatments fig 7 and adjusted them based on real time feedback on the treatments size and cost when participants pressed the usb button the treatment was registered in the database and the simulation ran from that year until the end year 2021 taking into consideration the new treatments at that point the participants moved within the scenario to the beginning of 2020 and a newly simulated infection layer representing one stochastic realization for that year was displayed despite the treatment of most 2019 infected areas the disease escaped and spread northward fig 8 b 2020 this spread occurred due to the delay between the treatment decision at the beginning of 2019 and actual management happening at the end of 2019 representing realistic delays between detection and taking action taking into account the future probability of spread extent fig 8 b 2021 and b 2022 and the ineffectiveness of small buffers to completely prevent outbreaks as in 2019 participants decided to treat the remaining infections and allocate half of their 2020 budget to treat larger areas in the north that were likely to become infected later fig 8 c 2020 at the beginning of 2021 all northward infection was successfully eradicated and the remaining infection was greatly reduced fig 8 c 2021 in the last simulated year participants were able to spend a much smaller budget to treat the few remaining infections fig 8 d 2021 although the beginning of year 2022 the end of the simulation showed a few remaining infections fig 8 d 2022 these could be easily treated by future management efforts this example demonstrates that allowing participants to manage throughout the simulation is essential for representing adaptive management decisions although a disease spread forecast based solely on initial treatment fig 8b is useful in planning there are limits to using it to explore strategies without the option to treat in subsequent years participants would not be able to test realistic management strategies such as how much budget to allocate in different years when and where to treat given the spatial distribution of the infection or whether it is even possible to eradicate the disease given a realistic budget 5 discussion given the large uncertainties associated with forecasting social ecological phenomena researchers have been advocating for management experiments that could both develop scientific knowledge and lead to improved management policies and practices serrouya et al 2019 adaptive management has been shown to accomplish both through continuous learning by doing that takes into account the outcomes of previous strategies and models system behavior using updated knowledge walters and holling 1990 given that successful implementation of this approach requires effective communication across science policy and management bosch et al 2003 participatory modeling has been used to engage stakeholders in learning model development and creating management strategies lynam et al 2010 fujitani et al 2017 smith et al 2007 however participatory modeling efforts have been lacking in methods that allow stakeholders to explore how different spatially and temporally explicit interventions may impact a complex social ecological system in our work we demonstrated how using simulation steering allows stakeholders to explore more realistic decisions through adopting an adaptive management approach as aforementioned this work was motivated by the needs of stakeholders voiced at an initial participatory modeling session gaydos et al 2019 at which stakeholders could manage the infection only in the initial year of the simulation that exercise helped them to learn about disease spread behavior and strategies to reduce spread but also revealed that the disease cannot be contained without continued yearly interventions leading us to integrate adaptive management capabilities into the modeling framework the new approach allowed stakeholders to ask more questions about their ability to eradicate the disease how much of a yearly budget to allocate and which areas should be treated first at the workshop described here participants used the new approach to for example experiment with front loading the budget to see whether that would be more effective to eradicate the disease fig 8 as an analysis of the workshop reports gaydos 2020 the adaptive management approach empowered stakeholders to generate novel intervention scenarios and participants combined several tactics and changed their approaches through time which may better reflect the realistic complexities of control workshop participants gave good usability scores to both interfaces pops tangible landscape and pops forecasting platform and perceived them as useful for targeting treatment areas gaydos 2020 these results show us that using sophisticated realistic steerable spatio temporal simulations does not have to be difficult for stakeholders without modeling expertise indeed studies developing interactive visual interfaces affirm that complexities of the model and the steering implementation can be hidden behind intuitive visual interfaces that expose only information and behavior that are meaningful and practical for the stakeholders afzal et al 2011 ribičić et al 2013 niño ruiz et al 2017 in our case both interfaces provided the stakeholders the same steering capabilities despite featuring different implementations of steering due to the practicalities of their different web vs desktop deployments although the behavior of the simulation is the same pops forecasting platform currently requires more time to progress through each steering step that is to be expected given the steering implementation fig 3b but we are currently working on minimizing the delay by using better web deployment strategies nevertheless this delay did not negatively impact the workshop as participants used that time for more discussion to help stakeholders develop and evaluate more scenarios in the allotted time we decided to limit their ability to go back in time and change past interventions although this prevented them from fine tuning individual scenarios they managed to test a wider range of strategies the option to change past treatments to fine tune approaches is likely more important for analysts working with pops forecasting platform to finalize treatment plans although considerable research has been devoted to overcoming the technological challenges of computational steering the conceptual challenges of steering stochastic simulations have been overlooked the common aggregation approach ribičić et al 2013 fig 1a is limited in its ability to represent spatial structure and patterns explicitly modeled by many urban growth disease or wildfire simulations moreover this approach is problematic when differences in the intermediate states of multiple stochastic realizations would result in significantly different steering decisions the developed steering approaches single realization aggregation and adaptive management approach provide a framework to address these challenges by better defining the role of each method in modeling and the associated implications for interpreting results since this work focused on the adaptive management approach future work could further investigate the applicability and limitation of each of the developed methods for different spatio temporal simulations such as urban growth flooding based on their specific behavior and questions researchers and decision makers are interested in answering additionally a possible avenue to mitigate the limitation associated with using single stochastic realization in the adaptive management approach can be using a more sophisticated mechanism for selecting the single run to mitigate any concerns about representing a likely reality the run could be selected by weighing several of its characteristics including e g its size pattern or value range while at the same time the more extreme runs can be visualized alongside to provide a more complete picture of the stochastic variations to facilitate further research we envision developing a geospatial library with a simple steering interface that would streamline integrating spatio temporal steering into current and new geospatial simulations e g peckham et al 2013 neteler and mitasova 2008 such a library would encourage more interactive modeling in geospatial research making the research process and results more accessible to domain experts and decision makers 6 conclusions in this study we addressed the often overlooked conceptual and implementation challenges of steering stochastic spatio temporal simulations our suggested approach combining aggregate views of future estimates with a single realization representing the past provides a novel solution to steering multiple stochastic realizations and one that is particularly applicable for testing adaptive management strategies the adaptive management modeling approach developed to help stakeholders design more realistic management scenarios can better represent the realities of decision making by allowing stakeholders to decide when and where they manage based on past actions current observations and future forecasting the participatory modeling case study demonstrated that the stakeholders were able to ask better more realistic questions about the feasibility of disease eradication budget allocations and management priorities given the large uncertainties typically associated with social ecological phenomena the adaptive management modeling approach focuses attention on developing strategies that decision makers can adopt based on the current on ground situation although our case study highlights the application and importance of spatio temporal steering for disease management the described approach is generally applicable and relevant for a variety of stochastic geospatial models 7 software and data availability this work is based on pops forecasting and control system an open source project composed of several software components under gnu gpl v2 and later c library pops r package rpops c grass gis addon r pops spread pops forecasting platform based django web framework links to github repositories are accessible from osf project osf io q32p9 additionally tangible landscape tangible geospatial modeling and visualization system integrated with grass gis licensed under gnu gpl v2 and later was customized for steering and the code is available under a separate branch on github github com tangible landscape grass tangible landscape tree pops steering data used for sod modeling are accessible from r pops spread tutorial page grasswiki osgeo org wiki sod spread tutorial declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this material was made possible in part by cooperative agreements from the united states department of agriculture s animal and plant health inspection service aphis it may not necessarily express aphis views the authors thank dr megan skrip from the center for geospatial analytics ncsu for her help with language editing and for her valuable comments that improved the manuscript author contributions a p and v p performed the research a p and c j wrote the steering code v p provided programming support d g prepared and led the workshop a p wrote the initial draft a p d g c j v p h m and r m edited the manuscript 
25958,the deepwater horizon dwh oil spill likely affected ecosystem services in the gulf of mexico to test this hypothesis we configured a ecopath with ecosim model and quantified the effects of commercial fisheries and particulate organic carbon poc sequestration from 2004 to 2014 encompassing dwh the yield of five functional groups were used to calculate changes in fishery catch and detritus biomass as a proxy for carbon buried offshore to calculate poc sequestration the model predicted an estimated loss of 15 16 million per year 13 in stone crab fisheries but estimated gains of up to 20 million per year 11 in the other four groups from 2010 to 2012 model simulations estimated a loss of 1200 0 15 in the ability of the northern gulf of mexico offshore environment to sequester poc in 2010 the dwh simulation led to an increase in fisheries overall and decrease in poc sequestration ecosystem services in 2010 keywords ecopath gulf of mexico ecosystem services oil spill model deepwater horizon 1 introduction the gulf of mexico gom is affected by multiple stressors such as habitat loss degraded water quality overfishing hypoxia and harmful algal blooms nrc 2013 the explosion of the deepwater horizon dwh drilling platform on 20 april 2010 followed by an 87 day uncontrolled oil spill added another stressor affecting approximately a 11 200 km2 of the surface offshore environment macdonald et al 2015 and 8400 km2 of the bottom chanton et al 2014 this large area of contamination likely impacted offshore ecosystem services the provisioning services in the offshore environment include the acquisition of fish shellfish oil gas minerals and chemical compounds for manufacturing armstrong et al 2012 the regulating services include the regulation of gas and climate through the biological pump waste regulation and detoxification through bioturbation and biodiversity armstrong et al 2012 the offshore supporting services include habitat nutrient cycling water cycling chemosynthetic primary production and resilience armstrong et al 2012 it is possible that any of the aforementioned services could have been affected by the dwh however work to date on offshore ecosystem services has focused on market based services such as tourism and commercial and recreational fisheries worm et al 2006 white et al 2012 cavanagh et al 2016 martin et al 2016 identification of goods and services that exist in the offshore environment armstrong et al 2010 werner et al 2014 barbier 2017 and assessing the value that stakeholders place on specific offshore services yoskowitz et al 2016 lau et al 2018 yet these studies have not addressed the potential loss in carbon sequestration due to this offshore uncontrolled oil spill from the deep ocean ecosystem services are the direct and indirect contributions from ecosystems that support sustain and enrich human life peterson and lubchenco 1997 holmlund and hammer 1999 carollo et al 2013 yoskowitz et al 2016 there are four different ecosystem service categories provisioning regulating cultural and supporting services mea 2005 provisioning services are the goods produced by ecosystems and directly consumed by humans regulating services are the processes that maintain the conditions favorable to life cultural services are the non material benefits such as aesthetic values supporting services drive the other three services therefore valuation of ecosystem services focuses on provisioning regulating and cultural services to people nrc 2013 considering value provided by the ecosystem can change the way decisions makers manage ecosystems but requires more data on interactions within the ecosystems and connections to specific human benefits nrc 2013 in this study we investigate whether and how offshore ecosystem services were affected by the dwh blowout waste regulation ecosystem services did change following the dwh event washburn et al 2018 but what about the other services to estimate ecosystem services changes resulting from the oil spill effects on the ecosystem must be quantified changes in goods and services must be quantified and change in cost to society must be quantified nrc 2013 the effects on two ecosystem services commercial fisheries and particulate organic carbon poc sequestration were estimated this was accomplished by building a model of multiple species to account for changes at the level of the fishing sector which each catch multiple species the model also captured detrital production from several sources such as dead fish benthos and plankton as a proxy for poc sequestration ecopath with ecosim ewe was used because it takes into account the aforementioned processes has relaxed data requirements is commonly used is user friendly and is free therefore the aim of this study was to build an ewe model to test whether there were losses in commercial fisheries and poc sequestration as a result of the dwh oil spill 2 methods to test whether there were losses in ecosystem services as a result of the dwh oil spill an ewe model version 6 5 14034 0 of the northern gulf of mexico was built and the changes to ecosystem services were calculated from the model outputs ewe utilizes a trophic flows model based on the mass balance fluxes of biomass christensen et al 2005 the foundation of the ecopath model is formed by two equations christensen et al 2005 the production equation christensen et al 2005 1 b i p b i y i j 1 n b j q b j d c j i e i b a i b i p b i 1 e e i or more simply for species i production catches predation mortality net migration biomass accumulation other mortality the consumption equation christensen et al 2005 2 b q b b p b 1 g s q 1 t m p b q b g s or more simply consumption production respiration unassimilated food in equation one eq 1 i refers to the prey and j refers to the predator for the remaining representations in both equations above b is biomass p is production rate y is fishery catch q is consumption dc is the fraction of prey i in the average predator j diet e is emigration ba is biomass accumulation ee is ecotrophic efficiency gs is autotrophy and tm is the unassimilated fraction a trophic flow approach enables consideration of the whole ecosystem from phytoplankton to detritus to benthos to fish christensen et al 2005 within ewe two main linked routines were used ecopath and ecosim ecopath is a static mass balance picture of the ecosystem and ecosim allows for the representation of temporal dynamics christensen et al 2005 the model is described in full in the supplementary materials in brief the model was generated by expanding upon an existing northern gulf of mexico model by suprenand et al 2015 first the original infauna functional group was divided into meiofauna and macrofauna size classes because these two groups responded differently to the dwh oil spill second oil forcing functions were added to simulate the effect of the dwh blowout fig 1 finally ecosystem services were linked to the relevant functional groups and monetary evaluation methods were applied 2 1 ewe model simulations the domain of the ewe model ranges from 24 to 31 n latitude to 80 98 w longitude with depths ranging from 0 to 2000 m including both nearshore and offshore zones fig 2 the original model suprenand et al 2015 contained 48 functional groups tables 1 and 2 we added meiofauna and macrofauna functional groups oil forcing functions and removed red tide as a fishery from the suprenand et al 2015 model the meiofauna functional group includes nematodes copepods ostracods and kinorhynchs the macrofauna functional group includes polychaetes isopods and amphipods two simulations starting with initial conditions in 2004 were run and predicting forward to 2014 1 no oil and 2 oil to improve the predictive power of the model vulnerabilities were optimized by fitting to a time series and outputs were compared to observational data when possible following heymans et al 2016 s best practices statistical analysis of model fit was checked against the 2004 2014 observational data by calculating correlation coefficient r root means squared error rmse reliability index ri average error ae average absolute error aae modeling efficiency mef pearson correlation spearman correlation and kendell correlation in excel 2016 for catch and relative biomass of model outputs and observational data stow et al 2009 olsen et al 2016 correlations greater than 0 5 are highly correlated olsen et al 2016 rmse ae and aae are the measure of the discrepancy size and indicate a good fit when the values are close to one ri is the average factor by which predictions differ from observations values close to one indicate good predictions stow et al 2009 mef is the objective model performance values above zero indicate above average performance olsen et al 2016 2 1 1 addition of meiofauna and macrofauna in the original model suprenand et al 2015 organisms living on in the sediment included blue crabs stone crabs benthic invertebrates and infauna for the present study the original infauna functional group biomass 20 t km2 was further divided into the meiofauna and macrofauna functional groups this was done because meiofauna between 0 042 and 0 3 mm and macrofauna 0 3 mm responded differently to the dwh spill baguley et al 2015 washburn et al 2016 this distinction may also have an effect on ecosystem service valuation because certain fish species and age groups preferentially feed on different groups for example meiofauna are an important food source for the juvenile stages of many fish species mullaney and gale 1996 de morais and bodiou 1984 the proportion of meiofauna to macrofauna changes with depth and the model encompasses a large depth range therefore a realistic separation of the two groups had to be established thiel 1979 starting proportions of meiofauna and macrofauna were based on thiel 1979 but were changed to the following during model balancing 12 t km2 for meiofauna and 11 5 t km2 for macrofauna the ratio values for production biomass p b and consumption biomass q b for macrofauna and meiofauna functional groups were taken from arreguín sánchez et al 2002 macrofauna values are based on polycheate rates p b of 4 yr 1 and a q b of 21 yr 1 meiofauna values are a p b of 8 yr 1 and a q b of 53 yr 1 following the aforementioned additions the diet matrix was updated pre balance diagnostic tests were run link 2010 and the model was balanced table sm1 refer to supplemental material for the full explanation 2 1 2 oil forcing functions we created oil response curves to estimate c which is a linear scaler on growth efficiency g in equation 3 thus equation 3 determines the change in group biomass between time steps db dt while incorporating changes in feeding efficiency from sub lethal oil impacts for the remaining representations i refers to the prey j refers to the predator b is biomass f is a functional relationship used to predict consumption rates m is natural mortality f is fishing mortality i is immigration rate out of the ecosystem and e is emigration a similar approach was used by ainsworth et al 2011 within equation 3 c was calculated from individual dose response models where z is baseline total mortality from ecopath basic input θ is a scaling factor on total mortality because of oil exposure k is the total biomass exposed to oil and bh is the total biomass in the habitat area eq 4 linear scalar on growth efficiency equation ainsworth et al 2011 3 d b i d t c g i j 1 n f b j b i j 1 n f b j b i i i b i m i f i e i link to individual dose response models 4 c 1 θ z z k b h this function was entered as a modifier within ecosim for each month the spill persisted april october however because the original model included the entire northern gulf of mexico ngom the percent of each functional group effected by the spill needed to be calculated k bh eq 4 table 3 and then multiplied by percent mortality from the dose response models to generate an accurate forcing function for each group eq 4 2 1 2 1 fish forcing functions the appropriate fish dose response model was chosen based on the work of dornberger et al 2016 who looked at the impact of the deepwater horizon oil spill on the frequency of fish lesions a proxy for mortality rate the hockey stick model implied that below a certain oil concentration there were no lethal effects on the population for our work the following parameters from dornberger et al 2016 were input into equation 5 oil threshold oil thresh 2 942 ppb and slope m 0 1051 yr 1 the oil thresh is the oil concentration level above which population level effects increase log linearly horness et al 1998 johnson et al 2002 in equation 5 θ is a scaling factor on total mortality z is baseline total mortality from ecopath basic input and m is the rate of change in the population response oil was determined by examining the predicted water column oil concentrations by depth from the simulations reported in perlin et al 2020 figure sm1 and the depth ranges of fish groups table sm2 in summary change in biomass over time was determined from eq 3 c was calculated from eq 4 and θ was calculated from eq 5 for the fish functional groups 5 θ z z m l o g o i l o i l t h r e s h i f o i l o i l t h r e s h o t h e r w i s e 2 1 2 2 invertebrate forcing functions dose response models were used to determine the impact of the deepwater horizon oil spill on invertebrates affects for shrimp groups came from echols et al 2016 and stone crabs and blue crabs was calculated based on dwhnrda 2016 to determine the impact on meiofauna and macrofauna the dose response model of balthis et al 2017 was used effects on macro and micro zooplankton groups came from almeda et al 2013 full details available in the supplemental material 2 1 2 3 primary producer forcing functions the results of studying the impact of oil on phytoplankton have been mixed therefore for our estimates we combined the phytoplankton findings of hu et al 2011 and the toxicity findings of garr et al 2014 when we generated the forcing function for phytoplankton for sea grasses 95 mortality was assumed within the model in an area of 21 13 km2 which is 8 3 of the entire model shoreline area table 4 based on silliman et al 2012 and nixon et al 2016 the impact on attached microalgae was calculated by using the same exposure response as the phytoplankton but with the area and exposure time of seagrass full details available in the supplemental material 2 2 observational data and ecosim tuning the predictability of the model was improved by tuning the model to observational data collected for catch and biomass values throughout the entire northern gom from 2004 to 2014 fish and shrimp data were obtained from seamap s public database which provides catch per unit effort cpue information for the entire northern gom bottom line catch information was standardized across surveys by calculating cpue based on the number of hook hours at each station trawl data was standardized across surveys by calculating cpue based on trawl distance the data was averaged for each year to correct for differences in the number of surveys and stations sampled commercial and recreational landings data were obtained from noaa s public landings statistics nmfs 2016 from 2004 to 2014 and used to tune fisheries yield outputs from the model discards that were entered within ecopath were added to the observational totals the comparison of modeled to observed dwh effects for fisheries data was performed on biomass values averaged for three years pre and post dwh from 2007 to 2009 and 2010 to 2012 to keep the analysis balanced vulnerabilities were determined by running an optimization routine to minimize discrepancies versus observational data vulnerabilities are the degree to which a large increase in predator biomass will lead to predation mortality for a given prey this was accomplished by vulnerability search where each interaction was selected individually to achieve the best fit against observational data a separate vulnerability search was run and applied for the no oil and the oil simulations to further match the model outputs to the observational data a production anomaly optimization christensen et al 2005 was run and applied to phytoplankton production manual calibration of the model was also done in ecopath to further improve overall model fit however in the course of tuning the model the changes led to decreased ecotrophic efficiency ee for 6 18 mullet 0 04 18 mullet 0 02 catfish 0 04 and large coastal shark 0 02 2 3 ewe simulations two simulations starting with initial conditions in 2004 and predicting forward to 2014 were run 1 no oil and 2 oil in both simulations fishing was simulated by using the same fleets as suprenand et al 2015 which was kept constant based on initial landings entered in ecopath under no oil conditions the baseline tuned ecosim model was run to simulate the dwh blowout conditions the oil forcing function values from table 4 were applied as a modifier to search rate for consumers in general functions were entered for 2010 year six in the model from april october for all functional groups the functions were applied at different times for the following groups from april june of year seven for meiofauna and macrofauna from april june for zooplankton from april september for phytoplankton and from may august for seagrass and attached microalgae the absolute biomass and yield values from both simulations were compared in order to measure potential impact ecosim outputs results for absolute biomass are in metric t km2 these results were multiplied by the habitat area for each functional group to measure changes in metric tons the final results are presented as percent change in 2010 and 2011 biomass this was calculated by subtracting oil scenario values from no oil scenario values for the same year 2 4 ecosystem services to test for changes in ecosystem services a service was assigned following the millennium ecosystem assessment mea 2005 framework six functional groups were chosen for further analysis concerning changes in ecosystem services shrimp blue crabs stone crabs grouper red snapper and detritus for commercial fisheries the model yield outputs by functional group and year from 2008 to 2012 were multiplied by the habitat area in which each group was found resulting in a value of metric tons this assumes that fisheries are operating in the entire habitat area of the functional group metric tons were then multiplied by the ex vessel price to get monetary change the use of ex vessel prices is important in assessing fisheries management and economic impact sumaila et al 2007 the approach of valuing a fishery using ex vessel prices where the focus is primarily on modeled bio physical changes to the fishery has been examined with ocean acidification cooley and doney 2009 ecosystem based management in large marine ecosystems christensen et al 2009 commercial fisheries losses because of closures due to dwh mccrea strub et al 2011 and commercial fisheries losses estimated up to seven years after dwh sumaila et al 2012 to determine the change in ex vessel value of the commercial fisheries yield the inflation adjusted ex vessel prices from nmfs 2016 were applied to the model outputs for 2010 2011 2012 2013 2014 for example the modeled amount of red snapper catch in pounds was multiplied by the nmfs 2016 price of 3 13 for 2010 3 20 for 2011 3 34 for 2012 3 89 for 2013 and 4 04 for 2014 model output dollar values were rounded to two significant digits at the ocean surface atmospheric carbon is taken up by phytoplankton through photosynthesis when the phytoplankton die their remains and the carbon they have incorporated sink to the seafloor when this incorporated carbon cannot return to the atmosphere for at least 100 years or when it reaches depths greater than 1000 m it is considered to be sequestered guidi et al 2015 once the organic remains reach the seafloor it is called phytodetritus which adds to the organic remains of other organisms collectively called detritus we used the detrital biomass predicted by the model to calculate how much of the carbon stored within became buried through carbon sequestration it is important to note that detrital biomass includes not only phytoplankton but 20 from every trophic interaction this method only takes into account the particulate organic carbon poc sequestration therefore the results are only estimating changes in poc sequestration we determined how much of the atmospheric carbon was sequestered in the deep sea through this process guidi et al 2015 calculated carbon sequestration values for the 56 biogeochemical provinces longhurst 1995 taking into account the amount that is remineralized and never reaches the seafloor two sequestration units were provided 1 sequestration at 2000 m and 2 sequestration at the top of the permanent pycnocline guidi et al 2015 because our model only extends to 2000 m we calculated sequestration at the top of the permanent pycnocline which starts at 200 m and extends to 1000 m melvin et al 2016 therefore when we calculated the change in sequestration from the model output we only considered the area of the model found below 200 m the gulf of mexico is not counted among the 56 biogeochemical provinces considered in guidi et al 2015 therefore we compared two estimates of carbon sequestration the values for the gulf stream 1 81 tg c yr and the global value 0 72 pg c yr this represents 0 00024 and 0 095 of the total atmospheric carbon based on a total of 760 gt mcleod et al 2011 3 results 3 1 model fit model fit analysis was performed for catch and biomass from 2004 to 2014 statistical analysis of catch model fit found mef was above zero for catfish and atlantic croaker but below zero for all other catch groups table 5 catch r was at or above 0 5 for catfish red snapper and atlantic croaker table 5 the spearman correlation was at or above 0 5 and significant for pompano and atlantic croaker table 5 catch ri was close to one and error rmse ae and aae was close to zero for all groups table 5 statistical analysis of relative biomass model fit found mef was above zero for mullet ladyfish grouper jacks shrimp blue crab red snapper and atlantic croaker table 6 biomass r was at or above 0 5 for jacks pinfish menhaden shrimp blue crab and atlantic croaker table 6 the spearman correlation was at or above 0 5 and significant for pinfish and red snapper table 6 biomass ri was close to one for all except pompano and error rmse ae and aae was close to zero for red drum sea trout jacks catfish and blue crab table 6 graphical representation of the comparison of predicted catch and predicted relative biomass to time series data can be found in the supplemental material figures sm2 sm8 3 2 scenarios when the absolute biomass output from no oil conditions was compared to the oil simulation biomass decreased in the dwh blowout simulation for 16 and increased for 33 groups in 2010 table 7 the percent changes in biomass were larger in 2011 than in 2010 table 7 the highest percent change in 2010 was seen in atlantic croaker with a decrease of 23 78 compared to a 44839 increase in mullet 18 in 2011 table 7 the mullet functional group had high error values when compared to observational data table 6 3 3 model fit to observed dwh effects percent change in observed biomass pre 2007 2009 and post the dwh 2010 2012 varied by functional group fig 3 percent change was larger within the observational data than the model outputs directional changes were in agreement for 62 of the functional groups that had observational time series data available pre and post the dwh fig 3 these groups included blue crabs red snapper older atlantic croaker pinfish shrimp menhaden catfish grouper 3 and bay anchovy 3 4 ecosystem services monetary valuation was performed for yield outputs of grouper red snapper shrimp stone crab and blue crabs the yields for all groups except stone crab were higher in the spill scenario table 8 the change in yield for each resulted in estimated monetary changes ranging from 18 million to 15 million in 2010 table 9 to determine how poc sequestration could have been altered following the dwh blow out the change in the amount of detritus in the offshore environment 200 2000 m was measured from the model outputs when compared to the no oil simulation this resulted in a detrital decrease of 57 640 80 metric tons in 2010 and a decrease of 43 292 21 metric tons in 2011 when carbon sequestration percentages were applied to the model outputs sequestration in 2010 decreased by 0 15 which equated to 33 89 metric tons based on the average global sequestration rate guidi et al 2015 and 0 09 metric tons based on the gulf stream sequestration rate guidi et al 2015 in 2010 in 2011 sequestration increased by 0 06 equating to 12 98 metric tons global average and 0 03 metric tons gulf stream rate 4 discussion it is always important in environmental assessment to be able to understand how an event affects people one way to do that is to translate biophysical impacts into ecosystem service impacts in the case of natural resource damage assessments nrda there is a need to monetize due to legal obligations for damage assessment therefore methods exist to transform sampling data to lost economic value nas 2012 here an approach is presented that quantifies how offshore ecosystem services were affected by the dwh importantly the methods here allow for valuation of indirect benefits such as the maintained availability of prey items on which exploited species depend 4 1 overall model fit overall the model had an ideal ri consistently predicting catch and relative biomass with an average multiplicative factor of one tables 5 and 6 pompano relative biomass was the only exception table 6 the other functional groups differed in their objective mef the tendency to vary with the observational data correlation and their prediction accuracy error tables 5 and 6 the model performed best overall when predicting catfish catch atlantic croaker catch jacks relative biomass blue crabs relative biomass and atlantic croaker relative biomass however in addition the model had an above average mef when predicting pompano catch mullet relative biomass ladyfish relative biomass grouper relative biomass shrimp relative biomass and red snapper relative biomass tables 5 and 6 therefore the model produced good reliable predictions for 15 of the catch groups and 47 of the relative biomass groups with the time series observational data available tuning of the model to match observational trends resulted in low ee values for 4 functional groups the ee value for lc shark was low 0 02 but it may be justifiable as there are few predators on large sharks however low ee values for mullet 6 18 mullet 0 04 and 18 mullet 0 02 and catfish 0 04 reflect that there is predation on these groups not identified in the available diet information further revisions to the model should work to identify additional potential predators although benthic diet data is limiting thus the sensitivity of mullet and catfish to top down trophic effects may be conservatively estimated this model is based on a simplified gulf of mexico food web which could explain the differences seen with the observational data and low ee values not all diet interactions are represented in particular the absence of sea birds and dolphins who are important predators of many fish groups 4 2 differences between simulations the dwh simulation showed an overall positive impact on functional group biomass percent change the most negatively impacted group was the atlantic croaker in 2010 with a decrease of 23 78 table 7 in 2011 the values showed a greater percent change when compared to the no oil simulation it is surprising that some functional groups that previously showed a slight decline in biomass in 2010 increased dramatically in 2011 table 7 this increase in biomass was because of a decrease in predation pressure on these groups in the oil spill simulation fishing effort was not changed in the simulation as seen in table 7 mackrel jacks red snapper older lc sharks red drum sea trout and catfish decreased in biomass in the 2011 oil simulation 4 3 model fit to observed dwh effects model predictions agreed with observational data with regard to directional changes but underestimated magnitude of change directional trends were in agreement for 63 of the functional groups that had observational data available these groups included blue crabs red snapper older atlantic croaker pinfish shrimp menhaden catfish grouper 3 bay anchovy meiofauna and macrofauna fig 3 baguley et al 2015 found that meiofauna abundance increased in offshore areas approximately 5 months after the spill with increases ranging from 104 to 197 in the dwh scenario meiofauna biomass increased immediately after the spill and reaches 0 75 above no oil conditions in 2010 washburn et al 2016 found that macrofauna abundance decreased 30 85 in the highly impacted zone of the dwh spill in the model macrofauna biomass decreased 0 12 6 months after the spill then increased the following year when model outputs are compared with survey data the majority of the functional groups should match the biomass trends direction but not necessarily magnitude kaplan and marshall 2016 this is the case with the dwh model 63 of the functional groups match the directional changes in the observational data after the dwh oil spill fig 3 the model does not take into account other sources of disturbance such as red tide hypoxia harmful algal blooms and hurricanes which would have compounded the impacts from the dwh oil spill in addition changes were made to the observational sampling effort there were more stations sampled with seamap from 2007 to 2009 then from 2010 to 2012 seamap adopted new sampling methods in 2010 that could have increased the capture rate in addition to other sources of disturbance differences in observed versus modeled outputs maybe attributed to model discrepancies because of model structure and parameter error the discrepancies include one general fisheries dose response model area of impact calculation and no fisheries closures included simulated impact of oil toxicity on fish functional groups was based on one general dose response model which has been informed by the current knowledge regarding oil spill impacts on fish populations the intensity of the toxic affect depends on the fish species the life stage the oil concentration and the oil composition mosbech 2002 mccay et al 2004 and incardona et al 2011 in addition oil exposure is not always associated with an immediate lethal outcome for example see heintz et al 2000 incardona et al 2013 and incardona et al 2014 toxic effects of oil exposure in fish include cardiac toxicity incardona et al 2013 incardona et al 2014 incardona and scholz 2015 morris et al 2015a morris et al 2015b reduced growth ortell et al 2015 reduced immune function ortell et al 2015 and reduced swim performance mager et al 2014 morris et al 2015b for the model it was presumed that the effects of oil exposure likely led to death by indirectly impacting their survival rates refer to moles and norcross 1998 meador et al 2006 the area of the model spans the entire northern gom to account for the small portion of this area impacted by the dwh spill the proportion of the population affected was accounted for when calculating the forcing function the area of impact in the water column was calculated using the area of the surface oil slick however oil entrained at depth likely spread differently below the surface paris et al 2012 le hénaff et al 2012 nevertheless using the surface slick may be appropriate because the organic flocculent and hydrocarbons mixed near the surface provided a conduit for benthic deposition schwing et al 2015 in considering the surface slick the mass accumulation at the bottom was also considered a comparison of the two approaches would be informative 4 4 ecosystem service changes while some oil spill models focus on the physical and chemical aspects of a spill others focus on the biological ecological aspects okey and pauly 1998 french mccay 2004 mccay 2003 afenyo et al 2017 carroll et al 2018 ainsworth et al 2018 fewer still have examined changes in ecosystem services following the dwh washburn et al 2018 the model presented here is the first ewe model to simulate the effects of the dwh oil spill while valuing ecosystem services of the ecosystem services valued the greatest impact was seen within the commercial stone crab industry with an estimated ex vessel loss of 15 million in 2010 this value is lower than the potential minimum loss of 247 million based on fisheries closures and the visual extent of the oil spill using ex vessel price information mccrea strub et al 2011 it is also lower than a seven year loss projection of 1 6 billion in commercial fisheries total revenue losses sumaila et al 2012 it is not surprising that these other estimates are higher than the current model because they consider economic loss resulting from fisheries closures and total revenue following the deepwater horizon blowout fisheries closures were implemented to aid in the recovery these closures were not implemented in the current version of the model and could have affected biomass changes seen in the ewe simulation for the oil spill scenario in addition the current model underestimated changes in biomass in the dwh simulation thereby underestimating catch the loss in the ability of the system to sequester carbon brings forth interesting social impacts one way to illuminate the loss of this service is by applying the social cost of co2 to the change in sequestration the social cost of co2 is a monetary estimate of the damages associated with the increasing carbon emissions iwgscc 2015 the value includes changes in human health property damage from increased flood risk net agricultural productivity and the value of ecosystem services because of climate change iwgscc 2015 the iwgscc 2015 value of 36 per metric ton of co2 3 discount rate was applied to the model output dollar costs were rounded to two significant digits and given the estimated decrease in sequestration in 2010 of 33 89 metric tons global average and 0 09 metric tons gulf stream rate this is equivalent to a social cost loss of 1200 and 3 respectively there was an increase in sequestration in 2011 equating to 12 98 metric tons global average and 0 03 metric tons gulf stream rate this is equivalent to a social cost gain of 470 and 1 respectively 4 5 comparisons to published oil spill models model simulations of a north cape oil spill on the south coast of rhode island simulated biological effects using direct mortality and lost production over a one hundred day period mccay 2003 the north cape oil spill simulation predicted up to a 40 loss in average sensitivity species and up to 90 loss in sensitive species for demersal fish and invertebrates mccay 2003 percent loss was lower in the 2010 ewe dwh simulation with the greatest percent loss seen within atlantic croaker 23 table 7 an oil spill model of the northeast artic cod fishery found a maximum of a 12 decrease in adult cod biomass within 90 days using direct mortality carroll et al 2018 this decrease is closer to the decreases seen in the 2010 ewe dwh simulation but there are no directly comparable functional groups to artic cod table 7 an atlantis model of the dwh oil spill simulated oil effects with direct mortality and growth reductions ainsworth et al 2018 the atlantis model predicted in areas most heavily impacted that the biomass of large reef fish decreased by 25 50 and large demersal fish decreased by 40 70 with the largest decreases occurring 7 16 months after the spill that estimate employed the same perlin et al 2020 oil model concentrations and the same hockey stick model from dornberger et al 2016 the ewe 2010 dwh simulation predicted up to a 3 decrease in reef fish pigfish and up to a 24 decrease in demersal fish atlantic croaker table 7 the atlantis model also predicted a general reduction in 2011 of catch from 20 to 40 ainsworth et al 2018 overall catch had a percent change increase of 5 in the ewe dwh simulation from 2010 to 2011 but percent change decreased by 264 from 2011 to 2012 the ewe dwh model predictions of biomass loss were lower than other published oil spill models and often led to increased biomass instead the most likely reason for this is that the oil effects were applied only as a forcing function on consumer search rate and not through direct mortality or a combination of the two this approach only takes into account the sublethal effects of the dwh oil spill occurring on growth rate similar approaches were used to simulate climate change impacts ainsworth et al 2011 suprenand and ainsworth 2017 other approaches to simulating mortality effects include direct mortality which is often achieved in ewe by using a pseudo fishery mccay 2003 ainsworth et al 2018 carroll et al 2018 dileone and ainsworth 2019 thus population impacts estimated here may be conservative representing both lethal and sublethal effects may provide more accurate predictions because both occur in an oil spill and future model revisions should compare the two 4 6 improvements to the current biophysical model the ewe model predictions did not correspond to directional trends and magnitude changes in observational data for all functional groups one way to improve this is to develop specific dose response curves for each functional group instead of using one dose response value for all fish groups this will only be possible once more data on the response of individual fish species becomes available second seasonal effects can be added for example research has suggested that changes within zooplankton are variable and depend on the time of the year carassou et al 2014 the same is true for phytoplankton and sub sequentially benthic fauna whose main food sources are the phytoplankton that fall from the surface improvements can also be made to the calculations for the area of the water column effects which could be done by developing a geographic information system gis layer showing oiling area by depth and implementing improved oil model results the marine oil snow sedimentation and flocculent accumulation mossfa daly et al 2016 event was not included instead the model only estimated changes in detritus as a result of normal ecosystems process 5 conclusion the approach presented here is an important step towards understanding and valuing changes to ecosystem services despite discrepancies between observed and predicted results the model and the methods employed provide valuable tools that can applied to any ewe oil spill model this approach can be applied to different perturbations and different environments ecopath is the preferred tool for fisheries in the european union fretzer 2016 here it is demonstrated that ecopath is a valuable tool to resource managers and decision makers because it can estimate changes in ecosystem services declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements research reported here was primarily supported by the gulf research program of the national academy of sciences under award number 200005982 the content is solely the responsibility of the authors and does not necessarily represent the official views of the gulf research program or the national academy of sciences additional research support was provided by a grant from the gulf of mexico research initiative c image ii no sa 15 16 data are publicly available through the gulf of mexico research initiative information data cooperative griidc at https data gulfresearchinitiative org doi 10 7266 n7 skas 7477 partial support to p montagna was provided by national oceanic and atmospheric administration office of education educational partnership program award number na16sec48100009 the views expressed herein are those of the authors and do not necessarily reflect the views of u s department of commerce noaa or any of its personnel support for cbp and np was provided by a grant from gomri c image ii data are publicly available at griidc https data gulfresearchinitiative org pelagos symfony data r4 x267 000 0038 many people at texas a m university corpus christi tamucc helped in completing this project several tamucc students helped to obtain biomass and community structure data this included undergraduate student tiffany hawkins and graduate students meagan hardegree amanda gordon and travis washburn elani morgan tamucc helped with data management appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104793 
25958,the deepwater horizon dwh oil spill likely affected ecosystem services in the gulf of mexico to test this hypothesis we configured a ecopath with ecosim model and quantified the effects of commercial fisheries and particulate organic carbon poc sequestration from 2004 to 2014 encompassing dwh the yield of five functional groups were used to calculate changes in fishery catch and detritus biomass as a proxy for carbon buried offshore to calculate poc sequestration the model predicted an estimated loss of 15 16 million per year 13 in stone crab fisheries but estimated gains of up to 20 million per year 11 in the other four groups from 2010 to 2012 model simulations estimated a loss of 1200 0 15 in the ability of the northern gulf of mexico offshore environment to sequester poc in 2010 the dwh simulation led to an increase in fisheries overall and decrease in poc sequestration ecosystem services in 2010 keywords ecopath gulf of mexico ecosystem services oil spill model deepwater horizon 1 introduction the gulf of mexico gom is affected by multiple stressors such as habitat loss degraded water quality overfishing hypoxia and harmful algal blooms nrc 2013 the explosion of the deepwater horizon dwh drilling platform on 20 april 2010 followed by an 87 day uncontrolled oil spill added another stressor affecting approximately a 11 200 km2 of the surface offshore environment macdonald et al 2015 and 8400 km2 of the bottom chanton et al 2014 this large area of contamination likely impacted offshore ecosystem services the provisioning services in the offshore environment include the acquisition of fish shellfish oil gas minerals and chemical compounds for manufacturing armstrong et al 2012 the regulating services include the regulation of gas and climate through the biological pump waste regulation and detoxification through bioturbation and biodiversity armstrong et al 2012 the offshore supporting services include habitat nutrient cycling water cycling chemosynthetic primary production and resilience armstrong et al 2012 it is possible that any of the aforementioned services could have been affected by the dwh however work to date on offshore ecosystem services has focused on market based services such as tourism and commercial and recreational fisheries worm et al 2006 white et al 2012 cavanagh et al 2016 martin et al 2016 identification of goods and services that exist in the offshore environment armstrong et al 2010 werner et al 2014 barbier 2017 and assessing the value that stakeholders place on specific offshore services yoskowitz et al 2016 lau et al 2018 yet these studies have not addressed the potential loss in carbon sequestration due to this offshore uncontrolled oil spill from the deep ocean ecosystem services are the direct and indirect contributions from ecosystems that support sustain and enrich human life peterson and lubchenco 1997 holmlund and hammer 1999 carollo et al 2013 yoskowitz et al 2016 there are four different ecosystem service categories provisioning regulating cultural and supporting services mea 2005 provisioning services are the goods produced by ecosystems and directly consumed by humans regulating services are the processes that maintain the conditions favorable to life cultural services are the non material benefits such as aesthetic values supporting services drive the other three services therefore valuation of ecosystem services focuses on provisioning regulating and cultural services to people nrc 2013 considering value provided by the ecosystem can change the way decisions makers manage ecosystems but requires more data on interactions within the ecosystems and connections to specific human benefits nrc 2013 in this study we investigate whether and how offshore ecosystem services were affected by the dwh blowout waste regulation ecosystem services did change following the dwh event washburn et al 2018 but what about the other services to estimate ecosystem services changes resulting from the oil spill effects on the ecosystem must be quantified changes in goods and services must be quantified and change in cost to society must be quantified nrc 2013 the effects on two ecosystem services commercial fisheries and particulate organic carbon poc sequestration were estimated this was accomplished by building a model of multiple species to account for changes at the level of the fishing sector which each catch multiple species the model also captured detrital production from several sources such as dead fish benthos and plankton as a proxy for poc sequestration ecopath with ecosim ewe was used because it takes into account the aforementioned processes has relaxed data requirements is commonly used is user friendly and is free therefore the aim of this study was to build an ewe model to test whether there were losses in commercial fisheries and poc sequestration as a result of the dwh oil spill 2 methods to test whether there were losses in ecosystem services as a result of the dwh oil spill an ewe model version 6 5 14034 0 of the northern gulf of mexico was built and the changes to ecosystem services were calculated from the model outputs ewe utilizes a trophic flows model based on the mass balance fluxes of biomass christensen et al 2005 the foundation of the ecopath model is formed by two equations christensen et al 2005 the production equation christensen et al 2005 1 b i p b i y i j 1 n b j q b j d c j i e i b a i b i p b i 1 e e i or more simply for species i production catches predation mortality net migration biomass accumulation other mortality the consumption equation christensen et al 2005 2 b q b b p b 1 g s q 1 t m p b q b g s or more simply consumption production respiration unassimilated food in equation one eq 1 i refers to the prey and j refers to the predator for the remaining representations in both equations above b is biomass p is production rate y is fishery catch q is consumption dc is the fraction of prey i in the average predator j diet e is emigration ba is biomass accumulation ee is ecotrophic efficiency gs is autotrophy and tm is the unassimilated fraction a trophic flow approach enables consideration of the whole ecosystem from phytoplankton to detritus to benthos to fish christensen et al 2005 within ewe two main linked routines were used ecopath and ecosim ecopath is a static mass balance picture of the ecosystem and ecosim allows for the representation of temporal dynamics christensen et al 2005 the model is described in full in the supplementary materials in brief the model was generated by expanding upon an existing northern gulf of mexico model by suprenand et al 2015 first the original infauna functional group was divided into meiofauna and macrofauna size classes because these two groups responded differently to the dwh oil spill second oil forcing functions were added to simulate the effect of the dwh blowout fig 1 finally ecosystem services were linked to the relevant functional groups and monetary evaluation methods were applied 2 1 ewe model simulations the domain of the ewe model ranges from 24 to 31 n latitude to 80 98 w longitude with depths ranging from 0 to 2000 m including both nearshore and offshore zones fig 2 the original model suprenand et al 2015 contained 48 functional groups tables 1 and 2 we added meiofauna and macrofauna functional groups oil forcing functions and removed red tide as a fishery from the suprenand et al 2015 model the meiofauna functional group includes nematodes copepods ostracods and kinorhynchs the macrofauna functional group includes polychaetes isopods and amphipods two simulations starting with initial conditions in 2004 were run and predicting forward to 2014 1 no oil and 2 oil to improve the predictive power of the model vulnerabilities were optimized by fitting to a time series and outputs were compared to observational data when possible following heymans et al 2016 s best practices statistical analysis of model fit was checked against the 2004 2014 observational data by calculating correlation coefficient r root means squared error rmse reliability index ri average error ae average absolute error aae modeling efficiency mef pearson correlation spearman correlation and kendell correlation in excel 2016 for catch and relative biomass of model outputs and observational data stow et al 2009 olsen et al 2016 correlations greater than 0 5 are highly correlated olsen et al 2016 rmse ae and aae are the measure of the discrepancy size and indicate a good fit when the values are close to one ri is the average factor by which predictions differ from observations values close to one indicate good predictions stow et al 2009 mef is the objective model performance values above zero indicate above average performance olsen et al 2016 2 1 1 addition of meiofauna and macrofauna in the original model suprenand et al 2015 organisms living on in the sediment included blue crabs stone crabs benthic invertebrates and infauna for the present study the original infauna functional group biomass 20 t km2 was further divided into the meiofauna and macrofauna functional groups this was done because meiofauna between 0 042 and 0 3 mm and macrofauna 0 3 mm responded differently to the dwh spill baguley et al 2015 washburn et al 2016 this distinction may also have an effect on ecosystem service valuation because certain fish species and age groups preferentially feed on different groups for example meiofauna are an important food source for the juvenile stages of many fish species mullaney and gale 1996 de morais and bodiou 1984 the proportion of meiofauna to macrofauna changes with depth and the model encompasses a large depth range therefore a realistic separation of the two groups had to be established thiel 1979 starting proportions of meiofauna and macrofauna were based on thiel 1979 but were changed to the following during model balancing 12 t km2 for meiofauna and 11 5 t km2 for macrofauna the ratio values for production biomass p b and consumption biomass q b for macrofauna and meiofauna functional groups were taken from arreguín sánchez et al 2002 macrofauna values are based on polycheate rates p b of 4 yr 1 and a q b of 21 yr 1 meiofauna values are a p b of 8 yr 1 and a q b of 53 yr 1 following the aforementioned additions the diet matrix was updated pre balance diagnostic tests were run link 2010 and the model was balanced table sm1 refer to supplemental material for the full explanation 2 1 2 oil forcing functions we created oil response curves to estimate c which is a linear scaler on growth efficiency g in equation 3 thus equation 3 determines the change in group biomass between time steps db dt while incorporating changes in feeding efficiency from sub lethal oil impacts for the remaining representations i refers to the prey j refers to the predator b is biomass f is a functional relationship used to predict consumption rates m is natural mortality f is fishing mortality i is immigration rate out of the ecosystem and e is emigration a similar approach was used by ainsworth et al 2011 within equation 3 c was calculated from individual dose response models where z is baseline total mortality from ecopath basic input θ is a scaling factor on total mortality because of oil exposure k is the total biomass exposed to oil and bh is the total biomass in the habitat area eq 4 linear scalar on growth efficiency equation ainsworth et al 2011 3 d b i d t c g i j 1 n f b j b i j 1 n f b j b i i i b i m i f i e i link to individual dose response models 4 c 1 θ z z k b h this function was entered as a modifier within ecosim for each month the spill persisted april october however because the original model included the entire northern gulf of mexico ngom the percent of each functional group effected by the spill needed to be calculated k bh eq 4 table 3 and then multiplied by percent mortality from the dose response models to generate an accurate forcing function for each group eq 4 2 1 2 1 fish forcing functions the appropriate fish dose response model was chosen based on the work of dornberger et al 2016 who looked at the impact of the deepwater horizon oil spill on the frequency of fish lesions a proxy for mortality rate the hockey stick model implied that below a certain oil concentration there were no lethal effects on the population for our work the following parameters from dornberger et al 2016 were input into equation 5 oil threshold oil thresh 2 942 ppb and slope m 0 1051 yr 1 the oil thresh is the oil concentration level above which population level effects increase log linearly horness et al 1998 johnson et al 2002 in equation 5 θ is a scaling factor on total mortality z is baseline total mortality from ecopath basic input and m is the rate of change in the population response oil was determined by examining the predicted water column oil concentrations by depth from the simulations reported in perlin et al 2020 figure sm1 and the depth ranges of fish groups table sm2 in summary change in biomass over time was determined from eq 3 c was calculated from eq 4 and θ was calculated from eq 5 for the fish functional groups 5 θ z z m l o g o i l o i l t h r e s h i f o i l o i l t h r e s h o t h e r w i s e 2 1 2 2 invertebrate forcing functions dose response models were used to determine the impact of the deepwater horizon oil spill on invertebrates affects for shrimp groups came from echols et al 2016 and stone crabs and blue crabs was calculated based on dwhnrda 2016 to determine the impact on meiofauna and macrofauna the dose response model of balthis et al 2017 was used effects on macro and micro zooplankton groups came from almeda et al 2013 full details available in the supplemental material 2 1 2 3 primary producer forcing functions the results of studying the impact of oil on phytoplankton have been mixed therefore for our estimates we combined the phytoplankton findings of hu et al 2011 and the toxicity findings of garr et al 2014 when we generated the forcing function for phytoplankton for sea grasses 95 mortality was assumed within the model in an area of 21 13 km2 which is 8 3 of the entire model shoreline area table 4 based on silliman et al 2012 and nixon et al 2016 the impact on attached microalgae was calculated by using the same exposure response as the phytoplankton but with the area and exposure time of seagrass full details available in the supplemental material 2 2 observational data and ecosim tuning the predictability of the model was improved by tuning the model to observational data collected for catch and biomass values throughout the entire northern gom from 2004 to 2014 fish and shrimp data were obtained from seamap s public database which provides catch per unit effort cpue information for the entire northern gom bottom line catch information was standardized across surveys by calculating cpue based on the number of hook hours at each station trawl data was standardized across surveys by calculating cpue based on trawl distance the data was averaged for each year to correct for differences in the number of surveys and stations sampled commercial and recreational landings data were obtained from noaa s public landings statistics nmfs 2016 from 2004 to 2014 and used to tune fisheries yield outputs from the model discards that were entered within ecopath were added to the observational totals the comparison of modeled to observed dwh effects for fisheries data was performed on biomass values averaged for three years pre and post dwh from 2007 to 2009 and 2010 to 2012 to keep the analysis balanced vulnerabilities were determined by running an optimization routine to minimize discrepancies versus observational data vulnerabilities are the degree to which a large increase in predator biomass will lead to predation mortality for a given prey this was accomplished by vulnerability search where each interaction was selected individually to achieve the best fit against observational data a separate vulnerability search was run and applied for the no oil and the oil simulations to further match the model outputs to the observational data a production anomaly optimization christensen et al 2005 was run and applied to phytoplankton production manual calibration of the model was also done in ecopath to further improve overall model fit however in the course of tuning the model the changes led to decreased ecotrophic efficiency ee for 6 18 mullet 0 04 18 mullet 0 02 catfish 0 04 and large coastal shark 0 02 2 3 ewe simulations two simulations starting with initial conditions in 2004 and predicting forward to 2014 were run 1 no oil and 2 oil in both simulations fishing was simulated by using the same fleets as suprenand et al 2015 which was kept constant based on initial landings entered in ecopath under no oil conditions the baseline tuned ecosim model was run to simulate the dwh blowout conditions the oil forcing function values from table 4 were applied as a modifier to search rate for consumers in general functions were entered for 2010 year six in the model from april october for all functional groups the functions were applied at different times for the following groups from april june of year seven for meiofauna and macrofauna from april june for zooplankton from april september for phytoplankton and from may august for seagrass and attached microalgae the absolute biomass and yield values from both simulations were compared in order to measure potential impact ecosim outputs results for absolute biomass are in metric t km2 these results were multiplied by the habitat area for each functional group to measure changes in metric tons the final results are presented as percent change in 2010 and 2011 biomass this was calculated by subtracting oil scenario values from no oil scenario values for the same year 2 4 ecosystem services to test for changes in ecosystem services a service was assigned following the millennium ecosystem assessment mea 2005 framework six functional groups were chosen for further analysis concerning changes in ecosystem services shrimp blue crabs stone crabs grouper red snapper and detritus for commercial fisheries the model yield outputs by functional group and year from 2008 to 2012 were multiplied by the habitat area in which each group was found resulting in a value of metric tons this assumes that fisheries are operating in the entire habitat area of the functional group metric tons were then multiplied by the ex vessel price to get monetary change the use of ex vessel prices is important in assessing fisheries management and economic impact sumaila et al 2007 the approach of valuing a fishery using ex vessel prices where the focus is primarily on modeled bio physical changes to the fishery has been examined with ocean acidification cooley and doney 2009 ecosystem based management in large marine ecosystems christensen et al 2009 commercial fisheries losses because of closures due to dwh mccrea strub et al 2011 and commercial fisheries losses estimated up to seven years after dwh sumaila et al 2012 to determine the change in ex vessel value of the commercial fisheries yield the inflation adjusted ex vessel prices from nmfs 2016 were applied to the model outputs for 2010 2011 2012 2013 2014 for example the modeled amount of red snapper catch in pounds was multiplied by the nmfs 2016 price of 3 13 for 2010 3 20 for 2011 3 34 for 2012 3 89 for 2013 and 4 04 for 2014 model output dollar values were rounded to two significant digits at the ocean surface atmospheric carbon is taken up by phytoplankton through photosynthesis when the phytoplankton die their remains and the carbon they have incorporated sink to the seafloor when this incorporated carbon cannot return to the atmosphere for at least 100 years or when it reaches depths greater than 1000 m it is considered to be sequestered guidi et al 2015 once the organic remains reach the seafloor it is called phytodetritus which adds to the organic remains of other organisms collectively called detritus we used the detrital biomass predicted by the model to calculate how much of the carbon stored within became buried through carbon sequestration it is important to note that detrital biomass includes not only phytoplankton but 20 from every trophic interaction this method only takes into account the particulate organic carbon poc sequestration therefore the results are only estimating changes in poc sequestration we determined how much of the atmospheric carbon was sequestered in the deep sea through this process guidi et al 2015 calculated carbon sequestration values for the 56 biogeochemical provinces longhurst 1995 taking into account the amount that is remineralized and never reaches the seafloor two sequestration units were provided 1 sequestration at 2000 m and 2 sequestration at the top of the permanent pycnocline guidi et al 2015 because our model only extends to 2000 m we calculated sequestration at the top of the permanent pycnocline which starts at 200 m and extends to 1000 m melvin et al 2016 therefore when we calculated the change in sequestration from the model output we only considered the area of the model found below 200 m the gulf of mexico is not counted among the 56 biogeochemical provinces considered in guidi et al 2015 therefore we compared two estimates of carbon sequestration the values for the gulf stream 1 81 tg c yr and the global value 0 72 pg c yr this represents 0 00024 and 0 095 of the total atmospheric carbon based on a total of 760 gt mcleod et al 2011 3 results 3 1 model fit model fit analysis was performed for catch and biomass from 2004 to 2014 statistical analysis of catch model fit found mef was above zero for catfish and atlantic croaker but below zero for all other catch groups table 5 catch r was at or above 0 5 for catfish red snapper and atlantic croaker table 5 the spearman correlation was at or above 0 5 and significant for pompano and atlantic croaker table 5 catch ri was close to one and error rmse ae and aae was close to zero for all groups table 5 statistical analysis of relative biomass model fit found mef was above zero for mullet ladyfish grouper jacks shrimp blue crab red snapper and atlantic croaker table 6 biomass r was at or above 0 5 for jacks pinfish menhaden shrimp blue crab and atlantic croaker table 6 the spearman correlation was at or above 0 5 and significant for pinfish and red snapper table 6 biomass ri was close to one for all except pompano and error rmse ae and aae was close to zero for red drum sea trout jacks catfish and blue crab table 6 graphical representation of the comparison of predicted catch and predicted relative biomass to time series data can be found in the supplemental material figures sm2 sm8 3 2 scenarios when the absolute biomass output from no oil conditions was compared to the oil simulation biomass decreased in the dwh blowout simulation for 16 and increased for 33 groups in 2010 table 7 the percent changes in biomass were larger in 2011 than in 2010 table 7 the highest percent change in 2010 was seen in atlantic croaker with a decrease of 23 78 compared to a 44839 increase in mullet 18 in 2011 table 7 the mullet functional group had high error values when compared to observational data table 6 3 3 model fit to observed dwh effects percent change in observed biomass pre 2007 2009 and post the dwh 2010 2012 varied by functional group fig 3 percent change was larger within the observational data than the model outputs directional changes were in agreement for 62 of the functional groups that had observational time series data available pre and post the dwh fig 3 these groups included blue crabs red snapper older atlantic croaker pinfish shrimp menhaden catfish grouper 3 and bay anchovy 3 4 ecosystem services monetary valuation was performed for yield outputs of grouper red snapper shrimp stone crab and blue crabs the yields for all groups except stone crab were higher in the spill scenario table 8 the change in yield for each resulted in estimated monetary changes ranging from 18 million to 15 million in 2010 table 9 to determine how poc sequestration could have been altered following the dwh blow out the change in the amount of detritus in the offshore environment 200 2000 m was measured from the model outputs when compared to the no oil simulation this resulted in a detrital decrease of 57 640 80 metric tons in 2010 and a decrease of 43 292 21 metric tons in 2011 when carbon sequestration percentages were applied to the model outputs sequestration in 2010 decreased by 0 15 which equated to 33 89 metric tons based on the average global sequestration rate guidi et al 2015 and 0 09 metric tons based on the gulf stream sequestration rate guidi et al 2015 in 2010 in 2011 sequestration increased by 0 06 equating to 12 98 metric tons global average and 0 03 metric tons gulf stream rate 4 discussion it is always important in environmental assessment to be able to understand how an event affects people one way to do that is to translate biophysical impacts into ecosystem service impacts in the case of natural resource damage assessments nrda there is a need to monetize due to legal obligations for damage assessment therefore methods exist to transform sampling data to lost economic value nas 2012 here an approach is presented that quantifies how offshore ecosystem services were affected by the dwh importantly the methods here allow for valuation of indirect benefits such as the maintained availability of prey items on which exploited species depend 4 1 overall model fit overall the model had an ideal ri consistently predicting catch and relative biomass with an average multiplicative factor of one tables 5 and 6 pompano relative biomass was the only exception table 6 the other functional groups differed in their objective mef the tendency to vary with the observational data correlation and their prediction accuracy error tables 5 and 6 the model performed best overall when predicting catfish catch atlantic croaker catch jacks relative biomass blue crabs relative biomass and atlantic croaker relative biomass however in addition the model had an above average mef when predicting pompano catch mullet relative biomass ladyfish relative biomass grouper relative biomass shrimp relative biomass and red snapper relative biomass tables 5 and 6 therefore the model produced good reliable predictions for 15 of the catch groups and 47 of the relative biomass groups with the time series observational data available tuning of the model to match observational trends resulted in low ee values for 4 functional groups the ee value for lc shark was low 0 02 but it may be justifiable as there are few predators on large sharks however low ee values for mullet 6 18 mullet 0 04 and 18 mullet 0 02 and catfish 0 04 reflect that there is predation on these groups not identified in the available diet information further revisions to the model should work to identify additional potential predators although benthic diet data is limiting thus the sensitivity of mullet and catfish to top down trophic effects may be conservatively estimated this model is based on a simplified gulf of mexico food web which could explain the differences seen with the observational data and low ee values not all diet interactions are represented in particular the absence of sea birds and dolphins who are important predators of many fish groups 4 2 differences between simulations the dwh simulation showed an overall positive impact on functional group biomass percent change the most negatively impacted group was the atlantic croaker in 2010 with a decrease of 23 78 table 7 in 2011 the values showed a greater percent change when compared to the no oil simulation it is surprising that some functional groups that previously showed a slight decline in biomass in 2010 increased dramatically in 2011 table 7 this increase in biomass was because of a decrease in predation pressure on these groups in the oil spill simulation fishing effort was not changed in the simulation as seen in table 7 mackrel jacks red snapper older lc sharks red drum sea trout and catfish decreased in biomass in the 2011 oil simulation 4 3 model fit to observed dwh effects model predictions agreed with observational data with regard to directional changes but underestimated magnitude of change directional trends were in agreement for 63 of the functional groups that had observational data available these groups included blue crabs red snapper older atlantic croaker pinfish shrimp menhaden catfish grouper 3 bay anchovy meiofauna and macrofauna fig 3 baguley et al 2015 found that meiofauna abundance increased in offshore areas approximately 5 months after the spill with increases ranging from 104 to 197 in the dwh scenario meiofauna biomass increased immediately after the spill and reaches 0 75 above no oil conditions in 2010 washburn et al 2016 found that macrofauna abundance decreased 30 85 in the highly impacted zone of the dwh spill in the model macrofauna biomass decreased 0 12 6 months after the spill then increased the following year when model outputs are compared with survey data the majority of the functional groups should match the biomass trends direction but not necessarily magnitude kaplan and marshall 2016 this is the case with the dwh model 63 of the functional groups match the directional changes in the observational data after the dwh oil spill fig 3 the model does not take into account other sources of disturbance such as red tide hypoxia harmful algal blooms and hurricanes which would have compounded the impacts from the dwh oil spill in addition changes were made to the observational sampling effort there were more stations sampled with seamap from 2007 to 2009 then from 2010 to 2012 seamap adopted new sampling methods in 2010 that could have increased the capture rate in addition to other sources of disturbance differences in observed versus modeled outputs maybe attributed to model discrepancies because of model structure and parameter error the discrepancies include one general fisheries dose response model area of impact calculation and no fisheries closures included simulated impact of oil toxicity on fish functional groups was based on one general dose response model which has been informed by the current knowledge regarding oil spill impacts on fish populations the intensity of the toxic affect depends on the fish species the life stage the oil concentration and the oil composition mosbech 2002 mccay et al 2004 and incardona et al 2011 in addition oil exposure is not always associated with an immediate lethal outcome for example see heintz et al 2000 incardona et al 2013 and incardona et al 2014 toxic effects of oil exposure in fish include cardiac toxicity incardona et al 2013 incardona et al 2014 incardona and scholz 2015 morris et al 2015a morris et al 2015b reduced growth ortell et al 2015 reduced immune function ortell et al 2015 and reduced swim performance mager et al 2014 morris et al 2015b for the model it was presumed that the effects of oil exposure likely led to death by indirectly impacting their survival rates refer to moles and norcross 1998 meador et al 2006 the area of the model spans the entire northern gom to account for the small portion of this area impacted by the dwh spill the proportion of the population affected was accounted for when calculating the forcing function the area of impact in the water column was calculated using the area of the surface oil slick however oil entrained at depth likely spread differently below the surface paris et al 2012 le hénaff et al 2012 nevertheless using the surface slick may be appropriate because the organic flocculent and hydrocarbons mixed near the surface provided a conduit for benthic deposition schwing et al 2015 in considering the surface slick the mass accumulation at the bottom was also considered a comparison of the two approaches would be informative 4 4 ecosystem service changes while some oil spill models focus on the physical and chemical aspects of a spill others focus on the biological ecological aspects okey and pauly 1998 french mccay 2004 mccay 2003 afenyo et al 2017 carroll et al 2018 ainsworth et al 2018 fewer still have examined changes in ecosystem services following the dwh washburn et al 2018 the model presented here is the first ewe model to simulate the effects of the dwh oil spill while valuing ecosystem services of the ecosystem services valued the greatest impact was seen within the commercial stone crab industry with an estimated ex vessel loss of 15 million in 2010 this value is lower than the potential minimum loss of 247 million based on fisheries closures and the visual extent of the oil spill using ex vessel price information mccrea strub et al 2011 it is also lower than a seven year loss projection of 1 6 billion in commercial fisheries total revenue losses sumaila et al 2012 it is not surprising that these other estimates are higher than the current model because they consider economic loss resulting from fisheries closures and total revenue following the deepwater horizon blowout fisheries closures were implemented to aid in the recovery these closures were not implemented in the current version of the model and could have affected biomass changes seen in the ewe simulation for the oil spill scenario in addition the current model underestimated changes in biomass in the dwh simulation thereby underestimating catch the loss in the ability of the system to sequester carbon brings forth interesting social impacts one way to illuminate the loss of this service is by applying the social cost of co2 to the change in sequestration the social cost of co2 is a monetary estimate of the damages associated with the increasing carbon emissions iwgscc 2015 the value includes changes in human health property damage from increased flood risk net agricultural productivity and the value of ecosystem services because of climate change iwgscc 2015 the iwgscc 2015 value of 36 per metric ton of co2 3 discount rate was applied to the model output dollar costs were rounded to two significant digits and given the estimated decrease in sequestration in 2010 of 33 89 metric tons global average and 0 09 metric tons gulf stream rate this is equivalent to a social cost loss of 1200 and 3 respectively there was an increase in sequestration in 2011 equating to 12 98 metric tons global average and 0 03 metric tons gulf stream rate this is equivalent to a social cost gain of 470 and 1 respectively 4 5 comparisons to published oil spill models model simulations of a north cape oil spill on the south coast of rhode island simulated biological effects using direct mortality and lost production over a one hundred day period mccay 2003 the north cape oil spill simulation predicted up to a 40 loss in average sensitivity species and up to 90 loss in sensitive species for demersal fish and invertebrates mccay 2003 percent loss was lower in the 2010 ewe dwh simulation with the greatest percent loss seen within atlantic croaker 23 table 7 an oil spill model of the northeast artic cod fishery found a maximum of a 12 decrease in adult cod biomass within 90 days using direct mortality carroll et al 2018 this decrease is closer to the decreases seen in the 2010 ewe dwh simulation but there are no directly comparable functional groups to artic cod table 7 an atlantis model of the dwh oil spill simulated oil effects with direct mortality and growth reductions ainsworth et al 2018 the atlantis model predicted in areas most heavily impacted that the biomass of large reef fish decreased by 25 50 and large demersal fish decreased by 40 70 with the largest decreases occurring 7 16 months after the spill that estimate employed the same perlin et al 2020 oil model concentrations and the same hockey stick model from dornberger et al 2016 the ewe 2010 dwh simulation predicted up to a 3 decrease in reef fish pigfish and up to a 24 decrease in demersal fish atlantic croaker table 7 the atlantis model also predicted a general reduction in 2011 of catch from 20 to 40 ainsworth et al 2018 overall catch had a percent change increase of 5 in the ewe dwh simulation from 2010 to 2011 but percent change decreased by 264 from 2011 to 2012 the ewe dwh model predictions of biomass loss were lower than other published oil spill models and often led to increased biomass instead the most likely reason for this is that the oil effects were applied only as a forcing function on consumer search rate and not through direct mortality or a combination of the two this approach only takes into account the sublethal effects of the dwh oil spill occurring on growth rate similar approaches were used to simulate climate change impacts ainsworth et al 2011 suprenand and ainsworth 2017 other approaches to simulating mortality effects include direct mortality which is often achieved in ewe by using a pseudo fishery mccay 2003 ainsworth et al 2018 carroll et al 2018 dileone and ainsworth 2019 thus population impacts estimated here may be conservative representing both lethal and sublethal effects may provide more accurate predictions because both occur in an oil spill and future model revisions should compare the two 4 6 improvements to the current biophysical model the ewe model predictions did not correspond to directional trends and magnitude changes in observational data for all functional groups one way to improve this is to develop specific dose response curves for each functional group instead of using one dose response value for all fish groups this will only be possible once more data on the response of individual fish species becomes available second seasonal effects can be added for example research has suggested that changes within zooplankton are variable and depend on the time of the year carassou et al 2014 the same is true for phytoplankton and sub sequentially benthic fauna whose main food sources are the phytoplankton that fall from the surface improvements can also be made to the calculations for the area of the water column effects which could be done by developing a geographic information system gis layer showing oiling area by depth and implementing improved oil model results the marine oil snow sedimentation and flocculent accumulation mossfa daly et al 2016 event was not included instead the model only estimated changes in detritus as a result of normal ecosystems process 5 conclusion the approach presented here is an important step towards understanding and valuing changes to ecosystem services despite discrepancies between observed and predicted results the model and the methods employed provide valuable tools that can applied to any ewe oil spill model this approach can be applied to different perturbations and different environments ecopath is the preferred tool for fisheries in the european union fretzer 2016 here it is demonstrated that ecopath is a valuable tool to resource managers and decision makers because it can estimate changes in ecosystem services declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements research reported here was primarily supported by the gulf research program of the national academy of sciences under award number 200005982 the content is solely the responsibility of the authors and does not necessarily represent the official views of the gulf research program or the national academy of sciences additional research support was provided by a grant from the gulf of mexico research initiative c image ii no sa 15 16 data are publicly available through the gulf of mexico research initiative information data cooperative griidc at https data gulfresearchinitiative org doi 10 7266 n7 skas 7477 partial support to p montagna was provided by national oceanic and atmospheric administration office of education educational partnership program award number na16sec48100009 the views expressed herein are those of the authors and do not necessarily reflect the views of u s department of commerce noaa or any of its personnel support for cbp and np was provided by a grant from gomri c image ii data are publicly available at griidc https data gulfresearchinitiative org pelagos symfony data r4 x267 000 0038 many people at texas a m university corpus christi tamucc helped in completing this project several tamucc students helped to obtain biomass and community structure data this included undergraduate student tiffany hawkins and graduate students meagan hardegree amanda gordon and travis washburn elani morgan tamucc helped with data management appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104793 
25959,we present an algorithm that is well suited to find the linear layout of the multiple flow direction network directed acyclic graph for an efficient implicit computation of the erosion term in landscape evolution models the time complexity of the algorithm varies linearly with the number of nodes in the domain making it very efficient the resulting numerical scheme allows us to achieve accurate steady state solutions in conditions of high erosion rates leading to heavily dissected landscapes we also establish that contrary to single flow direction methods such as d8 d multiple flow direction method follows the theoretical prediction of the linear stability analysis and correctly captures the transition from smooth to the channelized regimes we finally show that the obtained numerical solutions follow the theoretical temporal variation of mean elevation keywords landscape evolution modeling linear ordering efficient numerical solver hydrogeomorphology 1 introduction the spatial organization of ridges and valleys on earth and other planets serve as the footprint of various processes such as weathering erosion sedimentation creep and tectonic uplift etc carr and malin 2000 chen et al 2014 fowler 2011 seybold et al 2018 tomasko et al 2005 their relative value controls the landscape profile from smooth to heavily dissected one with complex topographies landscape evolution models lems have been developed to explain the role of these processes on the formation and evolution of the earth s surface birnir et al 2001 coulthard 2001 tucker et al 2001 istanbulluoglu and bras 2005 koons 1989 perron et al 2012 roering 2008 smith and bretherton 1972 willgoose et al 1991 lems aim to simulate the dynamics of land surface over large spatial and temporal scales e g in the order of square km and million years where solving mass and momentum equations of water flow over the surface becomes computationally impractical at such large spatiotemporal scales the assumption of uniform precipitation over the domain and constant water velocity at every point in the direction of steepest descent is suitable leading to minimalist models of landscape evolution chen et al 2014 fowler 2011 as a result the water elevation term in lems is replaced by the drainage area thereby bypassing the need for employing water transport equations explicitly various flow direction methods have been developed to compute the drainage area and produce the flow network in a computationally inexpensive way qin et al 2007 costa cabral and burges 1994 freeman 1991 holmgren 1994 o callaghan and mark 1984 paik 2012 quinn et al 1991 tarboton 1997 bonetti et al 2020 the choice of flow direction method affects the type of flow network and calculation of the drainage area which in turn affects the erosion term in the lem and the accuracy of the solution implicit algorithms have been applied to solve the stream power equation the erosion term mostly for the single flow direction method barnes 2019 braun and willett 2013 these algorithms have allowed obtaining efficient simulations of landscape evolution in this work we build upon these contributions extending them to include multiple flow direction methods such as d which provide a better approximation of the specific drainage area as the surrogate of water flux at a point see section 2 compared to single flow direction methods such as d8 gallant and hutchinson 2011 goodchild et al 1996 pan et al 2004 wolock and mccabe 1995 contrarily to single flow direction methods multiple flow direction methods generate a tangled flow network across the domain which makes it difficult to find the linear ordering of nodes in the network necessary for efficient computation in the implicit solver for this reason lems using a multiple flow direction method have been solved explicitly in time hooshyar et al 2019 perron et al 2008 roering 2008 the explicit solver however poses a strict constraint on the time step and makes it computationally impractical to solve lem for highly dissected landscapes the paper is organized as follows we present the governing equations for the lem in the detachment limited condition in section 2 in section 3 we show the difference between network traversal in single and multiple flow direction networks for an efficient implicit computation of the erosion term in section 4 we describe the proposed algorithm with a worked out example using d method in section 5 we provide the results of various numerical experiments performed using the proposed algorithm and evaluate the accuracy of obtained results by comparing them with theoretical solutions we show the simulation results obtained using m8 method to indicate the scope of presented algorithm in appendix a we finally present the pseudocode for the d method to show the implementation of the algorithm in appendix b 2 governing equations we focus on the detachment limited conditions which assume the resistance to incision is the limiting condition for erosion rate rather than the hauling capacity of the channel to carry the material out of the domain so that the eroded material does not get redeposited within the domain howard 1994 izumi and parker 1995 under these premises the temporal dynamics of land surface elevation z is described as 1 z t d 2 z k a m z n u where d is the creep diffusion coefficient k is an erosion coefficient m and n are model parameters whipple and tucker 1999 u is the uplift rate and a is the specific drainage area the parameters d k m n and u are assumed constant while the equation for the variable a will be given in what follows the first term on the right hand side rhs of equation 1 is the sediment flux due to soil creep the assumption of soil creep flux being proportional to the topographic gradient recasts this term as a linear diffusion term culling 1960 1963 the second term on the rhs of equation 1 represents the sediment flux due to water erosion in detachment limited condition it is usually supposed to be proportional to the energy expenditure rate of the stream giving it the form of a nonlinear nonlocal sink term kirkby 1971 seidl and dietrich 1992 seidl et al 1994 perron et al 2008 the nonlocality in erosion term is due to the presence of the specific drainage area which makes the boundary conditions crucial in this model tectonic uplift is the external forcing acting beneath the surface which is modeled as a constant source term on the rhs of equation 1 as shown in fig 1 the total drainage area a is the horizontal projection of the area that flows to a finite portion of the contour line correspondingly the specific drainage area a for a point in the domain is defined for a point as lim w 0 a w where w is the length of contour line passing through that point bonetti et al 2018 gallant and hutchinson 2011 and a is the corresponding contributing area see fig 1 as shown in bonetti et al 2018 the specific drainage area equation is 2 a z z 1 since the negative gradient vector points toward the steepest descent the flow velocity direction is parallel opposite in sign to the gradient vector as written in equation 2 this is apparent from the erosion term in equation 1 where the slope of flow direction at a point in the domain is the magnitude of the negative gradient vector of the elevation field for a domain with typical length l the two coupled pdes are non dimensionalized in bonetti et al 2020 to obtain a dimensionless quantity 3 c i k l m n d n u 1 n whose value indicates the tendency to form channels low value indicates a smooth profile while high value implies a dissected landscape for this reason we refer to it as the channelization index equations 1 and 2 form a closed system of nonlinear partial differential equations to be solved with suitable initial and boundary conditions these equations can be solved analytically in a special case m n 1 for non dissected geometries in simple domains see bonetti et al 2020 and section 5 1 1 for channelized cases in complex topographies the governing equations must be solved numerically from a numerical point of view the existence of a flow network draining the entire landscape presents a way to employ it for the drainage area computation which is needed every time the landscape elevation surface is updated in the absence of this crucial information the direct numerical solution of equations 1 and 2 would be much more complicated thus different flow direction methods d8 m8 d etc have been used to provide a numerical approximation of the underlying flow network and compute the drainage area a assuming grid size to be adequate to give a good approximation of flow width the field of a is then approximated as a δ x where δ x is the grid size chirico et al 2005 tarboton 1997 thus indirectly solving equation 2 numerically over the discretized domain the flow network obtained at each time step is then used in the discretized form of equation 1 to update the elevation at each point in the domain 3 flow direction methods and an efficient implicit calculation the advantage of having no restrictions on the size of time step in implicit formulations is counteracted by an expensive computation of the coupled nonlinear equations at every time step langtangen and linge 2017 we address this issue by decoupling the equations by transforming the system into upper lower triangular we follow here the approach by barnes 2019 and braun and willett 2013 for single flow direction method which we extend to multiple flow direction method by traversing the network linear layout in a way that the number of operations to update elevations at a time step varies linearly with the total number of nodes the algorithm to construct this linear layout of the flow network for an efficient implicit computation of the erosion term depends on the connectivity among the nodes which in turn depends on the type of flow direction method in a single flow direction method the flow from a node can only go to one node downstream of it koons 1989 a node in the flow network therefore can have multiple donors upstream nodes draining to the node but must have a single receiver node to which this node is passing its flow this means that there exists a one to many relationship among nodes in the flow network looking from downstream to upstream with a unique path from each node to a node without any receiver sink node this layout is an anti arborescence or in tree tree which is a directed tree with root r such that there exists a unique path directed from any node n to the root r hoque and das 2016 fig 2 a shows the typical formation of anti arborescence trees with multiple outlets acting as the sink nodes of the respective trees the level of a node is defined as the number of direct edges between that node and the sink sink node forms the first level of the tree sink s donors are on the second level donors of sink s donors occupy the third level and so on in barnes 2019 braun and willett 2013 different linear layouts of the in tree structure depth first and breadth first traversal respectively have been employed to propose efficient implicit algorithms for a single flow direction network for a multiple flow direction method a node can have multiple donors as well as multiple receivers as shown in fig 2 b several flow lines can intersect and diverge in this paradigm which leads to the intermingling of various branches of the in tree structure tarboton 1997 this framework does not remain an in tree any more as it displays a directed acyclic graph pahl and damrath 2001 the definition of the level of a node becomes the maximum value of the number of direct edges between that node and the connected sinks in the example of fig 2 nodes 4 5 8 and 10 are at the third level in the single flow direction network as they can be identified as donors to donors of the sink node however in the case of multiple flow direction network only node 8 remains in that level with nodes 5 and 10 drifting to the fourth level and node 4 drifting to the fifth level thus there is a many to many relationship among the nodes of a multiple flow direction network compared to the one to many relationship in a single flow direction network the proposed algorithm renders the linear layout of the multiple flow direction network considering the possibility of multiple receivers as well as donors for a node during the network traversal this fundamental change in the node connectivity modifies the criteria of the linear ordering for an efficient implicit calculation of the erosion term in lems 4 the algorithm the algorithm the pseudocode is presented in appendix b for implicitly solving the erosion term in a multiple flow direction network starts by determining the donors of all nodes using the information about the receivers of the nodes section 4 1 it creates the queue linear ordering to process nodes and then implicitly updates elevations for all nodes in the domain using the erosion term section 4 2 the proposed algorithm does not depend on the positioning of the nodes and it can be used for any regular or irregular mesh d a multiple flow direction method is used to compute the total drainage area a and test this algorithm 4 1 determination of the donors the maximum number of donors possible for a node is the number of neighbors of that node this value is eight for the rectangular grid chosen in our numerical model we used a two dimensional matrix d n 8 to store the donors information for each node where n n is the number of nodes information about donors of each node is obtained from the receiver array which is assembled based on the node connectivity in a flow network the receiver array r n k is a two dimensional matrix where n is the number of nodes and k is the maximum number of receivers allowed by the flow direction method k 2 for d each node is checked to assess whether it is the receiver of its neighbor if it is that neighbor is stored as one of the donors of the node 4 2 ordering to process nodes the queue q is the one dimensional data structure that contains the traversal order of nodes in the network the sequence of nodes in q is such that the elevation values of a node s receivers are already updated before that node s elevation is updated implicitly table 1 presents the ordering of nodes q for the multiple flow direction network in fig 2 b this ordering allows computing elevation for every node in the domain implicitly with the time complexity of the algorithm varying linearly with the number of nodes at the beginning of each time step nodes without receivers sink nodes are added to the queue and are marked as processed we extract an element from the front of the queue and visit its donor if all receivers of that donor are already processed its elevation is updated implicitly by solving equation 4 z e p 1 2 z e p k a m z e p 1 2 n δ t where z e p is elevation value of the donor at previous time step z e p 1 2 is the updated elevation using the erosion term z e p 1 2 is the slope and δ t is the time step in multiple flow direction networks the slope at a node can be calculated as the vector sum of two or more directions in d the node being visited can have two flow receiving neighbors tarboton 1997 one in any cardinal direction with z c p 1 2 as the updated surface elevation and one in the adjacent diagonal direction with z d p 1 2 as the updated surface elevation if the grid spacing is δ x the downward slope z e p 1 2 is calculated as 5 z e p 1 2 z c p 1 2 z e p 1 2 δ x 2 z d p 1 2 z c p 1 2 δ x 2 1 2 after obtaining the slope the non linear equation 4 can be solved for a node using the root finding algorithms like brent s method or newton raphson method kiusalaas 2013 ram 2009 the node is then marked as processed and is pushed into the queue this step modifies the network traversal from the single flow direction network since a donor of the node in the queue cannot be immediately processed until its all other receivers are processed to get the final elevation of a node we first update the elevation by implicitly solving equation 4 using the proposed algorithm followed by implicitly updating diffusion and uplift as 6 z e p 1 z e p 1 2 d 2 z e p 1 u δ t where z e p 1 is the final updated elevation after a time step we have employed the five point stencil second order central difference formula for discretizing the laplace operator 2 becker and kaus 2017 this results in a 5 diagonal sparse matrix system which is solved using the lgmres algorithm baker et al 2005 the sorting of the nodes for the erosion term is attainable due to the loopless directed acyclic graph formed by the multiple flow direction methods the presented algorithm for an efficient implicit update of elevation values of the erosion term is the application of modified breadth first topological sort based on the principle of possible multiple donor receiver relationships with a queue kahn 1962 tarjan 1976 cormen et al 2009 this combination of the topological sort algorithm with the implicit solver speeds up the simulations by turning the system in upper lower triangular as has been done previously for the single flow direction method barnes 2019 braun and willett 2013 5 numerical results we performed numerical experiments for the square and rectangular domains with boundary nodes at fixed zero elevations we start with a flat surface with random spatial noise as the initial topography and update elevation values over the entire domain until the topographic steady state is reached willett and brandon 2002 we consider diffusion and erosion coefficients model parameters m and n and the uplift rate to be constant in space and time in this study we further considered the flow width is equal to the grid spacing and ignored any sub grid resolution features for large grid spacing the reader is referred to the methods discussed to accurately scale the flow width present in the sub grid scale of the discretized domain in tucker and hancock 2010 howard 1994 pelletier 2010 2012 5 1 code and solution verification we compared numerical solutions to the analytical solutions as a part of code verification and computed the observed level of accuracy for the solution verification oberkampf and roy 2010 roache 1998 roy 2005 5 1 1 code verification with analytical solution for a semi infinite domain of width l l with parameters m n 1 the steady state analytical solution can be obtained following bonetti et al 2020 assuming that the elevation decreases monotonically on the either side of divide in 1d transect and defining x x l 2 equation 1 at steady state becomes 7 d 2 z k a z u 0 equation 2 yields a x in 1d which gives the final form of equation 7 as 8 d z k x z u 0 with the boundary conditions z x 0 0 and z x 0 z o at the divide equation 8 is solved as 9 z x z o u x 2 2 d f q p 1 1 3 2 2 k x 2 2 d 10 z x 2 u 2 d k d a w k x 2 2 d where f q p and d a w are the generalized hypergeometric function and dawson function respectively equation 9 gives the symmetric unchannelized hillslope profile for width l with divide in the middle to compare the simulation results with the analytical solution we considered a rectangular domain with a high aspect ratio l y l x 5 a steady state solution was obtained with the presented algorithm for c i 10 20 40 and 55 we compared the computed mean elevation profile along the length of the domain neglecting the extreme sections with the analytical solution given by equation 9 the mean elevation profile along the length resembles the analytical profile until the first channel instability occurs fig 3 e and f only after the first channelization the mean elevation profile starts deviating from the analytical solution as expected fig 3 g and h 5 1 2 solution verification the proposed algorithm is theoretically first order accurate in space as well as time if high accuracy is required a high order scheme such as the crank nicolson temporal method can be employed to get the second order temporal accuracy langtangen and linge 2017 the finite difference discretization used in this study may not be suitable for the cases where discontinuities e g knickpoint migration exist in the solution moin 2010 toro 2009 under such circumstances other numerical schemes such as finite volume method may be a better choice campforts and govers 2015 in the present study however the governing equations contain a linear diffusion term representing soil creep this prevents the formation of singularities in the solution and does not lead to detrimental numerical errors in the model to test the accuracy of our solutions we decreased the grid spacing keeping model parameters and boundary conditions same and observed the change in the numerical error as well as the spatial patterns in the steady state landscape profiles solutions for two meshes m 1 and m 2 with grid spacing δ x and δ x 2 respectively can be written as f 1 f e x a c t o δ x p and f 2 f e x a c t o δ x 2 p expanding o δ x p and o δ x 2 p terms and neglecting higher order terms these equations can be written as 11 f 1 f e x a c t c p δ x p f 2 f e x a c t c p δ x 2 p taking the difference of these two equations and taking the logarithm on both sides gives 12 log ε p log δ x c where ε f 1 f 2 pseudo error and c c is a constant this means that the slope of linear plot of pseudo error versus grid spacing gives the order of accuracy of implementation in the spatial convergence test we considered four grid spacing δ x 0 5 δ x 2 δ x 4 and δ x 8 for a square domain side length 20 m for c i 62 fig 4 a b c we computed pseudo errors using mean elevation as a metric for these cases and obtained the best fit line on a scatter plot of grid spacing vs pseudo error as can be observed in fig 4 e a reduction in spatial resolution increase in grid spacing elevates the numerical pseudo error this decrease in the accuracy of the model solutions based on grid spacing is independent of the flow direction method employed for determining the specific drainage area the slope of the best fit line is 0 98 close to one which shows the observed level of accuracy from the implementation is first order and hence follows the theoretical predictions normalized hypsometric curves for the four cases were found to be in good agreement indicating that the proportion of land at various levels remains unaltered in the spatial convergence test fig 4 d 5 2 single vs multiple flow direction method and the first channelization linear stability analysis on the steady state analytical solution equation 9 performed by bonetti et al 2020 shows that the first channel instability occurs for c i 37 here we focused on the initiation of the first channel for a rectangular domain with a high aspect ratio l y l x 5 using single and multiple flow direction methods d8 and d respectively we also analyzed the steady state landscape profiles for different values of c i using d8 and d flow direction methods for d8 the implementation of braun and willett 2013 algorithm in landlab was used to compute the erosive term in the solver hobley et al 2017 while for d we used our proposed algorithm in close agreement with the theoretical analysis the first channel instability was found to occur at c i 35 for d method while the first channel was observed only around c i 90 for d8 method fig 5 a d we further compared the slope of the mean elevation profile along the length at the boundary for both the flow direction methods with the analytical solution for the unchannelized case equation 10 as seen in fig 5 e the slope starts deviating for d when the first channel instability occurs at c i around 35 while it occurs around c i 90 for d8 this indicates that the transition from smooth to dissected landscape is not captured well by d8 method our results parallel the conclusion of the numerical investigation in gallant and hutchinson 2011 where the theoretical values of a obtained from equation 2 are compared with the approximated values applying different flow direction methods such as d8 d and demon for simple geometries d8 especially gives poor results whereas d most accurately approximates a on hillslopes gallant and hutchinson 2011 this clearly shows the inadequacy of the single flow direction method in cases where a good approximation of the specific drainage area is needed these results are crucial for the efficient algorithms that are employed in hydrologic modeling and are dependent on flow direction methods for example the watershed marching algorithm to retrieve a watershed boundary which currently works with a single flow direction method due to the generated flow network being a tree compared to the multiple flow direction method which produces a directed acyclic graph celko 2012 haag et al 2018 haag and shokoufandeh 2019 however the above analysis shows that the achieved simplified flow network by the single flow direction method trades off the accuracy of the approximation of specific catchment area hence affecting the obtained elevation profiles this implies the requirement for modifications in these fast parallelizable algorithms for including the complicated flow networks generated by multiple flow direction methods for hydrologic modeling 5 3 mean elevation dynamics we further assessed the accuracy of the proposed algorithm by considering the transient evolution of mean elevation for which it is possible to have an analytical expression for a rectangular domain with fixed boundary elevations the mean elevation is given by 13 z 1 a z d s where represents closed surface integral and d s is the infinitesimal area element in the domain having area a using equation 1 and assuming m n 1 the temporal dynamics of mean elevation can be written as 14 d z d t 1 a d 2 z k a z u d s the divergence theorem allows us to write the first term on rhs of equation 14 as d z n d ω where n is the normal vector to the domain boundary ω this term gives the summation of the gradient of boundary nodes along the normal to the boundaries the second term on rhs of equation 14 can be expressed using orthogonal curvilinear coordinates u v where u directs along the contour lines and v along the stream lines gallant and hutchinson 2011 jeffreys and jeffreys 1999 the length elements along u and v are d w l u d u and d l l v d v respectively where l u x u 2 y u 2 and l v x v 2 y v 2 further the infinitesimal area d s in u v coordinate system is j d v d u where j is the jacobian defined as x u y v x v y u as a consequence of the orthogonality of u and v we have j l u l v using these relations the integral equation for a in u v coordinate system is derived in gallant and hutchinson 2011 as 15 a 1 l u v j d v and the slope is defined as 16 z 1 l v z v substituting these expressions in the second term on rhs of equation 14 we get 17 k a a z d s k a u v 1 l u v j d v 1 l v z v j d v d u using integration by parts equation 17 is further modified as 18 k a a z d s k a u z v j d v v 0 v b v v v j d v z d v d u where v 0 and v b are the along stream coordinates at the initiation and end of each stream line at the initiation points v v 0 we have a 0 and thus v j d v 0 at the end points v v b we have imposed the boundary condition of z 0 given these conditions the first term in the integrand on rhs of equation 18 is zero for any stream line u z v j d v v 0 v b 0 which simplifies the equation as 19 k a a z d s k u v z j d v d u k z for a high value of c i d 0 the contribution from the first term on rhs of equation 14 is negligible as the slope at boundary decreases on increasing the value of c i fig 5 e which makes the temporal variation of mean elevation as 20 d z d t k z u the solution is 21 u k z t u k z o e k t where z o is the mean elevation at time t 0 z t is the value at any time t thus the landscape reaches steady state with mean elevation value reaching u k fig 6 a we considered a square domain side length 100 m u k 0 02 c i 10 6 to compare the temporal variation of mean elevation obtained from the numerical solutions with the derived analytical expression we simulated the numerical model starting from different initial values of mean elevation z o and observed the temporal trajectory as shown in fig 6 b the steady state mean elevation for the numerical algorithm reaches the value of u k for any z o there is also a good match between the numerical and analytical trajectories of mean elevation indicating accurate transient solutions provided by the presented numerical algorithm using d method 5 4 high values of c i a major issue with the explicit solvers using a multiple flow direction method is the limitation on the time step size for the erosion term as per the stability criteria this limitation poses a practicality constraint on obtaining numerical solutions for the high values of c i our algorithm resolves this issue by using an efficient implicit computation of the erosion term which does not impose any restrictions on the maximal time step value for this part of the model to illustrate this point the solver was employed to get steady state landscape profiles for c i 50 to c i 50 000 fig 7 represents the steady state solutions for the rectangular domain l y l x 5 along with the variation of change in mean elevation value over consecutive time steps shown in respective insets for different values of c i increasingly complex channel forms are obtained for high values of c i bonetti et al 2020 after the initial period of channel initiation change in the mean elevation decreases smoothly until topographic steady state is reached indicating the absence of any numerical instability engendered by the proposed algorithm these results demonstrate high efficiency and robustness of the solver for a varied range of parameter values 6 conclusion and discussion extending the interesting contributions of barnes 2019 braun and willett 2013 we proposed an efficient algorithm for the multiple flow direction network to compute implicitly the erosion term of equation 1 in the numerical model of the detachment limited landscape evolution dynamics the lack of constraint on the time step for updating the elevation by the erosion term offers a way to obtain accurate steady state solutions for the wide range of c i in particular the numerical solutions obtained closely follow the theoretical predictions of channel instability when approximating a using d method the mean elevation dynamics obtained by the numerical solution is also in good agreement with the theoretical analysis the discrete description of the surface elevation for the numerical study is done either using a regular grid in this work or vector based triangular irregular network tin ivanov et al 1029 qu et al 2014 pilesjö and hasan 2014 based on the grid type different single and multiple flow direction methods have been proposed for computing the specific drainage area since the present algorithm for the erosion term depends only on the connectivity among the nodes in the flow direction networks rather than their spatial positions it is adaptable to any discrete mesh as a part of future work the algorithm can be employed to examine the effects of grid representations and flow direction methods on the computed a and obtained numerical steady state elevation profiles by comparing them with the theoretical benchmarks presented in this work instead of using flow direction methods to compute the specific drainage area recent contributions directly solve specific drainage area equation 2 for a raster digital elevation model qin et al 2017 lem however requires calculating the specific drainage area at every time step making the approach computationally expensive the water flow equation can also be made more detailed than our minimalist model including for example water diffusion over the landscape fowler 2011 multiple flow direction methods such as m8 method have been proposed for such cases qin et al 2007 quinn et al 1991 the problem we discussed in this work finding the linear ordering of nodes in multiple flow direction networks is a part of the so called task scheduling problems where the edges of the graph represent dependency between the tasks deep et al 2017 in the case of the lem that is the connection of one node to another based on the flow distribution the topological sorting algorithms for ordering these dependencies in directed acyclic graphs are getting more attention these days due to the increase in computational requirements in various fields and the development of high performance infrastructure starting from kahn s algorithm kahn 1962 there have been several algorithms that have been developed such as depth first search for finding two edge disjoint spanning trees in tarjan 1976 a parallel algorithm by ma et al 1997 a dynamic algorithm for random dag by pearce and kelly 2007 etc we are currently working to link our results with those of optimal channel networks banavar et al 2001 rinaldo et al 2014 as well as optimal transport problems bergamaschi et al 2019 and extend the algorithm to efficiently simulate the vascularization and branching problems in 3d domains declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge support from the us national science foundation nsf grants ear 1331846 and ear 1338694 and bp through the carbon mitigation initiative cmi at princeton university a p and m h also acknowledge the support from the princeton institute for international and regional studies piirs and the princeton environmental institute pei the authors are pleased to acknowledge that the simulations presented in this article were performed on computational resources managed and supported by princeton research computing a consortium of groups including the princeton institute for computational science and engineering picscie and the office of information technology s high performance computing center and visualization laboratory at princeton university well commented source code and the simulation results discussed in the paper are available at https github com shashankanand1996 lem appendix a m8 flow direction method in the mathematical formulation of lem we assumed that the water goes in the direction of steepest descent as indicated by equation 2 d8 single and d multiple flow direction methods follow the same concept with d splitting the flow between neighboring receivers only when the flow direction does not coincide with directions pointing toward neighbors cardinal and diagonal directions in a rectangular grid we therefore kept the discussion in the main text up to these methods to compare numerical results with analytical predictions using governing equations 1 and 2 other multiple flow direction methods such as m8 method distribute the flow to all downstream neighbors from a node where the proportion of flow is decided based on various matrices slope proportion some power of slope proportion etc qin et al 2007 quinn et al 1991 from the numerical point of view the presented algorithm can be applied to any multiple flow direction method to show the scope of the presented study we applied the algorithm using m8 method to obtain steady state solutions for a square domain fig a 8 the flow proportion received by the downstream neighbors of a node was decided based on slope proportion in the landlab environment hobley et al 2017 as shown in fig a 8 the increasing value of c i from 125 to 250 amplifies the channelization in the square domain as expected figure a 8 simulation results using m8 flow direction method for computing a in a square domain of side length 50 m with 1 m grid spacing and parameters m n 1 0 d 5 0 10 5 m 2 year 1 u 1 0 10 3 m year 1 brown ridge blue valley a c i 125 with average δ t 8896 years until steady state is reached at 6 95 10 6 years b c i 250 with average δ t 3334 years until steady state is reached at 4 53 10 6 years figure a 8 appendix b pseudocode the details about the implementation of the proposed algorithm are presented in this appendix the algorithm is written for serial programming as a python function that is compatible with the modeling environment provided by landlab hobley et al 2017 by keeping track of nodes at the same level it can be easily extended for parallel programming algorithm 1 generates the queue q for the input flow network and employs algorithm 2 to update elevation for any node implicitly using the erosion term d n 8 is a two dimensional array that is used to store the donors information for each node where n is the number of nodes another way of doing this is using the adjacency list which has the space complexity of o v e where v is the number of vertices nodes and e is the total number of edges in the network cormen et al 2009 this approach is extremely useful when the number of neighbors is large and the graph is sparse s is the flag used in receiver array r to indicate sink nodes at each time step a one dimensional array z n 1 is employed to store elevation values for all the nodes in row major order i e if the x coordinate of the node is i and y coordinate is j its location in z is n c j i n c is length and n r is width of the rectangular grid a n 1 is the one dimensional array that stores the specific drainage area in row major order for each node in the domain in the queue a new element is inserted at the end q p u s h and the first element is deleted q p o p following first in first out order fifo cormen et al 2009 we use p n 1 a boolean array to mark if the node has been processed or not one indicating the processed node this array helps to identify if the elevation of a node being visited can be updated implicitly or not this step is necessary as nodes in a multiple flow direction network have multiple receivers image 2 image 3 
25959,we present an algorithm that is well suited to find the linear layout of the multiple flow direction network directed acyclic graph for an efficient implicit computation of the erosion term in landscape evolution models the time complexity of the algorithm varies linearly with the number of nodes in the domain making it very efficient the resulting numerical scheme allows us to achieve accurate steady state solutions in conditions of high erosion rates leading to heavily dissected landscapes we also establish that contrary to single flow direction methods such as d8 d multiple flow direction method follows the theoretical prediction of the linear stability analysis and correctly captures the transition from smooth to the channelized regimes we finally show that the obtained numerical solutions follow the theoretical temporal variation of mean elevation keywords landscape evolution modeling linear ordering efficient numerical solver hydrogeomorphology 1 introduction the spatial organization of ridges and valleys on earth and other planets serve as the footprint of various processes such as weathering erosion sedimentation creep and tectonic uplift etc carr and malin 2000 chen et al 2014 fowler 2011 seybold et al 2018 tomasko et al 2005 their relative value controls the landscape profile from smooth to heavily dissected one with complex topographies landscape evolution models lems have been developed to explain the role of these processes on the formation and evolution of the earth s surface birnir et al 2001 coulthard 2001 tucker et al 2001 istanbulluoglu and bras 2005 koons 1989 perron et al 2012 roering 2008 smith and bretherton 1972 willgoose et al 1991 lems aim to simulate the dynamics of land surface over large spatial and temporal scales e g in the order of square km and million years where solving mass and momentum equations of water flow over the surface becomes computationally impractical at such large spatiotemporal scales the assumption of uniform precipitation over the domain and constant water velocity at every point in the direction of steepest descent is suitable leading to minimalist models of landscape evolution chen et al 2014 fowler 2011 as a result the water elevation term in lems is replaced by the drainage area thereby bypassing the need for employing water transport equations explicitly various flow direction methods have been developed to compute the drainage area and produce the flow network in a computationally inexpensive way qin et al 2007 costa cabral and burges 1994 freeman 1991 holmgren 1994 o callaghan and mark 1984 paik 2012 quinn et al 1991 tarboton 1997 bonetti et al 2020 the choice of flow direction method affects the type of flow network and calculation of the drainage area which in turn affects the erosion term in the lem and the accuracy of the solution implicit algorithms have been applied to solve the stream power equation the erosion term mostly for the single flow direction method barnes 2019 braun and willett 2013 these algorithms have allowed obtaining efficient simulations of landscape evolution in this work we build upon these contributions extending them to include multiple flow direction methods such as d which provide a better approximation of the specific drainage area as the surrogate of water flux at a point see section 2 compared to single flow direction methods such as d8 gallant and hutchinson 2011 goodchild et al 1996 pan et al 2004 wolock and mccabe 1995 contrarily to single flow direction methods multiple flow direction methods generate a tangled flow network across the domain which makes it difficult to find the linear ordering of nodes in the network necessary for efficient computation in the implicit solver for this reason lems using a multiple flow direction method have been solved explicitly in time hooshyar et al 2019 perron et al 2008 roering 2008 the explicit solver however poses a strict constraint on the time step and makes it computationally impractical to solve lem for highly dissected landscapes the paper is organized as follows we present the governing equations for the lem in the detachment limited condition in section 2 in section 3 we show the difference between network traversal in single and multiple flow direction networks for an efficient implicit computation of the erosion term in section 4 we describe the proposed algorithm with a worked out example using d method in section 5 we provide the results of various numerical experiments performed using the proposed algorithm and evaluate the accuracy of obtained results by comparing them with theoretical solutions we show the simulation results obtained using m8 method to indicate the scope of presented algorithm in appendix a we finally present the pseudocode for the d method to show the implementation of the algorithm in appendix b 2 governing equations we focus on the detachment limited conditions which assume the resistance to incision is the limiting condition for erosion rate rather than the hauling capacity of the channel to carry the material out of the domain so that the eroded material does not get redeposited within the domain howard 1994 izumi and parker 1995 under these premises the temporal dynamics of land surface elevation z is described as 1 z t d 2 z k a m z n u where d is the creep diffusion coefficient k is an erosion coefficient m and n are model parameters whipple and tucker 1999 u is the uplift rate and a is the specific drainage area the parameters d k m n and u are assumed constant while the equation for the variable a will be given in what follows the first term on the right hand side rhs of equation 1 is the sediment flux due to soil creep the assumption of soil creep flux being proportional to the topographic gradient recasts this term as a linear diffusion term culling 1960 1963 the second term on the rhs of equation 1 represents the sediment flux due to water erosion in detachment limited condition it is usually supposed to be proportional to the energy expenditure rate of the stream giving it the form of a nonlinear nonlocal sink term kirkby 1971 seidl and dietrich 1992 seidl et al 1994 perron et al 2008 the nonlocality in erosion term is due to the presence of the specific drainage area which makes the boundary conditions crucial in this model tectonic uplift is the external forcing acting beneath the surface which is modeled as a constant source term on the rhs of equation 1 as shown in fig 1 the total drainage area a is the horizontal projection of the area that flows to a finite portion of the contour line correspondingly the specific drainage area a for a point in the domain is defined for a point as lim w 0 a w where w is the length of contour line passing through that point bonetti et al 2018 gallant and hutchinson 2011 and a is the corresponding contributing area see fig 1 as shown in bonetti et al 2018 the specific drainage area equation is 2 a z z 1 since the negative gradient vector points toward the steepest descent the flow velocity direction is parallel opposite in sign to the gradient vector as written in equation 2 this is apparent from the erosion term in equation 1 where the slope of flow direction at a point in the domain is the magnitude of the negative gradient vector of the elevation field for a domain with typical length l the two coupled pdes are non dimensionalized in bonetti et al 2020 to obtain a dimensionless quantity 3 c i k l m n d n u 1 n whose value indicates the tendency to form channels low value indicates a smooth profile while high value implies a dissected landscape for this reason we refer to it as the channelization index equations 1 and 2 form a closed system of nonlinear partial differential equations to be solved with suitable initial and boundary conditions these equations can be solved analytically in a special case m n 1 for non dissected geometries in simple domains see bonetti et al 2020 and section 5 1 1 for channelized cases in complex topographies the governing equations must be solved numerically from a numerical point of view the existence of a flow network draining the entire landscape presents a way to employ it for the drainage area computation which is needed every time the landscape elevation surface is updated in the absence of this crucial information the direct numerical solution of equations 1 and 2 would be much more complicated thus different flow direction methods d8 m8 d etc have been used to provide a numerical approximation of the underlying flow network and compute the drainage area a assuming grid size to be adequate to give a good approximation of flow width the field of a is then approximated as a δ x where δ x is the grid size chirico et al 2005 tarboton 1997 thus indirectly solving equation 2 numerically over the discretized domain the flow network obtained at each time step is then used in the discretized form of equation 1 to update the elevation at each point in the domain 3 flow direction methods and an efficient implicit calculation the advantage of having no restrictions on the size of time step in implicit formulations is counteracted by an expensive computation of the coupled nonlinear equations at every time step langtangen and linge 2017 we address this issue by decoupling the equations by transforming the system into upper lower triangular we follow here the approach by barnes 2019 and braun and willett 2013 for single flow direction method which we extend to multiple flow direction method by traversing the network linear layout in a way that the number of operations to update elevations at a time step varies linearly with the total number of nodes the algorithm to construct this linear layout of the flow network for an efficient implicit computation of the erosion term depends on the connectivity among the nodes which in turn depends on the type of flow direction method in a single flow direction method the flow from a node can only go to one node downstream of it koons 1989 a node in the flow network therefore can have multiple donors upstream nodes draining to the node but must have a single receiver node to which this node is passing its flow this means that there exists a one to many relationship among nodes in the flow network looking from downstream to upstream with a unique path from each node to a node without any receiver sink node this layout is an anti arborescence or in tree tree which is a directed tree with root r such that there exists a unique path directed from any node n to the root r hoque and das 2016 fig 2 a shows the typical formation of anti arborescence trees with multiple outlets acting as the sink nodes of the respective trees the level of a node is defined as the number of direct edges between that node and the sink sink node forms the first level of the tree sink s donors are on the second level donors of sink s donors occupy the third level and so on in barnes 2019 braun and willett 2013 different linear layouts of the in tree structure depth first and breadth first traversal respectively have been employed to propose efficient implicit algorithms for a single flow direction network for a multiple flow direction method a node can have multiple donors as well as multiple receivers as shown in fig 2 b several flow lines can intersect and diverge in this paradigm which leads to the intermingling of various branches of the in tree structure tarboton 1997 this framework does not remain an in tree any more as it displays a directed acyclic graph pahl and damrath 2001 the definition of the level of a node becomes the maximum value of the number of direct edges between that node and the connected sinks in the example of fig 2 nodes 4 5 8 and 10 are at the third level in the single flow direction network as they can be identified as donors to donors of the sink node however in the case of multiple flow direction network only node 8 remains in that level with nodes 5 and 10 drifting to the fourth level and node 4 drifting to the fifth level thus there is a many to many relationship among the nodes of a multiple flow direction network compared to the one to many relationship in a single flow direction network the proposed algorithm renders the linear layout of the multiple flow direction network considering the possibility of multiple receivers as well as donors for a node during the network traversal this fundamental change in the node connectivity modifies the criteria of the linear ordering for an efficient implicit calculation of the erosion term in lems 4 the algorithm the algorithm the pseudocode is presented in appendix b for implicitly solving the erosion term in a multiple flow direction network starts by determining the donors of all nodes using the information about the receivers of the nodes section 4 1 it creates the queue linear ordering to process nodes and then implicitly updates elevations for all nodes in the domain using the erosion term section 4 2 the proposed algorithm does not depend on the positioning of the nodes and it can be used for any regular or irregular mesh d a multiple flow direction method is used to compute the total drainage area a and test this algorithm 4 1 determination of the donors the maximum number of donors possible for a node is the number of neighbors of that node this value is eight for the rectangular grid chosen in our numerical model we used a two dimensional matrix d n 8 to store the donors information for each node where n n is the number of nodes information about donors of each node is obtained from the receiver array which is assembled based on the node connectivity in a flow network the receiver array r n k is a two dimensional matrix where n is the number of nodes and k is the maximum number of receivers allowed by the flow direction method k 2 for d each node is checked to assess whether it is the receiver of its neighbor if it is that neighbor is stored as one of the donors of the node 4 2 ordering to process nodes the queue q is the one dimensional data structure that contains the traversal order of nodes in the network the sequence of nodes in q is such that the elevation values of a node s receivers are already updated before that node s elevation is updated implicitly table 1 presents the ordering of nodes q for the multiple flow direction network in fig 2 b this ordering allows computing elevation for every node in the domain implicitly with the time complexity of the algorithm varying linearly with the number of nodes at the beginning of each time step nodes without receivers sink nodes are added to the queue and are marked as processed we extract an element from the front of the queue and visit its donor if all receivers of that donor are already processed its elevation is updated implicitly by solving equation 4 z e p 1 2 z e p k a m z e p 1 2 n δ t where z e p is elevation value of the donor at previous time step z e p 1 2 is the updated elevation using the erosion term z e p 1 2 is the slope and δ t is the time step in multiple flow direction networks the slope at a node can be calculated as the vector sum of two or more directions in d the node being visited can have two flow receiving neighbors tarboton 1997 one in any cardinal direction with z c p 1 2 as the updated surface elevation and one in the adjacent diagonal direction with z d p 1 2 as the updated surface elevation if the grid spacing is δ x the downward slope z e p 1 2 is calculated as 5 z e p 1 2 z c p 1 2 z e p 1 2 δ x 2 z d p 1 2 z c p 1 2 δ x 2 1 2 after obtaining the slope the non linear equation 4 can be solved for a node using the root finding algorithms like brent s method or newton raphson method kiusalaas 2013 ram 2009 the node is then marked as processed and is pushed into the queue this step modifies the network traversal from the single flow direction network since a donor of the node in the queue cannot be immediately processed until its all other receivers are processed to get the final elevation of a node we first update the elevation by implicitly solving equation 4 using the proposed algorithm followed by implicitly updating diffusion and uplift as 6 z e p 1 z e p 1 2 d 2 z e p 1 u δ t where z e p 1 is the final updated elevation after a time step we have employed the five point stencil second order central difference formula for discretizing the laplace operator 2 becker and kaus 2017 this results in a 5 diagonal sparse matrix system which is solved using the lgmres algorithm baker et al 2005 the sorting of the nodes for the erosion term is attainable due to the loopless directed acyclic graph formed by the multiple flow direction methods the presented algorithm for an efficient implicit update of elevation values of the erosion term is the application of modified breadth first topological sort based on the principle of possible multiple donor receiver relationships with a queue kahn 1962 tarjan 1976 cormen et al 2009 this combination of the topological sort algorithm with the implicit solver speeds up the simulations by turning the system in upper lower triangular as has been done previously for the single flow direction method barnes 2019 braun and willett 2013 5 numerical results we performed numerical experiments for the square and rectangular domains with boundary nodes at fixed zero elevations we start with a flat surface with random spatial noise as the initial topography and update elevation values over the entire domain until the topographic steady state is reached willett and brandon 2002 we consider diffusion and erosion coefficients model parameters m and n and the uplift rate to be constant in space and time in this study we further considered the flow width is equal to the grid spacing and ignored any sub grid resolution features for large grid spacing the reader is referred to the methods discussed to accurately scale the flow width present in the sub grid scale of the discretized domain in tucker and hancock 2010 howard 1994 pelletier 2010 2012 5 1 code and solution verification we compared numerical solutions to the analytical solutions as a part of code verification and computed the observed level of accuracy for the solution verification oberkampf and roy 2010 roache 1998 roy 2005 5 1 1 code verification with analytical solution for a semi infinite domain of width l l with parameters m n 1 the steady state analytical solution can be obtained following bonetti et al 2020 assuming that the elevation decreases monotonically on the either side of divide in 1d transect and defining x x l 2 equation 1 at steady state becomes 7 d 2 z k a z u 0 equation 2 yields a x in 1d which gives the final form of equation 7 as 8 d z k x z u 0 with the boundary conditions z x 0 0 and z x 0 z o at the divide equation 8 is solved as 9 z x z o u x 2 2 d f q p 1 1 3 2 2 k x 2 2 d 10 z x 2 u 2 d k d a w k x 2 2 d where f q p and d a w are the generalized hypergeometric function and dawson function respectively equation 9 gives the symmetric unchannelized hillslope profile for width l with divide in the middle to compare the simulation results with the analytical solution we considered a rectangular domain with a high aspect ratio l y l x 5 a steady state solution was obtained with the presented algorithm for c i 10 20 40 and 55 we compared the computed mean elevation profile along the length of the domain neglecting the extreme sections with the analytical solution given by equation 9 the mean elevation profile along the length resembles the analytical profile until the first channel instability occurs fig 3 e and f only after the first channelization the mean elevation profile starts deviating from the analytical solution as expected fig 3 g and h 5 1 2 solution verification the proposed algorithm is theoretically first order accurate in space as well as time if high accuracy is required a high order scheme such as the crank nicolson temporal method can be employed to get the second order temporal accuracy langtangen and linge 2017 the finite difference discretization used in this study may not be suitable for the cases where discontinuities e g knickpoint migration exist in the solution moin 2010 toro 2009 under such circumstances other numerical schemes such as finite volume method may be a better choice campforts and govers 2015 in the present study however the governing equations contain a linear diffusion term representing soil creep this prevents the formation of singularities in the solution and does not lead to detrimental numerical errors in the model to test the accuracy of our solutions we decreased the grid spacing keeping model parameters and boundary conditions same and observed the change in the numerical error as well as the spatial patterns in the steady state landscape profiles solutions for two meshes m 1 and m 2 with grid spacing δ x and δ x 2 respectively can be written as f 1 f e x a c t o δ x p and f 2 f e x a c t o δ x 2 p expanding o δ x p and o δ x 2 p terms and neglecting higher order terms these equations can be written as 11 f 1 f e x a c t c p δ x p f 2 f e x a c t c p δ x 2 p taking the difference of these two equations and taking the logarithm on both sides gives 12 log ε p log δ x c where ε f 1 f 2 pseudo error and c c is a constant this means that the slope of linear plot of pseudo error versus grid spacing gives the order of accuracy of implementation in the spatial convergence test we considered four grid spacing δ x 0 5 δ x 2 δ x 4 and δ x 8 for a square domain side length 20 m for c i 62 fig 4 a b c we computed pseudo errors using mean elevation as a metric for these cases and obtained the best fit line on a scatter plot of grid spacing vs pseudo error as can be observed in fig 4 e a reduction in spatial resolution increase in grid spacing elevates the numerical pseudo error this decrease in the accuracy of the model solutions based on grid spacing is independent of the flow direction method employed for determining the specific drainage area the slope of the best fit line is 0 98 close to one which shows the observed level of accuracy from the implementation is first order and hence follows the theoretical predictions normalized hypsometric curves for the four cases were found to be in good agreement indicating that the proportion of land at various levels remains unaltered in the spatial convergence test fig 4 d 5 2 single vs multiple flow direction method and the first channelization linear stability analysis on the steady state analytical solution equation 9 performed by bonetti et al 2020 shows that the first channel instability occurs for c i 37 here we focused on the initiation of the first channel for a rectangular domain with a high aspect ratio l y l x 5 using single and multiple flow direction methods d8 and d respectively we also analyzed the steady state landscape profiles for different values of c i using d8 and d flow direction methods for d8 the implementation of braun and willett 2013 algorithm in landlab was used to compute the erosive term in the solver hobley et al 2017 while for d we used our proposed algorithm in close agreement with the theoretical analysis the first channel instability was found to occur at c i 35 for d method while the first channel was observed only around c i 90 for d8 method fig 5 a d we further compared the slope of the mean elevation profile along the length at the boundary for both the flow direction methods with the analytical solution for the unchannelized case equation 10 as seen in fig 5 e the slope starts deviating for d when the first channel instability occurs at c i around 35 while it occurs around c i 90 for d8 this indicates that the transition from smooth to dissected landscape is not captured well by d8 method our results parallel the conclusion of the numerical investigation in gallant and hutchinson 2011 where the theoretical values of a obtained from equation 2 are compared with the approximated values applying different flow direction methods such as d8 d and demon for simple geometries d8 especially gives poor results whereas d most accurately approximates a on hillslopes gallant and hutchinson 2011 this clearly shows the inadequacy of the single flow direction method in cases where a good approximation of the specific drainage area is needed these results are crucial for the efficient algorithms that are employed in hydrologic modeling and are dependent on flow direction methods for example the watershed marching algorithm to retrieve a watershed boundary which currently works with a single flow direction method due to the generated flow network being a tree compared to the multiple flow direction method which produces a directed acyclic graph celko 2012 haag et al 2018 haag and shokoufandeh 2019 however the above analysis shows that the achieved simplified flow network by the single flow direction method trades off the accuracy of the approximation of specific catchment area hence affecting the obtained elevation profiles this implies the requirement for modifications in these fast parallelizable algorithms for including the complicated flow networks generated by multiple flow direction methods for hydrologic modeling 5 3 mean elevation dynamics we further assessed the accuracy of the proposed algorithm by considering the transient evolution of mean elevation for which it is possible to have an analytical expression for a rectangular domain with fixed boundary elevations the mean elevation is given by 13 z 1 a z d s where represents closed surface integral and d s is the infinitesimal area element in the domain having area a using equation 1 and assuming m n 1 the temporal dynamics of mean elevation can be written as 14 d z d t 1 a d 2 z k a z u d s the divergence theorem allows us to write the first term on rhs of equation 14 as d z n d ω where n is the normal vector to the domain boundary ω this term gives the summation of the gradient of boundary nodes along the normal to the boundaries the second term on rhs of equation 14 can be expressed using orthogonal curvilinear coordinates u v where u directs along the contour lines and v along the stream lines gallant and hutchinson 2011 jeffreys and jeffreys 1999 the length elements along u and v are d w l u d u and d l l v d v respectively where l u x u 2 y u 2 and l v x v 2 y v 2 further the infinitesimal area d s in u v coordinate system is j d v d u where j is the jacobian defined as x u y v x v y u as a consequence of the orthogonality of u and v we have j l u l v using these relations the integral equation for a in u v coordinate system is derived in gallant and hutchinson 2011 as 15 a 1 l u v j d v and the slope is defined as 16 z 1 l v z v substituting these expressions in the second term on rhs of equation 14 we get 17 k a a z d s k a u v 1 l u v j d v 1 l v z v j d v d u using integration by parts equation 17 is further modified as 18 k a a z d s k a u z v j d v v 0 v b v v v j d v z d v d u where v 0 and v b are the along stream coordinates at the initiation and end of each stream line at the initiation points v v 0 we have a 0 and thus v j d v 0 at the end points v v b we have imposed the boundary condition of z 0 given these conditions the first term in the integrand on rhs of equation 18 is zero for any stream line u z v j d v v 0 v b 0 which simplifies the equation as 19 k a a z d s k u v z j d v d u k z for a high value of c i d 0 the contribution from the first term on rhs of equation 14 is negligible as the slope at boundary decreases on increasing the value of c i fig 5 e which makes the temporal variation of mean elevation as 20 d z d t k z u the solution is 21 u k z t u k z o e k t where z o is the mean elevation at time t 0 z t is the value at any time t thus the landscape reaches steady state with mean elevation value reaching u k fig 6 a we considered a square domain side length 100 m u k 0 02 c i 10 6 to compare the temporal variation of mean elevation obtained from the numerical solutions with the derived analytical expression we simulated the numerical model starting from different initial values of mean elevation z o and observed the temporal trajectory as shown in fig 6 b the steady state mean elevation for the numerical algorithm reaches the value of u k for any z o there is also a good match between the numerical and analytical trajectories of mean elevation indicating accurate transient solutions provided by the presented numerical algorithm using d method 5 4 high values of c i a major issue with the explicit solvers using a multiple flow direction method is the limitation on the time step size for the erosion term as per the stability criteria this limitation poses a practicality constraint on obtaining numerical solutions for the high values of c i our algorithm resolves this issue by using an efficient implicit computation of the erosion term which does not impose any restrictions on the maximal time step value for this part of the model to illustrate this point the solver was employed to get steady state landscape profiles for c i 50 to c i 50 000 fig 7 represents the steady state solutions for the rectangular domain l y l x 5 along with the variation of change in mean elevation value over consecutive time steps shown in respective insets for different values of c i increasingly complex channel forms are obtained for high values of c i bonetti et al 2020 after the initial period of channel initiation change in the mean elevation decreases smoothly until topographic steady state is reached indicating the absence of any numerical instability engendered by the proposed algorithm these results demonstrate high efficiency and robustness of the solver for a varied range of parameter values 6 conclusion and discussion extending the interesting contributions of barnes 2019 braun and willett 2013 we proposed an efficient algorithm for the multiple flow direction network to compute implicitly the erosion term of equation 1 in the numerical model of the detachment limited landscape evolution dynamics the lack of constraint on the time step for updating the elevation by the erosion term offers a way to obtain accurate steady state solutions for the wide range of c i in particular the numerical solutions obtained closely follow the theoretical predictions of channel instability when approximating a using d method the mean elevation dynamics obtained by the numerical solution is also in good agreement with the theoretical analysis the discrete description of the surface elevation for the numerical study is done either using a regular grid in this work or vector based triangular irregular network tin ivanov et al 1029 qu et al 2014 pilesjö and hasan 2014 based on the grid type different single and multiple flow direction methods have been proposed for computing the specific drainage area since the present algorithm for the erosion term depends only on the connectivity among the nodes in the flow direction networks rather than their spatial positions it is adaptable to any discrete mesh as a part of future work the algorithm can be employed to examine the effects of grid representations and flow direction methods on the computed a and obtained numerical steady state elevation profiles by comparing them with the theoretical benchmarks presented in this work instead of using flow direction methods to compute the specific drainage area recent contributions directly solve specific drainage area equation 2 for a raster digital elevation model qin et al 2017 lem however requires calculating the specific drainage area at every time step making the approach computationally expensive the water flow equation can also be made more detailed than our minimalist model including for example water diffusion over the landscape fowler 2011 multiple flow direction methods such as m8 method have been proposed for such cases qin et al 2007 quinn et al 1991 the problem we discussed in this work finding the linear ordering of nodes in multiple flow direction networks is a part of the so called task scheduling problems where the edges of the graph represent dependency between the tasks deep et al 2017 in the case of the lem that is the connection of one node to another based on the flow distribution the topological sorting algorithms for ordering these dependencies in directed acyclic graphs are getting more attention these days due to the increase in computational requirements in various fields and the development of high performance infrastructure starting from kahn s algorithm kahn 1962 there have been several algorithms that have been developed such as depth first search for finding two edge disjoint spanning trees in tarjan 1976 a parallel algorithm by ma et al 1997 a dynamic algorithm for random dag by pearce and kelly 2007 etc we are currently working to link our results with those of optimal channel networks banavar et al 2001 rinaldo et al 2014 as well as optimal transport problems bergamaschi et al 2019 and extend the algorithm to efficiently simulate the vascularization and branching problems in 3d domains declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge support from the us national science foundation nsf grants ear 1331846 and ear 1338694 and bp through the carbon mitigation initiative cmi at princeton university a p and m h also acknowledge the support from the princeton institute for international and regional studies piirs and the princeton environmental institute pei the authors are pleased to acknowledge that the simulations presented in this article were performed on computational resources managed and supported by princeton research computing a consortium of groups including the princeton institute for computational science and engineering picscie and the office of information technology s high performance computing center and visualization laboratory at princeton university well commented source code and the simulation results discussed in the paper are available at https github com shashankanand1996 lem appendix a m8 flow direction method in the mathematical formulation of lem we assumed that the water goes in the direction of steepest descent as indicated by equation 2 d8 single and d multiple flow direction methods follow the same concept with d splitting the flow between neighboring receivers only when the flow direction does not coincide with directions pointing toward neighbors cardinal and diagonal directions in a rectangular grid we therefore kept the discussion in the main text up to these methods to compare numerical results with analytical predictions using governing equations 1 and 2 other multiple flow direction methods such as m8 method distribute the flow to all downstream neighbors from a node where the proportion of flow is decided based on various matrices slope proportion some power of slope proportion etc qin et al 2007 quinn et al 1991 from the numerical point of view the presented algorithm can be applied to any multiple flow direction method to show the scope of the presented study we applied the algorithm using m8 method to obtain steady state solutions for a square domain fig a 8 the flow proportion received by the downstream neighbors of a node was decided based on slope proportion in the landlab environment hobley et al 2017 as shown in fig a 8 the increasing value of c i from 125 to 250 amplifies the channelization in the square domain as expected figure a 8 simulation results using m8 flow direction method for computing a in a square domain of side length 50 m with 1 m grid spacing and parameters m n 1 0 d 5 0 10 5 m 2 year 1 u 1 0 10 3 m year 1 brown ridge blue valley a c i 125 with average δ t 8896 years until steady state is reached at 6 95 10 6 years b c i 250 with average δ t 3334 years until steady state is reached at 4 53 10 6 years figure a 8 appendix b pseudocode the details about the implementation of the proposed algorithm are presented in this appendix the algorithm is written for serial programming as a python function that is compatible with the modeling environment provided by landlab hobley et al 2017 by keeping track of nodes at the same level it can be easily extended for parallel programming algorithm 1 generates the queue q for the input flow network and employs algorithm 2 to update elevation for any node implicitly using the erosion term d n 8 is a two dimensional array that is used to store the donors information for each node where n is the number of nodes another way of doing this is using the adjacency list which has the space complexity of o v e where v is the number of vertices nodes and e is the total number of edges in the network cormen et al 2009 this approach is extremely useful when the number of neighbors is large and the graph is sparse s is the flag used in receiver array r to indicate sink nodes at each time step a one dimensional array z n 1 is employed to store elevation values for all the nodes in row major order i e if the x coordinate of the node is i and y coordinate is j its location in z is n c j i n c is length and n r is width of the rectangular grid a n 1 is the one dimensional array that stores the specific drainage area in row major order for each node in the domain in the queue a new element is inserted at the end q p u s h and the first element is deleted q p o p following first in first out order fifo cormen et al 2009 we use p n 1 a boolean array to mark if the node has been processed or not one indicating the processed node this array helps to identify if the elevation of a node being visited can be updated implicitly or not this step is necessary as nodes in a multiple flow direction network have multiple receivers image 2 image 3 
