index,text
25875,recent technological advances in collecting data on emission sources meteorological conditions and concentration of air pollutants in urban areas offer invaluable opportunities for the better understanding of air quality problems however processing large sets of data to extract statistically valid evidence poses many challenges from both the conceptual and technical viewpoints air quality data acquisition cleaning and authentication are necessary and crucial preliminary phases to support descriptive predictive and prescriptive models and to ensure that aggregated and high quality information is delivered to the central and local governments decision makers and citizens automated software tools can facilitate drawing conclusions based on the information contained in the data limiting subjective judgment and providing repeatability however the costly state of the art software applications developed by major vendors are inaccessible to many cities and townships in the developing world moreover their usage creates dependency on proprietary solutions which can hinder the possibility of evolving the data processing and analysis protocols we present an open source web application for air quality data analysis and visualization called aire based on the r statistical framework and shiny web package aire has been developed in collaboration with the colombian environmental authorities and implements best practices validated by experts in air quality we believe that the process of developing aire was extremely valuable with the ultimate purpose of supporting cities in air quality management while strengthening local capabilities to improve urban air pollution this open access tool simplifies and makes air quality data analysis and visualization accessible with the desirable effect of removing ownership costs fostering appropriation by non expert users and ultimately promoting informed decision making for the general public and the local government authorities we present the performance of this tool over a series of examples of open data collected by the air quality monitoring network of bogotá colombia keywords aire air quality software open source r shiny web application 1 introduction 1 1 context poor air quality is a growing global concern gulia et al 2015 baklanov et al 2016 recent evidence shows that 8 out of 10 people worldwide are exposed to air pollution levels that are considered harmful to human health landrigan et al 2018 air quality management in many cities around the world calls for the deployment of surveillance systems to monitor air pollution levels such systems are normally composed by a set of stations to measure ambient air pollutant concentrations as well as meteorological variables these networks vary widely in reliability and representation influenced by factors such as topography demography meteorology and socioeconomic development air pollution in the world s megacities 1994 due to the many complexities of air quality monitoring large cities often opt for commercial software packages that help them configure administrate and maintain air monitoring infrastructures as well as creating reports for the decision makers in environmental policy however most smaller cities and towns in less developed countries cannot afford such products instead they generate their reports using very basic tools which are unsuitable for database management and big data analysis furthermore there is often lack of expertise and adequate capabilities in local administrations to ensure the reliability of air monitoring networks data cleaning and the subsequent analyses with the increasing availability and popularity of cheaper air quality measurement sensors gulia et al 2020 the amount of data to be analyzed will grow and will call more and more for advanced and automated software support the impact of poor air quality also has a socioeconomic class facet as according to the who in low and middle income countries approximately 98 of city inhabitants are living in environmental conditions that do not meet the guidelines for air pollutant exposure in high income countries this percentage yet still very high reduces to 56 osseiran and chriscaden 2016 among the reasons for this difference are the unavailability of reliable air monitoring infrastructure and or the lack of analytical capabilities to develop effective public policies without the support offered by automation even guaranteeing compliance with existing air quality standards and regulations for data reporting becomes difficult for example in colombia the subject of our case study the responsibility of air quality management is assigned to both local and regional environmental authorities mostly decentralized and independent agencies that have the responsibility to administrate and operate air quality monitoring networks such independence may lead to a wide range of interpretations on data treatment and reporting guidelines suggested by the national government creating the necessity for integrated software solutions and data analysis approaches 1 2 available software packages for air quality analysis currently there are several software applications capable of reading structured data sets of air quality measurements and performing statistical or visual analysis we can distinguish them in two classes proprietary applications with commercial purposes and non commercial software which includes many academic developments commercial software application usually have a friendly user interface and can perform all the necessary steps for different types of analyses these tools can load the data sets or even directly interface with the station of a monitoring network to obtain the data perform pre configured and user defined analyses and create detailed reports of air quality for instance the israeli software suite envida envista ltd and envista air 2007 provides an integrated solution to retrieve store and analyze measurements collected by an air quality measurement network it can retrieve the data from a database and perform analyses such as wind roses histograms correlations in different aggregation of time and dynamic gis visualizations another program is the german software ambient air quality monitoring kisters which is intended for public authorities and can connect to different monitoring networks to provide local regional or national level data managing and analysis the main advantage offered by commercial products is the quick access to many types of analyses to support decision making they automate error prone data manipulation tasks and reduce subjective judgment in data analysis a clear disadvantage is of course the cost since these software packages are usually very expensive to acquire and many governmental entities especially in low income regions may not be able to afford them moreover the usage of these commercial solutions may create dependence on the vendors which might limit the creation of technical know how that goes beyond the tool s defined functionality and hinder the possibility of autonomously adapting to new needs and conditions there are several non commercial solutions that offer nice and intuitive graphical user interface and visualization features for instance giovanni a web accessible tool developed by nasa goddard earth sciences data and information services center prados et al 2010 allows loading and analyzing many diverse types of open datasets including air quality measurements collected by satellites and on ground stations giovanni provides time series and spatial visualization of data on maps moreover it allows exporting data in various formats including the compressed keyhole markup language kml for reusing them in 3d google earth visualizations the kml data format is also used by the solution described in chen 2019 where the authors propose an application that allows visualizing in near real time air quality data they describe in chen 2019 a software residing at a server side which can be configured to continuously gather air quality open data and structure it into kml files ready to be consumed by clients using google earth examples of heat map visualizations are shown in chen 2019 for china air quality data another software application that aims at providing intuitive visualizations of air quality data is proposed in lu et al 2017 a web interface is provided which allows displaying pollution at various levels from country to regions cities down to monitoring stations at each level the time dimension of the data is summarized by a sunburst like chart which can be zoomed for changing the granularity level the software application allows visualizing the aqi index as well as the concentration of user selected pollutants some other studies in the literature report about software applications that can load and visualize air quality measurements together with predictions generated with various types of machine learning approaches for instance ofoegbu et al 2014 proposes to a software application that can connect to a database containing the air quality data and generate visualizations for pollutant time series and their predictions as well as aqi predictions using artificial neuronal networks and decision trees type of models in tomić et al 2014 the authors present a client server solution that is capable of acquiring measurement through a tcp ip network and display visualizations of the collected data together with prediction obtained with an auto regressive moving average model the solution includes a client for devices running the android operating system so that the data visualization services can be accessed remotely several solutions for air quality analysis and visualizations based on the statistical software r have also been proposed the open source r package openair is currently the most popular software for air quality data analysis carslaw and ropkins 2012 the package is developed at king s college in london by the entity in charge of the city s air monitoring network it is capable of reading a set of air quality data and performing a variety of possible analyses in particular openair provides extensive data filtering features such as sorting by weekdays seasons daylight hours etc creating wind rose time series and geo mapped visualizations this package is more like a library and does not have a user interface thus it requires the user to be already familiar with r programming language nevertheless the open access and advanced data handling functionalities of r make of it a very attractive option for air quality research for instance oh and park 2015 shows how r can be used for exploring modeling and visualizing a dataset of air quality measurements collected by a monitoring network in a very recent paper the authors of feenstra et al 2020 present an open source r package called airsensor and the web application dataviewer built using shiny together they provide comprehensive data handling and visualization features for exploring data collected by networks of low cost air quality sensors finally we mention sisaire the web application used by ideam the meteorological agency of colombia to support the pipeline of air quality data capture and analysis ideam instituto de hidrología 2018 sisaire allows 1 collecting information on the meteorological and air quality variables generated by the regional environmental authorities and 2 facilitating data access and consultation to citizens and institutions in charge of research on environmental issues 1 3 our objective the objective of this work is to introduce aire a web based open source software tool for air quality data cleaning analysis visualization and reporting aire incorporates best practices validated by experts in the field of air quality management and offers a repeatable rigorous high quality data analysis approach in air pollution and a friendly user interface for the visualization of results and reports without requiring the intervention of skilled data scientists by proposing aire we authors intend to provide an open solution for air quality data analysis which is compliant with the requirements of colombian protocols for air quality analysis and reporting using aire allows speeding up the execution of tedious and error prone tasks and generating a wealth of intuitive data visualizations aire can be easily modified and extended users killed with r can flexibly reuse the existing features and adapt them to new requirements the software can be freely downloaded from its gitlab repository hosted at duke university see the appendix for details the rest of this paper entails the methodology in developing the aire package in section 2 its visualization capabilities in section 3 our validation method in section 4 and finally section 5 concludes the work and lights the way forward for research on this topic 2 methodology in this section we present our approach to the development of aire we followed a software engineering incremental and iterative development process larman and basili 2003 according to this process the functionalities to be included in aire are first defined in terms of user requirements gathered from the intended users and with the aid of technical experts the software development was organized according to a combined incremental spiral life cycle whereby subsequent intermediate releases were produced each one based on a set of requirements for each requirement set proof of concept prototypes were developed and validated with users and technical experts in colombia technical requirements included compliance with the process of data cleaning according to the rules for the analysis of air quality data defined by the colombian environmental authorities ministerio de ambiente y desarrollo sostenible 2008 this protocol is maintained by the institute of hydrology meteorology and environmental studies ideam the central and technical agency of the colombian ministry of environment we strongly emphasized on the development of an easy to use and reproducible data cleaning process with the objective of allowing any user to execute it and re obtain the same validated data sets it is worthwhile mentioning that air quality data collected by monitoring networks in colombia is openly accessible and the information contained therein is available to the general public however without simple and statistically well grounded tools to extract such information from the time series of the variables stored in the data sets their usefulness is diminished therefore our work was also driven by the imperative need of creating intuitive though flexible visualizations of the validated data sets thus our development process also included methodologies for the design and analysis of information through visual analytics techniques munzner 2014 in the next four subsections we describe the main elements that guided the development of aire first we present the required input data and second we discuss the intended users of our tool third we introduce the required functionality of aire and fourth we finally detail the software architecture that is devised in order to facilitate its appropriation reuse and future improvements 2 1 input data the format in which data is made available by an air quality monitoring network depends on its specific data collection operations as well as on the technological infrastructure of the network any software tool including aire is however bound to specific formats of the input data which we describe in this section data collected by a monitoring network is usually organized in tables where each row provides measured values at a given moment in time a single monitoring station is usually a collection of measurement devices that gather data on several variables of interest including contaminants and possibly other environmental variables such as the meteorological factors hence each single measurement can be thought of as a tuple t i m e v a r i a b l e l o c a t i o n m e a s u r e d v a l u e data is commonly stored and made available in textual file format and one of the elements of the tuple is implicitly used to identify the file content if the element is the location e g a monitoring station then the data would be logically structured as shown on the left of table 1 with each row providing the values of all variables measured by a station if the pollutant variable e g carbon monoxide is used for determining the file content then the data has a similar format to what is shown in the right hand side of table 1 where each row provides the values of the variable measured by all available monitoring stations in colombia environmental agencies can choose between the two formats shown in table 1 to store and interchange their data we found format a to be more commonly adopted by environmental agencies that manage smaller monitoring networks and manually collect data which is the most frequent case in colombia the data stored in format a may not have a constant period of reporting the format b is commonly adopted by environmental agencies that own automated monitoring networks usually located in large cities such as bogotá and medellín the time series stored in this format have a constant reporting period hourly values we choose format b in aire for storage and calculations the reason behind this decision is that this format facilitates data processing and the creation of tidy data sets at the expense of storing missing values moreover most analyses only use a single pollutant and compare its concentration among different locations however aire can also import data stored in format a and internally convert it to format b for this conversion the software assumes a single measurement is available for each day all analyses are then based on daily measurements 2 2 intended users although developed based on our interaction with the colombian air quality managers aire provides a general framework for the analysis of open access air quality data many countries and cities in latin america share similar concerns and needs for air quality data analysis and reporting franco et al 2019 in particular medium and small sized cities and towns could complement their air quality management practices by applying aire to feed and support the decision making process of environmental authorities moreover aire could be of interest to scholars working in air quality as it offers a first cut approximation tool for exploring the dynamics of air pollution finally since the software is endowed with a friendly and intuitive web based interface we believe it could improve the access to air quality descriptive analysis generating interest in a larger audience including non specialists the agencies in charge of environmental monitoring in colombia can be split into two groups according to the type of monitoring network infrastructure agencies with automatic stations typically found in larger cities endowed with specialized software applications that support the data analysis and reporting agencies managing manual stations commonly found in smaller cities and towns around the country using general purpose software to perform data analysis often without the assistance of specialized personnel a common problem of all agencies is the lack of resources devoted to knowledge management and research in these topics as a result the personnel in such agencies is usually hired on short contract terms and they may change quickly in a relatively short time period this situation hinders the build up and development of technical know how in the agencies and may generate inconsistencies in the handling and analysis of data over time in such circumstances the adoption of aire a tool endowed with a web interface which facilitates data treatment according to agreed policies can be of help in providing consistent air quality analysis results moreover the open source nature of aire makes it possible to customize and adapt accordingly to the evolving needs and regulations in air quality management and analysis 2 3 functional requirements of aire we collected the data analytics needs of the environmental agencies with the help of academic experts on air quality the requirements were gathered with the continuous feedback of the main national authorities of air quality management although many different types of functionalities could be automated the requirements that we chose to implement in aire are those considered to provide the most value and remove the necessity of tedious and error prone handling of data the following requirements of data treatment cleaning authentication and analysis have been expressed by the colombian environmental agencies data availability determine where for which measuring locations when for which time periods and how much valid data is available this allows an understanding of whether there is sufficient data to perform the desired analyses and if so how representative results will be the norms may prescribe lower bounds on the amount of available data necessary for conducting certain types of statistical analyses for instance according to colombian regulations at least 75 out of the 24 hourly measurements collected in a day must be valid for determining whether that day exceeded or not the threshold set by national standards days that exceed national standards determine the number of days that the contamination has exceeded the norms of the daily maximum contaminant concentration allowed an analysis task required by the national standards and a key indicator for all environmental agencies verify that the concentration values are expressed at reference conditions 25 c and 760 mm hg comparative analysis identify the locations with the highest concentration of contaminants compare different moments of the day such as traffic peak hours with valley hours compare different days such as holidays or weekdays with weekends these requirements call for a variety of different analyses and therefore require a flexible mechanism to allow environmental agencies compare the data in all meaningful ways time series visualize the historical data of the concentration of contaminants in the form of time series for a given measurement location this is a common step in air quality analysis which allows determining trends and visually identifying seasonality in pollutant concentration analysis of indexes use the data to calculate and visualize aggregate indices defined by the national regulations for example in colombia national guidelines prescribe the use of a color scale index to facilitate the communication of air quality levels pollutant time series estimate the pattern of pollutant concentrations through the day for the whole set of historical data or restricted to a specific time range and or location these distinct classes of requirements represent the basis for the development of aire and are reflected into the software architecture of the tool as detailed in the next subsection 2 4 software architecture of aire we designed a modular architecture for aire which will facilitate the future evolution and addition of software features with limited implementation efforts fig 1 shows a diagram of the tool architecture where each module is a different r script most scripts are implemented using a shiny chang et al module that defines both the user interface and the server logic the main advantage of this architecture is the use of a replication structure that allows to easily plug in new modules in particular the modules that implement the analysis functionalities have low or no coupling at all if a new feature is implemented then the developers would only need to write the feature code in a single r script and then integrate it with the data analysis module this facilitates the addition of further analysis functionality in aire the main disadvantage of this architecture is that modules have to be independent of each other which may require the duplication of functionalities to some extent however we decided to trade code optimization and computational performance with the ease in extension and understanding 3 visualizations in this section we describe the visualization functionality of aire to address each of the requirements mentioned in the previous section throughout this section we present examples based on the data from the bogotá air quality monitoring network rmcab hereafter by its initials in spanish red de monitoreo de calidad del aire de bogotá this network has been operating since 1998 collecting hourly measurements of several meteorological variables and air pollutants after being collected authenticated and analyzed the data is stored in an open data repository from where it can be retrieved in the form of textual files we have processed the data of 12 out of the 14 stations of the network excluding the mobile monitoring station that has been dedicated to the evaluation of air pollution at several locations along heavy traffic roads and a station that only reports meteorological data we focus on the analysis of pm2 5 and pm10 pollutants which have been on the watch for their frequently high concentrations within the urban area of bogotá mura et al 2020 3 1 analysis of data availability to address the requirement of data cleaning and authentication we develop two visualization tools in the form of a stacked bar chart and a heat map fig 2 depicts an example of the stacked bar chart providing an overall description of the data availability for each monitoring station in our case study of bogotá each bar in fig 2 represents the concentration of pm2 5 in one of the 12 stations throughout the city over ten years of data collection period and the stacked colors indicate the percentage of data that is affected by the user defined cleaning and authentication rules set in the application each rule states one of the properties that the measurement should possess to be considered valid these rules can be flexibly chosen among a configured list of user defined requirements for data cleaning and authentication the blue colored sections show the percentage of valid data that satisfies all the rules orange for data that did not pass the numeric only criteria some measurements may be replaced with text green for the data that did not pass the positive number rule some concentrations are erroneously reported as negative red for data that violate the detection limit rule values that are outside of the equipment detection limit and purple for the data that violates the cross validation rule some simultaneously measured concentrations of pm2 5 are higher than those of pm10 as it can be appreciated in fig 2 the station located in kennedy provides the highest percentage of valid data measurements for pm2 5 in our case study the heat map in fig 3 shows how data availability changes over time in this map the x axis shows the time and the y axis has a row for each one of the monitoring stations for each monitoring station the percentage of available data for each single day in the time interval is shown according to a color scheme yellow is used when availability is 100 i e the number of valid hourly data samples collected during that day is equal to 24 and purple for availability near 0 i e no valid records were taken during that day to calculate the daily average of the data we used the r package openair carslaw and ropkins 2012 the heat map visualization allows environmental agencies to determine at a glance which periods of time and which locations have useful data to conduct air quality analyses in the case of bogotá it is easy to see that there are many periods of time in color green which indicates limited quality of data as well as periods for which data is not available at all 3 2 comparison against national standards like many other countries colombian laws require all environmental agencies to report the number of days in a year that the concentration of contaminants is higher than the threshold allowed by the national standards to address this requirement we developed in aire the visualization tool shown in fig 4 to perform this analysis handling of missing data is critical although not explicitly mentioned by the norms the chart shows the number of days below the maximum level permitted by national standards the number of days without enough valid data to reach a conclusion and the number of days that the air quality violates the thresholds set by the national standards calculated by the daily averages most countries require a minimum proportion of valid data per unit time to declare this measure conclusive in the case of colombia the norm suggests at least 75 of valid data measurements in a day the bars for the days without enough valid data in gray are purposefully located in the middle of each stacked bar chart to provide a clue of the fact that some of them may belong to the green category and some others to the red one we have added a slider control for this visualization in the graphical user interface of aire so that the user can change the threshold set on the minimum percentage of valid data and observe the changes in the updated chart this visualization tool provides a clear view of the number of days above the limit permitted by the national standards the red section of the bars it is easy to see in fig 4 that in the case of bogotá most stations do not exceed the limit set by the law this plot can be easily interpreted by the general public as one of the intended audience of our package 3 3 comparative analyses the task of conducting comparative analyses requires a high degree of flexibility and generality by the environmental agencies and therefore it is among the most challenging for them this requirement calls for two major pieces of functionality a flexible sub setting of the data so that the user can adequately define the sets of data to be used for the comparative analysis and a general graphical rendering of the metrics computed on the selected data sets to implement a solution for the flexible sub setting of the data we have developed slide controls shown in fig 5 with a simple and intuitive process the user can define multiple comparison ranges and assign a name to each of them each comparison range identifies a subset of the data the data is further segmented by monitoring stations for instance if the user were interested in comparing the concentrations of pm2 5 measured on fridays saturdays and sundays in the period 2014 2016 she could define three comparison ranges and name them according to the day then use the year selection control to choose the period of time for the analysis and check the corresponding box of the specific day to complete the data subsetting each comparison range would be associated with a subset of the measured values of the variable i e pm2 5 in the example of bogotá in fig 5 which would then be sent to data analysis and rendering we visualize the comparisons through grouped box plots as the general rendering functionality this visualization functionality allows for a quick comparison of the data subsets within each station and among different stations for instance fig 6 shows the result of the per day analysis for bogotá as mentioned above from the chart we can easily observe that sundays were cleaner than fridays at all the stations for the selected time period also it is possible to pinpoint the stations with higher contamination levels e g carvajal sevillana and kennedy in our case study notice that results for 2 out of the 12 stations engativá and minambiente do not appear in fig 6 as they have no valid data for this analysis this same functionality can be used for a variety of analyses for instance one that is periodically conducted by the environmental authorities in colombia is the comparison of air quality between regular weekdays and the days when special vehicular traffic restrictions are in place specifically once a year bogotá declares a car free day where the transit of private vehicles cars and motorcycles is banned this day called día sin carro usually takes place on the first thursday of february as it is one of the most polluted months for the city lozano 2004 mura et al 2020 it is clear that such one day initiatives as día sin carro are more apt to raise awareness among the general public rather than meaningfully contributing to an immediate reduction in the air pollution however it is interesting to analyze the air pollution data collected by the monitoring network during such special occasions and compare with a set of properly chosen control days fig 7 shows the outcome of a comparative analysis conducted by this functionality in aire for the 2018 día sin carro edition which occurred on thursday february the 1st in fig 7 we focus on pm2 5 measured concentrations and the control days are the other thursdays of february of the same year results are shown for the nine stations that have valid data available for the analysis without intending to enter into a detailed analysis we observe that the visualization reported in fig 7 allows to identify several monitoring stations that reported appreciable reductions in pm2 5 measured concentrations while others did not show clear benefits from the traffic restrictions furthermore the data collected at the two monitoring stations of carvajal sevillana and kennedy both located in the south western part of the city indicate that air quality worsened during the día sin carro this may be due to the increase in public transportation offers deployed to satisfy the mobility demand the bogotá public transportation fleet has indeed been recognized as a major contributor to the city s air pollution of pm2 5 or smaller particles franco et al 2016 morales betancourt et al 2019 castillo camacho et al 2020 3 4 time series analysis a common simple visualization method for pollutant historical data is a line chart line charts or time series provide an immediate intuition on short term trends peak periods and variability it is also the required format of visualization for all colombian environmental agencies to report to the ministry of environment in fig 8 a time series visualization is produced for a user selected monitoring station and a control is added for choosing an aggregation level for the data points which can be displayed at the hour no aggregation day week month quarter or annual frequency 3 5 reporting air quality indexes several indicators and indexes have been proposed and established to quantify and communicate air pollution emissions concentrations and human health impacts internationally franco et al 2019 such indicators help bridge the science policy gap by synthesizing complex scientific information and presenting it to the citizens stakeholders and policy makers in understandable ways hsu et al 2013 for example the air quality index aqi widely used by the united states environmental protection agency us epa presents daily measurements of several major air pollutants on a color coded scale so citizens and other stakeholders can easily correlate one color with an air quality condition i e good moderate bad and furthermore identify spatial or temporal hot spots of poor air quality in a city colombia has developed her own air quality index ica for its acronym in spanish both ica s calculation methodology and ica cut off points definitions are established in the colombian national air quality standard document resolution 2254 of 2017 ministerio de ambiente y desarrollo sostenible 2017 table 2 shows the ica scale for pm2 5 and pm10 ica s methodology is based on the guidelines given by the united states environmental protection agency us epa through its technical assistance document for the reporting of daily air quality version of september 2018 us environmental protection agency and technical assistance document for the reporting of daily air quality 2018 given the relevance of the air quality indices aire includes the function of visualizing such indexes through heat maps in fig 9 the horizontal axis is the time and on the vertical axis a categorical variable ica is reported for each monitoring station each cell of the heat map reports the average ica color for the day at a given station when there is not enough data during a day to reliably calculate an average value the cell is given a null value which is graphically rendered using the gray color this visualization serves several purposes such as to find patterns in the behavior of pm concentrations over time for instance fig 9 shows that the months in the middle of the year i e june and july tend to be cleaner than the other months in bogotá also it allows to compare among various stations for example it is easy to spot that the stations of kennedy and sevillana are the most polluted in the city finally it permits identifying particular events that affect the city when most stations simultaneously report very high concentration of pollutants for instance we can appreciate in fig 9 one of such events occurring in february 2016 when the smoke caused by a massive wildfire on the mountains bordering the eastern side of bogotá generated a surge in air pollution levels we can observe such an event in detail on the chart presented in fig 10 which is obtained by zooming in fig 9 over the specific time window of the wildfire all the visualizations of aire are rendered using the plotly library sievert et al 2017 which provides several controls that are directly accessible to the user by clicking on the displayed images 3 6 analysis of daily pollution patterns this analysis is of particular interest to the environmental agencies that can regularly register hourly data it can be used to identify peak hours and changes in the daily patterns of contamination over time for instance as a result of traffic circulation restrictions fig 11 shows the line chart that aire produces for the daily pollution pattern for the concentration of pm10 in bogotá the x axis shows the hours of the day and the y axis the concentration of the pollutant each line shows the hourly average computed for the days of a specific year the plot also shows a 95 confidence interval for each hourly point as pointed out in other studies e g mura et al 2020 this kind of plot makes it easy to observe that the concentration of pm10 has steadily diminished through the years in bogotá 4 validation of aire in this section we describe the process of validating the implemented functionalities of the software by experts and intended users besides loading cleaning and data analysis functionalities aire took into account requirements related to usability and intuitiveness of visualizations therefore actors with different expertise participated in the overall development process fig 12 depicts a summary of the contributions and roles that the different actors played in the verification and validation process of aire aire was developed by an inter disciplinary research team that included undergraduate and graduate students researchers and professors at universidad de los andes the team joined forces from the department of industrial engineering whose skills revolve mainly around data science and the department of civil and environmental engineering which mostly contributed on the technical air quality facet of the project this combination of complementary competences ensured that diverse perspectives were represented generated a stimulating and challenging learning environment and provided a high degree of criticism instrumental to sustaining continuous verification of the project outcomes we developed aire with the continuous accompaniment of the colombian ministry of environment minambiente the main governmental entity responsible for the development of public policies that have an impact on the environment although minambiente itself is not involved in the monitoring of air quality it has the role of defining norms and guidelines recommending best practices and disseminating knowledge in order to improve the performance of local environmental agencies when starting the development of aire we contacted minambiente to present the early software prototypes the possibility of distributing an open source tool for air quality to local environmental authorities and to foster its use for the sake of a more consistent data analysis proved to be very attractive to minambiente hence minambiente sponsored our project and based on their profound knowledge of the practices and needs of air quality monitoring we jointly defined a roadmap for functionality addition this gave aire an institutional endorsement we developed a first version of aire and then liaised with ideam the national meteorological agency which maintains the protocols for data collection cleaning aggregation and analysis to realize software validation we carefully reviewed with ideam the data cleaning and analysis functionality of aire pinpointing any non compliance with the defined protocols the fix of these defects and the development of a further set of visualizations defined the requirements of the second version of aire in the final stages of aire development minambiente held an event that brought together representatives of all colombian local environmental agencies target users of the software we were granted a space to show a live demo of aire although this may not be considered a formal acceptance test the reception of the software was mostly positive we could additionally gather requirements on new desirable features to facilitate the identification of outliers and counting the number of days in each interval of the ica index which is now implemented in the final version of aire see visualization in fig 4 in conjunciton with these activities bogotá s central environmental agency secretaría distrital de ambiente made important contributions in separate meetings by validating the box plot visualizations that aire generates to show the results of comparative analyses the implementation of data handling and visualization requirements were also validated by academia experts outside universidad de los andes we presented aire at a scientific conference organized by the colombian association for operational research asocio held in 2017 díaz baquero et al 2017a we discussed the algorithmic details of the implementation and improvements to the visualizations including controls that would allow zooming into compact graphical renderings now implemented in fig 10 and smoothing to facilitate the identification of trends within the data not yet implemented finally this work was also presented to representatives of academia with expertise in air quality a demo of aire was given to a panel composed by academics using data of the monitoring network of bogotá at the 2017 colombian congress and international conference on air quality and public health casap díaz baquero et al 2017b experts and researchers suggested adding to aire visualizations on the amount of available data before analysis which has now been implemented in several analyses in other visualizations we added slide control bars to allow the user choose a tolerance level for the amount of missing data see for instance fig 4 we also presented aire at the 23 rd international sustainable development research society isdrs conference franco et al 2017 the importance of making results reproducible for all analyses upon which high level decisions would be made was remarked as well as the opportunity of endowing aire with the functionality to export cleaned data now implemented and tables and plots into reports not yet implemented 5 conclusion and future work this work presents aire a web based open source software package in r and shiny for air quality data cleaning and authentication analysis visualization and reporting the software implements data loading cleaning and analysis while providing various visualization functionalities aiming at strengthening the process of analysis and reporting of air quality data by entities in charge of managing air pollution aire was developed and validated in collaboration with the colombian environmental authorities and academia experts the code of the tool can be freely accessed from the gitlab page of aire see appendix a the authors believe that the process of developing aire was extremely valuable through gathering the public sector and academia around the same table with the ultimate purpose of supporting cities in air quality management while strengthening local capabilities to improve urban air pollution we believe that aire can offer a useful complement to other open source initiatives for the analysis and interpretation of air pollution data the future work on aire includes the implementation of new functionalities such as the analysis of meteorological variables and spatial distribution of air pollution in a specific geographical location several local environmental agencies have also suggested adding spacial analysis capabilities visualizing the locations of monitoring stations in a city together with aggregated data would be of great help to the authorities even more so if a spacial distribution of contaminants would be added by using a spacial interpolation model we also believe that an important step would be to reach the general public as the audience of aire visualizations thus raising awareness and creating participation finally the addition of predictive and prescriptive analysis capabilities as well as generating a historical report of all operations performed and results obtained during a session will be considered as discussed earlier declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a software availability developer juan josé díaz contact address cra 1 n 18a 12 dep of industrial engineering universidad de los andes bogotá colombia contact emails jj diaz1067 uniandes edu co ivan mura dukekunshan edu cn tested browsers firefox and google chrome software required r rstudio r packages required shiny ggplot2 plotly openair rcolorbrewer scales grid shinythemes programming language r available since 2018 availability the aire source code package is stored in a freely accessible gitlab repository hosted by duke university https gitlab oit duke edu im90 aire cost free 
25875,recent technological advances in collecting data on emission sources meteorological conditions and concentration of air pollutants in urban areas offer invaluable opportunities for the better understanding of air quality problems however processing large sets of data to extract statistically valid evidence poses many challenges from both the conceptual and technical viewpoints air quality data acquisition cleaning and authentication are necessary and crucial preliminary phases to support descriptive predictive and prescriptive models and to ensure that aggregated and high quality information is delivered to the central and local governments decision makers and citizens automated software tools can facilitate drawing conclusions based on the information contained in the data limiting subjective judgment and providing repeatability however the costly state of the art software applications developed by major vendors are inaccessible to many cities and townships in the developing world moreover their usage creates dependency on proprietary solutions which can hinder the possibility of evolving the data processing and analysis protocols we present an open source web application for air quality data analysis and visualization called aire based on the r statistical framework and shiny web package aire has been developed in collaboration with the colombian environmental authorities and implements best practices validated by experts in air quality we believe that the process of developing aire was extremely valuable with the ultimate purpose of supporting cities in air quality management while strengthening local capabilities to improve urban air pollution this open access tool simplifies and makes air quality data analysis and visualization accessible with the desirable effect of removing ownership costs fostering appropriation by non expert users and ultimately promoting informed decision making for the general public and the local government authorities we present the performance of this tool over a series of examples of open data collected by the air quality monitoring network of bogotá colombia keywords aire air quality software open source r shiny web application 1 introduction 1 1 context poor air quality is a growing global concern gulia et al 2015 baklanov et al 2016 recent evidence shows that 8 out of 10 people worldwide are exposed to air pollution levels that are considered harmful to human health landrigan et al 2018 air quality management in many cities around the world calls for the deployment of surveillance systems to monitor air pollution levels such systems are normally composed by a set of stations to measure ambient air pollutant concentrations as well as meteorological variables these networks vary widely in reliability and representation influenced by factors such as topography demography meteorology and socioeconomic development air pollution in the world s megacities 1994 due to the many complexities of air quality monitoring large cities often opt for commercial software packages that help them configure administrate and maintain air monitoring infrastructures as well as creating reports for the decision makers in environmental policy however most smaller cities and towns in less developed countries cannot afford such products instead they generate their reports using very basic tools which are unsuitable for database management and big data analysis furthermore there is often lack of expertise and adequate capabilities in local administrations to ensure the reliability of air monitoring networks data cleaning and the subsequent analyses with the increasing availability and popularity of cheaper air quality measurement sensors gulia et al 2020 the amount of data to be analyzed will grow and will call more and more for advanced and automated software support the impact of poor air quality also has a socioeconomic class facet as according to the who in low and middle income countries approximately 98 of city inhabitants are living in environmental conditions that do not meet the guidelines for air pollutant exposure in high income countries this percentage yet still very high reduces to 56 osseiran and chriscaden 2016 among the reasons for this difference are the unavailability of reliable air monitoring infrastructure and or the lack of analytical capabilities to develop effective public policies without the support offered by automation even guaranteeing compliance with existing air quality standards and regulations for data reporting becomes difficult for example in colombia the subject of our case study the responsibility of air quality management is assigned to both local and regional environmental authorities mostly decentralized and independent agencies that have the responsibility to administrate and operate air quality monitoring networks such independence may lead to a wide range of interpretations on data treatment and reporting guidelines suggested by the national government creating the necessity for integrated software solutions and data analysis approaches 1 2 available software packages for air quality analysis currently there are several software applications capable of reading structured data sets of air quality measurements and performing statistical or visual analysis we can distinguish them in two classes proprietary applications with commercial purposes and non commercial software which includes many academic developments commercial software application usually have a friendly user interface and can perform all the necessary steps for different types of analyses these tools can load the data sets or even directly interface with the station of a monitoring network to obtain the data perform pre configured and user defined analyses and create detailed reports of air quality for instance the israeli software suite envida envista ltd and envista air 2007 provides an integrated solution to retrieve store and analyze measurements collected by an air quality measurement network it can retrieve the data from a database and perform analyses such as wind roses histograms correlations in different aggregation of time and dynamic gis visualizations another program is the german software ambient air quality monitoring kisters which is intended for public authorities and can connect to different monitoring networks to provide local regional or national level data managing and analysis the main advantage offered by commercial products is the quick access to many types of analyses to support decision making they automate error prone data manipulation tasks and reduce subjective judgment in data analysis a clear disadvantage is of course the cost since these software packages are usually very expensive to acquire and many governmental entities especially in low income regions may not be able to afford them moreover the usage of these commercial solutions may create dependence on the vendors which might limit the creation of technical know how that goes beyond the tool s defined functionality and hinder the possibility of autonomously adapting to new needs and conditions there are several non commercial solutions that offer nice and intuitive graphical user interface and visualization features for instance giovanni a web accessible tool developed by nasa goddard earth sciences data and information services center prados et al 2010 allows loading and analyzing many diverse types of open datasets including air quality measurements collected by satellites and on ground stations giovanni provides time series and spatial visualization of data on maps moreover it allows exporting data in various formats including the compressed keyhole markup language kml for reusing them in 3d google earth visualizations the kml data format is also used by the solution described in chen 2019 where the authors propose an application that allows visualizing in near real time air quality data they describe in chen 2019 a software residing at a server side which can be configured to continuously gather air quality open data and structure it into kml files ready to be consumed by clients using google earth examples of heat map visualizations are shown in chen 2019 for china air quality data another software application that aims at providing intuitive visualizations of air quality data is proposed in lu et al 2017 a web interface is provided which allows displaying pollution at various levels from country to regions cities down to monitoring stations at each level the time dimension of the data is summarized by a sunburst like chart which can be zoomed for changing the granularity level the software application allows visualizing the aqi index as well as the concentration of user selected pollutants some other studies in the literature report about software applications that can load and visualize air quality measurements together with predictions generated with various types of machine learning approaches for instance ofoegbu et al 2014 proposes to a software application that can connect to a database containing the air quality data and generate visualizations for pollutant time series and their predictions as well as aqi predictions using artificial neuronal networks and decision trees type of models in tomić et al 2014 the authors present a client server solution that is capable of acquiring measurement through a tcp ip network and display visualizations of the collected data together with prediction obtained with an auto regressive moving average model the solution includes a client for devices running the android operating system so that the data visualization services can be accessed remotely several solutions for air quality analysis and visualizations based on the statistical software r have also been proposed the open source r package openair is currently the most popular software for air quality data analysis carslaw and ropkins 2012 the package is developed at king s college in london by the entity in charge of the city s air monitoring network it is capable of reading a set of air quality data and performing a variety of possible analyses in particular openair provides extensive data filtering features such as sorting by weekdays seasons daylight hours etc creating wind rose time series and geo mapped visualizations this package is more like a library and does not have a user interface thus it requires the user to be already familiar with r programming language nevertheless the open access and advanced data handling functionalities of r make of it a very attractive option for air quality research for instance oh and park 2015 shows how r can be used for exploring modeling and visualizing a dataset of air quality measurements collected by a monitoring network in a very recent paper the authors of feenstra et al 2020 present an open source r package called airsensor and the web application dataviewer built using shiny together they provide comprehensive data handling and visualization features for exploring data collected by networks of low cost air quality sensors finally we mention sisaire the web application used by ideam the meteorological agency of colombia to support the pipeline of air quality data capture and analysis ideam instituto de hidrología 2018 sisaire allows 1 collecting information on the meteorological and air quality variables generated by the regional environmental authorities and 2 facilitating data access and consultation to citizens and institutions in charge of research on environmental issues 1 3 our objective the objective of this work is to introduce aire a web based open source software tool for air quality data cleaning analysis visualization and reporting aire incorporates best practices validated by experts in the field of air quality management and offers a repeatable rigorous high quality data analysis approach in air pollution and a friendly user interface for the visualization of results and reports without requiring the intervention of skilled data scientists by proposing aire we authors intend to provide an open solution for air quality data analysis which is compliant with the requirements of colombian protocols for air quality analysis and reporting using aire allows speeding up the execution of tedious and error prone tasks and generating a wealth of intuitive data visualizations aire can be easily modified and extended users killed with r can flexibly reuse the existing features and adapt them to new requirements the software can be freely downloaded from its gitlab repository hosted at duke university see the appendix for details the rest of this paper entails the methodology in developing the aire package in section 2 its visualization capabilities in section 3 our validation method in section 4 and finally section 5 concludes the work and lights the way forward for research on this topic 2 methodology in this section we present our approach to the development of aire we followed a software engineering incremental and iterative development process larman and basili 2003 according to this process the functionalities to be included in aire are first defined in terms of user requirements gathered from the intended users and with the aid of technical experts the software development was organized according to a combined incremental spiral life cycle whereby subsequent intermediate releases were produced each one based on a set of requirements for each requirement set proof of concept prototypes were developed and validated with users and technical experts in colombia technical requirements included compliance with the process of data cleaning according to the rules for the analysis of air quality data defined by the colombian environmental authorities ministerio de ambiente y desarrollo sostenible 2008 this protocol is maintained by the institute of hydrology meteorology and environmental studies ideam the central and technical agency of the colombian ministry of environment we strongly emphasized on the development of an easy to use and reproducible data cleaning process with the objective of allowing any user to execute it and re obtain the same validated data sets it is worthwhile mentioning that air quality data collected by monitoring networks in colombia is openly accessible and the information contained therein is available to the general public however without simple and statistically well grounded tools to extract such information from the time series of the variables stored in the data sets their usefulness is diminished therefore our work was also driven by the imperative need of creating intuitive though flexible visualizations of the validated data sets thus our development process also included methodologies for the design and analysis of information through visual analytics techniques munzner 2014 in the next four subsections we describe the main elements that guided the development of aire first we present the required input data and second we discuss the intended users of our tool third we introduce the required functionality of aire and fourth we finally detail the software architecture that is devised in order to facilitate its appropriation reuse and future improvements 2 1 input data the format in which data is made available by an air quality monitoring network depends on its specific data collection operations as well as on the technological infrastructure of the network any software tool including aire is however bound to specific formats of the input data which we describe in this section data collected by a monitoring network is usually organized in tables where each row provides measured values at a given moment in time a single monitoring station is usually a collection of measurement devices that gather data on several variables of interest including contaminants and possibly other environmental variables such as the meteorological factors hence each single measurement can be thought of as a tuple t i m e v a r i a b l e l o c a t i o n m e a s u r e d v a l u e data is commonly stored and made available in textual file format and one of the elements of the tuple is implicitly used to identify the file content if the element is the location e g a monitoring station then the data would be logically structured as shown on the left of table 1 with each row providing the values of all variables measured by a station if the pollutant variable e g carbon monoxide is used for determining the file content then the data has a similar format to what is shown in the right hand side of table 1 where each row provides the values of the variable measured by all available monitoring stations in colombia environmental agencies can choose between the two formats shown in table 1 to store and interchange their data we found format a to be more commonly adopted by environmental agencies that manage smaller monitoring networks and manually collect data which is the most frequent case in colombia the data stored in format a may not have a constant period of reporting the format b is commonly adopted by environmental agencies that own automated monitoring networks usually located in large cities such as bogotá and medellín the time series stored in this format have a constant reporting period hourly values we choose format b in aire for storage and calculations the reason behind this decision is that this format facilitates data processing and the creation of tidy data sets at the expense of storing missing values moreover most analyses only use a single pollutant and compare its concentration among different locations however aire can also import data stored in format a and internally convert it to format b for this conversion the software assumes a single measurement is available for each day all analyses are then based on daily measurements 2 2 intended users although developed based on our interaction with the colombian air quality managers aire provides a general framework for the analysis of open access air quality data many countries and cities in latin america share similar concerns and needs for air quality data analysis and reporting franco et al 2019 in particular medium and small sized cities and towns could complement their air quality management practices by applying aire to feed and support the decision making process of environmental authorities moreover aire could be of interest to scholars working in air quality as it offers a first cut approximation tool for exploring the dynamics of air pollution finally since the software is endowed with a friendly and intuitive web based interface we believe it could improve the access to air quality descriptive analysis generating interest in a larger audience including non specialists the agencies in charge of environmental monitoring in colombia can be split into two groups according to the type of monitoring network infrastructure agencies with automatic stations typically found in larger cities endowed with specialized software applications that support the data analysis and reporting agencies managing manual stations commonly found in smaller cities and towns around the country using general purpose software to perform data analysis often without the assistance of specialized personnel a common problem of all agencies is the lack of resources devoted to knowledge management and research in these topics as a result the personnel in such agencies is usually hired on short contract terms and they may change quickly in a relatively short time period this situation hinders the build up and development of technical know how in the agencies and may generate inconsistencies in the handling and analysis of data over time in such circumstances the adoption of aire a tool endowed with a web interface which facilitates data treatment according to agreed policies can be of help in providing consistent air quality analysis results moreover the open source nature of aire makes it possible to customize and adapt accordingly to the evolving needs and regulations in air quality management and analysis 2 3 functional requirements of aire we collected the data analytics needs of the environmental agencies with the help of academic experts on air quality the requirements were gathered with the continuous feedback of the main national authorities of air quality management although many different types of functionalities could be automated the requirements that we chose to implement in aire are those considered to provide the most value and remove the necessity of tedious and error prone handling of data the following requirements of data treatment cleaning authentication and analysis have been expressed by the colombian environmental agencies data availability determine where for which measuring locations when for which time periods and how much valid data is available this allows an understanding of whether there is sufficient data to perform the desired analyses and if so how representative results will be the norms may prescribe lower bounds on the amount of available data necessary for conducting certain types of statistical analyses for instance according to colombian regulations at least 75 out of the 24 hourly measurements collected in a day must be valid for determining whether that day exceeded or not the threshold set by national standards days that exceed national standards determine the number of days that the contamination has exceeded the norms of the daily maximum contaminant concentration allowed an analysis task required by the national standards and a key indicator for all environmental agencies verify that the concentration values are expressed at reference conditions 25 c and 760 mm hg comparative analysis identify the locations with the highest concentration of contaminants compare different moments of the day such as traffic peak hours with valley hours compare different days such as holidays or weekdays with weekends these requirements call for a variety of different analyses and therefore require a flexible mechanism to allow environmental agencies compare the data in all meaningful ways time series visualize the historical data of the concentration of contaminants in the form of time series for a given measurement location this is a common step in air quality analysis which allows determining trends and visually identifying seasonality in pollutant concentration analysis of indexes use the data to calculate and visualize aggregate indices defined by the national regulations for example in colombia national guidelines prescribe the use of a color scale index to facilitate the communication of air quality levels pollutant time series estimate the pattern of pollutant concentrations through the day for the whole set of historical data or restricted to a specific time range and or location these distinct classes of requirements represent the basis for the development of aire and are reflected into the software architecture of the tool as detailed in the next subsection 2 4 software architecture of aire we designed a modular architecture for aire which will facilitate the future evolution and addition of software features with limited implementation efforts fig 1 shows a diagram of the tool architecture where each module is a different r script most scripts are implemented using a shiny chang et al module that defines both the user interface and the server logic the main advantage of this architecture is the use of a replication structure that allows to easily plug in new modules in particular the modules that implement the analysis functionalities have low or no coupling at all if a new feature is implemented then the developers would only need to write the feature code in a single r script and then integrate it with the data analysis module this facilitates the addition of further analysis functionality in aire the main disadvantage of this architecture is that modules have to be independent of each other which may require the duplication of functionalities to some extent however we decided to trade code optimization and computational performance with the ease in extension and understanding 3 visualizations in this section we describe the visualization functionality of aire to address each of the requirements mentioned in the previous section throughout this section we present examples based on the data from the bogotá air quality monitoring network rmcab hereafter by its initials in spanish red de monitoreo de calidad del aire de bogotá this network has been operating since 1998 collecting hourly measurements of several meteorological variables and air pollutants after being collected authenticated and analyzed the data is stored in an open data repository from where it can be retrieved in the form of textual files we have processed the data of 12 out of the 14 stations of the network excluding the mobile monitoring station that has been dedicated to the evaluation of air pollution at several locations along heavy traffic roads and a station that only reports meteorological data we focus on the analysis of pm2 5 and pm10 pollutants which have been on the watch for their frequently high concentrations within the urban area of bogotá mura et al 2020 3 1 analysis of data availability to address the requirement of data cleaning and authentication we develop two visualization tools in the form of a stacked bar chart and a heat map fig 2 depicts an example of the stacked bar chart providing an overall description of the data availability for each monitoring station in our case study of bogotá each bar in fig 2 represents the concentration of pm2 5 in one of the 12 stations throughout the city over ten years of data collection period and the stacked colors indicate the percentage of data that is affected by the user defined cleaning and authentication rules set in the application each rule states one of the properties that the measurement should possess to be considered valid these rules can be flexibly chosen among a configured list of user defined requirements for data cleaning and authentication the blue colored sections show the percentage of valid data that satisfies all the rules orange for data that did not pass the numeric only criteria some measurements may be replaced with text green for the data that did not pass the positive number rule some concentrations are erroneously reported as negative red for data that violate the detection limit rule values that are outside of the equipment detection limit and purple for the data that violates the cross validation rule some simultaneously measured concentrations of pm2 5 are higher than those of pm10 as it can be appreciated in fig 2 the station located in kennedy provides the highest percentage of valid data measurements for pm2 5 in our case study the heat map in fig 3 shows how data availability changes over time in this map the x axis shows the time and the y axis has a row for each one of the monitoring stations for each monitoring station the percentage of available data for each single day in the time interval is shown according to a color scheme yellow is used when availability is 100 i e the number of valid hourly data samples collected during that day is equal to 24 and purple for availability near 0 i e no valid records were taken during that day to calculate the daily average of the data we used the r package openair carslaw and ropkins 2012 the heat map visualization allows environmental agencies to determine at a glance which periods of time and which locations have useful data to conduct air quality analyses in the case of bogotá it is easy to see that there are many periods of time in color green which indicates limited quality of data as well as periods for which data is not available at all 3 2 comparison against national standards like many other countries colombian laws require all environmental agencies to report the number of days in a year that the concentration of contaminants is higher than the threshold allowed by the national standards to address this requirement we developed in aire the visualization tool shown in fig 4 to perform this analysis handling of missing data is critical although not explicitly mentioned by the norms the chart shows the number of days below the maximum level permitted by national standards the number of days without enough valid data to reach a conclusion and the number of days that the air quality violates the thresholds set by the national standards calculated by the daily averages most countries require a minimum proportion of valid data per unit time to declare this measure conclusive in the case of colombia the norm suggests at least 75 of valid data measurements in a day the bars for the days without enough valid data in gray are purposefully located in the middle of each stacked bar chart to provide a clue of the fact that some of them may belong to the green category and some others to the red one we have added a slider control for this visualization in the graphical user interface of aire so that the user can change the threshold set on the minimum percentage of valid data and observe the changes in the updated chart this visualization tool provides a clear view of the number of days above the limit permitted by the national standards the red section of the bars it is easy to see in fig 4 that in the case of bogotá most stations do not exceed the limit set by the law this plot can be easily interpreted by the general public as one of the intended audience of our package 3 3 comparative analyses the task of conducting comparative analyses requires a high degree of flexibility and generality by the environmental agencies and therefore it is among the most challenging for them this requirement calls for two major pieces of functionality a flexible sub setting of the data so that the user can adequately define the sets of data to be used for the comparative analysis and a general graphical rendering of the metrics computed on the selected data sets to implement a solution for the flexible sub setting of the data we have developed slide controls shown in fig 5 with a simple and intuitive process the user can define multiple comparison ranges and assign a name to each of them each comparison range identifies a subset of the data the data is further segmented by monitoring stations for instance if the user were interested in comparing the concentrations of pm2 5 measured on fridays saturdays and sundays in the period 2014 2016 she could define three comparison ranges and name them according to the day then use the year selection control to choose the period of time for the analysis and check the corresponding box of the specific day to complete the data subsetting each comparison range would be associated with a subset of the measured values of the variable i e pm2 5 in the example of bogotá in fig 5 which would then be sent to data analysis and rendering we visualize the comparisons through grouped box plots as the general rendering functionality this visualization functionality allows for a quick comparison of the data subsets within each station and among different stations for instance fig 6 shows the result of the per day analysis for bogotá as mentioned above from the chart we can easily observe that sundays were cleaner than fridays at all the stations for the selected time period also it is possible to pinpoint the stations with higher contamination levels e g carvajal sevillana and kennedy in our case study notice that results for 2 out of the 12 stations engativá and minambiente do not appear in fig 6 as they have no valid data for this analysis this same functionality can be used for a variety of analyses for instance one that is periodically conducted by the environmental authorities in colombia is the comparison of air quality between regular weekdays and the days when special vehicular traffic restrictions are in place specifically once a year bogotá declares a car free day where the transit of private vehicles cars and motorcycles is banned this day called día sin carro usually takes place on the first thursday of february as it is one of the most polluted months for the city lozano 2004 mura et al 2020 it is clear that such one day initiatives as día sin carro are more apt to raise awareness among the general public rather than meaningfully contributing to an immediate reduction in the air pollution however it is interesting to analyze the air pollution data collected by the monitoring network during such special occasions and compare with a set of properly chosen control days fig 7 shows the outcome of a comparative analysis conducted by this functionality in aire for the 2018 día sin carro edition which occurred on thursday february the 1st in fig 7 we focus on pm2 5 measured concentrations and the control days are the other thursdays of february of the same year results are shown for the nine stations that have valid data available for the analysis without intending to enter into a detailed analysis we observe that the visualization reported in fig 7 allows to identify several monitoring stations that reported appreciable reductions in pm2 5 measured concentrations while others did not show clear benefits from the traffic restrictions furthermore the data collected at the two monitoring stations of carvajal sevillana and kennedy both located in the south western part of the city indicate that air quality worsened during the día sin carro this may be due to the increase in public transportation offers deployed to satisfy the mobility demand the bogotá public transportation fleet has indeed been recognized as a major contributor to the city s air pollution of pm2 5 or smaller particles franco et al 2016 morales betancourt et al 2019 castillo camacho et al 2020 3 4 time series analysis a common simple visualization method for pollutant historical data is a line chart line charts or time series provide an immediate intuition on short term trends peak periods and variability it is also the required format of visualization for all colombian environmental agencies to report to the ministry of environment in fig 8 a time series visualization is produced for a user selected monitoring station and a control is added for choosing an aggregation level for the data points which can be displayed at the hour no aggregation day week month quarter or annual frequency 3 5 reporting air quality indexes several indicators and indexes have been proposed and established to quantify and communicate air pollution emissions concentrations and human health impacts internationally franco et al 2019 such indicators help bridge the science policy gap by synthesizing complex scientific information and presenting it to the citizens stakeholders and policy makers in understandable ways hsu et al 2013 for example the air quality index aqi widely used by the united states environmental protection agency us epa presents daily measurements of several major air pollutants on a color coded scale so citizens and other stakeholders can easily correlate one color with an air quality condition i e good moderate bad and furthermore identify spatial or temporal hot spots of poor air quality in a city colombia has developed her own air quality index ica for its acronym in spanish both ica s calculation methodology and ica cut off points definitions are established in the colombian national air quality standard document resolution 2254 of 2017 ministerio de ambiente y desarrollo sostenible 2017 table 2 shows the ica scale for pm2 5 and pm10 ica s methodology is based on the guidelines given by the united states environmental protection agency us epa through its technical assistance document for the reporting of daily air quality version of september 2018 us environmental protection agency and technical assistance document for the reporting of daily air quality 2018 given the relevance of the air quality indices aire includes the function of visualizing such indexes through heat maps in fig 9 the horizontal axis is the time and on the vertical axis a categorical variable ica is reported for each monitoring station each cell of the heat map reports the average ica color for the day at a given station when there is not enough data during a day to reliably calculate an average value the cell is given a null value which is graphically rendered using the gray color this visualization serves several purposes such as to find patterns in the behavior of pm concentrations over time for instance fig 9 shows that the months in the middle of the year i e june and july tend to be cleaner than the other months in bogotá also it allows to compare among various stations for example it is easy to spot that the stations of kennedy and sevillana are the most polluted in the city finally it permits identifying particular events that affect the city when most stations simultaneously report very high concentration of pollutants for instance we can appreciate in fig 9 one of such events occurring in february 2016 when the smoke caused by a massive wildfire on the mountains bordering the eastern side of bogotá generated a surge in air pollution levels we can observe such an event in detail on the chart presented in fig 10 which is obtained by zooming in fig 9 over the specific time window of the wildfire all the visualizations of aire are rendered using the plotly library sievert et al 2017 which provides several controls that are directly accessible to the user by clicking on the displayed images 3 6 analysis of daily pollution patterns this analysis is of particular interest to the environmental agencies that can regularly register hourly data it can be used to identify peak hours and changes in the daily patterns of contamination over time for instance as a result of traffic circulation restrictions fig 11 shows the line chart that aire produces for the daily pollution pattern for the concentration of pm10 in bogotá the x axis shows the hours of the day and the y axis the concentration of the pollutant each line shows the hourly average computed for the days of a specific year the plot also shows a 95 confidence interval for each hourly point as pointed out in other studies e g mura et al 2020 this kind of plot makes it easy to observe that the concentration of pm10 has steadily diminished through the years in bogotá 4 validation of aire in this section we describe the process of validating the implemented functionalities of the software by experts and intended users besides loading cleaning and data analysis functionalities aire took into account requirements related to usability and intuitiveness of visualizations therefore actors with different expertise participated in the overall development process fig 12 depicts a summary of the contributions and roles that the different actors played in the verification and validation process of aire aire was developed by an inter disciplinary research team that included undergraduate and graduate students researchers and professors at universidad de los andes the team joined forces from the department of industrial engineering whose skills revolve mainly around data science and the department of civil and environmental engineering which mostly contributed on the technical air quality facet of the project this combination of complementary competences ensured that diverse perspectives were represented generated a stimulating and challenging learning environment and provided a high degree of criticism instrumental to sustaining continuous verification of the project outcomes we developed aire with the continuous accompaniment of the colombian ministry of environment minambiente the main governmental entity responsible for the development of public policies that have an impact on the environment although minambiente itself is not involved in the monitoring of air quality it has the role of defining norms and guidelines recommending best practices and disseminating knowledge in order to improve the performance of local environmental agencies when starting the development of aire we contacted minambiente to present the early software prototypes the possibility of distributing an open source tool for air quality to local environmental authorities and to foster its use for the sake of a more consistent data analysis proved to be very attractive to minambiente hence minambiente sponsored our project and based on their profound knowledge of the practices and needs of air quality monitoring we jointly defined a roadmap for functionality addition this gave aire an institutional endorsement we developed a first version of aire and then liaised with ideam the national meteorological agency which maintains the protocols for data collection cleaning aggregation and analysis to realize software validation we carefully reviewed with ideam the data cleaning and analysis functionality of aire pinpointing any non compliance with the defined protocols the fix of these defects and the development of a further set of visualizations defined the requirements of the second version of aire in the final stages of aire development minambiente held an event that brought together representatives of all colombian local environmental agencies target users of the software we were granted a space to show a live demo of aire although this may not be considered a formal acceptance test the reception of the software was mostly positive we could additionally gather requirements on new desirable features to facilitate the identification of outliers and counting the number of days in each interval of the ica index which is now implemented in the final version of aire see visualization in fig 4 in conjunciton with these activities bogotá s central environmental agency secretaría distrital de ambiente made important contributions in separate meetings by validating the box plot visualizations that aire generates to show the results of comparative analyses the implementation of data handling and visualization requirements were also validated by academia experts outside universidad de los andes we presented aire at a scientific conference organized by the colombian association for operational research asocio held in 2017 díaz baquero et al 2017a we discussed the algorithmic details of the implementation and improvements to the visualizations including controls that would allow zooming into compact graphical renderings now implemented in fig 10 and smoothing to facilitate the identification of trends within the data not yet implemented finally this work was also presented to representatives of academia with expertise in air quality a demo of aire was given to a panel composed by academics using data of the monitoring network of bogotá at the 2017 colombian congress and international conference on air quality and public health casap díaz baquero et al 2017b experts and researchers suggested adding to aire visualizations on the amount of available data before analysis which has now been implemented in several analyses in other visualizations we added slide control bars to allow the user choose a tolerance level for the amount of missing data see for instance fig 4 we also presented aire at the 23 rd international sustainable development research society isdrs conference franco et al 2017 the importance of making results reproducible for all analyses upon which high level decisions would be made was remarked as well as the opportunity of endowing aire with the functionality to export cleaned data now implemented and tables and plots into reports not yet implemented 5 conclusion and future work this work presents aire a web based open source software package in r and shiny for air quality data cleaning and authentication analysis visualization and reporting the software implements data loading cleaning and analysis while providing various visualization functionalities aiming at strengthening the process of analysis and reporting of air quality data by entities in charge of managing air pollution aire was developed and validated in collaboration with the colombian environmental authorities and academia experts the code of the tool can be freely accessed from the gitlab page of aire see appendix a the authors believe that the process of developing aire was extremely valuable through gathering the public sector and academia around the same table with the ultimate purpose of supporting cities in air quality management while strengthening local capabilities to improve urban air pollution we believe that aire can offer a useful complement to other open source initiatives for the analysis and interpretation of air pollution data the future work on aire includes the implementation of new functionalities such as the analysis of meteorological variables and spatial distribution of air pollution in a specific geographical location several local environmental agencies have also suggested adding spacial analysis capabilities visualizing the locations of monitoring stations in a city together with aggregated data would be of great help to the authorities even more so if a spacial distribution of contaminants would be added by using a spacial interpolation model we also believe that an important step would be to reach the general public as the audience of aire visualizations thus raising awareness and creating participation finally the addition of predictive and prescriptive analysis capabilities as well as generating a historical report of all operations performed and results obtained during a session will be considered as discussed earlier declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a software availability developer juan josé díaz contact address cra 1 n 18a 12 dep of industrial engineering universidad de los andes bogotá colombia contact emails jj diaz1067 uniandes edu co ivan mura dukekunshan edu cn tested browsers firefox and google chrome software required r rstudio r packages required shiny ggplot2 plotly openair rcolorbrewer scales grid shinythemes programming language r available since 2018 availability the aire source code package is stored in a freely accessible gitlab repository hosted by duke university https gitlab oit duke edu im90 aire cost free 
25876,evaluating distributed soil erosion models is challenging because of the uncertainty in models and measurements of system responses here we present an approach to evaluate soil erosion and sediment delivery models which incorporates sediment source fingerprinting and sediment rating curve uncertainty into model testing we applied the generalized likelihood uncertainty estimation glue methodology to the sediment delivery distributed model sedd for a large catchment in southeast brazil the model was not rejected as 23 4 of model realizations were considered behavioral fingerprinting results and sedd simulations showed a partial agreement regarding the identification of the main sediment sources in the catchment however grid based estimates of soil erosion and sediment delivery rates were highly uncertain which restricted the model s usefulness for quantifying sediment dynamics although our results are case specific similar levels of error might be expected in erosion models elsewhere the representation of such errors should be standard practice keywords soil erosion models sediment loads sediment fingerprinting rusle sedd glue 1 introduction spatially distributed soil erosion and sediment delivery models have received significant attention from the erosion modelling community arguably due to their potential usefulness for identifying erosion prone areas and the main sediment sources within large catchments however evaluating the usefulness of such models is inherently challenging measurements of model parameters and system responses are necessarily uncertain the spatial and temporal resolution of models and observational data are frequently divergent and the definition of what is a useful model is often subjective oreskes and belitz 2001 moreover our ability to measure erosion rates across landscapes is limited and methods for doing so are known to be flawed parsons 2019 since model evaluation is an essential step to recognize model failure and to consequently gain knowledge about the modeled phenomena how should we proceed in face of the aforementioned challenges currently the most common approach for testing distributed erosion models at the catchment scale is based on a comparison between observed and modeled outlet sediment loads the estimation of observed loads usually rely on i suspended solid measurements and or sediment rating curves didoné et al 2015 jain and ramsankaran 2018 krasa et al 2019 vigiak et al 2015 ii temporally spaced bathymetric surveys or excavations of ponds and reservoirs de vente et al 2008 eekhout et al 2018 tanyaş et al 2015 or iii radiometric dating of lake sediment cores smith et al 2018b although a comparison against sediment loads can give an indication of a models capability to simulate sediment transport rates at the outlet of a catchment it provides no information on the adequacy with which models simulate erosion patterns or identify sediment sources moreover models have been known to reproduce observed outlet sediment loads for the wrong reasons through misrepresenting internal catchment processes see pontes 2017 for an example therefore the outlet based approach for testing distributed erosion models has received criticism favis mortlock et al 2001 govers 2011 jetten et al 2003 parsons et al 2009 and modelers have pursued other sources of data to evaluate internal process representations for instance field monitoring of erosion features combined with volumetric measurements of rills gullies and sediment deposition drapes can provide spatially referenced information of internal erosion dynamics that are commensurate with model simulations evans and brazier 2005 takken et al 1999 van oost et al 2005 alternatively tracing techniques have been used to estimate medium to long term soil redistribution rates which are also comparable to distributed erosion model outputs lacoste et al 2014 porto and walling 2015 walling et al 2003 warren et al 2005 more recently zweifel et al 2019 and fischer et al 2018 demonstrated how aerial images could be used to visually classify the severity of erosion features and how this classification was appropriate to assess the capability of spatially distributed models to relatively rank erosion prone areas while the previously described sources of data for model testing are useful for evaluating simulations of on site erosion they offer little information about sediment transport to water courses and subsequent off site erosion impacts therefore they cannot be used to test the sediment delivery or routing components of distributed erosion models models such as watem sedem van oost et al 2000 van rompaey et al 2001 verstraeten et al 2010 morgan morgan finey mmf morgan 2001 morgan et al 1984 and the sediment delivery distributed model sedd ferro and minacapilli 1995 ferro and porto 2000 represent hillslope connectivity to the stream network either by routing sediment transport capacity along the flowpath or by estimating a topography based sediment delivery ratio these models are therefore not only able to simulate how much sediment is delivered to water courses but also to identify where it comes from to evaluate the quality of such simulations quantitative data of sediment provenance is necessary a technique that provides quantitative apportionments of sediment provenance is sediment source fingerprinting in this approach physical and biogeochemical attributes of sink sediments are used to trace their origin from potential upstream sources klages and hsieh 1975 yu and oldfield 1989 walling and woodward 1995 relative source contributions are then calculated by solving end member un mixing models based on source and sink sediment tracer concentrations collins et al 1997 cooper et al 2014 laceby and olley 2015 such estimates are conceivably comparable to the outputs of distributed soil erosion models with a sediment routing delivery component however a meaningful comparison requires fingerprinting source stratifications to be reasonably analogous to model outputs an interesting example was presented by wilkinson et al 2013 in which sediment fingerprinting was used to model the contributions of different erosion processes i e surface and subsurface to sediment loads in the burdekin river basin australia 130 000 km2 the resulting source apportionments were compared to the sediment budget river network sednet model outputs wilkinson et al 2009 since sednet calculates sediment budgets by differentiating inputs from different erosion processes i e gullies sheetwash results provided a useful analogy likewise borrelli et al 2018 were able to compare land use source apportionments from alewell et al 2016 to watem sedem model outputs in a 41 km2 catchment on the swiss plateau however a difficulty when testing erosion models in particular and environmental models in general arises from the epistemic uncertainties in model structures parameter estimation and the forcing testing data beven 2019 that is uncertainty is a result of a lack of knowledge about i the modeled phenomena models are inherently flawed approximations of reality ii the model parameters we cannot measure model parameters in every point in space and even if we could parameters are often empirical abstract aggregations that require calibration and iii the observational data erosion is a highly variable phenomenon and our methods for measuring it are somewhat inadequate testing models as hypotheses therefore requires representing the uncertainties in both models and the things we call observational data of systems responses beven 2018 it also requires a clear definition of model purpose and of the limits of acceptability of model error beven 2006 2009 these concepts provide the foundation of the generalized likelihood uncertainty estimation glue beven and binley 1992 methodology in which monte carlo simulations are used to create a large number of possible model realizations by sampling uncertain model parameters if the response surface does not produce acceptable realizations of the observational data then the model itself can be rejected as not useful for prediction at least under the testing conditions beven 2009 although sediment fingerprinting models are now consistently applied in stochastic structures usually relying on monte carlo simulations evrard et al 2013 pulley et al 2016 smith and blake 2014 or bayesian modelling blake et al 2018 cooper and krueger 2017 soil erosion models are more frequently used in a deterministic fashion moreover outlet sediment loads which are the common forcing testing data with which models are evaluated are also represented deterministically therefore an uncertainty based framework for incorporating sediment fingerprinting into soil erosion model testing is lacking in this study we present a novel approach to evaluate spatially distributed soil erosion sediment delivery models that represents the uncertainties in both models and observational data since we understand that the purpose of spatially distributed models is to not only to provide acceptable simulations of outlet transport rates but also to represent sediment dynamics within a catchment we use sediment loads and sediment fingerprinting source apportionments as model evaluation data by use of the glue methodology we apply the sedd model to a 6600 km2 river basin in southeast brazil although glue has been used in other soil erosion modelling applications our testing framework is the first to define limits of acceptability of model error according to the uncertainty in the observational sediment load data a further novelty is the evaluation of behavioral model simulations against sediment fingerprinting source apportionments which have been stratified based on a hierarchical tributary design that facilitates model comparisons along different stages of sediment transport our approach is implemented on free gis software and programming languages being fully reproducible and or adaptable elsewhere the outcomes of this research therefore provide a much needed open source framework for incorporating uncertainty analysis into distributed soil erosion models applications moreover it demonstrates how sediment fingerprinting and potentially other sources of data can be assimilated into model testing within a stochastic structure 2 methods 2 1 catchment description the mortes river drains an area of approximately 6600 km2 in the south of the state of minas gerais brazil fig 1 the river s headwaters are in mantiqueira mountain range and it flows until its confluence with the grande river at the funil hydroelectric power plant reservoir elevation within the basin ranges from 1414 m to 807 m according to köppen s classification the climate in the area is predominantly humid subtropical with dry winters and warm summers i e a cwb climate type alvares et al 2013 average annual rainfall is approximately 1500 mm fick 2017 which is almost entirely concentrated in the spring and summer months hapludoxes 48 and dystrustepts 35 are the main soil classes in the basin table 1 the first are very deep highly weathered leached soils while the latter are much less pedogenetically developed shallow and erosion prone most of the catchment is occupied by pastures 66 often degraded by over grazing and or lack of adequate management remaining forest areas 22 are mostly found on ridges and buffer strips along the stream network croplands which are mostly composed of maize fields for silage production occupy a small portion of the catchment area 5 eucalypt forests 5 are commonly planted for charcoal manufacturing most of the agricultural areas notably in the carandaí mortes pequeno and pirapetinga sub catchments are associated with the occurrence of hapludoxes fig 1 table 1 dystrustepts support extensive pastures for raising dairy cattle and or eucalypt plantations the mortes river basin was chosen for this study due to the availability of continuous sediment concentration and discharge data from the ibituruna gauging station fig 1 although water discharge records are frequently made available by the brazilian water agency sediment concentration data are difficult to obtain moreover field observations and bathymetric surveys have shown that the mortes river delta is the main sedimentation zone in the funil reservoir although the reservoir was built in 2003 the high sedimentation rates in mortes river already impede navigation near its delta 2 2 sediment load data suspended sediment concentration mg l 1 and water discharge m3 s 1 were monitored in the ibituruna gauging station fig 1 from march 2008 to december 2012 batista et al 2017 measurements were taken on an approximately monthly basis resulting in 44 observations in order to estimate long term sediment loads we fitted a sediment rating curve relating suspended solid concentration to water discharge by ordinary least squares both variables were log transformed as the relationship between sediment concentration and discharge in the log scale is approximately linear vigiak and bende michl 2013 the goodness of fit of the linear model was visually assessed with residual and quantile quantile plots these and all other statistical analyses here presented were performed with the r programming language r core team 2019 in order to propagate the error of the fitted model 104 posterior simulations of the model coefficients were generated by an informal bayesian function from the r package arm gelman and hill 2007 this function uses the model residual standard errors to create multivariate normal distributions of model coefficients thus preserving their correlation when estimating posterior simulations next daily sediment concentrations values were calculated based on continuous mean daily discharge records from the brazilian water agency for the ibituruna gauging station 1992 2013 and the simulations of model coefficients these concentration values were used to estimate daily sediment loads ton day 1 which were subsequently aggregated into monthly annual and average annual transport rates in summary the 104 simulations of daily sediment concentrations were used to propagate the rating curve uncertainty when calculating long term sediment loads as pointed out by vigiak and bende michl 2013 this approach only quantifies the regression uncertainty and the actual errors associated to sediment load calculations might be underestimated more detailed descriptions of bayesian and bootstrapping methods for propagating the uncertainty of sediment rating curves can be found in rustomji and wilkinson 2008 and in vigiak and bende michl 2013 2 3 sediment fingerprinting data 2 3 1 sampling design and sample collection in order to facilitate a comparison between sedd model outputs and fingerprinting source apportionments we developed a tributary sampling design in which sub catchments of the mortes river basin were treated as end member sediment sources in addition we adopted a hierarchical approach for sink sediment sampling blake et al 2018 boudreault et al 2019 koiter et al 2013 that is considering the disconnectivity of the sediment cascade on large river basins due to the variability of residence times of sediment storage koiter et al 2013 we understood it was important to sample sink sediments at different nodes of the main river channel fig 1 as a result of our sampling design three nodes with four potential upstream sources each were stratified within the catchment node 1 has four main tributaries the mortes river mrt itself before its confluence with the elvas river elv the carandaí river crd and the santo antônio river sta due to our hierarchical approach node 1 sediments become a potential source of the next downstream node hence node 2 sources are comprised by the mortes pequeno river mpq the peixe river pxe the set of small tributaries in the mid catchment t1 and node 1 similarly node 3 on the catchment outlet receives sediments from the pirapetinga river pir the tabões river tab the set of small tributaries in the lower catchment t2 and node 2 sediment sampling was conducted in two different periods to represent transport dynamics during the well defined seasons of the local climate during september 2017 dry season all nodes and sources were sampled in february 2018 during the rainy season we retrieved extra samples from the sink sediment nodes source samples were taken from lag deposits of tributaries near their confluence with the main river channel the uppermost layer 1 2 cm of freshly deposited sediments from river margins was scrapped with a plastic trowel and approximately 15 scrapes were combined into one individual sample we collected a total of 20 composite samples per each tributary except for sources t1 and t2 these sources are comprised by a set of small tributaries that drain directly to the mortes river fig 1 hence 25 and 17 samples were retrieved in t1 and t2 respectively 4 5 samples from each small tributary sampling sink sediments from nodes 1 and 2 followed the same methods described above during the dry season 20 samples were collected from each of these nodes whereas during the rainy season 12 and 20 samples were retrieved from nodes 1 and 2 respectively given that the mortes river flows into the funil reservoir samples from node 3 were taken from the bottom of the shallow river delta before its confluence with the grande river at the node 3 site 26 and 12 composite samples were collected during the dry and rainy seasons respectively 2 3 2 laboratory analyses sediment samples were oven dried at 60 c and dry sieved with a 0 2 mm mesh subsequently total concentration of the 21 following elements was determined by inductive coupled plasma optical emission spectrometry icp oes al as ba ca cd ce co cr cu fe k la mg mn ni pb se ti v zn zr the 0 2 mm particle size was selected according to the texture analysis of the sink sediments from node 3 at the mortes river delta supplementary material table 1 the analysis indicated that an important fraction of the target sediments were composed of fine sand 0 2 0 05 mm and that fractionating samples to a finer size could lead to unrepresentative results 2 3 3 element selection the first step of tracer selection is to investigate the composition of source and sink sediments here we started with an exploratory analysis by visually examining box plots of element concentrations next a range test was performed to verify if sink element concentrations were well bounded by the source mixing polygon that is if element contents on sink sediments are enriched or depleted in relation to source samples then there is evidence that elements might not be behaving conservatively during sediment transport or there is a missing source laceby et al 2017 smith and blake 2014 moreover a mismatch of element concentrations on source and sink sediments may compromise the numerical solutions of the un mixing models collins et al 2013 the range test therefore aims not only to eliminate elements plotting outside the mixing polygon from further analyses but also to provide an initial insight into the quality of the geochemical data different approaches have been employed for analyzing conservative behavior and for performing range tests smith et al 2018a wilkinson et al 2015 although earlier research might have focused on maximum and minimum tracer values distribution based un mixing models bayesian or frequentitst requires an examination of the distributions of tracer concentrations considering the structure of the bootstrapping approach we employed for solving our un mixing model we adopted a mean and standard deviation range test that is we assumed that means and standard deviations of log transformed tracer concentrations on sink sediments should plot within the means and standard deviations of the source log transformed tracer concentrations this ensures that during the monte carlo simulation sampled sink element contents will always be within the source range the means and standard deviation range test was performed locally for each node and sampling season given the heterogeneity of land uses and geological pedological backgrounds of the sub catchments comprising sediment sources in the mortes river basin i e catchments do not display a definite pattern of source signal development agents table 1 a process based approach to element selection e g batista et al 2019 koiter et al 2013 laceby et al 2015 was not appropriate to this research hence we adopted a more common statistical procedure in which elements passing the range test were submitted to a step wise forward linear discriminant analysis lda niveau 0 1 this approach aims to define a minimum set of tracers that maximize source discrimination and elements selected by the lda were used for modelling again the procedure was repeated for all nodes and sampling seasons 2 3 4 un mixing modelling relative sediment source contributions were calculated by minimizing the sum of squared residuals srr of the un mixing model 1 s s r i 1 n c i s 1 m p s s s i c i 2 where n is the number of elements used for modeling c i is the concentration of element i in the target sediment m is the number of sources p s is the optimized relative contribution of source s and s si is the concentration of element i in source s optimization constraints were set to ensure that source contributions p s were non negative and that their sum equaled 1 in order to quantify the uncertainty in the un mixing model source apportionments we employed the bootstrapping methods described in batista et al 2019 the model was solved by a monte carlo simulation with 2500 iterations for each iteration log transformed element concentrations were sampled from multivariate normal distributions generated from the source and sink geochemical data during the monte carlo simulation values were back transformed by an exponential function log transformation was applied to avoid sampling negative element concentrations and to force a near normal distribution on the typically skewed sediment geochemistry data the optimization function was scripted with the r package rsolnp ghalanos and theussl 2015 whereas the monte carlo simulation here and elsewhere in this study with the package foreach calway et al 2017 2 4 sedd model description the sedd model calculates a spatially distributed sediment delivery ratio sdri that expresses the proportion of eroded sediments that are delivered to the stream network ferro and minacapilli 1995 ferro and porto 2000 the model does not represent channel erosion or deposition processes and sediments reaching the stream network are assumed to reach the catchment outlet following a grid based structure the sdri was calculated as 2 s d r i exp β l i s i where sdr i is the soil delivery ratio of a grid cell i β is a catchment specific empirical parameter m 1 l i is the flow length from cell i to the nearest stream channel m along the flow path and s i is the slope of cell i m m 1 typically the empirical parameter β is calibrated to minimize the errors of sediment load predictions fernandez et al 2003 fu et al 2006 lin et al 2016 whereas the flow length and slope parameters can be derived from dem processing the sdri grid is used to calculate area specific sediment yields ssyi ton ha 1 yr 1 which quantifies the amount of sediments that are delivered from cell i to the stream network 3 s s y i s d r i a i where ssy i is the specific sediment yield for a grid cell i sdr i is the soil delivery ratio for a grid cell i and a i is the annual soil loss computed by revised universal soil loss equation rusle for a grid cell i rusle estimates average annual erosion rates by the following empirical equation renard et al 1997 4 a r k l s c p where a is soil loss per unit area t ha 1 yr 1 r is the rainfall and runoff erosivity factor mj mm ha 1 h 1 yr 1 k is soil erodibility factor t ha h ha 1 mj 1 mm 1 ls is the topographic factor representing slope length and steepness dimensionless c is the cover management factor dimensionless and p is the support practice factor dimensionless as the sedd model neglects channel deposition the total sediment yield at the catchment outlet can be calculated as the sum of ssyi values weighted by cell area equivalently the mean of ssyi values corresponds to the area specific sediment yield in the catchment and the same calculations can be employed at sub catchment scale with this approach sub catchment relative contributions can be estimated based on sedd results of note we chose sedd for three main reasons i the model requires calibration which makes it particularly suitable for glue ii the model has few parameters which facilitates computer processing when making a large number of simulations with large spatial datasets and iii the model is rusle based which gives us the opportunity to scrutinize the uncertainty associated to the most widely used soil erosion model in world 2 5 glue the glue methodology can be summarized in five decision steps beven 2009 which include i the definition of a likelihood measure to evaluate model realizations ii definition of a rejection criteria for non behavioral model realizations iii definition of uncertain model parameters iv definition of distributions to characterize parameter uncertainty and v definition of a simulation method for creating model realizations we did not establish a formal likelihood measure to evaluate model realizations as the rejection criteria for non behavioral simulations was set according to an actual range of system responses that is all model realizations which produced sediment load responses within the 95 prediction interval of the sediment load rating curve were considered behavioral since the sedd temporal scale is inherited from the rusle the model simulates long term average annual sediment yields therefore for comparison purposes the sediment rating curve estimates were aggregated into a 22 years average model realizations were generated by a monte carlo simulation with 1000 iterations fig 2 sedd parameter β was sampled from a log uniform distribution with minimum and maximum parameters retrieved from typical values reported in the literature min 0 000001 m 1 max 0 1 m 1 e g porto and walling 2015 taguas et al 2011 we used a log uniform distribution to ensure that the extreme values of this broad range were sampled during the simulation the threshold for stream definition which affects drainage density and therefore distance to streams l i was sampled from a uniform distribution min 50 000 m2 max 5 000 000 m2 to represent the uncertainty in the dem derived model variables we created a pseudo random error surface for each model iteration mean and standard deviation of dem errors were retrieved from the nasa srtm report rodriguez et al 2006 μ 1 7 m σ 4 1 m and used to create a normally distributed error field which was added to the original dem all terrain attributes used in the models were then calculated within the monte carlo simulation all herein described spatial analyses were supported by saga gis conrad et al 2015 and the r package rsaga brenning and bangs 2015 the r scripts used for model implementation and uncertainty analysis are provided as supplementary material along with raw fingerprinting and discharge data additional r packages used in the simulations include raster hijmans and van etten 2012 trucnorm mersmann et al 2018 doparallel ooi et al 2019 rgdal bivand et al 2019 wgcna langfelder and horvath 2019 since we understood that rusle factors were not parameters requiring calibration or conditioning but instead uncertain model variables we performed a forward uncertainty analysis similarly to biesemans et al 2000 and van rompaey and govers 2002 although this can be seen as a separate analysis rusle error was propagated into sedd simulations as we explain in the following the forward error analysis was performed with a monte carlo simulation with 1000 iterations in order to represent the uncertainty in the rusle r factor we first calculated a deterministic rainfall erosivity map supplementary material fig 1 this was carried out with average monthly and annual rainfall grids from worldclim fick 2017 and the regression equation developed by aquino et al 2014 this regression equation estimates annual or average annual in this case ei30 index values and it was originally fitted using detailed rainfall data from the municipality of lavras for each iteration of the monte carlo simulation we added a normally distributed error surface to the deterministic rainfall erosivity map with mean equal zero and a standard deviation equal to 10 of mean deterministic r factor for the catchment for the k factor we created truncated normal distributions for each soil class occurring in the catchment soil map feam 2010 the discrete soil map was rasterized and for each simulation a grid cell erodibility value was sampled according to its corresponding soil class distribution parameters were set according to published k factor values for brazilian soils see silva et al 2019 although in general there were not enough different estimations of k factor values for individual soil classes to create data based probability distributions we used the available published data and our own interpretation to infer distribution parameters table 2 uncertainty in the ls factor was represented following the dem error propagation described above slope rad and catchment area m2 grids were created for each model iteration these grids were subsequently used to calculate the ls factor with the equation of desmet and govers 1996 a maximum threshold of 10 800 m2 was enforced to the catchment area grid which corresponds to maximum flow length of 360 m for a 30 m resolution dem this was performed to avoid spuriously high ls factor values in flow concentration areas as usually carried out in rusle applications panagos et al 2015 schmidt et al 2019 the maximum threshold was empirically defined based on remote sensing and field observations from the study area in this case lower flow accumulation thresholds would lead to hillslopes being identified as flow concentration zones e g gullies ravines hollows when rill and interril should still be more representative of the erosion processes similarly to the k factor errors in the c factor estimation were propagated by creating truncated normal distributions for individual land use classes table 3 the land use grid was produced using 30 m resolution landsat 8 surface reflectance images from 2013 and the methods described in batista et al 2017 since no widespread support management practices are found in the catchment agricultural areas no specific procedure was applied to represent p factor uncertainty however the c factor distribution parameters for cropland and eucalypt were set to reflect occasional contour cropping and or crop residue management a summary of published c factor values for typical brazilian land uses and crop management can be found in silva et al 2019 the resulting rusle model realizations were used as input for the sedd model simulations moreover we performed a sensitivity analysis by fixing each model factor and sampling the remaining variables in new monte carlos simulations each with 1000 iterations this enabled us to evaluate the proportion of model variance explained by each factor it should be highlighted that forward error propagation is essentially subjective given its total dependence on the assumptions made by the modeler about potential sources of uncertainty beven 2009 our approach presents a rather conservative estimate of model uncertainty basically representing the errors involved in parameter estimation this is because we could not describe all the sources of error in the model structure moreover we wanted to constrain model realizations based on choices of factor values that modelers are expected to make hence we did not want to give the models full freedom if all parameters and variables are allowed to vary beyond a range of physical meaning models are capable of reproducing almost any answer usually for the wrong reasons see batista et al 2019a 2 5 1 spatial representation of model uncertainty in order to represent the spatial uncertainty of the final sedd model predictions we first filtered the behavioral model simulations according to the criterion previously described next we calculated the 2 5 50 and 97 5 quantiles for each grid cell ssy i estimates absolute error grids were then calculated by subtracting the 97 5 grid by the 2 5 grid relative errors were determined as 5 r e i a e i m i 100 where ae i is the absolute error for a grid cell i and m i is the simulation median for grid cell i the filtered behavioral model realizations were also used to calculate total sediment yields from the sub catchments described in table 1 these calculations were used to estimate the relative contribution of the sub catchments to the aggregated sediment yields at each sink sampling location i e nodes 1 2 and 3 relative contributions were calculated by dividing individual sub catchment sediment yields by the sum of all loads from contributing sub catchments the sedd estimated relative contributions were then evaluated against fingerprinting source apportionments the same approach was employed for creating rusle error maps except in this case all model simulations behavioral or not were considered when calculating grid cell quantiles 3 results 3 1 discharge curve the error propagation method used to represent the uncertainty in the sediment rating curve resulted in a broad estimate of average annual specific sediment yields with a 95 prediction interval of 0 47 11 95 ton ha 1 yr 1 mean 3 45 ton ha 1 yr 1 median 2 52 ton ha 1 yr 1 fig 3 a as expected annual estimates of sediment loads were more uncertain for the years with greater discharge and sediment transport fig 3 c monthly calculations revealed that over 85 of the annual sediment load is transported from november to march the monthly relative contributions to annual sediment yield showed less uncertainty than annual and average annual estimates fig 3 b 3 2 sediment fingerprinting 3 2 1 element selection our exploratory analysis demonstrated that cu and zn displayed spurious concentration patterns as a large proportion of sample measurements 30 were below detection limit these elements were therefore omitted from further scrutiny of the remaining 19 elements 16 84 17 87 and nine 47 plotted within the source mixing polygons for nodes 1 2 and 3 respectively for the dry season table 4 for the rainy season 18 95 elements passed the range test for nodes 1 and 2 whereas only seven 37 elements were within source range for node 3 sediments for node 1 the forward step wise lda selected 12 elements for the dry season sediments whereas 11 elements were selected for the rainy season table 4 the lda for both seasons showed a reclassification accuracy of 100 for node 2 the discriminant analysis selected 10 elements for the dry season and 11 for the rainy season again all samples were correctly reclassified during the lda cross validation as fewer elements passed the range test for node 3 only six elements were selected by the lda for both seasons reclassification accuracy was lower in this case with 91 and 82 for the dry and rainy seasons respectively the largest errors associated to the lda reclassification for node 3 source samples can be visualized in the bi plots displayed in fig 4 3 2 2 un mixing model results un mixing model solutions for node 1 were highly uncertain for both seasons as demonstrated by the broad density curves displayed in fig 5 according to model estimates sources crd and elv seem to dominate sediment contributions in relation to mrt and sta at least considering the median and interquartile iqr values of the simulated source apportionments table 5 results for node 2 were less uncertain and revealed a greater contrast between seasonal sediment transport dynamics during the dry season the un mixing model indicated that a significant portion of sediments reaching node 2 are derived from pxe median 50 iqr 32 64 however such contributions decrease during the rainy season for which the models suggest a large apportion of sediments from node 1 median 60 iqr 44 74 modeled source contributions from mpq and t1 were relatively low for both seasons table 5 model solutions for node 3 displayed a similar pattern to node 2 regarding the seasonal variation of source contributions during the dry season a greater proportion of sediments were estimated to derive from the sources proximally located to the catchment outlet particularly tab median 33 iqr 15 50 however rainy season source apportionments indicate that most of the sediments reaching the funil reservoir are originated on the upstream areas of the catchment which are represented by node 2 median 61 iqr 34 80 this illustrates how even in the relative short time period represented by our study sediments from the upper and mid catchment area are transported throughout the river network given that most of the mortes river sediment load is transported during the rainy season it is plausible to assume that upstream sediments are important contributors to reservoir sedimentation 3 3 rusle uncertainty the results of the forward error analysis revealed that rusle estimates were highly uncertain in spite of the moderately conservative assumptions made regarding sources of model error the median of grid cell absolute errors was of 29 ton ha 1 yr 1 which translated to a median relative error of 588 as expected the highest absolute errors in the rusle estimates were associated to the areas with higher erosion rate predictions fig 6 b c contrarily relative errors were higher on the areas with lower soil loss estimates this is possibly a result of small variations on sampled parameter values leading to a large relative fluctuation on the low erosion predictions fig 6 a considering the median of the simulations as a point based estimate of erosion rates the influence of soil erodibility on soil loss predictions was evident in fig 6c upper and mid catchment areas where dystrustepts are widespread had overall higher erosion rates according to the model simulations moreover modeled erosion hot spots are visibly associated to areas with high flow accumulation and more intensive land uses e g cropland eucalypt the sensitivity analysis demonstrated that the c factor was the largest source of uncertainty in the model predictions the proportion of model variance explained by the c factor had a median value of 45 iqr 30 56 the ls median 21 iqr 17 27 and k factors median 15 iqr 10 20 also contributed significantly to the propagated model errors the r factor had a small influence on overall model uncertainty median 3 iqr 2 5 3 4 sedd results from the 1000 sedd model realizations generated by the monte carlo simulation 234 were behavioral that is 234 model realizations provided estimates of outlet based ssy within the 95 prediction interval of the sediment rating curve 0 47 11 95 ton ha 1 yr 1 most of the non behavioral model response surface was associated to an overestimation of the curve calculated sediment yields fig 7 a by analyzing the dotty plots of sampled parameter values it was clear that the empirical parameter β had a preponderant influence on the model results fig 7 c behavioral model realizations are concentrated within a relatively narrow range of β values whereas acceptable system representations are spread throughout the sampled values of stream definition thresholds the fluctuation of mean catchment sdr i values in the catchment led to a linear increment of estimated ssy fig 7 b indicating little influence of rusle simulation results in the outlet aggregated sedd model predictions behavioral model realizations had mean sdr i values between 5 and 50 which highlights the uncertainty in the model predictions considering the median of the behavioral model realizations grid cell ssy i estimates had a median value of 0 06 ton ha 1 yr 1 whereas the median of analogous absolute error values was 6 64 ton ha 1 yr 1 although outlet lumped model results seem to be little influenced by the uncertainty in the rusle or in the stream definition threshold the errors derived from such input variables parameters are explicit when the uncertainty of spatially distributed ssy estimates are presented in fig 8 a areas with large absolute errors in the ssy map clearly match the rusle errors displayed in fig 7 b moreover the influence of stream definition threshold uncertainty is visible in the surroundings of lower order streams 3 4 1 evaluation of sedd results against fingerprinting source apportionments distributions of relative source contributions estimated by the sedd model overall displayed a similar pattern to the rainy season fingerprinting source apportionments except for node 1 fig 9 opposite to the fingerprinting results sedd simulations indicated that mrt was the main source of sediments iqr 52 9 53 4 reaching the main river channel node 2 results revealed an agreement between rainy season fingerprinting and sedd estimated relative source contributions as all sedd model realizations were bound by the iqr of the fingerprinting apportionments however sedd simulations calculate an even larger contribution of node 1 sediments iqr 72 6 73 0 similarly fingerprinting results for the rainy season for node 3 showed a similar pattern to the sedd simulations both models indicate that node 2 sediments are the largest contributors to outlet sediment loads although sedd results again suggest a greater contribution of upstream sediments node 2 iqr 84 3 85 7 moreover sedd estimated tab contributions iqr 3 2 3 4 were considerably lower than the ones estimated by the sediment fingerprinting un mixing models 4 discussion 4 1 uncertainty in the forcing data the model testing framework presented here demonstrated how uncertainty permeates all facets of soil erosion models and the things we call observational data the error propagation method used to represent the uncertainty in the sediment rating curve resulted in such broad estimates of average annual sediment loads that many different sedd realizations were able to encompass the forcing data similar results have been reported by other soil erosion modelers banis et al 2004 janes et al 2018 and a logical conclusion is that we need better data in order to reject non behavioral model realizations importantly the uncertainty in the average estimates of annual sediment loads highlight how sediment concentration measurements at the ibituruna gauging station need to be intensified particularly during high flow events by comparing discharge values recorded during the sediment concentration measurements and those observed between 1992 and 2013 it becomes clear that high flow events may not have been adequately represented by the current sampling regime supplementary material figs 2 and 3 extrapolations of the rating curve for extreme events may therefore result in additional uncertainty in the sediment load estimates considering the importance of the mortes river for hydroelectric power generation as well as the high sedimentation rates observed at the funil reservoir we strongly recommend establishing a thorough water and sediment monitoring program in the catchment nevertheless even if more accurate and precise sediment load data were available our approach has demonstrated how very different spatial model representations can produce similar outlet responses despite the fact that we only considered behavioral simulations while calculating the uncertainty of grid cell ssy i estimates absolute model errors were almost hundred fold the median of the predictions this brings to question if the numerical spatial predictions are at all useful furthermore it demonstrates how misleading it can be to neglect model and observational data uncertainty in soil erosion and sediment delivery models 4 2 uncertainty in the sedd model the fact that the sedd results were mostly driven by the empirical and somewhat abstract parameter β raises some concerns about the quality of process representation in the model the common deterministic parameter optimization method for calibrating β should be therefore disputed if sedd model simulations are to provide meaningful system representations alternative methods for deriving β values should be encouraged e g ferro and stefano 2003 porto and walling 2015 nonetheless given the sensitivity of the model to parameter β representing the uncertainty associated to the parameter estimation is paramount in spite of the large errors associated to grid cell ssyi estimates aggregated sub catchment relative contributions calculated from the sedd simulations were precise as shown by the narrow uncertainty bands in fig 9 as the sum of the grid based model realizations should somewhat converge sub catchment sediment loads are expected to display smaller variances than the grid cell rates this demonstrates that at the very least spatially aggregated results were consistent as the model repeatedly identified the same sub catchments as the main sediment sources the accuracy of these estimates is difficult to assess although some insight can be gained by an evaluation against fingerprinting source apportionments 4 3 uncertainty in sediment fingerprinting source apportionments the bootstrapping method for solving the un mixing models resulted in uncertain sediment fingerprinting estimates of relative source contributions particularly for node 1 bootstrapping methods are known to produce somewhat spurious uncertainty bands for un mixing model results as local optimization functions frequently yield numerical solutions where one source provides 0 or 100 of the contributions cooper et al 2014 this is illustrated by the bi modal density curves in fig 4 nonetheless the uncertainty of node 1 un mixing model solutions might imply an issue with the data moreover the negligible contributions from mrt by far the largest sub catchment in the basin during the rainy season questions the consistency of the model results as a narrative it might be the case that there was an issue of particle size incommensurability between mrt and node 1 sink sediments mrt element concentrations were overall higher than in the remaining node 1 sources and sink samples which might indicate mrt sediments were composed by smaller sized particles an alternative hypothesis is that sediment storage in the mrt sub catchment is influenced by small hydroelectric plants or channel regulation in the mortes river before its confluence with the elvas river regardless fingerprinting and sedd model outputs showed a contrasting pattern for node 1 and we have no supporting evidence to corroborate either of the system representations on the contrary the overall correspondence of fingerprinting un mixing model solutions and sedd simulations of relative source contributions for nodes 2 and 3 while considering the uncertainty in both system representations provides some conditional corroboration of the methods although the sedd model simulates long term sediment transport dynamics and the fingerprinting approach was limited by the temporal scale of our sampling both modelling exercises designated that most of the sediments reaching nodes 2 and 3 are originated from farther upstream sources that is at least under the reasonable assumption that rainy season fingerprinting results represent the bulk of the sediment transport dynamics in the catchment for management purposes the convergence of model results is an important outcome of this research different models and sources of data have indicated that the sediments reaching the funil reservoir by the mortes river come from the mid and upper catchment areas even during a relatively short temporal scale i e the rainy season covered by the sediment sampling hence reducing reservoir sedimentation rates requires widespread soil conservation efforts throughout the entire catchment instead of local proximal interventions 4 4 uncertainty in the rusle another valuable outcome of this research was demonstrating how uncertain common large scale distributed rusle applications are although rusle is the most widely used soil erosion model in the world alewell et al 2019 studies which have attempted to quantify model error are scarce e g tetzlaff et al 2013 our results indicate that numerical rusle predictions of spatially distributed erosion rates lack utility given the uncertainty in the model outputs see fig 5 of course these results are case specific and entirely determined by the assumptions made about potential sources of model error which we understand were cautious that is the uncertainty in the rainfall erosivity regression equation was not properly assessed let alone in the equations relating rainfall intensity to kinetic energy wilken et al 2018 additionally errors in the soil and land use map classifications were not entirely represented nor were the potential errors in the plot based experiments used to generate rusle factors nearing 2000 parsons 2019 hence similar or larger errors should be expected in comparable spatially distributed rusle applications elsewhere unless otherwise demonstrated since we were explicit about our assumptions regarding sources of model error and fully reported the distribution parameters used in the uncertainty analysis readers can interpret the outputs accordingly we expect this should attenuate part of the subjectivity necessarily involved in forward uncertainty assessments moreover as model results are reported as distributions different levels of confidence can be attributed to the simulations according to the potential consequences one might expect from model mispredictions quinton 1997 overall results from our forward error analysis indicate that rusle modeled spatially distributed erosion rates should be viewed with extreme caution particularly when actual numerical model outputs are used to project the influence of climate and land use changes on future erosion rates due to the difficulties involved in large scale model parameterization the costs of plot based experiments for developing empirical model factors and the multiplicative structure of the rusle and usle family models we suspect that model applications will remain largely uncertain this might be particularly true for developing countries such as brazil where data scarcity further complicates model parameterization under such conditions model testing should hereon focus on evaluating if the models are at least consistently capable of relatively ranking erosion prone areas as in fischer et al 2018 importantly the high uncertainty associated with the rusle predictions contradicts the argument that usle type models are less error prone due to their low input requirements this is corroborated by schürz et al 2019 who demonstrated how model parameterization choices could lead to a variation in the order of two magnitudes in usle estimated soil losses these results indicate that large scale rusle simulations are likely to be no less uncertain than more complex process based models as pointed out by favis mortlock et al 2001 there is little justification in adopting the usle or its derivatives as standards by which to measure the quality of all other models this makes the evaluation biased towards the standard and inhibits the development of competing theories hence we suggest that the modelling community should explore alternative options for simulating soil erosion and sediment connectivity as different models might be more or less adequate according to the purpose scale and conditions of the modelling application recent developments with mmf eekhout et al 2018 peñuela et al 2018 smith et al 2018 tan et al 2018 geowepp poeppl et al 2019 and openlisem baartman et al 2020 starkloff et al 2018 demonstrate how soil erosion models can still be useful tools for understanding processes nevertheless we recommend that any further model evaluation which is a critical step for developing knowledge and confidence or doubt in model predictions should be firmly established upon an uncertainty based framework 4 5 limitations and future research an important limitation of the comparison described here between sedd model outputs and fingerprinting results stems from the temporal divergence of the analyses while the sedd model operates on a long term average annual time step our fingerprinting data provided only a snapshot of the sediment dynamics in the catchment moreover the sedd model does not represent channel erosion and deposition processes assuming a long term quasi equilibrium condition between hillslope sediment yield and the fluvial system although our fingerprinting modelling results indicate a strong connectivity between upstream tributaries and the mortes river outlet they also demonstrate that transient storage might be an important regulator of the sediment budget in the catchment as expected hence in order to improve the modelling of the catchment loads it should be necessary to combine hillslope soil erosion and sediment delivery simulations with models that explicitly account for river sediment dynamics e g czuba and foufoula georgiou 2014 schmitt et al 2018 additional modelling and measurement improvements should also investigate the contribution of gullies to catchment sediment loads as gully erosion was observed in field inspections during the sediment sampling campaigns furthermore future research should focus on elucidating temporal trends in sediment dynamics by use of different fallout radionuclides such as 7be and 210pbex evrard et al 2010 gellis et al 2019 le gall et al 2017 this should provide more fit for purpose evidence to evaluate soil erosion and sediment delivery models at different time scales 5 conclusions soil erosion models and the measurements of system responses we call observational data are necessarily uncertain the representation of such uncertainty is indispensable here we provided a framework for incorporating the uncertainty of sediment rating curves sediment fingerprinting un mixing models and soil erosion sediment delivery models into the glue methodology more specifically the framework was applied to the rusle based sedd model at a large catchment in southeast brazil our results have shown how large scale spatially distributed rusle applications are highly uncertain this means model applications of such type cannot afford to disregard uncertainty analysis and that modeled erosion rates should be interpreted with upmost caution sedd simulations of catchment sediment yields were also highly uncertain mostly due to the errors in the rating curve forcing data and the sensitivity of the model to the empirical parameter β spatially distributed simulations of area specific sediment yields were even more uncertain which meant the grid based numerical model outputs were of little utility however when the sedd model outputs were lumped into sub catchment relative contributions results were at least consistent the comparison between sedd model outputs the fingerprinting source apportionments presented here was facilitated by the hierarchical tributary sampling design moreover the uncertainty based framework enabled us to compare distributions of model realizations of relative source contributions the comparison revealed an overall similarity of fingerprinting and sedd modeled distributions of source apportionments although large discrepancies were found in part of the catchment ultimately we found that under the testing conditions the sedd model might be useful for identifying the sub catchments that contribute to most of the sediment load in the mortes river basin conversely the uncertainty in the simulations questions the model s usefulness for calculating actual erosion and sediment delivery rates from a falsifacationist perspective the model could not be rejected as multiple model realizations produced acceptable system representations however this was largely facilitated by the uncertainty in the forcing data one of the most important conclusions from this research is that we need better data in order to reject models or model realizations and therefore to improve our understanding of soil erosion and sediment transport in large river catchments this will require multiple sources of data and honest representations of the uncertainty in models and observations of system responses declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was funded in part by the coordination of improvement of higher level education personnel capes process number 88881 190317 2018 01 the national counsel of technological and scientific development cnpq process numbers 306511 2017 7 and 202938 2018 2 and the minas gerais state research foundation fapemig process numbers cag apq 01053 15 and apq 00802 18 comments from anonymous reviewers helped to improve the quality of this paper they are highly appreciated we are also thankful to claudia mignani for helping with our figures and to tom de lima for his assitance during field work appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 supplementary material fig 1 rainfall erosivity for the mortes river catchment supplementary material fig 1 supplementary material fig 2 violin a and box plots b of the water discharge values at the ibituruna gauging station during the suspended sediment concentration measurements ssc and the period to which the sediment rating curve was applied 1992 2013 supplementary material fig 2 supplementary material fig 3 monthly rainfall a and discharge b for the ibituruna gauging station 1992 2013 shaded area represents the 95 confidence interval of the data rainfall data for 2017 and 2018 when fingerprint sediments were sampled are also plotted in fig 3a no rainfall data was recorded for september 2017 supplementary material fig 3 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 104961 
25876,evaluating distributed soil erosion models is challenging because of the uncertainty in models and measurements of system responses here we present an approach to evaluate soil erosion and sediment delivery models which incorporates sediment source fingerprinting and sediment rating curve uncertainty into model testing we applied the generalized likelihood uncertainty estimation glue methodology to the sediment delivery distributed model sedd for a large catchment in southeast brazil the model was not rejected as 23 4 of model realizations were considered behavioral fingerprinting results and sedd simulations showed a partial agreement regarding the identification of the main sediment sources in the catchment however grid based estimates of soil erosion and sediment delivery rates were highly uncertain which restricted the model s usefulness for quantifying sediment dynamics although our results are case specific similar levels of error might be expected in erosion models elsewhere the representation of such errors should be standard practice keywords soil erosion models sediment loads sediment fingerprinting rusle sedd glue 1 introduction spatially distributed soil erosion and sediment delivery models have received significant attention from the erosion modelling community arguably due to their potential usefulness for identifying erosion prone areas and the main sediment sources within large catchments however evaluating the usefulness of such models is inherently challenging measurements of model parameters and system responses are necessarily uncertain the spatial and temporal resolution of models and observational data are frequently divergent and the definition of what is a useful model is often subjective oreskes and belitz 2001 moreover our ability to measure erosion rates across landscapes is limited and methods for doing so are known to be flawed parsons 2019 since model evaluation is an essential step to recognize model failure and to consequently gain knowledge about the modeled phenomena how should we proceed in face of the aforementioned challenges currently the most common approach for testing distributed erosion models at the catchment scale is based on a comparison between observed and modeled outlet sediment loads the estimation of observed loads usually rely on i suspended solid measurements and or sediment rating curves didoné et al 2015 jain and ramsankaran 2018 krasa et al 2019 vigiak et al 2015 ii temporally spaced bathymetric surveys or excavations of ponds and reservoirs de vente et al 2008 eekhout et al 2018 tanyaş et al 2015 or iii radiometric dating of lake sediment cores smith et al 2018b although a comparison against sediment loads can give an indication of a models capability to simulate sediment transport rates at the outlet of a catchment it provides no information on the adequacy with which models simulate erosion patterns or identify sediment sources moreover models have been known to reproduce observed outlet sediment loads for the wrong reasons through misrepresenting internal catchment processes see pontes 2017 for an example therefore the outlet based approach for testing distributed erosion models has received criticism favis mortlock et al 2001 govers 2011 jetten et al 2003 parsons et al 2009 and modelers have pursued other sources of data to evaluate internal process representations for instance field monitoring of erosion features combined with volumetric measurements of rills gullies and sediment deposition drapes can provide spatially referenced information of internal erosion dynamics that are commensurate with model simulations evans and brazier 2005 takken et al 1999 van oost et al 2005 alternatively tracing techniques have been used to estimate medium to long term soil redistribution rates which are also comparable to distributed erosion model outputs lacoste et al 2014 porto and walling 2015 walling et al 2003 warren et al 2005 more recently zweifel et al 2019 and fischer et al 2018 demonstrated how aerial images could be used to visually classify the severity of erosion features and how this classification was appropriate to assess the capability of spatially distributed models to relatively rank erosion prone areas while the previously described sources of data for model testing are useful for evaluating simulations of on site erosion they offer little information about sediment transport to water courses and subsequent off site erosion impacts therefore they cannot be used to test the sediment delivery or routing components of distributed erosion models models such as watem sedem van oost et al 2000 van rompaey et al 2001 verstraeten et al 2010 morgan morgan finey mmf morgan 2001 morgan et al 1984 and the sediment delivery distributed model sedd ferro and minacapilli 1995 ferro and porto 2000 represent hillslope connectivity to the stream network either by routing sediment transport capacity along the flowpath or by estimating a topography based sediment delivery ratio these models are therefore not only able to simulate how much sediment is delivered to water courses but also to identify where it comes from to evaluate the quality of such simulations quantitative data of sediment provenance is necessary a technique that provides quantitative apportionments of sediment provenance is sediment source fingerprinting in this approach physical and biogeochemical attributes of sink sediments are used to trace their origin from potential upstream sources klages and hsieh 1975 yu and oldfield 1989 walling and woodward 1995 relative source contributions are then calculated by solving end member un mixing models based on source and sink sediment tracer concentrations collins et al 1997 cooper et al 2014 laceby and olley 2015 such estimates are conceivably comparable to the outputs of distributed soil erosion models with a sediment routing delivery component however a meaningful comparison requires fingerprinting source stratifications to be reasonably analogous to model outputs an interesting example was presented by wilkinson et al 2013 in which sediment fingerprinting was used to model the contributions of different erosion processes i e surface and subsurface to sediment loads in the burdekin river basin australia 130 000 km2 the resulting source apportionments were compared to the sediment budget river network sednet model outputs wilkinson et al 2009 since sednet calculates sediment budgets by differentiating inputs from different erosion processes i e gullies sheetwash results provided a useful analogy likewise borrelli et al 2018 were able to compare land use source apportionments from alewell et al 2016 to watem sedem model outputs in a 41 km2 catchment on the swiss plateau however a difficulty when testing erosion models in particular and environmental models in general arises from the epistemic uncertainties in model structures parameter estimation and the forcing testing data beven 2019 that is uncertainty is a result of a lack of knowledge about i the modeled phenomena models are inherently flawed approximations of reality ii the model parameters we cannot measure model parameters in every point in space and even if we could parameters are often empirical abstract aggregations that require calibration and iii the observational data erosion is a highly variable phenomenon and our methods for measuring it are somewhat inadequate testing models as hypotheses therefore requires representing the uncertainties in both models and the things we call observational data of systems responses beven 2018 it also requires a clear definition of model purpose and of the limits of acceptability of model error beven 2006 2009 these concepts provide the foundation of the generalized likelihood uncertainty estimation glue beven and binley 1992 methodology in which monte carlo simulations are used to create a large number of possible model realizations by sampling uncertain model parameters if the response surface does not produce acceptable realizations of the observational data then the model itself can be rejected as not useful for prediction at least under the testing conditions beven 2009 although sediment fingerprinting models are now consistently applied in stochastic structures usually relying on monte carlo simulations evrard et al 2013 pulley et al 2016 smith and blake 2014 or bayesian modelling blake et al 2018 cooper and krueger 2017 soil erosion models are more frequently used in a deterministic fashion moreover outlet sediment loads which are the common forcing testing data with which models are evaluated are also represented deterministically therefore an uncertainty based framework for incorporating sediment fingerprinting into soil erosion model testing is lacking in this study we present a novel approach to evaluate spatially distributed soil erosion sediment delivery models that represents the uncertainties in both models and observational data since we understand that the purpose of spatially distributed models is to not only to provide acceptable simulations of outlet transport rates but also to represent sediment dynamics within a catchment we use sediment loads and sediment fingerprinting source apportionments as model evaluation data by use of the glue methodology we apply the sedd model to a 6600 km2 river basin in southeast brazil although glue has been used in other soil erosion modelling applications our testing framework is the first to define limits of acceptability of model error according to the uncertainty in the observational sediment load data a further novelty is the evaluation of behavioral model simulations against sediment fingerprinting source apportionments which have been stratified based on a hierarchical tributary design that facilitates model comparisons along different stages of sediment transport our approach is implemented on free gis software and programming languages being fully reproducible and or adaptable elsewhere the outcomes of this research therefore provide a much needed open source framework for incorporating uncertainty analysis into distributed soil erosion models applications moreover it demonstrates how sediment fingerprinting and potentially other sources of data can be assimilated into model testing within a stochastic structure 2 methods 2 1 catchment description the mortes river drains an area of approximately 6600 km2 in the south of the state of minas gerais brazil fig 1 the river s headwaters are in mantiqueira mountain range and it flows until its confluence with the grande river at the funil hydroelectric power plant reservoir elevation within the basin ranges from 1414 m to 807 m according to köppen s classification the climate in the area is predominantly humid subtropical with dry winters and warm summers i e a cwb climate type alvares et al 2013 average annual rainfall is approximately 1500 mm fick 2017 which is almost entirely concentrated in the spring and summer months hapludoxes 48 and dystrustepts 35 are the main soil classes in the basin table 1 the first are very deep highly weathered leached soils while the latter are much less pedogenetically developed shallow and erosion prone most of the catchment is occupied by pastures 66 often degraded by over grazing and or lack of adequate management remaining forest areas 22 are mostly found on ridges and buffer strips along the stream network croplands which are mostly composed of maize fields for silage production occupy a small portion of the catchment area 5 eucalypt forests 5 are commonly planted for charcoal manufacturing most of the agricultural areas notably in the carandaí mortes pequeno and pirapetinga sub catchments are associated with the occurrence of hapludoxes fig 1 table 1 dystrustepts support extensive pastures for raising dairy cattle and or eucalypt plantations the mortes river basin was chosen for this study due to the availability of continuous sediment concentration and discharge data from the ibituruna gauging station fig 1 although water discharge records are frequently made available by the brazilian water agency sediment concentration data are difficult to obtain moreover field observations and bathymetric surveys have shown that the mortes river delta is the main sedimentation zone in the funil reservoir although the reservoir was built in 2003 the high sedimentation rates in mortes river already impede navigation near its delta 2 2 sediment load data suspended sediment concentration mg l 1 and water discharge m3 s 1 were monitored in the ibituruna gauging station fig 1 from march 2008 to december 2012 batista et al 2017 measurements were taken on an approximately monthly basis resulting in 44 observations in order to estimate long term sediment loads we fitted a sediment rating curve relating suspended solid concentration to water discharge by ordinary least squares both variables were log transformed as the relationship between sediment concentration and discharge in the log scale is approximately linear vigiak and bende michl 2013 the goodness of fit of the linear model was visually assessed with residual and quantile quantile plots these and all other statistical analyses here presented were performed with the r programming language r core team 2019 in order to propagate the error of the fitted model 104 posterior simulations of the model coefficients were generated by an informal bayesian function from the r package arm gelman and hill 2007 this function uses the model residual standard errors to create multivariate normal distributions of model coefficients thus preserving their correlation when estimating posterior simulations next daily sediment concentrations values were calculated based on continuous mean daily discharge records from the brazilian water agency for the ibituruna gauging station 1992 2013 and the simulations of model coefficients these concentration values were used to estimate daily sediment loads ton day 1 which were subsequently aggregated into monthly annual and average annual transport rates in summary the 104 simulations of daily sediment concentrations were used to propagate the rating curve uncertainty when calculating long term sediment loads as pointed out by vigiak and bende michl 2013 this approach only quantifies the regression uncertainty and the actual errors associated to sediment load calculations might be underestimated more detailed descriptions of bayesian and bootstrapping methods for propagating the uncertainty of sediment rating curves can be found in rustomji and wilkinson 2008 and in vigiak and bende michl 2013 2 3 sediment fingerprinting data 2 3 1 sampling design and sample collection in order to facilitate a comparison between sedd model outputs and fingerprinting source apportionments we developed a tributary sampling design in which sub catchments of the mortes river basin were treated as end member sediment sources in addition we adopted a hierarchical approach for sink sediment sampling blake et al 2018 boudreault et al 2019 koiter et al 2013 that is considering the disconnectivity of the sediment cascade on large river basins due to the variability of residence times of sediment storage koiter et al 2013 we understood it was important to sample sink sediments at different nodes of the main river channel fig 1 as a result of our sampling design three nodes with four potential upstream sources each were stratified within the catchment node 1 has four main tributaries the mortes river mrt itself before its confluence with the elvas river elv the carandaí river crd and the santo antônio river sta due to our hierarchical approach node 1 sediments become a potential source of the next downstream node hence node 2 sources are comprised by the mortes pequeno river mpq the peixe river pxe the set of small tributaries in the mid catchment t1 and node 1 similarly node 3 on the catchment outlet receives sediments from the pirapetinga river pir the tabões river tab the set of small tributaries in the lower catchment t2 and node 2 sediment sampling was conducted in two different periods to represent transport dynamics during the well defined seasons of the local climate during september 2017 dry season all nodes and sources were sampled in february 2018 during the rainy season we retrieved extra samples from the sink sediment nodes source samples were taken from lag deposits of tributaries near their confluence with the main river channel the uppermost layer 1 2 cm of freshly deposited sediments from river margins was scrapped with a plastic trowel and approximately 15 scrapes were combined into one individual sample we collected a total of 20 composite samples per each tributary except for sources t1 and t2 these sources are comprised by a set of small tributaries that drain directly to the mortes river fig 1 hence 25 and 17 samples were retrieved in t1 and t2 respectively 4 5 samples from each small tributary sampling sink sediments from nodes 1 and 2 followed the same methods described above during the dry season 20 samples were collected from each of these nodes whereas during the rainy season 12 and 20 samples were retrieved from nodes 1 and 2 respectively given that the mortes river flows into the funil reservoir samples from node 3 were taken from the bottom of the shallow river delta before its confluence with the grande river at the node 3 site 26 and 12 composite samples were collected during the dry and rainy seasons respectively 2 3 2 laboratory analyses sediment samples were oven dried at 60 c and dry sieved with a 0 2 mm mesh subsequently total concentration of the 21 following elements was determined by inductive coupled plasma optical emission spectrometry icp oes al as ba ca cd ce co cr cu fe k la mg mn ni pb se ti v zn zr the 0 2 mm particle size was selected according to the texture analysis of the sink sediments from node 3 at the mortes river delta supplementary material table 1 the analysis indicated that an important fraction of the target sediments were composed of fine sand 0 2 0 05 mm and that fractionating samples to a finer size could lead to unrepresentative results 2 3 3 element selection the first step of tracer selection is to investigate the composition of source and sink sediments here we started with an exploratory analysis by visually examining box plots of element concentrations next a range test was performed to verify if sink element concentrations were well bounded by the source mixing polygon that is if element contents on sink sediments are enriched or depleted in relation to source samples then there is evidence that elements might not be behaving conservatively during sediment transport or there is a missing source laceby et al 2017 smith and blake 2014 moreover a mismatch of element concentrations on source and sink sediments may compromise the numerical solutions of the un mixing models collins et al 2013 the range test therefore aims not only to eliminate elements plotting outside the mixing polygon from further analyses but also to provide an initial insight into the quality of the geochemical data different approaches have been employed for analyzing conservative behavior and for performing range tests smith et al 2018a wilkinson et al 2015 although earlier research might have focused on maximum and minimum tracer values distribution based un mixing models bayesian or frequentitst requires an examination of the distributions of tracer concentrations considering the structure of the bootstrapping approach we employed for solving our un mixing model we adopted a mean and standard deviation range test that is we assumed that means and standard deviations of log transformed tracer concentrations on sink sediments should plot within the means and standard deviations of the source log transformed tracer concentrations this ensures that during the monte carlo simulation sampled sink element contents will always be within the source range the means and standard deviation range test was performed locally for each node and sampling season given the heterogeneity of land uses and geological pedological backgrounds of the sub catchments comprising sediment sources in the mortes river basin i e catchments do not display a definite pattern of source signal development agents table 1 a process based approach to element selection e g batista et al 2019 koiter et al 2013 laceby et al 2015 was not appropriate to this research hence we adopted a more common statistical procedure in which elements passing the range test were submitted to a step wise forward linear discriminant analysis lda niveau 0 1 this approach aims to define a minimum set of tracers that maximize source discrimination and elements selected by the lda were used for modelling again the procedure was repeated for all nodes and sampling seasons 2 3 4 un mixing modelling relative sediment source contributions were calculated by minimizing the sum of squared residuals srr of the un mixing model 1 s s r i 1 n c i s 1 m p s s s i c i 2 where n is the number of elements used for modeling c i is the concentration of element i in the target sediment m is the number of sources p s is the optimized relative contribution of source s and s si is the concentration of element i in source s optimization constraints were set to ensure that source contributions p s were non negative and that their sum equaled 1 in order to quantify the uncertainty in the un mixing model source apportionments we employed the bootstrapping methods described in batista et al 2019 the model was solved by a monte carlo simulation with 2500 iterations for each iteration log transformed element concentrations were sampled from multivariate normal distributions generated from the source and sink geochemical data during the monte carlo simulation values were back transformed by an exponential function log transformation was applied to avoid sampling negative element concentrations and to force a near normal distribution on the typically skewed sediment geochemistry data the optimization function was scripted with the r package rsolnp ghalanos and theussl 2015 whereas the monte carlo simulation here and elsewhere in this study with the package foreach calway et al 2017 2 4 sedd model description the sedd model calculates a spatially distributed sediment delivery ratio sdri that expresses the proportion of eroded sediments that are delivered to the stream network ferro and minacapilli 1995 ferro and porto 2000 the model does not represent channel erosion or deposition processes and sediments reaching the stream network are assumed to reach the catchment outlet following a grid based structure the sdri was calculated as 2 s d r i exp β l i s i where sdr i is the soil delivery ratio of a grid cell i β is a catchment specific empirical parameter m 1 l i is the flow length from cell i to the nearest stream channel m along the flow path and s i is the slope of cell i m m 1 typically the empirical parameter β is calibrated to minimize the errors of sediment load predictions fernandez et al 2003 fu et al 2006 lin et al 2016 whereas the flow length and slope parameters can be derived from dem processing the sdri grid is used to calculate area specific sediment yields ssyi ton ha 1 yr 1 which quantifies the amount of sediments that are delivered from cell i to the stream network 3 s s y i s d r i a i where ssy i is the specific sediment yield for a grid cell i sdr i is the soil delivery ratio for a grid cell i and a i is the annual soil loss computed by revised universal soil loss equation rusle for a grid cell i rusle estimates average annual erosion rates by the following empirical equation renard et al 1997 4 a r k l s c p where a is soil loss per unit area t ha 1 yr 1 r is the rainfall and runoff erosivity factor mj mm ha 1 h 1 yr 1 k is soil erodibility factor t ha h ha 1 mj 1 mm 1 ls is the topographic factor representing slope length and steepness dimensionless c is the cover management factor dimensionless and p is the support practice factor dimensionless as the sedd model neglects channel deposition the total sediment yield at the catchment outlet can be calculated as the sum of ssyi values weighted by cell area equivalently the mean of ssyi values corresponds to the area specific sediment yield in the catchment and the same calculations can be employed at sub catchment scale with this approach sub catchment relative contributions can be estimated based on sedd results of note we chose sedd for three main reasons i the model requires calibration which makes it particularly suitable for glue ii the model has few parameters which facilitates computer processing when making a large number of simulations with large spatial datasets and iii the model is rusle based which gives us the opportunity to scrutinize the uncertainty associated to the most widely used soil erosion model in world 2 5 glue the glue methodology can be summarized in five decision steps beven 2009 which include i the definition of a likelihood measure to evaluate model realizations ii definition of a rejection criteria for non behavioral model realizations iii definition of uncertain model parameters iv definition of distributions to characterize parameter uncertainty and v definition of a simulation method for creating model realizations we did not establish a formal likelihood measure to evaluate model realizations as the rejection criteria for non behavioral simulations was set according to an actual range of system responses that is all model realizations which produced sediment load responses within the 95 prediction interval of the sediment load rating curve were considered behavioral since the sedd temporal scale is inherited from the rusle the model simulates long term average annual sediment yields therefore for comparison purposes the sediment rating curve estimates were aggregated into a 22 years average model realizations were generated by a monte carlo simulation with 1000 iterations fig 2 sedd parameter β was sampled from a log uniform distribution with minimum and maximum parameters retrieved from typical values reported in the literature min 0 000001 m 1 max 0 1 m 1 e g porto and walling 2015 taguas et al 2011 we used a log uniform distribution to ensure that the extreme values of this broad range were sampled during the simulation the threshold for stream definition which affects drainage density and therefore distance to streams l i was sampled from a uniform distribution min 50 000 m2 max 5 000 000 m2 to represent the uncertainty in the dem derived model variables we created a pseudo random error surface for each model iteration mean and standard deviation of dem errors were retrieved from the nasa srtm report rodriguez et al 2006 μ 1 7 m σ 4 1 m and used to create a normally distributed error field which was added to the original dem all terrain attributes used in the models were then calculated within the monte carlo simulation all herein described spatial analyses were supported by saga gis conrad et al 2015 and the r package rsaga brenning and bangs 2015 the r scripts used for model implementation and uncertainty analysis are provided as supplementary material along with raw fingerprinting and discharge data additional r packages used in the simulations include raster hijmans and van etten 2012 trucnorm mersmann et al 2018 doparallel ooi et al 2019 rgdal bivand et al 2019 wgcna langfelder and horvath 2019 since we understood that rusle factors were not parameters requiring calibration or conditioning but instead uncertain model variables we performed a forward uncertainty analysis similarly to biesemans et al 2000 and van rompaey and govers 2002 although this can be seen as a separate analysis rusle error was propagated into sedd simulations as we explain in the following the forward error analysis was performed with a monte carlo simulation with 1000 iterations in order to represent the uncertainty in the rusle r factor we first calculated a deterministic rainfall erosivity map supplementary material fig 1 this was carried out with average monthly and annual rainfall grids from worldclim fick 2017 and the regression equation developed by aquino et al 2014 this regression equation estimates annual or average annual in this case ei30 index values and it was originally fitted using detailed rainfall data from the municipality of lavras for each iteration of the monte carlo simulation we added a normally distributed error surface to the deterministic rainfall erosivity map with mean equal zero and a standard deviation equal to 10 of mean deterministic r factor for the catchment for the k factor we created truncated normal distributions for each soil class occurring in the catchment soil map feam 2010 the discrete soil map was rasterized and for each simulation a grid cell erodibility value was sampled according to its corresponding soil class distribution parameters were set according to published k factor values for brazilian soils see silva et al 2019 although in general there were not enough different estimations of k factor values for individual soil classes to create data based probability distributions we used the available published data and our own interpretation to infer distribution parameters table 2 uncertainty in the ls factor was represented following the dem error propagation described above slope rad and catchment area m2 grids were created for each model iteration these grids were subsequently used to calculate the ls factor with the equation of desmet and govers 1996 a maximum threshold of 10 800 m2 was enforced to the catchment area grid which corresponds to maximum flow length of 360 m for a 30 m resolution dem this was performed to avoid spuriously high ls factor values in flow concentration areas as usually carried out in rusle applications panagos et al 2015 schmidt et al 2019 the maximum threshold was empirically defined based on remote sensing and field observations from the study area in this case lower flow accumulation thresholds would lead to hillslopes being identified as flow concentration zones e g gullies ravines hollows when rill and interril should still be more representative of the erosion processes similarly to the k factor errors in the c factor estimation were propagated by creating truncated normal distributions for individual land use classes table 3 the land use grid was produced using 30 m resolution landsat 8 surface reflectance images from 2013 and the methods described in batista et al 2017 since no widespread support management practices are found in the catchment agricultural areas no specific procedure was applied to represent p factor uncertainty however the c factor distribution parameters for cropland and eucalypt were set to reflect occasional contour cropping and or crop residue management a summary of published c factor values for typical brazilian land uses and crop management can be found in silva et al 2019 the resulting rusle model realizations were used as input for the sedd model simulations moreover we performed a sensitivity analysis by fixing each model factor and sampling the remaining variables in new monte carlos simulations each with 1000 iterations this enabled us to evaluate the proportion of model variance explained by each factor it should be highlighted that forward error propagation is essentially subjective given its total dependence on the assumptions made by the modeler about potential sources of uncertainty beven 2009 our approach presents a rather conservative estimate of model uncertainty basically representing the errors involved in parameter estimation this is because we could not describe all the sources of error in the model structure moreover we wanted to constrain model realizations based on choices of factor values that modelers are expected to make hence we did not want to give the models full freedom if all parameters and variables are allowed to vary beyond a range of physical meaning models are capable of reproducing almost any answer usually for the wrong reasons see batista et al 2019a 2 5 1 spatial representation of model uncertainty in order to represent the spatial uncertainty of the final sedd model predictions we first filtered the behavioral model simulations according to the criterion previously described next we calculated the 2 5 50 and 97 5 quantiles for each grid cell ssy i estimates absolute error grids were then calculated by subtracting the 97 5 grid by the 2 5 grid relative errors were determined as 5 r e i a e i m i 100 where ae i is the absolute error for a grid cell i and m i is the simulation median for grid cell i the filtered behavioral model realizations were also used to calculate total sediment yields from the sub catchments described in table 1 these calculations were used to estimate the relative contribution of the sub catchments to the aggregated sediment yields at each sink sampling location i e nodes 1 2 and 3 relative contributions were calculated by dividing individual sub catchment sediment yields by the sum of all loads from contributing sub catchments the sedd estimated relative contributions were then evaluated against fingerprinting source apportionments the same approach was employed for creating rusle error maps except in this case all model simulations behavioral or not were considered when calculating grid cell quantiles 3 results 3 1 discharge curve the error propagation method used to represent the uncertainty in the sediment rating curve resulted in a broad estimate of average annual specific sediment yields with a 95 prediction interval of 0 47 11 95 ton ha 1 yr 1 mean 3 45 ton ha 1 yr 1 median 2 52 ton ha 1 yr 1 fig 3 a as expected annual estimates of sediment loads were more uncertain for the years with greater discharge and sediment transport fig 3 c monthly calculations revealed that over 85 of the annual sediment load is transported from november to march the monthly relative contributions to annual sediment yield showed less uncertainty than annual and average annual estimates fig 3 b 3 2 sediment fingerprinting 3 2 1 element selection our exploratory analysis demonstrated that cu and zn displayed spurious concentration patterns as a large proportion of sample measurements 30 were below detection limit these elements were therefore omitted from further scrutiny of the remaining 19 elements 16 84 17 87 and nine 47 plotted within the source mixing polygons for nodes 1 2 and 3 respectively for the dry season table 4 for the rainy season 18 95 elements passed the range test for nodes 1 and 2 whereas only seven 37 elements were within source range for node 3 sediments for node 1 the forward step wise lda selected 12 elements for the dry season sediments whereas 11 elements were selected for the rainy season table 4 the lda for both seasons showed a reclassification accuracy of 100 for node 2 the discriminant analysis selected 10 elements for the dry season and 11 for the rainy season again all samples were correctly reclassified during the lda cross validation as fewer elements passed the range test for node 3 only six elements were selected by the lda for both seasons reclassification accuracy was lower in this case with 91 and 82 for the dry and rainy seasons respectively the largest errors associated to the lda reclassification for node 3 source samples can be visualized in the bi plots displayed in fig 4 3 2 2 un mixing model results un mixing model solutions for node 1 were highly uncertain for both seasons as demonstrated by the broad density curves displayed in fig 5 according to model estimates sources crd and elv seem to dominate sediment contributions in relation to mrt and sta at least considering the median and interquartile iqr values of the simulated source apportionments table 5 results for node 2 were less uncertain and revealed a greater contrast between seasonal sediment transport dynamics during the dry season the un mixing model indicated that a significant portion of sediments reaching node 2 are derived from pxe median 50 iqr 32 64 however such contributions decrease during the rainy season for which the models suggest a large apportion of sediments from node 1 median 60 iqr 44 74 modeled source contributions from mpq and t1 were relatively low for both seasons table 5 model solutions for node 3 displayed a similar pattern to node 2 regarding the seasonal variation of source contributions during the dry season a greater proportion of sediments were estimated to derive from the sources proximally located to the catchment outlet particularly tab median 33 iqr 15 50 however rainy season source apportionments indicate that most of the sediments reaching the funil reservoir are originated on the upstream areas of the catchment which are represented by node 2 median 61 iqr 34 80 this illustrates how even in the relative short time period represented by our study sediments from the upper and mid catchment area are transported throughout the river network given that most of the mortes river sediment load is transported during the rainy season it is plausible to assume that upstream sediments are important contributors to reservoir sedimentation 3 3 rusle uncertainty the results of the forward error analysis revealed that rusle estimates were highly uncertain in spite of the moderately conservative assumptions made regarding sources of model error the median of grid cell absolute errors was of 29 ton ha 1 yr 1 which translated to a median relative error of 588 as expected the highest absolute errors in the rusle estimates were associated to the areas with higher erosion rate predictions fig 6 b c contrarily relative errors were higher on the areas with lower soil loss estimates this is possibly a result of small variations on sampled parameter values leading to a large relative fluctuation on the low erosion predictions fig 6 a considering the median of the simulations as a point based estimate of erosion rates the influence of soil erodibility on soil loss predictions was evident in fig 6c upper and mid catchment areas where dystrustepts are widespread had overall higher erosion rates according to the model simulations moreover modeled erosion hot spots are visibly associated to areas with high flow accumulation and more intensive land uses e g cropland eucalypt the sensitivity analysis demonstrated that the c factor was the largest source of uncertainty in the model predictions the proportion of model variance explained by the c factor had a median value of 45 iqr 30 56 the ls median 21 iqr 17 27 and k factors median 15 iqr 10 20 also contributed significantly to the propagated model errors the r factor had a small influence on overall model uncertainty median 3 iqr 2 5 3 4 sedd results from the 1000 sedd model realizations generated by the monte carlo simulation 234 were behavioral that is 234 model realizations provided estimates of outlet based ssy within the 95 prediction interval of the sediment rating curve 0 47 11 95 ton ha 1 yr 1 most of the non behavioral model response surface was associated to an overestimation of the curve calculated sediment yields fig 7 a by analyzing the dotty plots of sampled parameter values it was clear that the empirical parameter β had a preponderant influence on the model results fig 7 c behavioral model realizations are concentrated within a relatively narrow range of β values whereas acceptable system representations are spread throughout the sampled values of stream definition thresholds the fluctuation of mean catchment sdr i values in the catchment led to a linear increment of estimated ssy fig 7 b indicating little influence of rusle simulation results in the outlet aggregated sedd model predictions behavioral model realizations had mean sdr i values between 5 and 50 which highlights the uncertainty in the model predictions considering the median of the behavioral model realizations grid cell ssy i estimates had a median value of 0 06 ton ha 1 yr 1 whereas the median of analogous absolute error values was 6 64 ton ha 1 yr 1 although outlet lumped model results seem to be little influenced by the uncertainty in the rusle or in the stream definition threshold the errors derived from such input variables parameters are explicit when the uncertainty of spatially distributed ssy estimates are presented in fig 8 a areas with large absolute errors in the ssy map clearly match the rusle errors displayed in fig 7 b moreover the influence of stream definition threshold uncertainty is visible in the surroundings of lower order streams 3 4 1 evaluation of sedd results against fingerprinting source apportionments distributions of relative source contributions estimated by the sedd model overall displayed a similar pattern to the rainy season fingerprinting source apportionments except for node 1 fig 9 opposite to the fingerprinting results sedd simulations indicated that mrt was the main source of sediments iqr 52 9 53 4 reaching the main river channel node 2 results revealed an agreement between rainy season fingerprinting and sedd estimated relative source contributions as all sedd model realizations were bound by the iqr of the fingerprinting apportionments however sedd simulations calculate an even larger contribution of node 1 sediments iqr 72 6 73 0 similarly fingerprinting results for the rainy season for node 3 showed a similar pattern to the sedd simulations both models indicate that node 2 sediments are the largest contributors to outlet sediment loads although sedd results again suggest a greater contribution of upstream sediments node 2 iqr 84 3 85 7 moreover sedd estimated tab contributions iqr 3 2 3 4 were considerably lower than the ones estimated by the sediment fingerprinting un mixing models 4 discussion 4 1 uncertainty in the forcing data the model testing framework presented here demonstrated how uncertainty permeates all facets of soil erosion models and the things we call observational data the error propagation method used to represent the uncertainty in the sediment rating curve resulted in such broad estimates of average annual sediment loads that many different sedd realizations were able to encompass the forcing data similar results have been reported by other soil erosion modelers banis et al 2004 janes et al 2018 and a logical conclusion is that we need better data in order to reject non behavioral model realizations importantly the uncertainty in the average estimates of annual sediment loads highlight how sediment concentration measurements at the ibituruna gauging station need to be intensified particularly during high flow events by comparing discharge values recorded during the sediment concentration measurements and those observed between 1992 and 2013 it becomes clear that high flow events may not have been adequately represented by the current sampling regime supplementary material figs 2 and 3 extrapolations of the rating curve for extreme events may therefore result in additional uncertainty in the sediment load estimates considering the importance of the mortes river for hydroelectric power generation as well as the high sedimentation rates observed at the funil reservoir we strongly recommend establishing a thorough water and sediment monitoring program in the catchment nevertheless even if more accurate and precise sediment load data were available our approach has demonstrated how very different spatial model representations can produce similar outlet responses despite the fact that we only considered behavioral simulations while calculating the uncertainty of grid cell ssy i estimates absolute model errors were almost hundred fold the median of the predictions this brings to question if the numerical spatial predictions are at all useful furthermore it demonstrates how misleading it can be to neglect model and observational data uncertainty in soil erosion and sediment delivery models 4 2 uncertainty in the sedd model the fact that the sedd results were mostly driven by the empirical and somewhat abstract parameter β raises some concerns about the quality of process representation in the model the common deterministic parameter optimization method for calibrating β should be therefore disputed if sedd model simulations are to provide meaningful system representations alternative methods for deriving β values should be encouraged e g ferro and stefano 2003 porto and walling 2015 nonetheless given the sensitivity of the model to parameter β representing the uncertainty associated to the parameter estimation is paramount in spite of the large errors associated to grid cell ssyi estimates aggregated sub catchment relative contributions calculated from the sedd simulations were precise as shown by the narrow uncertainty bands in fig 9 as the sum of the grid based model realizations should somewhat converge sub catchment sediment loads are expected to display smaller variances than the grid cell rates this demonstrates that at the very least spatially aggregated results were consistent as the model repeatedly identified the same sub catchments as the main sediment sources the accuracy of these estimates is difficult to assess although some insight can be gained by an evaluation against fingerprinting source apportionments 4 3 uncertainty in sediment fingerprinting source apportionments the bootstrapping method for solving the un mixing models resulted in uncertain sediment fingerprinting estimates of relative source contributions particularly for node 1 bootstrapping methods are known to produce somewhat spurious uncertainty bands for un mixing model results as local optimization functions frequently yield numerical solutions where one source provides 0 or 100 of the contributions cooper et al 2014 this is illustrated by the bi modal density curves in fig 4 nonetheless the uncertainty of node 1 un mixing model solutions might imply an issue with the data moreover the negligible contributions from mrt by far the largest sub catchment in the basin during the rainy season questions the consistency of the model results as a narrative it might be the case that there was an issue of particle size incommensurability between mrt and node 1 sink sediments mrt element concentrations were overall higher than in the remaining node 1 sources and sink samples which might indicate mrt sediments were composed by smaller sized particles an alternative hypothesis is that sediment storage in the mrt sub catchment is influenced by small hydroelectric plants or channel regulation in the mortes river before its confluence with the elvas river regardless fingerprinting and sedd model outputs showed a contrasting pattern for node 1 and we have no supporting evidence to corroborate either of the system representations on the contrary the overall correspondence of fingerprinting un mixing model solutions and sedd simulations of relative source contributions for nodes 2 and 3 while considering the uncertainty in both system representations provides some conditional corroboration of the methods although the sedd model simulates long term sediment transport dynamics and the fingerprinting approach was limited by the temporal scale of our sampling both modelling exercises designated that most of the sediments reaching nodes 2 and 3 are originated from farther upstream sources that is at least under the reasonable assumption that rainy season fingerprinting results represent the bulk of the sediment transport dynamics in the catchment for management purposes the convergence of model results is an important outcome of this research different models and sources of data have indicated that the sediments reaching the funil reservoir by the mortes river come from the mid and upper catchment areas even during a relatively short temporal scale i e the rainy season covered by the sediment sampling hence reducing reservoir sedimentation rates requires widespread soil conservation efforts throughout the entire catchment instead of local proximal interventions 4 4 uncertainty in the rusle another valuable outcome of this research was demonstrating how uncertain common large scale distributed rusle applications are although rusle is the most widely used soil erosion model in the world alewell et al 2019 studies which have attempted to quantify model error are scarce e g tetzlaff et al 2013 our results indicate that numerical rusle predictions of spatially distributed erosion rates lack utility given the uncertainty in the model outputs see fig 5 of course these results are case specific and entirely determined by the assumptions made about potential sources of model error which we understand were cautious that is the uncertainty in the rainfall erosivity regression equation was not properly assessed let alone in the equations relating rainfall intensity to kinetic energy wilken et al 2018 additionally errors in the soil and land use map classifications were not entirely represented nor were the potential errors in the plot based experiments used to generate rusle factors nearing 2000 parsons 2019 hence similar or larger errors should be expected in comparable spatially distributed rusle applications elsewhere unless otherwise demonstrated since we were explicit about our assumptions regarding sources of model error and fully reported the distribution parameters used in the uncertainty analysis readers can interpret the outputs accordingly we expect this should attenuate part of the subjectivity necessarily involved in forward uncertainty assessments moreover as model results are reported as distributions different levels of confidence can be attributed to the simulations according to the potential consequences one might expect from model mispredictions quinton 1997 overall results from our forward error analysis indicate that rusle modeled spatially distributed erosion rates should be viewed with extreme caution particularly when actual numerical model outputs are used to project the influence of climate and land use changes on future erosion rates due to the difficulties involved in large scale model parameterization the costs of plot based experiments for developing empirical model factors and the multiplicative structure of the rusle and usle family models we suspect that model applications will remain largely uncertain this might be particularly true for developing countries such as brazil where data scarcity further complicates model parameterization under such conditions model testing should hereon focus on evaluating if the models are at least consistently capable of relatively ranking erosion prone areas as in fischer et al 2018 importantly the high uncertainty associated with the rusle predictions contradicts the argument that usle type models are less error prone due to their low input requirements this is corroborated by schürz et al 2019 who demonstrated how model parameterization choices could lead to a variation in the order of two magnitudes in usle estimated soil losses these results indicate that large scale rusle simulations are likely to be no less uncertain than more complex process based models as pointed out by favis mortlock et al 2001 there is little justification in adopting the usle or its derivatives as standards by which to measure the quality of all other models this makes the evaluation biased towards the standard and inhibits the development of competing theories hence we suggest that the modelling community should explore alternative options for simulating soil erosion and sediment connectivity as different models might be more or less adequate according to the purpose scale and conditions of the modelling application recent developments with mmf eekhout et al 2018 peñuela et al 2018 smith et al 2018 tan et al 2018 geowepp poeppl et al 2019 and openlisem baartman et al 2020 starkloff et al 2018 demonstrate how soil erosion models can still be useful tools for understanding processes nevertheless we recommend that any further model evaluation which is a critical step for developing knowledge and confidence or doubt in model predictions should be firmly established upon an uncertainty based framework 4 5 limitations and future research an important limitation of the comparison described here between sedd model outputs and fingerprinting results stems from the temporal divergence of the analyses while the sedd model operates on a long term average annual time step our fingerprinting data provided only a snapshot of the sediment dynamics in the catchment moreover the sedd model does not represent channel erosion and deposition processes assuming a long term quasi equilibrium condition between hillslope sediment yield and the fluvial system although our fingerprinting modelling results indicate a strong connectivity between upstream tributaries and the mortes river outlet they also demonstrate that transient storage might be an important regulator of the sediment budget in the catchment as expected hence in order to improve the modelling of the catchment loads it should be necessary to combine hillslope soil erosion and sediment delivery simulations with models that explicitly account for river sediment dynamics e g czuba and foufoula georgiou 2014 schmitt et al 2018 additional modelling and measurement improvements should also investigate the contribution of gullies to catchment sediment loads as gully erosion was observed in field inspections during the sediment sampling campaigns furthermore future research should focus on elucidating temporal trends in sediment dynamics by use of different fallout radionuclides such as 7be and 210pbex evrard et al 2010 gellis et al 2019 le gall et al 2017 this should provide more fit for purpose evidence to evaluate soil erosion and sediment delivery models at different time scales 5 conclusions soil erosion models and the measurements of system responses we call observational data are necessarily uncertain the representation of such uncertainty is indispensable here we provided a framework for incorporating the uncertainty of sediment rating curves sediment fingerprinting un mixing models and soil erosion sediment delivery models into the glue methodology more specifically the framework was applied to the rusle based sedd model at a large catchment in southeast brazil our results have shown how large scale spatially distributed rusle applications are highly uncertain this means model applications of such type cannot afford to disregard uncertainty analysis and that modeled erosion rates should be interpreted with upmost caution sedd simulations of catchment sediment yields were also highly uncertain mostly due to the errors in the rating curve forcing data and the sensitivity of the model to the empirical parameter β spatially distributed simulations of area specific sediment yields were even more uncertain which meant the grid based numerical model outputs were of little utility however when the sedd model outputs were lumped into sub catchment relative contributions results were at least consistent the comparison between sedd model outputs the fingerprinting source apportionments presented here was facilitated by the hierarchical tributary sampling design moreover the uncertainty based framework enabled us to compare distributions of model realizations of relative source contributions the comparison revealed an overall similarity of fingerprinting and sedd modeled distributions of source apportionments although large discrepancies were found in part of the catchment ultimately we found that under the testing conditions the sedd model might be useful for identifying the sub catchments that contribute to most of the sediment load in the mortes river basin conversely the uncertainty in the simulations questions the model s usefulness for calculating actual erosion and sediment delivery rates from a falsifacationist perspective the model could not be rejected as multiple model realizations produced acceptable system representations however this was largely facilitated by the uncertainty in the forcing data one of the most important conclusions from this research is that we need better data in order to reject models or model realizations and therefore to improve our understanding of soil erosion and sediment transport in large river catchments this will require multiple sources of data and honest representations of the uncertainty in models and observations of system responses declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was funded in part by the coordination of improvement of higher level education personnel capes process number 88881 190317 2018 01 the national counsel of technological and scientific development cnpq process numbers 306511 2017 7 and 202938 2018 2 and the minas gerais state research foundation fapemig process numbers cag apq 01053 15 and apq 00802 18 comments from anonymous reviewers helped to improve the quality of this paper they are highly appreciated we are also thankful to claudia mignani for helping with our figures and to tom de lima for his assitance during field work appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 supplementary material fig 1 rainfall erosivity for the mortes river catchment supplementary material fig 1 supplementary material fig 2 violin a and box plots b of the water discharge values at the ibituruna gauging station during the suspended sediment concentration measurements ssc and the period to which the sediment rating curve was applied 1992 2013 supplementary material fig 2 supplementary material fig 3 monthly rainfall a and discharge b for the ibituruna gauging station 1992 2013 shaded area represents the 95 confidence interval of the data rainfall data for 2017 and 2018 when fingerprint sediments were sampled are also plotted in fig 3a no rainfall data was recorded for september 2017 supplementary material fig 3 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 104961 
25877,sensitivity analysis sa is en route to becoming an integral part of mathematical modeling the tremendous potential benefits of sa are however yet to be fully realized both for advancing mechanistic and data driven modeling of human and natural systems and in support of decision making in this perspective paper a multidisciplinary group of researchers and practitioners revisit the current status of sa and outline research challenges in regard to both theoretical frameworks and their applications to solve real world problems six areas are discussed that warrant further attention including 1 structuring and standardizing sa as a discipline 2 realizing the untapped potential of sa for systems modeling 3 addressing the computational burden of sa 4 progressing sa in the context of machine learning 5 clarifying the relationship and role of sa to uncertainty quantification and 6 evolving the use of sa in support of decision making an outlook for the future of sa is provided that underlines how sa must underpin a wide variety of activities to better serve science and society keywords sensitivity analysis mathematical modeling machine learning uncertainty quantification decision making model validation and verification model robustness policy support 1 introduction 1 1 the whats and whys of sensitivity analysis sensitivity analysis sa in the most general sense is the study of how the outputs of a system are related to and are influenced by its inputs in many applications the system in question involves a single or a set of mathematical models encoded using computer software that simulates the functioning of a real world system of interest such mathematical models can be data driven also called statistical directly mapping inputs to outputs engelbrecht et al 1995 rodriguez et al 2010 or mechanistic also called process based solving a set of differential or other mathematical equations governing the possibly spatio temporal behaviors of the underlying processes maxwell and miller 2005 haghnegahdar et al 2017 inputs of interest commonly referred to as factors in sa may include model parameters forcing variables boundary and initial conditions choices of model structural configurations assumptions and constraints outputs may include any functions of model responses including those that may vary over a spatio temporal domain objective functions such as a production or cost function in cost benefit analysis or an error function in model calibration why is sa useful in short it addresses several fundamental overarching purposes of systems analysis and modeling a scientific discovery to explore causalities and how different processes hypotheses parameters scales and their combinations and interactions affect a system e g gupta and razavi 2018 b dimensionality reduction to identify uninfluential factors in a system that may be redundant and fixed or removed in subsequent analyses e g sobol et al 2007 c data worth assessment to identify processes parameters and scales that dominantly control a system for which new data acquisition reduces targeted uncertainty the most e g guillaume et al 2019 partington et al 2020 and d decision support to quantify the sensitivity of an expected outcome to different decision options constraints assumptions and or uncertainties e g tarantola et al 2002 sa is now considered a requirement for good modeling practice as indicated by some existing guidelines european commission 2015 saltelli et al 2020 in general and regardless of any specific purpose sa aims to exploit the sparsity of factors principle a heuristic stating that very often only a small subset of factors in a system have a significant influence on a specific system output box and meyer 1986 1 2 why this position paper sa is a relatively new area of research it has roots in design of experiments doe which is a broad family of statistical methods conceived in the early 20th century for designing efficient experiments to acquire representative information about the existence of effects of one or multiple variables on another variable in a system fisher 1953 doe primarily worked in the context of costly noise prone lab or field environments the field of sa started to materialize in the 1970s and 80s with the beginning of the widespread availability of computers for mathematical modeling e g cukier et al 1973 and the extension of doe to the design and analysis of computer experiments dace which are typically noise free or deterministic in the sense that replicating a computer experiment with the same inputs results in identical model responses sacks et al 1989 more broadly the notion of sensitivity has historically but informally been a building block in various types of study particularly in decision making where what if scenarios or policy effectiveness are assessed by changing one or multiple factors at a time tarantola et al 2000 what is the status quo in sa we believe that sa is en route to becoming a mature and independent but interdisciplinary and enabling field of science tremendous advances in both theory and application of sa have been accomplished as documented in the recent reviews by norton 2015 iooss and lemaître 2015 wei et al 2015 razavi and gupta 2015 borgonovo and plischke 2016 pianosi et al 2016 borgonovo et al 2017 borgonovo 2017 ghanem et al 2017 gupta and razavi 2017 and saltelli et al 2019 particularly in recent years research and practice in sa have gained significant momentum with many researchers from a variety of backgrounds contributing to a variety of theoretical frameworks based on the type of applications in their respective disciplines despite these advances realization of the benefits and true potential of sa across the sciences has been hampered by several challenges amongst others the most pressing challenge is that sa is still a paradigm defined largely by method rather than purpose various methods have been developed rooted in different philosophies towards sa razavi and gupta 2015 but often the purpose has been defined on the basis of how a given method works and its capabilities as well as its authors disciplinary research focus narrow views lack of communication among scientists across disciplines and ignoring uncertainties in models can conceal the benefits of sa to researchers and practitioners leading to an underuse of sa in many branches of modeling and consequently in support of model based policy making saltelli et al 2019 in this perspective paper we synthesize key aspects of the state of the art and by taking a forward looking approach outline some grand challenges facing the new frontiers and opportunities for sa we draw from the multidisciplinary views and expertise of the authorship team which includes natural scientists engineers decision scientists computer scientists systems analysts and mathematicians and provide opinions on the possible and desirable future evolutions of sa science and practice our overarching goal is to contribute to the establishment of sa as a distinct and essential interdisciplinary enabling science we believe that sa science must formally become an integral part of systems analysis in general and mathematical modeling in particular to unleash the capabilities of sa for addressing emerging problems in engineering technology the natural and social sciences and human natural systems the following sections are structured such that one could directly read a section of interest without attention to the other sections 2 an overview of the state of the art to many sensitivity analysis sa simply means a process in which one or multiple factors in a problem are changed to evaluate their effects on some outcome or quantity of interest such a process has a long history of application perhaps in all areas of science examples include assessment of the effectiveness of a decision option in a policy making problem the impact of a problem constraint on the optimality of a cost or benefit function via shadow prices or the role and function of a model parameter in generating a model output such analyses are generally referred to as local sensitivity analysis lsa because the sensitivity of the problem is assessed only around a nominal point in the problem space lsa is simple intuitive and appropriate in very specific circumstances it has however been commonly used much more broadly in circumstances where it has been criticized for providing only a localized view of the problem space especially when used in the context of investigating parameter importance in mathematical modeling saltelli and annoni 2010 saltelli et al 2019 the modern era of sa has focused on a notion that is commonly referred to as global sensitivity analysis gsa saltelli et al 2000 as it attempts to provide a global representation of how the different factors work and interact across the full problem space to influence some function of the system output see fig 1 in this section we briefly summarize four categories of gsa derivative based distribution based variogram based and regression based we also provide overviews of sa when enabled with response surface methods progress in sa with correlated factors and software tools available for sa 2 1 derivative based approach derivative based methods are a natural and intuitive extension of lsa where measures of local sensitivity are computed at many base points across factor space and somehow averaged to provide a global assessment of sensitivity such measures are said to be derivative based as they either analytically compute derivatives or numerically quantify the change in output when factors of interest continuous or discrete are perturbed around a point perturbations typically occur one at a time and by some specific perturbation size campolongo et al 2011 the different derivative based methods differ in the ways that they choose the base points the perturbation size and the distributional properties of the sampled derivatives e g first or second moment to provide an average global measure of sensitivity see e g morris 1991 campolongo et al 2007 sobol and kucherenko 2009 campolongo et al 2011 lamboni et al 2013 rakovec et al 2014 the outcome of these methods is sensitive to these choices among which the sensitivity to perturbation size is generally overlooked even though it can be profound shin et al 2013 haghnegahdar and razavi 2017 2 2 distribution based approach distribution based methods adopt a different philosophy that bases the analysis on the distributional properties of the output itself and attempts to quantify how the different inputs contribute to forming those properties the most common distribution based method is based on the analysis of output variance decomposing that variance into portions attributed to individuals or groups of inputs sobol 1993 owen 1994 homma and saltelli 1996 such a variance based sa was first conceived in the context of non linear dependence as far back as 1905 pearson 1905 and later in terms of a fourier analysis in the 70s cukier et al 1978 the full variance based sa framework was laid down by ilya sobol in 1993 sobol 1993 then linked to the derivative based sa via poincaré inequalities by sobol and kucherenko 2009 see also roustant et al 2017 some distribution based methods go beyond variance and investigate how higher order moments of the output depend on the inputs for example the method of dell oca et al 2017 particularly focuses on skewness and kurtosis some other distribution based methods are however moment independent in that they measure the difference between the unconditional distribution of the output and its conditional counterparts when one or more inputs are fixed for example the method of borgonovo 2007 measures this difference via the borgonovo index while the methods of krzykacz hausmann 2001 and pianosi and wagener 2015 pawn use the mutual information and kolmogorov smirnov test respectively another example is the commonly called regional sensitivity analysis rsa which rather than fixing inputs defines conditional distributions based on thresholds for the model response spear et al 1994 hamby 1994 2 3 variogram based approach more recently a third category has emerged based on the theory of variograms that bridges derivative and distribution based methods razavi and gupta 2016a 2016b sheikholeslami and razavi 2020 the variogram based approach recognizes that model outputs are not always randomly distributed and they possess as do their partial derivatives a spatially ordered covariance structure in the input space anisotropic variograms can characterize this structure by quantifying the variance of change in the output as a function of perturbation size in individual inputs variogram based sensitivity measures can be considered more comprehensive than other approaches in the sense that they integrate global sensitivity information across a range of perturbation scales derivative based and variance based sensitivity measures are also produced as a side product of calculating variogram effects the efficiency and applicability of the variogram based approach are demonstrated in razavi et al 2019 becker 2020 and puy et al 2020a 2 4 regression based approach regression based sa has a long history traditionally referring to methods that infer sensitivity information from coefficients of a typically linear regression model fitted to a sample of model response surface points kleijnen 1995 those early methods from a gsa point of view have been criticized for their heavy reliance on the prior assumption regarding model response form e g linear or polynomial equation and the fact that if the quality of fit is poor the sensitivity estimates are not reliable razavi and gupta 2015 from an lsa point of view however they have proven useful for dimensionality reduction via orthogonal decompositions from parameter samples kambhatla and leen 1997 or locally approximated sensitivity matrices tonkin and doherty 2005 also such methods when using quadratic regression allow characterization of parameter interactions in the inverse problem e g shin et al 2015 more recently regression based sa has witnessed a new generation of methods arising from the machine learning community the goal of these methods typically is to provide the commonly called variable importance measures following two general approaches in one they assess the importance of each or a sub set of inputs in constructing a response surface via for example multivariate adaptive regression splines mars friedman 1991 if the inclusion of an input or set of inputs significantly improves the quality of fit they are deemed important gan et al 2014 the other approach conversely first fits a response surface model using all inputs and then assesses how the quality of fit degrades when the sample points for each input or set of inputs are permuted breiman 2001 lakshmanan et al 2015 while these approaches are typically restricted to importance in fitting data they do have the advantage of also extending well to classification models for example using random forests e g hapfelmeier et al 2014 allowing for sensitivity measures that apply to both discrete and continuous variables 2 5 response surface assisted sa in the early 2000s applied mathematicians formally working on design and analysis of computer experiments dace started building linkages with sa santner et al 2003 fang et al 2005 their emphasis has particularly been placed on linking sa and asymptotic statistical theory janon et al 2014a gamboa et al 2016 space filling designs of experiments tissot and prieur 2015 gilquin et al 2020 structural reliability fort et al 2016 and bayesian estimation pronzato 2019 moreover response surface surrogates rooted in dace such as polynomial chaos and gaussian process regression have found applications to approximate sensitivity measures in the case of computationally intensive models oakley and o hagan 2004 janon et al 2014a 2014b wang et al 2020 a review of these linkages can be found in ghanem et al 2017 2 6 sa with correlated inputs one persistent issue in sa is that nearly all applications regardless of the method used rest on the assumption that inputs are uncorrelated sobol 1993 hoeffding 1948 inputs however can be correlated and their joint distribution can take a variety of forms in real world problems correlation in this context refers to statistical dependency between any subset of inputs independent of the system under investigation the correlation effect is different from the interaction effect which refers to the presence of non additivity of the effects of individual inputs on the system output razavi and gupta 2015 it is now being increasingly recognized that ignoring correlation effects and multivariate distributional properties of inputs largely biases or even falsifies any sa results do and razavi 2020 recently several methods have been developed to account for such properties including extensions of the hoeffding sobol decomposition chastaing et al 2012 regression based methods xu and gertner 2008 copula based methods kucherenko 2012 do and razavi 2020 sheikholeslami et al 2020 and game theory concepts owen 2014 owen and prieur 2017 iooss and prieur 2019 2 7 software tools and applications to promote and advance the use of sa there has been tremendous albeit fragmented progress in building computer packages these operationalize the various sa methods via different programming languages and include dakota adams et al 2020 in c sobolgsa kucherenko and zaccheus 2020 in c matlab and python uqlab marelli and sudret 2014 in matlab openturns baudin et al 2017 in python and c the sensitivity package iooss et al 2018 in r salib herman and usher 2017 in python psuade tong 2015 in c vars tool razavi et al 2019 in matlab and c safe pianosi et al 2015 pianosi et al 2020 in matlab r and python mads jl in julia vesselinov et al 2019 and sensobol puy 2020 as discussed in douglas smith et al 2020 software programs for sa adopt different design philosophies which reflect different disciplinary foci and vary in terms of usability including extent of documentation and assumption of users prior knowledge in parallel the generation of test beds for different methods and software packages is receiving increasing attention razavi et al 2019 becker 2020 applications of sa are widespread across many fields including earth system modeling wagener and pianosi 2019 engineering guo et al 2016 biomechanics becker et al 2011 water quality modeling koo et al 2020a and 2020b hydrology shin et al 2013 haghnegahdar and razavi 2017 water security puy et al 2020c nuclear safety saltelli and tarantola 2002 iooss and marrel 2019 and epidemiology burgess et al 2017 vanderweele and ding 2017 to name a few the most quoted handbook for sa is a primer for global sensitivity analysis saltelli et al 2008 with over 5000 citations from across the scientific disciplines a cross disciplinary review of sa applications can be found in saltelli et al 2019 the wide and growing use of sa suggests that a cohesive treatment of sa as a discipline in its own right inclusive of a well honed syllabus for teaching will have a large beneficial impact across the sciences in general 3 challenges and new frontiers given the significant progress and popularity of sensitivity analysis sa in recent years it is timely to revisit the fundamentals of this relatively young research area identify its grand challenges and research gaps and probe into the ways forward to this end the multidisciplinary authorship team has identified six major themes of challenges and outlook as outlined in fig 2 in the following we discuss our perspective on the past present and future of sa under each theme in a dedicated section the overarching objective here is to identify possible future avenues that will take sa to the next level one that is especially beneficial to meeting the challenges of modeling complex societal and environmental problems e g elsawah et al 2020a 3 1 towards a structured generalized and standardized sa discipline while sa is now considered by some to be standard practice in modeling norton 2015 pianosi et al 2016 razavi and gupta 2015 it is not a formally recognized discipline nor a coherent subfield of applied mathematics or applied statistics for example it is spread across the mathematics subject classification ams 2020 sociologically disciplinary fields are communication systems enabling discourse and the dissemination of knowledge between practitioners stichweh 2001 additionally formally recognized disciplines are organizationally distinct with communities structured around the production of knowledge and training of practitioners casetti 1999 stichweh 2001 sa fulfils all of these criteria with the exception of an organizational community that make it distinct from related fields of study for instance there are no academic institutions nor is there any scientific journal focused on sa the only official link binding part of the sa family is an international conference called sensitivity analysis of model output samo held once every three years since 1995 the 9th instalment of which was held in 2019 in barcelona spain with the forthcoming 10th instalment to take place in tallahassee florida usa in 2022 consequently the current family of sa researchers and practitioners is spread over many disciplines and there are dramatic differences in the sa capacity and maturity in different contexts despite the lack of a specialized journal for sa there is a relatively large and growing number of publications on the subject within the water resources research area alone the number of sa publications has grown significantly over the past decades based on a search in the clarivate analytics web of science platform the yearly publication count is now equal to about one third of publications in the related but well established field of optimization razavi and gupta 2015 the fact that optimization is an extensive discipline with several dedicated journals suggests that sa should rightfully seek to become an independent discipline albeit one that interacts effectively with other disciplines in the meantime treatment of sa as a subject is diverse and couched in disciplinary specific terms and traditions and taught to varying standards saltelli 2018 accordingly the emphasis and importance of sa across the fields in which it is applied are equally as diverse this state of affairs is not unlike the beginnings of other scientific fields such as computer science which separated from mathematics and engineering sometime in the 1940s tedre 2007 denning et al 1989 and hydrology which did not find its footing as a separate discipline until the 1950s or arguably the early 90s mccurley and jawitz 2017 klemeš 1986 the diverse treatment of sa research is partly responsible for the existing sluggishness to accept sa as a discipline 3 1 1 recognize sa as a discipline recognition of sa as a discipline in its own right requires universal acknowledgement that methodological developments and guidance for application of sa are frequently transferable across application contexts a major challenge to overcome is that of inconsistencies in terminology methodology and fundamental definitions across the contexts and disciplines in which sa is applied saltelli et al 2019 arguably there are examples in the literature where sa practice has been perfunctory or inappropriate possibly misinforming the users and even a resulting policy saltelli and annoni 2015 saltelli et al 2019 furthermore although the links between sa and the well established field of uncertainty quantification uq are clear to sa researchers see section 3 5 most sa applications seen in the academic literature do not adequately map the uncertainty in model inference saltelli and annoni 2010 ferretti et al 2016 saltelli et al 2019 the exceptions are typically publications written by sa researchers who pursue sa methodological developments this has the unfortunate result that sa related publications fall largely into two classes proposals for new or refined methods with illustrative applications written by sa researchers or application papers with often poor quality sa written by non sa researchers the scientific journals where these findings are published are largely disconnected for the two classes in an ideal world modeling teams should include at least one researcher versed in sa given the wide applicability of sa its practitioners are dispersed across the sciences and their work similarly disseminated there is a need to establish a publication outlet focused specifically on sa to signify the separate concerns and foci of research establishment of an sa specific journal would strengthen communication of the current state of sa research particularly for uninitiated modelers another challenge to proper uptake of sa is that its application in some fields might appear under other titles for example a recent article in nature adam 2020 refers to modeling activities in which thousands of versions of the model are run with a range of assumptions inputs to provide a spread of scenarios with different probabilities as ensemble modeling such activities however are typically considered as uq and possibly sa in the environmental modeling community sa type activities are also seen under the title single model perturbed physics ensembles in the climate modeling community bellprat et al 2012 confusingly the expression climate simulation ensemble is more often used to indicate the case where different models for example developed by different teams are applied to the same problem donev et al 2016 ipcc 2016 see a critical discussion in saltelli stark et al 2015 the risk of these diverging nomenclatures is that research advances made in uq and sa may go unnoticed by some communities 3 1 2 address possible inconsistencies in sa different sa methods are based on fundamentally different philosophies and therefore can result in different sometimes conflicting results for the same problem tang et al 2007 razavi and gupta 2015 this inherent nature of the state of the art is unlike many other fields for example while the field of optimization contains a vast variety of approaches methods and applications all these may boil down to a common well defined philosophy that is to find an optimal solution to a formulated problem given certain objective functions and constraints maier et al 2014 maier et al 2019 in other words an optimal solution to a given problem formulation would remain optimal regardless of the optimization method used whereas according to current theory the sensitivity assessment for a problem might appear to be quite different depending on the sa method used for improved consistency two key questions need to be answered before choosing an sa method and carrying out the analysis 1 why do i need to run sa for a given problem and what is the underlying question that sa is expected to answer and 2 how should i design the sa experiment to address that underlying question thought out answers are critical as otherwise most users tend to use methods developed in their own camp and are therefore most comfortable with rather than methods that are most suitable for the purpose and problem at hand thus focusing on the purpose would facilitate a shift in method selection principles from legacy to adequacy addor and melsen 2019 we emphasize that we do not necessarily advocate the reconciliation of different philosophies and methods but are pointing out that because sa addresses multiple related problems it must be made clear why a particular method or set of methods is the right match for a given research question in addition to the purpose and chosen method s sa researchers and practitioners generally need to be more mindful of the subjective but often overlooked decisions they make in the configuration of a method the sensitivity of sa to such decisions for example sa algorithm parameters may be quite significant for some methods and ignoring it might result in questionable results the significance of this issue has been discussed recently by haghnegahdar and razavi 2017 in the context of derivative based methods e g morris 1991 and by puy et al 2020b in the context of the pawn method pianosi and wagener 2015 3 1 3 teach sa more broadly and consistently formalizing a structured generalized and standardized sa discipline is attainable in a foreseeable future central towards this goal is to invest in systematic and coherent teaching of sa to students across disciplines who will become the next generation of researchers and practitioners sa is currently taught on an ad hoc basis mostly via small workshops tailored to specific aspects of sa or as a part of courses related to systems analysis or uncertainty quantification perhaps the most formal and systematic effort to teach sa has been a summer school on sa run by the european commission s joint research centre held ten times between 1999 and 2018 sa needs to become an independent but integral part of the curriculum across the relevant disciplines alongside other topics such as optimization and validation verification and uncertainty quantification vvuq see e g national research council 2012 new interdisciplinary graduate courses need to be developed to comprehensively cover sa and to teach and promote best sa practice as discussed in the following sections the sa discipline has extensive untapped potential for a variety of problems and applications 3 2 untapped potential of sa for mathematical modeling historically the majority of formal sa applications have been directed towards mathematical models to better understand how they work and diagnose their deficiencies and to contribute to their calibration and verification saltelli et al 2000 in this context a dominant application of sa is for parameter screening to support model calibration by identifying and fixing non influential parameters there is potential however for sa to further address several challenges in mathematical modeling through advancements in the management of uncertainty assessment of model quality through testing and diagnostics and tackling non identifiability and model reduction for example mathematical modeling could benefit from structure and standards based on statistical principles saltelli 2019 including a systemic appraisal of model uncertainties and sensitivities in the following we outline the potential of sa to advance mathematical modeling 3 2 1 management of uncertainty management of uncertainty through its characterization and attribution should be at the heart of the scientific method and a fortiori in the use of science for policy funtowicz and ravetz 1990 the problem of uncertainty management is core to the modeling craft and should be an integral part of any model development and evaluation exercise jakeman et al 2006 eker et al 2018 while sa has significant potential its application often does not adequately map the uncertainty in model inference in a recent five point manifesto for responsible modeling global sa is invoked as essential to the task of mapping the uncertainty in every model assumption saltelli et al 2020 a major step forward in mathematical modeling will be to better evaluate uncertainty in model predictions and to trace that uncertainty back to its sources across the model components parameters and inputs sa is uniquely positioned to do so as it can help to decompose the prediction uncertainty and attribute it to the individual factors and their interactions basically sa can help answer a critical question razavi et al 2019 when and how does uncertainty matter a major but almost totally neglected issue in mathematical modeling is that while models are becoming more and more complex they are treated more and more like a black box even by model developers themselves in real world applications those models tend to be used without much attention to their complicated internal mechanics and not always justified assumptions a manifestation of this issue is the fact that many modern physically based models include countless numbers of hard coded parameters e g see mendoza et al 2015 supported by the rationale explicit or implicit that scientists can characterize those parameters with absolute certainty such practice can render progressive initiatives on open science vicente saez and martínez fuentes 2018 and open modeling e g openmod 2020 less effective sa is much needed to prize open and cast light into these black boxes and to illuminate the dominant sources of uncertainty furthermore the quality of both statistical and mechanistic models struggles with common issues when dealing with uncertainty in statistics the p test can be misused to overestimate the probability of having found a true effect colquhoun 2014 stark and saltelli 2018 likewise certainty may be overestimated in modeling studies nearing and gupta 2018 thus producing unreasonably precise estimates even in situations of pervasive uncertainty or ignorance saltelli et al 2015 thompson and smith 2019 including in important cases where science needs to inform policy pilkey and pilkey jarvis 2007 it is an old refrain in mathematical modeling that since models are often over parameterized with respect to the information that can be extracted from the available data it can sometimes appear that they can be made to conclude anything we choose hornberger and spear 1981 a nalogous to under and over fitting issues in statistical models mechanistic model development suffers from a trade off between model completeness and propagation error saltelli 2019 see fig 3 the former refers to the adequacy of a model in terms of for instance how many aspects of the underlying system are included in the model the latter also known as the uncertainty cascade effect christie et al 2011 refers to the notion that adding each new aspect to the model for example a new parameter which itself is uncertain potentially increases the overall uncertainty in the output such tradeoffs challenge model developers to calibrate the right level of complexity in the construction of models sa can facilitate this process by characterizing and attributing the contributions to overall model error so as to identify the sweet spot where uncertainty attains a minimum as a simple example consider the case where the uncertainty of concern is with respect to a sought measure of predictive model error around an observational quantity of interest the sweet spot would be where no significant improvement in model performance occurs by adding more parameters within a given model structure where significance corresponds to a level commensurate with the errors noise in the observations of interest e g see jakeman and hornberger 1993 3 2 2 diagnostic testing and model verification sa has significant potential to help in diagnosing the behavior of a mathematical model and for assessing how plausibly the model mimics the system under study for the given application this capability provides understanding of how a model works and points to the parts of a model that are deficient a key to this end is to properly frame the sa problem and articulate that understanding the sensitivity of what to what matters for this purpose as outlined in gupta and razavi 2018 the former what may be chosen from any of the following categories a one or more model performance metrics that quantify the goodness of fit of the model responses to observed data e g mean squared errors b a specific targeted aspect of those responses e g extremes or percentiles such as peak flows in a hydrologic model c a compressed set of properties that characterize those responses e g hydrologic signatures such as runoff ratio or d the entire spatio temporally varying trajectory of responses themselves the latter what may include continuous or discrete variables describing model parameters forcings and structural assumptions to diagnostically test a model one may compare sa results with expert knowledge on how the underlying system being modeled works example studies based on time varying and time aggregate sa results include wagener et al 2003 herman et al 2013 haghnegahdar et al 2017 and razavi and gupta 2019 moreover the recent emergence of given data sa methods will provide unprecedented opportunities for model diagnostic testing as they can directly be applied to observed data as well in addition to the mathematical models plischke et al 2013 sheikholeslami and razavi 2020 the knowledge gained via sa can be documented in a model s user guide to help practitioners configure and parameterize the model more effectively diagnostic evaluation of models in this manner is analogous to property based testing claessen and hughes 2000 wherein the logical properties of model behavior are evaluated according to expected behavior failure of a model to conform to expected behavior falsifies the assumption that the model is correctly implemented in addition a study using mathematical models may face a diversity of errors and subjectivities these may stem from process conceptualization and mathematical representation parameterization inputs and boundary conditions discretization choices in space and time numerical solvers and software coding up to and including the framing and biases of the modelers themselves oreskes 2000 iwanaga et al 2021 modelers however do not subscribe to a unified reliable and agreed on code of good practices for testing their models and the quality of the inference that they produce more work is needed to develop testing strategies based on sa that cover the diversity of subjective factors involved in the process of model development e g peeters 2017 3 2 3 non identifiability and model reduction most models are poorly identifiable largely because of over parameterization relative to the data and information available see guillaume et al 2019 for an overview of identifiability the assessment of model appropriateness for a purpose requires understanding of its identifiability the sources of any non identifiability and the influence of any non identifiability on the model guillaume et al 2019 sa and identifiability analysis ia are different but complementary primarily because sa is about the properties of a model itself while ia is more about model properties with respect to observed data it can be shown that an insensitive parameter is non identifiable but the converse is not necessarily true that is a sensitive parameter may or may not be identifiable therefore sa can help in part to recognize the non identifiable components of a model knowledge of identifiability can be used to simplify a model structure by fixing or combining parameters that on their own are ineffective in influencing model outputs model reduction however should be done with caution as a parameter that seems non influential under a particular condition might become quite influential under a new condition e g see tonkin and doherty 2009 for example a snowmelt parameter in a hydrologic model has no influence in time periods without snow whereas it becomes dominantly influential in snowmelt seasons in such cases fixing the parameter will limit the agility and therefore the fidelity of the model in mimicking the underlying system also fixing parameters that have small sensitivity indices may result in model variations that cannot be explained in the lower dimensional space hart and gremau 2019 3 2 4 the reproducibility crisis and sa the challenges of modeling need to be seen in the broader context of the so called reproducibility crisis saltelli and funtowicz 2017 saltelli 2018 where misuse or abuse of statistics stark and saltelli 2018 gigerenzer and marewski 2015 leek et al 2017 singh chawla 2017 wasserstein and lazar 2016 gigerenzer 2018 is often cited as the root cause of the crisis ioannidis 2005 current non reproducible science is ecologically fit to the existing science governance arrangements smaldino and mcelreath 2016 including its publish or perish culture banobi et al 2011 and is resistent to reform chalmers and glasziou 2009 edwards and roy 2017 in the field of clinical medical research for instance the percentage of non reproducible studies may be as high as 85 gigerenzer and marewski 2015 the field of mathematical and computational modeling has started grappling with the reproducibility crisis as well hutton et al 2016 saltelli 2019 saltelli bammer et al 2020 development of research specific software is at the core of modern modeling efforts most modelers however are not formally taught software development practices hannay et al 2009 such that models are rarely designed and developed in a manner that supports further use or reuse beyond its original research specific context the consequent lack of accessible code and data then feeds into issues of reproducibility hutton et al 2016 hut et al 2017 as alluded to above the publish or perish culture limits the recognition researchers receive for developing and maintaining long lived software associated data and supporting documentation that underpins reproducibility in some cases code and data may not be accessible at all even after contacting authors stodden et al 2018 one observation is that researchers want to perform research not write software crouch et al 2013 sletholt et al 2012 that said there is increasing recognition of the importance and benefits of supporting open and accessible research software support of initiatives towards improving computational reproducibility has been growing e g ahalt et al 2014 crouch et al 2013 culminating recently with the fair findable accessible interoperable reusable principles for open and accessible data management wilkinson et al 2016 some journals now award open code badges kidwell et al 2016 to highlight publications with accessible code and data sa and its practitioners can contribute to addressing the aspects of this crisis which directly affect mathematical modeling reproducible model based studies need the kind of transparency that sa can offer by way of making explicit the conditionality of model based inference as well as the conditionality of the associated uncertainties essential to this end is to standardize and promote best sa practice along with the development of sa related software that can easily be coupled with any model there has been an increasing number of open software packages which democratize both common and experimental sa techniques and applications a non exhaustive list is provided in section 2 3 3 computational aspects and robustness of sa algorithms computational burden has been a major hindrance to the application of modern sa methods to real world problems many of the existing sa methods have remained under utilized in various disciplines as they require a large sample size particularly for models with higher dimensional spaces state of the art spatially distributed models are typically computationally demanding themselves and take minutes hours or even days for a single run although the growth of computing resources is making the application of current algorithms to existing problems more affordable see e g prieur et al 2019 not everyone has access to powerful computing and there will always be modeling problems where computing power will not be quite enough for existing algorithms for example most i e 70 sa applications in earth and environmental systems modeling have been limited to low dimensional models i e with 20 or fewer factors involved sheikholeslami et al 2020 whereas there are abundant applications with models that can have up to hundreds e g 900 in borgonovo and smith 2011 or even thousands e g 40 000 in lu et al 2020 of parameters in the machine learning context the number of model parameters can reach millions e g bert houlsby et al 2019 even trillions of parameters e g zero rajbhandari et al 2020 the application of sa with machine learning is further complicated because of the fundamental differences between machine learning and other types of models see section 3 4 computational obstacles need to be properly assessed and addressed so that sa can be applied to any model particularly those whose results are of immediate significance to society inadequate or non existent application of sa leads to models for which society cannot characterize their confidence modeling is a social activity and the acceptance of model prescriptions regarding for example addressing an industrial risk financial crisis hurricane or a pandemic calls for mutual trust between model developers and end users saltelli et al 2020 sa can help with building this trust by providing insights into the internal functioning of models the future therefore needs new generations of algorithms to keep pace with the ever increasing complexity and dimensionality of the state of the art models building on known theoretical and empirical convergence rates for many sampling based approaches further theory could identify fundamental limits on existing classes of algorithms to help identify breakthroughs required 3 3 1 essential definitions and components a complete assessment of the computational performance of any sa algorithm must be conducted across four aspects efficiency convergence reliability and robustness efficiency refers to the amount of time number of computations required to perform sa and is often assessed by the number of model runs i e sample size required for the algorithm to converge to some specified level convergence of an sa algorithm is non trivial to assess as the answer typically cannot be preordained from theory it depends on several factors including the model type and its complexity the overall objective of the sa e g prioritization where sample size can generally be smaller versus screening the sa method itself definition of convergence and level of certainty required choice of time period for the input forcing variables and the width of parameter ranges and distribution sampled see shin et al 2013 reliability refers to any measure of correctness of sa results and its accurate assessment requires the availability of the true sa results reliability of an algorithm may only be assessed when the model is simple e g simple algebraic test functions or the true results are somehow given robustness often used in lieu of reliability measures how consistent an algorithm performance remains when the sample points and algorithm parameters change for example an sa algorithm is robust to sampling variability if its performance remains almost identical when applied on two different sample sets taken from the same model in cases where running multiple replicates of the same experiment is not possible bootstrapping efron 1987 is often used with sa algorithms to estimate robustness in the form of uncertainty distributions on sensitivity indices without requiring additional model evaluations addressing computational challenges requires a proper understanding of the design functioning and interaction of the three general components of any sa algorithm these components are 1 the experimental design that employs a sampling strategy to select sample points in the factor space of interest 2 the function evaluation procedure to run the model collect and store sampled model responses i e obtained by running a model many times on a spatio temporal and or other domains and 3 the integration mechanism to numerically integrate the sampled data to estimate sensitivity indices the choice of experimental design is often dictated by the integration mechanism of the sa algorithm of interest for example in the method of morris 1991 the mechanism that integrates the elementary effects across the factor space requires sample points to be taken equally spaced from each other by a given distance by changing one factor at a time as a result a sample taken for one sa algorithm may not be useable by another algorithm or possibly for other purposes the function evaluation procedure is typically the most computationally intensive component of sa this is not only because of the computational burden of running the models themselves but also the overhead for storing retrieving and manipulating the model responses on high resolution domains e g spatio temporal the following sections outline possible progress in addressing computational challenges with respect to the above three components 3 3 2 experimental design and integration improving the efficiency of sa is tied to improving experimental designs in conjunction with integration mechanisms for example consider that global sensitivity measures are commonly written as an average over the distribution of the input of interest xi of an inner statistic borgonovo et al 2016 the brute force application of the definition of global importance measures would lead to an estimation cost of c n n k where n is the number of runs from the distribution of input xi n is the number of runs needed for the inner statistic and k is the number of model inputs if n 1000 n 1000 and k 10 we are already at 10 000 000 model runs this cost can be reduced to c n k 2 by using the sampling and integration mechanism of saltelli 2002 to estimate first and total order variance based sensitivity indices a recently developed approach to sampling and integration is to extract information contained in all pairs of sample points rather than the individual points this is useful because the number of pairs 2 combinations grows quadratically n2 2 with the sample size n razavi and gupta 2016a for example if n 1000 we get 499 500 pairs but doubling the sample size to n 2000 results in a fourfold increase to 1 999 000 pairs razavi and gupta 2016b becker 2020 and puy et al 2020a have shown the efficiency of this approach in estimating variance based total order effects through the method variogram analysis response surfaces vars alternatively the future of sa may step more towards sampling free algorithms that can work on any given data see e g plischke 2010 plischke et al 2013 pianosi and wagener 2018 sheikholeslami and razavi 2020 such approaches may be referred to as given data sensitivity analysis or alternatively green sensitivity analysis in that they can recycle available samples for example from previous model runs allowing for samples to be incrementally obtained and thereby avoiding the squandering of computational budget the computational cost of the corresponding estimators is then n model evaluations in addition to being green some of these approaches tend to be computationally much more efficient than other methods and in certain cases produce robust sa estimates with a very small sample size sheikholeslami and razavi 2020 notably most algorithms for given data sa involve a parameter tuning step that may be non trivial with a bias variance compromise perspective which is to avoid both over fitting and over smoothing examples of such parameters include the bandwidth for kernel regression based estimators the number of leaves for random forest based estimators the truncation argument for spectral procedures etc bootstrapping efron 1987 may be used for the selection of such parameters see e g heredia et al 2020 but it may consume an excessive amount of computation time adaptive selection of these parameters or developing parameter independent algorithms is a challenging issue that needs to be addressed in future more recently authors have proposed parameter estimation procedures based on nearest neighbors broto 2020 rank statistics gamboa et al 2020 and robustness based optimization sheikholeslami and razavi 2020 these methods are still relatively new and need to be tested across a range of problems with different dimensionalities see e g puy et al 2020a above all convergence considerations need to be at the heart of the development and application of any sa algorithms recently developed convergence criteria e g sarrazin et al 2016 sheikholeslami et al 2020 and stopping criteria see e g gilquin et al 2020 rugama et al 2018 can be useful in this regard in general the literature is replete with studies that indicate convergence for a particular model or function and some particular instances of the above other factors but these offer only limited guidance consequently many users usually choose the computational budget i e the number of model runs for sa on an ad hoc basis rather than on convergence reliability and robustness considerations rather than relying on guidance from past studies analysts should be encouraged to adopt methods and software packages that explicitly address this issue bootstrapped estimates of sensitivity indices are for example now common and enabled by default in r and python packages sa users should assess convergence rates as the sample size increases based on intermediate results but a typical hindrance is that many sampling strategies involve one stage sampling that generates the entire set of sample points at once requiring the user to specify the sample size a priori sheikholeslami and razavi 2017 this is a disadvantage as it is unlikely for users to know the optimal sample size that enables the algorithm to converge to robust results therefore there is a need for sequential or multi stage sampling strategies such as sobol sequences sobol 1967 sobol et al 2011 and progressive latin hypercube sampling plhs sheikholeslami and razavi 2017 that enlarge the sample size during the course of sa while preserving the distributional properties of interest the superior performance of sobol low discrepancy sequences over random sampling has been demonstrated in several studies gilquin et al 2020 gilquin et al 2017 sheikholeslami et al 2017 rugama et al 2018 kucherenko et al 2011 kucherenko et al 2015 furthermore as the value of sa for high resolution e g spatio temporal model outputs is increasingly recognized innovative strategies to handle the increased storage and retrieval overhead are needed currently all model runs are typically stored first requiring excessive storage capacity for large models and sensitivity indices are computed post hoc future developments similar to jakeman et al 2020 and terraz et al 2017 can helpfully include sa algorithms that merge function evaluation and integration mechanisms such that sensitivity indices are updated as new results are made available 3 3 3 function evaluations much attention has been geared towards the function evaluation procedure under the umbrella of surrogate modeling surrogate models also called response surface models metamodels or emulators are used in lieu of computationally intensive models and can be statistical i e response surface surrogates or mechanistic i e lower fidelity mechanistic surrogates razavi et al 2012 sa methods based on response surface surrogates build approximations such as polynomial chaos expansions xiu and karniadakis 2002 q rs hdmr zuniga et al 2013 and gaussian process kriging rasmussen and williams 2005 using a limited number of expensive model evaluations once built sensitivity measures can be estimated by sampling the surrogate instead of the original model at negligible cost or in some cases can be estimated analytically sudret 2008 marrel et al 2009 due to the sheer computational expense of some models building an accurate surrogate using only data from the most trusted numerical model can be challenging to reduce the computational burden of building surrogates multi fidelity methods combine limited high fidelity data with larger amounts of lower fidelity data coming from models with reduced physics or coarser numerical discretization sa methods using multi fidelity approximations can produce sensitivity estimates that converge to high fidelity estimates but do so at a fraction of the cost palar et al 2018 moreover these methods can build upon the aforementioned advances made for single fidelity models and adaptively allocate samples to resolve uncertainties and sensitivities jakeman et al 2020 the use of surrogate modeling introduces a new challenge accounting for the uncertainty arising from the surrogate model itself e g model error combined with the errors of the estimation procedure while progress has been made in the assessment of surrogate modeling uncertainty see e g jones 2001 oakley and o hagan 2004 sóbester et al 2005 razavi et al 2012 janon et al 2014a and 2014b and some bayesian approaches are already capable of incorporating this uncertainty into posterior distributions of sensitivity measures oakley and o hagan 2004 gramacy and taddy 2010 further advancements to properly incorporate such uncertainties in sa estimates and respective confidence intervals are likely surrogate modeling strategies particularly those based on response surfaces become less effective in high dimensional problems razavi et al 2012 to address limitations related to high dimensionality adaptive and goal oriented buzzard 2012 jakeman et al 2020 approaches can be used these approaches can allocate samples to lower dimensional subspaces in a manner that addresses the curse of dimensionality and results in enormous computational gains lastly an issue hindering the application of sa to large complex models is that some models may fail to run properly crash at particular points in the factor space and not produce a response simulation failures mainly occur due to non robust numerical implementations the violation of numerical stability conditions or errors in programming sa algorithms are typically ill equipped to deal with such failures as they require running models under many configurations of factors in addition to improving properties of the original models e g kavetski and clark 2010 more research is needed to equip sa algorithms to handle model failures which is becoming a more pressing issue as the complexity of mathematical models grows one of the very first studies addressing this issue in the context of sa is sheikholeslami et al 2019 where a surrogate modeling strategy is used to fill in model output values when the original model fails to handle this issue strategies can also be adopted from other types of analyses for example bachoc et al 2016 used a design of experiments strategy to detect computation failures and code instabilities and bachoc et al 2020b developed a method to classify model parameters to computation failure or success groups during optimization 3 4 sa and machine learning machine learning ml has achieved unprecedented performance in complex tasks typically performed by humans such as image classification krizhevsky et al 2017 natural language processing yang et al 2018 and gaming silver et al 2018 this success combined with the growth in computational power and the increasing availability of big data has motivated the application of ml to a wide range of problems across many disciplines including the earth sciences reichstein et al 2019 robotics torresen 2018 medicine hosny et al 2018 and finance lee et al 2019 research and development with ml are now viewed as a major avenue forward by many industrial sectors such as energy security cyber security transport defence aeronautics and aerospace avsi 2020 deep learning dl has emerged in recent years as a leading ml approach for a wide variety of regression and classification applications goodfellow et al 2016 dl is a newly formalized term that refers to the way artificial neural networks anns with more than one hidden layer learn representations from data with a rich and long history dating back to the 1980s e g rumelhart et al 1986 hornik et al 1989 anns have become perhaps the most popular tool for ml therefore major portions of this section are primarily focused on anns a critical challenge facing ml particularly dl applications is their typical lack of interpretability and explainability these two terms usually used interchangeably in the literature refer to the ability of a model developer to make sense of why the model functions the way it does and to explain that to a user rudin 2019 samek and müller 2019 roscher et al 2020 in many real world applications the acceptance and use of an ml model s outputs require an explanation of why and how the model works in addition transparency and auditing of ml models can raise legal issues nowadays rudin 2019 especially when personal data are involved what complicates this further is that ml is reliant on processes that infer correlation rather than causation obermeyer and emanuel 2016 sa can offer new opportunities for the development and application of ml these opportunities are rooted in the fact that sa and ml in many cases look at the same problem via two different approaches in fact a goal of ml in most application areas especially in environmental modeling is to construct a function that maps variables in an input space to those in an output space hastie et al 2009 razavi and tolson 2011 generally such functions are purely data driven not accounting for any underlying processes physical or otherwise similarly sa looks at the relationship between the inputs and outputs but instead of constructing a mapping function it estimates the relational strength between each single or group of inputs and the outputs via different sensitivity indices such informal commonalities between tools for sa and ml provide significant potential for each field to benefit from the other in exploring the potential however one must be mindful of the central differences that exist between computer experiments that provide data for sa and more general experiments including those in laboratories or in fields which provide data for ml these differences are as follows 1 computer experiments are usually deterministic with the exception of stochastic models such as agent based simulators whereas real world data commonly used in ml are usually polluted with observational errors often with unknown properties 2 the linkage between the input and output variables in computer experiments is generally via hypothesized causal relationships while this is not necessarily the case in other types of experiments 3 in ml applications users typically need to have access to very large data sets but this is typically not possible in the computer modeling context where physical data acquisition such as for model verification may be very expensive 4 in computer experiments users have full control over the experimental design and the way a sample is taken whereas this is usually not the case in other types of experiments the following sub sections explain how the fields of sa and ml already have and can continue to cross fertilize 3 4 1 feature and structure selection in ml sa can support feature ranking and selection where the term feature is equivalent to the term factor in common sa literature the objective is to find the dependency strength between features and targeted labels to enable the user to choose the features that best explain and possibly predict the output of interest sa can be used prior to ml model design and training to choose only the features that are most statistically associated with the output data galelli et al 2014 the classic non sa approaches to do so include the standard statistical correlation metrics e g pearson correlation coefficient spearman s rank correlation coefficient and kendall s tau information theoretic metrics e g entropy mutual information and dissimilarity measures and advanced dependence measures such as distance covariance and hilbert schmidt independence criterion da veiga 2015 these classic approaches can be complemented by the advanced sa techniques that work directly on sample data in the absence of any model for example the recently developed given data sa paradigm plischke et al 2013 sheikholeslami and razavi 2020 can be used on data available for ml to rank features according to their relational strength with the output of interest another approach could be to use the target and conditional sa raguet and marrel 2018 that enhances feature selection when the underlying phenomenon is under represented in the dataset e g unbalanced datasets associated with the prediction of rare or extreme output events special care however needs to be taken in sa on training data because sample data on features are by and large real world data or properties thereof typically having undefined distributions and unclear correlation structures and spatial dependencies sa can also be applied to an ml model after training to identify the controlling features and how they interact to generate the model output a simple way to do so is local sa examples include the use of one factor at a time sa e g lek et al 1995 1996 maier and dandy 1997 liong et al 2000 and the calculation of partial derivatives of the model outputs in response to changes in the model inputs e g dimopoulos et al 1995 1999 tison et al 2007 vasilakos et al 2009 mount et al 2013 such assessments can also be expanded following the concepts of global sa for example importance score methods for feature ranking in ml based on permutation and resampling have informal roots in sa and strong connections with sobol sensitivity theory examples of such methods with anns and random forests include breiman 2001 lakshmanan et al 2015 gregorutti 2015 wei et al 2015 and benoumechiara 2019 more formally sa has been used to rank features in anns and random forests according to their importance in explaining the variation in outputs fock 2014 fernández navarro 2017 zhang 2019 thus sa can point to the most influential features learned by an ml model its most active parts and detect interactions between features lundberg and lee 2017 lundberg et al 2020 ribeiro et al 2016 štrumbelj and kononenko 2014 in this way sa can potentially enable the identification of optimal levels of structural complexity of ml models which is particularly useful in designing deep learning constructions for example efast and random balance design have been used to prune redundant neurons in anns lauret et al 2006 li and chen 2018 3 4 2 interpretability and explainability of ml despite being very successful ml has been criticized for being a black box where the reasons for an answer are unknown this challenge may offset the value of ml in a range of applications particularly where researchers and decision makers seek transparency sa can help in peering inside the ml to improve its explainability and interpretability e g lundberg and lee 2017 the goal here is to produce explanations that are intelligible and meaningful to end users which aid in improving transparency and building trust help in identifying the best ml model among several comparably performing models and enable diagnosis of model errors samek et al 2019 a significant portion of efforts in the literature to provide explanations has been based on the assessment of feature importance for developed ml models as described in section 3 4 1 thus sa offers new opportunities to provide insights into the general behavior of a model by highlighting how the different features influence the model output such insights are particularly important for the structural validation of ml models humphrey et al 2017 if those models are not structurally valid their behavioural response to different input stimuli can be erratic and counter to physical system understanding making them difficult to apply in practice with confidence see wu et al 2014 for a discussion validann humphrey et al 2017 is an example software package using sa for this purpose in the assessment of feature importance sa has to deal with two challenges often encountered in ml as is the case in some other types of modeling the often high dimensionality of the feature space and multicollinearity dependencies between those features these challenges are discussed in sections 3 5 3 and 3 5 4 in addition to assessing the sensitivities to the features sa in principle has potential to be applied to any part of ml models including their structure e g the number of layers and neurons in anns and parameters e g weights and biases in anns such practice however can be hampered by fundamental differences between standard sa applications to mechanistic models and those to ml models these differences as outlined below necessitate further research to develop sa strategies particularly tailored for ml first unlike mechanistic models the structure of many ml models particularly anns is based on the notion of connectionism meaning their internal operations are massively parallelized for example in a mechanistic hydrologic model the soil parameterization equation may be solely responsible for representing how soil columns store and release water while other parts of the model may be in charge of other physical processes in the case of an ann based hydrologic model however one may not be able to single out what neuron or group of neurons is responsible for representing the same soil processes in fact if one re trains that ann model with a different parameter initialization a wholly separate group of neurons might end up being responsible for the soil processes second the statistical properties of the parameters of ml models e g weights are usually not process informed mount et al 2016 in the case of sa of mechanistic models the inputs considered are typically model parameters sampled by an experimental design with known statistical properties defined by the user however in the ml context this is not easily doable and for example it is non trivial to assign a range to the weights of an ann for a given problem see kingston et al 2005a razavi and tolson 2011 in general the value of sa for providing insight into and extracting knowledge from ml models can be improved significantly by using state of the art model development practices maier et al 2010 wu et al 2014 that improve parameter identifiability guillaume et al 2019 such as input variable selection see galelli et al 2014 and model structure selection kingston et al 2008 and by accounting for physical plausibility kingston et al 2005b and parameter uncertainty kingston et al 2005a 2006 explicitly during the model calibration process sa can also support interpretability and explainability of ml in the context of classification in this context a major problem is with examining and explaining the robustness of decision boundaries for classification with respect to data and or model hypotheses sa can provide insights into the robustness with respect to the specification of input distributions for example bachoc et al 2020a applied a sensitivity index developed by lemaître et al 2015 for robustness analysis of decision boundaries in classification moreover sa is useful to provide explanations in the context of classification robustness of classification for example is subject to the decision boundaries that can be identified through ml the decision boundaries may for example be sensitive to the distribution of inputs and or model hypotheses specific sa methods have been successfully applied to explain the influential factors on decision boundaries and robustness analysis e g lemaître et al 2015 sueur et al 2017 bachoc et al 2020a gauchy et al 2020 sa can also identify influential inputs regarding the occurrence of critical events which are important in the robustness assessment of decision boundaries raguet and marrel 2018 spagnol et al 2019 marrel and chabridon 2020 molnar 2019 3 4 3 ml powered sa progress in ml undoubtedly provides fertile ground for new ideas in sa most notably the ml capability to provide efficient data driven function approximation has provided tremendous opportunities for surrogate modeling in the context of sa when computer experiments are intensive example methods arising from ml that have been used in sa include gaussian processes rasmussen 2004 yang et al 2018 generalized polynomial chaos expansions sudret 2008 reduced basis methods hesthaven et al 2016 and anns beh et al 2017 see section 3 3 3 for more on this subject moreover dependence measures and kernel based indices used in ml gretton et al 2005 have been introduced to the sa community by da veiga 2015 and further extended by de lozzo and marrel 2016 especially in regard to the hilbert schmidt independence criterion hsic which detects features that are non influential on an output of interest for screening purposes real world applications that nowadays use these sa methods include nuclear safety iooss and marrel 2019 marrel and chabridon 2020 the use of shapley values shapley 1953 to develop importance measures being able to deal with dependent inputs features have emerged recently but independently in the sa owen 2014 and ml lundberg et al 2017 communities cross fertilization of ideas between ml and sa is expected to continue and grow over time broto et al 2020 mase et al 2020 hoyt and owen 2020 3 5 sa and uncertainty quantification sa in the context of uncertainty quantification uq has a long tradition stemming back to works such as bier 1982 in which global sensitivity measures were introduced to identify the key drivers of uncertainty in complex risk assessment problems since then there has been a growing synergy between uq and sa generally uq is the science of quantitative characterization and reduction of the uncertainty regarding a certain outcome of a system or model while sa for uq is focused on identifying the dominant controls of that uncertainty for brevity we do not summarize this rich history referring to saltelli et al 2008 sullivan 2015 borgonovo 2017 and to the handbook of uncertainty quantification by ghanem et al 2017 despite significant advances sa for uq still faces a number of challenges these include possible misconceptions in framing an sa problem for a uq purpose incompatibility of some sa frameworks for some model types complications with handling multivariate and correlated input spaces sensitivity of uq to problem setup and uncertainty in the sa results themselves in the following we explain these challenges and possible ways to address them 3 5 1 mind the goal of uq with respect to sa when sa is used in a uq application the underlying purpose of uq should dictate the framing of the sa problem and the method used in general there are two types of uq inverse uq and forward uq the former aims to estimate unknown model parameters from data while the latter propagates input uncertainties through a model to estimate output uncertainty in the case of inverse uq sa should be used to identify the parameters most informed by data for example by looking at the sensitivity of the misfit between the data and model predictions see section 3 2 3 in the case of forward uq however one needs to identify the factors that influence the prediction the most for a discussion refer to gupta and razavi 2018 and butler et al 2020 there is an implicit possibly flawed assumption in many applications of sa that the direction in factor space informed by data is parallel to the direction which informs predictions this assumption can yield misleading results as those two directions can often be orthogonal for example fixing parameters identified as non influential by sa in the inverse uq setting can lead to significant underestimation of prediction uncertainty this is because those parameters while being the largest source of uncertainty have been ignored in forward uq in most cases if the identification of a parameter is informed by data the uncertainty around it will decrease in the context of uq a comprehensive sa practice is one that identifies both of those important directions with such a practice the utility of sa for uq can be greatly improved as it would allow for the efficient estimation of uncertain parameters and quantify predictive uncertainty simultaneously to do so for a given problem sa needs to be applied in the two different settings independently one to assess the sensitivity of a goodness of fit metric to the factors and the other to assess the sensitivity of the predicted quantity itself gupta and razavi 2018 shin et al 2013 3 5 2 no single method for all model types sa can be used for a wide variety of model types for example those expressed in the form of partial differential equations pdes such as contaminant transport models e g wei et al 2014 those that are linked to the solution of an optimization problem e g dice lamontagne et al 2019 or stocfor3 lu et al 2020 or those that are in an agent based form fadikar et al 2018 different model types are engineered in different ways and this challenge demands systematic research that avoids encouraging an apply the same hammer attitude with respect to methods for example sa enabled with response surface surrogates see section 3 3 3 can be useful for lower dimensional problems with smooth parameter output maps while sampling based approaches can be more useful for non smooth models and higher dimensions becker 2020 moreover to date most sa methods have been developed for deterministic models that is the same input always produces the same output while little attention has been given to models with stochastic responses such as can occur with agent based models while the majority of sa applications have been to assess parametric variations in agent based models lee et al 2015 an open research question is how to include alternative agent based elements in a comprehensive sa so that one can assess sensitivity of the response to changes in the rule of an agent simultaneously with changes in a parameter moreover this question should be expanded to models in general where there is a challenge to jointly consider changes in model structure and parameters the exploration of global sa for optimization is a subject of recent research spagnol et al 2019 similarly optimization problems may call for the use of information theory based methods in fact early works such as avriel and williams 1970 show that the information value is a natural sensitivity measure when a decision support model is cast in the form of an optimization problem similarly felli and hazen 1999 oakley 2009 and strong and oakley 2013 suggest using the information value as a sensitivity measure to explicitly compare decision alternatives recently borgonovo et al 2021 discuss the conditions under which global sensitivity measures can be interpreted as information value thus the most important input is also the input that is most informative for the decision problem at hand for classification tools for low dimensional visualization of high dimensional data e g van der maaten and hinton 2008 could be explored for their useability within sa for stochastic models we note that the literature in the management sciences has addressed the problem intensively rubinstein 1989 hong 2009 hong and liu 2009 and investigators in other disciplines might benefit from those results 3 5 3 multivariate and correlated input spaces one of the most critical assumptions decisions in uq is the choice of the multivariate distributional properties of uncertain input variables which are propagated through the model in practice the marginal probability density functions pdfs of the inputs are obtained via various means such as direct measurements statistical inference design or operation rules and expert judgment and can be accompanied by an estimated level of accuracy or confidence in addition uq problems often come with certain constraints on the input for example when the input space is non rectangular and or when the inputs are dependent in such cases some sa methods become handicapped such as when the functional anova expansion becomes ill posed owen and prieur 2017 in general improper multivariate distributional properties including the correlation structure among inputs may lead to wrong inferences even if the most appropriate sa method is used do and razavi 2020 the field of sa in terms of methods to handle input constraints and correlation structures is still embryonic of the very few studies available one may refer to the work of kucherenko et al 2017 for non rectangular input spaces and to kucherenko et al 2012 tarantola and mara 2017 and do and razavi 2020 for correlated input spaces promising methods seem also to be moment independent methods borgonovo 2007 dependence measures da veiga 2015 de lozzo and marrel 2016 and shapley values owen and prieur 2017 whose definitions remain well posed in the presence of input constraints nonetheless the presence of constraints also impacts other aspects of sa such as the interpretation of interactions and the assessment of direction of change for these aspects also further research is needed to identify the most appropriate methods 3 5 4 curse of dimensionality the state of the art models that are often encountered in uq problems are commonly associated with high dimensionality and significant computational burden as discussed in section 3 3 higher dimensionality exacerbates the difficulty of assigning multivariate distributions to uncertain inputs as discussed in the previous section a second difficulty is with anova type expansions where the number of interaction terms is exponential in the number of inputs such cases require excessively large sample sizes often becoming computationally prohibitive a third difficulty is with the sampling strategies themselves in high dimensional spaces many modern sampling strategies optimize the way samples are taken to allow a parsimonious use of the model and to maximize efficiency given the available computational budget pronzato and müller 2012 pázman and pronzato 2014 sheikholeslami and razavi 2017 becker et al 2018 however optimization based sampling in high dimensional spaces can become challenging due to the curse of dimensionality greedy sampling methods can be used to reduce the computational cost of optimization based sampling methods oakley and o hagan 2004 maday et al 2009 schaback and wendlend 2006 jakeman et al 2019 harbrecht et al 2020 but while being efficient in many cases they can still ultimately suffer from the curse of dimensionality often the lower dimensional subspaces that impact estimates of uncertainty are efficiently described by linear or possibly non linear combinations of parameters where sa is a means to an end being unable to uniquely identify individual parameters is often inconsequential by moving beyond identifying directions aligned with the axes of the parameter space significant dimension reduction can be achieved ideally sa should identify directions that are most influential consider a simulator that is a nonlinear function of the equally weighted sum of parameters y p1 p2 2 each parameter will be found to be important but only one direction will have a non zero influence on the function in 2d the x y plane the function will be constant in all orthogonal directions recently great success has been achieved using methods such as active subspaces which find linear rotations of the parameters which are important constantine 2015 finding non axial directions has been used successfully to reduce the cost of inverse uq these methods work by restricting resources to identifying and exploiting subspaces that are informed by data and can reduce the computational cost of the inverse problem by orders of magnitude tonkin and doherty 2005 spantini et al 2015 when estimating data informed prediction uncertainty e g combining forward and inverse uq the optimal approach is to find the directions that are both informed by data and that influence predictions when quantifying uncertainty for linear models these directions can be computed exactly using generalized eigenvalue decompositions initial work has been carried out for linear models lieberman and willcox 2014 but further research is needed especially for non linear models kucherenko et al 2011 showed that it is not the model nominal dimensions but effective dimensions that define the model complexity in this respect they loosely divided models into three types a models with only a few important variables b models with equally important variables and with dominant low order interactions terms in their anova decomposition and c models with equally important variables and with dominant high order interaction terms they argued that type a and b models have low effective dimensions and therefore their handling with sa is relatively easy regardless of their nominal dimensionality 3 5 5 sensitivity of uq to modeling choices the assessment of how uncertainty estimates change with different modeling decisions such as numerical discretization schemes is important but often ignored for example when using numerical solutions to pdes between two coupled models the choice of accuracy and cost of uq depends on the mesh and timestep size of each model and the resolution of the coupling between the models in space and time identifying how sensitive a prediction is to these choices can significantly reduce the cost of uq for example if the final prediction is relatively insensitive to one model the resolution of that model and its coupling can be coarse without trading off accuracy the other important decision one may make is what output to choose in framing the sa problem indeed various outputs can be considered such as the mean of the model response its variance a probability that the output exceeds a threshold or a quantile of the output for example in the context of risk or reliability analysis liu et al 2006 the sa results would be different if the quantity of interest is related to the tail of the distribution of a model output such as a failure probability a quantile or a super quantile hong and liu 2010 lemaître et al 2015 compared to a case where the sensitivity of a measure of central tendency is of interest the problem of matching the output of interest with the sensitivity measure is discussed in borgonovo et al 2016 where several global sensitivity measures variance based moment independent quantile based are examined from an information value viewpoint see also section 3 2 2 for a discussion on the framing of the sa problem from a model verification point of view the goal oriented sa framework fort et al 2016 is relevant in this context as well moreover in many applications the probability density functions pdfs used to describe the uncertainty in inputs may themselves be highly uncertain morio 2011 this second level of input uncertainty is often the case where there are no data available regarding the processes that those inputs control addressing this type of uncertainty is an essential and fruitful area of research and has recently been attracting attention input pdf robustness analysis has been recently defined as a particular setting of sa lemaitre et al 2015 gauchy et al 2020 other example works include chabridon et al 2018 for rare event reliability analysis schöbi and sudret 2019 in the context of probability boxes hart and gremaud 2019a 2019b for variance based indices and meynaoui et al 2019 for hilbert schmidt independence criterion hsic 3 5 6 uncertainty in sa results themselves a major component of best practice in sa is the assessment of uncertainty in the estimates of sensitivity measures this uncertainty is directly related to reliability and robustness of sa as discussed in section 3 3 while well known methods such as bootstrapping efron 1987 are available to provide an uncertainty estimate it is notable that a minority of works apply this quantification systematically bootstrapping does however need to be handled with caution as strictly the samples taken should be random as with monte carlo samples and it requires smoothness and symmetry of the bootstrap distribution which is not always attainable care is also required to check if the sample size is too small to contain enough information for bootstrapping more advanced bootstrap procedures are required if the distribution is skewed or multimodal such as bias corrected and accelerated bootstrap intervals efron 1987 recent progress has seen more general bootstrap like methods that can work well for different types of samples and sampling strategies including bootstrapping of samples generated by quasi monte carlo or latin hypercube sampling based on multiple independent replicates of an estimator owen 2013 heuristic approaches such as introducing dummy parameters zadeh et al 2017 and model variable augmentation mai and tolson 2019 have also shown promise furthermore future work could follow bayesian methods for the calculation of confidence intervals on the estimates of global sensitivity measures for example studies refer to oakley and o hagan 2004 le gratiet et al 2014 and antoniano villalobos et al 2020 further research is needed especially in the presence of the curse of dimensionality sensitivity analysis of sensitivity analysis has been also suggested as a way to measure the influence of the analysis own design parameters haghnegahdar and razavi 2017 puy et al 2020a puy et al 2020b and the choice of methods razavi and gupta 2015 mora et al 2019 3 6 sa in support of decision making 3 6 1 the deep roots of sa in the field of decision making sa has historically but informally been a major building block of the decision making process the notion of sa for example has been embedded in the classic and widely used concepts of shadow prices dorfman et al 1987 and scenario analysis duinker and greig 2007 elsawah et al 2020b the former used in constrained optimization quantifies how much more profit i e objective function one would get by increasing the amount of a resource by one unit i e constraints this practice can be viewed as one factor at a time local sa oat sa one form of lsa often on continuous variables around an optimal point in the decision variable space the latter however revolves around what if scenarios evaluation of policy effectiveness analysis of causality and robustness analysis where one or several variables at once are changed around a base case or within a factor space to evaluate change in the outcome a what if scenario evaluates the effect of a change in inputs on a decision outcome while policy effectiveness evaluates either the effect size continuous variables or existence of effect of a policy change discrete variables analysis of causality attributes change in the output to change in inputs and robustness analysis can either test whether a recommendation changes guillaume et al 2016 or evaluate the effect of changes in factor space mcphail et al 2018 robustness is particularly recognized as useful in addressing uncertainty arising from the existence of multiple plausible futures maier et al 2016 iwanaga et al 2020 and other forms of deep uncertainty marchau et al 2019 the above examples indicate how informal and often local sa has contributed and will continue to contribute to a variety of decision making problems we note that while lsa has been often criticized for being perfunctory saltelli and annoni 2010 when used to support mathematical modeling see section 3 2 it is an essential means for many decision support systems where the users need to assess the impact of a change in a policy or the environment on the status quo consistent with the role and function of sa these strong ties exist because decision making is in fact fundamentally about identifying how objectives of interest are influenced by possible interventions in economics the ceteris paribus concept a latin phrase meaning all else being equal is on the basis of a one factor at a time local sa this concept is used in mainstream economic thinking to measure the effect of a shock to one economic variable e g the price of a commodity or a set of wages on another provided all other variables remain the same economists know well that this is a crude approximation mirowski 2013 so the point is the use one makes of it for example this approach would be good to understand the system but poor to prescribe a policy response similar cautions apply to the what if scenarios described above to address these possible limitations formal sa has recently found its footing in decision science as described below 3 6 2 modern sa for decision making under uncertainty more recently formal approaches to sa particularly for global sa have emerged as a means to support decision making under uncertainty sa can decompose uncertainty in the outcome of a decision option and attribute that to different sources of uncertainty in a decision problem identifying the dominant controls of uncertainty and how they interact adds transparency to the problem and guides the decision making process towards minding the uncertainties that matter the most tarantola et al 2002 first laid down a framework on how modern sa can support decision analysis which has since gained significant momentum by outlining sa capabilities to 1 understand whether the current state of knowledge on input uncertainty is sufficient to enable a decision to be taken maier et al 2016 2 identify data sources or parameters that require investing resources for knowledge improvement to achieve the desired level of confidence in making a decision lamontagne et al 2019 3 in the presence of different policy options clarify how various uncertainty sources and their extents affect the confidence in the expected outcome of each policy marangoni et al 2017 4 flag models used out of context and to a degree of complexity not supported by available information for the decision problem at hand herman et al 2015 and 5 invalidate policy assessments in cases where untested possibly unjustified assumptions dominantly control model outputs workman et al 2020 puy et al 2020c 3 6 3 sa and robustness of decisions under deep uncertainty assessment of the robustness of decision alternatives is becoming increasingly important in light of deep uncertainty which refers to a situation when stakeholders do not know or cannot agree on a system model that relates action to consequences the probability distributions to represent uncertainty in the inputs to the model and or how to value the desirability of alternative outcomes lempert et al 2003 maier et al 2016 addressing deep uncertainty requires the identification of options that perform well over a wide range of plausible future conditions this creates additional challenges and opportunities for the development of sa approaches in support of decision making especially in terms of how to best perturb model inputs robustness in this context refers to the insensitivity of a decision outcome to variation in model inputs and parameters and in general to the assumptions made in the decision making process the robustness of the utility of a particular decision alternative can be quantified with the aid of robustness metrics which use different ways to combine model outputs from different sensitivity trials corresponding to different sets of plausible combinations of model inputs into a single value to quantify different aspects of the performance of a decision alternative over these trials such as best case performance worst case performance average performance and variability in performance see mcphail et al 2018 sa based assessment of the robustness of decision alternatives under deep uncertainty requires careful consideration of the way the model input space is sampled see mcphail et al 2020 depending on the philosophy that underpins the robustness assessment if the goal is to quantify robustness under as broad a range of future conditions as possible then a large number of samples that cover the model input space as uniformly as possible is required however if the goal is to calculate robustness under possible future states of the world that represent alternative plausible conditions under different assumptions mahmoud et al 2009 the number of samples used is quite small 10 as each sample generally corresponds to a coherent narrative storyline scenario of an alternative hypothetical future van notten et al 2005 such scenarios are plausible stories about the future of a system that is too complex to predict wiek et al 2013 elsawah et al 2020b and are often obtained via participatory processes involving a variety of stakeholders e g riddell et al 2018 wada et al 2019 razavi et al 2020 it should also be noted that due to the temporal dimension associated with deep uncertainty the samples of the model input space often correspond to time series e g guo et al 2018 culley et al 2019 riddell et al 2019 irrespective of which philosophical approach underpins the robustness assessment 3 6 4 sa and ranking of decision alternatives sa has mainly been used to determine the sensitivity of model outputs to plausible changes in model inputs and parameters while this can provide useful information to support decision making it does not assess the sensitivity of the relative ranking or preference of different decision alternatives to potential changes in model inputs and parameters this can be achieved by sa when focused on identifying the smallest combined changes in model inputs and parameters that result in performance rank equivalence of two decision alternatives which can be expressed as a distance metric the smaller this metric the more robust insensitive the relative performance rank of a particular decision alternative and vice versa in addition the model inputs and parameters that have the largest influence on the relative performance rank of decision alternatives can be identified while such sensitivity analyses have already been applied to simulation models e g ravalico et al 2009 2010 marangoni et al 2017 lamontagne et al 2019 puy et al 2020c as well as decision models such as multi criteria decision analysis hyde et al 2005 2006 herman et al 2015 ganji et al 2016 they need to be developed further especially under conditions of deep uncertainty to ensure robust decision outcomes are achieved 3 6 5 sa and qualitative aspects of decision making sa is not only a quantitative paradigm but also an epistemological one when used for regulation and policy making sa must be broadened to include consideration of epistemological aspects linked to the plurality of disciplines and interested actors at play different norms and incommensurable values may emerge in this context questions such as what are the different narratives of a problem who is telling what story and which of these narratives are being privileged in the modeling activity carried out to support the decision making process are naturally brought to the fore to address these issues saltelli et al 2013 proposed a framework for sensitivity auditing sensitivity auditing emphasizes the framing of a decision analysis its institutional context and the motivations and interests of the researchers stakeholders and policy makers involved an analyst can scrutinize a mathematical model used to assist a decision making process against the sensitivity auditing checklist to cast light on potential criticalities such as 1 rhetorical use of mathematical modeling 2 identification of the underpinning technical assumptions 3 uncertainty inflation or deflation 4 unaddressed uncertainty and sensitivity of the model at the time the results are published 5 lack of model transparency 6 frames privileged and frames excluded and 7 incomplete or lack of sa the european commission 2015 and the european science academies sapea 2019 recommend sensitivity auditing in the context of modeling in support of policy making example applications of sensitivity auditing are found in the fields of education oecd pisa study araujo et al 2017 food security saltelli and lo piano 2017 public health and nutrition lo piano and robinson 2019 and sustainability metrics galli et al 2016 the above seven points of sensitivity auditing are also substantially subsumed in the manifesto for responsible modeling published in nature saltelli et al 2020 further attempts to thoroughly capture the quality of the knowledge in a modeling activity include the model pedigree concept eker et al 2018 and the numeral unit spread assessment and pedigree nusap framework van der sluijs et al 2005 for knowledge quality assessment incidentally both nusap and sensitivity auditing are approaches belonging to the tradition of post normal science a style of use of science for policy that becomes relevant when facts are uncertain values are in dispute stakes are high and decisions are urgent funtowicz and ravetz 1993 3 6 6 revisiting the link between sa and decision making while formal sa is finding its footing in the area of decision making there is a need to revisit the principles of decision making to identify where and how decision theories and applications have perhaps informally been based on the fundamentals of sa such efforts could facilitate bridging the two fields and take advantage of recent advances in sa in emerging decision problems across a variety of domains as well as correspondingly motivate advances in sa methodology for decision making in this process one must be mindful of the commonly asked questions by decision makers studies with formal sa methods often tend to answer different often more sophisticated questions to those related to specific quantities of interest that decision makers care most about therefore to be most useful decision makers need to be engaged in the process of co formulating the sa problem to ensure it addresses the right question s 4 synthesis and concluding remarks the process of developing the common perspective expressed in this paper across the multidisciplinary team of authors faced interesting challenges related principally to the various disciplinary and methodological views as well as experiences across different application areas that diversity promoted a synergy and more comprehensive coverage of potential opportunities to strengthen the role of sa as summarized in the following key messages a collective efforts are needed to structure generalize and standardize the state of the art in sa such that it forms a distinct cross field discipline section 3 1 such efforts must emphasize 1 teaching sa as integral to systems analysis and modeling and decision making 2 developing protocols for best sa practice that are transferable across specific contexts and applications and 3 launching scientific journals dedicated to sa b much work is needed to realize the tremendous untapped potential of sa for mathematical modeling of socio environmental and other societal problems which are confounded by uncertainty section 3 2 sa can help with the management of uncertainty by 1 characterizing how models and the underlying real world systems work 2 identifying the adequate level of model complexity for the problem of interest and 3 pointing to possible model deficiencies and non identifiability issues as well as where to invest to reduce critical uncertainties c computational burden is recognized as a major hindrance to the application of sa to cases where sa can be most useful such as for high dimensional problems section 3 3 greater efforts should be directed to developing sa algorithms that are 1 more computationally efficient 2 more statistically robust 3 able in particular to consume recycled samples however taken and 4 able to provide credible confidence measures on their results d the recent revitalized rise of machine learning ml particularly deep learning methods could be further enhanced by formal theories of sa section 3 4 the great potential of sa needs to be discovered for the following purposes and beyond 1 explainability and interpretability of ml 2 input variable selection 3 enabling ml to work with small data where big data sizes are not available and 4 building trust in ml models e sa is a much needed complement and or building block to most uncertainty quantification uq practices regardless of whether the aim is forward or inverse uq section 3 5 sa and uq need to be better combined to support a variety of purposes including 1 apportioning uncertainty 2 handling the curse of dimensionality 3 addressing unknowns around the distribution of inputs and their correlation structure and 4 assessing the sensitivity of uncertainty estimates to various choices made in the design of a uq problem f decision and policy making under uncertainty can significantly benefit formally or informally from advancements in sa including from the notion of sensitivity auditing that is an extension of sa where systems models and the legitimacy of processes undertaken in their development are used to support policy section 3 6 conversely sa can benefit from reflecting on and formalizing ways in which decision making and decision makers have previously used sa concepts informally sa when used in support of decision making can address critical questions such as 1 where and how does uncertainty matter 2 have the impacts of all important assumptions been treated 3 where should we invest to increase confidence in the expected outcome of a policy option and 4 has the policy uncertainty been artificially inflated or constrained all together the above points call for more cross fertilization of different research and practice streams on sa across a wide range of disciplines an implication of this broadening of sa is that it should be considered a multi discipline it is a subject that is intrinsically of interest to multiple disciplines that will continue to have distinct but ideally interconnected literature mathematicians and computer scientists are interested in more efficient ways of calculating measures decision scientists are interested in identifying different measures modelers systems analysts are interested in how they can use those measures in their work and decision makers are interested in the outputs and implications of the analyses sa is a vertically integrated deep topic those at the surface at the highest level of abstraction do not want to know and do not really need to know what is happening at the bottom they simply apply a method already developed for their purpose conversely those at the bottom produce fundamental work that does not always need to be directly responsive to immediate demands at the top for example the computational inefficiency of an algorithm in practice does not matter much at the development stages of new theories from this perspective sa therefore needs coordination rather than consensus we expect that multiple views and even definitions of core concepts will continue to co exist but the field needs to ensure that cross fertilization of ideas continues and expands to allow different disciplines and application areas to benefit from one another despite their differences in order to be of progressive impact to society it is crucial that this coordination then connects with the needs of planners policy analysts and decision makers with active engagement supporting the development of a shared understanding of the questions that they want answered as well as the questions they do not yet know they want answered the authorship team of this perspective invites discussion and collaboration with researchers and practitioners across every area of science interested in theories developments and applications of sa in our vision over the next decade sa will underpin a wide variety of activities around scientific discovery and decision support declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this perspective paper is the outcome of a one day workshop called the future of sensitivity analysis which was held as a satellite event to the ninth international conference on sensitivity analysis of model output samo october 28 31 2019 in barcelona spain we are thankful to the sponsors of this event including the french research association on stochastic methods for the analysis of numerical codes mascot num open evidence research at universitat oberta de catalunya the joint research centre of the european commission the university of bergen norway and the french cerfacs centre européen de recherche et de formation avancée en calcul scientifique the financial and logistic support to saman razavi including underwriting open access publication fees by the integrated modeling program for canada impc under the framework of global water futures gwf is acknowledged furthermore part of the efforts leading to this paper was supported by the national socio environmental synthesis center of the united states under funding received from the national science foundation dbi 1639145 sandia national laboratories is a multi mission laboratory managed and operated by national technology and engineering solutions of sandia llc a wholly owned subsidiary of honeywell international inc for the u s department of energy s national nuclear security administration under contract de na 0003525 the views expressed in the article do not necessarily represent the views of the u s department of energy or the united states government john jakeman s work was supported by the u s department of energy office of science office of advanced scientific computing research scientific discovery through advanced computing scidac program joseph guillaume received funding from an australian research council discovery early career award project no de190100317 arnald puy worked on this paper on a marie sklodowska curie global fellowship grant number 792178 takuya iwanaga is supported through an australian government research training program agrtp scholarship and the anu hilda john endowment fund we would like to thank dan ames editor in chief for insightful comments and encouragement 
25877,sensitivity analysis sa is en route to becoming an integral part of mathematical modeling the tremendous potential benefits of sa are however yet to be fully realized both for advancing mechanistic and data driven modeling of human and natural systems and in support of decision making in this perspective paper a multidisciplinary group of researchers and practitioners revisit the current status of sa and outline research challenges in regard to both theoretical frameworks and their applications to solve real world problems six areas are discussed that warrant further attention including 1 structuring and standardizing sa as a discipline 2 realizing the untapped potential of sa for systems modeling 3 addressing the computational burden of sa 4 progressing sa in the context of machine learning 5 clarifying the relationship and role of sa to uncertainty quantification and 6 evolving the use of sa in support of decision making an outlook for the future of sa is provided that underlines how sa must underpin a wide variety of activities to better serve science and society keywords sensitivity analysis mathematical modeling machine learning uncertainty quantification decision making model validation and verification model robustness policy support 1 introduction 1 1 the whats and whys of sensitivity analysis sensitivity analysis sa in the most general sense is the study of how the outputs of a system are related to and are influenced by its inputs in many applications the system in question involves a single or a set of mathematical models encoded using computer software that simulates the functioning of a real world system of interest such mathematical models can be data driven also called statistical directly mapping inputs to outputs engelbrecht et al 1995 rodriguez et al 2010 or mechanistic also called process based solving a set of differential or other mathematical equations governing the possibly spatio temporal behaviors of the underlying processes maxwell and miller 2005 haghnegahdar et al 2017 inputs of interest commonly referred to as factors in sa may include model parameters forcing variables boundary and initial conditions choices of model structural configurations assumptions and constraints outputs may include any functions of model responses including those that may vary over a spatio temporal domain objective functions such as a production or cost function in cost benefit analysis or an error function in model calibration why is sa useful in short it addresses several fundamental overarching purposes of systems analysis and modeling a scientific discovery to explore causalities and how different processes hypotheses parameters scales and their combinations and interactions affect a system e g gupta and razavi 2018 b dimensionality reduction to identify uninfluential factors in a system that may be redundant and fixed or removed in subsequent analyses e g sobol et al 2007 c data worth assessment to identify processes parameters and scales that dominantly control a system for which new data acquisition reduces targeted uncertainty the most e g guillaume et al 2019 partington et al 2020 and d decision support to quantify the sensitivity of an expected outcome to different decision options constraints assumptions and or uncertainties e g tarantola et al 2002 sa is now considered a requirement for good modeling practice as indicated by some existing guidelines european commission 2015 saltelli et al 2020 in general and regardless of any specific purpose sa aims to exploit the sparsity of factors principle a heuristic stating that very often only a small subset of factors in a system have a significant influence on a specific system output box and meyer 1986 1 2 why this position paper sa is a relatively new area of research it has roots in design of experiments doe which is a broad family of statistical methods conceived in the early 20th century for designing efficient experiments to acquire representative information about the existence of effects of one or multiple variables on another variable in a system fisher 1953 doe primarily worked in the context of costly noise prone lab or field environments the field of sa started to materialize in the 1970s and 80s with the beginning of the widespread availability of computers for mathematical modeling e g cukier et al 1973 and the extension of doe to the design and analysis of computer experiments dace which are typically noise free or deterministic in the sense that replicating a computer experiment with the same inputs results in identical model responses sacks et al 1989 more broadly the notion of sensitivity has historically but informally been a building block in various types of study particularly in decision making where what if scenarios or policy effectiveness are assessed by changing one or multiple factors at a time tarantola et al 2000 what is the status quo in sa we believe that sa is en route to becoming a mature and independent but interdisciplinary and enabling field of science tremendous advances in both theory and application of sa have been accomplished as documented in the recent reviews by norton 2015 iooss and lemaître 2015 wei et al 2015 razavi and gupta 2015 borgonovo and plischke 2016 pianosi et al 2016 borgonovo et al 2017 borgonovo 2017 ghanem et al 2017 gupta and razavi 2017 and saltelli et al 2019 particularly in recent years research and practice in sa have gained significant momentum with many researchers from a variety of backgrounds contributing to a variety of theoretical frameworks based on the type of applications in their respective disciplines despite these advances realization of the benefits and true potential of sa across the sciences has been hampered by several challenges amongst others the most pressing challenge is that sa is still a paradigm defined largely by method rather than purpose various methods have been developed rooted in different philosophies towards sa razavi and gupta 2015 but often the purpose has been defined on the basis of how a given method works and its capabilities as well as its authors disciplinary research focus narrow views lack of communication among scientists across disciplines and ignoring uncertainties in models can conceal the benefits of sa to researchers and practitioners leading to an underuse of sa in many branches of modeling and consequently in support of model based policy making saltelli et al 2019 in this perspective paper we synthesize key aspects of the state of the art and by taking a forward looking approach outline some grand challenges facing the new frontiers and opportunities for sa we draw from the multidisciplinary views and expertise of the authorship team which includes natural scientists engineers decision scientists computer scientists systems analysts and mathematicians and provide opinions on the possible and desirable future evolutions of sa science and practice our overarching goal is to contribute to the establishment of sa as a distinct and essential interdisciplinary enabling science we believe that sa science must formally become an integral part of systems analysis in general and mathematical modeling in particular to unleash the capabilities of sa for addressing emerging problems in engineering technology the natural and social sciences and human natural systems the following sections are structured such that one could directly read a section of interest without attention to the other sections 2 an overview of the state of the art to many sensitivity analysis sa simply means a process in which one or multiple factors in a problem are changed to evaluate their effects on some outcome or quantity of interest such a process has a long history of application perhaps in all areas of science examples include assessment of the effectiveness of a decision option in a policy making problem the impact of a problem constraint on the optimality of a cost or benefit function via shadow prices or the role and function of a model parameter in generating a model output such analyses are generally referred to as local sensitivity analysis lsa because the sensitivity of the problem is assessed only around a nominal point in the problem space lsa is simple intuitive and appropriate in very specific circumstances it has however been commonly used much more broadly in circumstances where it has been criticized for providing only a localized view of the problem space especially when used in the context of investigating parameter importance in mathematical modeling saltelli and annoni 2010 saltelli et al 2019 the modern era of sa has focused on a notion that is commonly referred to as global sensitivity analysis gsa saltelli et al 2000 as it attempts to provide a global representation of how the different factors work and interact across the full problem space to influence some function of the system output see fig 1 in this section we briefly summarize four categories of gsa derivative based distribution based variogram based and regression based we also provide overviews of sa when enabled with response surface methods progress in sa with correlated factors and software tools available for sa 2 1 derivative based approach derivative based methods are a natural and intuitive extension of lsa where measures of local sensitivity are computed at many base points across factor space and somehow averaged to provide a global assessment of sensitivity such measures are said to be derivative based as they either analytically compute derivatives or numerically quantify the change in output when factors of interest continuous or discrete are perturbed around a point perturbations typically occur one at a time and by some specific perturbation size campolongo et al 2011 the different derivative based methods differ in the ways that they choose the base points the perturbation size and the distributional properties of the sampled derivatives e g first or second moment to provide an average global measure of sensitivity see e g morris 1991 campolongo et al 2007 sobol and kucherenko 2009 campolongo et al 2011 lamboni et al 2013 rakovec et al 2014 the outcome of these methods is sensitive to these choices among which the sensitivity to perturbation size is generally overlooked even though it can be profound shin et al 2013 haghnegahdar and razavi 2017 2 2 distribution based approach distribution based methods adopt a different philosophy that bases the analysis on the distributional properties of the output itself and attempts to quantify how the different inputs contribute to forming those properties the most common distribution based method is based on the analysis of output variance decomposing that variance into portions attributed to individuals or groups of inputs sobol 1993 owen 1994 homma and saltelli 1996 such a variance based sa was first conceived in the context of non linear dependence as far back as 1905 pearson 1905 and later in terms of a fourier analysis in the 70s cukier et al 1978 the full variance based sa framework was laid down by ilya sobol in 1993 sobol 1993 then linked to the derivative based sa via poincaré inequalities by sobol and kucherenko 2009 see also roustant et al 2017 some distribution based methods go beyond variance and investigate how higher order moments of the output depend on the inputs for example the method of dell oca et al 2017 particularly focuses on skewness and kurtosis some other distribution based methods are however moment independent in that they measure the difference between the unconditional distribution of the output and its conditional counterparts when one or more inputs are fixed for example the method of borgonovo 2007 measures this difference via the borgonovo index while the methods of krzykacz hausmann 2001 and pianosi and wagener 2015 pawn use the mutual information and kolmogorov smirnov test respectively another example is the commonly called regional sensitivity analysis rsa which rather than fixing inputs defines conditional distributions based on thresholds for the model response spear et al 1994 hamby 1994 2 3 variogram based approach more recently a third category has emerged based on the theory of variograms that bridges derivative and distribution based methods razavi and gupta 2016a 2016b sheikholeslami and razavi 2020 the variogram based approach recognizes that model outputs are not always randomly distributed and they possess as do their partial derivatives a spatially ordered covariance structure in the input space anisotropic variograms can characterize this structure by quantifying the variance of change in the output as a function of perturbation size in individual inputs variogram based sensitivity measures can be considered more comprehensive than other approaches in the sense that they integrate global sensitivity information across a range of perturbation scales derivative based and variance based sensitivity measures are also produced as a side product of calculating variogram effects the efficiency and applicability of the variogram based approach are demonstrated in razavi et al 2019 becker 2020 and puy et al 2020a 2 4 regression based approach regression based sa has a long history traditionally referring to methods that infer sensitivity information from coefficients of a typically linear regression model fitted to a sample of model response surface points kleijnen 1995 those early methods from a gsa point of view have been criticized for their heavy reliance on the prior assumption regarding model response form e g linear or polynomial equation and the fact that if the quality of fit is poor the sensitivity estimates are not reliable razavi and gupta 2015 from an lsa point of view however they have proven useful for dimensionality reduction via orthogonal decompositions from parameter samples kambhatla and leen 1997 or locally approximated sensitivity matrices tonkin and doherty 2005 also such methods when using quadratic regression allow characterization of parameter interactions in the inverse problem e g shin et al 2015 more recently regression based sa has witnessed a new generation of methods arising from the machine learning community the goal of these methods typically is to provide the commonly called variable importance measures following two general approaches in one they assess the importance of each or a sub set of inputs in constructing a response surface via for example multivariate adaptive regression splines mars friedman 1991 if the inclusion of an input or set of inputs significantly improves the quality of fit they are deemed important gan et al 2014 the other approach conversely first fits a response surface model using all inputs and then assesses how the quality of fit degrades when the sample points for each input or set of inputs are permuted breiman 2001 lakshmanan et al 2015 while these approaches are typically restricted to importance in fitting data they do have the advantage of also extending well to classification models for example using random forests e g hapfelmeier et al 2014 allowing for sensitivity measures that apply to both discrete and continuous variables 2 5 response surface assisted sa in the early 2000s applied mathematicians formally working on design and analysis of computer experiments dace started building linkages with sa santner et al 2003 fang et al 2005 their emphasis has particularly been placed on linking sa and asymptotic statistical theory janon et al 2014a gamboa et al 2016 space filling designs of experiments tissot and prieur 2015 gilquin et al 2020 structural reliability fort et al 2016 and bayesian estimation pronzato 2019 moreover response surface surrogates rooted in dace such as polynomial chaos and gaussian process regression have found applications to approximate sensitivity measures in the case of computationally intensive models oakley and o hagan 2004 janon et al 2014a 2014b wang et al 2020 a review of these linkages can be found in ghanem et al 2017 2 6 sa with correlated inputs one persistent issue in sa is that nearly all applications regardless of the method used rest on the assumption that inputs are uncorrelated sobol 1993 hoeffding 1948 inputs however can be correlated and their joint distribution can take a variety of forms in real world problems correlation in this context refers to statistical dependency between any subset of inputs independent of the system under investigation the correlation effect is different from the interaction effect which refers to the presence of non additivity of the effects of individual inputs on the system output razavi and gupta 2015 it is now being increasingly recognized that ignoring correlation effects and multivariate distributional properties of inputs largely biases or even falsifies any sa results do and razavi 2020 recently several methods have been developed to account for such properties including extensions of the hoeffding sobol decomposition chastaing et al 2012 regression based methods xu and gertner 2008 copula based methods kucherenko 2012 do and razavi 2020 sheikholeslami et al 2020 and game theory concepts owen 2014 owen and prieur 2017 iooss and prieur 2019 2 7 software tools and applications to promote and advance the use of sa there has been tremendous albeit fragmented progress in building computer packages these operationalize the various sa methods via different programming languages and include dakota adams et al 2020 in c sobolgsa kucherenko and zaccheus 2020 in c matlab and python uqlab marelli and sudret 2014 in matlab openturns baudin et al 2017 in python and c the sensitivity package iooss et al 2018 in r salib herman and usher 2017 in python psuade tong 2015 in c vars tool razavi et al 2019 in matlab and c safe pianosi et al 2015 pianosi et al 2020 in matlab r and python mads jl in julia vesselinov et al 2019 and sensobol puy 2020 as discussed in douglas smith et al 2020 software programs for sa adopt different design philosophies which reflect different disciplinary foci and vary in terms of usability including extent of documentation and assumption of users prior knowledge in parallel the generation of test beds for different methods and software packages is receiving increasing attention razavi et al 2019 becker 2020 applications of sa are widespread across many fields including earth system modeling wagener and pianosi 2019 engineering guo et al 2016 biomechanics becker et al 2011 water quality modeling koo et al 2020a and 2020b hydrology shin et al 2013 haghnegahdar and razavi 2017 water security puy et al 2020c nuclear safety saltelli and tarantola 2002 iooss and marrel 2019 and epidemiology burgess et al 2017 vanderweele and ding 2017 to name a few the most quoted handbook for sa is a primer for global sensitivity analysis saltelli et al 2008 with over 5000 citations from across the scientific disciplines a cross disciplinary review of sa applications can be found in saltelli et al 2019 the wide and growing use of sa suggests that a cohesive treatment of sa as a discipline in its own right inclusive of a well honed syllabus for teaching will have a large beneficial impact across the sciences in general 3 challenges and new frontiers given the significant progress and popularity of sensitivity analysis sa in recent years it is timely to revisit the fundamentals of this relatively young research area identify its grand challenges and research gaps and probe into the ways forward to this end the multidisciplinary authorship team has identified six major themes of challenges and outlook as outlined in fig 2 in the following we discuss our perspective on the past present and future of sa under each theme in a dedicated section the overarching objective here is to identify possible future avenues that will take sa to the next level one that is especially beneficial to meeting the challenges of modeling complex societal and environmental problems e g elsawah et al 2020a 3 1 towards a structured generalized and standardized sa discipline while sa is now considered by some to be standard practice in modeling norton 2015 pianosi et al 2016 razavi and gupta 2015 it is not a formally recognized discipline nor a coherent subfield of applied mathematics or applied statistics for example it is spread across the mathematics subject classification ams 2020 sociologically disciplinary fields are communication systems enabling discourse and the dissemination of knowledge between practitioners stichweh 2001 additionally formally recognized disciplines are organizationally distinct with communities structured around the production of knowledge and training of practitioners casetti 1999 stichweh 2001 sa fulfils all of these criteria with the exception of an organizational community that make it distinct from related fields of study for instance there are no academic institutions nor is there any scientific journal focused on sa the only official link binding part of the sa family is an international conference called sensitivity analysis of model output samo held once every three years since 1995 the 9th instalment of which was held in 2019 in barcelona spain with the forthcoming 10th instalment to take place in tallahassee florida usa in 2022 consequently the current family of sa researchers and practitioners is spread over many disciplines and there are dramatic differences in the sa capacity and maturity in different contexts despite the lack of a specialized journal for sa there is a relatively large and growing number of publications on the subject within the water resources research area alone the number of sa publications has grown significantly over the past decades based on a search in the clarivate analytics web of science platform the yearly publication count is now equal to about one third of publications in the related but well established field of optimization razavi and gupta 2015 the fact that optimization is an extensive discipline with several dedicated journals suggests that sa should rightfully seek to become an independent discipline albeit one that interacts effectively with other disciplines in the meantime treatment of sa as a subject is diverse and couched in disciplinary specific terms and traditions and taught to varying standards saltelli 2018 accordingly the emphasis and importance of sa across the fields in which it is applied are equally as diverse this state of affairs is not unlike the beginnings of other scientific fields such as computer science which separated from mathematics and engineering sometime in the 1940s tedre 2007 denning et al 1989 and hydrology which did not find its footing as a separate discipline until the 1950s or arguably the early 90s mccurley and jawitz 2017 klemeš 1986 the diverse treatment of sa research is partly responsible for the existing sluggishness to accept sa as a discipline 3 1 1 recognize sa as a discipline recognition of sa as a discipline in its own right requires universal acknowledgement that methodological developments and guidance for application of sa are frequently transferable across application contexts a major challenge to overcome is that of inconsistencies in terminology methodology and fundamental definitions across the contexts and disciplines in which sa is applied saltelli et al 2019 arguably there are examples in the literature where sa practice has been perfunctory or inappropriate possibly misinforming the users and even a resulting policy saltelli and annoni 2015 saltelli et al 2019 furthermore although the links between sa and the well established field of uncertainty quantification uq are clear to sa researchers see section 3 5 most sa applications seen in the academic literature do not adequately map the uncertainty in model inference saltelli and annoni 2010 ferretti et al 2016 saltelli et al 2019 the exceptions are typically publications written by sa researchers who pursue sa methodological developments this has the unfortunate result that sa related publications fall largely into two classes proposals for new or refined methods with illustrative applications written by sa researchers or application papers with often poor quality sa written by non sa researchers the scientific journals where these findings are published are largely disconnected for the two classes in an ideal world modeling teams should include at least one researcher versed in sa given the wide applicability of sa its practitioners are dispersed across the sciences and their work similarly disseminated there is a need to establish a publication outlet focused specifically on sa to signify the separate concerns and foci of research establishment of an sa specific journal would strengthen communication of the current state of sa research particularly for uninitiated modelers another challenge to proper uptake of sa is that its application in some fields might appear under other titles for example a recent article in nature adam 2020 refers to modeling activities in which thousands of versions of the model are run with a range of assumptions inputs to provide a spread of scenarios with different probabilities as ensemble modeling such activities however are typically considered as uq and possibly sa in the environmental modeling community sa type activities are also seen under the title single model perturbed physics ensembles in the climate modeling community bellprat et al 2012 confusingly the expression climate simulation ensemble is more often used to indicate the case where different models for example developed by different teams are applied to the same problem donev et al 2016 ipcc 2016 see a critical discussion in saltelli stark et al 2015 the risk of these diverging nomenclatures is that research advances made in uq and sa may go unnoticed by some communities 3 1 2 address possible inconsistencies in sa different sa methods are based on fundamentally different philosophies and therefore can result in different sometimes conflicting results for the same problem tang et al 2007 razavi and gupta 2015 this inherent nature of the state of the art is unlike many other fields for example while the field of optimization contains a vast variety of approaches methods and applications all these may boil down to a common well defined philosophy that is to find an optimal solution to a formulated problem given certain objective functions and constraints maier et al 2014 maier et al 2019 in other words an optimal solution to a given problem formulation would remain optimal regardless of the optimization method used whereas according to current theory the sensitivity assessment for a problem might appear to be quite different depending on the sa method used for improved consistency two key questions need to be answered before choosing an sa method and carrying out the analysis 1 why do i need to run sa for a given problem and what is the underlying question that sa is expected to answer and 2 how should i design the sa experiment to address that underlying question thought out answers are critical as otherwise most users tend to use methods developed in their own camp and are therefore most comfortable with rather than methods that are most suitable for the purpose and problem at hand thus focusing on the purpose would facilitate a shift in method selection principles from legacy to adequacy addor and melsen 2019 we emphasize that we do not necessarily advocate the reconciliation of different philosophies and methods but are pointing out that because sa addresses multiple related problems it must be made clear why a particular method or set of methods is the right match for a given research question in addition to the purpose and chosen method s sa researchers and practitioners generally need to be more mindful of the subjective but often overlooked decisions they make in the configuration of a method the sensitivity of sa to such decisions for example sa algorithm parameters may be quite significant for some methods and ignoring it might result in questionable results the significance of this issue has been discussed recently by haghnegahdar and razavi 2017 in the context of derivative based methods e g morris 1991 and by puy et al 2020b in the context of the pawn method pianosi and wagener 2015 3 1 3 teach sa more broadly and consistently formalizing a structured generalized and standardized sa discipline is attainable in a foreseeable future central towards this goal is to invest in systematic and coherent teaching of sa to students across disciplines who will become the next generation of researchers and practitioners sa is currently taught on an ad hoc basis mostly via small workshops tailored to specific aspects of sa or as a part of courses related to systems analysis or uncertainty quantification perhaps the most formal and systematic effort to teach sa has been a summer school on sa run by the european commission s joint research centre held ten times between 1999 and 2018 sa needs to become an independent but integral part of the curriculum across the relevant disciplines alongside other topics such as optimization and validation verification and uncertainty quantification vvuq see e g national research council 2012 new interdisciplinary graduate courses need to be developed to comprehensively cover sa and to teach and promote best sa practice as discussed in the following sections the sa discipline has extensive untapped potential for a variety of problems and applications 3 2 untapped potential of sa for mathematical modeling historically the majority of formal sa applications have been directed towards mathematical models to better understand how they work and diagnose their deficiencies and to contribute to their calibration and verification saltelli et al 2000 in this context a dominant application of sa is for parameter screening to support model calibration by identifying and fixing non influential parameters there is potential however for sa to further address several challenges in mathematical modeling through advancements in the management of uncertainty assessment of model quality through testing and diagnostics and tackling non identifiability and model reduction for example mathematical modeling could benefit from structure and standards based on statistical principles saltelli 2019 including a systemic appraisal of model uncertainties and sensitivities in the following we outline the potential of sa to advance mathematical modeling 3 2 1 management of uncertainty management of uncertainty through its characterization and attribution should be at the heart of the scientific method and a fortiori in the use of science for policy funtowicz and ravetz 1990 the problem of uncertainty management is core to the modeling craft and should be an integral part of any model development and evaluation exercise jakeman et al 2006 eker et al 2018 while sa has significant potential its application often does not adequately map the uncertainty in model inference in a recent five point manifesto for responsible modeling global sa is invoked as essential to the task of mapping the uncertainty in every model assumption saltelli et al 2020 a major step forward in mathematical modeling will be to better evaluate uncertainty in model predictions and to trace that uncertainty back to its sources across the model components parameters and inputs sa is uniquely positioned to do so as it can help to decompose the prediction uncertainty and attribute it to the individual factors and their interactions basically sa can help answer a critical question razavi et al 2019 when and how does uncertainty matter a major but almost totally neglected issue in mathematical modeling is that while models are becoming more and more complex they are treated more and more like a black box even by model developers themselves in real world applications those models tend to be used without much attention to their complicated internal mechanics and not always justified assumptions a manifestation of this issue is the fact that many modern physically based models include countless numbers of hard coded parameters e g see mendoza et al 2015 supported by the rationale explicit or implicit that scientists can characterize those parameters with absolute certainty such practice can render progressive initiatives on open science vicente saez and martínez fuentes 2018 and open modeling e g openmod 2020 less effective sa is much needed to prize open and cast light into these black boxes and to illuminate the dominant sources of uncertainty furthermore the quality of both statistical and mechanistic models struggles with common issues when dealing with uncertainty in statistics the p test can be misused to overestimate the probability of having found a true effect colquhoun 2014 stark and saltelli 2018 likewise certainty may be overestimated in modeling studies nearing and gupta 2018 thus producing unreasonably precise estimates even in situations of pervasive uncertainty or ignorance saltelli et al 2015 thompson and smith 2019 including in important cases where science needs to inform policy pilkey and pilkey jarvis 2007 it is an old refrain in mathematical modeling that since models are often over parameterized with respect to the information that can be extracted from the available data it can sometimes appear that they can be made to conclude anything we choose hornberger and spear 1981 a nalogous to under and over fitting issues in statistical models mechanistic model development suffers from a trade off between model completeness and propagation error saltelli 2019 see fig 3 the former refers to the adequacy of a model in terms of for instance how many aspects of the underlying system are included in the model the latter also known as the uncertainty cascade effect christie et al 2011 refers to the notion that adding each new aspect to the model for example a new parameter which itself is uncertain potentially increases the overall uncertainty in the output such tradeoffs challenge model developers to calibrate the right level of complexity in the construction of models sa can facilitate this process by characterizing and attributing the contributions to overall model error so as to identify the sweet spot where uncertainty attains a minimum as a simple example consider the case where the uncertainty of concern is with respect to a sought measure of predictive model error around an observational quantity of interest the sweet spot would be where no significant improvement in model performance occurs by adding more parameters within a given model structure where significance corresponds to a level commensurate with the errors noise in the observations of interest e g see jakeman and hornberger 1993 3 2 2 diagnostic testing and model verification sa has significant potential to help in diagnosing the behavior of a mathematical model and for assessing how plausibly the model mimics the system under study for the given application this capability provides understanding of how a model works and points to the parts of a model that are deficient a key to this end is to properly frame the sa problem and articulate that understanding the sensitivity of what to what matters for this purpose as outlined in gupta and razavi 2018 the former what may be chosen from any of the following categories a one or more model performance metrics that quantify the goodness of fit of the model responses to observed data e g mean squared errors b a specific targeted aspect of those responses e g extremes or percentiles such as peak flows in a hydrologic model c a compressed set of properties that characterize those responses e g hydrologic signatures such as runoff ratio or d the entire spatio temporally varying trajectory of responses themselves the latter what may include continuous or discrete variables describing model parameters forcings and structural assumptions to diagnostically test a model one may compare sa results with expert knowledge on how the underlying system being modeled works example studies based on time varying and time aggregate sa results include wagener et al 2003 herman et al 2013 haghnegahdar et al 2017 and razavi and gupta 2019 moreover the recent emergence of given data sa methods will provide unprecedented opportunities for model diagnostic testing as they can directly be applied to observed data as well in addition to the mathematical models plischke et al 2013 sheikholeslami and razavi 2020 the knowledge gained via sa can be documented in a model s user guide to help practitioners configure and parameterize the model more effectively diagnostic evaluation of models in this manner is analogous to property based testing claessen and hughes 2000 wherein the logical properties of model behavior are evaluated according to expected behavior failure of a model to conform to expected behavior falsifies the assumption that the model is correctly implemented in addition a study using mathematical models may face a diversity of errors and subjectivities these may stem from process conceptualization and mathematical representation parameterization inputs and boundary conditions discretization choices in space and time numerical solvers and software coding up to and including the framing and biases of the modelers themselves oreskes 2000 iwanaga et al 2021 modelers however do not subscribe to a unified reliable and agreed on code of good practices for testing their models and the quality of the inference that they produce more work is needed to develop testing strategies based on sa that cover the diversity of subjective factors involved in the process of model development e g peeters 2017 3 2 3 non identifiability and model reduction most models are poorly identifiable largely because of over parameterization relative to the data and information available see guillaume et al 2019 for an overview of identifiability the assessment of model appropriateness for a purpose requires understanding of its identifiability the sources of any non identifiability and the influence of any non identifiability on the model guillaume et al 2019 sa and identifiability analysis ia are different but complementary primarily because sa is about the properties of a model itself while ia is more about model properties with respect to observed data it can be shown that an insensitive parameter is non identifiable but the converse is not necessarily true that is a sensitive parameter may or may not be identifiable therefore sa can help in part to recognize the non identifiable components of a model knowledge of identifiability can be used to simplify a model structure by fixing or combining parameters that on their own are ineffective in influencing model outputs model reduction however should be done with caution as a parameter that seems non influential under a particular condition might become quite influential under a new condition e g see tonkin and doherty 2009 for example a snowmelt parameter in a hydrologic model has no influence in time periods without snow whereas it becomes dominantly influential in snowmelt seasons in such cases fixing the parameter will limit the agility and therefore the fidelity of the model in mimicking the underlying system also fixing parameters that have small sensitivity indices may result in model variations that cannot be explained in the lower dimensional space hart and gremau 2019 3 2 4 the reproducibility crisis and sa the challenges of modeling need to be seen in the broader context of the so called reproducibility crisis saltelli and funtowicz 2017 saltelli 2018 where misuse or abuse of statistics stark and saltelli 2018 gigerenzer and marewski 2015 leek et al 2017 singh chawla 2017 wasserstein and lazar 2016 gigerenzer 2018 is often cited as the root cause of the crisis ioannidis 2005 current non reproducible science is ecologically fit to the existing science governance arrangements smaldino and mcelreath 2016 including its publish or perish culture banobi et al 2011 and is resistent to reform chalmers and glasziou 2009 edwards and roy 2017 in the field of clinical medical research for instance the percentage of non reproducible studies may be as high as 85 gigerenzer and marewski 2015 the field of mathematical and computational modeling has started grappling with the reproducibility crisis as well hutton et al 2016 saltelli 2019 saltelli bammer et al 2020 development of research specific software is at the core of modern modeling efforts most modelers however are not formally taught software development practices hannay et al 2009 such that models are rarely designed and developed in a manner that supports further use or reuse beyond its original research specific context the consequent lack of accessible code and data then feeds into issues of reproducibility hutton et al 2016 hut et al 2017 as alluded to above the publish or perish culture limits the recognition researchers receive for developing and maintaining long lived software associated data and supporting documentation that underpins reproducibility in some cases code and data may not be accessible at all even after contacting authors stodden et al 2018 one observation is that researchers want to perform research not write software crouch et al 2013 sletholt et al 2012 that said there is increasing recognition of the importance and benefits of supporting open and accessible research software support of initiatives towards improving computational reproducibility has been growing e g ahalt et al 2014 crouch et al 2013 culminating recently with the fair findable accessible interoperable reusable principles for open and accessible data management wilkinson et al 2016 some journals now award open code badges kidwell et al 2016 to highlight publications with accessible code and data sa and its practitioners can contribute to addressing the aspects of this crisis which directly affect mathematical modeling reproducible model based studies need the kind of transparency that sa can offer by way of making explicit the conditionality of model based inference as well as the conditionality of the associated uncertainties essential to this end is to standardize and promote best sa practice along with the development of sa related software that can easily be coupled with any model there has been an increasing number of open software packages which democratize both common and experimental sa techniques and applications a non exhaustive list is provided in section 2 3 3 computational aspects and robustness of sa algorithms computational burden has been a major hindrance to the application of modern sa methods to real world problems many of the existing sa methods have remained under utilized in various disciplines as they require a large sample size particularly for models with higher dimensional spaces state of the art spatially distributed models are typically computationally demanding themselves and take minutes hours or even days for a single run although the growth of computing resources is making the application of current algorithms to existing problems more affordable see e g prieur et al 2019 not everyone has access to powerful computing and there will always be modeling problems where computing power will not be quite enough for existing algorithms for example most i e 70 sa applications in earth and environmental systems modeling have been limited to low dimensional models i e with 20 or fewer factors involved sheikholeslami et al 2020 whereas there are abundant applications with models that can have up to hundreds e g 900 in borgonovo and smith 2011 or even thousands e g 40 000 in lu et al 2020 of parameters in the machine learning context the number of model parameters can reach millions e g bert houlsby et al 2019 even trillions of parameters e g zero rajbhandari et al 2020 the application of sa with machine learning is further complicated because of the fundamental differences between machine learning and other types of models see section 3 4 computational obstacles need to be properly assessed and addressed so that sa can be applied to any model particularly those whose results are of immediate significance to society inadequate or non existent application of sa leads to models for which society cannot characterize their confidence modeling is a social activity and the acceptance of model prescriptions regarding for example addressing an industrial risk financial crisis hurricane or a pandemic calls for mutual trust between model developers and end users saltelli et al 2020 sa can help with building this trust by providing insights into the internal functioning of models the future therefore needs new generations of algorithms to keep pace with the ever increasing complexity and dimensionality of the state of the art models building on known theoretical and empirical convergence rates for many sampling based approaches further theory could identify fundamental limits on existing classes of algorithms to help identify breakthroughs required 3 3 1 essential definitions and components a complete assessment of the computational performance of any sa algorithm must be conducted across four aspects efficiency convergence reliability and robustness efficiency refers to the amount of time number of computations required to perform sa and is often assessed by the number of model runs i e sample size required for the algorithm to converge to some specified level convergence of an sa algorithm is non trivial to assess as the answer typically cannot be preordained from theory it depends on several factors including the model type and its complexity the overall objective of the sa e g prioritization where sample size can generally be smaller versus screening the sa method itself definition of convergence and level of certainty required choice of time period for the input forcing variables and the width of parameter ranges and distribution sampled see shin et al 2013 reliability refers to any measure of correctness of sa results and its accurate assessment requires the availability of the true sa results reliability of an algorithm may only be assessed when the model is simple e g simple algebraic test functions or the true results are somehow given robustness often used in lieu of reliability measures how consistent an algorithm performance remains when the sample points and algorithm parameters change for example an sa algorithm is robust to sampling variability if its performance remains almost identical when applied on two different sample sets taken from the same model in cases where running multiple replicates of the same experiment is not possible bootstrapping efron 1987 is often used with sa algorithms to estimate robustness in the form of uncertainty distributions on sensitivity indices without requiring additional model evaluations addressing computational challenges requires a proper understanding of the design functioning and interaction of the three general components of any sa algorithm these components are 1 the experimental design that employs a sampling strategy to select sample points in the factor space of interest 2 the function evaluation procedure to run the model collect and store sampled model responses i e obtained by running a model many times on a spatio temporal and or other domains and 3 the integration mechanism to numerically integrate the sampled data to estimate sensitivity indices the choice of experimental design is often dictated by the integration mechanism of the sa algorithm of interest for example in the method of morris 1991 the mechanism that integrates the elementary effects across the factor space requires sample points to be taken equally spaced from each other by a given distance by changing one factor at a time as a result a sample taken for one sa algorithm may not be useable by another algorithm or possibly for other purposes the function evaluation procedure is typically the most computationally intensive component of sa this is not only because of the computational burden of running the models themselves but also the overhead for storing retrieving and manipulating the model responses on high resolution domains e g spatio temporal the following sections outline possible progress in addressing computational challenges with respect to the above three components 3 3 2 experimental design and integration improving the efficiency of sa is tied to improving experimental designs in conjunction with integration mechanisms for example consider that global sensitivity measures are commonly written as an average over the distribution of the input of interest xi of an inner statistic borgonovo et al 2016 the brute force application of the definition of global importance measures would lead to an estimation cost of c n n k where n is the number of runs from the distribution of input xi n is the number of runs needed for the inner statistic and k is the number of model inputs if n 1000 n 1000 and k 10 we are already at 10 000 000 model runs this cost can be reduced to c n k 2 by using the sampling and integration mechanism of saltelli 2002 to estimate first and total order variance based sensitivity indices a recently developed approach to sampling and integration is to extract information contained in all pairs of sample points rather than the individual points this is useful because the number of pairs 2 combinations grows quadratically n2 2 with the sample size n razavi and gupta 2016a for example if n 1000 we get 499 500 pairs but doubling the sample size to n 2000 results in a fourfold increase to 1 999 000 pairs razavi and gupta 2016b becker 2020 and puy et al 2020a have shown the efficiency of this approach in estimating variance based total order effects through the method variogram analysis response surfaces vars alternatively the future of sa may step more towards sampling free algorithms that can work on any given data see e g plischke 2010 plischke et al 2013 pianosi and wagener 2018 sheikholeslami and razavi 2020 such approaches may be referred to as given data sensitivity analysis or alternatively green sensitivity analysis in that they can recycle available samples for example from previous model runs allowing for samples to be incrementally obtained and thereby avoiding the squandering of computational budget the computational cost of the corresponding estimators is then n model evaluations in addition to being green some of these approaches tend to be computationally much more efficient than other methods and in certain cases produce robust sa estimates with a very small sample size sheikholeslami and razavi 2020 notably most algorithms for given data sa involve a parameter tuning step that may be non trivial with a bias variance compromise perspective which is to avoid both over fitting and over smoothing examples of such parameters include the bandwidth for kernel regression based estimators the number of leaves for random forest based estimators the truncation argument for spectral procedures etc bootstrapping efron 1987 may be used for the selection of such parameters see e g heredia et al 2020 but it may consume an excessive amount of computation time adaptive selection of these parameters or developing parameter independent algorithms is a challenging issue that needs to be addressed in future more recently authors have proposed parameter estimation procedures based on nearest neighbors broto 2020 rank statistics gamboa et al 2020 and robustness based optimization sheikholeslami and razavi 2020 these methods are still relatively new and need to be tested across a range of problems with different dimensionalities see e g puy et al 2020a above all convergence considerations need to be at the heart of the development and application of any sa algorithms recently developed convergence criteria e g sarrazin et al 2016 sheikholeslami et al 2020 and stopping criteria see e g gilquin et al 2020 rugama et al 2018 can be useful in this regard in general the literature is replete with studies that indicate convergence for a particular model or function and some particular instances of the above other factors but these offer only limited guidance consequently many users usually choose the computational budget i e the number of model runs for sa on an ad hoc basis rather than on convergence reliability and robustness considerations rather than relying on guidance from past studies analysts should be encouraged to adopt methods and software packages that explicitly address this issue bootstrapped estimates of sensitivity indices are for example now common and enabled by default in r and python packages sa users should assess convergence rates as the sample size increases based on intermediate results but a typical hindrance is that many sampling strategies involve one stage sampling that generates the entire set of sample points at once requiring the user to specify the sample size a priori sheikholeslami and razavi 2017 this is a disadvantage as it is unlikely for users to know the optimal sample size that enables the algorithm to converge to robust results therefore there is a need for sequential or multi stage sampling strategies such as sobol sequences sobol 1967 sobol et al 2011 and progressive latin hypercube sampling plhs sheikholeslami and razavi 2017 that enlarge the sample size during the course of sa while preserving the distributional properties of interest the superior performance of sobol low discrepancy sequences over random sampling has been demonstrated in several studies gilquin et al 2020 gilquin et al 2017 sheikholeslami et al 2017 rugama et al 2018 kucherenko et al 2011 kucherenko et al 2015 furthermore as the value of sa for high resolution e g spatio temporal model outputs is increasingly recognized innovative strategies to handle the increased storage and retrieval overhead are needed currently all model runs are typically stored first requiring excessive storage capacity for large models and sensitivity indices are computed post hoc future developments similar to jakeman et al 2020 and terraz et al 2017 can helpfully include sa algorithms that merge function evaluation and integration mechanisms such that sensitivity indices are updated as new results are made available 3 3 3 function evaluations much attention has been geared towards the function evaluation procedure under the umbrella of surrogate modeling surrogate models also called response surface models metamodels or emulators are used in lieu of computationally intensive models and can be statistical i e response surface surrogates or mechanistic i e lower fidelity mechanistic surrogates razavi et al 2012 sa methods based on response surface surrogates build approximations such as polynomial chaos expansions xiu and karniadakis 2002 q rs hdmr zuniga et al 2013 and gaussian process kriging rasmussen and williams 2005 using a limited number of expensive model evaluations once built sensitivity measures can be estimated by sampling the surrogate instead of the original model at negligible cost or in some cases can be estimated analytically sudret 2008 marrel et al 2009 due to the sheer computational expense of some models building an accurate surrogate using only data from the most trusted numerical model can be challenging to reduce the computational burden of building surrogates multi fidelity methods combine limited high fidelity data with larger amounts of lower fidelity data coming from models with reduced physics or coarser numerical discretization sa methods using multi fidelity approximations can produce sensitivity estimates that converge to high fidelity estimates but do so at a fraction of the cost palar et al 2018 moreover these methods can build upon the aforementioned advances made for single fidelity models and adaptively allocate samples to resolve uncertainties and sensitivities jakeman et al 2020 the use of surrogate modeling introduces a new challenge accounting for the uncertainty arising from the surrogate model itself e g model error combined with the errors of the estimation procedure while progress has been made in the assessment of surrogate modeling uncertainty see e g jones 2001 oakley and o hagan 2004 sóbester et al 2005 razavi et al 2012 janon et al 2014a and 2014b and some bayesian approaches are already capable of incorporating this uncertainty into posterior distributions of sensitivity measures oakley and o hagan 2004 gramacy and taddy 2010 further advancements to properly incorporate such uncertainties in sa estimates and respective confidence intervals are likely surrogate modeling strategies particularly those based on response surfaces become less effective in high dimensional problems razavi et al 2012 to address limitations related to high dimensionality adaptive and goal oriented buzzard 2012 jakeman et al 2020 approaches can be used these approaches can allocate samples to lower dimensional subspaces in a manner that addresses the curse of dimensionality and results in enormous computational gains lastly an issue hindering the application of sa to large complex models is that some models may fail to run properly crash at particular points in the factor space and not produce a response simulation failures mainly occur due to non robust numerical implementations the violation of numerical stability conditions or errors in programming sa algorithms are typically ill equipped to deal with such failures as they require running models under many configurations of factors in addition to improving properties of the original models e g kavetski and clark 2010 more research is needed to equip sa algorithms to handle model failures which is becoming a more pressing issue as the complexity of mathematical models grows one of the very first studies addressing this issue in the context of sa is sheikholeslami et al 2019 where a surrogate modeling strategy is used to fill in model output values when the original model fails to handle this issue strategies can also be adopted from other types of analyses for example bachoc et al 2016 used a design of experiments strategy to detect computation failures and code instabilities and bachoc et al 2020b developed a method to classify model parameters to computation failure or success groups during optimization 3 4 sa and machine learning machine learning ml has achieved unprecedented performance in complex tasks typically performed by humans such as image classification krizhevsky et al 2017 natural language processing yang et al 2018 and gaming silver et al 2018 this success combined with the growth in computational power and the increasing availability of big data has motivated the application of ml to a wide range of problems across many disciplines including the earth sciences reichstein et al 2019 robotics torresen 2018 medicine hosny et al 2018 and finance lee et al 2019 research and development with ml are now viewed as a major avenue forward by many industrial sectors such as energy security cyber security transport defence aeronautics and aerospace avsi 2020 deep learning dl has emerged in recent years as a leading ml approach for a wide variety of regression and classification applications goodfellow et al 2016 dl is a newly formalized term that refers to the way artificial neural networks anns with more than one hidden layer learn representations from data with a rich and long history dating back to the 1980s e g rumelhart et al 1986 hornik et al 1989 anns have become perhaps the most popular tool for ml therefore major portions of this section are primarily focused on anns a critical challenge facing ml particularly dl applications is their typical lack of interpretability and explainability these two terms usually used interchangeably in the literature refer to the ability of a model developer to make sense of why the model functions the way it does and to explain that to a user rudin 2019 samek and müller 2019 roscher et al 2020 in many real world applications the acceptance and use of an ml model s outputs require an explanation of why and how the model works in addition transparency and auditing of ml models can raise legal issues nowadays rudin 2019 especially when personal data are involved what complicates this further is that ml is reliant on processes that infer correlation rather than causation obermeyer and emanuel 2016 sa can offer new opportunities for the development and application of ml these opportunities are rooted in the fact that sa and ml in many cases look at the same problem via two different approaches in fact a goal of ml in most application areas especially in environmental modeling is to construct a function that maps variables in an input space to those in an output space hastie et al 2009 razavi and tolson 2011 generally such functions are purely data driven not accounting for any underlying processes physical or otherwise similarly sa looks at the relationship between the inputs and outputs but instead of constructing a mapping function it estimates the relational strength between each single or group of inputs and the outputs via different sensitivity indices such informal commonalities between tools for sa and ml provide significant potential for each field to benefit from the other in exploring the potential however one must be mindful of the central differences that exist between computer experiments that provide data for sa and more general experiments including those in laboratories or in fields which provide data for ml these differences are as follows 1 computer experiments are usually deterministic with the exception of stochastic models such as agent based simulators whereas real world data commonly used in ml are usually polluted with observational errors often with unknown properties 2 the linkage between the input and output variables in computer experiments is generally via hypothesized causal relationships while this is not necessarily the case in other types of experiments 3 in ml applications users typically need to have access to very large data sets but this is typically not possible in the computer modeling context where physical data acquisition such as for model verification may be very expensive 4 in computer experiments users have full control over the experimental design and the way a sample is taken whereas this is usually not the case in other types of experiments the following sub sections explain how the fields of sa and ml already have and can continue to cross fertilize 3 4 1 feature and structure selection in ml sa can support feature ranking and selection where the term feature is equivalent to the term factor in common sa literature the objective is to find the dependency strength between features and targeted labels to enable the user to choose the features that best explain and possibly predict the output of interest sa can be used prior to ml model design and training to choose only the features that are most statistically associated with the output data galelli et al 2014 the classic non sa approaches to do so include the standard statistical correlation metrics e g pearson correlation coefficient spearman s rank correlation coefficient and kendall s tau information theoretic metrics e g entropy mutual information and dissimilarity measures and advanced dependence measures such as distance covariance and hilbert schmidt independence criterion da veiga 2015 these classic approaches can be complemented by the advanced sa techniques that work directly on sample data in the absence of any model for example the recently developed given data sa paradigm plischke et al 2013 sheikholeslami and razavi 2020 can be used on data available for ml to rank features according to their relational strength with the output of interest another approach could be to use the target and conditional sa raguet and marrel 2018 that enhances feature selection when the underlying phenomenon is under represented in the dataset e g unbalanced datasets associated with the prediction of rare or extreme output events special care however needs to be taken in sa on training data because sample data on features are by and large real world data or properties thereof typically having undefined distributions and unclear correlation structures and spatial dependencies sa can also be applied to an ml model after training to identify the controlling features and how they interact to generate the model output a simple way to do so is local sa examples include the use of one factor at a time sa e g lek et al 1995 1996 maier and dandy 1997 liong et al 2000 and the calculation of partial derivatives of the model outputs in response to changes in the model inputs e g dimopoulos et al 1995 1999 tison et al 2007 vasilakos et al 2009 mount et al 2013 such assessments can also be expanded following the concepts of global sa for example importance score methods for feature ranking in ml based on permutation and resampling have informal roots in sa and strong connections with sobol sensitivity theory examples of such methods with anns and random forests include breiman 2001 lakshmanan et al 2015 gregorutti 2015 wei et al 2015 and benoumechiara 2019 more formally sa has been used to rank features in anns and random forests according to their importance in explaining the variation in outputs fock 2014 fernández navarro 2017 zhang 2019 thus sa can point to the most influential features learned by an ml model its most active parts and detect interactions between features lundberg and lee 2017 lundberg et al 2020 ribeiro et al 2016 štrumbelj and kononenko 2014 in this way sa can potentially enable the identification of optimal levels of structural complexity of ml models which is particularly useful in designing deep learning constructions for example efast and random balance design have been used to prune redundant neurons in anns lauret et al 2006 li and chen 2018 3 4 2 interpretability and explainability of ml despite being very successful ml has been criticized for being a black box where the reasons for an answer are unknown this challenge may offset the value of ml in a range of applications particularly where researchers and decision makers seek transparency sa can help in peering inside the ml to improve its explainability and interpretability e g lundberg and lee 2017 the goal here is to produce explanations that are intelligible and meaningful to end users which aid in improving transparency and building trust help in identifying the best ml model among several comparably performing models and enable diagnosis of model errors samek et al 2019 a significant portion of efforts in the literature to provide explanations has been based on the assessment of feature importance for developed ml models as described in section 3 4 1 thus sa offers new opportunities to provide insights into the general behavior of a model by highlighting how the different features influence the model output such insights are particularly important for the structural validation of ml models humphrey et al 2017 if those models are not structurally valid their behavioural response to different input stimuli can be erratic and counter to physical system understanding making them difficult to apply in practice with confidence see wu et al 2014 for a discussion validann humphrey et al 2017 is an example software package using sa for this purpose in the assessment of feature importance sa has to deal with two challenges often encountered in ml as is the case in some other types of modeling the often high dimensionality of the feature space and multicollinearity dependencies between those features these challenges are discussed in sections 3 5 3 and 3 5 4 in addition to assessing the sensitivities to the features sa in principle has potential to be applied to any part of ml models including their structure e g the number of layers and neurons in anns and parameters e g weights and biases in anns such practice however can be hampered by fundamental differences between standard sa applications to mechanistic models and those to ml models these differences as outlined below necessitate further research to develop sa strategies particularly tailored for ml first unlike mechanistic models the structure of many ml models particularly anns is based on the notion of connectionism meaning their internal operations are massively parallelized for example in a mechanistic hydrologic model the soil parameterization equation may be solely responsible for representing how soil columns store and release water while other parts of the model may be in charge of other physical processes in the case of an ann based hydrologic model however one may not be able to single out what neuron or group of neurons is responsible for representing the same soil processes in fact if one re trains that ann model with a different parameter initialization a wholly separate group of neurons might end up being responsible for the soil processes second the statistical properties of the parameters of ml models e g weights are usually not process informed mount et al 2016 in the case of sa of mechanistic models the inputs considered are typically model parameters sampled by an experimental design with known statistical properties defined by the user however in the ml context this is not easily doable and for example it is non trivial to assign a range to the weights of an ann for a given problem see kingston et al 2005a razavi and tolson 2011 in general the value of sa for providing insight into and extracting knowledge from ml models can be improved significantly by using state of the art model development practices maier et al 2010 wu et al 2014 that improve parameter identifiability guillaume et al 2019 such as input variable selection see galelli et al 2014 and model structure selection kingston et al 2008 and by accounting for physical plausibility kingston et al 2005b and parameter uncertainty kingston et al 2005a 2006 explicitly during the model calibration process sa can also support interpretability and explainability of ml in the context of classification in this context a major problem is with examining and explaining the robustness of decision boundaries for classification with respect to data and or model hypotheses sa can provide insights into the robustness with respect to the specification of input distributions for example bachoc et al 2020a applied a sensitivity index developed by lemaître et al 2015 for robustness analysis of decision boundaries in classification moreover sa is useful to provide explanations in the context of classification robustness of classification for example is subject to the decision boundaries that can be identified through ml the decision boundaries may for example be sensitive to the distribution of inputs and or model hypotheses specific sa methods have been successfully applied to explain the influential factors on decision boundaries and robustness analysis e g lemaître et al 2015 sueur et al 2017 bachoc et al 2020a gauchy et al 2020 sa can also identify influential inputs regarding the occurrence of critical events which are important in the robustness assessment of decision boundaries raguet and marrel 2018 spagnol et al 2019 marrel and chabridon 2020 molnar 2019 3 4 3 ml powered sa progress in ml undoubtedly provides fertile ground for new ideas in sa most notably the ml capability to provide efficient data driven function approximation has provided tremendous opportunities for surrogate modeling in the context of sa when computer experiments are intensive example methods arising from ml that have been used in sa include gaussian processes rasmussen 2004 yang et al 2018 generalized polynomial chaos expansions sudret 2008 reduced basis methods hesthaven et al 2016 and anns beh et al 2017 see section 3 3 3 for more on this subject moreover dependence measures and kernel based indices used in ml gretton et al 2005 have been introduced to the sa community by da veiga 2015 and further extended by de lozzo and marrel 2016 especially in regard to the hilbert schmidt independence criterion hsic which detects features that are non influential on an output of interest for screening purposes real world applications that nowadays use these sa methods include nuclear safety iooss and marrel 2019 marrel and chabridon 2020 the use of shapley values shapley 1953 to develop importance measures being able to deal with dependent inputs features have emerged recently but independently in the sa owen 2014 and ml lundberg et al 2017 communities cross fertilization of ideas between ml and sa is expected to continue and grow over time broto et al 2020 mase et al 2020 hoyt and owen 2020 3 5 sa and uncertainty quantification sa in the context of uncertainty quantification uq has a long tradition stemming back to works such as bier 1982 in which global sensitivity measures were introduced to identify the key drivers of uncertainty in complex risk assessment problems since then there has been a growing synergy between uq and sa generally uq is the science of quantitative characterization and reduction of the uncertainty regarding a certain outcome of a system or model while sa for uq is focused on identifying the dominant controls of that uncertainty for brevity we do not summarize this rich history referring to saltelli et al 2008 sullivan 2015 borgonovo 2017 and to the handbook of uncertainty quantification by ghanem et al 2017 despite significant advances sa for uq still faces a number of challenges these include possible misconceptions in framing an sa problem for a uq purpose incompatibility of some sa frameworks for some model types complications with handling multivariate and correlated input spaces sensitivity of uq to problem setup and uncertainty in the sa results themselves in the following we explain these challenges and possible ways to address them 3 5 1 mind the goal of uq with respect to sa when sa is used in a uq application the underlying purpose of uq should dictate the framing of the sa problem and the method used in general there are two types of uq inverse uq and forward uq the former aims to estimate unknown model parameters from data while the latter propagates input uncertainties through a model to estimate output uncertainty in the case of inverse uq sa should be used to identify the parameters most informed by data for example by looking at the sensitivity of the misfit between the data and model predictions see section 3 2 3 in the case of forward uq however one needs to identify the factors that influence the prediction the most for a discussion refer to gupta and razavi 2018 and butler et al 2020 there is an implicit possibly flawed assumption in many applications of sa that the direction in factor space informed by data is parallel to the direction which informs predictions this assumption can yield misleading results as those two directions can often be orthogonal for example fixing parameters identified as non influential by sa in the inverse uq setting can lead to significant underestimation of prediction uncertainty this is because those parameters while being the largest source of uncertainty have been ignored in forward uq in most cases if the identification of a parameter is informed by data the uncertainty around it will decrease in the context of uq a comprehensive sa practice is one that identifies both of those important directions with such a practice the utility of sa for uq can be greatly improved as it would allow for the efficient estimation of uncertain parameters and quantify predictive uncertainty simultaneously to do so for a given problem sa needs to be applied in the two different settings independently one to assess the sensitivity of a goodness of fit metric to the factors and the other to assess the sensitivity of the predicted quantity itself gupta and razavi 2018 shin et al 2013 3 5 2 no single method for all model types sa can be used for a wide variety of model types for example those expressed in the form of partial differential equations pdes such as contaminant transport models e g wei et al 2014 those that are linked to the solution of an optimization problem e g dice lamontagne et al 2019 or stocfor3 lu et al 2020 or those that are in an agent based form fadikar et al 2018 different model types are engineered in different ways and this challenge demands systematic research that avoids encouraging an apply the same hammer attitude with respect to methods for example sa enabled with response surface surrogates see section 3 3 3 can be useful for lower dimensional problems with smooth parameter output maps while sampling based approaches can be more useful for non smooth models and higher dimensions becker 2020 moreover to date most sa methods have been developed for deterministic models that is the same input always produces the same output while little attention has been given to models with stochastic responses such as can occur with agent based models while the majority of sa applications have been to assess parametric variations in agent based models lee et al 2015 an open research question is how to include alternative agent based elements in a comprehensive sa so that one can assess sensitivity of the response to changes in the rule of an agent simultaneously with changes in a parameter moreover this question should be expanded to models in general where there is a challenge to jointly consider changes in model structure and parameters the exploration of global sa for optimization is a subject of recent research spagnol et al 2019 similarly optimization problems may call for the use of information theory based methods in fact early works such as avriel and williams 1970 show that the information value is a natural sensitivity measure when a decision support model is cast in the form of an optimization problem similarly felli and hazen 1999 oakley 2009 and strong and oakley 2013 suggest using the information value as a sensitivity measure to explicitly compare decision alternatives recently borgonovo et al 2021 discuss the conditions under which global sensitivity measures can be interpreted as information value thus the most important input is also the input that is most informative for the decision problem at hand for classification tools for low dimensional visualization of high dimensional data e g van der maaten and hinton 2008 could be explored for their useability within sa for stochastic models we note that the literature in the management sciences has addressed the problem intensively rubinstein 1989 hong 2009 hong and liu 2009 and investigators in other disciplines might benefit from those results 3 5 3 multivariate and correlated input spaces one of the most critical assumptions decisions in uq is the choice of the multivariate distributional properties of uncertain input variables which are propagated through the model in practice the marginal probability density functions pdfs of the inputs are obtained via various means such as direct measurements statistical inference design or operation rules and expert judgment and can be accompanied by an estimated level of accuracy or confidence in addition uq problems often come with certain constraints on the input for example when the input space is non rectangular and or when the inputs are dependent in such cases some sa methods become handicapped such as when the functional anova expansion becomes ill posed owen and prieur 2017 in general improper multivariate distributional properties including the correlation structure among inputs may lead to wrong inferences even if the most appropriate sa method is used do and razavi 2020 the field of sa in terms of methods to handle input constraints and correlation structures is still embryonic of the very few studies available one may refer to the work of kucherenko et al 2017 for non rectangular input spaces and to kucherenko et al 2012 tarantola and mara 2017 and do and razavi 2020 for correlated input spaces promising methods seem also to be moment independent methods borgonovo 2007 dependence measures da veiga 2015 de lozzo and marrel 2016 and shapley values owen and prieur 2017 whose definitions remain well posed in the presence of input constraints nonetheless the presence of constraints also impacts other aspects of sa such as the interpretation of interactions and the assessment of direction of change for these aspects also further research is needed to identify the most appropriate methods 3 5 4 curse of dimensionality the state of the art models that are often encountered in uq problems are commonly associated with high dimensionality and significant computational burden as discussed in section 3 3 higher dimensionality exacerbates the difficulty of assigning multivariate distributions to uncertain inputs as discussed in the previous section a second difficulty is with anova type expansions where the number of interaction terms is exponential in the number of inputs such cases require excessively large sample sizes often becoming computationally prohibitive a third difficulty is with the sampling strategies themselves in high dimensional spaces many modern sampling strategies optimize the way samples are taken to allow a parsimonious use of the model and to maximize efficiency given the available computational budget pronzato and müller 2012 pázman and pronzato 2014 sheikholeslami and razavi 2017 becker et al 2018 however optimization based sampling in high dimensional spaces can become challenging due to the curse of dimensionality greedy sampling methods can be used to reduce the computational cost of optimization based sampling methods oakley and o hagan 2004 maday et al 2009 schaback and wendlend 2006 jakeman et al 2019 harbrecht et al 2020 but while being efficient in many cases they can still ultimately suffer from the curse of dimensionality often the lower dimensional subspaces that impact estimates of uncertainty are efficiently described by linear or possibly non linear combinations of parameters where sa is a means to an end being unable to uniquely identify individual parameters is often inconsequential by moving beyond identifying directions aligned with the axes of the parameter space significant dimension reduction can be achieved ideally sa should identify directions that are most influential consider a simulator that is a nonlinear function of the equally weighted sum of parameters y p1 p2 2 each parameter will be found to be important but only one direction will have a non zero influence on the function in 2d the x y plane the function will be constant in all orthogonal directions recently great success has been achieved using methods such as active subspaces which find linear rotations of the parameters which are important constantine 2015 finding non axial directions has been used successfully to reduce the cost of inverse uq these methods work by restricting resources to identifying and exploiting subspaces that are informed by data and can reduce the computational cost of the inverse problem by orders of magnitude tonkin and doherty 2005 spantini et al 2015 when estimating data informed prediction uncertainty e g combining forward and inverse uq the optimal approach is to find the directions that are both informed by data and that influence predictions when quantifying uncertainty for linear models these directions can be computed exactly using generalized eigenvalue decompositions initial work has been carried out for linear models lieberman and willcox 2014 but further research is needed especially for non linear models kucherenko et al 2011 showed that it is not the model nominal dimensions but effective dimensions that define the model complexity in this respect they loosely divided models into three types a models with only a few important variables b models with equally important variables and with dominant low order interactions terms in their anova decomposition and c models with equally important variables and with dominant high order interaction terms they argued that type a and b models have low effective dimensions and therefore their handling with sa is relatively easy regardless of their nominal dimensionality 3 5 5 sensitivity of uq to modeling choices the assessment of how uncertainty estimates change with different modeling decisions such as numerical discretization schemes is important but often ignored for example when using numerical solutions to pdes between two coupled models the choice of accuracy and cost of uq depends on the mesh and timestep size of each model and the resolution of the coupling between the models in space and time identifying how sensitive a prediction is to these choices can significantly reduce the cost of uq for example if the final prediction is relatively insensitive to one model the resolution of that model and its coupling can be coarse without trading off accuracy the other important decision one may make is what output to choose in framing the sa problem indeed various outputs can be considered such as the mean of the model response its variance a probability that the output exceeds a threshold or a quantile of the output for example in the context of risk or reliability analysis liu et al 2006 the sa results would be different if the quantity of interest is related to the tail of the distribution of a model output such as a failure probability a quantile or a super quantile hong and liu 2010 lemaître et al 2015 compared to a case where the sensitivity of a measure of central tendency is of interest the problem of matching the output of interest with the sensitivity measure is discussed in borgonovo et al 2016 where several global sensitivity measures variance based moment independent quantile based are examined from an information value viewpoint see also section 3 2 2 for a discussion on the framing of the sa problem from a model verification point of view the goal oriented sa framework fort et al 2016 is relevant in this context as well moreover in many applications the probability density functions pdfs used to describe the uncertainty in inputs may themselves be highly uncertain morio 2011 this second level of input uncertainty is often the case where there are no data available regarding the processes that those inputs control addressing this type of uncertainty is an essential and fruitful area of research and has recently been attracting attention input pdf robustness analysis has been recently defined as a particular setting of sa lemaitre et al 2015 gauchy et al 2020 other example works include chabridon et al 2018 for rare event reliability analysis schöbi and sudret 2019 in the context of probability boxes hart and gremaud 2019a 2019b for variance based indices and meynaoui et al 2019 for hilbert schmidt independence criterion hsic 3 5 6 uncertainty in sa results themselves a major component of best practice in sa is the assessment of uncertainty in the estimates of sensitivity measures this uncertainty is directly related to reliability and robustness of sa as discussed in section 3 3 while well known methods such as bootstrapping efron 1987 are available to provide an uncertainty estimate it is notable that a minority of works apply this quantification systematically bootstrapping does however need to be handled with caution as strictly the samples taken should be random as with monte carlo samples and it requires smoothness and symmetry of the bootstrap distribution which is not always attainable care is also required to check if the sample size is too small to contain enough information for bootstrapping more advanced bootstrap procedures are required if the distribution is skewed or multimodal such as bias corrected and accelerated bootstrap intervals efron 1987 recent progress has seen more general bootstrap like methods that can work well for different types of samples and sampling strategies including bootstrapping of samples generated by quasi monte carlo or latin hypercube sampling based on multiple independent replicates of an estimator owen 2013 heuristic approaches such as introducing dummy parameters zadeh et al 2017 and model variable augmentation mai and tolson 2019 have also shown promise furthermore future work could follow bayesian methods for the calculation of confidence intervals on the estimates of global sensitivity measures for example studies refer to oakley and o hagan 2004 le gratiet et al 2014 and antoniano villalobos et al 2020 further research is needed especially in the presence of the curse of dimensionality sensitivity analysis of sensitivity analysis has been also suggested as a way to measure the influence of the analysis own design parameters haghnegahdar and razavi 2017 puy et al 2020a puy et al 2020b and the choice of methods razavi and gupta 2015 mora et al 2019 3 6 sa in support of decision making 3 6 1 the deep roots of sa in the field of decision making sa has historically but informally been a major building block of the decision making process the notion of sa for example has been embedded in the classic and widely used concepts of shadow prices dorfman et al 1987 and scenario analysis duinker and greig 2007 elsawah et al 2020b the former used in constrained optimization quantifies how much more profit i e objective function one would get by increasing the amount of a resource by one unit i e constraints this practice can be viewed as one factor at a time local sa oat sa one form of lsa often on continuous variables around an optimal point in the decision variable space the latter however revolves around what if scenarios evaluation of policy effectiveness analysis of causality and robustness analysis where one or several variables at once are changed around a base case or within a factor space to evaluate change in the outcome a what if scenario evaluates the effect of a change in inputs on a decision outcome while policy effectiveness evaluates either the effect size continuous variables or existence of effect of a policy change discrete variables analysis of causality attributes change in the output to change in inputs and robustness analysis can either test whether a recommendation changes guillaume et al 2016 or evaluate the effect of changes in factor space mcphail et al 2018 robustness is particularly recognized as useful in addressing uncertainty arising from the existence of multiple plausible futures maier et al 2016 iwanaga et al 2020 and other forms of deep uncertainty marchau et al 2019 the above examples indicate how informal and often local sa has contributed and will continue to contribute to a variety of decision making problems we note that while lsa has been often criticized for being perfunctory saltelli and annoni 2010 when used to support mathematical modeling see section 3 2 it is an essential means for many decision support systems where the users need to assess the impact of a change in a policy or the environment on the status quo consistent with the role and function of sa these strong ties exist because decision making is in fact fundamentally about identifying how objectives of interest are influenced by possible interventions in economics the ceteris paribus concept a latin phrase meaning all else being equal is on the basis of a one factor at a time local sa this concept is used in mainstream economic thinking to measure the effect of a shock to one economic variable e g the price of a commodity or a set of wages on another provided all other variables remain the same economists know well that this is a crude approximation mirowski 2013 so the point is the use one makes of it for example this approach would be good to understand the system but poor to prescribe a policy response similar cautions apply to the what if scenarios described above to address these possible limitations formal sa has recently found its footing in decision science as described below 3 6 2 modern sa for decision making under uncertainty more recently formal approaches to sa particularly for global sa have emerged as a means to support decision making under uncertainty sa can decompose uncertainty in the outcome of a decision option and attribute that to different sources of uncertainty in a decision problem identifying the dominant controls of uncertainty and how they interact adds transparency to the problem and guides the decision making process towards minding the uncertainties that matter the most tarantola et al 2002 first laid down a framework on how modern sa can support decision analysis which has since gained significant momentum by outlining sa capabilities to 1 understand whether the current state of knowledge on input uncertainty is sufficient to enable a decision to be taken maier et al 2016 2 identify data sources or parameters that require investing resources for knowledge improvement to achieve the desired level of confidence in making a decision lamontagne et al 2019 3 in the presence of different policy options clarify how various uncertainty sources and their extents affect the confidence in the expected outcome of each policy marangoni et al 2017 4 flag models used out of context and to a degree of complexity not supported by available information for the decision problem at hand herman et al 2015 and 5 invalidate policy assessments in cases where untested possibly unjustified assumptions dominantly control model outputs workman et al 2020 puy et al 2020c 3 6 3 sa and robustness of decisions under deep uncertainty assessment of the robustness of decision alternatives is becoming increasingly important in light of deep uncertainty which refers to a situation when stakeholders do not know or cannot agree on a system model that relates action to consequences the probability distributions to represent uncertainty in the inputs to the model and or how to value the desirability of alternative outcomes lempert et al 2003 maier et al 2016 addressing deep uncertainty requires the identification of options that perform well over a wide range of plausible future conditions this creates additional challenges and opportunities for the development of sa approaches in support of decision making especially in terms of how to best perturb model inputs robustness in this context refers to the insensitivity of a decision outcome to variation in model inputs and parameters and in general to the assumptions made in the decision making process the robustness of the utility of a particular decision alternative can be quantified with the aid of robustness metrics which use different ways to combine model outputs from different sensitivity trials corresponding to different sets of plausible combinations of model inputs into a single value to quantify different aspects of the performance of a decision alternative over these trials such as best case performance worst case performance average performance and variability in performance see mcphail et al 2018 sa based assessment of the robustness of decision alternatives under deep uncertainty requires careful consideration of the way the model input space is sampled see mcphail et al 2020 depending on the philosophy that underpins the robustness assessment if the goal is to quantify robustness under as broad a range of future conditions as possible then a large number of samples that cover the model input space as uniformly as possible is required however if the goal is to calculate robustness under possible future states of the world that represent alternative plausible conditions under different assumptions mahmoud et al 2009 the number of samples used is quite small 10 as each sample generally corresponds to a coherent narrative storyline scenario of an alternative hypothetical future van notten et al 2005 such scenarios are plausible stories about the future of a system that is too complex to predict wiek et al 2013 elsawah et al 2020b and are often obtained via participatory processes involving a variety of stakeholders e g riddell et al 2018 wada et al 2019 razavi et al 2020 it should also be noted that due to the temporal dimension associated with deep uncertainty the samples of the model input space often correspond to time series e g guo et al 2018 culley et al 2019 riddell et al 2019 irrespective of which philosophical approach underpins the robustness assessment 3 6 4 sa and ranking of decision alternatives sa has mainly been used to determine the sensitivity of model outputs to plausible changes in model inputs and parameters while this can provide useful information to support decision making it does not assess the sensitivity of the relative ranking or preference of different decision alternatives to potential changes in model inputs and parameters this can be achieved by sa when focused on identifying the smallest combined changes in model inputs and parameters that result in performance rank equivalence of two decision alternatives which can be expressed as a distance metric the smaller this metric the more robust insensitive the relative performance rank of a particular decision alternative and vice versa in addition the model inputs and parameters that have the largest influence on the relative performance rank of decision alternatives can be identified while such sensitivity analyses have already been applied to simulation models e g ravalico et al 2009 2010 marangoni et al 2017 lamontagne et al 2019 puy et al 2020c as well as decision models such as multi criteria decision analysis hyde et al 2005 2006 herman et al 2015 ganji et al 2016 they need to be developed further especially under conditions of deep uncertainty to ensure robust decision outcomes are achieved 3 6 5 sa and qualitative aspects of decision making sa is not only a quantitative paradigm but also an epistemological one when used for regulation and policy making sa must be broadened to include consideration of epistemological aspects linked to the plurality of disciplines and interested actors at play different norms and incommensurable values may emerge in this context questions such as what are the different narratives of a problem who is telling what story and which of these narratives are being privileged in the modeling activity carried out to support the decision making process are naturally brought to the fore to address these issues saltelli et al 2013 proposed a framework for sensitivity auditing sensitivity auditing emphasizes the framing of a decision analysis its institutional context and the motivations and interests of the researchers stakeholders and policy makers involved an analyst can scrutinize a mathematical model used to assist a decision making process against the sensitivity auditing checklist to cast light on potential criticalities such as 1 rhetorical use of mathematical modeling 2 identification of the underpinning technical assumptions 3 uncertainty inflation or deflation 4 unaddressed uncertainty and sensitivity of the model at the time the results are published 5 lack of model transparency 6 frames privileged and frames excluded and 7 incomplete or lack of sa the european commission 2015 and the european science academies sapea 2019 recommend sensitivity auditing in the context of modeling in support of policy making example applications of sensitivity auditing are found in the fields of education oecd pisa study araujo et al 2017 food security saltelli and lo piano 2017 public health and nutrition lo piano and robinson 2019 and sustainability metrics galli et al 2016 the above seven points of sensitivity auditing are also substantially subsumed in the manifesto for responsible modeling published in nature saltelli et al 2020 further attempts to thoroughly capture the quality of the knowledge in a modeling activity include the model pedigree concept eker et al 2018 and the numeral unit spread assessment and pedigree nusap framework van der sluijs et al 2005 for knowledge quality assessment incidentally both nusap and sensitivity auditing are approaches belonging to the tradition of post normal science a style of use of science for policy that becomes relevant when facts are uncertain values are in dispute stakes are high and decisions are urgent funtowicz and ravetz 1993 3 6 6 revisiting the link between sa and decision making while formal sa is finding its footing in the area of decision making there is a need to revisit the principles of decision making to identify where and how decision theories and applications have perhaps informally been based on the fundamentals of sa such efforts could facilitate bridging the two fields and take advantage of recent advances in sa in emerging decision problems across a variety of domains as well as correspondingly motivate advances in sa methodology for decision making in this process one must be mindful of the commonly asked questions by decision makers studies with formal sa methods often tend to answer different often more sophisticated questions to those related to specific quantities of interest that decision makers care most about therefore to be most useful decision makers need to be engaged in the process of co formulating the sa problem to ensure it addresses the right question s 4 synthesis and concluding remarks the process of developing the common perspective expressed in this paper across the multidisciplinary team of authors faced interesting challenges related principally to the various disciplinary and methodological views as well as experiences across different application areas that diversity promoted a synergy and more comprehensive coverage of potential opportunities to strengthen the role of sa as summarized in the following key messages a collective efforts are needed to structure generalize and standardize the state of the art in sa such that it forms a distinct cross field discipline section 3 1 such efforts must emphasize 1 teaching sa as integral to systems analysis and modeling and decision making 2 developing protocols for best sa practice that are transferable across specific contexts and applications and 3 launching scientific journals dedicated to sa b much work is needed to realize the tremendous untapped potential of sa for mathematical modeling of socio environmental and other societal problems which are confounded by uncertainty section 3 2 sa can help with the management of uncertainty by 1 characterizing how models and the underlying real world systems work 2 identifying the adequate level of model complexity for the problem of interest and 3 pointing to possible model deficiencies and non identifiability issues as well as where to invest to reduce critical uncertainties c computational burden is recognized as a major hindrance to the application of sa to cases where sa can be most useful such as for high dimensional problems section 3 3 greater efforts should be directed to developing sa algorithms that are 1 more computationally efficient 2 more statistically robust 3 able in particular to consume recycled samples however taken and 4 able to provide credible confidence measures on their results d the recent revitalized rise of machine learning ml particularly deep learning methods could be further enhanced by formal theories of sa section 3 4 the great potential of sa needs to be discovered for the following purposes and beyond 1 explainability and interpretability of ml 2 input variable selection 3 enabling ml to work with small data where big data sizes are not available and 4 building trust in ml models e sa is a much needed complement and or building block to most uncertainty quantification uq practices regardless of whether the aim is forward or inverse uq section 3 5 sa and uq need to be better combined to support a variety of purposes including 1 apportioning uncertainty 2 handling the curse of dimensionality 3 addressing unknowns around the distribution of inputs and their correlation structure and 4 assessing the sensitivity of uncertainty estimates to various choices made in the design of a uq problem f decision and policy making under uncertainty can significantly benefit formally or informally from advancements in sa including from the notion of sensitivity auditing that is an extension of sa where systems models and the legitimacy of processes undertaken in their development are used to support policy section 3 6 conversely sa can benefit from reflecting on and formalizing ways in which decision making and decision makers have previously used sa concepts informally sa when used in support of decision making can address critical questions such as 1 where and how does uncertainty matter 2 have the impacts of all important assumptions been treated 3 where should we invest to increase confidence in the expected outcome of a policy option and 4 has the policy uncertainty been artificially inflated or constrained all together the above points call for more cross fertilization of different research and practice streams on sa across a wide range of disciplines an implication of this broadening of sa is that it should be considered a multi discipline it is a subject that is intrinsically of interest to multiple disciplines that will continue to have distinct but ideally interconnected literature mathematicians and computer scientists are interested in more efficient ways of calculating measures decision scientists are interested in identifying different measures modelers systems analysts are interested in how they can use those measures in their work and decision makers are interested in the outputs and implications of the analyses sa is a vertically integrated deep topic those at the surface at the highest level of abstraction do not want to know and do not really need to know what is happening at the bottom they simply apply a method already developed for their purpose conversely those at the bottom produce fundamental work that does not always need to be directly responsive to immediate demands at the top for example the computational inefficiency of an algorithm in practice does not matter much at the development stages of new theories from this perspective sa therefore needs coordination rather than consensus we expect that multiple views and even definitions of core concepts will continue to co exist but the field needs to ensure that cross fertilization of ideas continues and expands to allow different disciplines and application areas to benefit from one another despite their differences in order to be of progressive impact to society it is crucial that this coordination then connects with the needs of planners policy analysts and decision makers with active engagement supporting the development of a shared understanding of the questions that they want answered as well as the questions they do not yet know they want answered the authorship team of this perspective invites discussion and collaboration with researchers and practitioners across every area of science interested in theories developments and applications of sa in our vision over the next decade sa will underpin a wide variety of activities around scientific discovery and decision support declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this perspective paper is the outcome of a one day workshop called the future of sensitivity analysis which was held as a satellite event to the ninth international conference on sensitivity analysis of model output samo october 28 31 2019 in barcelona spain we are thankful to the sponsors of this event including the french research association on stochastic methods for the analysis of numerical codes mascot num open evidence research at universitat oberta de catalunya the joint research centre of the european commission the university of bergen norway and the french cerfacs centre européen de recherche et de formation avancée en calcul scientifique the financial and logistic support to saman razavi including underwriting open access publication fees by the integrated modeling program for canada impc under the framework of global water futures gwf is acknowledged furthermore part of the efforts leading to this paper was supported by the national socio environmental synthesis center of the united states under funding received from the national science foundation dbi 1639145 sandia national laboratories is a multi mission laboratory managed and operated by national technology and engineering solutions of sandia llc a wholly owned subsidiary of honeywell international inc for the u s department of energy s national nuclear security administration under contract de na 0003525 the views expressed in the article do not necessarily represent the views of the u s department of energy or the united states government john jakeman s work was supported by the u s department of energy office of science office of advanced scientific computing research scientific discovery through advanced computing scidac program joseph guillaume received funding from an australian research council discovery early career award project no de190100317 arnald puy worked on this paper on a marie sklodowska curie global fellowship grant number 792178 takuya iwanaga is supported through an australian government research training program agrtp scholarship and the anu hilda john endowment fund we would like to thank dan ames editor in chief for insightful comments and encouragement 
25878,soil erosion models typically applied at basin and watershed scales are rarely evaluated at agricultural field scales due to the lack of spatially distributed time series data a novel unmanned aerial vehicle uav methodology was used to quantify farm field soil erosion from nine uav surveys and structure from motion sfm using a semi distributed approach we evaluated soil erosion estimates from the universal soil loss equation usle and water erosion prediction project wepp the annual erosion rate measured with the uav methodology was 18 83 t ha 1 yr 1 with usle and wepp predictions of 26 23 t ha 1 yr 1 and 16 41 t ha 1 yr 1 respectively modelled annual and sub annual erosion rates with wepp were within the upper limit of predictive accuracy while the usle tended to systematically overestimate soil erosion rates these outcomes have implications on the efficacy of conservation efforts which is highlighted through a discussion and comparison of different best management practice applications keywords agriculture evaluation erosion modelling farm field unmanned aerial vehicle 1 introduction soil erosion in agricultural systems is a pressing issue for water quality bennett et al 2001 michalak et al 2013 and agricultural sustainability pimentel 2006 fao 2015 montanarella 2015 soil erosion accounts for 75 billion tons of soil loss annually from arable land eld initiative 2015 resulting in a median productivity loss of 0 3 of crop yield per year fao 2015 with an estimated global economic impact of 400 billion usd per year eld initiative 2015 these trends are likely to be exacerbated as the demand for agricultural products continues to increase tilman et al 2011 while highly productive cropland is lost to urban growth estimated 1 8 2 4 by 2030 d amour et al 2017 and accelerated soil erosion processes from conventional agriculture n 448 median 18 t ha 1yr 1 montgomery 2007 continue to degrade arable land limited space for agricultural expansion has resulted in the expansion of agricultural cropland into marginal and highly erodible landscapes e g forested tropics foley et al 2011 re expansion into erodible agricultural landscapes that were previously taken out of production e g conservation reserve program in the united states bigelow et al 2020 and the conversion of less productive land e g summer fallow pasture into cropland e g canada statistics canada 2017 between 1985 and 2005 there was a global net increase of 2 41 of cropland area into these highly erodible landscapes foley et al 2011 within this context of agricultural land scarcity and the demand for agricultural products estimated to double by 2050 foley 2011 soil erosion agricultural production and sustainable land management will continue to be a critical global issue throughout the 21st century predicting the magnitude of soil loss in agricultural systems is a difficult environmental modelling problem part of this difficulty resides in the fact that an agricultural system is a coupled human natural system whereby both systems are highly heterogeneous e g natural weather soil human cropping practices land management and interactive given this complexity some have conceptually described agricultural systems as being stochastic since seemingly identical agricultural systems can have widely different hydrologic responses e g variability in replicate plots wendt et al 1986 nearing et al 1999 even when agricultural systems produce a similar hydrologic response sediment delivery is mediated by highly heterogeneous parameters e g microtopography from tillage tile drainage riparian buffer strips leading to a poor relationship between erosion rates soil loss from fields and sediment yield in waterways despite continued improvements in our understanding of edaphic processes and computational modelling decades of research continue to note that the predictive capabilities of soil erosion models are often quite poor takken et al 1999 favis mortlock et al 2001 jetten et al 2003 evans and brazier 2005 morgan and nearing 2011 evans 2013 and that erosion modelling is very error prone jetten et al 1999 p 537 this high degree of error and the inherent complexity of these human natural systems necessitates the need for testing and validating model predictions the advent of process based erosion models has led to the common misconception that soil erosion models can be applied to any agricultural system independent of testing and without a priori calibration morgan and nearing 2011 however both empirical and process based models rely on statistical relationships that need to be tested and validated against in situ soil erosion measurements testing the predictions of erosion models can be difficult when you consider that soil erosion measurements have a considerable amount of uncertainty associated with them stroosnijder 2005 and that the scales of measurement rarely correspond to the scales of natural and human processes driving erosion it is impossible to elucidate the process domains driving erosion using aspatial outlet based measurements e g sediment yield borrelli et al 2014 or even with spatially distributed isotopic tracer measurements e g caesium 137 walling et al 2003 remote sensing e g airborne imagery fischer et al 2018 and field based methods e g measuring rills and gullies takken et al 1999 provide some understanding of the dominant process domains contributing to soil erosion but they are constrained by spatial scale accuracy and the repeatability of measurements the disconnect between process domains that are measured and modelled is one of the biggest obstacles for testing and validating model predictions of soil erosion for example consider an outlet based method of measuring sediment yield for evaluating model predictions of soil loss the sediment yield subsumes all processes in a catchment i e all sediment sources and delivery mechanisms to produce a single aspatial number even if the modelled results accurately predict sediment yield at the catchment outlet the issue of equifinality cannot be addressed are the models getting the right answer for the right reasons without a spatial component included in the evaluation process the model may have a completely erroneous representation of the internal catchment dynamics while still producing a correct outlet response van oost et al 2005 the challenges of equifinality can only begin to be addressed if distributed data is used to ensure the model is behavioral jetten et al 2003 unfortunately spatial measurements of soil erosion are labor intensive expensive and time consuming as such they are seldom used for evaluating models morgan and nearing 2011 stroosnijder 2005 concludes a critical review on soil erosion measurements stating there is a crisis in erosion measurements p 172 due to poor empirical data and no new developments in technology for measuring soil erosion the democratization of unmanned aerial vehicles uavs also known as remotely piloted aircraft systems rpas and automated photogrammetric workflows i e structure from motion sfm herald a new advancement in remote sensing technology for measuring distributed soil erosion rates the potential of these novel technologies to accurately measure soil erosion is recognized in literature but they have yet to be used for testing erosion models batista et al 2019 the very high resolution 5 cm frequent and event based quantitative measurements of distributed erosion from uavs meinen and robinson 2020a can be used to create a time series of erosion data at a spatial scale that has not been previously achievable these data provide an opportunity to evaluate soil erosion models in a spatially distributed manner at the scale in which agricultural decisions take place i e farm field in addition to quantitative measurements optical imagery of erosional features can be used as an additional soft qualitative datapoint for model evaluation jetten et al 1999 while there still remain challenges in creating high fidelity data with sfm for change detection measurements meinen and robinson 2020b the combination of qualitative and quantitative erosion measurements provides a strong baseline for evaluating the performance of erosion models to advance the field of erosion modelling we collected novel very high resolution time series data using a uav for the purpose of quantifying semi distributed rates of soil erosion over an entire year we used these data to determine what is the accuracy of an empirically based universal soil loss equation usle wischmeier and smith 1978 and a process based water erosion prediction project wepp flanagan and nearing 1995 erosion model at estimating soil erosion rates on an agricultural field in southwestern ontario canada the performance of the usle and wepp were evaluated on both an annual and sub annual basis against uav based measurements of soil erosion 2 materials and methods 2 1 study site our study site is an agricultural field located in the upper reaches of the nith watershed ontario canada the nith watershed spans an area of 1130 km2 with 80 of its total land area devoted to agricultural production loomer and cooke 2011 the nith watershed is characterized by high rates of runoff and pollutant loading during the spring freshet loomer and cooke 2011 with peak precipitation occurring in the latter half of the growing season the nith watershed has been identified as a priority sub watershed of the grand river watershed due to its large contribution of phosphorus nitrogen and suspended sediments to the grand river holeton 2013 the grand river flows southwards into lake erie which continues to face the harmful effects of eutrophication and algae blooms caused by elevated levels of phosphorus originating from diffuse agricultural sources michalak et al 2013 the agricultural field used for measuring modelling and evaluating predictions of soil erosion is part of a heterogeneous agricultural system that is uniquely characterized by snowmelt and a dense tile drainage network the field is a steep 15 9 ha tile drained agricultural field under a 3 year crop rotation of corn soybean and winter wheat fig 1 which is a common rotation for southern ontario the chosen study year for field work spans from corn cultivation in 2018 to the spring of 2019 the field management regime consists of a fall moldboard plow following corn and oats a spring cultivator for seedbed preparation for corn and soybeans no till winter wheat and broadcast oats as a cover crop after winter wheat fig 1 the study site had tile drainage installed in the winter of 2017 2018 the tile installation was accompanied by an installation of soil berms and surface inlets at six locations that form catch basins to trap eroded sediments fig 2 soils on the study site are texturally classified as a mix of sandy loam and loam 2 2 data our data collection is focussed on six distinct drainage basins with soil berms and surface inlets i e catch basins outlined in fig 2 i e basins a b c d e and f for measuring and evaluating model estimates of soil loss by water erosion all field work was carried out by meinen and robinson 2020a from may 17 2018 to may 16 2019 when the study site was under corn production a comprehensive uav surveying methodology for the study site can be found in meinen and robinson 2020a with a discussion on uav flight design in meinen and robinson 2020b 2 2 1 field work the modelling of erosion in agricultural systems requires a representation of the following five factors climate soils topography vegetation and land management our climate data consists of hourly precipitation data collected from a meteorological station wellesley dam located 7 km from the study site soil data included average soil texture measured with a sieve and hydrometer analysis on 27 soil samples 15 cm depth menzies pluer et al 2020 and bulk density measured using bulk density rings on 16 soil samples 15 cm depth that were dried in a conventional oven at 105 celsius for 24 h and weighed topographic data was calculated from a digital elevation model dem created with optical imagery collected from a uav and sfm the raw data were collected at a resolution of 0 02 m and used to create a 0 02 m dem which was resampled to generate a 0 8 m dem for catchment discretization and resampled to 3 2 m for calculating primary terrain variables i e hillslope length width and slope all topographic calculations were done using arcgis v10 6 1 cropping and management practices were provided by the landowner fig 1 and crop stages were determined from in situ site surveys to model erosion using the usle and wepp our six study basins fig 2 were discretized into rectangular hillslopes table 1 and segmented into 3 5 overland flow elements to capture the hillslope shape rectangular hillslopes were segmented such that a hillslope constituted the point from where overland flow began to where either the slope gradient decreased such that deposition began or where the runoff entered a concentrated flow channel e g an ephemeral gully 2 2 2 uav erosion measurements the creation of a dataset for evaluating model estimates of soil erosion involved using a novel remote sensing approach to monitor surface change in each catch basin with optical imagery collected from a uav and sfm meinen and robinson 2020a across the study year a total of nine surveys were conducted to recreate the topography of each catch basin and to generate orthomosaics fig 3 the m3c2 algorithm lague et al 2013 0 15 m projection vertical normals was used in cloudcompare v2 9 1 https www danielgm net cc to calculate surface change i e deposition in each of the six catch basins using pointcloud data volumetric surface change was converted to a weight using an average measured soil bulk density of 1 32 g cm3 across the entire study year a total of 159 52 tonnes of sediment was deposited across the six catch basins table 2 corresponding to an erosion rate of 18 83 t ha 1 yr 1 2 3 erosion modelling 2 3 1 universal soil loss equation usle research on soil erosion in north america began in the early 20th century e g zingg 1940 smith 1941 musgrave 1947 and was accelerated after franklin roosevelt helped pass the soil conservation act of 1935 public law 74 46 to fulfil the requirements of this act the united states department of agriculture and the newly created soil conservation services developed the universal soil loss equation usle in the 1950 s as a tool to predict soil loss and help farmers with conservation planning the usle is a lumped empirical field scale model that predicts soil loss from rill and inter rill erosion based on 10 000 plot years of erosion data the usle was originally published in agricultural handbook no 282 wischmeier and smith 1965 and adopted widely based on the superseding publication in agricultural handbook no 537 wischmeier and smith 1978 the usle is expressed as a n 1 6 r k l s c n p where a is the soil loss per unit area t ha 1 yr 1 which is the sum of the products of six factors for six cropstages n whereby r is a rainfall and runoff factor mj mm ha 1 h 1 yr 1 k is a soil erodibility factor t ha h ha 1 mj 1 mm 1 l is a slope length factor unitless s is a slope steepness factor unitless c is a cover and management factor unitless and p is a supporting practice factor unitless wischmeier and smith 1978 these six factors combine human decision making and the predominant natural processes contributing to soil loss in an agricultural landscape the usle is the de facto standard for erosion modelling due to its simplicity and ease of use the usle and its many derivations e g revised usle renard et al 1991 modified usle williams 1975 are incorporated into erosion estimates in a wide variety of models areal nonpoint source watershed environment response simulation answers beasley et al 1980 field scale model for chemicals runoff and erosion from agricultural management systems creams knisel 1980 erosion productivity impact calculator epic williams 1989 agricultural non point source model agnps young et al 1989 soil and water assessment tool swat arnold 1994 sediment delivery distributed model sedd ferro and porto 2000 and the water and tillage erosion model sediment delivery model watem sedem van rompaey et al 2001 for this study all usle calculations are based on agricultural handbook no 537 wischmeier and smith 1978 the r factor was calculated using hourly precipitation data from the wellesley dam meteorological station using the follow equation wischmeier and smith 1978 r i 1 m e i 30 i where ei30 is the rainfall erosivity of a single storm event mj mm ha 1 h 1 i calculated as the total kinetic storm energy e mj ha 1 times the maximum 30 min rainfall intensity i30 mm h 1 for all storms in a given year m storms are only included in the calculation of r that exceed 12 7 mm of rainfall in a 6 hour period an additional sub factor rs can be included to account for the effects of winter runoff and snowmelt on soil loss we chose to exclude rs from our soil loss calculations since we observed in the previous year that snowmelt produced no observable amount of erosion fig 3a since the usle requires 30 min rainfall intensities for calculating the i30 our calculated r factor likely underpredicted storm intensity to correct for this underprediction we used a relationship developed by panagos et al 2015 to convert from 60 min intensities to 30 min intensities by multiplying our r factor by 1 5597 the k factor was calculated using field based measurements of soil texture i e 34 82 silt 13 27 clay and estimates of edaphic properties i e 2 organic matter b 2 c 3 based on the following equation for soils containing less than 70 silt and very fine sand wischmeier and smith 1978 100 k 2 1 m 1 14 10 4 12 a 3 25 b 2 2 5 c 3 0 1317 where k is the soil erodibility factor t ha h ha 1 mj 1 mm 1 m is the soil particle size parameter based on soil texture a is percent organic matter b is the soil structure code and c is the profile permeability class organic matter in the topsoil on the study site was estimated to be lower than regional averages 4 due to a tile drainage installation in the previous winter which inverted a large portion of the soil column the l and s factors are typically jointly calculated as the following wischmeier and smith 1978 l s λ 22 13 m 65 41 s i n 2 θ 4 56 sin θ 0 065 where ls is the slope length factor unitless λ is slope length m θ is the angle of the slope degrees and m is an exponent based on slope gradient see table 1 for hillslope topographic attributes the c factor and corresponding soil loss ratios slrs were calculated using the tables provided in agricultural handbook no 537 for high productivity corn with a fall moldboard plow table 3 no additional management practices were represented on our study site so the p value was initialized to a value of 1 unitless representing no net effect on soil erosion 2 3 2 water erosion prediction project wepp in 1985 the united states department of agriculture started research on a process based successor to the usle the water erosion predict project wepp laflen et al 1991 flanagan and nearing 1995 the wepp was developed using data from 50 experimental cropland laflen et al 1991 and rangeland plots gilley et al 1990 to accurately model the underlying hydrologic processes that contribute to soil erosion these processes are represented in the model using stochastic weather generation infiltration theory hydrology soil physics plant science hydraulics and erosion mechanics flanagan and nearing 1995 p 1 1 the wepp is a daily simulation model that computes soil loss and sediment delivery from rill and inter rill erosion for individual hillslopes or small watersheds flanagan and nearing 1995 the wepp erosion routine is based on a steady state sediment continuity equation that describes the movement of soil in rills d g d x d f d i where x is the distance downslope m g is the sediment load kg s 1 m 1 df is the rill erosion rate kg s 1 m 2 and di is the inter rill sediment delivery to rills kg s 1 m 2 erosion is conceptualized as a series of inter rill areas that deliver sediment to concentrated flow channels i e rills that is either deposited in the rill or transported off the hillslope for this study wepp model calculations are based on wepp version 2012 800 using the windows interface table 4 input parameters for modelling cligen was used to convert daily climate data i e precipitation and temperature into more temporally detailed distributions for soil loss and runoff calculations 2 4 analysis we focus on the six distinct drainage basins with soil berms and surface inlets i e basins a b c d e and f fig 2 for evaluating model estimates of soil erosion for the purposes of this study we define the rate of soil erosion as soil loss by water erosion to catch basins per unit area usle results are computed as soil loss per unit area i e soil loss from an individual hillslope and wepp results are computed as sediment delivery per unit length i e kg m 1 and multiplied by the hillslope width to convert into soil loss per unit area for comparing modelled results both model outputs reflect soil loss by water erosion from a hillslope to the lowest point in the catchment i e the catch basin the usle and wepp were used to estimate rates of soil erosion in each of the six catch basins on an annual and sub annual i e seasonal basis for evaluating the usle and wepp we conducted a blind evaluation batista et al 2019 p 4 whereby the usle and wepp were parameterized and soil erosion estimates were compared to uav based measurements of soil erosion for the 2018 2019 study year no calibration procedure was conducted measured rates of soil erosion were compared to model estimates of soil erosion for all six catch basins and evaluated based on the absolute magnitude of error percent error and nash sutcliffe model efficiency nse nash and sutcliffe 1970 n s e 1 i 1 n q m q p 2 i 1 n q m q m 2 where qm is the measured soil loss q m is the mean of measured soil loss and qp is the modelled soil loss nse values range from to 1 with a nse of 1 corresponding to a perfect match between measurements and model predictions a nse of 0 indicates that the modelled soil loss is as accurate as the mean of the measurements and a nse of less than 0 indicates that the mean of measurements is a better predictor than the erosion model for comparing the percent error between measured and modelled results we assume the erosion model has reached the upper limit of predictive accuracy if results are within 20 of measured values based on the average coefficients of variation from the replicate erosion plots of wendt et al 1986 uav based measurements of soil erosion are assumed accurate for the purposes of model evaluation but we include the uncertainty metrics of meinen and robinson 2020a in our results measurements of soil erosion had an average uncertainty of 29 based on a 0 04 m confidence interval for m3c2 surface change calculations soil erosion measurements likely underestimate actual erosion rates since sediment was lost into the surface inlets i e the depositional plumes in the catch basins used for measuring soil erosion rates were not representative of all the eroded sediments the exact volumetric amount of sediment lost into the subsurface drainage network is not known 3 results 3 1 model evaluation erosion model estimates were compared to uav based erosion measurements for each of the six study basins on both an annual table 5 and sub annual basis table 6 on an annual basis the total cumulative erosion rate i e soil loss by water erosion to catch basins per unit area for all six study basins was measured at 18 83 t ha 1 yr 1 the usle overestimated the erosion rate at 26 23 t ha 1 yr 1 whereas the wepp underestimated the erosion rate at 16 41 t ha 1 yr 1 table 5 assuming a natural stochastic variation of 20 in soil erosion rates i e 3 77 t ha 1 yr 1 only the aggregated results of wepp are within the upper limit of annual predictive accuracy at the semi distributed basin scale both the usle and wepp exhibited a higher range of variability with errors exceeding 20 when compared to annual uav measured values table 5 the absolute average magnitude of error of soil erosion estimates for the six catch basins were 7 36 t ha 1 yr 1 and 4 09 t ha 1 yr 1 for the usle and wepp respectively wepp estimates of soil erosion were more accurate than the usle for each basin with the exception of basin d the soil erosion rate of basin d was unexpectedly high considering its moderate slope and small size the five others basins had a closer agreement between measured and modelled soil loss using the wepp model and fell within or close to the upper limit of predictive accuracy the usle had a strong tendency to overestimate soil erosion rates for the study site whereas the wepp had a moderate underestimation of soil erosion rates sub annual estimates of soil erosion table 6 compared to uav measured values showed a much larger discrepancy between models than was observed at the annual temporal scale the total cumulative erosion rate for all six study basins showed a strong seasonal dependency with a measured soil loss of 16 08 t ha 1 0 00 t ha 1 and 2 73 t ha 1 for the growing season mature crop stubble and fallow winter periods respectively the usle estimated a soil loss of 20 06 t ha 1 0 87 t ha 1 and 5 30 t ha 1 while the wepp estimated a soil loss of 14 06 t ha 1 0 02 t ha 1 and 2 34 t ha 1 for the three respective periods the aggregated i e field scale sub annual results of the wepp are within the upper limit of predictive accuracy i e 20 measured values while the usle had a moderate overestimation of soil loss for all three periods at the semi distributed basin scale table 6 the usle and wepp once again exhibited a high range of variability with the wepp outperforming the usle for sub annual estimates the absolute average magnitude of error for the six catch basins for the three respective time periods was 5 33 t ha 1 0 65 t ha 1 and 2 47 t ha 1 for the usle overall 2 82 t ha 1 and 4 18 t ha 1 0 01 t ha 1 and 1 08 t ha 1 for the wepp overall 1 76 t ha 1 the usle had a sub annual nse of 0 80 exhibiting an overestimation of soil loss for most basins while the wepp had a sub annual nse of 0 96 with a close agreement between uav measured values and estimated soil loss 4 discussion 4 1 model evaluation an ongoing challenge in erosion modelling is recognizing the scale dependency of input parameters and ensuring that a model is correctly parameterized at the spatial e g hillslope field watershed and temporal scale e g annual sub annual of interest the model user makes the subjective choice on the spatial and temporal scales to measure estimate and interpolate input parameters given the same objectives different users will likely conceptualize and parameterize an agricultural system differently a simple blind model evaluation whereby an erosion model is parameterized run and compared to real measurements of soil loss increases the model users confidence that the correct parameter set was chosen and that the model is appropriate for the climate soils topography cropping system and land management practices of the system being studied the semi distributed model evaluation of the usle yielded accurate relative estimates of soil loss e g basin a was responsible for approximately 43 45 of total annual soil loss but overestimated soil loss during each cropstage and subsequently overestimated annual soil loss nse 0 56 analysis of the cropstage usle results demonstrated that this overestimation may have been a result of inadequate model input data rather than model error the i30 calculated for the establishment and fallow cropstages was likely too high the hourly rainfall intensities converted to 30 minute intensities did not correctly characterize the intensity of the most erosive storms the less accurate short term estimates produced by the usle are likely expected by practitioners with experience using the usle whereby long term average i e 20 22 years estimates of soil loss are expected to be more accurate however the usle was originally intended to be used for both long term and short term predictions of soil erosion wischmeier and smith 1978 state in handbook no 537 that with appropriate selection of its factor values the equation will compute the average soil loss for a multicrop system for a particular crop year in a rotation or for a particular cropstage period within a crop year p 3 with the caveat that the equation will be substantially less accurate p 4 for predicting individual storm events compared to long term averages the structure of the usle does not explicitly include runoff or seasonal temperature changes so short term i e cropstage 1 year annual predictions of soil loss in regions characterized by high temporal variability in temperatures with intermittent snowmelts will likely be worse than long term predictions while the usle includes a subfactor for thaw and snowmelt rs since erosion plots used in the model development were located in the midwest and pacific northwest united states the relationship is a single multiplicative factor of 1 5 times the december through march precipitation this relationship may relate well to the conditions of the pacific northwest whereby as much as 90 percent of the erosion on the steeply rolling wheat land has been estimated to derive from runoff associated with surface thaws and snowmelt wischmeier and smith 1978 p 7 but does not necessarily relate to the conditions of other winter climates we intentionally chose to exclude the rs sub factor from our study based on observations from the previous year 2017 2018 winter where snowmelt produced no observable amount of erosion fig 3a including the rs sub factor would have increased our error in the fallow cropstage from 95 to 268 the overestimation of winter soil loss may have also been exacerbated as a result of the soil loss ratio being too high for the stubble and rough fallow cropstages since frozen soils mediated erosional processes and snow melt produced very little soil erosion our semi distributed evaluation of the wepp provided insights into the applicability of the wepp for the conditions of our agricultural study site the wepp had accurate annual nse 0 97 and sub annual predictions nse 0 96 indicating that we correctly parameterized the model with our in situ measurements estimations and the parameters that we let wepp calculate from its database the wepps process based modelling structure and incorporation of temporally distributed management practices e g tillage plant science e g crop growth and hydrology e g snowmelt and associated runoff on a daily timestep allowed it to accurately model the seasonal dynamics of southwestern ontario although the wepp had a tendency to underpredict soil erosion rates aggregated field scale estimates of soil erosion were within the upper limit of predictive accuracy while our uav based evaluation approach and application to the climatic conditions in southwestern ontario are novel the general findings of our study corroborate existing literature both the usle and wepp model evaluations demonstrated that 1 annual estimates of soil erosion are more accurate than sub annual estimates e g jetten et al 1999 and 2 aggregated field scale estimates of soil erosion are more accurate than individual hillslope or basin estimates e g jetten et al 2003 model error increases as the spatial resolution of predictions becomes finer the stochastic variation in soil erosion rates is most pronounced at fine spatial and temporal scales e g hillslopes coarse scales e g full farm fields allow the model to have an averaging effect whereby overestimations and underestimations balance each other out the usle and wepp have been found to have similar annual predictive capabilities tiwari et al 2000 laflen et al 2004 albeit with a high range of error at shorter time scales the wepp was specifically designed to replace the usle by improving short term soil loss estimates by a process based representation of climate and hydrology flanagan and nearing 1995 our study corroborates that the wepp outperforms the usle at shorter time scales i e 1 year annual and sub annual at the farm field scale 4 2 erosion modelling and best management practices bmps there often exists a disconnect between the spatial scale at which best management practices bmps are operationalized and the spatial scale at which they are prescribed by environmental modellers to illustrate this disconnect we conducted a literature review using google scholar with the keywords erosion model agriculture and bmp thirty one of the first one hundred reviewed papers were relevant and contained a bmp case study whereby 14 45 2 used the swat model for bmp assessments 4 12 9 used usle rusle or rusle2 3 9 7 used agnps 2 6 5 used wepp and the remaining papers all employed a unique model twenty three 74 2 case studies used a combination of sediment yield discharge and nutrient measurements at catchment outlets for model evaluation while the remaining 8 studies 25 8 contained no model evaluation twenty one 67 7 studies used a hydrologic response unit hru i e a catchment with homogeneous land use soil and topography discretization for bmp implementation 5 16 1 used a raster based discretization 2 6 5 were unspecified and the remaining 3 studies 9 7 used farm fields or hillslopes for discretization wepp hillslopes abaci and papanicolaou 2009 agnps farm fields as individual cells yuan et al 2008 swat farm field as hru santhi et al 2006 while bmps are implemented at the farm field scale to mitigate soil loss the most common environmental modelling approach was to prescribe them at the scale of the hru and evaluate modelling results with point based measurements of sediment and discharge at watershed outlets e g bicknell et al 1985 moore et al 1992 bracmort et al 2006 rao et al 2009 betrie et al 2011 rousseau et al 2013 smith et al 2014 briak et al 2019 ricci et al 2020 while this approach is pragmatic for bmp studies comprising large spatial extents there is no evaluation or understanding of the hydrologic and sedimentologic processes at the most important spatial scale the individual farm field to facilitate model up scaling from individual farm fields to watersheds we advocate that environmental modellers discretize watersheds as a collection of individual farm fields and coalesce outlet and field based evaluations for a more holistic evaluation this two fold evaluation will allow the modeller to have a more in depth understanding of the dominant sediment transport processes from fields to waterways where bmps are implemented i e field scale evaluation and from waterways to catchment outlets i e outlet based evaluation this ensures the model is valid for a certain set of physiographic and climactic conditions addresses issues of model equifinality and provides insights into model uncertainty only using an outlet based evaluation may lead to a poor model implementation e g equifinal model and the implementation of ineffective bmps that can cause economic loss and degrade farmer confidence in governance and scientific modelling 4 3 field scale best management practice evaluation the uav based approach in this study was used to evaluate the applicability of the usle and wepp for a single agricultural field in southwestern ontario the evaluation demonstrated that the wepp was able to model the temporal variation in erosion rates whereas the usle had some challenges representing temporally distributed soil erosion rates table 6 based on this evaluation the wepp should be an effective tool for evaluating field scale bmps our wepp modelling results and field scale measurements demonstrate that the most effective bmp would be one that incorporates additional biomass into the soil during the early growing season this can be achieved by switching from a conventional fall plowing implement i e moldboard plow fig 1 to a conservation plowing implement i e chisel plow chisel plows have long plow shanks typically spaced 30 cm apart with sweeps shovels or chisel points that break up and stir the soil but do not invert the topsoil like a traditional moldboard plow this leaves a rough soil surface with sufficient biomass to cover the soil during the winter early spring and after the secondary tillage in the spring for final fitting when the site is most susceptible to erosion to evaluate the effects of switching tillage practices as a bmp to reduce long term soil loss on our study site we used the wepp model to estimate 10 year average 2005 2014 soil erosion rates for 1 a moldboard plow 2 a chisel plow with coulters and shovels and 3 a chisel plow with coulters and sweeps the fall moldboard plow buries 95 of crop residue whereas the chisel plow will only bury 30 55 of crop residue 55 with coulters and shovels 30 with coulters and sweeps studies demonstrate comparable yields between moldboard and chisel plowing for corn soybean and wheat albeit with small variations in yield depending on nutrient amendments singer et al 2004 and herbicide applications buhler 1992 making it a pragmatic bmp choice for ameliorating accelerated rates of soil erosion the wepp estimated the 10 year average erosion rate of the full 15 9 ha study at 8 12 t ha 1 yr 1 when using a fall moldboard plow replacing the moldboard plow with a chisel plow with coulters and shovels for the fall tillage reduced soil erosion by 43 resulting in a 10 year average erosion rate of 4 65 t ha 1 yr 1 table 7 supplementary fig 1 replacing the moldboard plow with a chisel plow with coulters and sweeps for the fall tillage reduced soil erosion by 64 resulting in a 10 year average erosion rate of 2 91 t ha 1 yr 1 table 7 supplementary fig 1 for all plowing implements the wepp estimated that the majority of long term soil erosion occurred during the early growing season with an average of 6 4 days per year with soil erosion winter events and snowmelt constituted 70 of the average long term runoff with 14 1 days per year with runoff but winter runoff events were rarely associated with soil loss supplementary figs 1 and 2 these results were consistent with our in situ observations and measurements during the 2018 2019 study year considering the regional context of our study site in that 68 1 of farms in the local county are using a conventional tillage system in their crop rotation e g moldboard plow statistics canada 2016 the transition to a conservation tillage system e g chisel plow could contribute to the amelioration of water quality issues in the region originating from sediment and particulate phosphorus losses for our farm the wepp model estimated a 64 long term reduction in soil loss when switching from a moldboard plow to a chisel plow with coulters and sweeps while no till management techniques would likely further reduce soil loss no till management practices can increase the risk of dissolved reactive phosphorus runoff king et al 2015 via macropore flow to subsurface tile drainage dissolved reactive phosphorus is readily bioavailable for biota uptake and is the limiting nutrient for primary production in adjacent aquatic systems i e lake erie since the majority of the upper nith watershed is tile drained loomer and cooke 2011 permanent no till systems are not recommended a conservation tillage with a chisel plow removes the macropore connectivity of soils with subsurface drainage lines while still ensuring there is biomass on the field surface to protect against the impacts of rainfall when the field is most susceptible to erosion 5 conclusions in this study we used semi distributed erosion measurements with a uav and sfm to evaluate the applicability of the usle and wepp to conditions in southwestern ontario canada while both models had satisfactory results the usle had a tendency to overestimate soil loss for each season which may have been the result of an incorrect characterization of rainfall intensity with our methodology in contrast to the usle our model evaluations highlighted that the process based modelling structure of the wepp modelled the hydrology of our study site correctly e g erosion from snowmelt and runoff and was able to accurately model soil loss at an annual and sub annual time step for both models model error tended to increase at shorter time scales and small spatial extents annual aggregated field scale estimates of soil erosion were more accurate for both models we strongly advocate that uav based model evaluations be conducted more commonly to ensure erosion models are behavioral before evaluating new bmps and before scaling out models to larger spatial extents uav based approaches collect the necessary qualitative and quantitative erosion measurements for model testing at the scale of the agricultural decision maker and can be used to inform models a priori future research should be aimed at improving the accuracy of the sfm mvs workflow for change detection such that fully distributed model evaluations can be conducted declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to acknowledge with gratitude infrastructure funding provided by the canadian foundation for innovation 32247 the ontario research fund flir systems the national sciences and engineering research council nserc of canada 06252 2014 547496 2020 as well as the ministry for research and innovation for support through the early researcher award er15 11 198 and the faculty of environment at the university of waterloo for student funding lastly we appreciate and thank the farming family who would like to remain anonymous for their collaboration in the work and for granting us access to their land appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 104962 
25878,soil erosion models typically applied at basin and watershed scales are rarely evaluated at agricultural field scales due to the lack of spatially distributed time series data a novel unmanned aerial vehicle uav methodology was used to quantify farm field soil erosion from nine uav surveys and structure from motion sfm using a semi distributed approach we evaluated soil erosion estimates from the universal soil loss equation usle and water erosion prediction project wepp the annual erosion rate measured with the uav methodology was 18 83 t ha 1 yr 1 with usle and wepp predictions of 26 23 t ha 1 yr 1 and 16 41 t ha 1 yr 1 respectively modelled annual and sub annual erosion rates with wepp were within the upper limit of predictive accuracy while the usle tended to systematically overestimate soil erosion rates these outcomes have implications on the efficacy of conservation efforts which is highlighted through a discussion and comparison of different best management practice applications keywords agriculture evaluation erosion modelling farm field unmanned aerial vehicle 1 introduction soil erosion in agricultural systems is a pressing issue for water quality bennett et al 2001 michalak et al 2013 and agricultural sustainability pimentel 2006 fao 2015 montanarella 2015 soil erosion accounts for 75 billion tons of soil loss annually from arable land eld initiative 2015 resulting in a median productivity loss of 0 3 of crop yield per year fao 2015 with an estimated global economic impact of 400 billion usd per year eld initiative 2015 these trends are likely to be exacerbated as the demand for agricultural products continues to increase tilman et al 2011 while highly productive cropland is lost to urban growth estimated 1 8 2 4 by 2030 d amour et al 2017 and accelerated soil erosion processes from conventional agriculture n 448 median 18 t ha 1yr 1 montgomery 2007 continue to degrade arable land limited space for agricultural expansion has resulted in the expansion of agricultural cropland into marginal and highly erodible landscapes e g forested tropics foley et al 2011 re expansion into erodible agricultural landscapes that were previously taken out of production e g conservation reserve program in the united states bigelow et al 2020 and the conversion of less productive land e g summer fallow pasture into cropland e g canada statistics canada 2017 between 1985 and 2005 there was a global net increase of 2 41 of cropland area into these highly erodible landscapes foley et al 2011 within this context of agricultural land scarcity and the demand for agricultural products estimated to double by 2050 foley 2011 soil erosion agricultural production and sustainable land management will continue to be a critical global issue throughout the 21st century predicting the magnitude of soil loss in agricultural systems is a difficult environmental modelling problem part of this difficulty resides in the fact that an agricultural system is a coupled human natural system whereby both systems are highly heterogeneous e g natural weather soil human cropping practices land management and interactive given this complexity some have conceptually described agricultural systems as being stochastic since seemingly identical agricultural systems can have widely different hydrologic responses e g variability in replicate plots wendt et al 1986 nearing et al 1999 even when agricultural systems produce a similar hydrologic response sediment delivery is mediated by highly heterogeneous parameters e g microtopography from tillage tile drainage riparian buffer strips leading to a poor relationship between erosion rates soil loss from fields and sediment yield in waterways despite continued improvements in our understanding of edaphic processes and computational modelling decades of research continue to note that the predictive capabilities of soil erosion models are often quite poor takken et al 1999 favis mortlock et al 2001 jetten et al 2003 evans and brazier 2005 morgan and nearing 2011 evans 2013 and that erosion modelling is very error prone jetten et al 1999 p 537 this high degree of error and the inherent complexity of these human natural systems necessitates the need for testing and validating model predictions the advent of process based erosion models has led to the common misconception that soil erosion models can be applied to any agricultural system independent of testing and without a priori calibration morgan and nearing 2011 however both empirical and process based models rely on statistical relationships that need to be tested and validated against in situ soil erosion measurements testing the predictions of erosion models can be difficult when you consider that soil erosion measurements have a considerable amount of uncertainty associated with them stroosnijder 2005 and that the scales of measurement rarely correspond to the scales of natural and human processes driving erosion it is impossible to elucidate the process domains driving erosion using aspatial outlet based measurements e g sediment yield borrelli et al 2014 or even with spatially distributed isotopic tracer measurements e g caesium 137 walling et al 2003 remote sensing e g airborne imagery fischer et al 2018 and field based methods e g measuring rills and gullies takken et al 1999 provide some understanding of the dominant process domains contributing to soil erosion but they are constrained by spatial scale accuracy and the repeatability of measurements the disconnect between process domains that are measured and modelled is one of the biggest obstacles for testing and validating model predictions of soil erosion for example consider an outlet based method of measuring sediment yield for evaluating model predictions of soil loss the sediment yield subsumes all processes in a catchment i e all sediment sources and delivery mechanisms to produce a single aspatial number even if the modelled results accurately predict sediment yield at the catchment outlet the issue of equifinality cannot be addressed are the models getting the right answer for the right reasons without a spatial component included in the evaluation process the model may have a completely erroneous representation of the internal catchment dynamics while still producing a correct outlet response van oost et al 2005 the challenges of equifinality can only begin to be addressed if distributed data is used to ensure the model is behavioral jetten et al 2003 unfortunately spatial measurements of soil erosion are labor intensive expensive and time consuming as such they are seldom used for evaluating models morgan and nearing 2011 stroosnijder 2005 concludes a critical review on soil erosion measurements stating there is a crisis in erosion measurements p 172 due to poor empirical data and no new developments in technology for measuring soil erosion the democratization of unmanned aerial vehicles uavs also known as remotely piloted aircraft systems rpas and automated photogrammetric workflows i e structure from motion sfm herald a new advancement in remote sensing technology for measuring distributed soil erosion rates the potential of these novel technologies to accurately measure soil erosion is recognized in literature but they have yet to be used for testing erosion models batista et al 2019 the very high resolution 5 cm frequent and event based quantitative measurements of distributed erosion from uavs meinen and robinson 2020a can be used to create a time series of erosion data at a spatial scale that has not been previously achievable these data provide an opportunity to evaluate soil erosion models in a spatially distributed manner at the scale in which agricultural decisions take place i e farm field in addition to quantitative measurements optical imagery of erosional features can be used as an additional soft qualitative datapoint for model evaluation jetten et al 1999 while there still remain challenges in creating high fidelity data with sfm for change detection measurements meinen and robinson 2020b the combination of qualitative and quantitative erosion measurements provides a strong baseline for evaluating the performance of erosion models to advance the field of erosion modelling we collected novel very high resolution time series data using a uav for the purpose of quantifying semi distributed rates of soil erosion over an entire year we used these data to determine what is the accuracy of an empirically based universal soil loss equation usle wischmeier and smith 1978 and a process based water erosion prediction project wepp flanagan and nearing 1995 erosion model at estimating soil erosion rates on an agricultural field in southwestern ontario canada the performance of the usle and wepp were evaluated on both an annual and sub annual basis against uav based measurements of soil erosion 2 materials and methods 2 1 study site our study site is an agricultural field located in the upper reaches of the nith watershed ontario canada the nith watershed spans an area of 1130 km2 with 80 of its total land area devoted to agricultural production loomer and cooke 2011 the nith watershed is characterized by high rates of runoff and pollutant loading during the spring freshet loomer and cooke 2011 with peak precipitation occurring in the latter half of the growing season the nith watershed has been identified as a priority sub watershed of the grand river watershed due to its large contribution of phosphorus nitrogen and suspended sediments to the grand river holeton 2013 the grand river flows southwards into lake erie which continues to face the harmful effects of eutrophication and algae blooms caused by elevated levels of phosphorus originating from diffuse agricultural sources michalak et al 2013 the agricultural field used for measuring modelling and evaluating predictions of soil erosion is part of a heterogeneous agricultural system that is uniquely characterized by snowmelt and a dense tile drainage network the field is a steep 15 9 ha tile drained agricultural field under a 3 year crop rotation of corn soybean and winter wheat fig 1 which is a common rotation for southern ontario the chosen study year for field work spans from corn cultivation in 2018 to the spring of 2019 the field management regime consists of a fall moldboard plow following corn and oats a spring cultivator for seedbed preparation for corn and soybeans no till winter wheat and broadcast oats as a cover crop after winter wheat fig 1 the study site had tile drainage installed in the winter of 2017 2018 the tile installation was accompanied by an installation of soil berms and surface inlets at six locations that form catch basins to trap eroded sediments fig 2 soils on the study site are texturally classified as a mix of sandy loam and loam 2 2 data our data collection is focussed on six distinct drainage basins with soil berms and surface inlets i e catch basins outlined in fig 2 i e basins a b c d e and f for measuring and evaluating model estimates of soil loss by water erosion all field work was carried out by meinen and robinson 2020a from may 17 2018 to may 16 2019 when the study site was under corn production a comprehensive uav surveying methodology for the study site can be found in meinen and robinson 2020a with a discussion on uav flight design in meinen and robinson 2020b 2 2 1 field work the modelling of erosion in agricultural systems requires a representation of the following five factors climate soils topography vegetation and land management our climate data consists of hourly precipitation data collected from a meteorological station wellesley dam located 7 km from the study site soil data included average soil texture measured with a sieve and hydrometer analysis on 27 soil samples 15 cm depth menzies pluer et al 2020 and bulk density measured using bulk density rings on 16 soil samples 15 cm depth that were dried in a conventional oven at 105 celsius for 24 h and weighed topographic data was calculated from a digital elevation model dem created with optical imagery collected from a uav and sfm the raw data were collected at a resolution of 0 02 m and used to create a 0 02 m dem which was resampled to generate a 0 8 m dem for catchment discretization and resampled to 3 2 m for calculating primary terrain variables i e hillslope length width and slope all topographic calculations were done using arcgis v10 6 1 cropping and management practices were provided by the landowner fig 1 and crop stages were determined from in situ site surveys to model erosion using the usle and wepp our six study basins fig 2 were discretized into rectangular hillslopes table 1 and segmented into 3 5 overland flow elements to capture the hillslope shape rectangular hillslopes were segmented such that a hillslope constituted the point from where overland flow began to where either the slope gradient decreased such that deposition began or where the runoff entered a concentrated flow channel e g an ephemeral gully 2 2 2 uav erosion measurements the creation of a dataset for evaluating model estimates of soil erosion involved using a novel remote sensing approach to monitor surface change in each catch basin with optical imagery collected from a uav and sfm meinen and robinson 2020a across the study year a total of nine surveys were conducted to recreate the topography of each catch basin and to generate orthomosaics fig 3 the m3c2 algorithm lague et al 2013 0 15 m projection vertical normals was used in cloudcompare v2 9 1 https www danielgm net cc to calculate surface change i e deposition in each of the six catch basins using pointcloud data volumetric surface change was converted to a weight using an average measured soil bulk density of 1 32 g cm3 across the entire study year a total of 159 52 tonnes of sediment was deposited across the six catch basins table 2 corresponding to an erosion rate of 18 83 t ha 1 yr 1 2 3 erosion modelling 2 3 1 universal soil loss equation usle research on soil erosion in north america began in the early 20th century e g zingg 1940 smith 1941 musgrave 1947 and was accelerated after franklin roosevelt helped pass the soil conservation act of 1935 public law 74 46 to fulfil the requirements of this act the united states department of agriculture and the newly created soil conservation services developed the universal soil loss equation usle in the 1950 s as a tool to predict soil loss and help farmers with conservation planning the usle is a lumped empirical field scale model that predicts soil loss from rill and inter rill erosion based on 10 000 plot years of erosion data the usle was originally published in agricultural handbook no 282 wischmeier and smith 1965 and adopted widely based on the superseding publication in agricultural handbook no 537 wischmeier and smith 1978 the usle is expressed as a n 1 6 r k l s c n p where a is the soil loss per unit area t ha 1 yr 1 which is the sum of the products of six factors for six cropstages n whereby r is a rainfall and runoff factor mj mm ha 1 h 1 yr 1 k is a soil erodibility factor t ha h ha 1 mj 1 mm 1 l is a slope length factor unitless s is a slope steepness factor unitless c is a cover and management factor unitless and p is a supporting practice factor unitless wischmeier and smith 1978 these six factors combine human decision making and the predominant natural processes contributing to soil loss in an agricultural landscape the usle is the de facto standard for erosion modelling due to its simplicity and ease of use the usle and its many derivations e g revised usle renard et al 1991 modified usle williams 1975 are incorporated into erosion estimates in a wide variety of models areal nonpoint source watershed environment response simulation answers beasley et al 1980 field scale model for chemicals runoff and erosion from agricultural management systems creams knisel 1980 erosion productivity impact calculator epic williams 1989 agricultural non point source model agnps young et al 1989 soil and water assessment tool swat arnold 1994 sediment delivery distributed model sedd ferro and porto 2000 and the water and tillage erosion model sediment delivery model watem sedem van rompaey et al 2001 for this study all usle calculations are based on agricultural handbook no 537 wischmeier and smith 1978 the r factor was calculated using hourly precipitation data from the wellesley dam meteorological station using the follow equation wischmeier and smith 1978 r i 1 m e i 30 i where ei30 is the rainfall erosivity of a single storm event mj mm ha 1 h 1 i calculated as the total kinetic storm energy e mj ha 1 times the maximum 30 min rainfall intensity i30 mm h 1 for all storms in a given year m storms are only included in the calculation of r that exceed 12 7 mm of rainfall in a 6 hour period an additional sub factor rs can be included to account for the effects of winter runoff and snowmelt on soil loss we chose to exclude rs from our soil loss calculations since we observed in the previous year that snowmelt produced no observable amount of erosion fig 3a since the usle requires 30 min rainfall intensities for calculating the i30 our calculated r factor likely underpredicted storm intensity to correct for this underprediction we used a relationship developed by panagos et al 2015 to convert from 60 min intensities to 30 min intensities by multiplying our r factor by 1 5597 the k factor was calculated using field based measurements of soil texture i e 34 82 silt 13 27 clay and estimates of edaphic properties i e 2 organic matter b 2 c 3 based on the following equation for soils containing less than 70 silt and very fine sand wischmeier and smith 1978 100 k 2 1 m 1 14 10 4 12 a 3 25 b 2 2 5 c 3 0 1317 where k is the soil erodibility factor t ha h ha 1 mj 1 mm 1 m is the soil particle size parameter based on soil texture a is percent organic matter b is the soil structure code and c is the profile permeability class organic matter in the topsoil on the study site was estimated to be lower than regional averages 4 due to a tile drainage installation in the previous winter which inverted a large portion of the soil column the l and s factors are typically jointly calculated as the following wischmeier and smith 1978 l s λ 22 13 m 65 41 s i n 2 θ 4 56 sin θ 0 065 where ls is the slope length factor unitless λ is slope length m θ is the angle of the slope degrees and m is an exponent based on slope gradient see table 1 for hillslope topographic attributes the c factor and corresponding soil loss ratios slrs were calculated using the tables provided in agricultural handbook no 537 for high productivity corn with a fall moldboard plow table 3 no additional management practices were represented on our study site so the p value was initialized to a value of 1 unitless representing no net effect on soil erosion 2 3 2 water erosion prediction project wepp in 1985 the united states department of agriculture started research on a process based successor to the usle the water erosion predict project wepp laflen et al 1991 flanagan and nearing 1995 the wepp was developed using data from 50 experimental cropland laflen et al 1991 and rangeland plots gilley et al 1990 to accurately model the underlying hydrologic processes that contribute to soil erosion these processes are represented in the model using stochastic weather generation infiltration theory hydrology soil physics plant science hydraulics and erosion mechanics flanagan and nearing 1995 p 1 1 the wepp is a daily simulation model that computes soil loss and sediment delivery from rill and inter rill erosion for individual hillslopes or small watersheds flanagan and nearing 1995 the wepp erosion routine is based on a steady state sediment continuity equation that describes the movement of soil in rills d g d x d f d i where x is the distance downslope m g is the sediment load kg s 1 m 1 df is the rill erosion rate kg s 1 m 2 and di is the inter rill sediment delivery to rills kg s 1 m 2 erosion is conceptualized as a series of inter rill areas that deliver sediment to concentrated flow channels i e rills that is either deposited in the rill or transported off the hillslope for this study wepp model calculations are based on wepp version 2012 800 using the windows interface table 4 input parameters for modelling cligen was used to convert daily climate data i e precipitation and temperature into more temporally detailed distributions for soil loss and runoff calculations 2 4 analysis we focus on the six distinct drainage basins with soil berms and surface inlets i e basins a b c d e and f fig 2 for evaluating model estimates of soil erosion for the purposes of this study we define the rate of soil erosion as soil loss by water erosion to catch basins per unit area usle results are computed as soil loss per unit area i e soil loss from an individual hillslope and wepp results are computed as sediment delivery per unit length i e kg m 1 and multiplied by the hillslope width to convert into soil loss per unit area for comparing modelled results both model outputs reflect soil loss by water erosion from a hillslope to the lowest point in the catchment i e the catch basin the usle and wepp were used to estimate rates of soil erosion in each of the six catch basins on an annual and sub annual i e seasonal basis for evaluating the usle and wepp we conducted a blind evaluation batista et al 2019 p 4 whereby the usle and wepp were parameterized and soil erosion estimates were compared to uav based measurements of soil erosion for the 2018 2019 study year no calibration procedure was conducted measured rates of soil erosion were compared to model estimates of soil erosion for all six catch basins and evaluated based on the absolute magnitude of error percent error and nash sutcliffe model efficiency nse nash and sutcliffe 1970 n s e 1 i 1 n q m q p 2 i 1 n q m q m 2 where qm is the measured soil loss q m is the mean of measured soil loss and qp is the modelled soil loss nse values range from to 1 with a nse of 1 corresponding to a perfect match between measurements and model predictions a nse of 0 indicates that the modelled soil loss is as accurate as the mean of the measurements and a nse of less than 0 indicates that the mean of measurements is a better predictor than the erosion model for comparing the percent error between measured and modelled results we assume the erosion model has reached the upper limit of predictive accuracy if results are within 20 of measured values based on the average coefficients of variation from the replicate erosion plots of wendt et al 1986 uav based measurements of soil erosion are assumed accurate for the purposes of model evaluation but we include the uncertainty metrics of meinen and robinson 2020a in our results measurements of soil erosion had an average uncertainty of 29 based on a 0 04 m confidence interval for m3c2 surface change calculations soil erosion measurements likely underestimate actual erosion rates since sediment was lost into the surface inlets i e the depositional plumes in the catch basins used for measuring soil erosion rates were not representative of all the eroded sediments the exact volumetric amount of sediment lost into the subsurface drainage network is not known 3 results 3 1 model evaluation erosion model estimates were compared to uav based erosion measurements for each of the six study basins on both an annual table 5 and sub annual basis table 6 on an annual basis the total cumulative erosion rate i e soil loss by water erosion to catch basins per unit area for all six study basins was measured at 18 83 t ha 1 yr 1 the usle overestimated the erosion rate at 26 23 t ha 1 yr 1 whereas the wepp underestimated the erosion rate at 16 41 t ha 1 yr 1 table 5 assuming a natural stochastic variation of 20 in soil erosion rates i e 3 77 t ha 1 yr 1 only the aggregated results of wepp are within the upper limit of annual predictive accuracy at the semi distributed basin scale both the usle and wepp exhibited a higher range of variability with errors exceeding 20 when compared to annual uav measured values table 5 the absolute average magnitude of error of soil erosion estimates for the six catch basins were 7 36 t ha 1 yr 1 and 4 09 t ha 1 yr 1 for the usle and wepp respectively wepp estimates of soil erosion were more accurate than the usle for each basin with the exception of basin d the soil erosion rate of basin d was unexpectedly high considering its moderate slope and small size the five others basins had a closer agreement between measured and modelled soil loss using the wepp model and fell within or close to the upper limit of predictive accuracy the usle had a strong tendency to overestimate soil erosion rates for the study site whereas the wepp had a moderate underestimation of soil erosion rates sub annual estimates of soil erosion table 6 compared to uav measured values showed a much larger discrepancy between models than was observed at the annual temporal scale the total cumulative erosion rate for all six study basins showed a strong seasonal dependency with a measured soil loss of 16 08 t ha 1 0 00 t ha 1 and 2 73 t ha 1 for the growing season mature crop stubble and fallow winter periods respectively the usle estimated a soil loss of 20 06 t ha 1 0 87 t ha 1 and 5 30 t ha 1 while the wepp estimated a soil loss of 14 06 t ha 1 0 02 t ha 1 and 2 34 t ha 1 for the three respective periods the aggregated i e field scale sub annual results of the wepp are within the upper limit of predictive accuracy i e 20 measured values while the usle had a moderate overestimation of soil loss for all three periods at the semi distributed basin scale table 6 the usle and wepp once again exhibited a high range of variability with the wepp outperforming the usle for sub annual estimates the absolute average magnitude of error for the six catch basins for the three respective time periods was 5 33 t ha 1 0 65 t ha 1 and 2 47 t ha 1 for the usle overall 2 82 t ha 1 and 4 18 t ha 1 0 01 t ha 1 and 1 08 t ha 1 for the wepp overall 1 76 t ha 1 the usle had a sub annual nse of 0 80 exhibiting an overestimation of soil loss for most basins while the wepp had a sub annual nse of 0 96 with a close agreement between uav measured values and estimated soil loss 4 discussion 4 1 model evaluation an ongoing challenge in erosion modelling is recognizing the scale dependency of input parameters and ensuring that a model is correctly parameterized at the spatial e g hillslope field watershed and temporal scale e g annual sub annual of interest the model user makes the subjective choice on the spatial and temporal scales to measure estimate and interpolate input parameters given the same objectives different users will likely conceptualize and parameterize an agricultural system differently a simple blind model evaluation whereby an erosion model is parameterized run and compared to real measurements of soil loss increases the model users confidence that the correct parameter set was chosen and that the model is appropriate for the climate soils topography cropping system and land management practices of the system being studied the semi distributed model evaluation of the usle yielded accurate relative estimates of soil loss e g basin a was responsible for approximately 43 45 of total annual soil loss but overestimated soil loss during each cropstage and subsequently overestimated annual soil loss nse 0 56 analysis of the cropstage usle results demonstrated that this overestimation may have been a result of inadequate model input data rather than model error the i30 calculated for the establishment and fallow cropstages was likely too high the hourly rainfall intensities converted to 30 minute intensities did not correctly characterize the intensity of the most erosive storms the less accurate short term estimates produced by the usle are likely expected by practitioners with experience using the usle whereby long term average i e 20 22 years estimates of soil loss are expected to be more accurate however the usle was originally intended to be used for both long term and short term predictions of soil erosion wischmeier and smith 1978 state in handbook no 537 that with appropriate selection of its factor values the equation will compute the average soil loss for a multicrop system for a particular crop year in a rotation or for a particular cropstage period within a crop year p 3 with the caveat that the equation will be substantially less accurate p 4 for predicting individual storm events compared to long term averages the structure of the usle does not explicitly include runoff or seasonal temperature changes so short term i e cropstage 1 year annual predictions of soil loss in regions characterized by high temporal variability in temperatures with intermittent snowmelts will likely be worse than long term predictions while the usle includes a subfactor for thaw and snowmelt rs since erosion plots used in the model development were located in the midwest and pacific northwest united states the relationship is a single multiplicative factor of 1 5 times the december through march precipitation this relationship may relate well to the conditions of the pacific northwest whereby as much as 90 percent of the erosion on the steeply rolling wheat land has been estimated to derive from runoff associated with surface thaws and snowmelt wischmeier and smith 1978 p 7 but does not necessarily relate to the conditions of other winter climates we intentionally chose to exclude the rs sub factor from our study based on observations from the previous year 2017 2018 winter where snowmelt produced no observable amount of erosion fig 3a including the rs sub factor would have increased our error in the fallow cropstage from 95 to 268 the overestimation of winter soil loss may have also been exacerbated as a result of the soil loss ratio being too high for the stubble and rough fallow cropstages since frozen soils mediated erosional processes and snow melt produced very little soil erosion our semi distributed evaluation of the wepp provided insights into the applicability of the wepp for the conditions of our agricultural study site the wepp had accurate annual nse 0 97 and sub annual predictions nse 0 96 indicating that we correctly parameterized the model with our in situ measurements estimations and the parameters that we let wepp calculate from its database the wepps process based modelling structure and incorporation of temporally distributed management practices e g tillage plant science e g crop growth and hydrology e g snowmelt and associated runoff on a daily timestep allowed it to accurately model the seasonal dynamics of southwestern ontario although the wepp had a tendency to underpredict soil erosion rates aggregated field scale estimates of soil erosion were within the upper limit of predictive accuracy while our uav based evaluation approach and application to the climatic conditions in southwestern ontario are novel the general findings of our study corroborate existing literature both the usle and wepp model evaluations demonstrated that 1 annual estimates of soil erosion are more accurate than sub annual estimates e g jetten et al 1999 and 2 aggregated field scale estimates of soil erosion are more accurate than individual hillslope or basin estimates e g jetten et al 2003 model error increases as the spatial resolution of predictions becomes finer the stochastic variation in soil erosion rates is most pronounced at fine spatial and temporal scales e g hillslopes coarse scales e g full farm fields allow the model to have an averaging effect whereby overestimations and underestimations balance each other out the usle and wepp have been found to have similar annual predictive capabilities tiwari et al 2000 laflen et al 2004 albeit with a high range of error at shorter time scales the wepp was specifically designed to replace the usle by improving short term soil loss estimates by a process based representation of climate and hydrology flanagan and nearing 1995 our study corroborates that the wepp outperforms the usle at shorter time scales i e 1 year annual and sub annual at the farm field scale 4 2 erosion modelling and best management practices bmps there often exists a disconnect between the spatial scale at which best management practices bmps are operationalized and the spatial scale at which they are prescribed by environmental modellers to illustrate this disconnect we conducted a literature review using google scholar with the keywords erosion model agriculture and bmp thirty one of the first one hundred reviewed papers were relevant and contained a bmp case study whereby 14 45 2 used the swat model for bmp assessments 4 12 9 used usle rusle or rusle2 3 9 7 used agnps 2 6 5 used wepp and the remaining papers all employed a unique model twenty three 74 2 case studies used a combination of sediment yield discharge and nutrient measurements at catchment outlets for model evaluation while the remaining 8 studies 25 8 contained no model evaluation twenty one 67 7 studies used a hydrologic response unit hru i e a catchment with homogeneous land use soil and topography discretization for bmp implementation 5 16 1 used a raster based discretization 2 6 5 were unspecified and the remaining 3 studies 9 7 used farm fields or hillslopes for discretization wepp hillslopes abaci and papanicolaou 2009 agnps farm fields as individual cells yuan et al 2008 swat farm field as hru santhi et al 2006 while bmps are implemented at the farm field scale to mitigate soil loss the most common environmental modelling approach was to prescribe them at the scale of the hru and evaluate modelling results with point based measurements of sediment and discharge at watershed outlets e g bicknell et al 1985 moore et al 1992 bracmort et al 2006 rao et al 2009 betrie et al 2011 rousseau et al 2013 smith et al 2014 briak et al 2019 ricci et al 2020 while this approach is pragmatic for bmp studies comprising large spatial extents there is no evaluation or understanding of the hydrologic and sedimentologic processes at the most important spatial scale the individual farm field to facilitate model up scaling from individual farm fields to watersheds we advocate that environmental modellers discretize watersheds as a collection of individual farm fields and coalesce outlet and field based evaluations for a more holistic evaluation this two fold evaluation will allow the modeller to have a more in depth understanding of the dominant sediment transport processes from fields to waterways where bmps are implemented i e field scale evaluation and from waterways to catchment outlets i e outlet based evaluation this ensures the model is valid for a certain set of physiographic and climactic conditions addresses issues of model equifinality and provides insights into model uncertainty only using an outlet based evaluation may lead to a poor model implementation e g equifinal model and the implementation of ineffective bmps that can cause economic loss and degrade farmer confidence in governance and scientific modelling 4 3 field scale best management practice evaluation the uav based approach in this study was used to evaluate the applicability of the usle and wepp for a single agricultural field in southwestern ontario the evaluation demonstrated that the wepp was able to model the temporal variation in erosion rates whereas the usle had some challenges representing temporally distributed soil erosion rates table 6 based on this evaluation the wepp should be an effective tool for evaluating field scale bmps our wepp modelling results and field scale measurements demonstrate that the most effective bmp would be one that incorporates additional biomass into the soil during the early growing season this can be achieved by switching from a conventional fall plowing implement i e moldboard plow fig 1 to a conservation plowing implement i e chisel plow chisel plows have long plow shanks typically spaced 30 cm apart with sweeps shovels or chisel points that break up and stir the soil but do not invert the topsoil like a traditional moldboard plow this leaves a rough soil surface with sufficient biomass to cover the soil during the winter early spring and after the secondary tillage in the spring for final fitting when the site is most susceptible to erosion to evaluate the effects of switching tillage practices as a bmp to reduce long term soil loss on our study site we used the wepp model to estimate 10 year average 2005 2014 soil erosion rates for 1 a moldboard plow 2 a chisel plow with coulters and shovels and 3 a chisel plow with coulters and sweeps the fall moldboard plow buries 95 of crop residue whereas the chisel plow will only bury 30 55 of crop residue 55 with coulters and shovels 30 with coulters and sweeps studies demonstrate comparable yields between moldboard and chisel plowing for corn soybean and wheat albeit with small variations in yield depending on nutrient amendments singer et al 2004 and herbicide applications buhler 1992 making it a pragmatic bmp choice for ameliorating accelerated rates of soil erosion the wepp estimated the 10 year average erosion rate of the full 15 9 ha study at 8 12 t ha 1 yr 1 when using a fall moldboard plow replacing the moldboard plow with a chisel plow with coulters and shovels for the fall tillage reduced soil erosion by 43 resulting in a 10 year average erosion rate of 4 65 t ha 1 yr 1 table 7 supplementary fig 1 replacing the moldboard plow with a chisel plow with coulters and sweeps for the fall tillage reduced soil erosion by 64 resulting in a 10 year average erosion rate of 2 91 t ha 1 yr 1 table 7 supplementary fig 1 for all plowing implements the wepp estimated that the majority of long term soil erosion occurred during the early growing season with an average of 6 4 days per year with soil erosion winter events and snowmelt constituted 70 of the average long term runoff with 14 1 days per year with runoff but winter runoff events were rarely associated with soil loss supplementary figs 1 and 2 these results were consistent with our in situ observations and measurements during the 2018 2019 study year considering the regional context of our study site in that 68 1 of farms in the local county are using a conventional tillage system in their crop rotation e g moldboard plow statistics canada 2016 the transition to a conservation tillage system e g chisel plow could contribute to the amelioration of water quality issues in the region originating from sediment and particulate phosphorus losses for our farm the wepp model estimated a 64 long term reduction in soil loss when switching from a moldboard plow to a chisel plow with coulters and sweeps while no till management techniques would likely further reduce soil loss no till management practices can increase the risk of dissolved reactive phosphorus runoff king et al 2015 via macropore flow to subsurface tile drainage dissolved reactive phosphorus is readily bioavailable for biota uptake and is the limiting nutrient for primary production in adjacent aquatic systems i e lake erie since the majority of the upper nith watershed is tile drained loomer and cooke 2011 permanent no till systems are not recommended a conservation tillage with a chisel plow removes the macropore connectivity of soils with subsurface drainage lines while still ensuring there is biomass on the field surface to protect against the impacts of rainfall when the field is most susceptible to erosion 5 conclusions in this study we used semi distributed erosion measurements with a uav and sfm to evaluate the applicability of the usle and wepp to conditions in southwestern ontario canada while both models had satisfactory results the usle had a tendency to overestimate soil loss for each season which may have been the result of an incorrect characterization of rainfall intensity with our methodology in contrast to the usle our model evaluations highlighted that the process based modelling structure of the wepp modelled the hydrology of our study site correctly e g erosion from snowmelt and runoff and was able to accurately model soil loss at an annual and sub annual time step for both models model error tended to increase at shorter time scales and small spatial extents annual aggregated field scale estimates of soil erosion were more accurate for both models we strongly advocate that uav based model evaluations be conducted more commonly to ensure erosion models are behavioral before evaluating new bmps and before scaling out models to larger spatial extents uav based approaches collect the necessary qualitative and quantitative erosion measurements for model testing at the scale of the agricultural decision maker and can be used to inform models a priori future research should be aimed at improving the accuracy of the sfm mvs workflow for change detection such that fully distributed model evaluations can be conducted declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to acknowledge with gratitude infrastructure funding provided by the canadian foundation for innovation 32247 the ontario research fund flir systems the national sciences and engineering research council nserc of canada 06252 2014 547496 2020 as well as the ministry for research and innovation for support through the early researcher award er15 11 198 and the faculty of environment at the university of waterloo for student funding lastly we appreciate and thank the farming family who would like to remain anonymous for their collaboration in the work and for granting us access to their land appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 104962 
25879,water exchange between the surface and subsurface is important for both water resource management and environmental protection in this paper we develop coupled surface and subsurface flow simulation capability in a parallel subsurface flow and reactive transport code pflotran we sequentially couple the diffusion wave based surface flow with the subsurface flow governed by the richards equation in pflotran these two flow domains are linked with a boundary condition switching method that ensures the continuity of pressure and flux at the surface subsurface interface we verify the coupled code against other existing hydrologic models and observation data using a number of numerical experiments the coupled hydrological model exhibits good performance in strong parallel scaling tests the new coupled surface and subsurface simulator significantly advance community simulation capability towards improving integrated hydrologic and biogeochemical understanding of complex systems such as watersheds and river corridors keywords surface flow integrated hydrological modeling boundary condition switching parallel computing 1 introduction surface and subsurface hydrologic systems are linked compartments of a hydrologic continuum accurate representation of the interaction between those systems is an important component of modeling the integrated hydrologic cycle for watershed systems sophocleous 2002 winter 2001 surface water and groundwater exchange has captured increasingly more attention due to its significant impact on biogeochemical processes in natural and engineered systems harvey and gooseff 2015 harvey et al 2019 song et al 2018 goldman et al 2017 graham et al 2019 increased capabilities in measurements both direct and indirect including satellite based observations have been accompanied by improved numerical methods and computing capacity to allow for greatly improved understanding of this highly transient and heterogeneous water mass flux maxwell et al 2014 kollet et al 2010 2017 sulis et al 2010 over the last couple decades several approaches have been developed for integrated surface and subsurface hydrological modeling furman 2008 kampf and burges 2007 we summarize the existing integrated surface subsurface hydrological models in terms of the coupling strategy and interface boundary conditions in table 1 these models have generally differed in the formulation of the governing equations the choice of surface subsurface interface boundary conditions and the method of linking surface and subsurface flow most of these approaches rely on some form of the hydrostatic shallow water equations representing surface flow routing coupled to some modified form of the richards equation for subsurface flow in variably saturated porous media the shallow water equations for the surface flow are solved in either 1d or 2d while the richards equation is commonly solved in either 1d vertical or 3d boundary conditions at the surface subsurface interface include first order exchange flux boundary condition switching and continuity of pressure and flux there are three different strategies to solve the coupled system of equations sequential non iterative asynchronous linking sequential iterative and fully coupled huang and yeh 2009 compared those coupling strategies and discussed their advantages and disadvantages full coupling is the most robust and accurate solution scheme but it forces the same time steps for surface and subsurface flows which often leads to long simulation time sequential non iterative or iterative coupling allows for different time steps for surface and subsurface flow usually imposing several sub steps for surface flow within each subsurface time step to balance between computational needs and solution accuracy the iterations between the surface and subsurface solvers improve solution accuracy another advantage of sequential coupling is to account for micro topography features that may have important effects on surface flow generation and subsurface flow and transport characteristics especially at small spatial scales camporese et al 2010 frei and fleckenstein 2014 moreover sequential coupling affords the flexibility to include more processes such as reactive transport thermal transport and microbes mediated biogeochemical reactions in different ways that are suited for the surface and subsurface domains kollet et al 2017 and maxwell et al 2014 conducted detailed inter comparisons between sequentially and fully coupled integrated hydrological models on a number of complex heterogeneous synthetic and real world cases they found that the model disagreement is primarily caused by the assumption of 1d unsaturated subsurface flows and reliance on the kinematic wave approximation in surface flows while the coupling strategy only plays a secondary role setting the boundary condition at the surface and subsurface interface is part of the coupling strategy first order exchange fluxes and pressure continuity are widely used in the community a boundary condition switching approach bixio et al 2000 has been developed to resolve the coupling without introducing new parameters to represent an exchange process or an interface property while preserving the continuity of fluxes and pressure head at the interface the boundary condition switching algorithm changes the type of boundary condition at the surface subsurface interface i e exchange fluxes or pressure continuity according to the surface ponding status it has been applied and tested in richards equation based subsurface models such as cathy camporese et al 2010 dagès et al 2012 sulis et al 2010 and gcs flow le et al 2015 integrated hydrological models have been employed at scales ranging from hillslopes to catchments to watersheds and more recently at the scale of continents maxwell et al 2015 high performance computing is essential for dealing with massive computational resources needed for high resolution and long term simulations especially at the larger scales hwang et al 2014 vivoni et al 2011 fatichi et al 2016 paniconi and putti 2015 several distributed hydrological models have been developed in parallel computing environments that make use of high performance computing these simulators are largely based on the message passing interface mpi standard there are also simulators that are based on the open multi processing openmp hwang et al 2014 or graphics processing units gpus anagnostopoulos et al 2015 le et al 2015 while openmp and gpu based models can be applied for problems composed of millions of degrees of freedom dofs mpi based models are necessary to address larger problems with up to billions of dofs since the memory requirements exceed what is available on a single node this has led to a continued reliance on mpi based models for large scale hydrologic analyses kollet et al 2010 vivoni et al 2011 in this study we implement sequential non iterative coupling approach with the boundary condition switching scheme to develop integrated hydrologic simulation capability in pflotran pflotran is an open source massively parallel multi physics simulator hammond et al 2014 developed and distributed under a gnu lgpl license and is freely available at www pflotran org open source simulators have become the trend for conducting multidisciplinary science through sharing and learning with the broader scientific community yu et al 2016 the open sharing culture attracts interested researchers and scientists to develop and use the simulators in highly collaborative and productive ways pflotran solves a system of generally nonlinear partial differential equations describing reactive multiphase flow and multicomponent biogeochemical transport in porous media pflotran has been developed from the ground up for parallel scalability and has been run on up to 2 18 processor cores with problem sizes up to 2 billion degrees of freedom lichtner et al 2017 since its inception pflotran has been applied widely to reactive flow and transport hammond and lichtner 2010 hammond et al 2011 lichtner and hammond 2012 data assimilation chen et al 2012 2013 hydrogeophysics johnson et al 2015 2017 carbon sequestration navarre sitchler et al 2013 ice modeling karra et al 2014 biogeochemical modeling tang et al 2016 and as a part of the community land model clm bisht et al 2017 among other applications our work here fills a critical capability gap for pflotran in surface flow processes thus significantly advancing community simulation capability towards improving integrated hydrologic and biogeochemical understanding of complex systems such as watersheds and river corridors graham et al 2019 this paper is organized as follows in section 2 we introduce the underlying design principles of the integrated capability including boundary condition switching approach for coupling the surface and subsurface flows the governing equations for each flow system and the space and time discretization and numerical implementation for solving the two systems of equations in section 3 we verified the new simulation capability in simulating standalone surface flows and integrated surface and subsurface flows using five test cases with known results from other well established simulators and observed data we also evaluated the parallel performance of the integrated model in section 3 concluding remarks are provided in the final section section 4 2 methodology 2 1 surface flow equation surface flow is often characterized by either the kinematic wave kw equation or the diffusion wave dw equation to approximate the saint venant equation the kw equation has problems with the flat terrain since it does not redistribute water on flat terrain on the other hand the dw equation for surface flow has been shown to provide more accurate results with computational efficiency the diffusion wave approximation is often written as 1 h t v h q r q e x where h m represents the surface water depth v m s 1 is surface water velocity q r m s 1 is rainfall rate and q e x m s 1 represents exchange fluxes between surface and subsurface domains the dw approximation allows for diffusion of a surface water wave by incorporating the pressure differential term it is also suitable for shallow water flow simulation on irregular topography panday and huyakorn 2004 painter et al 2016 the backwater effects can be accounted for by inclusion of a downstream boundary condition two types of boundary conditions at the outlet are usually used zero depth gradient zdg or critical depth cd outlet conditions manning s equation gottardi and venutelli 1993 is often used to establish a flow depth discharge relationship and the surface water velocity v is written as follows 2 v h 2 3 n m a n s s h z where n m a n m 1 3 s is the manning s coefficient s s is the slope of the water surface and z m is the ground elevation here we assume that v equals 0 when s s is less than 10 6 s s is computed with the dw approximation and written as 3 s s h z x 2 h z y 2 2 2 subsurface flow equation pflotran solves the variably saturated flow in the subsurface using the three dimensional 3 d richards equation richards 1931 the mixed form of the richards equation proposed by celia et al 1990 yields robust numerical solutions and maintains mass balance for unsaturated flow problems 4 t ϕ s η η q q w here t s is time ϕ is the porosity of soil matrix s m 3 m 3 is the water saturation η k m o l m 3 is the molar water density q m s 1 is the darcy flux and q w k m o l m 3 s 1 represents a source positive or a sink negative of water the darcy flux q is calculated following the darcy s law 5 q k k r μ p w w η g z here k m 2 is the intrinsic permeability k r is the relative permeability μ k g m 1 s 1 is the water viscosity p p a is the pressure w w k g k m o l 1 is the formula weight of water g m s 2 is the acceleration of gravity and z m is the vertical component of the position vector the water density and viscosity are computed as a function of temperature and pressure through an equation of state for water pflotran provides several relationships between capillary pressure and saturation such as the van genuchten van genuchten 1980 and brooks corey brooks and corey 1964 types supported relative permeability functions for the richards equation include the burdine 1953 and mualem 1976 types in this study we combine van genuchten s soil water retention curve and mualem s relative permeability function to calculate water saturation and relative permeability for the subsurface flow these constitutive relations are further described in appendix a 2 3 discretization and numerical implementation standard upwind cell centered finite volume spatial discretization patankar 1980 is used in both the surface and subsurface flows the spatial discretization applied in pflotran ensures local mass conservation because the exchange fluxes occur between the center of a surface grid cell and a subsurface grid cell fig 1 it can also avoid the error from the node to cell interpolation of the exchange fluxes at the surface subsurface interface that has been reported by fiorentini et al 2015 within cathy the governing partial differential equation for mass conservation in the surface and subsurface flow domains can be written in the general form as 6 t a f q with an accumulation term a a flux term f and a source sink term q integrating over a representative elementary volume rev corresponding to the n t h grid cell with volume v n and cell boundary v n yields 7 d d t v n a d v v n f d v v n q d v the accumulation term has the finite volume form 8 d d t v n a d v a n t δ t a n t δ t v n with time step δ t the flux term can be expanded as a surface integral using the gauss s theorem 9 v n f d v v n f d s n f n n s n n where the latter finite volume form f n n is based on the two point flux approximation where the sum over n involves all the nearest neighbor grid cells connected to the n t h grid cell with interfacial areas s n n the finite volume discretization for the source sink term has the form of 10 v n q d v q n v n where q n is the source sink term located within the n t h grid cell the residual function r n for the governing equation is given by 11 r n a n t δ t a n t δ t v n n f n n t δ t s n n q n v n for the surface flow system an explicit time scheme forward euler time integration scheme is used and the time step is limited by a stability condition the subsurface flow system uses a fully implicit solution scheme and is solved using an iterative solver based on the newton raphson method the detailed method of solution can be found on the pflotran website documentation pflotran org 2 4 coupling strategy and interface boundary condition we implement a sequential non iterative approach within pflotran using an improved boundary condition switching algorithm to couple the surface and subsurface flows the details of the surface subsurface coupling within a time step is shown in fig 2 the sequential solution process starts from surface flow module followed by the subsurface flow module for each coupling time step d t the surface flow module solves the flow equation with the rainfall rate q r and the exchange flux q e x between the two flow modules which is passed from the subsurface flow module at the end of the previous time step as source sink terms then the surface hydraulic head h s is passed to the subsurface flow module together with the rainfall rate q r before each newton iteration to solve subsurface flow equation the interface boundary condition type and the corresponding value needs to be assigned first the core principle is to perform a simple mass balance calculation by means of comparing the soil infiltration capacity q i c or potential infiltration rate with the water supply rate q w s 12 q w s h s d t q r the soil infiltration capacity q i c is calculated using surface head and soil water content just below the surface according to the darcy s law then the subsurface flow module performs a mass balance check to evaluate the ponding lamina p l 13 p l h s q r q i c d t the interface boundary condition type and value for a given time step are determined based on the calculated p l if p l is greater than zero it means that the exchange flux at the interface is q i c which is a function of the hydraulic gradient at the interface and the intrinsic and relative permeabilities of the top soil as shown in eq 5 thus the interface boundary condition is assigned as the dirichlet type by prescribing pressure p d as the ponding lamina pressure p p l 14 p p l ρ g p l p r e f where ρ k g m 3 is the water density and p r e f p a is the atmospheric pressure if p l is negative or zero it means that all the water supplied on the surface will be infiltrated therefore the interface boundary condition is switched to a neumann type with the neumann flux value q n set as the water supply rate q w s after the iterations of a time step converge the subsurface flow module determines the actual infiltration or exfiltration flux q a which is then passed to the surface flow module as q e x for the next time step if the interface boundary condition is the neumann type q e x is equal to q n if the boundary condition is the dirichlet type q a is calculated using the darcy s law as shown in eq 5 it is important to note that the above procedure checking ponding lamina and switching bc type is performed on each subsurface top cell hence there could be different bc types assigned on the interface boundary cells however the convergence check for the subsurface iterative loop is performed on all the subsurface cells if the maximum number of iterations is reached before achieving convergence the subsurface flow module cuts the time step in half and initializes the iteration again 2 5 parallel solvers pflotran incorporates parallel solvers and data structures provided by the portable extensible toolkit for scientific computation petsc balay et al 2017 framework to achieve parallelization through domain decomposition the petsc library is used because it allows the users to choose from a large range of solvers and preconditioners that can handle linear and nonlinear problems in parallel mode petsc has been developed continuously by active research scientists and has been applied on hydrological models such as ogs wang et al 2015 pwash123d gwo and yeh 2004 and gssha eller et al 2013 the linear system of partial differential equations pdes for surface flow at each time step is solved using a forward euler time integration scheme with the time steppers solvers ts framework in petsc the nonlinear system of pdes for subsurface flow resulting from the discretization at each implicit time step is solved using the scalable nonlinear equations solvers snes framework in petsc petsc solves these linear and nonlinear systems of equations in parallel using domain decomposition where mpi handles communication between sub domains metis parmetis are employed to decompose the surface and subsurface grids 3 model verification and performance evaluation there are no existing analytical solutions for the integrated surface and subsurface flow problems therefore we compare pflotran simulation results with published results from previous studies to assess the performance of the coupled surface and subsurface flow model and verify the coupled code the physical responses of these numerical experiments have been validated in the previous studies di giammarco et al 1996 kollet and maxwell 2006 panday and huyakorn 2004 shen and phanikumar 2010 sulis et al 2010 because the subsurface flow module has been tested extensively hammond et al 2014 and the surface flow module is newly developed we first test the surface flow module with a 2 d tilted v catchment experiment then we test the general overland flow generation mechanisms of the integrated surface and subsurface flow in a slope plane with minimal geometric complexity these tests include normal atmospheric forcing with step functions of rainfall followed by a recession we also test the overland flow generation mechanism with a heterogeneous subsurface slab beneath a slope plane finally we test the coupled model on a 3 d field scale hydrologic experiment abdul and gillham 1989 that features more realistic and complex catchment topography geometry and scale model parameters used for these test cases are summarized in table 2 evaporation is not considered in the verification test cases for model intercomparison we only select three representative models cathy hgs and parflow that participated in all of the two phase intercomparison projects kollet et al 2017 maxwell et al 2014 we obtained the simulation results of those three models from the authors for the field scale hydrologic experiment kollet et al 2017 and through digitizing for the other benchmark tests in maxwell et al 2014 we list several features of the three representative models compared with pflotran in table 3 the difference in coupling strategy and interface boundary conditions can be found from table 1 3 1 the 2 d tilted v catchment we conduct the 2 d tilted v catchment runoff simulation to verify the implementation of the surface flow model this test case has been widely used for verification of surface flow simulation after it was first introduced by di giammarco et al 1996 the overall shape of the flow domain consists of two 800 1000 m slopes connected by a 20 m wide channel fig 3 the surface slope is 5 perpendicular to the channel and 2 parallel to the channel the slope of the channel is 2 the grid size is 20 m in x and y directions throughout the domain the manning s roughness coefficient is constant at 2 5 10 4 m 1 3 m i n for the surface slope and 2 5 10 3 m 1 3 m i n for the channel table 2 the simulation consists of a 90 m i n rainfall event with a uniform intensity of 1 8 10 4 m m i n followed by 90 m i n of recession fig 4 a the boundary condition at the outlet of the channel is set to be the zero depth gradient condition and model outflow is measured at the outlet boundary pflotran surface flow simulations yield similar behaviors to the other three models in the predicted peak flow and recession phase fig 4b however there are more significant differences during the rising limb phase between the selected models the predicted time to steady state by pflotran lies in between that predicted by hgs and cathy while differs the most from parflow we find that pflotran and hgs behave similarly in the rising limb phase because they employ the same diffusion wave equation to describe the surface flow unlike parflow which uses the kinematic wave equation sulis et al 2010 in the recession phase there is greater model agreement because the surface flow almost routes at the channel with only one directional slope the slight difference between hgs and the other three models is due to the fact that hgs uses a node based discretization while the others are cell centered 3 2 saturation excess two saturation excess tests are aimed at investigating the dunne runoff produced by complete saturation of the subsurface and the intersection of the land surface by the water table following the same design introduced by kollet and maxwell 2006 the exposed water table generates runoff we specify a homogeneous saturated hydraulic conductivity of 6 94 10 4 m m i n which is larger than the rainfall rate of 3 3 10 4 m m i n these tests are conducted on an inclined sloping plane domain fig 5 that is 400 m long by 320 m wide with a uniform soil depth of 5 m the slope of the planes in x direction and y direction are 0 05 and 0 respectively the manning s roughness coefficient is constant at 3 31 10 4 m 1 3 m i n for the entire domain a surface grid of 80 m resolution is used and the vertical grid size is 0 0125 m we set a zero depth gradient boundary condition at the outlet for the surface flow model outflow is measured at one grid cell on the outlet boundary for subsurface flow a no flow boundary condition is prescribed at the bottom and vertical sides of domain we run the model with two different values of initial water table depth wtd 0 5 m and 1 0 m the initial condition of the unsaturated domain is set as hydrostatic fig 6 shows the initial water saturation for two different initial water table depths as x z cross sections the soil parameters are listed in table 2 the van genuchten relationships are adopted to describe the soil water retention characteristics a 200 m i n rainfall event with a uniform rate of 3 3 10 4 m m i n is applied to generate runoff followed by 100 m i n s of recession fig 7 a the outflow rate time series produced from pflotran simulations compare well with the other three models under both wetter fig 7b and drier fig 7c conditions as expected the model test with the shallower initial water table depth produces runoff earlier and yields a larger outflow magnitude than the test with the deeper initial water table the timing and shape of the recession limb of the hydrographs produced by pflotran are in good agreement with parflow and cathy because all three models use a cell centered discretization scheme hgs shows slightly larger outflow at earlier time and a lower rate at later time compared with other models because it uses a node based discretization scheme the pressure centered on the hgs node at the outlet responds quicker than those at the cell centers in other models we also notice a slight difference in peak flow between pflotran and cathy when the initial water table is deeper fig 7c while the two models behave more similarly under shallower initial water table fig 7b as the two models use the same boundary switching method their discrepancy under drier initial conditions may have come from the flux exchange between surface and subsurface flow that needs further investigation 3 3 infiltration excess the infiltration excess tests are designed to investigate the hortonian runoff produced by ensuring that the surface ponding occurs before complete saturation of the soil column it is achieved by specifying a saturated hydraulic conductivity k s smaller than the rainfall rate the tests were first introduced by kollet and maxwell 2006 we test two different low values of k s as listed in table 2 the higher k s value is expected to yield more infiltration than the lower value the domain and the discretization used in this test case are the same as in the saturation excess cases the manning s roughness coefficient and boundary condition for surface and subsurface flows are also the same as in the saturation excess case model outflow is measured at one grid cell on the outlet boundary the initial water table depth is set to be 1 0 m the soil parameters are listed in table 2 and the van genuchten relationships are used to describe the soil hydraulic retention characteristics same as in the saturation excess case rainfall is applied for 200 m i n s at a rate of 3 3 10 4 m m i n followed by 100 m i n s of recession fig 8 a the simulated outflow time series from pflotran compare well with those from the other three models under both high fig 8b and low fig 8c k s as expected the test case with lower k s produces earlier runoff and larger outflow because it allows less infiltration the timing and shape of the recession phase of the hydrographs produced by pflotran are in good agreement with the other three models same as in the saturation excess case hgs produces slightly larger outflow in the rising limb and smaller outflow at the recession period compared with other models 3 4 temporal discretization we investigate the influence of temporal discretization for the previous saturation excess and infiltration excess tests with the finest vertical discretization δ z 0 0125 m by comparing model outputs using constant coupling time step sizes δ t of 1 3 and 5 m i n s the results in fig 9 show that the outflow responses in the rising limbs of hydrographs differ slightly between the smaller and larger coupling δ t for both runoff processes indicating more rainfall is infiltrated into the subsurface layers when we use larger coupling δ t however the differences between these three coupling time steps are almost negligible compared to the differences between models as shown in sections 3 1 3 2 and 3 3 although a coupling time step of 1 m i n appears to be sufficient from this set of tests we choose coupling time steps of 6s in all the pflotran simulations cases to be conservative 3 5 slab case previous studies have demonstrated that spatial heterogeneity of subsurface hydraulic properties has a significant impact on runoff generation woolhiser et al 1996 merz and plate 1997 here we adopt a testing case with spatial heterogeneity following the work of kollet and maxwell 2006 we use the same inclined sloping plane domain as for the previous two cases and the subsurface soil is uniform with a saturated hydraulic conductivity value of 6 94 10 4 m m i n except for a 100 m long 0 05 m thick slab with a very low hydraulic conductivity of 6 94 10 6 m m i n fig 10 the slab with low hydraulic conductivity is designed to generate localized infiltration excess runoff while the rest of the domain will generate runoff through saturation excess the grid size for surface and subsurface flows is 10 m in the horizontal direction and 0 05 m in the vertical direction the manning s roughness coefficient boundary conditions for surface and subsurface flows rainfall intensity and duration fig 11 a recession period and domain of simulation are the same as in the previous saturation and infiltration excess cases modeled outflow is measured at one grid cell on the outlet boundary the initial water table is set at the 1 0 m depth the soil parameters are listed in table 2 and the van genuchten relationships are used to describe the soil hydraulic retention characteristics pflotran shows good agreement with the other three models on the basis of the modeled outflow results fig 11b the hydrograph is more complicated than the previous two cases showing two steep rising segments connected by a flat segment followed by recession the first rising segment of outflow is caused by infiltration excess on the low hydraulic conductivity slab and the second one is caused by saturation excess after the subsurface domain outside of the low permeability slab area reaches full saturation between 90 and 150 m i n s as illustrated in fig 12 the peak flow from pflotran is very similar to those produced by parflow and hgs cathy produces a slightly lower peak flow value to investigate how the subsurface saturation progresses over time in the slab case especially in the area adjacent to the low permeability slab we take x z cross sectional snapshots of the subsurface saturation profile produced by pflotran at times of 0 60 90 150 200 and 300 m i n s as shown in fig 12 this selected set of times span from initial time through the end of the modeled recession period and include times during the steep rising limb and at the peak coincident with the end of the precipitation period together they illustrate the complex physical responses of heterogeneous soil columns to infiltration saturation and lateral unsaturated flow while the water table in soil columns with uniform larger hydraulic conductivity rises quickly due to saturation excess the water table in the soil columns with heterogeneous lower hydraulic conductivity slab on top rises very slowly due to limits on the infiltration capacity surface ponding immediately occurs above the low permeability slab due to infiltration excess then the subsurface domains downstream of the slab become saturated due to infiltration of surface flow routed from above the slab the saturation in the slab area increases due to lateral flow that occurs in addition to the infiltration through the top boundary similar saturation profiles from other models can be found in maxwell et al 2014 fig 8 the results from pflotran are similar to those from parflow and hgs whereas they show more differences compared to those from cathy the differences between cathy and the other three models in both the outflow time series and subsurface saturation profiles might indicate some coupling numerical errors in cathy caused by flux exchanges between surface cells and subsurface nodes calculated from hydraulic gradient this coupling error could be magnified by complex geological conditions and multiple rainfall events 3 6 borden case the borden test case is based on the original field experiment conducted by abdul and gillham 1989 and it has been widely used as a test case for verifying or comparing hydrologic models jones et al 2006 kollet et al 2017 vanderkwaak 1999 the experiment was conducted on a small catchment consisting of a ditch that is approximately 2 m deep and was irrigated uniformly with water for 50 m i n s the spatial extent of the domain is approximately 20 80 m in the horizontal direction with an arbitrary horizontal base or datum at 0 m fig 13 here we use the 0 5 m digital elevation models dems as the top surface the maximum elevation is 4 642 m and the elevation of the ditch outlet is 2 986 m the initial water table is 0 2 m below the ditch outlet and the unsaturated zone above the water table is set to be hydrostatic initially the soil parameters and hydraulic properties are listed in table 2 and the van genuchten relationships are used to describe the soil hydraulic retention characteristics we set a critical depth boundary condition for surface flow and a no flow boundary condition for subsurface sides and base the simulation period includes the aforementioned 50 m i n s of rainfall with a rate of 3 33 10 4 m m i n followed by 50 m i n s of recession because the soil saturated hydraulic conductivity is 6 0 10 4 m m i n surface runoff is generated by saturation excess the horizontal mesh size is 0 5 m and the vertical grid size is set to gradually increase from 0 05 m at the top to 0 2 m at the base of the domain we compare the time series results of saturated storage unsaturated storage ponding storage and discharge at the outlet obtained from pflotran with those from the other three models as shown in fig 14 note that the original topography data dem is reinterpolated to generate the appropriate computing surface mesh for each model the model surface area is 1022 25 m 2 for all three models the storage assessments are normalized by the model surface area as the vertical mesh discretization differs among the models with irregular meshes the initial normalized saturated and unsaturated water storage differ slightly among the models the initial ponding storage is 0 because there is no surface flow before the irrigation event no normalization is applied to the discharge because only the approximate area of the test site is provided in the original field experiment none of the models is able to capture the exact discharge time series as observed as shown in fig 14d possibly due to errors or assumptions in the domain geometry or due to the influences of unmapped heterogeneity parflow captures the rising limb most closely but does not capture the relatively rapid falling limb seen in the data hgs captures the falling limb well but it fails to capture the rounded response near the peak pflotran produces responses very similar to hgs the peak is somewhat lower leading to a slower rising limb and the falling limb is slower than hgs cathy differs considerably from the other three models the remaining metrics are model derived with no field data available for comparison the normalized saturated storage is very similar among all models fig 14a the normalized unsaturated storage is also similar with pflotran showing a response that falls within the ranges of the other three models fig 14b the largest difference among the models is the normalized ponding storage fig 14c in particular pflotran predicts a higher peak and slower recession this difference is the likely cause of the difference between pflotran outputs and the observations with more water being held in the ponding storage and not released as outflow this in turn may be due to the different dem reinterpolation methods among the models leading to more surface water being retained in pflotran we also examine snapshots of the water saturation at the cross section x 40 m at time 0 the saturation profile reflects the hydrostatic initial condition fig 15 after 20 m i n s of infiltration the shallow subsurface is becoming wetter whereas there is little change in the deeper subsurface this pattern continues until the end of the applied infiltration and is followed by gradual redistribution during recession these results suggest that lateral unsaturated flow does not play a major role in the subsurface hydrodynamics for this field test the results are similar for parflow but differ from those of hgs and cathy see kollet et al 2017 fig 14 as kollet et al 2017 pointed out these inter modal differences should be taken into account when modeling real world applications supporting the use of model ensembles to quantify uncertainty and data assimilation methods to improve model accuracy 3 7 parallel performance the newly developed model coupling surface and subsurface flows inherits pflotran s inherent capacity in massively parallel computing we perform strong and weak scalability studies to assess the parallel performance of the coupled code in the strong scalability test we fix the problem size while changing the number of processes the parallel performance is measured through relative strong scaling efficiency defined as 15 e p t b p b t p p where t b is the execution time on a base process configuration with p b processes for the fixed size problem and t p is the run time with p processes the ideal relative strong scaling efficiency is achieved when e p i d e a l 1 in the weak scalability test a unit problem of size n unknowns dofs is simulated on one processor and both the problem size and the number of processors are increased correspondingly the parallel performance is measured through relative weak scaling efficiency which is written as 16 e n p t n b p b t n p where t n b p b is the simulation time for base configuration while the problem size is n b with p b processors and t n p is the simulation time while the problem size n and the number of processors p are scaled by the same coefficient α i e n α n b and p α p b for ideal weak parallel efficiency the simulation time will remain constant and e n p 1 for any number of processors however due to hardware configuration and software implementation the weak parallel efficiency is commonly less than 1 we conduct several tests on the nersc national energy research scientific computing center cori haswell compute machine each haswell compute node contains 2 processors and each processor has 16 cores thus each node contains 32 cores we first perform the strong scaling test on a sloping plane with the horizontal area of 200 160 m and vertical depth of 0 5 m the sloping plane only includes the slope of 0 05 in the x direction as illustrated in fig 16 the mesh includes 1000 800 5 grids in the x y and z directions respectively leading to a problem size of 0 8 m dofs for the surface flow and 4 0 m dofs for the subsurface flow the soil properties are the same as in the previous saturation excess test case the initial water table depth is 0 3 m and the initial condition of the unsaturated domain is set to be hydrostatic the simulation covers 10 m i n s of rain at a rate of 1 5 10 4 m m i n followed by 10 m i n s of recession as the rainfall rate is smaller than the saturated hydraulic conductivity the surface flow is generated through saturation excess fig 17 illustrates the results for two strong scaling tests where wall clock time is plotted versus the number of cores processes employed the dashed lines in fig 17 show the ideal slope for a linear speedup the green line represents the first test scenario where all cores on a node are utilized before additional nodes are added the second test scenario is shown in blue where a maximum of 2 cores per node are employed the latter scenario is included to elucidate the degradation in strong scaling performance due to the intra nodal memory contention within the first scenario information regarding the number of cores and nodes employed in each scenario is listed in table 4 the wall clock computation time for the surface flow only accounts for about 0 5 of the total wall clock time the fraction is small compared to the fraction of the total dofs associated with surface flow 0 8 4 8 16 7 the substantial difference in computation time may be due to the forward euler time integration scheme applied in the surface flow module compared to the implicit time scheme with iterative solver used for the subsurface flow module in the first scenario it is clear that the speedup is nearly linear between 1 4 and 32 256 processes for surface and subsurface flows respectively the sub linear speedup between 8 and 32 processes in fig 17 is caused by the fixed amount of memory cache being spread among an increasing number of cores on a single node the term fully packed implies that all cores on a node are being used as more cores are employed per node the memory cache is spread thinner causing the performance to degrade beyond 8 processes per node in order to mitigate memory competition between cores on node we applied only 2 processes node in the second scenario so that more nodes were applied in this scenario as demonstrated by the blue line in fig 17 the degradation between 8 and 32 processes disappears and the speedup is linear between 1 and 64 processes for surface flow and 1 256 processes for subsurface flow in this scenario the degradation in parallel performance after 64 processes for surface flow and after 512 processes for subsurface flow is due to decreasing computational work load per process relative to increasing inter processor communication when the number of dofs per process is smaller than 12 k 18 k hammond et al 2014 further illustrates this memory contention phenomenon for the relative weak scaling test we keep the problem domain size fixed and refine the grid resolution as the number of processes increased fig 18 shows the results from several weak scaling tests including two different numbers of dofs per process ideally the wall clock time and efficiency should be constant no matter the change of the number of processes however the surface and subsurface weak scaling efficiencies all drop quickly to below 0 2 as the number of processes increase these pflotran simulation results were generated using the stabilized biconjugate gradient bicgstab krylov solver with block jacobi ilu 0 preconditioning for the subsurface flow to obtain increasingly optimal weak scaling performance multilevel preconditioning would need to be employed within pflotran a potential topic for future research kollet et al 2010 and osei kuffuor et al 2014 demonstrate superior weak scaling performance of the multilevel solvers 4 conclusions in this study we couple a surface flow process to the high performance subsurface reactive flow and transport code pflotran the diffusion wave equation is used to represent the surface flow the coupling technique is based on the boundary condition switching method which ensures the continuity of pressure and flux at the surface subsurface interface we compare the performance of the developed model with other three similar hydrological models using several numerical benchmark test cases and a field scale test case the results from those numerical benchmark tests show good agreements among all of the hydrological models some model discrepancies are resulted from different discretization schemes used in individual models from the field scale test we find that the discrepancies in surface ponding and outflow become larger under more complex topography we also find that the consistency of spatial discretization from surface to subsurface domain with finite volume method in pflotran avoids the interpolation of exchange fluxes and surface ponding heads errors that have been reported in cathy the sequential non iterative method could reduce computing cost while achieving acceptable accuracy the parallel scaling test results show that our coupled hydrological model exhibits good strong parallel scaling and they also provide guidance on how to choose the number of computing processors to optimize computation time for large scale hyper resolution hydrological simulations pflotran s modular object oriented design affords great flexibility to link more subsurface processes such as reactive solute transport heat transfer and ecological processes driven by the integrated hydrological processes between the surface and subsurface domains such further extensions will pave the way for researchers to study coupled hydrobiogeochemical processes in watershed systems as well as at critical interfaces such as river corridors and coastal regions that are known to be biogeochemical hot spots of the earth system code availability the pflotran source code used in this research may be cloned from https gitlab pnnl gov sbrsfa pflotran ems see the file installation readme for installation instructions and the directory ems input decks for test input decks the source code is compatible with the xsdk 0 2 0 git tag of petsc https gitlab com petsc petsc declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this research was supported by the u s department of energy doe office of biological and environmental research ber as part of ber s subsurface biogeochemical research program sbr this contribution originates from the sbr scientific focus area sfa at the pacific northwest national laboratory pnnl and was supported by the partnership with the ideas watersheds this research used high performance computing resources from the university of arizona research data center rdc and the u s doe national energy research scientific computing center nersc we thank dr stefan kollet and dr mauro sulis for providing us with the dem data and model simulation results of borden case all the pflotran simulation results used to produce the figures in this study can be found at https gitlab pnnl gov sbrsfa pflotran ems or requested by contacting the corresponding author pnnl is operated for the doe by battelle memorial institute under contract de ac05 76rl01830 this paper describes objective technical results and analysis any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the u s department of energy or the united states government a soil retention curves and characteristic curves in this study we combined van genuchten s soil water retention curve and mualem s relative permeability function to calculate water saturation s and relative permeability k r for subsurface flow respectively they are expressed as following equations a 1 s e s s r s 0 s r 1 α p p c n m m 1 1 n a 2 k r s e 1 1 s e 1 m m 2 where s e is effective water saturation s r is residual water saturation s 0 is maximum water saturation α p p a 1 and n are van genuchten parameters that depend on the soil properties p c p a is capillary pressure we usually obtain the van genuchten parameter α m 1 so the conversion between α p and α is needed and it is listed as below equation a 3 α p α ρ g where ρ k g m 3 is mass water density g m s 2 is gravity 
25879,water exchange between the surface and subsurface is important for both water resource management and environmental protection in this paper we develop coupled surface and subsurface flow simulation capability in a parallel subsurface flow and reactive transport code pflotran we sequentially couple the diffusion wave based surface flow with the subsurface flow governed by the richards equation in pflotran these two flow domains are linked with a boundary condition switching method that ensures the continuity of pressure and flux at the surface subsurface interface we verify the coupled code against other existing hydrologic models and observation data using a number of numerical experiments the coupled hydrological model exhibits good performance in strong parallel scaling tests the new coupled surface and subsurface simulator significantly advance community simulation capability towards improving integrated hydrologic and biogeochemical understanding of complex systems such as watersheds and river corridors keywords surface flow integrated hydrological modeling boundary condition switching parallel computing 1 introduction surface and subsurface hydrologic systems are linked compartments of a hydrologic continuum accurate representation of the interaction between those systems is an important component of modeling the integrated hydrologic cycle for watershed systems sophocleous 2002 winter 2001 surface water and groundwater exchange has captured increasingly more attention due to its significant impact on biogeochemical processes in natural and engineered systems harvey and gooseff 2015 harvey et al 2019 song et al 2018 goldman et al 2017 graham et al 2019 increased capabilities in measurements both direct and indirect including satellite based observations have been accompanied by improved numerical methods and computing capacity to allow for greatly improved understanding of this highly transient and heterogeneous water mass flux maxwell et al 2014 kollet et al 2010 2017 sulis et al 2010 over the last couple decades several approaches have been developed for integrated surface and subsurface hydrological modeling furman 2008 kampf and burges 2007 we summarize the existing integrated surface subsurface hydrological models in terms of the coupling strategy and interface boundary conditions in table 1 these models have generally differed in the formulation of the governing equations the choice of surface subsurface interface boundary conditions and the method of linking surface and subsurface flow most of these approaches rely on some form of the hydrostatic shallow water equations representing surface flow routing coupled to some modified form of the richards equation for subsurface flow in variably saturated porous media the shallow water equations for the surface flow are solved in either 1d or 2d while the richards equation is commonly solved in either 1d vertical or 3d boundary conditions at the surface subsurface interface include first order exchange flux boundary condition switching and continuity of pressure and flux there are three different strategies to solve the coupled system of equations sequential non iterative asynchronous linking sequential iterative and fully coupled huang and yeh 2009 compared those coupling strategies and discussed their advantages and disadvantages full coupling is the most robust and accurate solution scheme but it forces the same time steps for surface and subsurface flows which often leads to long simulation time sequential non iterative or iterative coupling allows for different time steps for surface and subsurface flow usually imposing several sub steps for surface flow within each subsurface time step to balance between computational needs and solution accuracy the iterations between the surface and subsurface solvers improve solution accuracy another advantage of sequential coupling is to account for micro topography features that may have important effects on surface flow generation and subsurface flow and transport characteristics especially at small spatial scales camporese et al 2010 frei and fleckenstein 2014 moreover sequential coupling affords the flexibility to include more processes such as reactive transport thermal transport and microbes mediated biogeochemical reactions in different ways that are suited for the surface and subsurface domains kollet et al 2017 and maxwell et al 2014 conducted detailed inter comparisons between sequentially and fully coupled integrated hydrological models on a number of complex heterogeneous synthetic and real world cases they found that the model disagreement is primarily caused by the assumption of 1d unsaturated subsurface flows and reliance on the kinematic wave approximation in surface flows while the coupling strategy only plays a secondary role setting the boundary condition at the surface and subsurface interface is part of the coupling strategy first order exchange fluxes and pressure continuity are widely used in the community a boundary condition switching approach bixio et al 2000 has been developed to resolve the coupling without introducing new parameters to represent an exchange process or an interface property while preserving the continuity of fluxes and pressure head at the interface the boundary condition switching algorithm changes the type of boundary condition at the surface subsurface interface i e exchange fluxes or pressure continuity according to the surface ponding status it has been applied and tested in richards equation based subsurface models such as cathy camporese et al 2010 dagès et al 2012 sulis et al 2010 and gcs flow le et al 2015 integrated hydrological models have been employed at scales ranging from hillslopes to catchments to watersheds and more recently at the scale of continents maxwell et al 2015 high performance computing is essential for dealing with massive computational resources needed for high resolution and long term simulations especially at the larger scales hwang et al 2014 vivoni et al 2011 fatichi et al 2016 paniconi and putti 2015 several distributed hydrological models have been developed in parallel computing environments that make use of high performance computing these simulators are largely based on the message passing interface mpi standard there are also simulators that are based on the open multi processing openmp hwang et al 2014 or graphics processing units gpus anagnostopoulos et al 2015 le et al 2015 while openmp and gpu based models can be applied for problems composed of millions of degrees of freedom dofs mpi based models are necessary to address larger problems with up to billions of dofs since the memory requirements exceed what is available on a single node this has led to a continued reliance on mpi based models for large scale hydrologic analyses kollet et al 2010 vivoni et al 2011 in this study we implement sequential non iterative coupling approach with the boundary condition switching scheme to develop integrated hydrologic simulation capability in pflotran pflotran is an open source massively parallel multi physics simulator hammond et al 2014 developed and distributed under a gnu lgpl license and is freely available at www pflotran org open source simulators have become the trend for conducting multidisciplinary science through sharing and learning with the broader scientific community yu et al 2016 the open sharing culture attracts interested researchers and scientists to develop and use the simulators in highly collaborative and productive ways pflotran solves a system of generally nonlinear partial differential equations describing reactive multiphase flow and multicomponent biogeochemical transport in porous media pflotran has been developed from the ground up for parallel scalability and has been run on up to 2 18 processor cores with problem sizes up to 2 billion degrees of freedom lichtner et al 2017 since its inception pflotran has been applied widely to reactive flow and transport hammond and lichtner 2010 hammond et al 2011 lichtner and hammond 2012 data assimilation chen et al 2012 2013 hydrogeophysics johnson et al 2015 2017 carbon sequestration navarre sitchler et al 2013 ice modeling karra et al 2014 biogeochemical modeling tang et al 2016 and as a part of the community land model clm bisht et al 2017 among other applications our work here fills a critical capability gap for pflotran in surface flow processes thus significantly advancing community simulation capability towards improving integrated hydrologic and biogeochemical understanding of complex systems such as watersheds and river corridors graham et al 2019 this paper is organized as follows in section 2 we introduce the underlying design principles of the integrated capability including boundary condition switching approach for coupling the surface and subsurface flows the governing equations for each flow system and the space and time discretization and numerical implementation for solving the two systems of equations in section 3 we verified the new simulation capability in simulating standalone surface flows and integrated surface and subsurface flows using five test cases with known results from other well established simulators and observed data we also evaluated the parallel performance of the integrated model in section 3 concluding remarks are provided in the final section section 4 2 methodology 2 1 surface flow equation surface flow is often characterized by either the kinematic wave kw equation or the diffusion wave dw equation to approximate the saint venant equation the kw equation has problems with the flat terrain since it does not redistribute water on flat terrain on the other hand the dw equation for surface flow has been shown to provide more accurate results with computational efficiency the diffusion wave approximation is often written as 1 h t v h q r q e x where h m represents the surface water depth v m s 1 is surface water velocity q r m s 1 is rainfall rate and q e x m s 1 represents exchange fluxes between surface and subsurface domains the dw approximation allows for diffusion of a surface water wave by incorporating the pressure differential term it is also suitable for shallow water flow simulation on irregular topography panday and huyakorn 2004 painter et al 2016 the backwater effects can be accounted for by inclusion of a downstream boundary condition two types of boundary conditions at the outlet are usually used zero depth gradient zdg or critical depth cd outlet conditions manning s equation gottardi and venutelli 1993 is often used to establish a flow depth discharge relationship and the surface water velocity v is written as follows 2 v h 2 3 n m a n s s h z where n m a n m 1 3 s is the manning s coefficient s s is the slope of the water surface and z m is the ground elevation here we assume that v equals 0 when s s is less than 10 6 s s is computed with the dw approximation and written as 3 s s h z x 2 h z y 2 2 2 subsurface flow equation pflotran solves the variably saturated flow in the subsurface using the three dimensional 3 d richards equation richards 1931 the mixed form of the richards equation proposed by celia et al 1990 yields robust numerical solutions and maintains mass balance for unsaturated flow problems 4 t ϕ s η η q q w here t s is time ϕ is the porosity of soil matrix s m 3 m 3 is the water saturation η k m o l m 3 is the molar water density q m s 1 is the darcy flux and q w k m o l m 3 s 1 represents a source positive or a sink negative of water the darcy flux q is calculated following the darcy s law 5 q k k r μ p w w η g z here k m 2 is the intrinsic permeability k r is the relative permeability μ k g m 1 s 1 is the water viscosity p p a is the pressure w w k g k m o l 1 is the formula weight of water g m s 2 is the acceleration of gravity and z m is the vertical component of the position vector the water density and viscosity are computed as a function of temperature and pressure through an equation of state for water pflotran provides several relationships between capillary pressure and saturation such as the van genuchten van genuchten 1980 and brooks corey brooks and corey 1964 types supported relative permeability functions for the richards equation include the burdine 1953 and mualem 1976 types in this study we combine van genuchten s soil water retention curve and mualem s relative permeability function to calculate water saturation and relative permeability for the subsurface flow these constitutive relations are further described in appendix a 2 3 discretization and numerical implementation standard upwind cell centered finite volume spatial discretization patankar 1980 is used in both the surface and subsurface flows the spatial discretization applied in pflotran ensures local mass conservation because the exchange fluxes occur between the center of a surface grid cell and a subsurface grid cell fig 1 it can also avoid the error from the node to cell interpolation of the exchange fluxes at the surface subsurface interface that has been reported by fiorentini et al 2015 within cathy the governing partial differential equation for mass conservation in the surface and subsurface flow domains can be written in the general form as 6 t a f q with an accumulation term a a flux term f and a source sink term q integrating over a representative elementary volume rev corresponding to the n t h grid cell with volume v n and cell boundary v n yields 7 d d t v n a d v v n f d v v n q d v the accumulation term has the finite volume form 8 d d t v n a d v a n t δ t a n t δ t v n with time step δ t the flux term can be expanded as a surface integral using the gauss s theorem 9 v n f d v v n f d s n f n n s n n where the latter finite volume form f n n is based on the two point flux approximation where the sum over n involves all the nearest neighbor grid cells connected to the n t h grid cell with interfacial areas s n n the finite volume discretization for the source sink term has the form of 10 v n q d v q n v n where q n is the source sink term located within the n t h grid cell the residual function r n for the governing equation is given by 11 r n a n t δ t a n t δ t v n n f n n t δ t s n n q n v n for the surface flow system an explicit time scheme forward euler time integration scheme is used and the time step is limited by a stability condition the subsurface flow system uses a fully implicit solution scheme and is solved using an iterative solver based on the newton raphson method the detailed method of solution can be found on the pflotran website documentation pflotran org 2 4 coupling strategy and interface boundary condition we implement a sequential non iterative approach within pflotran using an improved boundary condition switching algorithm to couple the surface and subsurface flows the details of the surface subsurface coupling within a time step is shown in fig 2 the sequential solution process starts from surface flow module followed by the subsurface flow module for each coupling time step d t the surface flow module solves the flow equation with the rainfall rate q r and the exchange flux q e x between the two flow modules which is passed from the subsurface flow module at the end of the previous time step as source sink terms then the surface hydraulic head h s is passed to the subsurface flow module together with the rainfall rate q r before each newton iteration to solve subsurface flow equation the interface boundary condition type and the corresponding value needs to be assigned first the core principle is to perform a simple mass balance calculation by means of comparing the soil infiltration capacity q i c or potential infiltration rate with the water supply rate q w s 12 q w s h s d t q r the soil infiltration capacity q i c is calculated using surface head and soil water content just below the surface according to the darcy s law then the subsurface flow module performs a mass balance check to evaluate the ponding lamina p l 13 p l h s q r q i c d t the interface boundary condition type and value for a given time step are determined based on the calculated p l if p l is greater than zero it means that the exchange flux at the interface is q i c which is a function of the hydraulic gradient at the interface and the intrinsic and relative permeabilities of the top soil as shown in eq 5 thus the interface boundary condition is assigned as the dirichlet type by prescribing pressure p d as the ponding lamina pressure p p l 14 p p l ρ g p l p r e f where ρ k g m 3 is the water density and p r e f p a is the atmospheric pressure if p l is negative or zero it means that all the water supplied on the surface will be infiltrated therefore the interface boundary condition is switched to a neumann type with the neumann flux value q n set as the water supply rate q w s after the iterations of a time step converge the subsurface flow module determines the actual infiltration or exfiltration flux q a which is then passed to the surface flow module as q e x for the next time step if the interface boundary condition is the neumann type q e x is equal to q n if the boundary condition is the dirichlet type q a is calculated using the darcy s law as shown in eq 5 it is important to note that the above procedure checking ponding lamina and switching bc type is performed on each subsurface top cell hence there could be different bc types assigned on the interface boundary cells however the convergence check for the subsurface iterative loop is performed on all the subsurface cells if the maximum number of iterations is reached before achieving convergence the subsurface flow module cuts the time step in half and initializes the iteration again 2 5 parallel solvers pflotran incorporates parallel solvers and data structures provided by the portable extensible toolkit for scientific computation petsc balay et al 2017 framework to achieve parallelization through domain decomposition the petsc library is used because it allows the users to choose from a large range of solvers and preconditioners that can handle linear and nonlinear problems in parallel mode petsc has been developed continuously by active research scientists and has been applied on hydrological models such as ogs wang et al 2015 pwash123d gwo and yeh 2004 and gssha eller et al 2013 the linear system of partial differential equations pdes for surface flow at each time step is solved using a forward euler time integration scheme with the time steppers solvers ts framework in petsc the nonlinear system of pdes for subsurface flow resulting from the discretization at each implicit time step is solved using the scalable nonlinear equations solvers snes framework in petsc petsc solves these linear and nonlinear systems of equations in parallel using domain decomposition where mpi handles communication between sub domains metis parmetis are employed to decompose the surface and subsurface grids 3 model verification and performance evaluation there are no existing analytical solutions for the integrated surface and subsurface flow problems therefore we compare pflotran simulation results with published results from previous studies to assess the performance of the coupled surface and subsurface flow model and verify the coupled code the physical responses of these numerical experiments have been validated in the previous studies di giammarco et al 1996 kollet and maxwell 2006 panday and huyakorn 2004 shen and phanikumar 2010 sulis et al 2010 because the subsurface flow module has been tested extensively hammond et al 2014 and the surface flow module is newly developed we first test the surface flow module with a 2 d tilted v catchment experiment then we test the general overland flow generation mechanisms of the integrated surface and subsurface flow in a slope plane with minimal geometric complexity these tests include normal atmospheric forcing with step functions of rainfall followed by a recession we also test the overland flow generation mechanism with a heterogeneous subsurface slab beneath a slope plane finally we test the coupled model on a 3 d field scale hydrologic experiment abdul and gillham 1989 that features more realistic and complex catchment topography geometry and scale model parameters used for these test cases are summarized in table 2 evaporation is not considered in the verification test cases for model intercomparison we only select three representative models cathy hgs and parflow that participated in all of the two phase intercomparison projects kollet et al 2017 maxwell et al 2014 we obtained the simulation results of those three models from the authors for the field scale hydrologic experiment kollet et al 2017 and through digitizing for the other benchmark tests in maxwell et al 2014 we list several features of the three representative models compared with pflotran in table 3 the difference in coupling strategy and interface boundary conditions can be found from table 1 3 1 the 2 d tilted v catchment we conduct the 2 d tilted v catchment runoff simulation to verify the implementation of the surface flow model this test case has been widely used for verification of surface flow simulation after it was first introduced by di giammarco et al 1996 the overall shape of the flow domain consists of two 800 1000 m slopes connected by a 20 m wide channel fig 3 the surface slope is 5 perpendicular to the channel and 2 parallel to the channel the slope of the channel is 2 the grid size is 20 m in x and y directions throughout the domain the manning s roughness coefficient is constant at 2 5 10 4 m 1 3 m i n for the surface slope and 2 5 10 3 m 1 3 m i n for the channel table 2 the simulation consists of a 90 m i n rainfall event with a uniform intensity of 1 8 10 4 m m i n followed by 90 m i n of recession fig 4 a the boundary condition at the outlet of the channel is set to be the zero depth gradient condition and model outflow is measured at the outlet boundary pflotran surface flow simulations yield similar behaviors to the other three models in the predicted peak flow and recession phase fig 4b however there are more significant differences during the rising limb phase between the selected models the predicted time to steady state by pflotran lies in between that predicted by hgs and cathy while differs the most from parflow we find that pflotran and hgs behave similarly in the rising limb phase because they employ the same diffusion wave equation to describe the surface flow unlike parflow which uses the kinematic wave equation sulis et al 2010 in the recession phase there is greater model agreement because the surface flow almost routes at the channel with only one directional slope the slight difference between hgs and the other three models is due to the fact that hgs uses a node based discretization while the others are cell centered 3 2 saturation excess two saturation excess tests are aimed at investigating the dunne runoff produced by complete saturation of the subsurface and the intersection of the land surface by the water table following the same design introduced by kollet and maxwell 2006 the exposed water table generates runoff we specify a homogeneous saturated hydraulic conductivity of 6 94 10 4 m m i n which is larger than the rainfall rate of 3 3 10 4 m m i n these tests are conducted on an inclined sloping plane domain fig 5 that is 400 m long by 320 m wide with a uniform soil depth of 5 m the slope of the planes in x direction and y direction are 0 05 and 0 respectively the manning s roughness coefficient is constant at 3 31 10 4 m 1 3 m i n for the entire domain a surface grid of 80 m resolution is used and the vertical grid size is 0 0125 m we set a zero depth gradient boundary condition at the outlet for the surface flow model outflow is measured at one grid cell on the outlet boundary for subsurface flow a no flow boundary condition is prescribed at the bottom and vertical sides of domain we run the model with two different values of initial water table depth wtd 0 5 m and 1 0 m the initial condition of the unsaturated domain is set as hydrostatic fig 6 shows the initial water saturation for two different initial water table depths as x z cross sections the soil parameters are listed in table 2 the van genuchten relationships are adopted to describe the soil water retention characteristics a 200 m i n rainfall event with a uniform rate of 3 3 10 4 m m i n is applied to generate runoff followed by 100 m i n s of recession fig 7 a the outflow rate time series produced from pflotran simulations compare well with the other three models under both wetter fig 7b and drier fig 7c conditions as expected the model test with the shallower initial water table depth produces runoff earlier and yields a larger outflow magnitude than the test with the deeper initial water table the timing and shape of the recession limb of the hydrographs produced by pflotran are in good agreement with parflow and cathy because all three models use a cell centered discretization scheme hgs shows slightly larger outflow at earlier time and a lower rate at later time compared with other models because it uses a node based discretization scheme the pressure centered on the hgs node at the outlet responds quicker than those at the cell centers in other models we also notice a slight difference in peak flow between pflotran and cathy when the initial water table is deeper fig 7c while the two models behave more similarly under shallower initial water table fig 7b as the two models use the same boundary switching method their discrepancy under drier initial conditions may have come from the flux exchange between surface and subsurface flow that needs further investigation 3 3 infiltration excess the infiltration excess tests are designed to investigate the hortonian runoff produced by ensuring that the surface ponding occurs before complete saturation of the soil column it is achieved by specifying a saturated hydraulic conductivity k s smaller than the rainfall rate the tests were first introduced by kollet and maxwell 2006 we test two different low values of k s as listed in table 2 the higher k s value is expected to yield more infiltration than the lower value the domain and the discretization used in this test case are the same as in the saturation excess cases the manning s roughness coefficient and boundary condition for surface and subsurface flows are also the same as in the saturation excess case model outflow is measured at one grid cell on the outlet boundary the initial water table depth is set to be 1 0 m the soil parameters are listed in table 2 and the van genuchten relationships are used to describe the soil hydraulic retention characteristics same as in the saturation excess case rainfall is applied for 200 m i n s at a rate of 3 3 10 4 m m i n followed by 100 m i n s of recession fig 8 a the simulated outflow time series from pflotran compare well with those from the other three models under both high fig 8b and low fig 8c k s as expected the test case with lower k s produces earlier runoff and larger outflow because it allows less infiltration the timing and shape of the recession phase of the hydrographs produced by pflotran are in good agreement with the other three models same as in the saturation excess case hgs produces slightly larger outflow in the rising limb and smaller outflow at the recession period compared with other models 3 4 temporal discretization we investigate the influence of temporal discretization for the previous saturation excess and infiltration excess tests with the finest vertical discretization δ z 0 0125 m by comparing model outputs using constant coupling time step sizes δ t of 1 3 and 5 m i n s the results in fig 9 show that the outflow responses in the rising limbs of hydrographs differ slightly between the smaller and larger coupling δ t for both runoff processes indicating more rainfall is infiltrated into the subsurface layers when we use larger coupling δ t however the differences between these three coupling time steps are almost negligible compared to the differences between models as shown in sections 3 1 3 2 and 3 3 although a coupling time step of 1 m i n appears to be sufficient from this set of tests we choose coupling time steps of 6s in all the pflotran simulations cases to be conservative 3 5 slab case previous studies have demonstrated that spatial heterogeneity of subsurface hydraulic properties has a significant impact on runoff generation woolhiser et al 1996 merz and plate 1997 here we adopt a testing case with spatial heterogeneity following the work of kollet and maxwell 2006 we use the same inclined sloping plane domain as for the previous two cases and the subsurface soil is uniform with a saturated hydraulic conductivity value of 6 94 10 4 m m i n except for a 100 m long 0 05 m thick slab with a very low hydraulic conductivity of 6 94 10 6 m m i n fig 10 the slab with low hydraulic conductivity is designed to generate localized infiltration excess runoff while the rest of the domain will generate runoff through saturation excess the grid size for surface and subsurface flows is 10 m in the horizontal direction and 0 05 m in the vertical direction the manning s roughness coefficient boundary conditions for surface and subsurface flows rainfall intensity and duration fig 11 a recession period and domain of simulation are the same as in the previous saturation and infiltration excess cases modeled outflow is measured at one grid cell on the outlet boundary the initial water table is set at the 1 0 m depth the soil parameters are listed in table 2 and the van genuchten relationships are used to describe the soil hydraulic retention characteristics pflotran shows good agreement with the other three models on the basis of the modeled outflow results fig 11b the hydrograph is more complicated than the previous two cases showing two steep rising segments connected by a flat segment followed by recession the first rising segment of outflow is caused by infiltration excess on the low hydraulic conductivity slab and the second one is caused by saturation excess after the subsurface domain outside of the low permeability slab area reaches full saturation between 90 and 150 m i n s as illustrated in fig 12 the peak flow from pflotran is very similar to those produced by parflow and hgs cathy produces a slightly lower peak flow value to investigate how the subsurface saturation progresses over time in the slab case especially in the area adjacent to the low permeability slab we take x z cross sectional snapshots of the subsurface saturation profile produced by pflotran at times of 0 60 90 150 200 and 300 m i n s as shown in fig 12 this selected set of times span from initial time through the end of the modeled recession period and include times during the steep rising limb and at the peak coincident with the end of the precipitation period together they illustrate the complex physical responses of heterogeneous soil columns to infiltration saturation and lateral unsaturated flow while the water table in soil columns with uniform larger hydraulic conductivity rises quickly due to saturation excess the water table in the soil columns with heterogeneous lower hydraulic conductivity slab on top rises very slowly due to limits on the infiltration capacity surface ponding immediately occurs above the low permeability slab due to infiltration excess then the subsurface domains downstream of the slab become saturated due to infiltration of surface flow routed from above the slab the saturation in the slab area increases due to lateral flow that occurs in addition to the infiltration through the top boundary similar saturation profiles from other models can be found in maxwell et al 2014 fig 8 the results from pflotran are similar to those from parflow and hgs whereas they show more differences compared to those from cathy the differences between cathy and the other three models in both the outflow time series and subsurface saturation profiles might indicate some coupling numerical errors in cathy caused by flux exchanges between surface cells and subsurface nodes calculated from hydraulic gradient this coupling error could be magnified by complex geological conditions and multiple rainfall events 3 6 borden case the borden test case is based on the original field experiment conducted by abdul and gillham 1989 and it has been widely used as a test case for verifying or comparing hydrologic models jones et al 2006 kollet et al 2017 vanderkwaak 1999 the experiment was conducted on a small catchment consisting of a ditch that is approximately 2 m deep and was irrigated uniformly with water for 50 m i n s the spatial extent of the domain is approximately 20 80 m in the horizontal direction with an arbitrary horizontal base or datum at 0 m fig 13 here we use the 0 5 m digital elevation models dems as the top surface the maximum elevation is 4 642 m and the elevation of the ditch outlet is 2 986 m the initial water table is 0 2 m below the ditch outlet and the unsaturated zone above the water table is set to be hydrostatic initially the soil parameters and hydraulic properties are listed in table 2 and the van genuchten relationships are used to describe the soil hydraulic retention characteristics we set a critical depth boundary condition for surface flow and a no flow boundary condition for subsurface sides and base the simulation period includes the aforementioned 50 m i n s of rainfall with a rate of 3 33 10 4 m m i n followed by 50 m i n s of recession because the soil saturated hydraulic conductivity is 6 0 10 4 m m i n surface runoff is generated by saturation excess the horizontal mesh size is 0 5 m and the vertical grid size is set to gradually increase from 0 05 m at the top to 0 2 m at the base of the domain we compare the time series results of saturated storage unsaturated storage ponding storage and discharge at the outlet obtained from pflotran with those from the other three models as shown in fig 14 note that the original topography data dem is reinterpolated to generate the appropriate computing surface mesh for each model the model surface area is 1022 25 m 2 for all three models the storage assessments are normalized by the model surface area as the vertical mesh discretization differs among the models with irregular meshes the initial normalized saturated and unsaturated water storage differ slightly among the models the initial ponding storage is 0 because there is no surface flow before the irrigation event no normalization is applied to the discharge because only the approximate area of the test site is provided in the original field experiment none of the models is able to capture the exact discharge time series as observed as shown in fig 14d possibly due to errors or assumptions in the domain geometry or due to the influences of unmapped heterogeneity parflow captures the rising limb most closely but does not capture the relatively rapid falling limb seen in the data hgs captures the falling limb well but it fails to capture the rounded response near the peak pflotran produces responses very similar to hgs the peak is somewhat lower leading to a slower rising limb and the falling limb is slower than hgs cathy differs considerably from the other three models the remaining metrics are model derived with no field data available for comparison the normalized saturated storage is very similar among all models fig 14a the normalized unsaturated storage is also similar with pflotran showing a response that falls within the ranges of the other three models fig 14b the largest difference among the models is the normalized ponding storage fig 14c in particular pflotran predicts a higher peak and slower recession this difference is the likely cause of the difference between pflotran outputs and the observations with more water being held in the ponding storage and not released as outflow this in turn may be due to the different dem reinterpolation methods among the models leading to more surface water being retained in pflotran we also examine snapshots of the water saturation at the cross section x 40 m at time 0 the saturation profile reflects the hydrostatic initial condition fig 15 after 20 m i n s of infiltration the shallow subsurface is becoming wetter whereas there is little change in the deeper subsurface this pattern continues until the end of the applied infiltration and is followed by gradual redistribution during recession these results suggest that lateral unsaturated flow does not play a major role in the subsurface hydrodynamics for this field test the results are similar for parflow but differ from those of hgs and cathy see kollet et al 2017 fig 14 as kollet et al 2017 pointed out these inter modal differences should be taken into account when modeling real world applications supporting the use of model ensembles to quantify uncertainty and data assimilation methods to improve model accuracy 3 7 parallel performance the newly developed model coupling surface and subsurface flows inherits pflotran s inherent capacity in massively parallel computing we perform strong and weak scalability studies to assess the parallel performance of the coupled code in the strong scalability test we fix the problem size while changing the number of processes the parallel performance is measured through relative strong scaling efficiency defined as 15 e p t b p b t p p where t b is the execution time on a base process configuration with p b processes for the fixed size problem and t p is the run time with p processes the ideal relative strong scaling efficiency is achieved when e p i d e a l 1 in the weak scalability test a unit problem of size n unknowns dofs is simulated on one processor and both the problem size and the number of processors are increased correspondingly the parallel performance is measured through relative weak scaling efficiency which is written as 16 e n p t n b p b t n p where t n b p b is the simulation time for base configuration while the problem size is n b with p b processors and t n p is the simulation time while the problem size n and the number of processors p are scaled by the same coefficient α i e n α n b and p α p b for ideal weak parallel efficiency the simulation time will remain constant and e n p 1 for any number of processors however due to hardware configuration and software implementation the weak parallel efficiency is commonly less than 1 we conduct several tests on the nersc national energy research scientific computing center cori haswell compute machine each haswell compute node contains 2 processors and each processor has 16 cores thus each node contains 32 cores we first perform the strong scaling test on a sloping plane with the horizontal area of 200 160 m and vertical depth of 0 5 m the sloping plane only includes the slope of 0 05 in the x direction as illustrated in fig 16 the mesh includes 1000 800 5 grids in the x y and z directions respectively leading to a problem size of 0 8 m dofs for the surface flow and 4 0 m dofs for the subsurface flow the soil properties are the same as in the previous saturation excess test case the initial water table depth is 0 3 m and the initial condition of the unsaturated domain is set to be hydrostatic the simulation covers 10 m i n s of rain at a rate of 1 5 10 4 m m i n followed by 10 m i n s of recession as the rainfall rate is smaller than the saturated hydraulic conductivity the surface flow is generated through saturation excess fig 17 illustrates the results for two strong scaling tests where wall clock time is plotted versus the number of cores processes employed the dashed lines in fig 17 show the ideal slope for a linear speedup the green line represents the first test scenario where all cores on a node are utilized before additional nodes are added the second test scenario is shown in blue where a maximum of 2 cores per node are employed the latter scenario is included to elucidate the degradation in strong scaling performance due to the intra nodal memory contention within the first scenario information regarding the number of cores and nodes employed in each scenario is listed in table 4 the wall clock computation time for the surface flow only accounts for about 0 5 of the total wall clock time the fraction is small compared to the fraction of the total dofs associated with surface flow 0 8 4 8 16 7 the substantial difference in computation time may be due to the forward euler time integration scheme applied in the surface flow module compared to the implicit time scheme with iterative solver used for the subsurface flow module in the first scenario it is clear that the speedup is nearly linear between 1 4 and 32 256 processes for surface and subsurface flows respectively the sub linear speedup between 8 and 32 processes in fig 17 is caused by the fixed amount of memory cache being spread among an increasing number of cores on a single node the term fully packed implies that all cores on a node are being used as more cores are employed per node the memory cache is spread thinner causing the performance to degrade beyond 8 processes per node in order to mitigate memory competition between cores on node we applied only 2 processes node in the second scenario so that more nodes were applied in this scenario as demonstrated by the blue line in fig 17 the degradation between 8 and 32 processes disappears and the speedup is linear between 1 and 64 processes for surface flow and 1 256 processes for subsurface flow in this scenario the degradation in parallel performance after 64 processes for surface flow and after 512 processes for subsurface flow is due to decreasing computational work load per process relative to increasing inter processor communication when the number of dofs per process is smaller than 12 k 18 k hammond et al 2014 further illustrates this memory contention phenomenon for the relative weak scaling test we keep the problem domain size fixed and refine the grid resolution as the number of processes increased fig 18 shows the results from several weak scaling tests including two different numbers of dofs per process ideally the wall clock time and efficiency should be constant no matter the change of the number of processes however the surface and subsurface weak scaling efficiencies all drop quickly to below 0 2 as the number of processes increase these pflotran simulation results were generated using the stabilized biconjugate gradient bicgstab krylov solver with block jacobi ilu 0 preconditioning for the subsurface flow to obtain increasingly optimal weak scaling performance multilevel preconditioning would need to be employed within pflotran a potential topic for future research kollet et al 2010 and osei kuffuor et al 2014 demonstrate superior weak scaling performance of the multilevel solvers 4 conclusions in this study we couple a surface flow process to the high performance subsurface reactive flow and transport code pflotran the diffusion wave equation is used to represent the surface flow the coupling technique is based on the boundary condition switching method which ensures the continuity of pressure and flux at the surface subsurface interface we compare the performance of the developed model with other three similar hydrological models using several numerical benchmark test cases and a field scale test case the results from those numerical benchmark tests show good agreements among all of the hydrological models some model discrepancies are resulted from different discretization schemes used in individual models from the field scale test we find that the discrepancies in surface ponding and outflow become larger under more complex topography we also find that the consistency of spatial discretization from surface to subsurface domain with finite volume method in pflotran avoids the interpolation of exchange fluxes and surface ponding heads errors that have been reported in cathy the sequential non iterative method could reduce computing cost while achieving acceptable accuracy the parallel scaling test results show that our coupled hydrological model exhibits good strong parallel scaling and they also provide guidance on how to choose the number of computing processors to optimize computation time for large scale hyper resolution hydrological simulations pflotran s modular object oriented design affords great flexibility to link more subsurface processes such as reactive solute transport heat transfer and ecological processes driven by the integrated hydrological processes between the surface and subsurface domains such further extensions will pave the way for researchers to study coupled hydrobiogeochemical processes in watershed systems as well as at critical interfaces such as river corridors and coastal regions that are known to be biogeochemical hot spots of the earth system code availability the pflotran source code used in this research may be cloned from https gitlab pnnl gov sbrsfa pflotran ems see the file installation readme for installation instructions and the directory ems input decks for test input decks the source code is compatible with the xsdk 0 2 0 git tag of petsc https gitlab com petsc petsc declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this research was supported by the u s department of energy doe office of biological and environmental research ber as part of ber s subsurface biogeochemical research program sbr this contribution originates from the sbr scientific focus area sfa at the pacific northwest national laboratory pnnl and was supported by the partnership with the ideas watersheds this research used high performance computing resources from the university of arizona research data center rdc and the u s doe national energy research scientific computing center nersc we thank dr stefan kollet and dr mauro sulis for providing us with the dem data and model simulation results of borden case all the pflotran simulation results used to produce the figures in this study can be found at https gitlab pnnl gov sbrsfa pflotran ems or requested by contacting the corresponding author pnnl is operated for the doe by battelle memorial institute under contract de ac05 76rl01830 this paper describes objective technical results and analysis any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the u s department of energy or the united states government a soil retention curves and characteristic curves in this study we combined van genuchten s soil water retention curve and mualem s relative permeability function to calculate water saturation s and relative permeability k r for subsurface flow respectively they are expressed as following equations a 1 s e s s r s 0 s r 1 α p p c n m m 1 1 n a 2 k r s e 1 1 s e 1 m m 2 where s e is effective water saturation s r is residual water saturation s 0 is maximum water saturation α p p a 1 and n are van genuchten parameters that depend on the soil properties p c p a is capillary pressure we usually obtain the van genuchten parameter α m 1 so the conversion between α p and α is needed and it is listed as below equation a 3 α p α ρ g where ρ k g m 3 is mass water density g m s 2 is gravity 
