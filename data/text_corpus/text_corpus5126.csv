index,text
25630,complex environmental and public policy decisions profit from structured procedures such as multi criteria decision analysis mcda to support such decisions the new open source application valuedecisions provides advanced analysis and visualization with no programming expected from users based on multi attribute value theory mavt it offers analysis for decisions with conflicting and interacting objectives multiple stakeholders and uncertain consequences of options programmed in r the shiny web framework makes it accessible via a graphical user interface in the browser we exemplify using valuedecisions for a wastewater infrastructure planning case in the paris region we surveyed preferences of 655 citizens and conducted sensitivity analysis of preference parameters the best management options were robust across a range of preference profiles and assumptions to evaluate the app we developed a novel usability test based on the iso standard for software quality and surveyed students using valuedecisions for case studies graphical abstract image 1 keywords multi criteria decision analysis mcda software multi attribute value theory environmental decision analysis open source population survey 1 introduction environmental decisions and other public policy problems have characteristics that make them messy complex and difficult to tackle in a rational way they need to be addressed in a social context often concerning many stakeholders or decision makers with diverse views french and geldermann 2005 these may include citizens future generations and other interest groups with opposed values resulting in conflicts of interest because of the plurality of stakeholder views and the extent of decision consequences in space and time multiple objectives need to be considered in these decisions gregory et al 2012 haag et al 2019c since it is usually not possible to fully achieve all objectives difficult trade offs are required furthermore the outcomes of decision options and their environmental impact is uncertain reichert et al 2015 in response to these challenges structured transparent procedures such as multi criteria decision analysis mcda have been developed while the umbrella term mcda encompasses various approaches see belton and stewart 2002 greco et al 2016 cinelli et al 2020 the common element is the development of a quantitative or qualitative model of the decision situation these models support structuring information making trade offs and choosing decision options to implement mcda approaches are increasingly considered and applied for environmental problems hajkowicz and collins 2007 huang et al 2011 cegan et al 2017 esmail and geneletti 2018 and used in government agencies to address public issues kurth et al 2017 our work is based on multi attribute value and utility theory mavt maut keeney and raiffa 1993 in environmental management decision support based on this conceptual background has also been popularized under the term structured decision making gregory et al 2012 decision making processes can be facilitated by software tools that allow interactive model building exploration and visualization of results accordingly mcda approaches have often been developed in tandem with respective decision support software see korhonen et al 1992 for an early review especially group decision support has a tradition of technology driven approaches morton et al 2003 consequently a wide variety of mcda software has been developed online collections list more than 45 different products ewg mcda 2020 mcdm society 2020 systematic reviews and comparisons of software products are provided by french and xu 2005 and weistroffer and li 2016 for general applications and in mustajoki and marttunen 2017 specifically with regard to environmental planning processes when deciding which software to use one faces trade offs for instance regarding feature richness adaptability user friendliness speed or costs these objectives are often conflicting and software developments found different answers to these conflicts mustajoki and marttunen 2017 the fit of a software tool to the specific context in which it is used is crucial belton and hodgkin 1999 french and xu 2005 based on applied research in participatory environmental and public policy decision support e g borsuk et al 2008 lienert et al 2011 zheng et al 2016 haag et al 2019b harris lovett et al 2019 we identified five common requirements for decision analysis software in theses contexts i flexibility in representing stakeholder preferences across multiple potentially conflicting objectives ii ability to consider oftentimes large uncertainty of predicted consequences and stakeholder preferences as well as support to understand robustness of conclusions iii capability to compare results for multiple stakeholders iv provision of analysis via a graphical user interface to users such as consultants who do not necessarily have programming skills and v extendability and adaptability to new requirements in addition to these context specific requirements there are fundamental requirements of software quality see si 5 this study presents tests and evaluates a novel open source application valuedecisions which we developed to target these requirements as described above there is a rich history of software that has been successfully used to support multi criteria decisions see korhonen et al 1992 french and xu 2005 weistroffer and li 2016 mustajoki and marttunen 2017 however we found the flexibility to represent preferences and uncertain predictions often limited and a unified approach to all five requirements at once missing the main properties of valuedecisions to tackle the requirements are i support for complex hierarchical mavt models by combining non linear lowest level value functions with non additive aggregation functions that can represent most preference structures ii consideration of a range of probability distributions for predictions interactive sensitivity analyses and different visualizations of uncertainty iii simultaneous comparison of multiple stakeholder preference profiles without forcing aggregation of individual preferences iv a graphical user interface that can be accessed via a web browser with many options to save analysis results and v open source development based on r r core team 2021 which allows for extensions by interested users this combination of features is to the best of our knowledge unique valuedecisions attempts to strike a balance between simplicity and analytical possibilities that are relevant in the environmental field while being extensible free of charge and easily accessible for participatory decision support other aspects than the sheer feature richness of software are relevant one aspect we want to highlight is the issue of access our intention is to make multi criteria analysis accessible to interested users or stakeholders such as municipalities public authorities consultants and students even if users do not conduct the main analysis themselves it would be helpful if they could engage with decision models created by researchers for instance to increase trust and acceptance in the analysis hämäläinen 2015 voinov et al 2016 access can be limited by cost of commercial software incompatibilities of hardware software or operating systems and user rights in organizations we require new ways to provide analysis and models to stakeholders web apps as the app we introduce here are one comparatively new possibility to mitigate these access impediments the open source development means that the implementation is transparent and the app can be used free of charge access can also be limited by the required technical expertise to use software as a community we benefit from a variety of tools that fulfill needs of different target users see section 3 2 complex models and analysis are often only possible with frameworks that require programming such as utility for r reichert et al 2013 or decisi o rama for python chacon hurtado and scholten 2020 this can make analytic capabilities that are useful in environmental decision support for example regarding uncertainty inaccessible to interested users with valuedecisions we wanted to build a bridge by making advanced analysis available via a graphical user interface without the need to program this benefits users without programming skills but also facilitates interactive model exploration in workshop settings we propose that software development similar to decision making can profit from value focused thinking keeney 1992 while all software developers have an interest to praise their own tool the objectives of what should be achieved with the software should be kept at the forefront despite its importance empirical evaluation of decision support systems remains rare walling and vaneeckhaute 2020 therefore this study includes a structured evaluation of valuedecisions we used the software product quality requirements and evaluation square series of standards iso iec 25000 as reference in our development and basis to evaluate valuedecisions against defined objectives iso iec jtc 1 sc 7 2011 this iso standard defines two evaluation models to gain insights into the usability of the app we operationalized the quality in use model by a usability test in which representative users performed representative tasks additionally we self evaluated the application based on the product quality model of this framework si 5 as evaluation from an applied perspective we analyzed an unpublished real world environmental decision problem with valuedecisions the remainder of this paper is organized as follows in section 2 we introduce mcda models specifically multi attribute value theory that forms the theoretical basis for valuedecisions section 3 introduces the features and implementation of valuedecisions in section 4 we exemplify its use in a case study for wastewater infrastructure planning in the paris region based on a population survey with 655 respondents section 5 describes the development and results from the usability evaluation we end with a general discussion and conclusions in section 6 2 decision support based on multi attribute value theory mavt 2 1 the decision analysis process a decision making process can be structured as a sequence of phases which are intertwined and iterative in practice we can differentiate between phases of problem structuring data and information collection decision analysis negotiation and conflict resolution implementation and monitoring within this process the aim of multi criteria decision analysis is to develop and evaluate a set or sets of options also called alternatives strategies actions variants or scenarios depending on the literature on a set of objectives also called criteria and provide insights e g about optimal decisions involved trade offs and stakeholder perspectives based on a decision model the valuedecisions app supports the phase of analyzing the decision options from different stakeholder perspectives it builds upon an adequate structuring of the decision problem predictions of uncertain outcomes of options and the completed elicitation of stakeholder preferences the first steps of an analysis are fundamentally important not least due to path dependencies lahtinen et al 2017a in the problem structuring phase the boundaries of the decision problem are set and stakeholders identified e g lienert et al 2015 marttunen et al 2017 then often in a participatory process basic decision elements are defined the objectives what stakeholders find fundamentally important to achieve the decision options with which alternatives strategies actions the objectives can be achieved and attributes how to quantify or measure the performance of options e g gregory et al 2012 next predictions of how each option performs as measured by the attributes are needed predicting potential consequences of options is a major undertaking of natural and engineering science and a large component of some decision analysis projects predictions can be derived from quantitative models scientific literature or expert judgment as predictions are inherently uncertain in environmental problems we propose to express predictions probabilistically reichert et al 2015 the stakeholders preferences are elicited over these potential outcomes of options stakeholder preferences are usually elicited from individuals in interviews or group workshops eisenführ et al 2010 or increasingly online lienert et al 2016 aubert et al 2020 the defined structure and input data then allow us to build an evaluation model of the decision options see section 2 2 predictions and preferences are integrated and results calculated as a result each option receives a value or utility score allowing us to compare the performance of the options for each stakeholder perspective since any model based analysis depends upon inputs parameters and model structure further analysis of the model and conclusions for instance with sensitivity analyses is crucial based on the analysis we can recommend one or several options to implement negotiation or consensus finding among stakeholders as well as implementation and monitoring of a decision demand additional activities see gregory et al 2012 french and argyris 2018 2 2 decision models based on multi attribute value theory the theoretical underpinning of valuedecisions is multi attribute value theory mavt keeney and raiffa 1993 which is based on few rationality axioms it has several advantages as a concept for environmental decision support discussed in reichert et al 2015 mavt is based on the concept of value functions a value function maps from consequences as measured by attributes to the degree of relative achievement of objectives usually in the interval 0 1 the larger the value the better this option meets the objectives given the stakeholders preferences the options can then be ranked according to their values in case of uncertain outcomes a utility function provides this ranking of options in a rational decision the option with the highest value should be chosen when a decision problem covers multiple objectives a multi attribute value function is built which returns a single value based on the consequences of each option on several attributes a stepwise hierarchical procedure helps constructing this multi attribute value function 1 determine an objectives hierarchy to structure the model 2 identify shape and parameters of the lowest level value functions i e marginal value functions 3 find the type and parameters for aggregating values upwards in the hierarchy to calculate the overall value and optionally 4 convert values into utilities below we briefly explain each concept see reichert et al 2013 2015 or haag et al 2019a for an in depth treatment we propose to create a separate decision model for each stakeholder and compare the outcomes though ideas to aggregate over stakeholders also exist see section 3 4 5 2 2 1 objectives hierarchy objectives can be structured in form of a hierarchy keeney 1992 this also determines the structure of the evaluation model in many cases it is advisable to include no more than 10 15 objectives marttunen et al 2018 and reduce a higher number of objectives marttunen et al 2019 an example objectives hierarchy is given in the case study fig 3 2 2 2 lowest level value functions each lowest level objective is evaluated with respect to its attributes with help of a value function this leads to a non monetary common unit that allows comparing different types of attributes e g costs in with high phosphorus recovery in for simplicity we only consider lowest level value functions v i for single attributes x i not over several attributes such a value function v i x i is defined over the range which the attribute can take in the decision case covering all options e g costs from 90 to 150 a value of 0 corresponds to the worst possible level given the range of the attribute e g costs of 150 and a value of 1 to the best possible level e g costs of 90 the value function thus measures the relative degree of achievement of an objective the shape of the lowest level value functions depends on stakeholder preferences sometimes attribute levels may be translated linearly to values fig 1 lower panels but often stakeholders have non linear preferences e g scholten et al 2015 zheng et al 2016 langhans and lienert 2016 several methods for eliciting the shapes of the value functions exist eisenführ et al 2010 commonly we will obtain pairs of outcomes and corresponding values and can then interpolate between them or fit a parameterized function 2 2 3 aggregation model to calculate the total or overall value v x a of an option a across all consequences x a x 1 a x n a the performance on all objectives is aggregated this means the values of lower level objectives v i x i a are aggregated to a value at the next higher level of the hierarchy and so forth until a single total value v x a for each option a is reached see reichert et al 2013 2015 haag et al 2019a the most commonly used aggregation model for mavt is the additive model belton and stewart 2002 here the weighted arithmetic mean is used to calculate the aggregated value v x eq 1 v x 1 x n w i 1 n w i v i x i where v x 1 x n is the total value over the consequences x 1 x n v i x i is the value for the consequence measured by attribute i and w i is a weight parameter for attribute i with i 1 m w i 1 thus in addition to the lowest level value functions this model requires to specify weight parameters also called scaling factors weights can strongly affect the results see section 4 3 2 and weight elicitation is especially prone to biases morton and fasolo 2009 riabacke et al 2012 therefore careful weight elicitation with tested methods such as swing or trade off is advisable eisenführ et al 2010 elicited weights may still remain ambiguous for example because stakeholders may be inherently uncertain because preferences change in different future scenarios or because mistakes during elicitation occurred a possible procedure to investigate the effects of this uncertainty is local sensitivity analysis on each weight eisenführ et al 2010 section 4 3 the additive model is not always an adequate representation of stakeholder preferences an implication of the additive model is the possibility to compensate if one objective performs poorly e g high phosphorus recovery is not achieved it can be compensated by another well performing objective e g high water savings according to the given weights practical experience in many cases indicates that stakeholder preferences actually do not agree with some assumptions and implications of the additive model e g rowley et al 2012 langhans and lienert 2016 zheng et al 2016 haag et al 2019a reichert et al 2019 other non additive aggregation models have less strict assumptions and allow for interaction between objectives grabisch et al 2009 however non additive aggregation is so far rare in mcda software for mavt maut and not even mentioned in the reviews by mustajoki and marttunen 2017 or weistroffer and li 2016 exceptions are logical decisions logical decisions 2020 which supports multiplicative aggregation and the tool by cinelli et al 2021 which supports several mean functions an interesting alternative to the weighted arithmetic mean for aggregation in mavt is the weighted power mean also called weighted generalized mean haag et al 2019a 2019b using this aggregation function we are able to model various interactions between objectives such as partial non compensation to our knowledge valuedecisions is the first mcda software with graphical user interface that implements this aggregation the power mean is given by eq 2 v x 1 x n w γ i 1 n w i v i x i γ 1 γ f o r γ 0 i 1 n v i x i w i f o r γ 0 the weights and value function parameters are as in eq 1 we informally refer to the additional parameter γ as the non additivity parameter because for γ 1 the model reduces to the additive model and by varying γ we can express different non additive behaviors the weighted power mean covers a number of other aggregation functions of interest as special cases for certain values of γ see si 1 for an in depth discussion of their properties we refer to langhans et al 2014 and grabisch et al 2009 2 2 4 uncertainty and risk utility functions when outcomes of a decision are uncertain a stakeholder s risk attitude determines how to evaluate this uncertainty this can be captured with multi attribute utility theory maut keeney and raiffa 1993 directly eliciting multi attribute utility functions from stakeholders is demanding eisenführ et al 2010 and to our knowledge rarely done in complex practice applications however it is possible to transform the value function at the highest level of the objectives hierarchy to a utility function dyer and sarin 1982 reichert et al 2015 instead of calculating the value of options the expected utility is calculated this integrates over the uncertain predictions taking into account the risk attitude and leads to a unique ranking based on the expected utility in practice contexts we found it can be advantageous to work with values and explicit visualization of their uncertainties instead of discussing expected utilities 3 the valuedecisions app 3 1 overview valuedecisions supports multi criteria decision analysis modeled with multi attribute value theory it provides a middle ground between sophistication and user friendliness is flexible to extend and adapt and is open source programmed in r r core team 2021 it is accessible to users online as a web application that does not require software installation additionally it is available as a standard r package which can be run locally the users section 3 2 need to be familiar with the principles of mcda to sensibly use valuedecisions but programming capabilities beyond uploading two spreadsheets in the required format is not required valuedecisions is available at https www eawag ch en department ess main focus decision analysis da tools valuedecisions focuses on the mcda modeling stage i e the integration of predictions and preferences this includes calculating the mcda result i e identifying the best performing options and sensitivity analyses to further explore model inputs assumptions and results valuedecisions allows setting up and running a mcda model in a graphical user interface for complex decision problems including uncertainty of predictions many options can be evaluated in parallel for multiple stakeholders with possibly conflicting preferences results are presented in tables and visualized by various graphs a focus is on diverse visualizations of results that can be explored interactively for instance in a workshop users can easily change preference parameters to analyze the sensitivity of the results in steps including shapes of lowest level value functions non additive aggregation models and local sensitivity analyses of weights they can download intermediate and final results as individual graphs and tables or compiled in a report 3 2 target users and use cases we can differentiate between three potential user groups for mcda software mustajoki and marttunen 2017 1 do it yourself users belton and hodgkin 1999 who would like to apply mcda but have no specific training or experience 2 analysts and facilitators who need software support for facilitation analysis and visualization and 3 analysts and consultants who want to carry out sophisticated analyses valuedecisions is mainly targeted for the latter two user groups it can support consultants working in environmental decision cases public planning authorities municipalities other practitioners or students in the analysis phase of a mcda process these users may lack finances time or expertise to work with sophisticated software or programming r python matlab etc but wish to go beyond the limited capabilities of spreadsheet software additionally valuedecisions is powerful enough that academic researchers can explore applied mcda problems the app can be used in workshops with interactive exploration of options or in backroom analyses since valuedecisions requires a carefully conducted problem structuring and data collection phase to have taken place it is not meant for do it yourself users that said if stakeholders are provided the input files they can further explore the decision model for example to facilitate transparency and traceability voinov et al 2016 3 3 elements of mcda model and their implementation 3 3 1 input data on predictions and preferences users need to upload a file with information on the predicted performance of each decision option for each attribute predictions and a file with stakeholder preferences about these predictions preferences these are uploaded in two spreadsheets excel or tab separated values in the files the users enter all information for the analysis templates for the files can be downloaded once uploaded valuedecisions first conducts several checks for file validation the predictions file example in table si 1 specifies for each option the predicted performance of each attribute since the consequences of options for environmental decisions are usually uncertain probability distributions for the predicted outcomes can be specified for each option and attribute see section 3 3 2 the second file example in table si 2 specifies the structure of the objectives hierarchy of stakeholders used for evaluation as well as information on stakeholders preference parameters only the objectives hierarchy is essential to run valuedecisions including the names of upper level l2 and lower level l1 objectives attribute names and units and the best and worst possible outcome of attributes each stakeholder can have a different objectives hierarchy however the interpretation of results can become more complex if stakeholders do not share objectives in addition to the objectives hierarchy preference parameters for several different stakeholders can be defined including the shape of lowest level value functions global weights assigned to attributes and type of aggregation to be used in the model if no preference data are specified or a group of parameters is missing defaults are assumed these are that lowest level value functions are linear attributes have equal weights and the aggregation function is the weighted arithmetic mean additive model this can be useful for interactive use of valuedecisions for instance in a workshop the options can be explored together with the stakeholders and the preference parameters can be varied directly in the app interface 3 3 2 uncertainty of predictions in many environmental and public policy decisions there is large uncertainty about the consequences of management options in valuedecisions users can either calculate the results based on point predictions for each attribute and option or provide parameters of probability distributions for these predictions implemented probability distributions are uniform normal lognormal triangular beta generalized extreme value distributions and discrete distributions valuedecisions will generate samples from the specified distributions and propagate the uncertainty to the results with monte carlo simulation this leads to a distribution of the total value v x a of each option and to a distribution of rankings currently only independent probability distributions can be used i e the predictions of one attribute cannot depend on the predictions of another with other software e g utility reichert et al 2013 it is possible to work with predicted samples of non independent distributions however this requires that users create these samples beforehand with some creativity it is possible in valuedecisions to combine mcda with scenario analysis to account for large future uncertainty e g stewart et al 2013 scholten et al 2015 zheng et al 2016 the performance of options given a scenario can be entered as separate options e g option a would occur twice in the prediction file as a scenario1 and as a scenario2 and the results can be analyzed across scenarios alternatively different predictions and or preferences for each of the future scenarios can be uploaded and the results compiled outside of valuedecisions 3 3 3 lowest level value functions for each attribute on the lowest level of the hierarchy outcome value pairs that were elicited from stakeholders can be provided using least squares valuedecisions then fits a single attribute value function to these outcome value pairs the fit can be evaluated visually for lowest level value functions only single attribute value functions not multi attribute value functions are possible a wide variety of shapes can be specified such as linear exponential or sigmoid functions see si 2 alternatively linear interpolation between provided values or discrete value functions can be used thus users can represent many types of non linear preferences with fewer restrictions than in many other software products if no preference information is provided for an attribute linear value functions are assumed as default the effect of this assumption can be explored with sensitivity analysis see section 4 2 2 3 3 4 aggregation for multi attribute value functions in valuedecisions the objectives hierarchies used for evaluation can have two levels as default valuedecisions uses the additive model to calculate overall values the weight parameters for objectives need to be elicited from the stakeholders if no preference information for weights is provided equal weights are assumed for all attributes because valuedecisions also implements the weighted power mean as aggregation function eq 2 many alternative ways for aggregation besides the additive model can be used see si 1 this flexible approach to aggregation together with non linear lowest level value functions allows representing complex preference structures for each branch of the objectives hierarchy the γ parameter which specifies the degree of non additivity can be entered in the preferences input file in addition to the weights alternatively within valuedecisions a slider allows adapting the value of the parameter γ across the entire hierarchy allowing to explore the effects of using different weighted power mean functions 3 4 features and flow 3 4 1 structure and controls to users valuedecisions is essentially a website the basic flow of the app is organized as consecutive pages users can switch between pages via a navigation menu on top fig 2 on the start and information pages information and checklists are provided on the upload page users upload the two input files to the analysis predictions and preferences on the analysis page an interactive mcda analysis is run and on the reporting page a report containing graphics and tables of current results can be downloaded for instance as word file the resulting plots and tables of all steps of the analysis can also be saved individually the heart of valuedecisions is the analysis page fig 2 on its left side a control panel allows users to interact with the analysis on the right side users can switch between different types of analyses and displays that are organized in tabs on the control panel a run update button re runs the analysis after adjusting settings it is possible to switch between analyses with or without uncertainties in the predictions and adjust the number of monte carlo samples three filters can be applied to select 1 stakeholders for which results should be calculated 2 decision options for which results should be calculated 3 objectives for which results should be shown this only applies to certain plots the following parameters of the preference models can be changed this can be combined in arbitrary ways 1 the shape of the lowest level value functions can be changed collectively if this is active all lowest level value functions for all stakeholders are converted to exponential value functions with the curvature parameter c specified by a slider see eq si 3 for formula 2 the weights assigned to objectives can be adjusted individually users can select individual objectives and give these a new weight using a slider the newly assigned weight is applied for every stakeholder the other weights which were not modified are renormalized so that the previous relationships remain unchanged and the sum of weights equals one consequently the newly assigned weight will be the same for all stakeholders but the other weights remain stakeholder specific 3 the aggregation function can be changed collectively if this is active the aggregation model specified in the preferences input file or without specification the default additive aggregation is switched to the weighted power mean section 2 2 3 the value of the non additivity parameter γ can be adjusted with a slider see si 1 for implications the same value is assumed for all stakeholders and across all branches in addition to the control panel there are controls in the tabs that show the analysis results here users can switch between different presentations of the results for instance different types of graphs or switching the x axis variable 3 4 2 visualization of input data and intermediary steps exploratory visualization of the input data before calculating the full mcda model allows checking for mistakes in data entering and already gives insights into differences between stakeholders and options the structure of the evaluation is given by the hierarchy of objectives example in fig 3 the first tab on the analysis page shows the predicted performance of each option when uncertainties are considered the empirical distributions of the samples are shown as obtained by monte carlo sampling example in fig si 2 on the next tab the lowest level value functions that were fitted to the elicited preference data or modified in the app are displayed example in fig 1 a third tab shows the weights the stakeholders provided or that were specified in the app example in fig 4 3 4 3 mcda results the next three tabs of the analysis page provide results of the decision model and additional analysis there are several options for different types of graphics in the values tab the overall and partial values of each option are displayed example in fig 5 visualization options differ if uncertainty of the predictions is included in the ranks tab the overall rankings of the options are shown example in fig 2 or the rank distributions in case of uncertainty example in fig 9 and fig si 7 in the x vs y tab the resulting values for selected groups of objectives can be related to each other the most common example is plotting costs against benefits fig 10 the efficient frontier of cost efficient options can also be shown 3 4 4 sensitivity analyses valuedecisions allows extensive step wise sensitivity analyses and robustness analyses examples in schuwirth et al 2012 zheng et al 2016 haag et al 2019b one way to conduct this is via the controls for preference parameters left side panel fig 2 they can be varied interactively and in arbitrary combination by users to study the impacts on the results it can be tested whether results are sensitive to a the assumption of linear lowest level value functions or other functional shapes b changes of individual or combinations of weights and c the choice of the aggregation model for didactic reasons the effects of choosing a non additive aggregation model can also be visualized for two attributes as in reichert et al 2013 because results are often sensitive to weight parameters an additional local sensitivity analysis for the weight of each objective is possible example in fig 7 it shows the resulting overall values of the options when an objective s weight is varied from 0 this objective is not at all relevant in this context to 1 only this objective is relevant none of the other objectives matter a more advanced approach the weight stability interval wsi allows calculating intervals in which partial orders of options are stable mareschal 1988 in valuedecisions such intervals can only be visually read off the figures fig 7 3 4 5 comparing stakeholder groups preferences within and across groups can be considered in different ways in decision modeling see e g eisenführ et al 2010 and resulted in corresponding software implementations mustajoki and marttunen 2017 one option is to determine aggregated preference profiles the simplest approach is to average preference parameters across stakeholders for example aggregating individual weights using a weighted mean or median also see section 4 2 1 this can be sensible if we are interested in generalized preferences of large groups such as a population sample another possibility is to aggregate the utility or value of decision options across stakeholders keeney 2013 in environmental decisions stakeholder groups commonly have conflicting or opposing views gregory et al 2012 this calls for an approach that allows for comparisons by treating the preferences of individual stakeholders or groups separately we need to see in how far conflicting preferences affect the results this can then point towards leverage points for compromise options or be an input to further stakeholder deliberations learning about the stakeholders opinion can often more important than calculating a final ranking of options e g mustajoki and marttunen 2017 valuedecisions follows this latter approach an overall consensus is not enforced by the modeling approach but instead several stakeholders can be analyzed individually and compared interactive variation of weights and other preference parameters see section 3 4 4 can facilitate finding paths to consensus options the explicit visualization of conflicting results can be a way forward to further negotiation and deliberation this would be lost by averaging over the conflicting views 3 5 implementation as a reactive web app based on r valuedecisions is written in the programming language and software environment r r core team 2021 the mcda modeling and calculations partly build on our earlier work e g haag et al 2019b as well as the r package utility reichert et al 2013 being open source and using r means that the app is relatively easy to change and extend by users familiar with r in comparison to proprietary software using bespoke frameworks and data structures the interactive graphical user interface in the form of a web application is achieved with the shiny framework chang et al 2021 in addition we use several extension packages to shiny for visualizing tables different types of widgets and others most figures rely on ggplot2 wickham 2016 the reporting as downloadable word file was achieved with rmarkdown xie et al 2018 the creation of an r package followed the golem framework fay et al 2021 more generally the combination of specifying analysis in a programming language common in science e g r python or julia in the backend of an application and then building a web app to make the analysis available to users or stakeholders by a graphical user interface is a promising development model it offers many possibilities for scientific work but also for interactive communication of scientific results to stakeholders shiny applications are based on reactive programming which is different to an imperative if this do that programming style in a shiny application we define the endpoints outputs and their relation to sources inputs via conductors intermediate steps in other words we define the relationship of elements for instance from input files via calculations to a graph with results however we do not specify the exact time or order that calculations should occur in instead calculations are run when they need to valuedecision will recalculate and update only those outputs that are affected by user changes this is useful for interactive analysis as it lowers the required amount of recalculation because reactive programming focuses on the relation of elements it is relatively easy to add or remove components without affecting the whole valuedecisions can be accessed as a web app or downloaded as a standard r package to be executed locally building upon r makes the app platform independent it can be run on windows linux or macos systems deployment as a web app has advantages for users since they can use valuedecisions if they can run a modern web browser on their device independent of operating systems hardware and other installed software because valuedecisions is essentially a website its design is responsive and it can also be viewed on devices such as tablets or smartphones 4 application mcda for wastewater infrastructure planning in the paris region 4 1 context and decision structure the rivers of the greater paris region face increasing pressure partly caused by the wastewater management system esculier et al 2015 two factors contribute typical for other large cities worldwide lossouarn et al 2016 firstly continuous population growth leads to increases in wastewater production thus the pollution load entering wastewater treatment plants increases secondly regional river discharge is predicted to decrease due to climate change flipo et al 2021 this will lead to reduced dilution of the treated wastewater in receiving rivers currently wastewater management in the paris region complies with the french transposition of the water framework directive on nutrient concentrations siaap 2021 however given the pressures of population growth and climate change the main paris wastewater management authority has to invest significantly to avoid serious future damages of water ecosystems today s point in time is also suitable to make changes because of new infrastructure projects in the paris region le grand paris project dila 2021 innovative technical options have the potential to address the pressures at hopefully reasonable costs but entail radical system changes e g larsen et al 2016 hoffmann et al 2020 they imply moving from the centralized system where wastewater is transported in sewer networks to large treatment plants to semi or fully decentralized systems where wastewater is treated locally for instance in individual buildings waste streams like urine feces and greywater may be separated at the source which has technical advantages and allows resource recycling larsen et al 2009 decisions concerning future wastewater management inevitably affect citizens before deciding the main paris wastewater authority needs to know how the public would perceive unconventional decentralized options we conducted a study to evaluate options for urine and feces management for the paris region a workshop with 18 local stakeholders was held in june 2016 to select objectives for evaluating wastewater management options in new districts and to elaborate innovative options details in si 4 1 this process led to agreeing upon nine fundamental objectives and five wastewater management options the five selected options are status quo or business as usual 1 status quo urine source separation with on site concentration 2 usep conc urine source separation with on site storage 3 usep store urine and feces collection in a separate vacuum network with decentralized treatment 4 vacuum and dry toilets with on site underground composting chambers 5 compost the nine objectives were organized in a two level objectives hierarchy fig 3 the decision case was also the focus in an experimental paper addressing online preference elicitation for mcda aubert et al 2020 here we present novel results based on a representative population survey in the paris region we deployed two online population surveys to collect citizens preferences concerning the importance of objectives and trade offs they are willing to make which can be expressed by weights the surveys using swing weight elicitation and direct rating eisenführ et al 2010 are presented in aubert et al 2020 the obtained samples were representative of the regional statistics in terms of gender age and occupation the data from the population survey and the data on the predictions i e how the options perform on each attribute were reformatted as required by valuedecisions see examples predictions table si 1 preferences table si 2 these input files are available in the data package see data availability we aimed to generate advice for the main paris wastewater authority who needs to know how the public would perceive unconventional decentralized wastewater management options the main question in this study is are the results of the mcda robust concerning the best performing wastewater options despite possibly differing preferences of the sampled population and various uncertainties which wastewater options can we recommend to the paris authorities using valuedecisions we analyzed the decision problem including a comprehensive sensitivity analysis we investigated how robust the outcomes are to the weights the non elicited preference parameters the uncertainty of the predictions and visually explored costs of options versus their benefits 4 2 analyses 4 2 1 weight parameters and their influence first we investigated whether the results of the mcda differed between preferences of survey respondent groups of the paris region population one survey group did a direct rating of objectives dirrat n 357 and the other went through a swing weight elicitation within the latter group we discovered that 262 respondents did not understand the procedural instructions discussed in aubert et al 2020 we treated this group separately swinginvalid n 262 from the group that understood the instructions swing n 36 to obtain generalized weights we aggregated the individuals weight for each objective by taking the median weight for each of the three groups to obtain a median weight profile for each group we normalized the nine median weights to sum to one to check the sensitivity of the mcda results to the weight profiles we ran the analysis in valuedecisions handling the three groups as three stakeholders secondly as a sensitivity analysis we created two fictional extreme weight profiles as the weight profiles from the survey were similar between groups fig 4 these profiles are based on the idea of environmental attitudes milfont and duckitt 2006 stating that one can have a preservationist or utilitarian environmental attitude in the preservationist profile we equally distributed the weights between the objectives of protecting the environment and sustainable resource use ignoring the actual ranges of the attributes in the utilitarian profile we equally distributed the weights between social well being and economic performance we ran the sensitivity analysis in valuedecisions handling these two extreme weight profiles as two stakeholders thirdly we used the median weights of the swing group medswing to carry out a local one factor at a time sensitivity analyses e g eisenführ et al 2010 for two critical objectives first we doubled the weight of the low cost objective which is often decisive for the authorities and can be systematically underestimated in mcda marttunen et al 2018 additionally using the local weight sensitivity analysis in valuedecisions we investigated the results for all possible weights for the objective high chance of compliance by end users which might be the most critical to achieve a paradigm shift in wastewater management 4 2 2 non elicited preference parameters lowest level value functions aggregation model for time reasons it was not possible to elicit lowest level value functions and test whether the implications of using the additive model comply with the respondents preferences in the survey to test the robustness of the mcda results we carried out sensitivity analysis changing the two standard assumptions of using linear lowest level value functions and the additive model we used the weight profile of the swing group n 36 with median weights medswing for this analysis for the lowest level value functions we tested concave and convex shapes using an exponential function and setting the curvature of all value functions to c 5 or 5 respectively see eq si 3 we also repeated the analysis using the more general power mean aggregation to check the sensitivity of the results for following aggregation functions additive as comparison the aggregation parameter γ being 1 approaching the weighted geometric mean γ 0 04 and exploring the space between additive and geometric mean including γ 0 5 furthermore we included the two extremes maximum aggregation γ approaching and minimum aggregation γ approaching 4 2 3 uncertainty of predictions valuedecisions enables exploring the effect of uncertain predictions by propagating the uncertainty to the results with monte carlo simulation in our case this is particularly relevant given that high uncertainty was associated with some predictions e g for the objective high number of local jobs fig si 2 we investigated how these uncertainties affect the outcome to estimate whether it is worth or necessary to invest additional time to improve the predictions 4 2 4 cost benefit visualization valuedecisions enables us to represent the aggregated value over any combination of objectives as a function of any other combination of objectives such as low cost a cost benefit analysis is usually important for stakeholders they need to know if paying more actually pays off i e makes a difference in terms of achieving higher values on all other objectives we used the x vs y analysis in valuedecisions selecting the objective low cost for visualization on the x axis and the aggregated value from the mcda for the other eight objectives on the y axis again we used the swing median weight profile 4 3 results and discussion of the analyses 4 3 1 visualization of objectives predictions weights and lowest level value functions valuedecisions has various possibilities to visualize the input data as well as intermediate steps of the analysis this is helpful to verify that data were entered correctly but also to understand the results the evaluation structure is given by the objectives hierarchy fig 3 for the predictions we checked that the data displayed in the graphs and the tables corresponded to our input files for lowest level value functions we verified whether their shapes and slopes were as intended example in fig 1 we also visualized the performance of all alternatives on the lowest level objectives fig si 1 for the weights fig 4 we checked correspondence of displayed weights with the original input file at the lower and upper level of the objectives hierarchy 4 3 2 effect of weight profiles first we compared the effect of the three groups median weight profiles swing swinginvalid and dirrat obtained through the survey weights were relatively uniform within each of the groups in the most spread distribution swing the weights varied from 0 09 to 0 14 fig 4 the ordering of options and the obtained overall values was very similar across the three weight profiles fig 5 option 5 dry toilet with composting chamber had the highest overall values this was followed by option 4 vacuum network with decentralized treatment the worst performing option for all groups was the status quo despite somewhat different weight profiles the mcda results were thus similar for all three groups therefore we focus on the swing median weight profile for the following analyses as the median weights obtained by the survey were similar we created two extreme weight profiles to explore possible effects of other weight distributions preservationist preserve environment and utilitarian high socio economic performance see section 4 2 1 with such contrasted weight profiles fig si 4 the values of options fig 6 and their ranks of options differed greatly between the two groups for preservationists all options except the status quo performed similarly with values ranging from 0 62 to 0 70 fig 6 for utilitarians there was a similarly high achievement for option 5 dry toilet with composting chamber v 0 75 and 4 vacuum network with decentralized treatment v 0 61 this resulted from option 5 performing relatively well on all objectives except on social wellbeing fig si 1 however for utilitarians the performance of option 3 urine source separation with on site storage v 0 20 and option 2 urine source separation with on site concentration v 0 17 was markedly poor compared to preservationists the utilitarians high weights on the objectives social well being and economic performance but low weights on environmental protection and resource use can explain this interestingly the status quo option performed poorly for preservationists v 0 10 but considerably better for utilitarians v 0 32 however despite extremely different weights it was possible to find two consensus options for the groups namely 4 vacuum and 5 compost based on the assumption that the importance of low costs might have been underestimated by the population but is decisive to authorities we interactively doubled the weight of the cost objective from 0 1 to 0 2 for clarity of presentation we focus on the medswing original weight profile fig 4 and the analysis without prediction uncertainty the ranking of options remained the same despite this weight change and the values were of the same order of magnitude fig si 5 option 5 0 76 option 4 0 61 option 2 0 39 option 3 0 38 status quo 0 22 thus even if the importance of low costs might have been underestimated the results were not sensitive to the doubling of the weight valuedecisions also provides graphs for local sensitivity analyses of weights on each objective we discuss results for the objective high chance of compliance by end users cb comp the weight assigned to this objective by survey respondents was relatively low however end user compliance is actually a main concern of the paris wastewater authority as the results show cb comp is a relevant objective in the decision because the values and rankings of options can drastically change depending on the weight fig 7 in the extreme case option 5 compost best performing with the current weights would achieve the lowest performance if the weight of cb comp was increased to 0 62 or higher inversely 1 status quo worst performing with the current weights would achieve the second best value if the weight was increased to more than 0 42 it would even perform best along with option 4 vacuum if only objective cb comp was considered i e having weight 1 if the wastewater authority is not ready to bear a high risk regarding this objective option 4 vacuum might be recommendable it is only outperformed by option 5 compost when the weight for cb comp is relatively low but achieves higher values than option 5 as soon as the weight is higher than 0 19 visualized by crossing lines for options in fig 7 4 3 3 lowest level value functions what if not linear valuedecisions allows jointly changing all lowest level value functions to an exponential shape and varying the curvature parameter c see eq si 3 we tested the effect of strong curvatures where c 5 and c 5 example in fig 1 compared to using linear value functions as in the previous analyses for c 5 both the values and the ranking of options were partly impacted table 1 for c 5 the overall values strongly changed while the ranking remained consistent because option 5 compost or option 4 vacuum performed best in all three cases our recommendation is not impacted the robustness of the main conclusions also suggests that investing more time to elicit lowest level value functions from stakeholders is unlikely to add value to the analysis however this is dependent on the specific case and the model assumptions and needs to be verified in every new application 4 3 4 aggregation model what if not additive valuedecisions allows varying the non additivity parameter γ of the weighted power mean aggregation function to explore the effect of different aggregation models on the results see eq 2 this is relevant since we could not ask stakeholders about their preferences concerning the assumptions and implications of the additive model which often does not comply with people s preferences we tested the effect of various aggregation functions table 2 a rank reversal between option 5 compost being best and option 4 vacuum occurred around γ 0 25 with the minimum aggregation function all options received a value of zero because all options performed worst on at least one of the objectives fig si 1 similarly the high values for the maximum aggregation function are understandable when looking at figure si 1 all options performed best on at least one of the objectives except option 3 usep store which performed second best and quite well on the objective aa nut low discharge of nitrogen to the river and air this analysis again confirmed that the most robust recommendation would be option 5 compost closely followed by option 4 vacuum 4 3 5 including uncertainty of predictions our predictions for some attributes were highly uncertain for instance for the number of local jobs da job but rather certain for other attributes for instance water saving bb wat fig si 2 this prediction uncertainty can be propagated to the mcda results and can be represented by barcharts with error bars fig si 3 or boxplots fig 8 for clarity we again focus on the medswing weight profile fig 4 and our standard assumptions linear lowest level value functions additive model after 2000 monte carlos simulation runs option 4 vacuum performed best median value 0 70 closely followed by option 5 compost median value 0 64 while 1 status quo remained the worst performing option v 0 31 these findings also become clear with visualizations focusing on the ranks fig 9 fig si 7 across all simulation runs option 4 vacuum and option 5 compost achieved highest ranks often and never the bottom rank considering these results it would be advisable to lower the uncertainty of the predictions for option 4 vacuum and option 5 compost in order to establish with more certainty which of these would be the best performing option 4 3 6 cost benefit visualization the visualization of costs attribute db cost versus all other benefits calculated as aggregated value of the remaining objectives using our standard mcda model fig 10 confirmed previous results option 5 compost and option 4 vacuum performed almost equally well however option 5 compost was markedly more cost efficient than option 4 vacuum in fact option 5 compost with estimated costs of 92 per person and year was the only cost efficient option for the considered preferences which is also why there is no efficient frontier in fig 10 fig si 6 depicts such an efficient frontier for the utilitarian and preservationist perspectives option 1 status quo clearly performed worst lowest value resp benefit and having estimated costs of 125 per person and year is almost as expensive as option 4 vacuum the two most expensive options 2 usep conc and 3 usep store require a high cost investment for a rather small value gain since the predictions for cost had large uncertainties fig si 2 which is not visualized here we recommend the decisions makers to obtain better predictions at least for option 4 vacuum and option 5 compost to increase trust in the robustness of these findings 4 3 7 implications for the case study the results of the mcda using valuedecisions given the model the predictions and the weight preferences elicited online from a sample of 655 people in the paris region clearly indicate two best performing options 4 vacuum urine and feces collection in a separate vacuum network with decentralized treatment and 5 compost dry toilets with on site underground composting chambers these results are robust to changes in preferences model assumptions and to uncertainty in predictions we are confident to recommend these two options for further evaluation to the main wastewater authority in the paris region moreover our analyses allow us to recommend further investigating the predictions of costs and of compliance by end users for these two options this will increase our confidence which of the two options can be expected to perform best 5 evaluation of usability 5 1 evaluation criteria and questionnaire items we believe a value focused approach to software development is helpful one important aspect is usability for the intended users which is embodied in numerous user centered design approaches e g brhel et al 2015 we performed a usability test about the quality in use during the development of valuedecisions that we formulated based on the iso eic 25010 standard iso iec jtc 1 sc 7 2011 this standard suggests five criteria to evaluate usability we focused on three of these effectiveness efficiency and satisfaction which all belong to the quality in use category each criterion is specified by one or several sub criteria we did not evaluate the criterion freedom of risk as we foresee no economic risks valuedecisions is free and open source no health and safety risks e g no addiction risk and no environmental risks no special infrastructure or high electricity consumption is required the criterion context coverage is to our understanding covered in our evaluation of the product quality characteristics see section si 5 and was not evaluated from the user perspective the iso eic 25010 standard provides a structure for the evaluation but no concrete format table 3 and in more detail table si 4 present our operationalization of the standard the evaluation is based on user feedback collected with a questionnaire and on observation based on literature we developed between three to six items for each sub criterion to measure their achievement table si 4 the items are variants of a single question to measure a sub criterion in a reliable and robust manner kline 2000 we also asked users how often they required support e g from the user guide assistance from teaching assistant assistance from other students in two open text questions users could note what they appreciated about valuedecisions and what they recommend to improve to measure the criterion effectiveness we used the grades assigned to the student reports as a proxy indicator instead of questionnaire items 5 2 implementation and test users the purpose of the usability test was to evaluate the app in a systematic way and to reveal how to improve and further develop it for testing usability representative users should perform representative tasks lazar et al 2017 we conducted the usability test with master students with an environmental major who were learning mcda based on mavt and had a basic but sound method knowledge they are target users of the valuedecisions app see section 3 2 potentially they will work as environmental consultants or in government agencies and may use the app in their future career sixteen students carried out four environmental case studies using valuedecisions as part of a course at eth zurich in spring 2020 lienert 2020 we repeated the usability test in the in the spring 2021 course this time on a voluntary basis eight students from three case studies answered the students filled in the usability questionnaire as homework at the end of the course they were fully informed that they were test users and signed an informed consent form the survey was coded in limesurvey limesurvey gmbh 2020 the questions appeared in random order except if they implied a logical development all survey items are available in table si 4 the survey was pre tested by two research assistants and the teaching assistant 5 3 results and discussion of evaluation and response the evaluation of usability varied considerably across sub criteria and among students table 4 table si 5 while our sample is too small to allow for any generalization the usability testing nevertheless helped to better understand use of the valuedecisions app and weaknesses based on this feedback we made considerable efforts to improve the app with additions to the code and further explanations here we highlight few important results and responses based on the test in 2020 a full description is provided in si 6 the efficiency of using the app especially the time efficiency was rated relatively low we suspect that this is largely due to difficulties related to preparing properly formatted and valid input files the teaching assistant mainly supported students in finding formatting mistakes once they had set up correct input files the students were capable of using valuedecisions autonomously for carrying out their case studies we also included questions to assess reasons for loosing trust in the app across all exercises 14 of the 16 students had received error messages many students found that the messages did not indicate the source of the error clearly enough for them to know how to address it resulting in low evaluations of these questionnaire items ten students faced moments when they did not know what to do next and ten lost the running analyses and needed to restart the app hence while the general trust in valuedecisions was positive mean 4 12 students lost trust mean 2 59 due to technical difficulties as a response we implemented additional validation of input files after upload and provided more details what users need to change when this validation fails furthermore we added a checklist to support users in systematically controlling their files information about common error messages and potential solutions and extended the user guide we also decided to better describe the output data and analyses within the app lastly we provided an exemplary decision analysis choosing the holiday destination for my extended family which is understandable to many with input files that can be downloaded as templates the other sub criteria to satisfaction usefulness pleasure and comfort were on average rated positively table 4 most students found valuedecisions pleasant to use and stated that it had a user friendly interface they also felt comfortable using the app and found it easy to learn how to use it clear and understandable and flexible regarding the analysis possibilities this increased our confidence that the approach taken for valuedecisions is useable in practice and that people with basic understanding of mcda can successfully use valuedecisions and will have an effective and satisfying experience a repetition of the user feedback questionnaire in the 2021 mcda course confirmed the evaluation si 6 4 while the time needed to prepare the input file was still detrimental to the overall evaluation the other aspects were well perceived we conclude that users require a solid method understanding to understand why the information in the input files has to be specified in a certain way and to use the app well as for any new software a learning phase seems necessary prior to the efficient use of valuedecisions in addition to the software evaluation with the user survey we involved expert users from the decision analysis group at eawag in developing valuedecisions and several external scientists are already using it in their projects their continuous use and feedback led to various other improvements including bug fixes support for additional visualizations and download of interim simulation results furthermore the product quality elements of the iso eic 25010 standard iso iec jtc 1 sc 7 2011 served as a checklist to guide the software development see si 5 the usability test that we developed for this study based on the iso eic 25010 standard iso iec jtc 1 sc 7 2011 is general and can be adapted to other contexts and software the full questionnaire is provided in table si 4 our approach was pragmatic and a scientific validation of the scales see kline 2000 would need further research nevertheless as starting point our scales showed satisfactory internal reliability table si 5 which indicates that the items reliably and consistently measured the sub criterion they were designed for generally we found the value focused approach to software development and the evaluation of this via usability testing valuable for guiding development efforts 6 discussion and conclusion 6 1 supporting decisions with the valuedecisions app in an application concerning wastewater management in the paris region also see aubert et al 2020 we demonstrated how a public policy decision problem can be analyzed using valuedecisions we collected the preference data from the population before valuedecisions was conceived but could easily convert it to the appropriate format by performing extensive sensitivity analyses we found that key results were insensitive to preference parameters not collected in the online survey of 655 respondents such analysis supports determining for which aspects of the decision more information would be helpful and for which not in our case results are robust enough to conclude that eliciting more detailed preferences from stakeholders would likely not change the main results this is especially relevant for online surveys with lay people as in our case to avoid cognitive overload and tiring of respondents e g riabacke et al 2012 we did not elicit the shapes of single attribute value functions or the best fitting aggregation model earlier studies also concluded that not all preference parameters have to be known in detail schuwirth et al 2012 scholten et al 2015 but this is always case dependent uncertainty analyses allows us to recommend that it could be worthwhile to decrease the uncertainty of the input data for predictions if a clearer differentiation between the two best performing options is desired the focus on visualization given by valuedecisions was key for generating insights and should ease communicating about the robustness of the results to the paris decision makers two unconventional wastewater management options emerged as robust choices namely dry toilets with on site underground composting chambers and urine and feces collection in a separate vacuum network with decentralized treatment section 4 3 this answers the main concern of the paris wastewater authority it seems reasonable to consider a radical system change for wastewater management in certain contexts larsen et al 2016 hoffmann et al 2020 given the preferences of a representative sample of paris region citizens our results thus validate the momentum in the paris region toward unconventional systems pilot projects are implementing some options we considered in this analysis 6 2 alternative software for environmental decision support all tools have a context and use cases in which they are useful consequently valuedecisions was developed with specific use cases and users in mind section 3 2 as far as we are aware the combination of advanced features valuedecisions offers for instance with regard to preference modeling is unique especially combined with a simple spreadsheet interface for the input data a web interface and automatic reporting however depending on the specific decision case other software may be better suited see weistroffer and li 2016 mustajoki and marttunen 2017 mcda software development geared to different types of use and users is an active field of research with several recent additions e g chacon hurtado and scholten 2020 cinelli et al 2021 preference ab 2021 and we expect interesting developments in the future while not being able to do justice to all existing software we briefly point out some developments that address uses valuedecisions is lacking some mcda software focuses more on guiding users through the process of decision making mustajoki and marttunen 2017 emphasize that for non expert users software needs to provide automatic guidance for instance to overcome commonly encountered biases entscheidungsnavi is one online tool that has extensive user support to overcome common biases and guides users through the entire decision making process based on maut von nitzsch et al 2020 process guidance is also offered in software like v i s a decisions simul8 2021 or logical decisions logical decisions 2020 and others truly interactive elicitation of preferences is for instance possible with fitradeoff de almeida et al 2016 supporting decisions with multiple stakeholders is an important property in public decisions explicit support for group decision making is for instance provided by logical decisions logical decisions 2020 or helision preference ab 2021 furthermore spatial assessment can be a relevant feature of environmental problems the combination of mcda and geographic information systems gis is an active research area with various software developments e g greene et al 2011 keenan and jankowski 2019 valuedecisions does not allow for explicit spatial analysis but attributes relevant to spatial features can be used for example number of protected areas affected or space required other software allow for greater flexibility in modeling this is provided by programming libraries and packages for r such as utility reichert et al 2013 or mcda bigaret et al 2017 and for python such as decisi o rama chacon hurtado and scholten 2020 the latter also supports portfolio decision analysis lahtinen et al 2017b with an appropriate workflow software based on these libraries can also be coupled to complex prediction models examples of software with graphical user interface that supports different mcda algorithms are decerns linkov et al 2020 or decspace amador et al 2018 furthermore the diviz initiative aims at providing a common interface to many algorithms meyer and bigaret 2012 6 3 further development of valuedecisions valuedecisions can be extended in many ways thanks to its modular design features can be added without changing the basic structure five examples of further development are 1 support for uncertain preference parameters for instance stakeholders may specify a range of weight parameters rather than point estimates scholten et al 2015 or provide an estimate about how uncertain they were about their statements zheng et al 2016 by allowing uncertain preferences a more realistic picture of the uncertainty of results would be achieved this would extend the current approach which relies on performing sensitivity analyses regarding preferences if uncertainty of predictions and preferences should be considered jointly the concept of expected expected utility could be implemented haag et al 2019b 2 support for multi attribute utility theory section 2 2 4 by including the risk attitude of stakeholders and calculating expected utilities instead of values preferences about uncertain predictions could be directly considered as in scholten et al 2015 additionally the risk attitude could be interactively varied and robust options identified 3 support for smaa methods and metrics for analysis stochastic multi criteria acceptability analysis lahdelma et al 1998 these also can be useful for mcda problems where both predictions of options and preference information is uncertain e g zheng et al 2016 4 interactively support users in data entry and or data elicitation currently users need to provide input data with two spreadsheets this is error prone as issues with incompatible entries may arise and users may be unsure how to provide this information properly integrating the data entry or even preference elicitation step into valuedecisions would make for a more seamless experience such user guidance was also one recommendation of mustajoki and marttunen 2017 5 the automatic reporting could be extended one possibility is using the ideas of natural language generation to explain results to lay users wulf and bertsch 2017 we believe this to be an interesting feature for potential future users such as environmental consultants scientific software development exhibits the long tail phenomenon i e the large majority of software sees only very little uptake and reuse e g walling and vaneeckhaute 2020 software sustainability is a major issue for academic software e g venters et al 2014 and many tools in the long tail are eventually lost valuedecisions also faces this risk two aspects might mitigate this risk firstly valuedecisions is an open source development the source code is available and can be reused or modified freely by anyone secondly it is written in r which is a programming language commonly used for analysis in academia and the app comes in the standardized form of an r package this allows people without specific software development background to contribute to or change the app additionally this makes valuedecisions interoperable with other r packages for mcda such as utility reichert et al 2013 it is possible to exchange the calculations and algorithms in the backend while keeping the visualization and frontend of the application 7 conclusions effective decision support is facilitated by software tools that help analyze visualize and understand the key aspects of a decision problem this insight well known in practice has sparked numerous software developments since the 1980s reviewed e g by korhonen et al 1992 french and xu 2005 weistroffer and li 2016 mustajoki and marttunen 2017 here we introduced a novel open source development the valuedecisions app valuedecisions was developed to support analysts facilitators and interested stakeholders during the modeling stage of an mcda process it is targeted towards environmental and public policy problems key properties of the app are i the possibility to represent complex stakeholder preference structures over multiple objectives by building hierarchical mavt models that combine non linear lowest level value functions with non additive aggregation functions ii the possibility to consider uncertainties by different probability distributions for predictions interactive sensitivity analyses and visualizing uncertainty in different ways iii the comparison of results for multiple potentially conflicting stakeholder preference profiles iv a graphical user interface accessible via a web browser that focuses on producing insightful visualizations v open source development based on r which allows for modifications and extensions of the algorithms and visualizations we tested valuedecisions with data from an online survey of 655 citizens for an urban water management decision in the paris region valuedecisions allowed us to clearly identify robust options for wastewater management by using different sensitivity analyses to identify user needs we developed a structured usability test based on the iso eic 25010 standard iso iec jtc 1 sc 7 2011 students participating in a mcda lecture lienert 2020 used valuedecisions to tackle environmental decision problems their responses in the usability test pointed us toward important improvements of the app the usability survey we developed can now be used tested and improved in other applications given the large role software plays in applied projects we know surprisingly little about how it is used and what that use implies every software tool has certain affordances and enables users to do certain things but not others the field of behavioral operations research bor e g franco and hämäläinen 2016 hämäläinen 2015 has picked up such research we can only reiterate the call of mustajoki and marttunen 2017 to research the interaction of people with mcda software for example how people use software what kind of support they need and how the characteristics of the software affect people s learning and interpretation of results one important step in this direction is user centered evaluation cf walling and vaneeckhaute 2020 as we have operationalized here our basic motivation for developing valuedecisions was making mcda analysis for environmental decision problems accessible to a wider audience given the usability evaluations we are hopeful that valuedecisions can contribute to this we chose to make the software available free of charge and accessible as a web application without any installation we hope that the possibilities offered by the app will facilitate the application of decision analysis based on multi attribute value utility theory and eventually contribute to better structured transparent and well informed decisions software availability name of software valuedecisions developer and contact information concept design fridolin haag judit lienert programming fridolin haag kevin schönholzer sara schmid eth scientific it services contact judit lienert eawag ch year first available 2020 hardware required no specific requirements software required web browser for the r package version r base installation availability https www eawag ch en department ess main focus decision analysis da tools program language r cost free open source license agpl data availability data and software used in this paper are available on the eawag research data institutional repository https opendata eawag ch https doi org 10 25678 00048s credit author statement fridolin haag conceptualization methodology software writing original draft review editing visualization supervision project administration alice aubert conceptualization methodology investigation writing original draft review editing supervision usability test case study judit lienert conceptualization software testing and improvement writing original draft review editing funding acquisition supervision project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we are grateful to kevin schönholzer sara schmid and the its scientific software data management group at eth zürich for their contributions to programming valuedecisions we thank our colleagues qi zeng and sara schmid for supporting data analysis harald von waldow and markus kasper for valuable advice on software deployment and daniel hofmann and philipp beutler for feedback we thank the respondents of the paris case study for answering our survey and fabien esculier for collaboration and interacting with the paris region wastewater authorities we are indebted to the students of the mcda lecture in 2020 and 2021 for using valuedecisions and participating in the survey alice aubert was supported by an eawag postdoctoral fellowship 2015 grant number 5221 00492 009 08 df 15 and a swiss national science foundation ambizione grant project 173973 edanaga eawag also funded this applied research as project a 5 mcda tool under the 2019 2020 agreement between the scientific it services of eth zürich and eawag we wish to sincerely thank the four reviewers and the editor for their constructive comments which helped us to improve this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105361 
25630,complex environmental and public policy decisions profit from structured procedures such as multi criteria decision analysis mcda to support such decisions the new open source application valuedecisions provides advanced analysis and visualization with no programming expected from users based on multi attribute value theory mavt it offers analysis for decisions with conflicting and interacting objectives multiple stakeholders and uncertain consequences of options programmed in r the shiny web framework makes it accessible via a graphical user interface in the browser we exemplify using valuedecisions for a wastewater infrastructure planning case in the paris region we surveyed preferences of 655 citizens and conducted sensitivity analysis of preference parameters the best management options were robust across a range of preference profiles and assumptions to evaluate the app we developed a novel usability test based on the iso standard for software quality and surveyed students using valuedecisions for case studies graphical abstract image 1 keywords multi criteria decision analysis mcda software multi attribute value theory environmental decision analysis open source population survey 1 introduction environmental decisions and other public policy problems have characteristics that make them messy complex and difficult to tackle in a rational way they need to be addressed in a social context often concerning many stakeholders or decision makers with diverse views french and geldermann 2005 these may include citizens future generations and other interest groups with opposed values resulting in conflicts of interest because of the plurality of stakeholder views and the extent of decision consequences in space and time multiple objectives need to be considered in these decisions gregory et al 2012 haag et al 2019c since it is usually not possible to fully achieve all objectives difficult trade offs are required furthermore the outcomes of decision options and their environmental impact is uncertain reichert et al 2015 in response to these challenges structured transparent procedures such as multi criteria decision analysis mcda have been developed while the umbrella term mcda encompasses various approaches see belton and stewart 2002 greco et al 2016 cinelli et al 2020 the common element is the development of a quantitative or qualitative model of the decision situation these models support structuring information making trade offs and choosing decision options to implement mcda approaches are increasingly considered and applied for environmental problems hajkowicz and collins 2007 huang et al 2011 cegan et al 2017 esmail and geneletti 2018 and used in government agencies to address public issues kurth et al 2017 our work is based on multi attribute value and utility theory mavt maut keeney and raiffa 1993 in environmental management decision support based on this conceptual background has also been popularized under the term structured decision making gregory et al 2012 decision making processes can be facilitated by software tools that allow interactive model building exploration and visualization of results accordingly mcda approaches have often been developed in tandem with respective decision support software see korhonen et al 1992 for an early review especially group decision support has a tradition of technology driven approaches morton et al 2003 consequently a wide variety of mcda software has been developed online collections list more than 45 different products ewg mcda 2020 mcdm society 2020 systematic reviews and comparisons of software products are provided by french and xu 2005 and weistroffer and li 2016 for general applications and in mustajoki and marttunen 2017 specifically with regard to environmental planning processes when deciding which software to use one faces trade offs for instance regarding feature richness adaptability user friendliness speed or costs these objectives are often conflicting and software developments found different answers to these conflicts mustajoki and marttunen 2017 the fit of a software tool to the specific context in which it is used is crucial belton and hodgkin 1999 french and xu 2005 based on applied research in participatory environmental and public policy decision support e g borsuk et al 2008 lienert et al 2011 zheng et al 2016 haag et al 2019b harris lovett et al 2019 we identified five common requirements for decision analysis software in theses contexts i flexibility in representing stakeholder preferences across multiple potentially conflicting objectives ii ability to consider oftentimes large uncertainty of predicted consequences and stakeholder preferences as well as support to understand robustness of conclusions iii capability to compare results for multiple stakeholders iv provision of analysis via a graphical user interface to users such as consultants who do not necessarily have programming skills and v extendability and adaptability to new requirements in addition to these context specific requirements there are fundamental requirements of software quality see si 5 this study presents tests and evaluates a novel open source application valuedecisions which we developed to target these requirements as described above there is a rich history of software that has been successfully used to support multi criteria decisions see korhonen et al 1992 french and xu 2005 weistroffer and li 2016 mustajoki and marttunen 2017 however we found the flexibility to represent preferences and uncertain predictions often limited and a unified approach to all five requirements at once missing the main properties of valuedecisions to tackle the requirements are i support for complex hierarchical mavt models by combining non linear lowest level value functions with non additive aggregation functions that can represent most preference structures ii consideration of a range of probability distributions for predictions interactive sensitivity analyses and different visualizations of uncertainty iii simultaneous comparison of multiple stakeholder preference profiles without forcing aggregation of individual preferences iv a graphical user interface that can be accessed via a web browser with many options to save analysis results and v open source development based on r r core team 2021 which allows for extensions by interested users this combination of features is to the best of our knowledge unique valuedecisions attempts to strike a balance between simplicity and analytical possibilities that are relevant in the environmental field while being extensible free of charge and easily accessible for participatory decision support other aspects than the sheer feature richness of software are relevant one aspect we want to highlight is the issue of access our intention is to make multi criteria analysis accessible to interested users or stakeholders such as municipalities public authorities consultants and students even if users do not conduct the main analysis themselves it would be helpful if they could engage with decision models created by researchers for instance to increase trust and acceptance in the analysis hämäläinen 2015 voinov et al 2016 access can be limited by cost of commercial software incompatibilities of hardware software or operating systems and user rights in organizations we require new ways to provide analysis and models to stakeholders web apps as the app we introduce here are one comparatively new possibility to mitigate these access impediments the open source development means that the implementation is transparent and the app can be used free of charge access can also be limited by the required technical expertise to use software as a community we benefit from a variety of tools that fulfill needs of different target users see section 3 2 complex models and analysis are often only possible with frameworks that require programming such as utility for r reichert et al 2013 or decisi o rama for python chacon hurtado and scholten 2020 this can make analytic capabilities that are useful in environmental decision support for example regarding uncertainty inaccessible to interested users with valuedecisions we wanted to build a bridge by making advanced analysis available via a graphical user interface without the need to program this benefits users without programming skills but also facilitates interactive model exploration in workshop settings we propose that software development similar to decision making can profit from value focused thinking keeney 1992 while all software developers have an interest to praise their own tool the objectives of what should be achieved with the software should be kept at the forefront despite its importance empirical evaluation of decision support systems remains rare walling and vaneeckhaute 2020 therefore this study includes a structured evaluation of valuedecisions we used the software product quality requirements and evaluation square series of standards iso iec 25000 as reference in our development and basis to evaluate valuedecisions against defined objectives iso iec jtc 1 sc 7 2011 this iso standard defines two evaluation models to gain insights into the usability of the app we operationalized the quality in use model by a usability test in which representative users performed representative tasks additionally we self evaluated the application based on the product quality model of this framework si 5 as evaluation from an applied perspective we analyzed an unpublished real world environmental decision problem with valuedecisions the remainder of this paper is organized as follows in section 2 we introduce mcda models specifically multi attribute value theory that forms the theoretical basis for valuedecisions section 3 introduces the features and implementation of valuedecisions in section 4 we exemplify its use in a case study for wastewater infrastructure planning in the paris region based on a population survey with 655 respondents section 5 describes the development and results from the usability evaluation we end with a general discussion and conclusions in section 6 2 decision support based on multi attribute value theory mavt 2 1 the decision analysis process a decision making process can be structured as a sequence of phases which are intertwined and iterative in practice we can differentiate between phases of problem structuring data and information collection decision analysis negotiation and conflict resolution implementation and monitoring within this process the aim of multi criteria decision analysis is to develop and evaluate a set or sets of options also called alternatives strategies actions variants or scenarios depending on the literature on a set of objectives also called criteria and provide insights e g about optimal decisions involved trade offs and stakeholder perspectives based on a decision model the valuedecisions app supports the phase of analyzing the decision options from different stakeholder perspectives it builds upon an adequate structuring of the decision problem predictions of uncertain outcomes of options and the completed elicitation of stakeholder preferences the first steps of an analysis are fundamentally important not least due to path dependencies lahtinen et al 2017a in the problem structuring phase the boundaries of the decision problem are set and stakeholders identified e g lienert et al 2015 marttunen et al 2017 then often in a participatory process basic decision elements are defined the objectives what stakeholders find fundamentally important to achieve the decision options with which alternatives strategies actions the objectives can be achieved and attributes how to quantify or measure the performance of options e g gregory et al 2012 next predictions of how each option performs as measured by the attributes are needed predicting potential consequences of options is a major undertaking of natural and engineering science and a large component of some decision analysis projects predictions can be derived from quantitative models scientific literature or expert judgment as predictions are inherently uncertain in environmental problems we propose to express predictions probabilistically reichert et al 2015 the stakeholders preferences are elicited over these potential outcomes of options stakeholder preferences are usually elicited from individuals in interviews or group workshops eisenführ et al 2010 or increasingly online lienert et al 2016 aubert et al 2020 the defined structure and input data then allow us to build an evaluation model of the decision options see section 2 2 predictions and preferences are integrated and results calculated as a result each option receives a value or utility score allowing us to compare the performance of the options for each stakeholder perspective since any model based analysis depends upon inputs parameters and model structure further analysis of the model and conclusions for instance with sensitivity analyses is crucial based on the analysis we can recommend one or several options to implement negotiation or consensus finding among stakeholders as well as implementation and monitoring of a decision demand additional activities see gregory et al 2012 french and argyris 2018 2 2 decision models based on multi attribute value theory the theoretical underpinning of valuedecisions is multi attribute value theory mavt keeney and raiffa 1993 which is based on few rationality axioms it has several advantages as a concept for environmental decision support discussed in reichert et al 2015 mavt is based on the concept of value functions a value function maps from consequences as measured by attributes to the degree of relative achievement of objectives usually in the interval 0 1 the larger the value the better this option meets the objectives given the stakeholders preferences the options can then be ranked according to their values in case of uncertain outcomes a utility function provides this ranking of options in a rational decision the option with the highest value should be chosen when a decision problem covers multiple objectives a multi attribute value function is built which returns a single value based on the consequences of each option on several attributes a stepwise hierarchical procedure helps constructing this multi attribute value function 1 determine an objectives hierarchy to structure the model 2 identify shape and parameters of the lowest level value functions i e marginal value functions 3 find the type and parameters for aggregating values upwards in the hierarchy to calculate the overall value and optionally 4 convert values into utilities below we briefly explain each concept see reichert et al 2013 2015 or haag et al 2019a for an in depth treatment we propose to create a separate decision model for each stakeholder and compare the outcomes though ideas to aggregate over stakeholders also exist see section 3 4 5 2 2 1 objectives hierarchy objectives can be structured in form of a hierarchy keeney 1992 this also determines the structure of the evaluation model in many cases it is advisable to include no more than 10 15 objectives marttunen et al 2018 and reduce a higher number of objectives marttunen et al 2019 an example objectives hierarchy is given in the case study fig 3 2 2 2 lowest level value functions each lowest level objective is evaluated with respect to its attributes with help of a value function this leads to a non monetary common unit that allows comparing different types of attributes e g costs in with high phosphorus recovery in for simplicity we only consider lowest level value functions v i for single attributes x i not over several attributes such a value function v i x i is defined over the range which the attribute can take in the decision case covering all options e g costs from 90 to 150 a value of 0 corresponds to the worst possible level given the range of the attribute e g costs of 150 and a value of 1 to the best possible level e g costs of 90 the value function thus measures the relative degree of achievement of an objective the shape of the lowest level value functions depends on stakeholder preferences sometimes attribute levels may be translated linearly to values fig 1 lower panels but often stakeholders have non linear preferences e g scholten et al 2015 zheng et al 2016 langhans and lienert 2016 several methods for eliciting the shapes of the value functions exist eisenführ et al 2010 commonly we will obtain pairs of outcomes and corresponding values and can then interpolate between them or fit a parameterized function 2 2 3 aggregation model to calculate the total or overall value v x a of an option a across all consequences x a x 1 a x n a the performance on all objectives is aggregated this means the values of lower level objectives v i x i a are aggregated to a value at the next higher level of the hierarchy and so forth until a single total value v x a for each option a is reached see reichert et al 2013 2015 haag et al 2019a the most commonly used aggregation model for mavt is the additive model belton and stewart 2002 here the weighted arithmetic mean is used to calculate the aggregated value v x eq 1 v x 1 x n w i 1 n w i v i x i where v x 1 x n is the total value over the consequences x 1 x n v i x i is the value for the consequence measured by attribute i and w i is a weight parameter for attribute i with i 1 m w i 1 thus in addition to the lowest level value functions this model requires to specify weight parameters also called scaling factors weights can strongly affect the results see section 4 3 2 and weight elicitation is especially prone to biases morton and fasolo 2009 riabacke et al 2012 therefore careful weight elicitation with tested methods such as swing or trade off is advisable eisenführ et al 2010 elicited weights may still remain ambiguous for example because stakeholders may be inherently uncertain because preferences change in different future scenarios or because mistakes during elicitation occurred a possible procedure to investigate the effects of this uncertainty is local sensitivity analysis on each weight eisenführ et al 2010 section 4 3 the additive model is not always an adequate representation of stakeholder preferences an implication of the additive model is the possibility to compensate if one objective performs poorly e g high phosphorus recovery is not achieved it can be compensated by another well performing objective e g high water savings according to the given weights practical experience in many cases indicates that stakeholder preferences actually do not agree with some assumptions and implications of the additive model e g rowley et al 2012 langhans and lienert 2016 zheng et al 2016 haag et al 2019a reichert et al 2019 other non additive aggregation models have less strict assumptions and allow for interaction between objectives grabisch et al 2009 however non additive aggregation is so far rare in mcda software for mavt maut and not even mentioned in the reviews by mustajoki and marttunen 2017 or weistroffer and li 2016 exceptions are logical decisions logical decisions 2020 which supports multiplicative aggregation and the tool by cinelli et al 2021 which supports several mean functions an interesting alternative to the weighted arithmetic mean for aggregation in mavt is the weighted power mean also called weighted generalized mean haag et al 2019a 2019b using this aggregation function we are able to model various interactions between objectives such as partial non compensation to our knowledge valuedecisions is the first mcda software with graphical user interface that implements this aggregation the power mean is given by eq 2 v x 1 x n w γ i 1 n w i v i x i γ 1 γ f o r γ 0 i 1 n v i x i w i f o r γ 0 the weights and value function parameters are as in eq 1 we informally refer to the additional parameter γ as the non additivity parameter because for γ 1 the model reduces to the additive model and by varying γ we can express different non additive behaviors the weighted power mean covers a number of other aggregation functions of interest as special cases for certain values of γ see si 1 for an in depth discussion of their properties we refer to langhans et al 2014 and grabisch et al 2009 2 2 4 uncertainty and risk utility functions when outcomes of a decision are uncertain a stakeholder s risk attitude determines how to evaluate this uncertainty this can be captured with multi attribute utility theory maut keeney and raiffa 1993 directly eliciting multi attribute utility functions from stakeholders is demanding eisenführ et al 2010 and to our knowledge rarely done in complex practice applications however it is possible to transform the value function at the highest level of the objectives hierarchy to a utility function dyer and sarin 1982 reichert et al 2015 instead of calculating the value of options the expected utility is calculated this integrates over the uncertain predictions taking into account the risk attitude and leads to a unique ranking based on the expected utility in practice contexts we found it can be advantageous to work with values and explicit visualization of their uncertainties instead of discussing expected utilities 3 the valuedecisions app 3 1 overview valuedecisions supports multi criteria decision analysis modeled with multi attribute value theory it provides a middle ground between sophistication and user friendliness is flexible to extend and adapt and is open source programmed in r r core team 2021 it is accessible to users online as a web application that does not require software installation additionally it is available as a standard r package which can be run locally the users section 3 2 need to be familiar with the principles of mcda to sensibly use valuedecisions but programming capabilities beyond uploading two spreadsheets in the required format is not required valuedecisions is available at https www eawag ch en department ess main focus decision analysis da tools valuedecisions focuses on the mcda modeling stage i e the integration of predictions and preferences this includes calculating the mcda result i e identifying the best performing options and sensitivity analyses to further explore model inputs assumptions and results valuedecisions allows setting up and running a mcda model in a graphical user interface for complex decision problems including uncertainty of predictions many options can be evaluated in parallel for multiple stakeholders with possibly conflicting preferences results are presented in tables and visualized by various graphs a focus is on diverse visualizations of results that can be explored interactively for instance in a workshop users can easily change preference parameters to analyze the sensitivity of the results in steps including shapes of lowest level value functions non additive aggregation models and local sensitivity analyses of weights they can download intermediate and final results as individual graphs and tables or compiled in a report 3 2 target users and use cases we can differentiate between three potential user groups for mcda software mustajoki and marttunen 2017 1 do it yourself users belton and hodgkin 1999 who would like to apply mcda but have no specific training or experience 2 analysts and facilitators who need software support for facilitation analysis and visualization and 3 analysts and consultants who want to carry out sophisticated analyses valuedecisions is mainly targeted for the latter two user groups it can support consultants working in environmental decision cases public planning authorities municipalities other practitioners or students in the analysis phase of a mcda process these users may lack finances time or expertise to work with sophisticated software or programming r python matlab etc but wish to go beyond the limited capabilities of spreadsheet software additionally valuedecisions is powerful enough that academic researchers can explore applied mcda problems the app can be used in workshops with interactive exploration of options or in backroom analyses since valuedecisions requires a carefully conducted problem structuring and data collection phase to have taken place it is not meant for do it yourself users that said if stakeholders are provided the input files they can further explore the decision model for example to facilitate transparency and traceability voinov et al 2016 3 3 elements of mcda model and their implementation 3 3 1 input data on predictions and preferences users need to upload a file with information on the predicted performance of each decision option for each attribute predictions and a file with stakeholder preferences about these predictions preferences these are uploaded in two spreadsheets excel or tab separated values in the files the users enter all information for the analysis templates for the files can be downloaded once uploaded valuedecisions first conducts several checks for file validation the predictions file example in table si 1 specifies for each option the predicted performance of each attribute since the consequences of options for environmental decisions are usually uncertain probability distributions for the predicted outcomes can be specified for each option and attribute see section 3 3 2 the second file example in table si 2 specifies the structure of the objectives hierarchy of stakeholders used for evaluation as well as information on stakeholders preference parameters only the objectives hierarchy is essential to run valuedecisions including the names of upper level l2 and lower level l1 objectives attribute names and units and the best and worst possible outcome of attributes each stakeholder can have a different objectives hierarchy however the interpretation of results can become more complex if stakeholders do not share objectives in addition to the objectives hierarchy preference parameters for several different stakeholders can be defined including the shape of lowest level value functions global weights assigned to attributes and type of aggregation to be used in the model if no preference data are specified or a group of parameters is missing defaults are assumed these are that lowest level value functions are linear attributes have equal weights and the aggregation function is the weighted arithmetic mean additive model this can be useful for interactive use of valuedecisions for instance in a workshop the options can be explored together with the stakeholders and the preference parameters can be varied directly in the app interface 3 3 2 uncertainty of predictions in many environmental and public policy decisions there is large uncertainty about the consequences of management options in valuedecisions users can either calculate the results based on point predictions for each attribute and option or provide parameters of probability distributions for these predictions implemented probability distributions are uniform normal lognormal triangular beta generalized extreme value distributions and discrete distributions valuedecisions will generate samples from the specified distributions and propagate the uncertainty to the results with monte carlo simulation this leads to a distribution of the total value v x a of each option and to a distribution of rankings currently only independent probability distributions can be used i e the predictions of one attribute cannot depend on the predictions of another with other software e g utility reichert et al 2013 it is possible to work with predicted samples of non independent distributions however this requires that users create these samples beforehand with some creativity it is possible in valuedecisions to combine mcda with scenario analysis to account for large future uncertainty e g stewart et al 2013 scholten et al 2015 zheng et al 2016 the performance of options given a scenario can be entered as separate options e g option a would occur twice in the prediction file as a scenario1 and as a scenario2 and the results can be analyzed across scenarios alternatively different predictions and or preferences for each of the future scenarios can be uploaded and the results compiled outside of valuedecisions 3 3 3 lowest level value functions for each attribute on the lowest level of the hierarchy outcome value pairs that were elicited from stakeholders can be provided using least squares valuedecisions then fits a single attribute value function to these outcome value pairs the fit can be evaluated visually for lowest level value functions only single attribute value functions not multi attribute value functions are possible a wide variety of shapes can be specified such as linear exponential or sigmoid functions see si 2 alternatively linear interpolation between provided values or discrete value functions can be used thus users can represent many types of non linear preferences with fewer restrictions than in many other software products if no preference information is provided for an attribute linear value functions are assumed as default the effect of this assumption can be explored with sensitivity analysis see section 4 2 2 3 3 4 aggregation for multi attribute value functions in valuedecisions the objectives hierarchies used for evaluation can have two levels as default valuedecisions uses the additive model to calculate overall values the weight parameters for objectives need to be elicited from the stakeholders if no preference information for weights is provided equal weights are assumed for all attributes because valuedecisions also implements the weighted power mean as aggregation function eq 2 many alternative ways for aggregation besides the additive model can be used see si 1 this flexible approach to aggregation together with non linear lowest level value functions allows representing complex preference structures for each branch of the objectives hierarchy the γ parameter which specifies the degree of non additivity can be entered in the preferences input file in addition to the weights alternatively within valuedecisions a slider allows adapting the value of the parameter γ across the entire hierarchy allowing to explore the effects of using different weighted power mean functions 3 4 features and flow 3 4 1 structure and controls to users valuedecisions is essentially a website the basic flow of the app is organized as consecutive pages users can switch between pages via a navigation menu on top fig 2 on the start and information pages information and checklists are provided on the upload page users upload the two input files to the analysis predictions and preferences on the analysis page an interactive mcda analysis is run and on the reporting page a report containing graphics and tables of current results can be downloaded for instance as word file the resulting plots and tables of all steps of the analysis can also be saved individually the heart of valuedecisions is the analysis page fig 2 on its left side a control panel allows users to interact with the analysis on the right side users can switch between different types of analyses and displays that are organized in tabs on the control panel a run update button re runs the analysis after adjusting settings it is possible to switch between analyses with or without uncertainties in the predictions and adjust the number of monte carlo samples three filters can be applied to select 1 stakeholders for which results should be calculated 2 decision options for which results should be calculated 3 objectives for which results should be shown this only applies to certain plots the following parameters of the preference models can be changed this can be combined in arbitrary ways 1 the shape of the lowest level value functions can be changed collectively if this is active all lowest level value functions for all stakeholders are converted to exponential value functions with the curvature parameter c specified by a slider see eq si 3 for formula 2 the weights assigned to objectives can be adjusted individually users can select individual objectives and give these a new weight using a slider the newly assigned weight is applied for every stakeholder the other weights which were not modified are renormalized so that the previous relationships remain unchanged and the sum of weights equals one consequently the newly assigned weight will be the same for all stakeholders but the other weights remain stakeholder specific 3 the aggregation function can be changed collectively if this is active the aggregation model specified in the preferences input file or without specification the default additive aggregation is switched to the weighted power mean section 2 2 3 the value of the non additivity parameter γ can be adjusted with a slider see si 1 for implications the same value is assumed for all stakeholders and across all branches in addition to the control panel there are controls in the tabs that show the analysis results here users can switch between different presentations of the results for instance different types of graphs or switching the x axis variable 3 4 2 visualization of input data and intermediary steps exploratory visualization of the input data before calculating the full mcda model allows checking for mistakes in data entering and already gives insights into differences between stakeholders and options the structure of the evaluation is given by the hierarchy of objectives example in fig 3 the first tab on the analysis page shows the predicted performance of each option when uncertainties are considered the empirical distributions of the samples are shown as obtained by monte carlo sampling example in fig si 2 on the next tab the lowest level value functions that were fitted to the elicited preference data or modified in the app are displayed example in fig 1 a third tab shows the weights the stakeholders provided or that were specified in the app example in fig 4 3 4 3 mcda results the next three tabs of the analysis page provide results of the decision model and additional analysis there are several options for different types of graphics in the values tab the overall and partial values of each option are displayed example in fig 5 visualization options differ if uncertainty of the predictions is included in the ranks tab the overall rankings of the options are shown example in fig 2 or the rank distributions in case of uncertainty example in fig 9 and fig si 7 in the x vs y tab the resulting values for selected groups of objectives can be related to each other the most common example is plotting costs against benefits fig 10 the efficient frontier of cost efficient options can also be shown 3 4 4 sensitivity analyses valuedecisions allows extensive step wise sensitivity analyses and robustness analyses examples in schuwirth et al 2012 zheng et al 2016 haag et al 2019b one way to conduct this is via the controls for preference parameters left side panel fig 2 they can be varied interactively and in arbitrary combination by users to study the impacts on the results it can be tested whether results are sensitive to a the assumption of linear lowest level value functions or other functional shapes b changes of individual or combinations of weights and c the choice of the aggregation model for didactic reasons the effects of choosing a non additive aggregation model can also be visualized for two attributes as in reichert et al 2013 because results are often sensitive to weight parameters an additional local sensitivity analysis for the weight of each objective is possible example in fig 7 it shows the resulting overall values of the options when an objective s weight is varied from 0 this objective is not at all relevant in this context to 1 only this objective is relevant none of the other objectives matter a more advanced approach the weight stability interval wsi allows calculating intervals in which partial orders of options are stable mareschal 1988 in valuedecisions such intervals can only be visually read off the figures fig 7 3 4 5 comparing stakeholder groups preferences within and across groups can be considered in different ways in decision modeling see e g eisenführ et al 2010 and resulted in corresponding software implementations mustajoki and marttunen 2017 one option is to determine aggregated preference profiles the simplest approach is to average preference parameters across stakeholders for example aggregating individual weights using a weighted mean or median also see section 4 2 1 this can be sensible if we are interested in generalized preferences of large groups such as a population sample another possibility is to aggregate the utility or value of decision options across stakeholders keeney 2013 in environmental decisions stakeholder groups commonly have conflicting or opposing views gregory et al 2012 this calls for an approach that allows for comparisons by treating the preferences of individual stakeholders or groups separately we need to see in how far conflicting preferences affect the results this can then point towards leverage points for compromise options or be an input to further stakeholder deliberations learning about the stakeholders opinion can often more important than calculating a final ranking of options e g mustajoki and marttunen 2017 valuedecisions follows this latter approach an overall consensus is not enforced by the modeling approach but instead several stakeholders can be analyzed individually and compared interactive variation of weights and other preference parameters see section 3 4 4 can facilitate finding paths to consensus options the explicit visualization of conflicting results can be a way forward to further negotiation and deliberation this would be lost by averaging over the conflicting views 3 5 implementation as a reactive web app based on r valuedecisions is written in the programming language and software environment r r core team 2021 the mcda modeling and calculations partly build on our earlier work e g haag et al 2019b as well as the r package utility reichert et al 2013 being open source and using r means that the app is relatively easy to change and extend by users familiar with r in comparison to proprietary software using bespoke frameworks and data structures the interactive graphical user interface in the form of a web application is achieved with the shiny framework chang et al 2021 in addition we use several extension packages to shiny for visualizing tables different types of widgets and others most figures rely on ggplot2 wickham 2016 the reporting as downloadable word file was achieved with rmarkdown xie et al 2018 the creation of an r package followed the golem framework fay et al 2021 more generally the combination of specifying analysis in a programming language common in science e g r python or julia in the backend of an application and then building a web app to make the analysis available to users or stakeholders by a graphical user interface is a promising development model it offers many possibilities for scientific work but also for interactive communication of scientific results to stakeholders shiny applications are based on reactive programming which is different to an imperative if this do that programming style in a shiny application we define the endpoints outputs and their relation to sources inputs via conductors intermediate steps in other words we define the relationship of elements for instance from input files via calculations to a graph with results however we do not specify the exact time or order that calculations should occur in instead calculations are run when they need to valuedecision will recalculate and update only those outputs that are affected by user changes this is useful for interactive analysis as it lowers the required amount of recalculation because reactive programming focuses on the relation of elements it is relatively easy to add or remove components without affecting the whole valuedecisions can be accessed as a web app or downloaded as a standard r package to be executed locally building upon r makes the app platform independent it can be run on windows linux or macos systems deployment as a web app has advantages for users since they can use valuedecisions if they can run a modern web browser on their device independent of operating systems hardware and other installed software because valuedecisions is essentially a website its design is responsive and it can also be viewed on devices such as tablets or smartphones 4 application mcda for wastewater infrastructure planning in the paris region 4 1 context and decision structure the rivers of the greater paris region face increasing pressure partly caused by the wastewater management system esculier et al 2015 two factors contribute typical for other large cities worldwide lossouarn et al 2016 firstly continuous population growth leads to increases in wastewater production thus the pollution load entering wastewater treatment plants increases secondly regional river discharge is predicted to decrease due to climate change flipo et al 2021 this will lead to reduced dilution of the treated wastewater in receiving rivers currently wastewater management in the paris region complies with the french transposition of the water framework directive on nutrient concentrations siaap 2021 however given the pressures of population growth and climate change the main paris wastewater management authority has to invest significantly to avoid serious future damages of water ecosystems today s point in time is also suitable to make changes because of new infrastructure projects in the paris region le grand paris project dila 2021 innovative technical options have the potential to address the pressures at hopefully reasonable costs but entail radical system changes e g larsen et al 2016 hoffmann et al 2020 they imply moving from the centralized system where wastewater is transported in sewer networks to large treatment plants to semi or fully decentralized systems where wastewater is treated locally for instance in individual buildings waste streams like urine feces and greywater may be separated at the source which has technical advantages and allows resource recycling larsen et al 2009 decisions concerning future wastewater management inevitably affect citizens before deciding the main paris wastewater authority needs to know how the public would perceive unconventional decentralized options we conducted a study to evaluate options for urine and feces management for the paris region a workshop with 18 local stakeholders was held in june 2016 to select objectives for evaluating wastewater management options in new districts and to elaborate innovative options details in si 4 1 this process led to agreeing upon nine fundamental objectives and five wastewater management options the five selected options are status quo or business as usual 1 status quo urine source separation with on site concentration 2 usep conc urine source separation with on site storage 3 usep store urine and feces collection in a separate vacuum network with decentralized treatment 4 vacuum and dry toilets with on site underground composting chambers 5 compost the nine objectives were organized in a two level objectives hierarchy fig 3 the decision case was also the focus in an experimental paper addressing online preference elicitation for mcda aubert et al 2020 here we present novel results based on a representative population survey in the paris region we deployed two online population surveys to collect citizens preferences concerning the importance of objectives and trade offs they are willing to make which can be expressed by weights the surveys using swing weight elicitation and direct rating eisenführ et al 2010 are presented in aubert et al 2020 the obtained samples were representative of the regional statistics in terms of gender age and occupation the data from the population survey and the data on the predictions i e how the options perform on each attribute were reformatted as required by valuedecisions see examples predictions table si 1 preferences table si 2 these input files are available in the data package see data availability we aimed to generate advice for the main paris wastewater authority who needs to know how the public would perceive unconventional decentralized wastewater management options the main question in this study is are the results of the mcda robust concerning the best performing wastewater options despite possibly differing preferences of the sampled population and various uncertainties which wastewater options can we recommend to the paris authorities using valuedecisions we analyzed the decision problem including a comprehensive sensitivity analysis we investigated how robust the outcomes are to the weights the non elicited preference parameters the uncertainty of the predictions and visually explored costs of options versus their benefits 4 2 analyses 4 2 1 weight parameters and their influence first we investigated whether the results of the mcda differed between preferences of survey respondent groups of the paris region population one survey group did a direct rating of objectives dirrat n 357 and the other went through a swing weight elicitation within the latter group we discovered that 262 respondents did not understand the procedural instructions discussed in aubert et al 2020 we treated this group separately swinginvalid n 262 from the group that understood the instructions swing n 36 to obtain generalized weights we aggregated the individuals weight for each objective by taking the median weight for each of the three groups to obtain a median weight profile for each group we normalized the nine median weights to sum to one to check the sensitivity of the mcda results to the weight profiles we ran the analysis in valuedecisions handling the three groups as three stakeholders secondly as a sensitivity analysis we created two fictional extreme weight profiles as the weight profiles from the survey were similar between groups fig 4 these profiles are based on the idea of environmental attitudes milfont and duckitt 2006 stating that one can have a preservationist or utilitarian environmental attitude in the preservationist profile we equally distributed the weights between the objectives of protecting the environment and sustainable resource use ignoring the actual ranges of the attributes in the utilitarian profile we equally distributed the weights between social well being and economic performance we ran the sensitivity analysis in valuedecisions handling these two extreme weight profiles as two stakeholders thirdly we used the median weights of the swing group medswing to carry out a local one factor at a time sensitivity analyses e g eisenführ et al 2010 for two critical objectives first we doubled the weight of the low cost objective which is often decisive for the authorities and can be systematically underestimated in mcda marttunen et al 2018 additionally using the local weight sensitivity analysis in valuedecisions we investigated the results for all possible weights for the objective high chance of compliance by end users which might be the most critical to achieve a paradigm shift in wastewater management 4 2 2 non elicited preference parameters lowest level value functions aggregation model for time reasons it was not possible to elicit lowest level value functions and test whether the implications of using the additive model comply with the respondents preferences in the survey to test the robustness of the mcda results we carried out sensitivity analysis changing the two standard assumptions of using linear lowest level value functions and the additive model we used the weight profile of the swing group n 36 with median weights medswing for this analysis for the lowest level value functions we tested concave and convex shapes using an exponential function and setting the curvature of all value functions to c 5 or 5 respectively see eq si 3 we also repeated the analysis using the more general power mean aggregation to check the sensitivity of the results for following aggregation functions additive as comparison the aggregation parameter γ being 1 approaching the weighted geometric mean γ 0 04 and exploring the space between additive and geometric mean including γ 0 5 furthermore we included the two extremes maximum aggregation γ approaching and minimum aggregation γ approaching 4 2 3 uncertainty of predictions valuedecisions enables exploring the effect of uncertain predictions by propagating the uncertainty to the results with monte carlo simulation in our case this is particularly relevant given that high uncertainty was associated with some predictions e g for the objective high number of local jobs fig si 2 we investigated how these uncertainties affect the outcome to estimate whether it is worth or necessary to invest additional time to improve the predictions 4 2 4 cost benefit visualization valuedecisions enables us to represent the aggregated value over any combination of objectives as a function of any other combination of objectives such as low cost a cost benefit analysis is usually important for stakeholders they need to know if paying more actually pays off i e makes a difference in terms of achieving higher values on all other objectives we used the x vs y analysis in valuedecisions selecting the objective low cost for visualization on the x axis and the aggregated value from the mcda for the other eight objectives on the y axis again we used the swing median weight profile 4 3 results and discussion of the analyses 4 3 1 visualization of objectives predictions weights and lowest level value functions valuedecisions has various possibilities to visualize the input data as well as intermediate steps of the analysis this is helpful to verify that data were entered correctly but also to understand the results the evaluation structure is given by the objectives hierarchy fig 3 for the predictions we checked that the data displayed in the graphs and the tables corresponded to our input files for lowest level value functions we verified whether their shapes and slopes were as intended example in fig 1 we also visualized the performance of all alternatives on the lowest level objectives fig si 1 for the weights fig 4 we checked correspondence of displayed weights with the original input file at the lower and upper level of the objectives hierarchy 4 3 2 effect of weight profiles first we compared the effect of the three groups median weight profiles swing swinginvalid and dirrat obtained through the survey weights were relatively uniform within each of the groups in the most spread distribution swing the weights varied from 0 09 to 0 14 fig 4 the ordering of options and the obtained overall values was very similar across the three weight profiles fig 5 option 5 dry toilet with composting chamber had the highest overall values this was followed by option 4 vacuum network with decentralized treatment the worst performing option for all groups was the status quo despite somewhat different weight profiles the mcda results were thus similar for all three groups therefore we focus on the swing median weight profile for the following analyses as the median weights obtained by the survey were similar we created two extreme weight profiles to explore possible effects of other weight distributions preservationist preserve environment and utilitarian high socio economic performance see section 4 2 1 with such contrasted weight profiles fig si 4 the values of options fig 6 and their ranks of options differed greatly between the two groups for preservationists all options except the status quo performed similarly with values ranging from 0 62 to 0 70 fig 6 for utilitarians there was a similarly high achievement for option 5 dry toilet with composting chamber v 0 75 and 4 vacuum network with decentralized treatment v 0 61 this resulted from option 5 performing relatively well on all objectives except on social wellbeing fig si 1 however for utilitarians the performance of option 3 urine source separation with on site storage v 0 20 and option 2 urine source separation with on site concentration v 0 17 was markedly poor compared to preservationists the utilitarians high weights on the objectives social well being and economic performance but low weights on environmental protection and resource use can explain this interestingly the status quo option performed poorly for preservationists v 0 10 but considerably better for utilitarians v 0 32 however despite extremely different weights it was possible to find two consensus options for the groups namely 4 vacuum and 5 compost based on the assumption that the importance of low costs might have been underestimated by the population but is decisive to authorities we interactively doubled the weight of the cost objective from 0 1 to 0 2 for clarity of presentation we focus on the medswing original weight profile fig 4 and the analysis without prediction uncertainty the ranking of options remained the same despite this weight change and the values were of the same order of magnitude fig si 5 option 5 0 76 option 4 0 61 option 2 0 39 option 3 0 38 status quo 0 22 thus even if the importance of low costs might have been underestimated the results were not sensitive to the doubling of the weight valuedecisions also provides graphs for local sensitivity analyses of weights on each objective we discuss results for the objective high chance of compliance by end users cb comp the weight assigned to this objective by survey respondents was relatively low however end user compliance is actually a main concern of the paris wastewater authority as the results show cb comp is a relevant objective in the decision because the values and rankings of options can drastically change depending on the weight fig 7 in the extreme case option 5 compost best performing with the current weights would achieve the lowest performance if the weight of cb comp was increased to 0 62 or higher inversely 1 status quo worst performing with the current weights would achieve the second best value if the weight was increased to more than 0 42 it would even perform best along with option 4 vacuum if only objective cb comp was considered i e having weight 1 if the wastewater authority is not ready to bear a high risk regarding this objective option 4 vacuum might be recommendable it is only outperformed by option 5 compost when the weight for cb comp is relatively low but achieves higher values than option 5 as soon as the weight is higher than 0 19 visualized by crossing lines for options in fig 7 4 3 3 lowest level value functions what if not linear valuedecisions allows jointly changing all lowest level value functions to an exponential shape and varying the curvature parameter c see eq si 3 we tested the effect of strong curvatures where c 5 and c 5 example in fig 1 compared to using linear value functions as in the previous analyses for c 5 both the values and the ranking of options were partly impacted table 1 for c 5 the overall values strongly changed while the ranking remained consistent because option 5 compost or option 4 vacuum performed best in all three cases our recommendation is not impacted the robustness of the main conclusions also suggests that investing more time to elicit lowest level value functions from stakeholders is unlikely to add value to the analysis however this is dependent on the specific case and the model assumptions and needs to be verified in every new application 4 3 4 aggregation model what if not additive valuedecisions allows varying the non additivity parameter γ of the weighted power mean aggregation function to explore the effect of different aggregation models on the results see eq 2 this is relevant since we could not ask stakeholders about their preferences concerning the assumptions and implications of the additive model which often does not comply with people s preferences we tested the effect of various aggregation functions table 2 a rank reversal between option 5 compost being best and option 4 vacuum occurred around γ 0 25 with the minimum aggregation function all options received a value of zero because all options performed worst on at least one of the objectives fig si 1 similarly the high values for the maximum aggregation function are understandable when looking at figure si 1 all options performed best on at least one of the objectives except option 3 usep store which performed second best and quite well on the objective aa nut low discharge of nitrogen to the river and air this analysis again confirmed that the most robust recommendation would be option 5 compost closely followed by option 4 vacuum 4 3 5 including uncertainty of predictions our predictions for some attributes were highly uncertain for instance for the number of local jobs da job but rather certain for other attributes for instance water saving bb wat fig si 2 this prediction uncertainty can be propagated to the mcda results and can be represented by barcharts with error bars fig si 3 or boxplots fig 8 for clarity we again focus on the medswing weight profile fig 4 and our standard assumptions linear lowest level value functions additive model after 2000 monte carlos simulation runs option 4 vacuum performed best median value 0 70 closely followed by option 5 compost median value 0 64 while 1 status quo remained the worst performing option v 0 31 these findings also become clear with visualizations focusing on the ranks fig 9 fig si 7 across all simulation runs option 4 vacuum and option 5 compost achieved highest ranks often and never the bottom rank considering these results it would be advisable to lower the uncertainty of the predictions for option 4 vacuum and option 5 compost in order to establish with more certainty which of these would be the best performing option 4 3 6 cost benefit visualization the visualization of costs attribute db cost versus all other benefits calculated as aggregated value of the remaining objectives using our standard mcda model fig 10 confirmed previous results option 5 compost and option 4 vacuum performed almost equally well however option 5 compost was markedly more cost efficient than option 4 vacuum in fact option 5 compost with estimated costs of 92 per person and year was the only cost efficient option for the considered preferences which is also why there is no efficient frontier in fig 10 fig si 6 depicts such an efficient frontier for the utilitarian and preservationist perspectives option 1 status quo clearly performed worst lowest value resp benefit and having estimated costs of 125 per person and year is almost as expensive as option 4 vacuum the two most expensive options 2 usep conc and 3 usep store require a high cost investment for a rather small value gain since the predictions for cost had large uncertainties fig si 2 which is not visualized here we recommend the decisions makers to obtain better predictions at least for option 4 vacuum and option 5 compost to increase trust in the robustness of these findings 4 3 7 implications for the case study the results of the mcda using valuedecisions given the model the predictions and the weight preferences elicited online from a sample of 655 people in the paris region clearly indicate two best performing options 4 vacuum urine and feces collection in a separate vacuum network with decentralized treatment and 5 compost dry toilets with on site underground composting chambers these results are robust to changes in preferences model assumptions and to uncertainty in predictions we are confident to recommend these two options for further evaluation to the main wastewater authority in the paris region moreover our analyses allow us to recommend further investigating the predictions of costs and of compliance by end users for these two options this will increase our confidence which of the two options can be expected to perform best 5 evaluation of usability 5 1 evaluation criteria and questionnaire items we believe a value focused approach to software development is helpful one important aspect is usability for the intended users which is embodied in numerous user centered design approaches e g brhel et al 2015 we performed a usability test about the quality in use during the development of valuedecisions that we formulated based on the iso eic 25010 standard iso iec jtc 1 sc 7 2011 this standard suggests five criteria to evaluate usability we focused on three of these effectiveness efficiency and satisfaction which all belong to the quality in use category each criterion is specified by one or several sub criteria we did not evaluate the criterion freedom of risk as we foresee no economic risks valuedecisions is free and open source no health and safety risks e g no addiction risk and no environmental risks no special infrastructure or high electricity consumption is required the criterion context coverage is to our understanding covered in our evaluation of the product quality characteristics see section si 5 and was not evaluated from the user perspective the iso eic 25010 standard provides a structure for the evaluation but no concrete format table 3 and in more detail table si 4 present our operationalization of the standard the evaluation is based on user feedback collected with a questionnaire and on observation based on literature we developed between three to six items for each sub criterion to measure their achievement table si 4 the items are variants of a single question to measure a sub criterion in a reliable and robust manner kline 2000 we also asked users how often they required support e g from the user guide assistance from teaching assistant assistance from other students in two open text questions users could note what they appreciated about valuedecisions and what they recommend to improve to measure the criterion effectiveness we used the grades assigned to the student reports as a proxy indicator instead of questionnaire items 5 2 implementation and test users the purpose of the usability test was to evaluate the app in a systematic way and to reveal how to improve and further develop it for testing usability representative users should perform representative tasks lazar et al 2017 we conducted the usability test with master students with an environmental major who were learning mcda based on mavt and had a basic but sound method knowledge they are target users of the valuedecisions app see section 3 2 potentially they will work as environmental consultants or in government agencies and may use the app in their future career sixteen students carried out four environmental case studies using valuedecisions as part of a course at eth zurich in spring 2020 lienert 2020 we repeated the usability test in the in the spring 2021 course this time on a voluntary basis eight students from three case studies answered the students filled in the usability questionnaire as homework at the end of the course they were fully informed that they were test users and signed an informed consent form the survey was coded in limesurvey limesurvey gmbh 2020 the questions appeared in random order except if they implied a logical development all survey items are available in table si 4 the survey was pre tested by two research assistants and the teaching assistant 5 3 results and discussion of evaluation and response the evaluation of usability varied considerably across sub criteria and among students table 4 table si 5 while our sample is too small to allow for any generalization the usability testing nevertheless helped to better understand use of the valuedecisions app and weaknesses based on this feedback we made considerable efforts to improve the app with additions to the code and further explanations here we highlight few important results and responses based on the test in 2020 a full description is provided in si 6 the efficiency of using the app especially the time efficiency was rated relatively low we suspect that this is largely due to difficulties related to preparing properly formatted and valid input files the teaching assistant mainly supported students in finding formatting mistakes once they had set up correct input files the students were capable of using valuedecisions autonomously for carrying out their case studies we also included questions to assess reasons for loosing trust in the app across all exercises 14 of the 16 students had received error messages many students found that the messages did not indicate the source of the error clearly enough for them to know how to address it resulting in low evaluations of these questionnaire items ten students faced moments when they did not know what to do next and ten lost the running analyses and needed to restart the app hence while the general trust in valuedecisions was positive mean 4 12 students lost trust mean 2 59 due to technical difficulties as a response we implemented additional validation of input files after upload and provided more details what users need to change when this validation fails furthermore we added a checklist to support users in systematically controlling their files information about common error messages and potential solutions and extended the user guide we also decided to better describe the output data and analyses within the app lastly we provided an exemplary decision analysis choosing the holiday destination for my extended family which is understandable to many with input files that can be downloaded as templates the other sub criteria to satisfaction usefulness pleasure and comfort were on average rated positively table 4 most students found valuedecisions pleasant to use and stated that it had a user friendly interface they also felt comfortable using the app and found it easy to learn how to use it clear and understandable and flexible regarding the analysis possibilities this increased our confidence that the approach taken for valuedecisions is useable in practice and that people with basic understanding of mcda can successfully use valuedecisions and will have an effective and satisfying experience a repetition of the user feedback questionnaire in the 2021 mcda course confirmed the evaluation si 6 4 while the time needed to prepare the input file was still detrimental to the overall evaluation the other aspects were well perceived we conclude that users require a solid method understanding to understand why the information in the input files has to be specified in a certain way and to use the app well as for any new software a learning phase seems necessary prior to the efficient use of valuedecisions in addition to the software evaluation with the user survey we involved expert users from the decision analysis group at eawag in developing valuedecisions and several external scientists are already using it in their projects their continuous use and feedback led to various other improvements including bug fixes support for additional visualizations and download of interim simulation results furthermore the product quality elements of the iso eic 25010 standard iso iec jtc 1 sc 7 2011 served as a checklist to guide the software development see si 5 the usability test that we developed for this study based on the iso eic 25010 standard iso iec jtc 1 sc 7 2011 is general and can be adapted to other contexts and software the full questionnaire is provided in table si 4 our approach was pragmatic and a scientific validation of the scales see kline 2000 would need further research nevertheless as starting point our scales showed satisfactory internal reliability table si 5 which indicates that the items reliably and consistently measured the sub criterion they were designed for generally we found the value focused approach to software development and the evaluation of this via usability testing valuable for guiding development efforts 6 discussion and conclusion 6 1 supporting decisions with the valuedecisions app in an application concerning wastewater management in the paris region also see aubert et al 2020 we demonstrated how a public policy decision problem can be analyzed using valuedecisions we collected the preference data from the population before valuedecisions was conceived but could easily convert it to the appropriate format by performing extensive sensitivity analyses we found that key results were insensitive to preference parameters not collected in the online survey of 655 respondents such analysis supports determining for which aspects of the decision more information would be helpful and for which not in our case results are robust enough to conclude that eliciting more detailed preferences from stakeholders would likely not change the main results this is especially relevant for online surveys with lay people as in our case to avoid cognitive overload and tiring of respondents e g riabacke et al 2012 we did not elicit the shapes of single attribute value functions or the best fitting aggregation model earlier studies also concluded that not all preference parameters have to be known in detail schuwirth et al 2012 scholten et al 2015 but this is always case dependent uncertainty analyses allows us to recommend that it could be worthwhile to decrease the uncertainty of the input data for predictions if a clearer differentiation between the two best performing options is desired the focus on visualization given by valuedecisions was key for generating insights and should ease communicating about the robustness of the results to the paris decision makers two unconventional wastewater management options emerged as robust choices namely dry toilets with on site underground composting chambers and urine and feces collection in a separate vacuum network with decentralized treatment section 4 3 this answers the main concern of the paris wastewater authority it seems reasonable to consider a radical system change for wastewater management in certain contexts larsen et al 2016 hoffmann et al 2020 given the preferences of a representative sample of paris region citizens our results thus validate the momentum in the paris region toward unconventional systems pilot projects are implementing some options we considered in this analysis 6 2 alternative software for environmental decision support all tools have a context and use cases in which they are useful consequently valuedecisions was developed with specific use cases and users in mind section 3 2 as far as we are aware the combination of advanced features valuedecisions offers for instance with regard to preference modeling is unique especially combined with a simple spreadsheet interface for the input data a web interface and automatic reporting however depending on the specific decision case other software may be better suited see weistroffer and li 2016 mustajoki and marttunen 2017 mcda software development geared to different types of use and users is an active field of research with several recent additions e g chacon hurtado and scholten 2020 cinelli et al 2021 preference ab 2021 and we expect interesting developments in the future while not being able to do justice to all existing software we briefly point out some developments that address uses valuedecisions is lacking some mcda software focuses more on guiding users through the process of decision making mustajoki and marttunen 2017 emphasize that for non expert users software needs to provide automatic guidance for instance to overcome commonly encountered biases entscheidungsnavi is one online tool that has extensive user support to overcome common biases and guides users through the entire decision making process based on maut von nitzsch et al 2020 process guidance is also offered in software like v i s a decisions simul8 2021 or logical decisions logical decisions 2020 and others truly interactive elicitation of preferences is for instance possible with fitradeoff de almeida et al 2016 supporting decisions with multiple stakeholders is an important property in public decisions explicit support for group decision making is for instance provided by logical decisions logical decisions 2020 or helision preference ab 2021 furthermore spatial assessment can be a relevant feature of environmental problems the combination of mcda and geographic information systems gis is an active research area with various software developments e g greene et al 2011 keenan and jankowski 2019 valuedecisions does not allow for explicit spatial analysis but attributes relevant to spatial features can be used for example number of protected areas affected or space required other software allow for greater flexibility in modeling this is provided by programming libraries and packages for r such as utility reichert et al 2013 or mcda bigaret et al 2017 and for python such as decisi o rama chacon hurtado and scholten 2020 the latter also supports portfolio decision analysis lahtinen et al 2017b with an appropriate workflow software based on these libraries can also be coupled to complex prediction models examples of software with graphical user interface that supports different mcda algorithms are decerns linkov et al 2020 or decspace amador et al 2018 furthermore the diviz initiative aims at providing a common interface to many algorithms meyer and bigaret 2012 6 3 further development of valuedecisions valuedecisions can be extended in many ways thanks to its modular design features can be added without changing the basic structure five examples of further development are 1 support for uncertain preference parameters for instance stakeholders may specify a range of weight parameters rather than point estimates scholten et al 2015 or provide an estimate about how uncertain they were about their statements zheng et al 2016 by allowing uncertain preferences a more realistic picture of the uncertainty of results would be achieved this would extend the current approach which relies on performing sensitivity analyses regarding preferences if uncertainty of predictions and preferences should be considered jointly the concept of expected expected utility could be implemented haag et al 2019b 2 support for multi attribute utility theory section 2 2 4 by including the risk attitude of stakeholders and calculating expected utilities instead of values preferences about uncertain predictions could be directly considered as in scholten et al 2015 additionally the risk attitude could be interactively varied and robust options identified 3 support for smaa methods and metrics for analysis stochastic multi criteria acceptability analysis lahdelma et al 1998 these also can be useful for mcda problems where both predictions of options and preference information is uncertain e g zheng et al 2016 4 interactively support users in data entry and or data elicitation currently users need to provide input data with two spreadsheets this is error prone as issues with incompatible entries may arise and users may be unsure how to provide this information properly integrating the data entry or even preference elicitation step into valuedecisions would make for a more seamless experience such user guidance was also one recommendation of mustajoki and marttunen 2017 5 the automatic reporting could be extended one possibility is using the ideas of natural language generation to explain results to lay users wulf and bertsch 2017 we believe this to be an interesting feature for potential future users such as environmental consultants scientific software development exhibits the long tail phenomenon i e the large majority of software sees only very little uptake and reuse e g walling and vaneeckhaute 2020 software sustainability is a major issue for academic software e g venters et al 2014 and many tools in the long tail are eventually lost valuedecisions also faces this risk two aspects might mitigate this risk firstly valuedecisions is an open source development the source code is available and can be reused or modified freely by anyone secondly it is written in r which is a programming language commonly used for analysis in academia and the app comes in the standardized form of an r package this allows people without specific software development background to contribute to or change the app additionally this makes valuedecisions interoperable with other r packages for mcda such as utility reichert et al 2013 it is possible to exchange the calculations and algorithms in the backend while keeping the visualization and frontend of the application 7 conclusions effective decision support is facilitated by software tools that help analyze visualize and understand the key aspects of a decision problem this insight well known in practice has sparked numerous software developments since the 1980s reviewed e g by korhonen et al 1992 french and xu 2005 weistroffer and li 2016 mustajoki and marttunen 2017 here we introduced a novel open source development the valuedecisions app valuedecisions was developed to support analysts facilitators and interested stakeholders during the modeling stage of an mcda process it is targeted towards environmental and public policy problems key properties of the app are i the possibility to represent complex stakeholder preference structures over multiple objectives by building hierarchical mavt models that combine non linear lowest level value functions with non additive aggregation functions ii the possibility to consider uncertainties by different probability distributions for predictions interactive sensitivity analyses and visualizing uncertainty in different ways iii the comparison of results for multiple potentially conflicting stakeholder preference profiles iv a graphical user interface accessible via a web browser that focuses on producing insightful visualizations v open source development based on r which allows for modifications and extensions of the algorithms and visualizations we tested valuedecisions with data from an online survey of 655 citizens for an urban water management decision in the paris region valuedecisions allowed us to clearly identify robust options for wastewater management by using different sensitivity analyses to identify user needs we developed a structured usability test based on the iso eic 25010 standard iso iec jtc 1 sc 7 2011 students participating in a mcda lecture lienert 2020 used valuedecisions to tackle environmental decision problems their responses in the usability test pointed us toward important improvements of the app the usability survey we developed can now be used tested and improved in other applications given the large role software plays in applied projects we know surprisingly little about how it is used and what that use implies every software tool has certain affordances and enables users to do certain things but not others the field of behavioral operations research bor e g franco and hämäläinen 2016 hämäläinen 2015 has picked up such research we can only reiterate the call of mustajoki and marttunen 2017 to research the interaction of people with mcda software for example how people use software what kind of support they need and how the characteristics of the software affect people s learning and interpretation of results one important step in this direction is user centered evaluation cf walling and vaneeckhaute 2020 as we have operationalized here our basic motivation for developing valuedecisions was making mcda analysis for environmental decision problems accessible to a wider audience given the usability evaluations we are hopeful that valuedecisions can contribute to this we chose to make the software available free of charge and accessible as a web application without any installation we hope that the possibilities offered by the app will facilitate the application of decision analysis based on multi attribute value utility theory and eventually contribute to better structured transparent and well informed decisions software availability name of software valuedecisions developer and contact information concept design fridolin haag judit lienert programming fridolin haag kevin schönholzer sara schmid eth scientific it services contact judit lienert eawag ch year first available 2020 hardware required no specific requirements software required web browser for the r package version r base installation availability https www eawag ch en department ess main focus decision analysis da tools program language r cost free open source license agpl data availability data and software used in this paper are available on the eawag research data institutional repository https opendata eawag ch https doi org 10 25678 00048s credit author statement fridolin haag conceptualization methodology software writing original draft review editing visualization supervision project administration alice aubert conceptualization methodology investigation writing original draft review editing supervision usability test case study judit lienert conceptualization software testing and improvement writing original draft review editing funding acquisition supervision project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we are grateful to kevin schönholzer sara schmid and the its scientific software data management group at eth zürich for their contributions to programming valuedecisions we thank our colleagues qi zeng and sara schmid for supporting data analysis harald von waldow and markus kasper for valuable advice on software deployment and daniel hofmann and philipp beutler for feedback we thank the respondents of the paris case study for answering our survey and fabien esculier for collaboration and interacting with the paris region wastewater authorities we are indebted to the students of the mcda lecture in 2020 and 2021 for using valuedecisions and participating in the survey alice aubert was supported by an eawag postdoctoral fellowship 2015 grant number 5221 00492 009 08 df 15 and a swiss national science foundation ambizione grant project 173973 edanaga eawag also funded this applied research as project a 5 mcda tool under the 2019 2020 agreement between the scientific it services of eth zürich and eawag we wish to sincerely thank the four reviewers and the editor for their constructive comments which helped us to improve this paper appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105361 
25631,the assessment of the uncertainty about the evolution of complex processes usually requires different realizations consisting of multivariate temporal signals of environmental data however it is common to have only one observational set marinetools temporal is an open source python package for the non stationary parametric statistical analysis of vector random processes suitable for environmental and earth modelling it takes a single timeseries of observations and allows the simulation of many time series with the same probabilistic behavior the software generalizes the use of piecewise and compound distributions with any number of arbitrary continuous distributions the code contains among others multi model negative log likely functions wrapped normal distributions and generalized fourier timeseries expansion its programming philosophy significantly improves the computing time and makes it compatible with future extensions of scipy stats we apply it to the analysis of freshwater river discharge water currents and the simulation of ensemble projections of sea waves to show its capabilities keywords time expansion of parameters non stationary probability models stochastic characterization environmental modelling 1 introduction in environmental and engineering sciences the understanding and quantification of the evolution of processes are highly related to the capacity to measure the variables involved and the ability to build up models capable to reproduce their interrelationships moges et al 2021 some processes can be modeled as systems that respond to a forcing usually represented by one or several input time series if the driving agent has a random character as with climate forced processes it may be interesting to stochastically characterize the response however for a given measured or projected time series the model just provides a single realization of the output most processes are complex enough so that their statistical nature cannot be directly inferred from the joint distributions of the input variables under such circumstances simulation techniques can be used to obtain a large number of input realizations whose responses constitute a sample of the output from that sample it is possible to assess the uncertainty that the response inherits from the forcing see e g baquerizo and losada 2008 meier et al 2019 the study of this type of simulation techniques is nowadays a hot topic in climate driven processes refsgaard et al 2007 and climate change projections kundzewicz et al 2018 ellis et al 2021 uhe et al 2021 one of approaches consists in the stochastic characterization of a vector random process vrp coupling several probability models pms which parameters are assumed non stationary ns with a vector autoregressive model var which characterize the multivariate and temporal dependency see solari and van gelder 2011 solari and losada 2011 among others the theoretical applicability of the later approach has been illustrated in works where specific subsets of probability models were analyzed independently for different purposes among others i the observed wave climate variability in the preceding century and expected changes in projections under a climate change scenario lira loarca et al 2021 ii the optimal design and management of an oscillating water column system jalón et al 2016 lópez ruiz et al 2018 iii the planning of maintenance strategies of coastal structures lira loarca et al 2020 iv the analysis of monthly wolf sunspot number over a 22 year period cobos et al 2022 and v the simulation of estuarine water conditions for the management of the estuary cobos 2020 we follow a novel approach in marinetools temporal a package included in marinetools which is a python framework that integrates software that can be used in the search for solutions to real engineering and marine problems this temporal package is to the best of the authors knowledge the first open source and free available general tool aimed at providing users with a friendly general code to statistically characterize vector random processes vrp and to obtain realizations of them it generalizes the above mentioned approaches for vrps whose components are ns and have piecewise or compound distributions defined by means of any number of arbitrary continuous distributions its programming philosophy widely used in python significantly improves the computing time and allows the use of any probability distributions included in future extensions of scipy stats the most widely used python statistical package it is implemented in python an interpreted high level object oriented programming language widely used in the scientific community and it makes the most of the python packages ecosystem among the existing python packages it uses numpy which is the fundamental package for scientific computing in python harris et al 2020 scipy which offers a wide range of optimization and statistics routines virtanen et al 2020 matplotlib hunter 2007 that includes routines to obtain high quality graphics and pandas mckinney et al 2010 to analyze and manipulate data in addition to third party packages marinetools temporal also contains among others novel routines for the required negative log likely functions the generalized fourier time series expansion the use of wrapped normal distributions the multi model ensemble approach for climate projections and a whole set of plotting functions for the non stationary representation these examples use time series commonly used in coastal engineering sea waves significant wave height peak period and mean direction oceanography sea surface temperature and currents at open sea hydrology river discharge and astronomy sunspots number their choice is aimed at representing all the capabilities including i the analysis of circular variables ii the treatment of times series with persistent low values and iii the analysis of multi model climate projections in addition four more examples that enlarge even more the applicability of the software are included in the repository the paper is divided into five sections after the introduction the theoretical basis for fitting the parameters of the ns probability models pms the description of the temporal and multivariate dependence the time series simulation and the analysis of multi model climate data are briefly presented in section 2 some applications to illustrate the use of marinetools temporal are presented in section 3 the discussion about the algorithms some hints about its use and expected upgrades is given in section 4 and finally section 5 concludes the study 2 theoretical background and definition of the parameters this tool is aimed at 1 characterizing a vector random process x x 1 t x i t x n t that can be uni or multivariate where t belongs to a set of index on the basis of an observation of it at n o discrete points x o t j x 1 o t j x i o t j x n o t j for j 1 n o and 2 simulating other random realizations that have the same joint probabilistic behavior than the first one for the sake of simplicity from now on we will speak about temporal time series and assume that the rps are measured at equally spaced instants the characterization includes 1a the fit of the marginal ns distribution functions of each random variable x i and 1b the description of the dependence of the value of each component at time t j with previous values of all the components through a vectorial autoregressive model the tool can also deal with multi model time series coming from different combinations of global and regional climate models gcm rcm through the use of compound marginal variables and an ensemble averaged var model as proposed by lira loarca et al 2021 the time series simulation is based on those results it first generates a stationary multivariate time series of non exceedance probabilities and then it recovers the values of each variable by using the inverse of the non stationary cumulative distribution function fig 1 shows the flow chart of the algorithm the cascade analysis consists of several steps being the initial inputs the observed or hindcasted data x o t j j 1 n o and the dictionary that contain the key parameters that provide the probability structure of the rps the information regarding the var model and the simulation the tool has a simple syntax based on dictionaries see table 1 and it helps users to supply the input information particularly for step 1a which requires some knowledge about the phenomena under study at the end of each step the results of the analysis are saved to a file whose name and some information regarding the modelling process are appended to the input dictionary in the following a brief description of the methodology implemented in marinetools temporal is presented and at the same time the options and values of the parameters that represent the relation between mathematical and statistical methods used in the tool are commented the interested reader is referred to cobos et al 2022 lira loarca et al 2021 2020 egüen et al 2016 solari and losada 2011 solari and van gelder 2011 among others where several examples of its application can be found the description values and examples of the key variables for the analysis and simulation tool are defined in table 1 the identifiers mf td ts and ea given in the 1st column stand for the marginal fit the temporal dependence analysis the time series simulation and the ensemble average multimodel analysis of climate projections respectively 2 1 fit to marginal probability models mf options each random component x i is fitted to a marginal distribution f x i x i with the maximum likelihood method the variables that describe the angles of vector magnitudes such as the wind or wave direction require a special analysis the model allows to indicate that a variable is circular by including the key circular with the value true among others circular pms like von mises mardia and jupp 2009 included in scipy can be used the wrapped normal pewsey 2000 that was implemented ad hoc by following the philosophy of scipy package is also available for some variables such as the precipitation or the fresh water river discharge in water scarce areas the presence of abrupt changes in time makes difficult the fitting of multiple pms for those cases it is suggested to apply a parametric and monotonic transformation to deal with gaussian distributed data for that purpose the methods included in marinetools temporal are those proposed by box and cox 1964 and yeo and johnson 2000 this option can be set up by adding a dictionary with the key transform with a set of keys the first one make indicates that the transformation will be done if it is equal to true the model key indicates the selected transformation method items box cox or yeo johnson the users can also indicate to the tool whether they want to plot the cdf of the normalized data key plot equal to true 2 1 1 structure of the marginal cdfs the distribution can be constructed from multiple pms so that it can properly represent the non exceedance probability of all the plausible values including the tails and the body see appendix b 1 the distributions are defined either by a piecewise function key piecewise equal to true see cobos et al 2022 or by a mixture or compound distribution key piecewise equal to false the first option also contemplates the use of a single distribution in any of these choices the functions involved are specified in key fun for a piecewise distribution with more than one pm the user has to provide in key ws ps the initial guesses for the common endpoints percentiles of the intervals in which the real axes has been divided if a compound distribution is selected the values provided in ws ps are the first guesses of the weights of the first pm s the weight of the last pm is derived from the others so that their sum equals 1 2 1 2 choice of the stationary or ns character of the analysis the analysis can be stationary which means that the parameters of the pms i e shape location or scale do not vary in time otherwise they are assumed to vary over a given time interval with a duration of n y years and therefore the distribution function f x i x i t t is non stationary the choice is done by setting the option ns analysis to false or true respectively in that regard if the ns analysis is chosen the parameters are expanded into a generalized fourier series gfs over the interval 0 n y which is truncated to n f terms see appendix b 2 the basis functions are chosen by setting up basis function with one of the choices provided in table 2 and by indicating the number of terms to be retained in the series key no terms equal to an integer number if the basis function contains periodic functions it is recommended to perform a previous spectral or harmonic analysis of the time series in order to gain knowledge about the oscillatory components the tool selects by default n y 1 which allows to analyze time variations up to the yearly scale in some time series longer variations associated to climatic indexes oscillations can be found le mouël et al 2019 the tool offers an option to modify the basis period to n y years which allows the oscillations to be analyzed on a different see cobos et al 2022 with the choices provided by the user in the input dictionary for variable x i the tool internally computes an initial guess of all the parameters involved in the definition of the pms and the coefficients of the gfs and it finds the optimum values of these coefficients and the values provided in key ws ps the tool also shows some informative messages and includes some guidance plots to help in the selection of the marginal fit properties the package also contains several functions to assess the goodness of the fit of the analysis and the simulations some of these functions have been used to present the results of the study cases in section 3 2 2 multivariate and temporal dependency of the rps td options once the marginal probability structure of the random variables has been obtained the temporal multivariate or univariate dependency might be inferred by using autoregressive methods in this version the models available are the uni and multivariate autoregressive ar q and var q respectively that assume a linear relationship between the value of the rps at a given time and their past q values other possibilities such as arma arimax dar egarch gas or gaussian local level can be easily implemented with python functions included in pyflux package the information related to the temporal dependence is provided in a dictionary where the names of the variables to be jointly analyzed are given in key vars and the method selected in method starting from the first order the model tests all the orders up to the maximum value q indicated in order and provides the information from the one with the smaller value of the bayesian information criterion bic it is worth noting that although in the marginal fitting of every rp the time series can contain small gaps in time for the assessment of the temporal univariate or multivariate dependency the observations need to have the same resolution table 1 with id td contains a description of the possibilities that the tool offers for multivariate temporal dependency analysis their values syntax and examples 2 3 the simulation process ts options the simulation process is done by retrieving the information from dictionaries of the marginal fit multivariate and temporal dependency analysis and if selected the multi model ensemble average in addition the tool requires the starting and ending dates of the simulation period keys start and end in the standard datetime format of python and the number of simulations key nosim then it writes in different files every random realization of the univariate or multivariate rp as output the same temporal time step than the original dataset is used in order to guarantee the same probabilistic behavior 2 4 characterization of ensemble average multi model climate projections ea options climate services provide several climate projections from different combinations of gcm rcm climate models under a certain greenhouse gas concentration trajectory associated to a representative concentration pathway rcp adopted by the ipcc lira loarca et al 2021 proposed to gather the information of different gcm rcms through the use of a compound distribution that weights the ns marginal distributions and their var q coefficients to obtain a multi model ensemble average characterization of the multivariate random processes the package includes an option key ensemble equal to true to work with the multi model ensemble averages using equal weights a choice corresponding to the commonly used rule one model one vote recommended by the ipcc pörtner et al 2019 or the values provided in key weights to do so the marginal fit analysis and the temporal multivariate dependence analysis need to be done previously for all the models as explained in sections 2 1 and 2 2 the tool provides several files with the non exceedance probabilities of the compound variables and the corresponding var coefficients 3 applications to earth and environmental modelling in the following subsections we illustrate the capabilities of marinetools temporal with some novel applications for the analysis of environmental vrps more precisely the examples include i the univariate pm analysis of projections of freshwater river discharge at alcalá del río dam spain ii the multivariate analysis and temporal dependency of velocity currents at the strait of gibraltar atlantic ocean furthermore iii the simulation of new projections from marine climate data significant wave height peak period and incoming mean direction wind velocity and incoming mean direction at the alborán sea at the western mediterranean sea under the rcp8 5 scenario from the ensemble multivariate multi model information lira loarca et al 2021 we present a detailed description of the marinetools temporal framework methods and how they operate together to capture the non stationarity and to create simulations of some general examples small pieces of code will be shown next to the exemplifying dictionaries to run the analysis these applications and those described in table 3 have been selected in order to illustrate a set of representative options that the tool integrates however many other types of vrps can also be considered the documentation is available at the github gdfa ugr repository it includes the code case studies and the examples of table 3 included as jupiter notebook files this will allow interested readers to reproduce the applications with similar synthetic data sets and to facilitate its adaptation to other time series 3 1 freshwater river discharge projection at alcalá del río dam this first example is focused on a univariate time series denoted by q d t of the projection of daily fresh water river discharge from the alcalá del río dam to the guadalquivir river estuary 37 29 n 6 06 w from january 1st of 2020 to december 31st of 2040 this series are obtained from the hydrological predictions from the environment model hype forced with the atmospheric model remo 2009 for the rcp 2 6 scenario source swedish meteorological and hydrological institute smhi due to the strong regulation of the river at this dam the last one in the river course before its flow into the atlantic ocean the series shows low values in summer q d 40 m3 s that are almost squared in winter q d 1000 m3 s to deal with the high variability between seasons a box cox transformation with λ 0 0796 parameter is used the properties of the marginal fit are given in table 4 a weibull of maxima model was selected the highly temporal variability and the clear seasonal behavior lead to the sinusoidal temporal expansion over n y 1 year retaining n f 10 oscillatory terms covering frequencies up to 10 yr 1 the following code shows the python dictionary with the input information required for the marginal fit analysis in the present example image 1 with the previous dictionary params the dataset as a pandas dataframe data and importing the package analysis as from marinetools temporal import analysis the marginal fit function is invoked just coding analysis marginalfit data params the results will be saved in a file in a new folder called marginalfit fig 2 shows the empirical and theoretical fitted ns cdf the empirical ns cdf was computed by using a window size of 14 days which is large enough to obtain representative values of the empirical percentiles but not so long that lower significant variations are neglected in the authors experience for climatic variables with this temporal resolution a window length of 14 days is appropriate for time series longer than 20 years as it is observed in fig 2 the ns pm adequately reproduces the non stationary behavior during the year the theoretical pm captures the overall seasonal behavior during almost all the year for all the percentiles with a marked valley during summer and peaks in the previous and subsequent seasons only some deviations from the peaky behavior are observed during february march and at the end of november for the highest represented percentile 0 99 which is slightly underestimated 3 2 currents at the strait of gibraltar the second example analyzes the multivariate time series of the water current field mean current velocity u and mean incident current direction θ u hindcasted at 0 5058 m below the mean sea level at a point located in 35 9166 n 5 5 w at the strait of gibraltar data provided by marine copernicus system the hindcast time series has 27 years duration with data that spans from 1993 01 01 to 2019 12 31 with a daily temporal cadence the ibi iberian biscay irish ocean reanalysis system provides 3d ocean fields product identifier ibi multiyear phy 005 002 the ibi model numerical core is based on the nemo v3 6 ocean general circulation model run at 1 12 horizontal resolution the marginal fits were carried out with the characteristics given in table 4 briefly the univariate analysis of u was carried out using a gaussian pm the water current incident direction was fitted using a weibull of maxima pm for both variables the trigonometric fourier expansion was performed over a basis period of one year n y 1 with eight oscillatory components n f 8 fig 3 a shows the empirical and theoretical models of the ns cdf for u as it is observed the gaussian model fairly reproduces the temporal variation of the probability distribution function fig 3 b shows the same analysis with θ u a marked eastwards flow around 270 is observed showing that the mean currents at that location of the strait of gibraltar and at the selected depth flow into the mediterranean sea which is in accordance with observations of the water masses exchanges between the atlantic ocean and the mediterranean sea see e g sverdrup and fleming 1942 a weibull of maxima reproduced also fairly well the probability structure in time slightly overestimating the water current directions at the highest percentiles once the parameters of the marginal distributions were obtained the multivariate and temporal analysis were carried out the maximum q order analyzed of the var model was 72 these properties are set up in the dictionary as image 2 the following code is required to load the json file with the parameters from the preceding marginal fits from marinetools auxiliar import read params u read rjson filename u params diru read rjson filename diru given the dictionaries with the results from the marginal fit and the properties of the temporal and multivariate analysis the var model is applied by coding df dt analysis dependencies data params the results are saved to a file in a new folder called dependency among all the orders analyzed the one with the minimum bic was q 6 as it is observed in fig 4 the pattern of the joint density function of hindcast data u and θ u panel a is well reproduced by one of the random simulations panel b the simulation slightly reduces the probability of the modal bump yellow pixels at 275 and 0 4 m s approx 3 3 climate projections using ensemble average multi model ns distributions at the alborán sea finally a multivariate analysis for wave and wind climate projections at the alborán sea 3 608 w 36 66 n is presented from that purpose we used hourly data from the rcp 8 5 scenario and projections of the following global and regional climate models gcm rcm cclm4 8 17 miroc5 rca4 cnrm cm5 rca4 ec earth rca4 hadgem2 es rca4 ipsl cm5a mr and rca4 mpi esm lr ih 2019 the characteristics of these gcm rcm are described in pérez et al 2017 it comprises the significant wave height h s the peak wave period t p the mean incident wave direction θ m the wind velocity v w and the mean wind incoming direction θ w during an interval that spans from 2025 02 01 to 2046 01 01 source ih cantabria the ensemble mean properties were computed using equal weights a choice that represents the rule of one model one vote recommended by the ipcc pörtner et al 2019 the options selected for the analysis and the simulation of five realizations of 20 years are shown next image 3 image 4 image 5 with the previous dictionary params of the average multi model gcm rcm and importing the simulation package with from marinetools temporal import simulation the simulations are achieved by coding simulation simulation params the simulated time series are saved into files in a new folder called simulations for each gcm rcm i the variables were fitted using combinations of pms as fig 5 shows which properties are summarized in table 4 ii the temporal dependency was computed for q orders that range from 26 to 38 h the one corresponding to 26 h had the lowest bic which means that h s t p θ m v v θ w are strongly related with the previous 26 sea states this value is consistent with the one obtained by lira loarca et al 2021 using hindcast data and projections from the meteocean group of the university of genoa in the following the probabilistic characterization of one of the 20 years simulations and the obtained ensemble mean multi model are compared panels a and b of fig 6 show the joint cdfs for the simulated and ensemble averaged projections for h s t p and h s v v respectively it can be observed that the degree of agreement between the simulation and the hindcasted data is generally good which demonstrate that the simulation maintains the joint dependency moreover the r 2 coefficients between data of the joint probability density functions pdf of h s t p h s θ m h s v v and v v θ w of the simulation and the multi model ensemble were for all cases greater than 0 97 fig 7 shows the wind roses at alborán sea for ensemble mean data projections and one random simulation the tool enables the creation of several subplots and the chance to include into them any plot function available in the graphics package matplotlib a proper way to verify the temporal dependency of the simulation and the input data is by computing the autocorrelation function papoulis and saunders 1989 fig 8 depicts the autocorrelation of h s t p θ m v v and θ v for the simulation solid lines the shadow areas delimit the values obtained for the autocorrelation functions for all the rcms as it is observed the simulation and observations show a similar variability with values ranging between 0 7 and 1 the wind field shows a smaller autocorrelation than the rcms during 5 20 h lag also the autocorrelation of θ m is underestimated for all the lags this difficulty of reproducing directional variables already pointed out by monbet et al 2007 is clearly seen in this dataset that correspond to the gulf of cádiz a place where abrupt wind changes are experienced it should be highlighted however that the difference is lower than 1 showing autocorrelation values larger than 0 91 in all cases the results point out that the multivariate and the temporal dependency is also well reproduced 4 discussion the goal of marinetools temporal is the search for the optimum values for any combination of pms and several basis functions actually most of the computational time is spent on the estimation of the optimal parameters of each marginal ns distribution therefore while it is not critic an appropriate selection of the pms will certainly reduce the computational time a suitable combination of pms will usually ensure the convergence of the optimization process however in some cases a slight modification of the weights or percentiles of the common endpoints should be required the tool incorporates several sanity checks to provide a user friendly experience working with ns pms the choice of the basis function is also relevant for the optimization as some basis functions give a better fit with a considerable smaller amount of terms this is the case of the sinusoidal functions that usually requires about the half of terms in the series than the trigonometric of the modified fourier expansion the input time series for the marginal fits can have small gaps since to the search for the optimum value of the nllf does not require to have a regularly observed time series however for the temporal dependence analysis concomitant time series observed with a fixed time step are required the model has been coded to deal with any basis period the examples included for the analysis are done with one year and multiples of it because at temperate latitudes the yearly periodicity is predominant in climate data when long time series are available as it is the case for the sunspot number many years can be considered to capture longer time scale variations it is worth noting that when the basis period is larger than 1 year the initial date and time of the simulations have to be selected carefully in order to avoid an artificial shift of the oscillatory variability marinetools temporal has just included one of the temporal dependency models ar and var however there are many other possibilities in pyflux for python to characterize these relationships such as arma arimax dar egarch gas gaussian local level for independent variables the simulations will show no correlation in multivariate analysis so it is recommended in those cases an independently simulation of each variable the framework marinetools temporal gives a step forward in the need of bridge the gap between the current software development and the coastal and marine engineering practice magaña et al 2020 including advances not only in multivariate analysis which approach to marine design is not now only available for academic use as jonathan and ewans 2013 indicated the tool has been applied to the examples given in section 3 and table 3 which include timeseries coming from coastal engineering sea wave height peak period and mean direction oceanography sea surface temperature and currents at open sea and hydrology river discharge as proof of evidence that continuous rvps can be adequately analyzed and simulated the tool was also applied to analyze and simulate raining patterns results not shown additionally the use of new simulations generated within the marinetools framework might reveal behaviours that emerge from the intrinsic nature of the vector random process analyzed and derived processes which cannot be previously capture and analyze by using one realization which is paramount for structure design and environmental planning 5 conclusions marinetools is an open source project hosted on github the temporal package is dedicated to the analysis of stationary and ns rps and the simulation of earth and environmental data with the same probabilistic behaviour all scripts discussed in the present paper and the synthetic data files are in the repository further examples given as jupyter notebooks covering the full use of marinetools temporal are also available on github the present paper shows the options included in marinetools temporal together with the examples marginal fit of freshwater river discharge from a dam multivariate and temporal dependency of water currents at the ocean and multivariate simulations of sea and wind climate states it is demonstrated how the optimization of the nllf for ns pms succeeds for several combinations of the pms included in scipy stats and the wrap normal three main scripts which include the pre processing steps marginal fit multivariate and temporal analysis ensemble average multi model gcm rcm projections and finally the simulation of earth and environmental time series are presented in detail the use of marinetools temporal offers a very general framework which can be successfully applied to a wide variety of environmental and engineering problems software availability name of software marinetools developer environmental fluid dynamics group university of granada contact information mcobosb ugr es year first available 2021 software required specified at the github repository program language python program size 58 6 kb cost open source tools released under the gnu general public license v3 0 repository credit authors contribution statement m cobos 1 conceptualization methodology software writing original draft preparation p otiñar 1 writing draft preparation p magaña 1 data curation software a lira loarca 2 data curation software methodology a baquerizo 1 conceptualization methodology writing original draft preparation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was performed within the framework of the following projects 1 aquaclew which is part of era4cs an era net initiative by jpi climate and funded by formas se dlr de bmwfw at ifd dk mineco es anr fr with co funding by the european commission grant 690462 and 2 flooding and erosion works in coastal areas of andalusia under a climate change scenario funded by the ministry of agriculture livestock fisheries and sustainable development of the junta de andalucía contrat no contr 2018 66984 part of this study has been conducted using e u copernicus marine service information funding for open access charge universidad de granada cbua appendix a some useful information about the requirements installation and computing times can be found in the present appendix the model was developed in a virtual environment which packages can be installed from a requirements file github the working area can be easily and quickly raising up just using those packages the computational time mainly depends on the number of parameters to be optimized which at the same time depends on the number of gfs or polynomial time expansion and the number of pms so more than three pms are not recommended due to the computational time increases exponentially table 4 description of models and parameters for the analysis of the present paper the computational time was obtained with a user personal computer intel r core tm i7 9700f cpu 3 00 ghz and 8 cores in those cases the weights represented a threshold of the transition between pms and not the weight of the pm the optimization method more frequently used by authors is slsqp due to the high efficiency however in some cases a high cost computational method will be required for ensuring convergency a means that a reduction of the number of parameters involved is applied following solari and losada 2011 column names are described in section 2 table 4 var section circular transform gfs noterms ps ws fun computational time s q d 3 1 false yes sinusoidal 20 weibull of maxima 11 75 u 3 2 false no trigonometric 8 norm 21 54 θ u 3 2 true no trigonometric 8 weibull of maxima 14 25 h s 3 3 false no trigonometric 4 0 05 0 85 generalized pareto lognorm generalized pareto a 1854 24 t p 3 3 false no trigonometric 4 0 05 0 85 generalized pareto norm generalized pareto a 1766 18 θ m 3 3 true no trigonometric 4 0 49 0 61 0 39 0 51 truncated norm truncated norm 652 87 v v 3 3 false no trigonometric 4 0 05 0 85 generalized pareto gamma generalized pareto a 2759 81 θ v 3 3 true no trigonometric 4 0 42 0 48 0 52 0 58 truncated norm truncated norm 714 97 the use of marinetools temporal requires a basic understanding of the python programming language as well as of functional code design in particular guidelines for an easy installation of the required packages of marinetools temporal will be found in github repository some information about the properties of pms and methods using in the examples of section 3 are described in table 4 appendix b in this section the main formulas of the methodology are given b 1 marginal distribution the marginal distribution may have any of the following structures b 1 1 piecewise distributions the random variable x probability density function f x x is expressed as a piecewise function defined over a partition of the real axes into n y subintervals u α 1 u α for α 2 n i 1 i 1 u 1 and i n i u n i 1 1 f x x k 1 f 1 x x u 1 k 2 f 2 x u 1 x u 2 k α f α x u α 1 x u α k n i f n i x u n i 1 x in eq 1 f α denotes the probability density function pdf of the model selected for i α the values of the coefficients are 2 k α a 1 b 1 a α 1 b α 1 c 1 α 2 n i 1 c α a 1 b 1 a 2 b 2 a α 1 b α 1 1 where a α f α u α b α f α 1 u α and c α f α u α f α 1 u α 1 provided that b α and the denominator in eq 2 are both different from zero the corresponding dictionary must include the names to identify the n i pms selected key fun and the initial guesses of the values of the percentiles of the common endpoints in key ws ps that is of p α f 1 u α for α 1 n i 1 b 1 2 compound distribution the pdf of the random variable x is expressed as a weighted sum of n i different pdfs 3 f x x α 1 n f ω α f α x where w 1 w α w i 1 the dictionary of x must include the names to identify the n i pms selected key fun and the initial guesses of values of the percentiles of the n i 1 common endpoints in key ws ps that is ω α for α 1 n i 1 b 2 non stationarity if the distribution of x is non stationary the parameters of the pms models given in key fun can be approximated over the interval 0 n y by a truncated generalized fourier series if a t is any of those parameters its expression is given by 4 a t n 1 n i a n φ n t 0 n y where a n denotes any of the parameters and φ n t n is the set of basis functions b 3 temporal dependence the vector auto regressive var q model is applied to the following normalized time series 5 z x i t j φ 1 f x i x i o t j t j where φ 1 is the inverse of the gaussian cumulative distribution function with zero mean and unit standard deviation and f x i x i t t is the ns probability distribution function of x i denoting the values of the normalized series eq 5 at time t j as y j i z x i t j and y j y j 1 y j i y j n t where t stands for the vector transposition the dependence in time between variables in the var q model is given by 6 y j c a 1 y j 1 a 2 y j 2 a q y j q e j where c c 1 c i c n t contains the mean values of the variables a m m 1 q are the n n coefficients matrices and e j e j 1 e j i e j n t is the vector with the white noise error terms using eq 6 to relate data at an instant t j to their previous q values for j q 1 n o we obtain y aχ e where y y q 1 y q 2 y n χ χ q 1 χ q 2 χ n with χ j 1 y j 1 t y j q t t a a 1 a 2 a q and e e q 1 e q 2 e n the solution is obtained by means of minimum least square errors as a y χ t χ χ t 1 where e y aχ and q cov e is the covariance matrix of the error a detailed description can be found e g in lütkepohl 2005 
25631,the assessment of the uncertainty about the evolution of complex processes usually requires different realizations consisting of multivariate temporal signals of environmental data however it is common to have only one observational set marinetools temporal is an open source python package for the non stationary parametric statistical analysis of vector random processes suitable for environmental and earth modelling it takes a single timeseries of observations and allows the simulation of many time series with the same probabilistic behavior the software generalizes the use of piecewise and compound distributions with any number of arbitrary continuous distributions the code contains among others multi model negative log likely functions wrapped normal distributions and generalized fourier timeseries expansion its programming philosophy significantly improves the computing time and makes it compatible with future extensions of scipy stats we apply it to the analysis of freshwater river discharge water currents and the simulation of ensemble projections of sea waves to show its capabilities keywords time expansion of parameters non stationary probability models stochastic characterization environmental modelling 1 introduction in environmental and engineering sciences the understanding and quantification of the evolution of processes are highly related to the capacity to measure the variables involved and the ability to build up models capable to reproduce their interrelationships moges et al 2021 some processes can be modeled as systems that respond to a forcing usually represented by one or several input time series if the driving agent has a random character as with climate forced processes it may be interesting to stochastically characterize the response however for a given measured or projected time series the model just provides a single realization of the output most processes are complex enough so that their statistical nature cannot be directly inferred from the joint distributions of the input variables under such circumstances simulation techniques can be used to obtain a large number of input realizations whose responses constitute a sample of the output from that sample it is possible to assess the uncertainty that the response inherits from the forcing see e g baquerizo and losada 2008 meier et al 2019 the study of this type of simulation techniques is nowadays a hot topic in climate driven processes refsgaard et al 2007 and climate change projections kundzewicz et al 2018 ellis et al 2021 uhe et al 2021 one of approaches consists in the stochastic characterization of a vector random process vrp coupling several probability models pms which parameters are assumed non stationary ns with a vector autoregressive model var which characterize the multivariate and temporal dependency see solari and van gelder 2011 solari and losada 2011 among others the theoretical applicability of the later approach has been illustrated in works where specific subsets of probability models were analyzed independently for different purposes among others i the observed wave climate variability in the preceding century and expected changes in projections under a climate change scenario lira loarca et al 2021 ii the optimal design and management of an oscillating water column system jalón et al 2016 lópez ruiz et al 2018 iii the planning of maintenance strategies of coastal structures lira loarca et al 2020 iv the analysis of monthly wolf sunspot number over a 22 year period cobos et al 2022 and v the simulation of estuarine water conditions for the management of the estuary cobos 2020 we follow a novel approach in marinetools temporal a package included in marinetools which is a python framework that integrates software that can be used in the search for solutions to real engineering and marine problems this temporal package is to the best of the authors knowledge the first open source and free available general tool aimed at providing users with a friendly general code to statistically characterize vector random processes vrp and to obtain realizations of them it generalizes the above mentioned approaches for vrps whose components are ns and have piecewise or compound distributions defined by means of any number of arbitrary continuous distributions its programming philosophy widely used in python significantly improves the computing time and allows the use of any probability distributions included in future extensions of scipy stats the most widely used python statistical package it is implemented in python an interpreted high level object oriented programming language widely used in the scientific community and it makes the most of the python packages ecosystem among the existing python packages it uses numpy which is the fundamental package for scientific computing in python harris et al 2020 scipy which offers a wide range of optimization and statistics routines virtanen et al 2020 matplotlib hunter 2007 that includes routines to obtain high quality graphics and pandas mckinney et al 2010 to analyze and manipulate data in addition to third party packages marinetools temporal also contains among others novel routines for the required negative log likely functions the generalized fourier time series expansion the use of wrapped normal distributions the multi model ensemble approach for climate projections and a whole set of plotting functions for the non stationary representation these examples use time series commonly used in coastal engineering sea waves significant wave height peak period and mean direction oceanography sea surface temperature and currents at open sea hydrology river discharge and astronomy sunspots number their choice is aimed at representing all the capabilities including i the analysis of circular variables ii the treatment of times series with persistent low values and iii the analysis of multi model climate projections in addition four more examples that enlarge even more the applicability of the software are included in the repository the paper is divided into five sections after the introduction the theoretical basis for fitting the parameters of the ns probability models pms the description of the temporal and multivariate dependence the time series simulation and the analysis of multi model climate data are briefly presented in section 2 some applications to illustrate the use of marinetools temporal are presented in section 3 the discussion about the algorithms some hints about its use and expected upgrades is given in section 4 and finally section 5 concludes the study 2 theoretical background and definition of the parameters this tool is aimed at 1 characterizing a vector random process x x 1 t x i t x n t that can be uni or multivariate where t belongs to a set of index on the basis of an observation of it at n o discrete points x o t j x 1 o t j x i o t j x n o t j for j 1 n o and 2 simulating other random realizations that have the same joint probabilistic behavior than the first one for the sake of simplicity from now on we will speak about temporal time series and assume that the rps are measured at equally spaced instants the characterization includes 1a the fit of the marginal ns distribution functions of each random variable x i and 1b the description of the dependence of the value of each component at time t j with previous values of all the components through a vectorial autoregressive model the tool can also deal with multi model time series coming from different combinations of global and regional climate models gcm rcm through the use of compound marginal variables and an ensemble averaged var model as proposed by lira loarca et al 2021 the time series simulation is based on those results it first generates a stationary multivariate time series of non exceedance probabilities and then it recovers the values of each variable by using the inverse of the non stationary cumulative distribution function fig 1 shows the flow chart of the algorithm the cascade analysis consists of several steps being the initial inputs the observed or hindcasted data x o t j j 1 n o and the dictionary that contain the key parameters that provide the probability structure of the rps the information regarding the var model and the simulation the tool has a simple syntax based on dictionaries see table 1 and it helps users to supply the input information particularly for step 1a which requires some knowledge about the phenomena under study at the end of each step the results of the analysis are saved to a file whose name and some information regarding the modelling process are appended to the input dictionary in the following a brief description of the methodology implemented in marinetools temporal is presented and at the same time the options and values of the parameters that represent the relation between mathematical and statistical methods used in the tool are commented the interested reader is referred to cobos et al 2022 lira loarca et al 2021 2020 egüen et al 2016 solari and losada 2011 solari and van gelder 2011 among others where several examples of its application can be found the description values and examples of the key variables for the analysis and simulation tool are defined in table 1 the identifiers mf td ts and ea given in the 1st column stand for the marginal fit the temporal dependence analysis the time series simulation and the ensemble average multimodel analysis of climate projections respectively 2 1 fit to marginal probability models mf options each random component x i is fitted to a marginal distribution f x i x i with the maximum likelihood method the variables that describe the angles of vector magnitudes such as the wind or wave direction require a special analysis the model allows to indicate that a variable is circular by including the key circular with the value true among others circular pms like von mises mardia and jupp 2009 included in scipy can be used the wrapped normal pewsey 2000 that was implemented ad hoc by following the philosophy of scipy package is also available for some variables such as the precipitation or the fresh water river discharge in water scarce areas the presence of abrupt changes in time makes difficult the fitting of multiple pms for those cases it is suggested to apply a parametric and monotonic transformation to deal with gaussian distributed data for that purpose the methods included in marinetools temporal are those proposed by box and cox 1964 and yeo and johnson 2000 this option can be set up by adding a dictionary with the key transform with a set of keys the first one make indicates that the transformation will be done if it is equal to true the model key indicates the selected transformation method items box cox or yeo johnson the users can also indicate to the tool whether they want to plot the cdf of the normalized data key plot equal to true 2 1 1 structure of the marginal cdfs the distribution can be constructed from multiple pms so that it can properly represent the non exceedance probability of all the plausible values including the tails and the body see appendix b 1 the distributions are defined either by a piecewise function key piecewise equal to true see cobos et al 2022 or by a mixture or compound distribution key piecewise equal to false the first option also contemplates the use of a single distribution in any of these choices the functions involved are specified in key fun for a piecewise distribution with more than one pm the user has to provide in key ws ps the initial guesses for the common endpoints percentiles of the intervals in which the real axes has been divided if a compound distribution is selected the values provided in ws ps are the first guesses of the weights of the first pm s the weight of the last pm is derived from the others so that their sum equals 1 2 1 2 choice of the stationary or ns character of the analysis the analysis can be stationary which means that the parameters of the pms i e shape location or scale do not vary in time otherwise they are assumed to vary over a given time interval with a duration of n y years and therefore the distribution function f x i x i t t is non stationary the choice is done by setting the option ns analysis to false or true respectively in that regard if the ns analysis is chosen the parameters are expanded into a generalized fourier series gfs over the interval 0 n y which is truncated to n f terms see appendix b 2 the basis functions are chosen by setting up basis function with one of the choices provided in table 2 and by indicating the number of terms to be retained in the series key no terms equal to an integer number if the basis function contains periodic functions it is recommended to perform a previous spectral or harmonic analysis of the time series in order to gain knowledge about the oscillatory components the tool selects by default n y 1 which allows to analyze time variations up to the yearly scale in some time series longer variations associated to climatic indexes oscillations can be found le mouël et al 2019 the tool offers an option to modify the basis period to n y years which allows the oscillations to be analyzed on a different see cobos et al 2022 with the choices provided by the user in the input dictionary for variable x i the tool internally computes an initial guess of all the parameters involved in the definition of the pms and the coefficients of the gfs and it finds the optimum values of these coefficients and the values provided in key ws ps the tool also shows some informative messages and includes some guidance plots to help in the selection of the marginal fit properties the package also contains several functions to assess the goodness of the fit of the analysis and the simulations some of these functions have been used to present the results of the study cases in section 3 2 2 multivariate and temporal dependency of the rps td options once the marginal probability structure of the random variables has been obtained the temporal multivariate or univariate dependency might be inferred by using autoregressive methods in this version the models available are the uni and multivariate autoregressive ar q and var q respectively that assume a linear relationship between the value of the rps at a given time and their past q values other possibilities such as arma arimax dar egarch gas or gaussian local level can be easily implemented with python functions included in pyflux package the information related to the temporal dependence is provided in a dictionary where the names of the variables to be jointly analyzed are given in key vars and the method selected in method starting from the first order the model tests all the orders up to the maximum value q indicated in order and provides the information from the one with the smaller value of the bayesian information criterion bic it is worth noting that although in the marginal fitting of every rp the time series can contain small gaps in time for the assessment of the temporal univariate or multivariate dependency the observations need to have the same resolution table 1 with id td contains a description of the possibilities that the tool offers for multivariate temporal dependency analysis their values syntax and examples 2 3 the simulation process ts options the simulation process is done by retrieving the information from dictionaries of the marginal fit multivariate and temporal dependency analysis and if selected the multi model ensemble average in addition the tool requires the starting and ending dates of the simulation period keys start and end in the standard datetime format of python and the number of simulations key nosim then it writes in different files every random realization of the univariate or multivariate rp as output the same temporal time step than the original dataset is used in order to guarantee the same probabilistic behavior 2 4 characterization of ensemble average multi model climate projections ea options climate services provide several climate projections from different combinations of gcm rcm climate models under a certain greenhouse gas concentration trajectory associated to a representative concentration pathway rcp adopted by the ipcc lira loarca et al 2021 proposed to gather the information of different gcm rcms through the use of a compound distribution that weights the ns marginal distributions and their var q coefficients to obtain a multi model ensemble average characterization of the multivariate random processes the package includes an option key ensemble equal to true to work with the multi model ensemble averages using equal weights a choice corresponding to the commonly used rule one model one vote recommended by the ipcc pörtner et al 2019 or the values provided in key weights to do so the marginal fit analysis and the temporal multivariate dependence analysis need to be done previously for all the models as explained in sections 2 1 and 2 2 the tool provides several files with the non exceedance probabilities of the compound variables and the corresponding var coefficients 3 applications to earth and environmental modelling in the following subsections we illustrate the capabilities of marinetools temporal with some novel applications for the analysis of environmental vrps more precisely the examples include i the univariate pm analysis of projections of freshwater river discharge at alcalá del río dam spain ii the multivariate analysis and temporal dependency of velocity currents at the strait of gibraltar atlantic ocean furthermore iii the simulation of new projections from marine climate data significant wave height peak period and incoming mean direction wind velocity and incoming mean direction at the alborán sea at the western mediterranean sea under the rcp8 5 scenario from the ensemble multivariate multi model information lira loarca et al 2021 we present a detailed description of the marinetools temporal framework methods and how they operate together to capture the non stationarity and to create simulations of some general examples small pieces of code will be shown next to the exemplifying dictionaries to run the analysis these applications and those described in table 3 have been selected in order to illustrate a set of representative options that the tool integrates however many other types of vrps can also be considered the documentation is available at the github gdfa ugr repository it includes the code case studies and the examples of table 3 included as jupiter notebook files this will allow interested readers to reproduce the applications with similar synthetic data sets and to facilitate its adaptation to other time series 3 1 freshwater river discharge projection at alcalá del río dam this first example is focused on a univariate time series denoted by q d t of the projection of daily fresh water river discharge from the alcalá del río dam to the guadalquivir river estuary 37 29 n 6 06 w from january 1st of 2020 to december 31st of 2040 this series are obtained from the hydrological predictions from the environment model hype forced with the atmospheric model remo 2009 for the rcp 2 6 scenario source swedish meteorological and hydrological institute smhi due to the strong regulation of the river at this dam the last one in the river course before its flow into the atlantic ocean the series shows low values in summer q d 40 m3 s that are almost squared in winter q d 1000 m3 s to deal with the high variability between seasons a box cox transformation with λ 0 0796 parameter is used the properties of the marginal fit are given in table 4 a weibull of maxima model was selected the highly temporal variability and the clear seasonal behavior lead to the sinusoidal temporal expansion over n y 1 year retaining n f 10 oscillatory terms covering frequencies up to 10 yr 1 the following code shows the python dictionary with the input information required for the marginal fit analysis in the present example image 1 with the previous dictionary params the dataset as a pandas dataframe data and importing the package analysis as from marinetools temporal import analysis the marginal fit function is invoked just coding analysis marginalfit data params the results will be saved in a file in a new folder called marginalfit fig 2 shows the empirical and theoretical fitted ns cdf the empirical ns cdf was computed by using a window size of 14 days which is large enough to obtain representative values of the empirical percentiles but not so long that lower significant variations are neglected in the authors experience for climatic variables with this temporal resolution a window length of 14 days is appropriate for time series longer than 20 years as it is observed in fig 2 the ns pm adequately reproduces the non stationary behavior during the year the theoretical pm captures the overall seasonal behavior during almost all the year for all the percentiles with a marked valley during summer and peaks in the previous and subsequent seasons only some deviations from the peaky behavior are observed during february march and at the end of november for the highest represented percentile 0 99 which is slightly underestimated 3 2 currents at the strait of gibraltar the second example analyzes the multivariate time series of the water current field mean current velocity u and mean incident current direction θ u hindcasted at 0 5058 m below the mean sea level at a point located in 35 9166 n 5 5 w at the strait of gibraltar data provided by marine copernicus system the hindcast time series has 27 years duration with data that spans from 1993 01 01 to 2019 12 31 with a daily temporal cadence the ibi iberian biscay irish ocean reanalysis system provides 3d ocean fields product identifier ibi multiyear phy 005 002 the ibi model numerical core is based on the nemo v3 6 ocean general circulation model run at 1 12 horizontal resolution the marginal fits were carried out with the characteristics given in table 4 briefly the univariate analysis of u was carried out using a gaussian pm the water current incident direction was fitted using a weibull of maxima pm for both variables the trigonometric fourier expansion was performed over a basis period of one year n y 1 with eight oscillatory components n f 8 fig 3 a shows the empirical and theoretical models of the ns cdf for u as it is observed the gaussian model fairly reproduces the temporal variation of the probability distribution function fig 3 b shows the same analysis with θ u a marked eastwards flow around 270 is observed showing that the mean currents at that location of the strait of gibraltar and at the selected depth flow into the mediterranean sea which is in accordance with observations of the water masses exchanges between the atlantic ocean and the mediterranean sea see e g sverdrup and fleming 1942 a weibull of maxima reproduced also fairly well the probability structure in time slightly overestimating the water current directions at the highest percentiles once the parameters of the marginal distributions were obtained the multivariate and temporal analysis were carried out the maximum q order analyzed of the var model was 72 these properties are set up in the dictionary as image 2 the following code is required to load the json file with the parameters from the preceding marginal fits from marinetools auxiliar import read params u read rjson filename u params diru read rjson filename diru given the dictionaries with the results from the marginal fit and the properties of the temporal and multivariate analysis the var model is applied by coding df dt analysis dependencies data params the results are saved to a file in a new folder called dependency among all the orders analyzed the one with the minimum bic was q 6 as it is observed in fig 4 the pattern of the joint density function of hindcast data u and θ u panel a is well reproduced by one of the random simulations panel b the simulation slightly reduces the probability of the modal bump yellow pixels at 275 and 0 4 m s approx 3 3 climate projections using ensemble average multi model ns distributions at the alborán sea finally a multivariate analysis for wave and wind climate projections at the alborán sea 3 608 w 36 66 n is presented from that purpose we used hourly data from the rcp 8 5 scenario and projections of the following global and regional climate models gcm rcm cclm4 8 17 miroc5 rca4 cnrm cm5 rca4 ec earth rca4 hadgem2 es rca4 ipsl cm5a mr and rca4 mpi esm lr ih 2019 the characteristics of these gcm rcm are described in pérez et al 2017 it comprises the significant wave height h s the peak wave period t p the mean incident wave direction θ m the wind velocity v w and the mean wind incoming direction θ w during an interval that spans from 2025 02 01 to 2046 01 01 source ih cantabria the ensemble mean properties were computed using equal weights a choice that represents the rule of one model one vote recommended by the ipcc pörtner et al 2019 the options selected for the analysis and the simulation of five realizations of 20 years are shown next image 3 image 4 image 5 with the previous dictionary params of the average multi model gcm rcm and importing the simulation package with from marinetools temporal import simulation the simulations are achieved by coding simulation simulation params the simulated time series are saved into files in a new folder called simulations for each gcm rcm i the variables were fitted using combinations of pms as fig 5 shows which properties are summarized in table 4 ii the temporal dependency was computed for q orders that range from 26 to 38 h the one corresponding to 26 h had the lowest bic which means that h s t p θ m v v θ w are strongly related with the previous 26 sea states this value is consistent with the one obtained by lira loarca et al 2021 using hindcast data and projections from the meteocean group of the university of genoa in the following the probabilistic characterization of one of the 20 years simulations and the obtained ensemble mean multi model are compared panels a and b of fig 6 show the joint cdfs for the simulated and ensemble averaged projections for h s t p and h s v v respectively it can be observed that the degree of agreement between the simulation and the hindcasted data is generally good which demonstrate that the simulation maintains the joint dependency moreover the r 2 coefficients between data of the joint probability density functions pdf of h s t p h s θ m h s v v and v v θ w of the simulation and the multi model ensemble were for all cases greater than 0 97 fig 7 shows the wind roses at alborán sea for ensemble mean data projections and one random simulation the tool enables the creation of several subplots and the chance to include into them any plot function available in the graphics package matplotlib a proper way to verify the temporal dependency of the simulation and the input data is by computing the autocorrelation function papoulis and saunders 1989 fig 8 depicts the autocorrelation of h s t p θ m v v and θ v for the simulation solid lines the shadow areas delimit the values obtained for the autocorrelation functions for all the rcms as it is observed the simulation and observations show a similar variability with values ranging between 0 7 and 1 the wind field shows a smaller autocorrelation than the rcms during 5 20 h lag also the autocorrelation of θ m is underestimated for all the lags this difficulty of reproducing directional variables already pointed out by monbet et al 2007 is clearly seen in this dataset that correspond to the gulf of cádiz a place where abrupt wind changes are experienced it should be highlighted however that the difference is lower than 1 showing autocorrelation values larger than 0 91 in all cases the results point out that the multivariate and the temporal dependency is also well reproduced 4 discussion the goal of marinetools temporal is the search for the optimum values for any combination of pms and several basis functions actually most of the computational time is spent on the estimation of the optimal parameters of each marginal ns distribution therefore while it is not critic an appropriate selection of the pms will certainly reduce the computational time a suitable combination of pms will usually ensure the convergence of the optimization process however in some cases a slight modification of the weights or percentiles of the common endpoints should be required the tool incorporates several sanity checks to provide a user friendly experience working with ns pms the choice of the basis function is also relevant for the optimization as some basis functions give a better fit with a considerable smaller amount of terms this is the case of the sinusoidal functions that usually requires about the half of terms in the series than the trigonometric of the modified fourier expansion the input time series for the marginal fits can have small gaps since to the search for the optimum value of the nllf does not require to have a regularly observed time series however for the temporal dependence analysis concomitant time series observed with a fixed time step are required the model has been coded to deal with any basis period the examples included for the analysis are done with one year and multiples of it because at temperate latitudes the yearly periodicity is predominant in climate data when long time series are available as it is the case for the sunspot number many years can be considered to capture longer time scale variations it is worth noting that when the basis period is larger than 1 year the initial date and time of the simulations have to be selected carefully in order to avoid an artificial shift of the oscillatory variability marinetools temporal has just included one of the temporal dependency models ar and var however there are many other possibilities in pyflux for python to characterize these relationships such as arma arimax dar egarch gas gaussian local level for independent variables the simulations will show no correlation in multivariate analysis so it is recommended in those cases an independently simulation of each variable the framework marinetools temporal gives a step forward in the need of bridge the gap between the current software development and the coastal and marine engineering practice magaña et al 2020 including advances not only in multivariate analysis which approach to marine design is not now only available for academic use as jonathan and ewans 2013 indicated the tool has been applied to the examples given in section 3 and table 3 which include timeseries coming from coastal engineering sea wave height peak period and mean direction oceanography sea surface temperature and currents at open sea and hydrology river discharge as proof of evidence that continuous rvps can be adequately analyzed and simulated the tool was also applied to analyze and simulate raining patterns results not shown additionally the use of new simulations generated within the marinetools framework might reveal behaviours that emerge from the intrinsic nature of the vector random process analyzed and derived processes which cannot be previously capture and analyze by using one realization which is paramount for structure design and environmental planning 5 conclusions marinetools is an open source project hosted on github the temporal package is dedicated to the analysis of stationary and ns rps and the simulation of earth and environmental data with the same probabilistic behaviour all scripts discussed in the present paper and the synthetic data files are in the repository further examples given as jupyter notebooks covering the full use of marinetools temporal are also available on github the present paper shows the options included in marinetools temporal together with the examples marginal fit of freshwater river discharge from a dam multivariate and temporal dependency of water currents at the ocean and multivariate simulations of sea and wind climate states it is demonstrated how the optimization of the nllf for ns pms succeeds for several combinations of the pms included in scipy stats and the wrap normal three main scripts which include the pre processing steps marginal fit multivariate and temporal analysis ensemble average multi model gcm rcm projections and finally the simulation of earth and environmental time series are presented in detail the use of marinetools temporal offers a very general framework which can be successfully applied to a wide variety of environmental and engineering problems software availability name of software marinetools developer environmental fluid dynamics group university of granada contact information mcobosb ugr es year first available 2021 software required specified at the github repository program language python program size 58 6 kb cost open source tools released under the gnu general public license v3 0 repository credit authors contribution statement m cobos 1 conceptualization methodology software writing original draft preparation p otiñar 1 writing draft preparation p magaña 1 data curation software a lira loarca 2 data curation software methodology a baquerizo 1 conceptualization methodology writing original draft preparation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was performed within the framework of the following projects 1 aquaclew which is part of era4cs an era net initiative by jpi climate and funded by formas se dlr de bmwfw at ifd dk mineco es anr fr with co funding by the european commission grant 690462 and 2 flooding and erosion works in coastal areas of andalusia under a climate change scenario funded by the ministry of agriculture livestock fisheries and sustainable development of the junta de andalucía contrat no contr 2018 66984 part of this study has been conducted using e u copernicus marine service information funding for open access charge universidad de granada cbua appendix a some useful information about the requirements installation and computing times can be found in the present appendix the model was developed in a virtual environment which packages can be installed from a requirements file github the working area can be easily and quickly raising up just using those packages the computational time mainly depends on the number of parameters to be optimized which at the same time depends on the number of gfs or polynomial time expansion and the number of pms so more than three pms are not recommended due to the computational time increases exponentially table 4 description of models and parameters for the analysis of the present paper the computational time was obtained with a user personal computer intel r core tm i7 9700f cpu 3 00 ghz and 8 cores in those cases the weights represented a threshold of the transition between pms and not the weight of the pm the optimization method more frequently used by authors is slsqp due to the high efficiency however in some cases a high cost computational method will be required for ensuring convergency a means that a reduction of the number of parameters involved is applied following solari and losada 2011 column names are described in section 2 table 4 var section circular transform gfs noterms ps ws fun computational time s q d 3 1 false yes sinusoidal 20 weibull of maxima 11 75 u 3 2 false no trigonometric 8 norm 21 54 θ u 3 2 true no trigonometric 8 weibull of maxima 14 25 h s 3 3 false no trigonometric 4 0 05 0 85 generalized pareto lognorm generalized pareto a 1854 24 t p 3 3 false no trigonometric 4 0 05 0 85 generalized pareto norm generalized pareto a 1766 18 θ m 3 3 true no trigonometric 4 0 49 0 61 0 39 0 51 truncated norm truncated norm 652 87 v v 3 3 false no trigonometric 4 0 05 0 85 generalized pareto gamma generalized pareto a 2759 81 θ v 3 3 true no trigonometric 4 0 42 0 48 0 52 0 58 truncated norm truncated norm 714 97 the use of marinetools temporal requires a basic understanding of the python programming language as well as of functional code design in particular guidelines for an easy installation of the required packages of marinetools temporal will be found in github repository some information about the properties of pms and methods using in the examples of section 3 are described in table 4 appendix b in this section the main formulas of the methodology are given b 1 marginal distribution the marginal distribution may have any of the following structures b 1 1 piecewise distributions the random variable x probability density function f x x is expressed as a piecewise function defined over a partition of the real axes into n y subintervals u α 1 u α for α 2 n i 1 i 1 u 1 and i n i u n i 1 1 f x x k 1 f 1 x x u 1 k 2 f 2 x u 1 x u 2 k α f α x u α 1 x u α k n i f n i x u n i 1 x in eq 1 f α denotes the probability density function pdf of the model selected for i α the values of the coefficients are 2 k α a 1 b 1 a α 1 b α 1 c 1 α 2 n i 1 c α a 1 b 1 a 2 b 2 a α 1 b α 1 1 where a α f α u α b α f α 1 u α and c α f α u α f α 1 u α 1 provided that b α and the denominator in eq 2 are both different from zero the corresponding dictionary must include the names to identify the n i pms selected key fun and the initial guesses of the values of the percentiles of the common endpoints in key ws ps that is of p α f 1 u α for α 1 n i 1 b 1 2 compound distribution the pdf of the random variable x is expressed as a weighted sum of n i different pdfs 3 f x x α 1 n f ω α f α x where w 1 w α w i 1 the dictionary of x must include the names to identify the n i pms selected key fun and the initial guesses of values of the percentiles of the n i 1 common endpoints in key ws ps that is ω α for α 1 n i 1 b 2 non stationarity if the distribution of x is non stationary the parameters of the pms models given in key fun can be approximated over the interval 0 n y by a truncated generalized fourier series if a t is any of those parameters its expression is given by 4 a t n 1 n i a n φ n t 0 n y where a n denotes any of the parameters and φ n t n is the set of basis functions b 3 temporal dependence the vector auto regressive var q model is applied to the following normalized time series 5 z x i t j φ 1 f x i x i o t j t j where φ 1 is the inverse of the gaussian cumulative distribution function with zero mean and unit standard deviation and f x i x i t t is the ns probability distribution function of x i denoting the values of the normalized series eq 5 at time t j as y j i z x i t j and y j y j 1 y j i y j n t where t stands for the vector transposition the dependence in time between variables in the var q model is given by 6 y j c a 1 y j 1 a 2 y j 2 a q y j q e j where c c 1 c i c n t contains the mean values of the variables a m m 1 q are the n n coefficients matrices and e j e j 1 e j i e j n t is the vector with the white noise error terms using eq 6 to relate data at an instant t j to their previous q values for j q 1 n o we obtain y aχ e where y y q 1 y q 2 y n χ χ q 1 χ q 2 χ n with χ j 1 y j 1 t y j q t t a a 1 a 2 a q and e e q 1 e q 2 e n the solution is obtained by means of minimum least square errors as a y χ t χ χ t 1 where e y aχ and q cov e is the covariance matrix of the error a detailed description can be found e g in lütkepohl 2005 
25632,machine learning is a dynamic field with wide ranging applications including drought modeling and forecasting drought is a complex devastating natural disaster for which it is challenging to develop effective prediction models therefore our review focuses on basic information about machine learning methods mlms and their potential applications in developing efficient and effective drought forecasting models we observed that mlms have achieved significant advances in the robustness effectiveness and accuracy of the algorithms for drought modelling in recent years the performance comparison of mlms with other models provides a comprehensive conception of different model evaluation metrics further challenges of mlms such as inadequate training data sets noise outliers and observation bias for spatial data sets are explored finally our review conveys in depth understanding to researchers on machine learning applications in forecasting and modeling and provides drought mitigation strategy guidance for policymakers keywords machine learning deep learning forecasting drought big data 1 introduction recently machine learning ml has attracted attention from the scientific community for research in various fields such as engineering agriculture medicine marketing and earth and environmental sciences in particular high computational power and speed available sensor data and advances in big data analysis have driven ml to influence many aspects of research ml data science and artificial intelligence have become interchangeable terms in research educational and industrial applications of technology ml has made a significant contribution to sustainable environmental development through disaster risk management deparday et al 2019 drought forecasting using stochastic models considers only the present and past value of the same variable ignoring the effects of other variables a probabilistic model with bivariate and multivariate analysis for drought forecasting creates a problem when fitting the data distribution because of marginal distribution functions recently ml marj and meijerink 2011 belayneh et al 2014 2016 belayneh and adamowski 2013 kousari et al 2017 park et al 2016 deo and shahin 2016 and hybrid models jalalkamali et al 2015 shirmohammadi et al 2013 belayneh et al 2016 have been used as alternative methods for statistical li et al 2015 zhang et al 2019 time series chen et al 2012 belayneh et al 2014 barua et al 2012 and probabilistic models khadr 2016 chen et al 2016 nnaji et al 2016 hao et al 2016 and have shown high capabilities in nonlinear and dynamic time series drought modeling and forecasting drought is typically recognized as an attribute of nonlinearity and unstableness hao et al 2015 therefore stochastic models are not able to accurately grasp the characteristics of meteorological and hydrological data for drought monitoring wei et al 2012 in addition probabilistic models are computationally expensive to deal complex and heterogeneous data sets fung et al 2019 however mlms have the power of self organizing and self adaptive function with the nonlinear property are capable of modeling and forecasting hydrological and meteorological data for drought characterization liu et al 2018 drought is an inherently striking event induced by the climatological factors topography and water demand of a specific region ndcm 2014 drought is characterized as a complex natural hazard owing to the intensity severity and uncontrollable nature of this phenomena like other natural disasters drought leads to economic and property losses creates environmental and ecological imbalances and potentially threatens social losses vafakhah et al 2014 considering that drought is a complex and devastating natural hazard drought monitoring is a significant issue while other natural hazards such as floods earthquakes and cyclones are short term events drought can persist for years thus proper drought monitoring and forecasting that include meteorological and remote sensing data are required for planning and decision making however effective mitigation planning and adaptation strategy development primarily depend on the efficiency of the drought forecasting model or method in this context ml provides more accurate and efficient drought forecasting and can be applied in drought disaster risk management wilhite 2012 mallick et al 2013 there have been several review studies related to ml that either describe the fundamental concept of ml and forecasting methods holloway et al 2018 makridakis et al 2018 kersting 2018 or provide a detailed review on flood disaster mosavi et al 2018 and water management sun and scanlon 2019 review studies on drought disaster include some studies on modeling drought forecasting for example the review by fung et al 2019 primarily focused on different drought prediction models paying no attention to in depth applications of ml another review by mishara and singh 2011 focused on different models including probabilistic modeling global climate models and spatio temporal analysis for drought scenarios without addressing the field of ml presently there has been no organized and inclusive survey on ml related to drought disaster monitoring and forecasting therefore quantitative analysis is an urgent issue related to sustainable disaster management particularly for applying ml to drought monitoring and forecasting our review aims to explore the function and performance of ml algorithms that have been used for modeling and forecasting drought as well as challenges in disaster remote sensing the basic concept and development of ml and deep learning dl algorithms has been presented in the supplementary materials therefore in the following sections we first review the present use of ml in different types of drought studies next we review the potential of ml in forecasting drought over statistical time series and probabilistic modeling and also the factors that have hindered the performance of ml until now finally we discuss the challenges of using ml for drought forecasting in a geospatial context we expect our review to provide insight into the multidimensional nature of ml and facilitate information and learning exchanges among scientists and policymakers for better drought disaster management 2 mlm for environmental remote sensing with particular reference to drought monitoring and forecasting environmental monitoring of earth with various satellite and space born platforms provides versatile measurement of phenomena in the biosphere hydrosphere and atmosphere remote sensing is more convenient and practical than traditional weather station and statistical data gathering methods for various regional applications such as vegetation condition monitoring drought detection and prediction and crop yield measurement bhushan 2017 dalezios et al 2017 presently disaster management activities such as drought monitoring with various remote sensing satellites have increased the capability and reliability of drought quantification methodologies on a global scale in this context satellite sensors such as noah avhrr modis spot vegetation meris and sentinel from esa provide real time high resolution data for assessing vegetation cover soil moisture crop condition and water body estimation for lakes rivers and reservoirs these indicators provide a basic understanding of drought conditions in a given region dalezios et al 2017 furthermore satellite based observations and ground based rain gauge data are used for rainfall anomaly estimation which represents meteorological drought moreover vegetation indices i e ndvi vci tci and vhi derived from satellite data are useful for evaluating vegetation stress periods which can indicate agricultural drought drought occurs when there is a shortage of water for a substantially extended period over a large area and drought intensity exhibits significant spatiotemporal variation owing to the phenomenon s regionality thus understanding the spatial analysis of drought distribution is essential for effective management of surface and ground water yu et al 2014 bhushan 2017 drought affects a wide variety of sectors on the earth surface making it difficult to define universally considering drought hazards and recurrence in various areas drought may be classified as follows i meteorological drought usually illustrated by precipitation anomaly ii agricultural drought soil moisture depletion resulting from insufficient available water or iii hydrological drought surface and subsurface water shortage or deviation from normal conditions over a long period of time dalezios and spyropoulosand 2017 drought monitoring and analysis can be performed by converting satellite images to actionable information from a large data set this requires many statistical and geophysical methods and can inform environmental decision making manual processing and interpreting of remote sensing images and weather data is laborious and ml and dl are promising approaches for retrieving information and extracting features from large datasets drought monitoring and prediction in the sphere of computational intelligence known as ml is highly recommended and has become a crucial issue for ensuring accuracy mlms have been employed in many studies in identifying and forecasting different kinds of drought table 1 2 1 agricultural drought characterization and forecasting using mlm agricultural drought characterization and predictions using remote sensing data help provide an overview of drought conditions for a large geographic area pei et al 2013 recently various mlms have been used to characterize and forecast agricultural drought table 2 feng et al 2019 used three mlms such as brf svm and mlpnn multi layer perceptron neural network to produce a agricultural drought distribution map the model exhibited a high r2 value and predicted spei value that was highly correlated with the observed wheat yield exhibiting strong visual and statistical agreement soil moisture is considered an important indicator of water stress for vegetation using dl as an ml method lee et al 2018 estimated soil moisture using a dnn deep neural network model for agricultural drought monitoring the dnn algorithm performs supervised learning by following a feed forwarded neural network the backpropagation method consists of input layers hidden layers with several nodes and output layers the estimated soil moisture with the dnn model had a low rmse which improved the reliability of drought monitoring in another study nie et al 2018 predicted soil moisture using the svm rf and bpnn back propagation neural network models prediction results showed svm had better prediction results compared to rf and bpnn and identified precipitation relative humidity and terrain as the most influential factors affecting soil moisture park et al 2016 modeled spi as reference data for agricultural drought using rf brt and cubist mlm which was robust and flexible for the regression task of the sixteen drought indicators selected from remote sensing data for this study ndvi and lst land surface temperature had the highest relative importance for agricultural drought agricultural drought hazard was mapped by rahmati et al 2020 to evaluate relative departure soil moisture with the cart rt brt mars multivariate adaptive regression splines fda flexible discriminant analysis and svm models the rf model predicted that 21 of the area was very high risk with the highest accuracy the rf model also anticipated high drought risk for low mean annual precipitation on soil with low plant available water holding capacity because soil moisture is an important environmental factor for agricultural drought ffnn demonstrated a good relationship between in situ and remote sensing soil moisture projections the short term drought prediction model was developed by park et al 2018 using the rf model to consider three satellite based drought indices the scaled drought condition index sdci microwave integrated drought index midi and very short term drought index vsdi from modis and trmm data their drought prediction model improved the performance of three drought indices which were significantly correlated r 0 7 amsr e advanced microwave scanning radiometer on the earth observing system cdmi combined drought monitoring index esa cci european space agency esa climate change initiative et evapo transpiration evi enhanced vegetation index lai leaf area index mei multivariate enso index midi microwave integrated drought index mndwi modification of normalized difference water index msavi modified soil adjusted vegetation index msi moisture stress index smc soil moisture content nddi normalized difference drought index ndmi normalized difference moisture index ndvi normalized difference vegetation index ndw normalized difference water index ndwi normalized difference water index nir near infrared nmdi normalized multi band drought index noaa national oceanic and atmospheric administration oc svm one class support vector machine pawc plant available water holding capacity rdsm relative departure of soil moisture rmm mjo multivariate madden julian oscillation rsm relative soil moisture savi soil adjusted vegetation index swir1 short wavelength infrared 1 tir1 thermal infrared 1 sdci scaled drought condition index twi topographic wetness index vsdi very short term drought index wpsh a western pacific subtropical high area index wpsh i western pacific subtropical high intensity index wpsh rp western pacific subtropical high ridge position index wpsh wrp western pacific subtropical high western ridge point index 2 2 meteorological drought characterization and forecasting using mlms data driven models help to map and characterize meteorological drought enabling short and long term forecasting by assessing statistical rainfall distribution patterns ml with supervised and unsupervised modeling for drought forecasting has important implications for decision making to develop climate risk mitigation strategies a number of studies have been conducted using mlms for characterizing and forecasting meteorological drought table 3 ali et al 2018 designed a drought modeling framework for meteorological drought forecasting using an ml approach they predicted spi by applying the comm elm committee extreme learning machine model with respect to the comm pso anfis committee particle swarm optimization adaptive neuro fuzzy inference system and comm mlr committee multiple linear regression models the model used temperature precipitation and the southern oscillation index as predictor variables to forecast spi compared to the prediction accuracy performance of the three models the comm elm model rmse 0 307 and r 0 976 was remarkably better in the testing data set than the comm mlr models rmse 0 469 and r 0 961 and comm pso anfis rmse 0 674 and r 0 946 at islamabad station in pakistan belayneh and adamowski 2012 compared the different mlms for spi forecasting they used spi 6 and spi 12 with a lead time scale of 1 and 6 months to forecast spi with the ann svr and wnn models all mlms demonstrated improved forecasting results for spi 12 with decreased forecast lead time where wnn r2 0 882 and rmse 0 07 performed better than ann r2 0 769 and rmse 0 157 and svr r2 0 882 and rmse 0 07 model at ejersalele station upper awash basin of ethiopia the ann and xgboost algorithms were used to predict spei over 1 6 months to develop a new forecasting model by zhang et al 2019 that uses meteorological measures and climate signals from 1961 to 2016 the study used a 10 fold cross validation method for evaluating model performance and found that the xgboost model with the highest predictive skills r2 0 82 for predicting spi 3 than ann r2 0 79 and dlnm distributed lag nonlinear model r2 0 52 ali et al 2019 aimed to explore appropriate time scales to construct a standardized precipitation temperature index spti from meteorological station data using the boruta algorithm with the rf model in this model research station data was used as dependent variables while different time scales were used as predictor variables the results suggested that spti 1 was highly ranked and able to capture all the characteristics needed for drought classification in a supervised rf environment a high resolution meteorological drought index in which spi and spei were used as meteorological drought indices was constructed by rhee and im 2017 mlm such as dt decision tree rf and ert extreme randomized tree with remote sensing data were compared with the kringing interpolation technique ert exhibited better prediction accuracy 64 2 3 hydrological drought characterization and forecasting using mlm hydrological drought occurs just after meteorological drought when there is less available water or reservoir and river discharge than normal conditions over a certain period evolution of hydrological drought is not only due to precipitation shortage but it is regulated by complex physical mechanisms hao et al 2018 thus the development of hydrological drought from meteorological drought is a gradual process and while rainfall deficit is the primary causative mechanism other factors such as low temperature that creates snow accumulation low water shortage and extreme high temperature also initiate the occurrence of hydrological drought van loon and van lanen 2012 forecasting hydrological drought helps to design mitigation actions to manage water resources hydrological drought forecasting with mlms can describe nonlinearities in hydrological data most previous studies considered only streamflow prediction rather than concentrating on drought index forecasting recently however mlms have been used to forecast hydrological drought by applying the surface water supply index swsi reclamation drought index rdi and phdi to different remote sensing data sets table 4 agana and homaifar 2017 introduced the dbn model with restricted boltzmann machines rbm to train hydrological data to predict long term drought using the standardized streamflow index as input the dbn algorithm was also compared to the mlp and svr algorithms to evaluate the model efficiency for forecasting time scale drought conditions the dbn algorithm was determined to be more efficient for long term drought forecasting than the mlp and svr algorithms dehghani et al 2014 applied the ffann feed forward artificial neural network algorithm to train the standardized hydrological drought index shdi and forecast monthly time series streamflow discharge data hydrological drought forecasted with the shdi index using the ffann algorithm was more accurate groundwater drought monitoring was conducted using the ann model by seo and lee 2019 who used the standardized groundwater index sgi and validated it with the palmer drought severity index psdi the performance of sgi with the ann model was variable because of the short study period 13 years deo et al 2016 a wavelet based drought model was developed by using the w elm wavelet extreme learning machine model to forecast effective drought indices edi the model used 97 years of data from three hydrological stations partitioned for training validation and test data they compared performance of the model to ann lssvr w ann and w lssvr wavelet least square support vector regression for accuracy and computational efficiency and found that the w elm model had improved forecasting skill besides characterizing the above three drought categories comprehensive or integrated drought indices have been developed by different studies esfahanian et al 2017 han et al 2019 using mlms esfahanian et al 2017 developed a comprehensive drought index that reflects both hydrology agricultural and meteorological drought they also used the anfis model to develop a predictive drought model the model s r2 value was reported as 0 75 indicating it was capable of predicting drought han et al 2019 used a rf model to construct a combined drought monitoring index cdmi cdmi was found to be more correlated with spi and rsm relative soil moisture than other indices an integrated drought index with mlms has great value for exploring the relationship between various factors and understanding the future drought category and mitigation strategies 2 4 mlm evaluation for drought forecasting despite the advantages of using mlms for drought monitoring and forecasting numerous facts such as the bias variance tradeoff must be considered for mlms that depend on bias and variance either the model is not accurate and does not match the data well bias or it is not fixed and there may be an excess of variation variance it is impossible to completely overcome the bias and variance problems because of the tradeoff between them james et al 2003 however some models and particular model parameters perform better by fitting the point between bias variance trade off fig 1 model complexity and the amount of training data are also significant for model performance the more complex functions require more training data to improve the precision of the algorithm model complexity and the size of the training and validation data set yield different consequences such as i high training error ii low training error but high validation error and iii low training and validation error but high testing error which respectively create problems of imperfect models overfitting and training bias fig 1 deparday et al 2019 moreover the accuracy and performance of each model depend on model interpretability or the explainability of model complexity thus high accuracy can be achieved with a more complex model and more extensive data despite the struggle for interpretation known as an explainability accuracy tradeoff fig 1 therefore overfitting and underfitting should be considered when choosing the perfect model for a given data set 2 4 1 comparison of past studies using statistical and ml methods for drought predictions the drought forecasting performance of mlms discussed in different literatures is compared with statistical time series and probabilistic methods fig 2 we considered r2 which is a normalized measure to compare the model performance across the data sets for better comparison and evaluation we selected the model based on the homogeneity of the datasets where all the models predicted meteorological drought e g spi with a lead time of 1 6 months moreover the detailed description and analysis of the data sets for fig 7 can be found in the literature listed in the reference section results presented in fig 7 reveal that all mlms and hybrid models outperformed other models in drought forecasting the poor performance of the general statistical model may be due to the fact that linear regression is direct and simple with low computational costs while a significant number of variables is needed for accurate prediction furthermore because of its collinearity problems the general statistical model performs improperly in long lagged forecasting the statistical model also fits better with linear data and performs poorly with non linear data the probabilistic model is capable of dealing with complex distributions but requires a lot of computational time however in ml algorithms such as ann svr and rf a nonlinear property as well as the kernel dimension of a different data set can quickly identify all potential associations between predictors to help overcome overfitting problems in spite of these advantages a hybrid model can combine different models based on the problems confronted and perform better than other statistical and probabilistic models 2 4 2 other methods for ml algorithms evaluation a case study methods or model evaluation metrics are very important to assess an algorithm whether it is performing well or not the prediction accuracy of mlms is evaluated by using several methods in this regard different types of evaluation metrics such as i classification accuracy ii confusion matrix and iii auc roc area under curve receiver operating characteristic curve have been used in different studies an example of a confusion matrix for predicting meteorological drought using the fft fast and frugal decision tree model is presented in fig 3 spi 6 was calculated for 33 meteorological stations in bangladesh from trmm data for 16 years 2001 2016 rainfall anomaly maximum and minimum temperature from meteorological stations ndvi anomaly and lst from modis data were used as independent variables for the model another widely used metric for the evaluation of mlms is the auc roc curve we used a concise and representative data set 205 positive and 110 negative points of spi 6 as dependent variables for meteorological drought prediction to apply the roc methods here we used fft cart lr logistic regression rf and svm models to compare the model performances in predicting meteorological drought fig 8 we performed k means cluster analysis to compare mlms based on the drought forecasting model presented in table 1 and discussed in sections 2 1 2 2 and 2 3 for this comparative analysis we considered three parameters algorithm complexity ease of use and accuracy first we applied scree plot analysis fig 4 for cluster number identification then we employed cluster analysis by fixing the cluster number to three table 5 our findings showed that five mlms mlpnn cart anf dnn xgboost belonged to cluster 1 based on accuracy cluster 2 with 6 mlms elm anfis pca knn ls svm and brf was formed by the ease of use parameter finally cluster 3 was comprised of four mlms ann svm wann and bpnn identified as complex algorithms the first group of mlms had high accuracy for drought prediction when dealing with complex heterogeneous data sets the second group of mlms is very efficient easy to understand and lacks data redundancy with low noise sensitivity these mlms are thought to reduce complexity in pattern recognition finally the third group of mlms strongly contributed to the hybrid model and are suggested for hybrid model development because they forecast drought with excellent stability and high representativeness 2 5 different case studies in drought characterization and forecasting we selected different case studies to provide an overview of mlm performance for drought assessment these case studies represent different geospatial regions along with different data sets units of analysis and ml algorithms we used our own data sets as well as highlighted examples from other studies for synthesising the use of mlms in this section mainly the application of mlms is discussed based on classification and regression tasks table 6 the first case study represents the use of the fft model mainly for performing a classification task for predicting agricultural drought risk the fft algorithm was also compared with its performance with lr cart and svm models to indicate that fft had high accuracy in both training and testing data sets fft provides accurate and robust forecasts based on frugality simplicity and prediction accuracy in case study 2 benlayneh et al 2013 used a different data driven model which performed a regression task to forecast spi 12 long term drought this case study exposed the skill of the wavelet coupled model with ann and svr models the wa ann model outperforms the wa svr model because wavelet transforms de noise the spi time series and also reduce the sensitivity in the case of the ann model however lima et al 2013 suggested that the svr model was more effective in the case of precipitation forecasting while witten et al 2011 found the ann model superior in the evaluation phase the rf model also showed better performance in remote sensing applications for mjo based vsdi prediction in the case of case study 3 as confirmed by park et al 2016 ke et al 2017 and im et al 2016 in their studies rf had higher predictive skill than other ml approaches i e dt brt etc for regression tasks from the above case study it is clear that the performance of each mlm is not fixed but depends on the size of the training data the transformation of the spatial data sets for feature learning and the depth of the algorithms chevalier et al 2011 zhang et al 2016 considering the above facts further we investigated the use of mlms to reveal drought based on the algorithm depth we selected one dl algorithm i e dfnn deep forwarded neural network and two ml methods i e xgboost and rf according to the depth of the algorithm dfnn xgboost rf to predict smdi soil moisture deficit index using multi sensor remote sensing data modis trmm parsiann cdr chirps and esa cc soil moisture for soil moisture related drought over south asia for each model we considered 75 n 5491 for training and 12 5 n 915 for each of the testing and validation data sets besides we used seven input variables spi 6 spi 3 spei 6 spei 3 pci precipitation condition index vci and tci as predictor variables and smdi as a response variable due to space and page limitations only the results of the models are discussed the detailed of the processing of the predicted and response variables can be found in narasimhan and srinivasan 2005 kogan 1995 kogan 1997 zhang and jia 2019 mckee et al 1993 and vicente serrano et al 2010 results presented in fig 6 suggest that the dfnn model demonstrated low rmse for each iteration for both training and validation data sets compared to the xgboost and rf models moreover the predicted smdi by the dfnn model showed high consistency r2 0 94 with the observed smdi from the perspective of the above case studies we have found that the dl algorithm has had significant success in the areas of drought monitoring prediction and characterization because multilayer deep neural networks have more depth compared to other models they can implement extremely complex functions on their input datasets and facilitate more learning from the training dataset 3 current research trends challenges and future directions of mlm for drought monitoring and forecasting 3 1 current research trends the mlm application has increased in recent years reflecting the efforts and advances of different parts of the scientific community including engineers neuro scientists environmental experts and other researchers worldwide a simple statistical analysis was conducted based on existing literature to analyze and observe the current status of mlms in the field of drought monitoring the number of ml models used in the existing literature in different subject areas were adopted from sun and scanlon 2019 and papers published in peer reviewed journals from 2004 to 2020 were considered sun and scanlon 2019 followed the scopus database and identified 1451 articles about ml models in different subject areas the survey results showed that earth and planetary science 26 3 ranked first in the use of mlms followed by engineering 15 3 and environmental science 12 6 fig 7 a our review found 76 papers in which mlm was used for drought monitoring out of a total of 1451 documents from 2007 to 2017 we manually extracted approximately 390 keywords from 73 articles and performed word cloud analysis fig 7 b the visual presentation confirms that the ann artificial neural nets terms focused more on drought monitoring and forecasting than other models the largest font size in the figure indicates terms appearing with higher frequency in the keywords of peer reviewed literature these results confirmed that ann was the most popular model which was consistent with the results of fig 7 c presents the usage frequency of different mlms as shown in fig 7 c cart pca svm and rf were less commonly used than the ann and hybrid models the higher use of the ann model is anticipated because it has non linear properties that make it more robust for identifying all possible interactions between predictors 3 2 challenges above we focus on the use of mlms in different subject areas emphasizing current trends of mlms in drought monitoring however certain features related to mlms create challenges for adopting ml in drought studies to address drought hazard mitigation strategies drought monitoring requires a long time series and multisource data where satellite data has been used for climatological by retrieving precipitation soil moisture and evaporation and ecological by assessing vegetation health perspectives the main challenges of ml in drought monitoring by remote sensing are as follows i multimodal data leads to a variety of geometric content which includes different imaging geometries geographic information system layers and multitemporal aspects ii high spatiotemporal resolution often creates problems for time series data analysis to retrieve drought information iii insufficient training samples for spatial data sets are usually unable to present seasonal variability for drought monitoring iv feature extraction to retrieve drought information with remote sensing may encounter noise and outliers making ml a challenging task v interpretability of ml model affects the visualization of drought hazards with hyperspectral data set having little to no variety vi drought prediction through ml model may be inconsistent due to the observational bias that arises from inadequate individual knowledge of sensor adequacy and data quality and vii complex data with high noise sources and high dimensional space yield uncertainty of ml model for drought forecasting therefore the challenges of using mlms for drought monitoring with remote sensing data can only be overcome by applying the latest advances in computation and high performance computation technology such as dl and data mining these advances in computation technology play an important role in disaster remote sensing by establishing a big data analytics platform that improves decision science through drought feature extraction and visualization mapping 3 2 1 future directions of mlm for drought monitoring and forecasting future research directions for drought monitoring and forecasting with ml include combining all possible sensors multi sensors and adaptive fusion techniques and using an environmental applications platform to quantify the sustainable implications of ecological changes induced by human activities such combinations will create a new boundary between environmental sensing and data science for multiscale drought disaster monitoring and modeling analysis to minimize the gap between data and models researchers have suggested numerous methods including assimilation synthesis and the fusion of data models ladeau et al 2011 presently recorded data and real time data availability allow us to integrate different data sets from various sources to observe water resource availability trends for drought disaster monitoring thus big data integration techniques would be effective for drought disaster mapping and real time forecasting similarly we proposed a geospatial modeling platform combining the sensor web and model web models adopted and modified by chang and bai 2018 to retrieve time series drought information and decision support systems fig 8 this geospatial platform integrates the data fusion model with ml and data mining applications to observe and map drought disasters the platform is therefore helpful in the environmental decision making process for adopting a mitigation strategy though big data for hydro informatics still faces some challenges its application is perfect for drought visualization and forecasting in different studies for example zhai et al 2016 and verdin et al 2015 used a big data platform model for integrating multi sensors and other data sources to monitor hydrological disasters resulting in more reliable and robust forecasting a knowledge based system with big data platforms including ml and data mining will magnify the primary significance of drought disaster analysis for environmental decision making 4 conclusions this review provides a data based investigation of mlms used in drought monitoring and forecasting highlighting data mining and big data approaches for environmental decision making we believe our article provides useful insight into the field of ml and hope it will motivate further research exploring ml applications in drought disaster monitoring in this article we have concisely introduced and systematically analyzed a number of typical ml and dl methods that may be used with remote sensing data for drought forecasting different types of drought meteorological agricultural and hydrological forecasting using mlms containing both supervised and unsupervised algorithms have been summarized and discussed our survey evaluates current trends in ml algorithm application for drought monitoring including research challenges when using remote sensing data in summary the following inclusive statements can be formulated according to the findings drought forecasting using mlm is in the early phase of adoption the selection of mlm used for drought prediction with remote sensing data sets is primarily dependent on several factors including the amount of training data dimensional reduction forecasting accuracy and computational competency when comparing the advantages of mlms the ml model outperformed the classical statistical model for hypothesis space and extensive data sets by encouraging a range of possible solutions for a given problem for model performance in drought forecasting however issues of overfitting and underfitting were revealed these issues are related to model complexity and the volume of data in the training data set nevertheless ml and hybrid models had superior drought forecasting performance compared with statistical time series and probabilistic models although researchers encountered several challenges related to using the ml model these can be minimized by applying dl and data mining approaches thus the big data analysis platform enables the implementation of the latest and highest performance computational technology for decision making in the field of disaster remote sensing our present findings confirm that there are few big data applications for observing and mapping drought disasters therefore future research should be conducted to develop a geospatial platform that merges the data fusion model with ml and data mining techniques in the form of big data applications declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this study was supported by the cas strategic priority research program grant no xda19030402 the natural science foundation of china no 31671585 no 41871253 key basic research project of shandong natural science foundation of china no zr2017zb0422 and taishan scholar project of shandong province no tsxz201712 the first author would like to thanks cas twas president s fellowship program for the support during the study we are very grateful to the editor and anonymous reviewers for their valuable comments and suggestions appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105327 
25632,machine learning is a dynamic field with wide ranging applications including drought modeling and forecasting drought is a complex devastating natural disaster for which it is challenging to develop effective prediction models therefore our review focuses on basic information about machine learning methods mlms and their potential applications in developing efficient and effective drought forecasting models we observed that mlms have achieved significant advances in the robustness effectiveness and accuracy of the algorithms for drought modelling in recent years the performance comparison of mlms with other models provides a comprehensive conception of different model evaluation metrics further challenges of mlms such as inadequate training data sets noise outliers and observation bias for spatial data sets are explored finally our review conveys in depth understanding to researchers on machine learning applications in forecasting and modeling and provides drought mitigation strategy guidance for policymakers keywords machine learning deep learning forecasting drought big data 1 introduction recently machine learning ml has attracted attention from the scientific community for research in various fields such as engineering agriculture medicine marketing and earth and environmental sciences in particular high computational power and speed available sensor data and advances in big data analysis have driven ml to influence many aspects of research ml data science and artificial intelligence have become interchangeable terms in research educational and industrial applications of technology ml has made a significant contribution to sustainable environmental development through disaster risk management deparday et al 2019 drought forecasting using stochastic models considers only the present and past value of the same variable ignoring the effects of other variables a probabilistic model with bivariate and multivariate analysis for drought forecasting creates a problem when fitting the data distribution because of marginal distribution functions recently ml marj and meijerink 2011 belayneh et al 2014 2016 belayneh and adamowski 2013 kousari et al 2017 park et al 2016 deo and shahin 2016 and hybrid models jalalkamali et al 2015 shirmohammadi et al 2013 belayneh et al 2016 have been used as alternative methods for statistical li et al 2015 zhang et al 2019 time series chen et al 2012 belayneh et al 2014 barua et al 2012 and probabilistic models khadr 2016 chen et al 2016 nnaji et al 2016 hao et al 2016 and have shown high capabilities in nonlinear and dynamic time series drought modeling and forecasting drought is typically recognized as an attribute of nonlinearity and unstableness hao et al 2015 therefore stochastic models are not able to accurately grasp the characteristics of meteorological and hydrological data for drought monitoring wei et al 2012 in addition probabilistic models are computationally expensive to deal complex and heterogeneous data sets fung et al 2019 however mlms have the power of self organizing and self adaptive function with the nonlinear property are capable of modeling and forecasting hydrological and meteorological data for drought characterization liu et al 2018 drought is an inherently striking event induced by the climatological factors topography and water demand of a specific region ndcm 2014 drought is characterized as a complex natural hazard owing to the intensity severity and uncontrollable nature of this phenomena like other natural disasters drought leads to economic and property losses creates environmental and ecological imbalances and potentially threatens social losses vafakhah et al 2014 considering that drought is a complex and devastating natural hazard drought monitoring is a significant issue while other natural hazards such as floods earthquakes and cyclones are short term events drought can persist for years thus proper drought monitoring and forecasting that include meteorological and remote sensing data are required for planning and decision making however effective mitigation planning and adaptation strategy development primarily depend on the efficiency of the drought forecasting model or method in this context ml provides more accurate and efficient drought forecasting and can be applied in drought disaster risk management wilhite 2012 mallick et al 2013 there have been several review studies related to ml that either describe the fundamental concept of ml and forecasting methods holloway et al 2018 makridakis et al 2018 kersting 2018 or provide a detailed review on flood disaster mosavi et al 2018 and water management sun and scanlon 2019 review studies on drought disaster include some studies on modeling drought forecasting for example the review by fung et al 2019 primarily focused on different drought prediction models paying no attention to in depth applications of ml another review by mishara and singh 2011 focused on different models including probabilistic modeling global climate models and spatio temporal analysis for drought scenarios without addressing the field of ml presently there has been no organized and inclusive survey on ml related to drought disaster monitoring and forecasting therefore quantitative analysis is an urgent issue related to sustainable disaster management particularly for applying ml to drought monitoring and forecasting our review aims to explore the function and performance of ml algorithms that have been used for modeling and forecasting drought as well as challenges in disaster remote sensing the basic concept and development of ml and deep learning dl algorithms has been presented in the supplementary materials therefore in the following sections we first review the present use of ml in different types of drought studies next we review the potential of ml in forecasting drought over statistical time series and probabilistic modeling and also the factors that have hindered the performance of ml until now finally we discuss the challenges of using ml for drought forecasting in a geospatial context we expect our review to provide insight into the multidimensional nature of ml and facilitate information and learning exchanges among scientists and policymakers for better drought disaster management 2 mlm for environmental remote sensing with particular reference to drought monitoring and forecasting environmental monitoring of earth with various satellite and space born platforms provides versatile measurement of phenomena in the biosphere hydrosphere and atmosphere remote sensing is more convenient and practical than traditional weather station and statistical data gathering methods for various regional applications such as vegetation condition monitoring drought detection and prediction and crop yield measurement bhushan 2017 dalezios et al 2017 presently disaster management activities such as drought monitoring with various remote sensing satellites have increased the capability and reliability of drought quantification methodologies on a global scale in this context satellite sensors such as noah avhrr modis spot vegetation meris and sentinel from esa provide real time high resolution data for assessing vegetation cover soil moisture crop condition and water body estimation for lakes rivers and reservoirs these indicators provide a basic understanding of drought conditions in a given region dalezios et al 2017 furthermore satellite based observations and ground based rain gauge data are used for rainfall anomaly estimation which represents meteorological drought moreover vegetation indices i e ndvi vci tci and vhi derived from satellite data are useful for evaluating vegetation stress periods which can indicate agricultural drought drought occurs when there is a shortage of water for a substantially extended period over a large area and drought intensity exhibits significant spatiotemporal variation owing to the phenomenon s regionality thus understanding the spatial analysis of drought distribution is essential for effective management of surface and ground water yu et al 2014 bhushan 2017 drought affects a wide variety of sectors on the earth surface making it difficult to define universally considering drought hazards and recurrence in various areas drought may be classified as follows i meteorological drought usually illustrated by precipitation anomaly ii agricultural drought soil moisture depletion resulting from insufficient available water or iii hydrological drought surface and subsurface water shortage or deviation from normal conditions over a long period of time dalezios and spyropoulosand 2017 drought monitoring and analysis can be performed by converting satellite images to actionable information from a large data set this requires many statistical and geophysical methods and can inform environmental decision making manual processing and interpreting of remote sensing images and weather data is laborious and ml and dl are promising approaches for retrieving information and extracting features from large datasets drought monitoring and prediction in the sphere of computational intelligence known as ml is highly recommended and has become a crucial issue for ensuring accuracy mlms have been employed in many studies in identifying and forecasting different kinds of drought table 1 2 1 agricultural drought characterization and forecasting using mlm agricultural drought characterization and predictions using remote sensing data help provide an overview of drought conditions for a large geographic area pei et al 2013 recently various mlms have been used to characterize and forecast agricultural drought table 2 feng et al 2019 used three mlms such as brf svm and mlpnn multi layer perceptron neural network to produce a agricultural drought distribution map the model exhibited a high r2 value and predicted spei value that was highly correlated with the observed wheat yield exhibiting strong visual and statistical agreement soil moisture is considered an important indicator of water stress for vegetation using dl as an ml method lee et al 2018 estimated soil moisture using a dnn deep neural network model for agricultural drought monitoring the dnn algorithm performs supervised learning by following a feed forwarded neural network the backpropagation method consists of input layers hidden layers with several nodes and output layers the estimated soil moisture with the dnn model had a low rmse which improved the reliability of drought monitoring in another study nie et al 2018 predicted soil moisture using the svm rf and bpnn back propagation neural network models prediction results showed svm had better prediction results compared to rf and bpnn and identified precipitation relative humidity and terrain as the most influential factors affecting soil moisture park et al 2016 modeled spi as reference data for agricultural drought using rf brt and cubist mlm which was robust and flexible for the regression task of the sixteen drought indicators selected from remote sensing data for this study ndvi and lst land surface temperature had the highest relative importance for agricultural drought agricultural drought hazard was mapped by rahmati et al 2020 to evaluate relative departure soil moisture with the cart rt brt mars multivariate adaptive regression splines fda flexible discriminant analysis and svm models the rf model predicted that 21 of the area was very high risk with the highest accuracy the rf model also anticipated high drought risk for low mean annual precipitation on soil with low plant available water holding capacity because soil moisture is an important environmental factor for agricultural drought ffnn demonstrated a good relationship between in situ and remote sensing soil moisture projections the short term drought prediction model was developed by park et al 2018 using the rf model to consider three satellite based drought indices the scaled drought condition index sdci microwave integrated drought index midi and very short term drought index vsdi from modis and trmm data their drought prediction model improved the performance of three drought indices which were significantly correlated r 0 7 amsr e advanced microwave scanning radiometer on the earth observing system cdmi combined drought monitoring index esa cci european space agency esa climate change initiative et evapo transpiration evi enhanced vegetation index lai leaf area index mei multivariate enso index midi microwave integrated drought index mndwi modification of normalized difference water index msavi modified soil adjusted vegetation index msi moisture stress index smc soil moisture content nddi normalized difference drought index ndmi normalized difference moisture index ndvi normalized difference vegetation index ndw normalized difference water index ndwi normalized difference water index nir near infrared nmdi normalized multi band drought index noaa national oceanic and atmospheric administration oc svm one class support vector machine pawc plant available water holding capacity rdsm relative departure of soil moisture rmm mjo multivariate madden julian oscillation rsm relative soil moisture savi soil adjusted vegetation index swir1 short wavelength infrared 1 tir1 thermal infrared 1 sdci scaled drought condition index twi topographic wetness index vsdi very short term drought index wpsh a western pacific subtropical high area index wpsh i western pacific subtropical high intensity index wpsh rp western pacific subtropical high ridge position index wpsh wrp western pacific subtropical high western ridge point index 2 2 meteorological drought characterization and forecasting using mlms data driven models help to map and characterize meteorological drought enabling short and long term forecasting by assessing statistical rainfall distribution patterns ml with supervised and unsupervised modeling for drought forecasting has important implications for decision making to develop climate risk mitigation strategies a number of studies have been conducted using mlms for characterizing and forecasting meteorological drought table 3 ali et al 2018 designed a drought modeling framework for meteorological drought forecasting using an ml approach they predicted spi by applying the comm elm committee extreme learning machine model with respect to the comm pso anfis committee particle swarm optimization adaptive neuro fuzzy inference system and comm mlr committee multiple linear regression models the model used temperature precipitation and the southern oscillation index as predictor variables to forecast spi compared to the prediction accuracy performance of the three models the comm elm model rmse 0 307 and r 0 976 was remarkably better in the testing data set than the comm mlr models rmse 0 469 and r 0 961 and comm pso anfis rmse 0 674 and r 0 946 at islamabad station in pakistan belayneh and adamowski 2012 compared the different mlms for spi forecasting they used spi 6 and spi 12 with a lead time scale of 1 and 6 months to forecast spi with the ann svr and wnn models all mlms demonstrated improved forecasting results for spi 12 with decreased forecast lead time where wnn r2 0 882 and rmse 0 07 performed better than ann r2 0 769 and rmse 0 157 and svr r2 0 882 and rmse 0 07 model at ejersalele station upper awash basin of ethiopia the ann and xgboost algorithms were used to predict spei over 1 6 months to develop a new forecasting model by zhang et al 2019 that uses meteorological measures and climate signals from 1961 to 2016 the study used a 10 fold cross validation method for evaluating model performance and found that the xgboost model with the highest predictive skills r2 0 82 for predicting spi 3 than ann r2 0 79 and dlnm distributed lag nonlinear model r2 0 52 ali et al 2019 aimed to explore appropriate time scales to construct a standardized precipitation temperature index spti from meteorological station data using the boruta algorithm with the rf model in this model research station data was used as dependent variables while different time scales were used as predictor variables the results suggested that spti 1 was highly ranked and able to capture all the characteristics needed for drought classification in a supervised rf environment a high resolution meteorological drought index in which spi and spei were used as meteorological drought indices was constructed by rhee and im 2017 mlm such as dt decision tree rf and ert extreme randomized tree with remote sensing data were compared with the kringing interpolation technique ert exhibited better prediction accuracy 64 2 3 hydrological drought characterization and forecasting using mlm hydrological drought occurs just after meteorological drought when there is less available water or reservoir and river discharge than normal conditions over a certain period evolution of hydrological drought is not only due to precipitation shortage but it is regulated by complex physical mechanisms hao et al 2018 thus the development of hydrological drought from meteorological drought is a gradual process and while rainfall deficit is the primary causative mechanism other factors such as low temperature that creates snow accumulation low water shortage and extreme high temperature also initiate the occurrence of hydrological drought van loon and van lanen 2012 forecasting hydrological drought helps to design mitigation actions to manage water resources hydrological drought forecasting with mlms can describe nonlinearities in hydrological data most previous studies considered only streamflow prediction rather than concentrating on drought index forecasting recently however mlms have been used to forecast hydrological drought by applying the surface water supply index swsi reclamation drought index rdi and phdi to different remote sensing data sets table 4 agana and homaifar 2017 introduced the dbn model with restricted boltzmann machines rbm to train hydrological data to predict long term drought using the standardized streamflow index as input the dbn algorithm was also compared to the mlp and svr algorithms to evaluate the model efficiency for forecasting time scale drought conditions the dbn algorithm was determined to be more efficient for long term drought forecasting than the mlp and svr algorithms dehghani et al 2014 applied the ffann feed forward artificial neural network algorithm to train the standardized hydrological drought index shdi and forecast monthly time series streamflow discharge data hydrological drought forecasted with the shdi index using the ffann algorithm was more accurate groundwater drought monitoring was conducted using the ann model by seo and lee 2019 who used the standardized groundwater index sgi and validated it with the palmer drought severity index psdi the performance of sgi with the ann model was variable because of the short study period 13 years deo et al 2016 a wavelet based drought model was developed by using the w elm wavelet extreme learning machine model to forecast effective drought indices edi the model used 97 years of data from three hydrological stations partitioned for training validation and test data they compared performance of the model to ann lssvr w ann and w lssvr wavelet least square support vector regression for accuracy and computational efficiency and found that the w elm model had improved forecasting skill besides characterizing the above three drought categories comprehensive or integrated drought indices have been developed by different studies esfahanian et al 2017 han et al 2019 using mlms esfahanian et al 2017 developed a comprehensive drought index that reflects both hydrology agricultural and meteorological drought they also used the anfis model to develop a predictive drought model the model s r2 value was reported as 0 75 indicating it was capable of predicting drought han et al 2019 used a rf model to construct a combined drought monitoring index cdmi cdmi was found to be more correlated with spi and rsm relative soil moisture than other indices an integrated drought index with mlms has great value for exploring the relationship between various factors and understanding the future drought category and mitigation strategies 2 4 mlm evaluation for drought forecasting despite the advantages of using mlms for drought monitoring and forecasting numerous facts such as the bias variance tradeoff must be considered for mlms that depend on bias and variance either the model is not accurate and does not match the data well bias or it is not fixed and there may be an excess of variation variance it is impossible to completely overcome the bias and variance problems because of the tradeoff between them james et al 2003 however some models and particular model parameters perform better by fitting the point between bias variance trade off fig 1 model complexity and the amount of training data are also significant for model performance the more complex functions require more training data to improve the precision of the algorithm model complexity and the size of the training and validation data set yield different consequences such as i high training error ii low training error but high validation error and iii low training and validation error but high testing error which respectively create problems of imperfect models overfitting and training bias fig 1 deparday et al 2019 moreover the accuracy and performance of each model depend on model interpretability or the explainability of model complexity thus high accuracy can be achieved with a more complex model and more extensive data despite the struggle for interpretation known as an explainability accuracy tradeoff fig 1 therefore overfitting and underfitting should be considered when choosing the perfect model for a given data set 2 4 1 comparison of past studies using statistical and ml methods for drought predictions the drought forecasting performance of mlms discussed in different literatures is compared with statistical time series and probabilistic methods fig 2 we considered r2 which is a normalized measure to compare the model performance across the data sets for better comparison and evaluation we selected the model based on the homogeneity of the datasets where all the models predicted meteorological drought e g spi with a lead time of 1 6 months moreover the detailed description and analysis of the data sets for fig 7 can be found in the literature listed in the reference section results presented in fig 7 reveal that all mlms and hybrid models outperformed other models in drought forecasting the poor performance of the general statistical model may be due to the fact that linear regression is direct and simple with low computational costs while a significant number of variables is needed for accurate prediction furthermore because of its collinearity problems the general statistical model performs improperly in long lagged forecasting the statistical model also fits better with linear data and performs poorly with non linear data the probabilistic model is capable of dealing with complex distributions but requires a lot of computational time however in ml algorithms such as ann svr and rf a nonlinear property as well as the kernel dimension of a different data set can quickly identify all potential associations between predictors to help overcome overfitting problems in spite of these advantages a hybrid model can combine different models based on the problems confronted and perform better than other statistical and probabilistic models 2 4 2 other methods for ml algorithms evaluation a case study methods or model evaluation metrics are very important to assess an algorithm whether it is performing well or not the prediction accuracy of mlms is evaluated by using several methods in this regard different types of evaluation metrics such as i classification accuracy ii confusion matrix and iii auc roc area under curve receiver operating characteristic curve have been used in different studies an example of a confusion matrix for predicting meteorological drought using the fft fast and frugal decision tree model is presented in fig 3 spi 6 was calculated for 33 meteorological stations in bangladesh from trmm data for 16 years 2001 2016 rainfall anomaly maximum and minimum temperature from meteorological stations ndvi anomaly and lst from modis data were used as independent variables for the model another widely used metric for the evaluation of mlms is the auc roc curve we used a concise and representative data set 205 positive and 110 negative points of spi 6 as dependent variables for meteorological drought prediction to apply the roc methods here we used fft cart lr logistic regression rf and svm models to compare the model performances in predicting meteorological drought fig 8 we performed k means cluster analysis to compare mlms based on the drought forecasting model presented in table 1 and discussed in sections 2 1 2 2 and 2 3 for this comparative analysis we considered three parameters algorithm complexity ease of use and accuracy first we applied scree plot analysis fig 4 for cluster number identification then we employed cluster analysis by fixing the cluster number to three table 5 our findings showed that five mlms mlpnn cart anf dnn xgboost belonged to cluster 1 based on accuracy cluster 2 with 6 mlms elm anfis pca knn ls svm and brf was formed by the ease of use parameter finally cluster 3 was comprised of four mlms ann svm wann and bpnn identified as complex algorithms the first group of mlms had high accuracy for drought prediction when dealing with complex heterogeneous data sets the second group of mlms is very efficient easy to understand and lacks data redundancy with low noise sensitivity these mlms are thought to reduce complexity in pattern recognition finally the third group of mlms strongly contributed to the hybrid model and are suggested for hybrid model development because they forecast drought with excellent stability and high representativeness 2 5 different case studies in drought characterization and forecasting we selected different case studies to provide an overview of mlm performance for drought assessment these case studies represent different geospatial regions along with different data sets units of analysis and ml algorithms we used our own data sets as well as highlighted examples from other studies for synthesising the use of mlms in this section mainly the application of mlms is discussed based on classification and regression tasks table 6 the first case study represents the use of the fft model mainly for performing a classification task for predicting agricultural drought risk the fft algorithm was also compared with its performance with lr cart and svm models to indicate that fft had high accuracy in both training and testing data sets fft provides accurate and robust forecasts based on frugality simplicity and prediction accuracy in case study 2 benlayneh et al 2013 used a different data driven model which performed a regression task to forecast spi 12 long term drought this case study exposed the skill of the wavelet coupled model with ann and svr models the wa ann model outperforms the wa svr model because wavelet transforms de noise the spi time series and also reduce the sensitivity in the case of the ann model however lima et al 2013 suggested that the svr model was more effective in the case of precipitation forecasting while witten et al 2011 found the ann model superior in the evaluation phase the rf model also showed better performance in remote sensing applications for mjo based vsdi prediction in the case of case study 3 as confirmed by park et al 2016 ke et al 2017 and im et al 2016 in their studies rf had higher predictive skill than other ml approaches i e dt brt etc for regression tasks from the above case study it is clear that the performance of each mlm is not fixed but depends on the size of the training data the transformation of the spatial data sets for feature learning and the depth of the algorithms chevalier et al 2011 zhang et al 2016 considering the above facts further we investigated the use of mlms to reveal drought based on the algorithm depth we selected one dl algorithm i e dfnn deep forwarded neural network and two ml methods i e xgboost and rf according to the depth of the algorithm dfnn xgboost rf to predict smdi soil moisture deficit index using multi sensor remote sensing data modis trmm parsiann cdr chirps and esa cc soil moisture for soil moisture related drought over south asia for each model we considered 75 n 5491 for training and 12 5 n 915 for each of the testing and validation data sets besides we used seven input variables spi 6 spi 3 spei 6 spei 3 pci precipitation condition index vci and tci as predictor variables and smdi as a response variable due to space and page limitations only the results of the models are discussed the detailed of the processing of the predicted and response variables can be found in narasimhan and srinivasan 2005 kogan 1995 kogan 1997 zhang and jia 2019 mckee et al 1993 and vicente serrano et al 2010 results presented in fig 6 suggest that the dfnn model demonstrated low rmse for each iteration for both training and validation data sets compared to the xgboost and rf models moreover the predicted smdi by the dfnn model showed high consistency r2 0 94 with the observed smdi from the perspective of the above case studies we have found that the dl algorithm has had significant success in the areas of drought monitoring prediction and characterization because multilayer deep neural networks have more depth compared to other models they can implement extremely complex functions on their input datasets and facilitate more learning from the training dataset 3 current research trends challenges and future directions of mlm for drought monitoring and forecasting 3 1 current research trends the mlm application has increased in recent years reflecting the efforts and advances of different parts of the scientific community including engineers neuro scientists environmental experts and other researchers worldwide a simple statistical analysis was conducted based on existing literature to analyze and observe the current status of mlms in the field of drought monitoring the number of ml models used in the existing literature in different subject areas were adopted from sun and scanlon 2019 and papers published in peer reviewed journals from 2004 to 2020 were considered sun and scanlon 2019 followed the scopus database and identified 1451 articles about ml models in different subject areas the survey results showed that earth and planetary science 26 3 ranked first in the use of mlms followed by engineering 15 3 and environmental science 12 6 fig 7 a our review found 76 papers in which mlm was used for drought monitoring out of a total of 1451 documents from 2007 to 2017 we manually extracted approximately 390 keywords from 73 articles and performed word cloud analysis fig 7 b the visual presentation confirms that the ann artificial neural nets terms focused more on drought monitoring and forecasting than other models the largest font size in the figure indicates terms appearing with higher frequency in the keywords of peer reviewed literature these results confirmed that ann was the most popular model which was consistent with the results of fig 7 c presents the usage frequency of different mlms as shown in fig 7 c cart pca svm and rf were less commonly used than the ann and hybrid models the higher use of the ann model is anticipated because it has non linear properties that make it more robust for identifying all possible interactions between predictors 3 2 challenges above we focus on the use of mlms in different subject areas emphasizing current trends of mlms in drought monitoring however certain features related to mlms create challenges for adopting ml in drought studies to address drought hazard mitigation strategies drought monitoring requires a long time series and multisource data where satellite data has been used for climatological by retrieving precipitation soil moisture and evaporation and ecological by assessing vegetation health perspectives the main challenges of ml in drought monitoring by remote sensing are as follows i multimodal data leads to a variety of geometric content which includes different imaging geometries geographic information system layers and multitemporal aspects ii high spatiotemporal resolution often creates problems for time series data analysis to retrieve drought information iii insufficient training samples for spatial data sets are usually unable to present seasonal variability for drought monitoring iv feature extraction to retrieve drought information with remote sensing may encounter noise and outliers making ml a challenging task v interpretability of ml model affects the visualization of drought hazards with hyperspectral data set having little to no variety vi drought prediction through ml model may be inconsistent due to the observational bias that arises from inadequate individual knowledge of sensor adequacy and data quality and vii complex data with high noise sources and high dimensional space yield uncertainty of ml model for drought forecasting therefore the challenges of using mlms for drought monitoring with remote sensing data can only be overcome by applying the latest advances in computation and high performance computation technology such as dl and data mining these advances in computation technology play an important role in disaster remote sensing by establishing a big data analytics platform that improves decision science through drought feature extraction and visualization mapping 3 2 1 future directions of mlm for drought monitoring and forecasting future research directions for drought monitoring and forecasting with ml include combining all possible sensors multi sensors and adaptive fusion techniques and using an environmental applications platform to quantify the sustainable implications of ecological changes induced by human activities such combinations will create a new boundary between environmental sensing and data science for multiscale drought disaster monitoring and modeling analysis to minimize the gap between data and models researchers have suggested numerous methods including assimilation synthesis and the fusion of data models ladeau et al 2011 presently recorded data and real time data availability allow us to integrate different data sets from various sources to observe water resource availability trends for drought disaster monitoring thus big data integration techniques would be effective for drought disaster mapping and real time forecasting similarly we proposed a geospatial modeling platform combining the sensor web and model web models adopted and modified by chang and bai 2018 to retrieve time series drought information and decision support systems fig 8 this geospatial platform integrates the data fusion model with ml and data mining applications to observe and map drought disasters the platform is therefore helpful in the environmental decision making process for adopting a mitigation strategy though big data for hydro informatics still faces some challenges its application is perfect for drought visualization and forecasting in different studies for example zhai et al 2016 and verdin et al 2015 used a big data platform model for integrating multi sensors and other data sources to monitor hydrological disasters resulting in more reliable and robust forecasting a knowledge based system with big data platforms including ml and data mining will magnify the primary significance of drought disaster analysis for environmental decision making 4 conclusions this review provides a data based investigation of mlms used in drought monitoring and forecasting highlighting data mining and big data approaches for environmental decision making we believe our article provides useful insight into the field of ml and hope it will motivate further research exploring ml applications in drought disaster monitoring in this article we have concisely introduced and systematically analyzed a number of typical ml and dl methods that may be used with remote sensing data for drought forecasting different types of drought meteorological agricultural and hydrological forecasting using mlms containing both supervised and unsupervised algorithms have been summarized and discussed our survey evaluates current trends in ml algorithm application for drought monitoring including research challenges when using remote sensing data in summary the following inclusive statements can be formulated according to the findings drought forecasting using mlm is in the early phase of adoption the selection of mlm used for drought prediction with remote sensing data sets is primarily dependent on several factors including the amount of training data dimensional reduction forecasting accuracy and computational competency when comparing the advantages of mlms the ml model outperformed the classical statistical model for hypothesis space and extensive data sets by encouraging a range of possible solutions for a given problem for model performance in drought forecasting however issues of overfitting and underfitting were revealed these issues are related to model complexity and the volume of data in the training data set nevertheless ml and hybrid models had superior drought forecasting performance compared with statistical time series and probabilistic models although researchers encountered several challenges related to using the ml model these can be minimized by applying dl and data mining approaches thus the big data analysis platform enables the implementation of the latest and highest performance computational technology for decision making in the field of disaster remote sensing our present findings confirm that there are few big data applications for observing and mapping drought disasters therefore future research should be conducted to develop a geospatial platform that merges the data fusion model with ml and data mining techniques in the form of big data applications declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this study was supported by the cas strategic priority research program grant no xda19030402 the natural science foundation of china no 31671585 no 41871253 key basic research project of shandong natural science foundation of china no zr2017zb0422 and taishan scholar project of shandong province no tsxz201712 the first author would like to thanks cas twas president s fellowship program for the support during the study we are very grateful to the editor and anonymous reviewers for their valuable comments and suggestions appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105327 
25633,artificial intelligence ai techniques have substantially changed the research paradigm in the field of air quality forecasting due to their powerful performance considering the improvement in the availability of air quality data and the rapid proliferation of ai techniques it is necessary to comprehensively and quantitatively review the development of air quality forecasting with ai techniques during the last two decades 2000 2019 by scientometric and content analysis first an overview of the relevant countries institutions authors journals and papers is presented then the research hotspots and frontier evolution are explored by adopting reference co citation analysis and keyword co occurrence analysis furthermore this study conducts a content analysis to investigate current topical interests to identify research gaps and propose future research directions the analytical framework and the findings provide helpful insights into the prospects in air quality forecasting with ai techniques keywords air quality forecasting artificial intelligence machine learning scientometrics content analysis 1 introduction with economic development and accelerating industrialization the quality of human life is gradually improving however this improvement relies almost entirely on energy consumption and pollutant emission resulting in serious air pollution problems especially in developing countries according to a report by world health organization who 9 out of 10 people around the world breathe air containing high levels of pollutants more than 7 million people die every year from various diseases caused by air pollution including stroke heart disease lung cancer chronic obstructive pulmonary diseases and respiratory infections world health organization 2018 accurate and reliable air quality forecasting which is the basis for environmental authorities to take effective treatment measures plays an important role in improving air quality bai et al 2018 du et al 2020 moreover based on timely air quality forecasting residents can adjust their activities and prepare respiratory protective equipment in advance thereby mitigating the impact of air pollution on health and economy li et al 2019b in recent years with the improvement in data availability and the development of forecasting methods air quality forecasting has gradually become one of the hotspots in air pollution research beckerman et al 2013 zhou et al 2014 currently popular air quality forecasting methods include deterministic methods e g atmospheric diffusion models and chemical transfer models and statistical methods e g autoregressive integrated moving average model arima multiple linear regression mlr and various ai models although deterministic methods have been proven to provide valuable insights and sufficient interpretability for the diffusion mechanism of air pollutants they are computationally expensive and their forecasting results might be inaccurate due to the use of sophisticated prior knowledge and various usage constraints li et al 2017b stern et al 2008 notably among statistical methods ai models not only effectively capture the nonlinearity and high order interactions between the pollutant concentration time series and predictor variables but also require less domain expertise and resources as a data driven method yao et al 2018 therefore an increasing number of scholars have developed various powerful ai techniques to further advance the literature on air quality forecasting niu et al 2017 sun and sun 2017 there have been a few attempts to review the previous research in the field of air quality forecasting with ai techniques for example rybarczyk and zalakeviciute 2018 systematically review machine learning approaches for air quality modelling and provide the time distribution geographical distribution research classification and forecasting methods by investigating 46 papers on this topic cabaneros et al 2019 focus on previous prediction methods with artificial neural networks anns for modelling and forecasting air quality and comprehensively evaluate the latest developments regarding this theme moreover their results provide a normative guide for the ann model building process of follow up research however these previous review studies on the application of ai techniques in the field of air quality forecasting are mostly based on subjective screening and inductive analysis these traditional literature reviews on the one hand cover too few papers and too short time spans to provide a global analysis yu et al 2020 on the other hand they rely heavily on the authors expert knowledge of the domain and are often subject to bias huang et al 2019b scientometrics or named bibliometrics introduce quantitative methods to describe evaluate and monitor published research which contributes to the discovery of intellectual structures social networks and topical interests koseoglu et al 2016 zupic and čater 2014 in recent years scientometrics has been extensively employed to look back journals li and hale 2016 martínez lópez et al 2020 and professional fields dhital and rupakheti 2019 zhong et al 2019 1 1 according to our search in april 2020 more than 5768 scientometric papers have been published since 2015 of which 1133 in computer science interdisciplinary applications 538 in environmental sciences and 334 in management in particular scientometric papers related to air quality or air pollution are shown in table 1 importantly no scientometric research on the application of ai techniques in air quality forecasting has been conducted thus far which is exactly what we strive for in this work meanwhile given the intrinsic deficiencies in scientometric analysis 2 2 nath and chowdhury 2021 and xu et al 2018a indicate that scientometric analysis lacks topical details or in depth insights zupic and čater 2014 suggest that scientometric analysis are not a substitute for but a complement to traditional review such as extensive reading and synthesis we adopt the most advanced review paradigm kazemi et al 2019 maçaira et al 2018 nath and chowdhury 2021 and conduct a top down literature review by integrating scientometric and content analysis this study presents an extensive and comprehensive quantitative review as well as an in depth analysis of the field of air quality forecasting with ai techniques during the last two decades 2000 2019 in this article the scientometric software citespace is used to map cooperation networks co citation networks and co occurrence networks combining the classical basic statistical indicators we attempt to objectively and accurately review the field of air quality forecasting with ai techniques from the perspectives of specific analysis and research hotspot evolution accordingly specific analysis involves productive and significant countries institutions authors and their cooperation networks as well as journals with large numbers of publications and papers with large numbers of citations research hotspots and frontier evolution are extracted from the results of the reference co citation cluster keyword co occurrence analysis and content analysis in summary the study is driven by the following research questions 1 what is the efficient search strategy according to the characteristics of this field 2 how is this field of research organized in terms of countries institutions authors journals and papers 3 what are the research hotspots how do the frontiers evolve in this field and what should we pay attention to in future research our advanced review paradigm of integrated scientometric and content analysis provides a comprehensive yet in depth review the search strategy we propose for this field can benefit relevant researchers and practitioners our findings help researchers identify the current research advances competing groups and possibilities for cooperation and build their own work in this active field it is foreseeable that with the improvement in the availability of air quality data and the rapid proliferation of ai techniques the field of air quality forecasting with ai techniques will continue to develop steadily it is necessary to summarize and offer helpful insights in this field in a timely manner the rest of this study is organized as follows section 2 describes the process of search strategy paper screening and analysis tools and methods on this basis the research framework of this study is given section 3 provides a comprehensive analysis of the countries institutions authors journals and papers in this field the hotspots and frontier evolution are presented in section 4 finally the conclusions and limitations are summarized in section 5 2 research methodology 2 1 search strategy the previous papers examined in this study are collected from clarivate analytics web of science wos database wos is recognized by the academic community as the most authoritative database and it incorporates articles from more than 20 000 peer reviewed and high level journals from 1900 to the present and wos contains the essential metadata to realize scientometric analysis carvalho et al 2013 pan et al 2019 sassetti et al 2018 specifically the two largest subdatabases the science citation index expanded sci e and the social sciences citation index ssci are considered for collecting the related papers it is clear that air quality forecasting with ai techniques is an interdisciplinary study so we do not limit the scientific field of the papers which attempt to cover a larger number of results inspired by el alfy and mohammed 2020 we propose four sets of search keywords related to air quality forecasting ai techniques and exclusions table 2 lists the search keywords in each set in detail these keywords are not only derived from the search strategy of the previous reviews but also added gradually in the search process it is pointed out that searching for keywords in wos is not case sensitive but we still search for them in the form commonly found in papers in addition denotes a fuzzy search for example air pollut means air pollution and air pollutant while predict means predict predicted and prediction in particular there are many words prefixed with co which seriously interfere with the retrieval results therefore co is adopted to exclude such papers consistent with cabaneros et al 2019 we only consider ambient outdoor air quality forecasting and exclude papers related to indoor air quality we choose the advance search feature of wos to create queries by using field tags keywords boolean operators or and and not and parentheses the or operator is applied between the keywords that belong to the same set whereas the and operator is used to relate the different sets the not operator represents the keyword set to be excluded the specific search strategy is set as follows ts air pollut or air quality or atmospheric pollut or aqi or air quality index or pm2 5 or pm10 or o 3 or o3 or no2 or co or so2 and predict or forecast and artificial intelligence or ai or machine learning or ann or artificial neural network or neural network or bp or rbf or rnn or cnn or knn or deep learning or svm or elm or lstm or random forest or decision tree or ensemble or bagging or sae or boosting or adaboost or bayesian network not ts indoor air quality or iaq or co where ts means a full search of the selected keywords in titles abstracts and keywords of the articles in fact our search motivation was to ensure that the data are as comprehensive as possible given the diversity of language expression nevertheless this strategy also introduces too many irrelevant papers which need to be further screened 2 2 paper screening after setting the time span as 2000 2019 we obtained 1224 english articles or reviews in the field of air quality forecasting with ai techniques other types of documents including proceedings papers book chapters reprints and editorial material are not considered because they are not rigorously peer reviewed huang et al 2019b and most of the proceedings papers end up being published as articles or reviews next we select the appropriate papers by skimming through the titles and abstracts the appropriate papers need to explicitly involve the use of ai techniques to forecast environment outdoor air quality or air pollutant concentrations in addition papers on the following topics are also excluded from our selection 1 forecasting of health problems caused by air pollution 2 forecasting of air pollutant emissions including automobile exhaust and boiler gas 3 social studies of non computational models and 4 indoor air quality forecasting overall 643 effective papers are obtained by filtering under the above manual selection criteria and are used for scientometric analysis all data collection was completed in april 2020 2 3 analysis tools and methods there are currently more than 10 kinds of scientometric mapping software available for researchers such as citespace chen 2004 2006 vosviewer van eck and waltman 2010 and histcite garfield et al 2003 this study explores the knowledge distribution and evolution of air quality forecasting with ai techniques during the past two decades through citespace which is developed by professor chen of drexel university citespace presents the structure and patterns of scientific research by systematically creating various accessible knowledge maps zhong et al 2019 so it is more helpful than other scientometric mapping software to understand the meaning of knowledge maps in addition citespace allows the user to adjust the style of the network layout the colour size and location of nodes and labels which increases the possibility of improving the visualization quality there are three measures of nodes in the knowledge maps generated by citespace the frequency the betweenness centrality and the burst strength cui and zhang 2018 zhong et al 2019 the frequency has different meanings for different types of knowledge maps the number of published papers in the research collaboration network the number of co citation in the co citation network and the number of co occurrence in the co occurrence network meanwhile the frequency is represented by the size of nodes in citespace the betweenness centrality bc developed by freeman 1977 is calculated by eq 1 where g pq is the number of shortest paths between nodes p and q and n i pq is the number of shortest paths through node i in the paths above the nodes with high bc are usually the structural holes in the network the bc is represented by the size of the purple circle outside nodes in citespace only nodes with bc of at least 0 1 are displayed the burst strength bs is the index that measures the nodes citation burst in the burst detection where the algorithm was proposed by kleinberg 2003 1 b c i p q i n p q i g p q this study conducts a specific analysis of countries institutions authors journals and papers under the theme of air quality forecasting with ai techniques and discusses the hotspots and frontier evolution in this field the scientometric research methods involved in this study are as follows 1 cooperation network analysis including country institution and author cooperation network representing the macro meso and micro level respectively 2 co citation network analysis the reference co citation network which reveals the knowledge bases and paradigms of the field small 1973 3 cluster analysis based on the reference co citation network the references are automatically classified and labelled which provides insights for an underlying knowledge structure huang et al 2019b 4 co occurrence network analysis the keyword co occurrence network that reflects the evolution of research topics and hotspots he 1999 additional information through the basic statistical analysis is provided in this paper in summary the research framework of this paper is shown in fig 1 3 specific analysis 3 1 annual and discipline publication statistics the annual distribution of the 643 collected publications and their citations is shown in fig 2 the number of publications and citations has maintained almost the same growth pattern in the last twenty years overall the temporal distribution of publications and citations can be divided into two phases during the first phase from 2000 to 2013 the number of both increased linearly and slowly but with fluctuations in some years e g 2005 2006 during the second phase from 2014 to 2019 the number of both exhibits exponential growth reaching unprecedented 175 publications and 2894 citations in 2019 predictably this steady growth will continue as more data become available and more advanced ai techniques emerge air quality forecasting with ai techniques is a typical interdisciplinary study the distribution of the number of publications in the major disciplines is given in fig 3 considering that it allows a publication to belong to more than one discipline in wos the discipline that accounts for the most publications is environmental sciences with 387 publications they are followed by meteorology atmospheric sciences engineering environmental computer science artificial intelligence engineering electrical electronic and computer science interdisciplinary applications with 166 71 61 39 and 34 publications respectively 3 2 productive countries and their cooperation network international cooperation allows the sharing of knowledge across countries and expands academic influence this is why we analyse the productive countries first this study gives a histogram of the distribution of publications of the major producing countries from 2000 to 2019 the four most productive countries namely china the united states italy and the united kingdom produced 71 54 of the total publications in this field as shown in fig 4 china has established its leading position in this field since 2014 while the number of publications from other countries has fluctuated next we import the country information of all the authors into citespace to generate the country network as shown in fig 5 we adjust the number of nodes and edges in the cooperation network by changing the values of nodes and lrf in citespace to spread the network as flat as possible we move the location of the nodes so that the connection between the nodes can be displayed the country cooperation network contains 41 nodes and 70 links the size of nodes and labels represents the total number of articles published in the country the cooperation period between countries from past to present is shown with the link colour transition from a cool tone to a warm tone the thickness of the line represents the intensity of cooperation although china has the largest number of publications it does not play a bridging role in the network while the betweenness centrality of the united states italy and germany is greater than 0 1 contributing greatly to the development of the cooperation network the cooperation network of countries exhibits significant regional characteristics with asian countries and european countries gathering together and the united states acting as an intermediary this reflects the fact that air quality forecasting is not a country specific issue but a regional and global challenge finally we present additional information about the top 15 most productive countries in table 3 several countries have a high number citations per paper tp tc including greece tp tc 35 14 italy 28 73 france 27 69 canada 26 74 and germany 23 38 these values reflect the overall high quality of publications the united states is the first country to enter the field and south korea is the last all countries continue to be productive in this field until 2019 except for spain until 2018 3 3 prolific institutions and their cooperation network institutional cooperation networks can be beneficial by identifying exchange relationships and cooperation patterns in this field fig 6 shows the largest institutional cooperation network as well as two separate small networks the institutional cooperation network contains 133 nodes and 207 links some institutions received high frequency and betweeness centrality scores and these are chinese academy of sciences frequency 35 centrality 0 21 nanjing university of information science technology 14 0 13 peking university 11 0 15 nasa 10 0 11 and emory university 11 0 07 other institutions that are more frequently involved in the cooperation networks but do not occupy bridging roles and include dongbei university of finance economics 12 0 00 lanzhou university 11 0 02 and national center atmospheric research 9 0 05 comparing the results of the country cooperation network see fig 5 we find that although china has not played a leading role in international cooperation it has formed a huge domestic scientific research system centred on chinese academy of sciences moreover the institutional cooperation network in this field is almost a recent development as seen from the fact that the edges are almost warm table 4 displays information about the top 15 most prolific institutions the most prolific institutions are almost all from china and the united states except european commission joint research center from italy and univ de santiago de chile from chile three institutions city univ hong kong univ de santiago de chile and shanxi univ stand out because their citations per paper tp tc are all over 45 univ de santiago de chile began its research in 2000 while nanjing univ informat sci technol and emory univ entered the field in 2017 however the latter two universities have published 14 and 11 papers respectively in just three years it should be noted that univ of calif system tp 17 includes univ of calif davis 5 univ of calif los angeles 5 univ of calif berkeley 5 univ of calif irvine 3 univ of calif san diego 1 and univ of calif san francisco 1 nasa tp 13 includes nasa goddard space flight center 11 nasa jet propulsion laboratory 1 nasa ames research center 1 and nasa langley research center 1 3 4 influential authors and their cooperation network author collaboration network allows us to find the core authors in this field and determine the intensity of their collaboration chen 2017 to explore the influential authors in this field we first draw the cooperation network distribution of all the authors in the scientometric database as shown in fig 7 a the cooperation network distribution shows that there are no large scale partnerships among scholars active in the field of air quality forecasting with ai techniques but several small networks have emerged for example the largest author collaboration network is found in fig 7 b which is centred on yang liu a professor at emory university moreover we find that the largest author collaboration network was formed mainly in recent years in table 4 we rank the top 10 authors that have published more than 5 papers in this field liao et al 2019 point out that the author s name abbreviation phenomenon especially the name of the eastern scholars causes an inflated frequency with this in mind we cross check the authors names and publications and accurately identified each author s full name country and affiliation consistent with the institution see table 4 most authors come from the united states and china except patricio perez from chile and stefano galmarini from italy wenjian wang tops the list with 68 5 citations per paper but she has not published in the field since 2008 wang is followed by weizhen lu patricio perez and jianzhou wang with 49 60 46 36 and 35 86 citations per paper respectively notably four scholars yang liu yu zhun xia meng and alexei lyapustin have earned their place in the international academic community in just a few years consistent with the author cooperation networks see fig 7 the most influential authors have a low betweenness centrality 0 00 0 01 3 5 source journal analysis research in journals can help junior researchers and policy makers narrow the scope to obtain relevant information table 6 lists the top 15 high yield journals publishing a large number of studies in the field of air quality forecasting with ai techniques we list some indicators of these journals including citations per paper citations per year publication time span if from jcr and h5 from google scholar atmospheric environment ranks far ahead of other journals with 75 publications and 3521 citations four journals are ranked among the best in terms of both citations per paper tc tp and citations per year tc y and these are atmospheric environment tc tp 46 95 tc y 176 05 environmental modelling software 47 69 40 16 environmental science technology 33 67 36 07 and science of the total environment 27 92 43 63 which reflects the high quality of the papers published in these journals environmental science technology is the most influential journal according to if2018 5yif and h5 3 6 prominent papers analysis an analysis of papers can help us identify which the papers that have influenced the progress of research in this field and have captured scholars attention we first carry out the burst detection on the references of the analysed papers and table 7 displays the top 10 references with the strongest citation bursts during 2000 2019 the burst detection takes into account both the number of citations and the length of the period over which the citations occur huang et al 2019b the table is arranged according to the start of the references burst these milestone papers are all published before 2010 and citations increased dramatically within a year lasting 8 years until 2016 some references obtain high burst strength including grivas and chaloulakou 2006 strength 16 21 gardner and dorling 1999 13 05 and pérez et al 2000 11 75 these papers with burst citations must have hit the hotspot of research at that time tracking frequently cited papers not only helps scholars quickly locate classic research in the field but can also facilitate the subsequent identification of research directions pan et al 2019 table 8 provides a list of the top 20 most cited papers and their information on international and inter institution collaboration the citation data listed in the table are from the wos core collection kukkonen et al 2003 is the top cited article to date with a total of 202 citations patricio perez is the only author with 2 publications in table 5 three quarters of the most cited papers are published before 2008 the remaining five papers are published between 2013 and 2016 and deserve academic attention due to the large number of citations obtained in a short time only one paper is written by a single author eight publications are the result of international collaboration and 11 are written by single country authors moreover there are 11 inter institutionally collaborative publications and 8 single institute publications 4 hotspots and frontier evolution 4 1 reference co citation analysis the research hotspots and frontiers are reflected by the papers actively cited by scholars and can be used to describe the dynamic evolution of the field through analysing the co citation references this study objectively explored how the underlying knowledge base of this field evolves to achieve the balance between legibility and detail for visualization we select the cited references published within 8 years prior to each paper for co citation analysis for example if a paper in our scientometric database was published in 2005 its references from 1997 to 2005 are used for visualization as a result the reference co citation network contains 587 nodes and 5562 links as shown in fig 8 the node size represents the co citation frequency of each reference the colour change of the links reveals the timespan to explore how knowledge evolves and migrates temporally we perform a cluster analysis of the co citation network and 8 main clusters that could reflect the patterns and themes are displayed in fig 8 the log likelihood ratio llr algorithm dunning 1993 is used to label the clusters by extracting nominal terms from the keywords in references and the keywords supplemented by wos the size and sequence number of the label represents the size of the cluster that is the number of references contained in the cluster more information about the clusters is given in table 9 as seen from fig 8 and table 9 the research hotspots of air quality forecasting with ai techniques during the past 20 years are 4 neural network model mean year 1998 2 hourly o3 concentration 2005 3 new ensemble design 2006 5 cellular neural network 2008 7 artificial neural network 2010 6 land use regression model 2011 0 concentration forecasting 2013 and 1 satellite data 2014 in order the silhouette value of each cluster is above 0 5 which means that the clustering result is reasonable chen et al 2010 the major co citation associations occurred after 2010 which is consistent with the literature explosion of the same period 3 new ensemble design and 2 hourly o3 concentration are two of the longest spanning clusters while various neural networks are the most dominant methods in the field given that air pollution monitoring stations can only cover a small area it is unreasonable to use such local measurements to assess the pollution exposure of residents in an entire area 6 land use regression model incorporates spatial variables such as traffic topography and population density obtained through gis and is then applied to a large number of unmonitored sites in the study area adams and kanaroglou 2016 hoek et al 2008 although land use terms are usually time invariant and are limited in modelling short term variations di et al 2016 they are still an important forecasting variable on the spatial scale adams and kanaroglou 2016 hu et al 2017 in contrast 1 satellite data with comprehensive spatiotemporal coverage have recently been widely used for air quality forecasting at a spatiotemporal scale satellite data mainly refer to aerosol optical density aod data which are generally proven to be related to pm2 5 concentrations the main drawback of satellite data is that they may be missing for meteorological reasons but a beneficial characteristic is that multiple sources can complement each other hu et al 2017 citespace not only identifies the research themes and trends through clustering but also displays the representative of each cluster this study can help to understand the research themes by briefly reviewing these representative works zhu and hua 2017 gardner and dorling 1998 systematically review the application of anns especially the multilayer perceptron in atmospheric science the application of the multilayer perceptron mainly includes prediction function approximation and pattern classification grivas and chaloulakou 2006 extend the ann model to predict the hourly pm10 concentration in athens and compare their predicted results with those of other studies for predictions of hourly no2 and o3 concentrations mckeen et al 2005 pioneer real time ensemble ozone forecasting by combining seven different forecasting models kurt et al 2008 develop an online air pollution forecasting system in istanbul using a neural network and they increase the forecasting performance by designing a cumulative method optimizing the number of training days and choosing the day of the week as a parameter a neural network with stochastic variables for air quality prediction is presented by russo et al 2013 this method aims to reduce the number of input variables significantly under the premise of guaranteeing the prediction ability of the neural network model li et al 2011 investigate the strengths and limitations of satellite data in air quality forecasting feng et al 2015 apply an air mass trajectory based geographic model and wavelet transformation to improve the performance of the ann model for air quality forecasting hu et al 2017 propose a multivariate random forest model covering multiple predictors including aerosol optical depth aod data meteorological fields and land use variables to forecast pm2 5 concentrations in the united states these representatives potentially reflect the research hotspots at that time so they have been widely cited and become structural holes in the reference co citation network 4 2 keyword co occurrence analysis keywords not only represent the core issues discussed in the literature but also reflect hot research topics for a certain time period keyword co occurrence analysis kca knowledge maps determine hot research topics based on high frequency keywords yang and meng 2019 we select a group of keywords with high co occurrence frequency and plot the time zone view of the keyword co occurrence network as shown in fig 9 which clearly shows the evolution of the knowledge domain in the time dimension the time zone view aggregates the keywords that appear for the first time in the same year into the same time zone the size of nodes represents the co occurrence frequency of the keywords the links between the keywords indicate the inheritance of the literature the keywords are distributed almost evenly across time zones except for the gaps between 2010 and 2012 from the time zone view we can deduce the evolution of methods from neural networks to diversified machine learning including random forest and support vector machine this can be attributed to the fact that open source packages provide great convenience for invoking machine learning algorithms and building forecasting model frameworks cabaneros et al 2019 we also find changes in forecasting indicators from o3 and no2 to fine particulate matter pm10 and pm2 5 this may be because haze has become the most prominent pollution problem especially in developing countries such as china while other air pollutants have been effectively treated zhao and li 2019 to study the distribution and evolution of keywords more accurately we manually classify the keywords that appear more than 10 times in our scientometric record table 10 displays the five types of keywords 0 forecasting indicator 1 forecasting method 2 forecasting area 3 forecasting data and 4 health related words meanwhile some generic keywords with no real meaning are eliminated for instance prediction forecast forecasting and modi specifically most of the keywords gather in 0 forecasting indicator and 1 forecasting method the characterization of pollutants is becoming increasingly detailed for example from the initial pm2 5 to ground level pm2 5 and pm2 5 concentration ai techniques have evolved from anns to other machine learning algorithms and hybrid and ensemble methods jiang et al 2017 athens china and the united states in turn have become hotspot areas for air quality forecasting with ai techniques as for the data sources satellite derived aerosol optical depth data can effectively expand the range of air quality forecasting by its comprehensive spatiotemporal coverage hu et al 2017 finally the ultimate goal of air quality forecasting is to protect the health of residents through timely early warning and control measures exposure and mortality are high frequency words that reflect health 4 3 content analysis reference co citation analysis and keyword co occurrence analysis generally provide an overall picture of the evolution of hotspots and frontiers in air quality forecasting with ai techniques given the intrinsic deficiencies in scientometric analysis we follow kazemi et al 2019 and nath and chowdhury 2021 to undertake an in depth content analysis to track salient research frontiers and propose feasible and promising directions for future research to efficiently and rationally process the large dataset of 643 papers initially collected we first select the most important studies for targeted analysis and discussion the key studies of identification include 1 the top 10 references with the strongest citation bursts 2 the top 20 most cited papers 3 the representative papers of each cluster in the reference co citation network and 4 the papers published in journal citation reports jcr q1 source journals from 2017 to 2019 among them the first three are determined according to scientometric indicators almost all of which are early literature while recent important literature cannot be selected through scientometric indicators because there has not been enough time to accumulate citations therefore we follow xu et al 2018a to add papers published in high level journals jcr q1 source journals are selected for this study over the last three years to capture and track recent advances in this field another reason to support our identification of recent key studies is publication statistics we observe 314 papers published in the last three years almost half of the total published in the last 20 years and 140 papers published in jcr q1 source journals finally a final dataset of 178 key studies is determined next we carefully read these studies and summarize the forecasting data ai based forecasting techniques and forecasting designs of each paper 4 3 1 forecasting data and identified study types air quality forecasting research with ai techniques can be roughly divided into two categories according to the different forecasting data namely the forecasting of urban air pollutant concentrations and the forecasting of air pollution exposure on a large spatial scale the former mainly uses monitoring site data and meteorological data to focus on time series forecasting while the latter aims to capture the spatiotemporal variability of air pollutants by introducing a variety of auxiliary datasets including satellite derived data chemistry transport model ctm data meteorological data land use information data topographic data source emission data and socio economic data a limited number of air pollution monitoring sites can only cover key areas such as city centres due to high construction and maintenance costs 3 3 according to honarvar and sami 2019 the average construction and maintenance costs of air pollution sites are 200 k dollars and 30 k dollars per year respectively the high precision and reasonable distribution of monitoring stations can measure the overall pollution level of cities to a certain extent qi et al 2019 however the sparse and uneven spatial coverage of monitoring sites makes it difficult to accurately estimate and forecast residents air pollution exposure especially for suburban and rural residents who have less access to health care facilities and poorer socio economic conditions stafoggia et al 2019 consequently a large number of studies have investigated the effectiveness of satellite data with high space coverage in forecasting exposure chen et al 2019 di et al 2016 hu et al 2017 our content analysis reveals four research trends in this academic niche the impact of missing aod values and the corresponding corrective measures have been preliminarily explored various filling methods including random forest have been developed in the literature bi et al 2019 chen et al 2019 huang et al 2018 zhang et al 2018 in addition few studies have verified the potential of other satellite derived data to replace aod data with serious missing liu et al 2019 exploring new predictive variables is welcome recent attempts include atmospheric stability bai et al 2019 and spatial lag variables wang et al 2019b although bai et al 2019 find that the inclusion of atmospheric stability indicators into the pm2 5 modelling framework contributes little to the forecasting accuracy these useful attempts could benefit air quality forecasters it is an important and challenging task to predict the pollutant concentration of regional air pollution events for example several scholars from north america emphasize the importance of pm2 5 and ozone exposure forecasting in the united states and canada during wildfire seasons watson et al 2019 yao et al 2018 zou et al 2019 feng et al 2019 explore the challenge of pollutant emissions from straw burning in southern china to air quality forecasting despite the increasing availability of air pollution data overall high quality forecasting studies are lacking in data poor regions where pollution levels are often high gao et al 2018 geng et al 2018 it is a promising direction to develop air quality forecasting models in areas with no monitoring sites or sparse monitoring sites which will alleviate the shortage of air quality forecasting in many developing countries furthermore dense three dimensional buildings and streets in large cities hinder the diffusion of air pollutants and lead to sharp changes in air pollution levels on fine spatial scales huang et al 2019a yang et al 2018b therefore measurements from monitoring sites cannot reflect the true impact of air pollution at the human level mihăiţă et al 2019 more recent studies have explored novel data sources as supplements or substitutes for monitoring site data including handheld or vehicle mounted mobile sensor data huang et al 2019a unmanned aerial vehicle measurements yang et al 2018b image data uploaded by residents xi et al 2019 and real time traffic data provided by google maps honarvar and sami 2019 although the application of these new data in air quality forecasting with ai techniques is still in its infancy with the development of smart cities and the progress of big data techniques the data have a great potential to improve forecasting research and practical application in the future 4 3 2 the rise and fade of ai forecasting techniques ai techniques for air quality forecasting have evolved dynamically over the past 20 years with some methods increasingly used and others fading out of sight due to insufficient data scale and computing resources as well as the difficulty in setting the initial parameters and network structure the improvement of early neural network forecasting performance has been severely limited faganeli pucer et al 2018 after that support vector machines svm have gradually emerged due to their excellent learning ability on small sample datasets sun and sun 2017 because of its robustness to noise variables as well as its simplicity and insensitivity to parameter settings random forests are becoming increasingly popular especially in data intensive forecasting studies tang et al 2019 however such simple models ignore the relationships between different time steps of time series data li et al 2017b recurrent neural network rnn can efficiently process sequence inputs by setting memory capacity and the variants of rnn namely long short term memory network lstm and gated recurrent unit gru are reported to be more suitable for modelling long short temporal dependency by solving the exploding or vanishing gradient problems li et al 2017b lin et al 2021 moreover convolutional neural network cnn and graph neural network gnn have been examined to extract higher order spatial features of multiple sites qi et al 2019 wen et al 2019 these advanced deep learning models are composed of multiple processing layers to learn representations of data with multiple levels of abstraction they can achieve end to end learning without complex feature engineering sun et al 2019 the pioneering study on ai techniques performance comparison in air quality forecasting points out that no model can universally outperform all others in terms of forecasting accuracy watson et al 2019 xu et al 2018b the two major approaches to improve the forecasting performance of a single model are ensemble models and hybrid combination models considering that previous studies tend to confuse these two concepts we provide a detailed clarification here ensemble models which emphasize that multiple models can learn the inherent features of the targets from various perspectives can achieve better forecasting performance than a single model for example di et al 2019 develop a generalized additive model to ensemble the individual forecasting of gradient boosting neural network and random forests thus providing a superior model performance for pm2 5 forecasting of the entire contiguous united states hybrid models combine different models into a multi stage model to overcome their individual shortcomings the common hybrid model types are as follows 1 combining ai forecasting models with data pre processing approaches and or optimization methods such as various decomposition methods yuan et al 2019 outlier detection and correction wang et al 2020a clustering algorithms li et al 2019a and intelligent optimization algorithms xu et al 2017b 2 combining different ai forecasting models such as cnn lstm model wen et al 2019 lstm cnn model yang et al 2019 and gnn lstm model qi et al 2019 which are designed to extract and learn the spatiotemporal feature of air pollution concentration ensemble and hybrid models improve forecasting performance at the expense of computation this increased computing cost is negligible given the power of modern computers and cloud computing technology therefore we expect both ensemble and hybrid models to play an increasingly important role in air quality forecasting ensemble and hybrid models are also relevant to the recent requirement for reliable forecasting zhu et al 2018 one concern about ai techniques is the instability of forecasting performance ensemble models can effectively avoid the potentially unstable performance of individual models xiao et al 2018 hybrid models can incorporate multi objective optimization algorithms to improve forecasting accuracy and stability simultaneously du et al 2020 zhu et al 2018 another concern about whether ai techniques can make reliable forecasting is the interpretability of ai techniques there have been preliminary explainable attempts about variable importance measures in the air quality forecasting field such as random forests and neural networks with attention mechanisms grange et al 2018 lešnik et al 2019 in the future interpretable ai techniques will better quantify the impact of individual covariates on forecasting outcomes which allows for improved decision making when considering the implementation of prevention mechanisms 4 3 3 forecasting for better decisions emergent forecasting designs the primary objective of using ai techniques to forecast air quality is to improve the accuracy of forecasting air pollutant concentrations and this objective is further enhanced by powerful deep learning models however even though scholars claim that forecasts can help stakeholders such as authorities and the public make prevention decisions it is questionable whether these studies can actually be deployed to make a difference in the real world if only the average concentrations of air pollutants are predicted fortunately a variety of forecasting designs have emerged to help represent forecasting in a more comprehensible way so that authorities and citizens can make more beneficial use of the forecasts aznarte 2017 we can group these emerging research designs into the following two categories 1 develop probabilistic forecasting models that provide uncertain information compared with traditional point forecasting of air pollutant concentration values probabilistic forecasting focuses on the future probability distribution of concentrations abdullah and khalid 2012 in the field of air quality forecasting probabilistic forecasting has three main forms namely threshold forecasting interval forecasting and density forecasting concretely threshold forecasting aims to determine whether air pollution levels exceed air quality limits in the coming hours abdullah and khalid 2012 interval forecasting can provide the forecasting ranges for air pollutant concentrations at certain confidence levels which are also known as upper and lower bounds li and zhu 2018 wang et al 2020b by contrast density forecasting produces interval forecasts at any confidence level by obtaining continuous probability density functions faganeli pucer et al 2018 wang et al 2017b hence density forecasting can be easily transformed into threshold forecasting and interval forecasting these probabilistic forecasts can quantify the range of observed data changes caused by uncertain factors and provide more uncertain reference information for decision makers to help them formulate and implement air quality protection measures we hope that more air quality forecasting research will move into the fascinating and unexplored world of probabilistic forecasting 2 develop various air quality early warning monitoring systems including not only the forecasting module but also evaluations and other modules early warning monitoring systems can be roughly divided into two types according to the way in which different modules are combined the first type first evaluates the air quality degree to identify major pollutants followed by targeted forecasts li and zhu 2018 xu et al 2017a yang and wang 2017 the identification of major pollutants can greatly reduce the computing resources of subsequent forecasting without affecting the evaluation of air quality which is conducive to creating effective and practical air quality warning monitoring systems tailored for cities the second type evaluates the air quality degree based on all air pollutants forecasts li et al 2019b the most commonly used air quality evaluation tool is the air quality index aqi which simplifies the concentration of various air pollutants into a single conceptual numerical form and represents the air quality degree by grading wang et al 2017a zhu et al 2018 with the help of the aqi citizens without professional knowledge of atmospheric science can easily understand the current air quality nevertheless the aqi is calculated according to defined numerical rules while the air quality degree as a relatively vague concept lacks clear boundaries to grade wang et al 2019a as a result researchers have developed a variety of air quality evaluation methods based on fuzzy logic abdullah and khalid 2012 gorai et al 2015 li et al 2019b wang et al 2019a in addition we find that the authors in the field of air quality forecasting with ai techniques mostly come from the disciplines of statistics computer engineering and environmental geography at universities and institutes 4 4 the analysis of influential authors in section 3 also supports this finding in the future the field should attract more scholars from the disciplines of management and economics as well as from environmental or health authorities and associations who may provide better solutions for the design of practical forecasting studies 4 3 4 future research directions an important result revealed by the above analysis is that this research area has been driven largely by advances in forecasting data ai forecasting methods and forecasting designs in the foreseeable future with the rapid proliferation of ai techniques and the increased availability of air quality data there will be explosive growth in this field therefore combining the abovementioned driving factors for the development of this research area we recommend follow up studies considering these three aspects 1 combining air pollution expertise with ai techniques is expected to further drive the development of air quality forecasting as a data driven approach the forecasting performance of ai techniques relies heavily on the quality of input data considering this air pollution expertise will encourage modellers to explore new and effective data which not only enriches the research form of this field but also further contributes to the development of air pollution theory atmospheric environment a leading journal in the field of air quality forecasting has also expressed concern about the lack of in depth insight into forecasting in its recent guide for authors the studies that utilize novel data science tools that focus on forecasting and do not provide insight into atmospheric sources processes and impacts are not suitable for publication in atmospheric environment furthermore it is worth noting that some characteristic variables such as geographical location pollution source distribution and air mass trajectory have been introduced into ai forecasting models feng et al 2015 additionally given the requirement of being practical enough in the real world future modellers should focus more on how to make the models simple and easy to generalize without significantly reducing their forecasting power it might be a good option to draw on the relevant methods and ideas in other fields for example russo et al 2013 apply stochastic data analysis and statistical physics to considerably reduce the number of input variables 2 each step in the construction of the ai forecasting model should be formally and clearly stated the sparse cooperation network of authors in this field see fig 7 and previous review papers bai et al 2018 cabaneros et al 2019 suggest that the research paradigm has not yet been fully established although some modelling consensuses have been reached for example cabaneros et al 2019 systematically review ann models for ambient air quality forecasting and present the specification guidelines of ann forecasting models most of the specification guidelines also apply to other ai models for air quality forecasting such as the statement of the selected predictors utilized model structure and parameter settings the trade off between model complexity and model performance the statement of the computational penalty and running time and the corresponding computing environment in a word the current modelling process in this field is ambiguous and most of the key settings are in an ad hoc fashion cabaneros et al 2019 these guidelines are of great importance to improving the repeatability of the models applied in published papers thus contributing to comparability in particular for model driven forecasting papers the same data and computing environments should be chosen to validate the model performance to achieve comparability moreover for hybrid and ensemble models keep an eye on whether the running time meets the forecasting goals 3 future air quality forecasting research should better serve the decision making in terms of air pollution preventing and controlling wang et al 2019a point out that forecasting is meaningless if not applied to human life ai techniques can effectively process multisource and multimodal data such as images and satellite derived data which can help achieve accurate forecasts in areas with sparse monitoring sites thus promoting the joint development of regional and even global air pollution control geng et al 2018 xi et al 2019 although ai techniques have made great strides in air quality forecasting they have yet to be deployed on a large scale due to concerns about stability and interpretability lešnik et al 2019 we hope that more research can be done on the application of state of art ai models to make reliable forecasts in addition future forecasting designs should be more feasible practical and operational for decision makers including authorities and citizens using probability forecasting instead of point forecasting can provide more uncertain information and help authorities prepare prevention and control plans in advance air quality grading forecasting and evaluation are helpful for citizens to quickly understand the air quality and reasonably plan their future actions 5 conclusions and limitations accurate air quality forecasting is a prerequisite for controlling air pollution levels and reducing the exposure risk of residents this question has been extensively and continuously studied by scholars worldwide in the past two decades ai techniques represented by anns and other machine learning ml models have completely changed the form of research in this field due to their more accurate results and lower requirement for initial data with the help of scientometric and content analysis this study reviews the development and research hotspots of air quality forecasting with ai techniques from 2000 to 2019 the conclusions of this study are as follows first based on the review of previous search strategies this study innovatively proposes a set of effective search strategies according to the characteristics of this field we can improve the accuracy as much as possible while ensuring recall by combining four sets of topic keywords air quality forecasting ai techniques and exclusions the search strategy can provide directions for future scientometric research second this study provides a wide variety of statistics and visualizations of significant countries institutions authors journals and papers in the field in the form of specific analysis the methods used include cooperation network analysis co citation network analysis burst detection and basic statistical analysis this study presents the development process of air quality forecasting with ai techniques through several measurement indices such as frequency betweenness centrality burst strength and citations per year specifically the publications and citations in this field have grown steadily over the past two decades especially since 2014 more than 60 of papers in this field are published in environmental sciences wos category china has the largest number of publications and citations while the united states plays the most important structural role in the international cooperation network the most prolific institutions and authors are mainly from china and the united states in this field four journals including atmospheric environment environmental modelling software environmental science technology and science of the total environment outperform other journals in terms of citation per paper and citation per year finally this study explores the research hotspots and frontier evolution in this field by adopting cluster analysis of reference co citation network keyword co occurrence analysis and content analysis eight research topic clusters are automatically generated by the llr algorithm neural network model hourly o3 concentration new ensemble design cellular neural network artificial neural network land use regression model concentration forecasting and satellite data five keyword types are manually classified forecasting indicator forecasting method forecasting area forecasting data and health related words these analyses generally illustrate the important application of anns in air quality forecasting and the rapid proliferation of other machine learning algorithms in addition satellite data and hybrid and ensemble models are increasingly applied to the relevant studies in the content analysis section we further analyse 178 key studies from the perspectives of forecasting data forecasting techniques and forecasting designs based on this we emphasize the importance of air pollution expertise and the normative application of ai techniques and recommend future forecasting designs to better serve decision making this study is not exempt from limitations on the one hand due to statistical inconsistencies between citespace and wos some data may be lost or incorrect especially the authors full name it is therefore that we cross check the relevant data to reduce the error as much as possible on the other hand the constraint put on selecting papers from jcr q1 source journals for the content analysis also excludes certain papers that fit the inclusion criteria data availability the data are available and can be downloaded via web of science http webofscience com following the strategy presented in this study software availability the citespace software used in this study can be downloaded via the following website https sourceforge net projects citespace declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this research work was supported by the national natural science foundation of china under grant no 71774130 no 72101197 and no 71988101 the fundamental research funds for the central universities under grant no sk2021007 the authors thank the editor and the reviewers for invaluable comments and suggestions which have improved the quality of the paper immensely 
25633,artificial intelligence ai techniques have substantially changed the research paradigm in the field of air quality forecasting due to their powerful performance considering the improvement in the availability of air quality data and the rapid proliferation of ai techniques it is necessary to comprehensively and quantitatively review the development of air quality forecasting with ai techniques during the last two decades 2000 2019 by scientometric and content analysis first an overview of the relevant countries institutions authors journals and papers is presented then the research hotspots and frontier evolution are explored by adopting reference co citation analysis and keyword co occurrence analysis furthermore this study conducts a content analysis to investigate current topical interests to identify research gaps and propose future research directions the analytical framework and the findings provide helpful insights into the prospects in air quality forecasting with ai techniques keywords air quality forecasting artificial intelligence machine learning scientometrics content analysis 1 introduction with economic development and accelerating industrialization the quality of human life is gradually improving however this improvement relies almost entirely on energy consumption and pollutant emission resulting in serious air pollution problems especially in developing countries according to a report by world health organization who 9 out of 10 people around the world breathe air containing high levels of pollutants more than 7 million people die every year from various diseases caused by air pollution including stroke heart disease lung cancer chronic obstructive pulmonary diseases and respiratory infections world health organization 2018 accurate and reliable air quality forecasting which is the basis for environmental authorities to take effective treatment measures plays an important role in improving air quality bai et al 2018 du et al 2020 moreover based on timely air quality forecasting residents can adjust their activities and prepare respiratory protective equipment in advance thereby mitigating the impact of air pollution on health and economy li et al 2019b in recent years with the improvement in data availability and the development of forecasting methods air quality forecasting has gradually become one of the hotspots in air pollution research beckerman et al 2013 zhou et al 2014 currently popular air quality forecasting methods include deterministic methods e g atmospheric diffusion models and chemical transfer models and statistical methods e g autoregressive integrated moving average model arima multiple linear regression mlr and various ai models although deterministic methods have been proven to provide valuable insights and sufficient interpretability for the diffusion mechanism of air pollutants they are computationally expensive and their forecasting results might be inaccurate due to the use of sophisticated prior knowledge and various usage constraints li et al 2017b stern et al 2008 notably among statistical methods ai models not only effectively capture the nonlinearity and high order interactions between the pollutant concentration time series and predictor variables but also require less domain expertise and resources as a data driven method yao et al 2018 therefore an increasing number of scholars have developed various powerful ai techniques to further advance the literature on air quality forecasting niu et al 2017 sun and sun 2017 there have been a few attempts to review the previous research in the field of air quality forecasting with ai techniques for example rybarczyk and zalakeviciute 2018 systematically review machine learning approaches for air quality modelling and provide the time distribution geographical distribution research classification and forecasting methods by investigating 46 papers on this topic cabaneros et al 2019 focus on previous prediction methods with artificial neural networks anns for modelling and forecasting air quality and comprehensively evaluate the latest developments regarding this theme moreover their results provide a normative guide for the ann model building process of follow up research however these previous review studies on the application of ai techniques in the field of air quality forecasting are mostly based on subjective screening and inductive analysis these traditional literature reviews on the one hand cover too few papers and too short time spans to provide a global analysis yu et al 2020 on the other hand they rely heavily on the authors expert knowledge of the domain and are often subject to bias huang et al 2019b scientometrics or named bibliometrics introduce quantitative methods to describe evaluate and monitor published research which contributes to the discovery of intellectual structures social networks and topical interests koseoglu et al 2016 zupic and čater 2014 in recent years scientometrics has been extensively employed to look back journals li and hale 2016 martínez lópez et al 2020 and professional fields dhital and rupakheti 2019 zhong et al 2019 1 1 according to our search in april 2020 more than 5768 scientometric papers have been published since 2015 of which 1133 in computer science interdisciplinary applications 538 in environmental sciences and 334 in management in particular scientometric papers related to air quality or air pollution are shown in table 1 importantly no scientometric research on the application of ai techniques in air quality forecasting has been conducted thus far which is exactly what we strive for in this work meanwhile given the intrinsic deficiencies in scientometric analysis 2 2 nath and chowdhury 2021 and xu et al 2018a indicate that scientometric analysis lacks topical details or in depth insights zupic and čater 2014 suggest that scientometric analysis are not a substitute for but a complement to traditional review such as extensive reading and synthesis we adopt the most advanced review paradigm kazemi et al 2019 maçaira et al 2018 nath and chowdhury 2021 and conduct a top down literature review by integrating scientometric and content analysis this study presents an extensive and comprehensive quantitative review as well as an in depth analysis of the field of air quality forecasting with ai techniques during the last two decades 2000 2019 in this article the scientometric software citespace is used to map cooperation networks co citation networks and co occurrence networks combining the classical basic statistical indicators we attempt to objectively and accurately review the field of air quality forecasting with ai techniques from the perspectives of specific analysis and research hotspot evolution accordingly specific analysis involves productive and significant countries institutions authors and their cooperation networks as well as journals with large numbers of publications and papers with large numbers of citations research hotspots and frontier evolution are extracted from the results of the reference co citation cluster keyword co occurrence analysis and content analysis in summary the study is driven by the following research questions 1 what is the efficient search strategy according to the characteristics of this field 2 how is this field of research organized in terms of countries institutions authors journals and papers 3 what are the research hotspots how do the frontiers evolve in this field and what should we pay attention to in future research our advanced review paradigm of integrated scientometric and content analysis provides a comprehensive yet in depth review the search strategy we propose for this field can benefit relevant researchers and practitioners our findings help researchers identify the current research advances competing groups and possibilities for cooperation and build their own work in this active field it is foreseeable that with the improvement in the availability of air quality data and the rapid proliferation of ai techniques the field of air quality forecasting with ai techniques will continue to develop steadily it is necessary to summarize and offer helpful insights in this field in a timely manner the rest of this study is organized as follows section 2 describes the process of search strategy paper screening and analysis tools and methods on this basis the research framework of this study is given section 3 provides a comprehensive analysis of the countries institutions authors journals and papers in this field the hotspots and frontier evolution are presented in section 4 finally the conclusions and limitations are summarized in section 5 2 research methodology 2 1 search strategy the previous papers examined in this study are collected from clarivate analytics web of science wos database wos is recognized by the academic community as the most authoritative database and it incorporates articles from more than 20 000 peer reviewed and high level journals from 1900 to the present and wos contains the essential metadata to realize scientometric analysis carvalho et al 2013 pan et al 2019 sassetti et al 2018 specifically the two largest subdatabases the science citation index expanded sci e and the social sciences citation index ssci are considered for collecting the related papers it is clear that air quality forecasting with ai techniques is an interdisciplinary study so we do not limit the scientific field of the papers which attempt to cover a larger number of results inspired by el alfy and mohammed 2020 we propose four sets of search keywords related to air quality forecasting ai techniques and exclusions table 2 lists the search keywords in each set in detail these keywords are not only derived from the search strategy of the previous reviews but also added gradually in the search process it is pointed out that searching for keywords in wos is not case sensitive but we still search for them in the form commonly found in papers in addition denotes a fuzzy search for example air pollut means air pollution and air pollutant while predict means predict predicted and prediction in particular there are many words prefixed with co which seriously interfere with the retrieval results therefore co is adopted to exclude such papers consistent with cabaneros et al 2019 we only consider ambient outdoor air quality forecasting and exclude papers related to indoor air quality we choose the advance search feature of wos to create queries by using field tags keywords boolean operators or and and not and parentheses the or operator is applied between the keywords that belong to the same set whereas the and operator is used to relate the different sets the not operator represents the keyword set to be excluded the specific search strategy is set as follows ts air pollut or air quality or atmospheric pollut or aqi or air quality index or pm2 5 or pm10 or o 3 or o3 or no2 or co or so2 and predict or forecast and artificial intelligence or ai or machine learning or ann or artificial neural network or neural network or bp or rbf or rnn or cnn or knn or deep learning or svm or elm or lstm or random forest or decision tree or ensemble or bagging or sae or boosting or adaboost or bayesian network not ts indoor air quality or iaq or co where ts means a full search of the selected keywords in titles abstracts and keywords of the articles in fact our search motivation was to ensure that the data are as comprehensive as possible given the diversity of language expression nevertheless this strategy also introduces too many irrelevant papers which need to be further screened 2 2 paper screening after setting the time span as 2000 2019 we obtained 1224 english articles or reviews in the field of air quality forecasting with ai techniques other types of documents including proceedings papers book chapters reprints and editorial material are not considered because they are not rigorously peer reviewed huang et al 2019b and most of the proceedings papers end up being published as articles or reviews next we select the appropriate papers by skimming through the titles and abstracts the appropriate papers need to explicitly involve the use of ai techniques to forecast environment outdoor air quality or air pollutant concentrations in addition papers on the following topics are also excluded from our selection 1 forecasting of health problems caused by air pollution 2 forecasting of air pollutant emissions including automobile exhaust and boiler gas 3 social studies of non computational models and 4 indoor air quality forecasting overall 643 effective papers are obtained by filtering under the above manual selection criteria and are used for scientometric analysis all data collection was completed in april 2020 2 3 analysis tools and methods there are currently more than 10 kinds of scientometric mapping software available for researchers such as citespace chen 2004 2006 vosviewer van eck and waltman 2010 and histcite garfield et al 2003 this study explores the knowledge distribution and evolution of air quality forecasting with ai techniques during the past two decades through citespace which is developed by professor chen of drexel university citespace presents the structure and patterns of scientific research by systematically creating various accessible knowledge maps zhong et al 2019 so it is more helpful than other scientometric mapping software to understand the meaning of knowledge maps in addition citespace allows the user to adjust the style of the network layout the colour size and location of nodes and labels which increases the possibility of improving the visualization quality there are three measures of nodes in the knowledge maps generated by citespace the frequency the betweenness centrality and the burst strength cui and zhang 2018 zhong et al 2019 the frequency has different meanings for different types of knowledge maps the number of published papers in the research collaboration network the number of co citation in the co citation network and the number of co occurrence in the co occurrence network meanwhile the frequency is represented by the size of nodes in citespace the betweenness centrality bc developed by freeman 1977 is calculated by eq 1 where g pq is the number of shortest paths between nodes p and q and n i pq is the number of shortest paths through node i in the paths above the nodes with high bc are usually the structural holes in the network the bc is represented by the size of the purple circle outside nodes in citespace only nodes with bc of at least 0 1 are displayed the burst strength bs is the index that measures the nodes citation burst in the burst detection where the algorithm was proposed by kleinberg 2003 1 b c i p q i n p q i g p q this study conducts a specific analysis of countries institutions authors journals and papers under the theme of air quality forecasting with ai techniques and discusses the hotspots and frontier evolution in this field the scientometric research methods involved in this study are as follows 1 cooperation network analysis including country institution and author cooperation network representing the macro meso and micro level respectively 2 co citation network analysis the reference co citation network which reveals the knowledge bases and paradigms of the field small 1973 3 cluster analysis based on the reference co citation network the references are automatically classified and labelled which provides insights for an underlying knowledge structure huang et al 2019b 4 co occurrence network analysis the keyword co occurrence network that reflects the evolution of research topics and hotspots he 1999 additional information through the basic statistical analysis is provided in this paper in summary the research framework of this paper is shown in fig 1 3 specific analysis 3 1 annual and discipline publication statistics the annual distribution of the 643 collected publications and their citations is shown in fig 2 the number of publications and citations has maintained almost the same growth pattern in the last twenty years overall the temporal distribution of publications and citations can be divided into two phases during the first phase from 2000 to 2013 the number of both increased linearly and slowly but with fluctuations in some years e g 2005 2006 during the second phase from 2014 to 2019 the number of both exhibits exponential growth reaching unprecedented 175 publications and 2894 citations in 2019 predictably this steady growth will continue as more data become available and more advanced ai techniques emerge air quality forecasting with ai techniques is a typical interdisciplinary study the distribution of the number of publications in the major disciplines is given in fig 3 considering that it allows a publication to belong to more than one discipline in wos the discipline that accounts for the most publications is environmental sciences with 387 publications they are followed by meteorology atmospheric sciences engineering environmental computer science artificial intelligence engineering electrical electronic and computer science interdisciplinary applications with 166 71 61 39 and 34 publications respectively 3 2 productive countries and their cooperation network international cooperation allows the sharing of knowledge across countries and expands academic influence this is why we analyse the productive countries first this study gives a histogram of the distribution of publications of the major producing countries from 2000 to 2019 the four most productive countries namely china the united states italy and the united kingdom produced 71 54 of the total publications in this field as shown in fig 4 china has established its leading position in this field since 2014 while the number of publications from other countries has fluctuated next we import the country information of all the authors into citespace to generate the country network as shown in fig 5 we adjust the number of nodes and edges in the cooperation network by changing the values of nodes and lrf in citespace to spread the network as flat as possible we move the location of the nodes so that the connection between the nodes can be displayed the country cooperation network contains 41 nodes and 70 links the size of nodes and labels represents the total number of articles published in the country the cooperation period between countries from past to present is shown with the link colour transition from a cool tone to a warm tone the thickness of the line represents the intensity of cooperation although china has the largest number of publications it does not play a bridging role in the network while the betweenness centrality of the united states italy and germany is greater than 0 1 contributing greatly to the development of the cooperation network the cooperation network of countries exhibits significant regional characteristics with asian countries and european countries gathering together and the united states acting as an intermediary this reflects the fact that air quality forecasting is not a country specific issue but a regional and global challenge finally we present additional information about the top 15 most productive countries in table 3 several countries have a high number citations per paper tp tc including greece tp tc 35 14 italy 28 73 france 27 69 canada 26 74 and germany 23 38 these values reflect the overall high quality of publications the united states is the first country to enter the field and south korea is the last all countries continue to be productive in this field until 2019 except for spain until 2018 3 3 prolific institutions and their cooperation network institutional cooperation networks can be beneficial by identifying exchange relationships and cooperation patterns in this field fig 6 shows the largest institutional cooperation network as well as two separate small networks the institutional cooperation network contains 133 nodes and 207 links some institutions received high frequency and betweeness centrality scores and these are chinese academy of sciences frequency 35 centrality 0 21 nanjing university of information science technology 14 0 13 peking university 11 0 15 nasa 10 0 11 and emory university 11 0 07 other institutions that are more frequently involved in the cooperation networks but do not occupy bridging roles and include dongbei university of finance economics 12 0 00 lanzhou university 11 0 02 and national center atmospheric research 9 0 05 comparing the results of the country cooperation network see fig 5 we find that although china has not played a leading role in international cooperation it has formed a huge domestic scientific research system centred on chinese academy of sciences moreover the institutional cooperation network in this field is almost a recent development as seen from the fact that the edges are almost warm table 4 displays information about the top 15 most prolific institutions the most prolific institutions are almost all from china and the united states except european commission joint research center from italy and univ de santiago de chile from chile three institutions city univ hong kong univ de santiago de chile and shanxi univ stand out because their citations per paper tp tc are all over 45 univ de santiago de chile began its research in 2000 while nanjing univ informat sci technol and emory univ entered the field in 2017 however the latter two universities have published 14 and 11 papers respectively in just three years it should be noted that univ of calif system tp 17 includes univ of calif davis 5 univ of calif los angeles 5 univ of calif berkeley 5 univ of calif irvine 3 univ of calif san diego 1 and univ of calif san francisco 1 nasa tp 13 includes nasa goddard space flight center 11 nasa jet propulsion laboratory 1 nasa ames research center 1 and nasa langley research center 1 3 4 influential authors and their cooperation network author collaboration network allows us to find the core authors in this field and determine the intensity of their collaboration chen 2017 to explore the influential authors in this field we first draw the cooperation network distribution of all the authors in the scientometric database as shown in fig 7 a the cooperation network distribution shows that there are no large scale partnerships among scholars active in the field of air quality forecasting with ai techniques but several small networks have emerged for example the largest author collaboration network is found in fig 7 b which is centred on yang liu a professor at emory university moreover we find that the largest author collaboration network was formed mainly in recent years in table 4 we rank the top 10 authors that have published more than 5 papers in this field liao et al 2019 point out that the author s name abbreviation phenomenon especially the name of the eastern scholars causes an inflated frequency with this in mind we cross check the authors names and publications and accurately identified each author s full name country and affiliation consistent with the institution see table 4 most authors come from the united states and china except patricio perez from chile and stefano galmarini from italy wenjian wang tops the list with 68 5 citations per paper but she has not published in the field since 2008 wang is followed by weizhen lu patricio perez and jianzhou wang with 49 60 46 36 and 35 86 citations per paper respectively notably four scholars yang liu yu zhun xia meng and alexei lyapustin have earned their place in the international academic community in just a few years consistent with the author cooperation networks see fig 7 the most influential authors have a low betweenness centrality 0 00 0 01 3 5 source journal analysis research in journals can help junior researchers and policy makers narrow the scope to obtain relevant information table 6 lists the top 15 high yield journals publishing a large number of studies in the field of air quality forecasting with ai techniques we list some indicators of these journals including citations per paper citations per year publication time span if from jcr and h5 from google scholar atmospheric environment ranks far ahead of other journals with 75 publications and 3521 citations four journals are ranked among the best in terms of both citations per paper tc tp and citations per year tc y and these are atmospheric environment tc tp 46 95 tc y 176 05 environmental modelling software 47 69 40 16 environmental science technology 33 67 36 07 and science of the total environment 27 92 43 63 which reflects the high quality of the papers published in these journals environmental science technology is the most influential journal according to if2018 5yif and h5 3 6 prominent papers analysis an analysis of papers can help us identify which the papers that have influenced the progress of research in this field and have captured scholars attention we first carry out the burst detection on the references of the analysed papers and table 7 displays the top 10 references with the strongest citation bursts during 2000 2019 the burst detection takes into account both the number of citations and the length of the period over which the citations occur huang et al 2019b the table is arranged according to the start of the references burst these milestone papers are all published before 2010 and citations increased dramatically within a year lasting 8 years until 2016 some references obtain high burst strength including grivas and chaloulakou 2006 strength 16 21 gardner and dorling 1999 13 05 and pérez et al 2000 11 75 these papers with burst citations must have hit the hotspot of research at that time tracking frequently cited papers not only helps scholars quickly locate classic research in the field but can also facilitate the subsequent identification of research directions pan et al 2019 table 8 provides a list of the top 20 most cited papers and their information on international and inter institution collaboration the citation data listed in the table are from the wos core collection kukkonen et al 2003 is the top cited article to date with a total of 202 citations patricio perez is the only author with 2 publications in table 5 three quarters of the most cited papers are published before 2008 the remaining five papers are published between 2013 and 2016 and deserve academic attention due to the large number of citations obtained in a short time only one paper is written by a single author eight publications are the result of international collaboration and 11 are written by single country authors moreover there are 11 inter institutionally collaborative publications and 8 single institute publications 4 hotspots and frontier evolution 4 1 reference co citation analysis the research hotspots and frontiers are reflected by the papers actively cited by scholars and can be used to describe the dynamic evolution of the field through analysing the co citation references this study objectively explored how the underlying knowledge base of this field evolves to achieve the balance between legibility and detail for visualization we select the cited references published within 8 years prior to each paper for co citation analysis for example if a paper in our scientometric database was published in 2005 its references from 1997 to 2005 are used for visualization as a result the reference co citation network contains 587 nodes and 5562 links as shown in fig 8 the node size represents the co citation frequency of each reference the colour change of the links reveals the timespan to explore how knowledge evolves and migrates temporally we perform a cluster analysis of the co citation network and 8 main clusters that could reflect the patterns and themes are displayed in fig 8 the log likelihood ratio llr algorithm dunning 1993 is used to label the clusters by extracting nominal terms from the keywords in references and the keywords supplemented by wos the size and sequence number of the label represents the size of the cluster that is the number of references contained in the cluster more information about the clusters is given in table 9 as seen from fig 8 and table 9 the research hotspots of air quality forecasting with ai techniques during the past 20 years are 4 neural network model mean year 1998 2 hourly o3 concentration 2005 3 new ensemble design 2006 5 cellular neural network 2008 7 artificial neural network 2010 6 land use regression model 2011 0 concentration forecasting 2013 and 1 satellite data 2014 in order the silhouette value of each cluster is above 0 5 which means that the clustering result is reasonable chen et al 2010 the major co citation associations occurred after 2010 which is consistent with the literature explosion of the same period 3 new ensemble design and 2 hourly o3 concentration are two of the longest spanning clusters while various neural networks are the most dominant methods in the field given that air pollution monitoring stations can only cover a small area it is unreasonable to use such local measurements to assess the pollution exposure of residents in an entire area 6 land use regression model incorporates spatial variables such as traffic topography and population density obtained through gis and is then applied to a large number of unmonitored sites in the study area adams and kanaroglou 2016 hoek et al 2008 although land use terms are usually time invariant and are limited in modelling short term variations di et al 2016 they are still an important forecasting variable on the spatial scale adams and kanaroglou 2016 hu et al 2017 in contrast 1 satellite data with comprehensive spatiotemporal coverage have recently been widely used for air quality forecasting at a spatiotemporal scale satellite data mainly refer to aerosol optical density aod data which are generally proven to be related to pm2 5 concentrations the main drawback of satellite data is that they may be missing for meteorological reasons but a beneficial characteristic is that multiple sources can complement each other hu et al 2017 citespace not only identifies the research themes and trends through clustering but also displays the representative of each cluster this study can help to understand the research themes by briefly reviewing these representative works zhu and hua 2017 gardner and dorling 1998 systematically review the application of anns especially the multilayer perceptron in atmospheric science the application of the multilayer perceptron mainly includes prediction function approximation and pattern classification grivas and chaloulakou 2006 extend the ann model to predict the hourly pm10 concentration in athens and compare their predicted results with those of other studies for predictions of hourly no2 and o3 concentrations mckeen et al 2005 pioneer real time ensemble ozone forecasting by combining seven different forecasting models kurt et al 2008 develop an online air pollution forecasting system in istanbul using a neural network and they increase the forecasting performance by designing a cumulative method optimizing the number of training days and choosing the day of the week as a parameter a neural network with stochastic variables for air quality prediction is presented by russo et al 2013 this method aims to reduce the number of input variables significantly under the premise of guaranteeing the prediction ability of the neural network model li et al 2011 investigate the strengths and limitations of satellite data in air quality forecasting feng et al 2015 apply an air mass trajectory based geographic model and wavelet transformation to improve the performance of the ann model for air quality forecasting hu et al 2017 propose a multivariate random forest model covering multiple predictors including aerosol optical depth aod data meteorological fields and land use variables to forecast pm2 5 concentrations in the united states these representatives potentially reflect the research hotspots at that time so they have been widely cited and become structural holes in the reference co citation network 4 2 keyword co occurrence analysis keywords not only represent the core issues discussed in the literature but also reflect hot research topics for a certain time period keyword co occurrence analysis kca knowledge maps determine hot research topics based on high frequency keywords yang and meng 2019 we select a group of keywords with high co occurrence frequency and plot the time zone view of the keyword co occurrence network as shown in fig 9 which clearly shows the evolution of the knowledge domain in the time dimension the time zone view aggregates the keywords that appear for the first time in the same year into the same time zone the size of nodes represents the co occurrence frequency of the keywords the links between the keywords indicate the inheritance of the literature the keywords are distributed almost evenly across time zones except for the gaps between 2010 and 2012 from the time zone view we can deduce the evolution of methods from neural networks to diversified machine learning including random forest and support vector machine this can be attributed to the fact that open source packages provide great convenience for invoking machine learning algorithms and building forecasting model frameworks cabaneros et al 2019 we also find changes in forecasting indicators from o3 and no2 to fine particulate matter pm10 and pm2 5 this may be because haze has become the most prominent pollution problem especially in developing countries such as china while other air pollutants have been effectively treated zhao and li 2019 to study the distribution and evolution of keywords more accurately we manually classify the keywords that appear more than 10 times in our scientometric record table 10 displays the five types of keywords 0 forecasting indicator 1 forecasting method 2 forecasting area 3 forecasting data and 4 health related words meanwhile some generic keywords with no real meaning are eliminated for instance prediction forecast forecasting and modi specifically most of the keywords gather in 0 forecasting indicator and 1 forecasting method the characterization of pollutants is becoming increasingly detailed for example from the initial pm2 5 to ground level pm2 5 and pm2 5 concentration ai techniques have evolved from anns to other machine learning algorithms and hybrid and ensemble methods jiang et al 2017 athens china and the united states in turn have become hotspot areas for air quality forecasting with ai techniques as for the data sources satellite derived aerosol optical depth data can effectively expand the range of air quality forecasting by its comprehensive spatiotemporal coverage hu et al 2017 finally the ultimate goal of air quality forecasting is to protect the health of residents through timely early warning and control measures exposure and mortality are high frequency words that reflect health 4 3 content analysis reference co citation analysis and keyword co occurrence analysis generally provide an overall picture of the evolution of hotspots and frontiers in air quality forecasting with ai techniques given the intrinsic deficiencies in scientometric analysis we follow kazemi et al 2019 and nath and chowdhury 2021 to undertake an in depth content analysis to track salient research frontiers and propose feasible and promising directions for future research to efficiently and rationally process the large dataset of 643 papers initially collected we first select the most important studies for targeted analysis and discussion the key studies of identification include 1 the top 10 references with the strongest citation bursts 2 the top 20 most cited papers 3 the representative papers of each cluster in the reference co citation network and 4 the papers published in journal citation reports jcr q1 source journals from 2017 to 2019 among them the first three are determined according to scientometric indicators almost all of which are early literature while recent important literature cannot be selected through scientometric indicators because there has not been enough time to accumulate citations therefore we follow xu et al 2018a to add papers published in high level journals jcr q1 source journals are selected for this study over the last three years to capture and track recent advances in this field another reason to support our identification of recent key studies is publication statistics we observe 314 papers published in the last three years almost half of the total published in the last 20 years and 140 papers published in jcr q1 source journals finally a final dataset of 178 key studies is determined next we carefully read these studies and summarize the forecasting data ai based forecasting techniques and forecasting designs of each paper 4 3 1 forecasting data and identified study types air quality forecasting research with ai techniques can be roughly divided into two categories according to the different forecasting data namely the forecasting of urban air pollutant concentrations and the forecasting of air pollution exposure on a large spatial scale the former mainly uses monitoring site data and meteorological data to focus on time series forecasting while the latter aims to capture the spatiotemporal variability of air pollutants by introducing a variety of auxiliary datasets including satellite derived data chemistry transport model ctm data meteorological data land use information data topographic data source emission data and socio economic data a limited number of air pollution monitoring sites can only cover key areas such as city centres due to high construction and maintenance costs 3 3 according to honarvar and sami 2019 the average construction and maintenance costs of air pollution sites are 200 k dollars and 30 k dollars per year respectively the high precision and reasonable distribution of monitoring stations can measure the overall pollution level of cities to a certain extent qi et al 2019 however the sparse and uneven spatial coverage of monitoring sites makes it difficult to accurately estimate and forecast residents air pollution exposure especially for suburban and rural residents who have less access to health care facilities and poorer socio economic conditions stafoggia et al 2019 consequently a large number of studies have investigated the effectiveness of satellite data with high space coverage in forecasting exposure chen et al 2019 di et al 2016 hu et al 2017 our content analysis reveals four research trends in this academic niche the impact of missing aod values and the corresponding corrective measures have been preliminarily explored various filling methods including random forest have been developed in the literature bi et al 2019 chen et al 2019 huang et al 2018 zhang et al 2018 in addition few studies have verified the potential of other satellite derived data to replace aod data with serious missing liu et al 2019 exploring new predictive variables is welcome recent attempts include atmospheric stability bai et al 2019 and spatial lag variables wang et al 2019b although bai et al 2019 find that the inclusion of atmospheric stability indicators into the pm2 5 modelling framework contributes little to the forecasting accuracy these useful attempts could benefit air quality forecasters it is an important and challenging task to predict the pollutant concentration of regional air pollution events for example several scholars from north america emphasize the importance of pm2 5 and ozone exposure forecasting in the united states and canada during wildfire seasons watson et al 2019 yao et al 2018 zou et al 2019 feng et al 2019 explore the challenge of pollutant emissions from straw burning in southern china to air quality forecasting despite the increasing availability of air pollution data overall high quality forecasting studies are lacking in data poor regions where pollution levels are often high gao et al 2018 geng et al 2018 it is a promising direction to develop air quality forecasting models in areas with no monitoring sites or sparse monitoring sites which will alleviate the shortage of air quality forecasting in many developing countries furthermore dense three dimensional buildings and streets in large cities hinder the diffusion of air pollutants and lead to sharp changes in air pollution levels on fine spatial scales huang et al 2019a yang et al 2018b therefore measurements from monitoring sites cannot reflect the true impact of air pollution at the human level mihăiţă et al 2019 more recent studies have explored novel data sources as supplements or substitutes for monitoring site data including handheld or vehicle mounted mobile sensor data huang et al 2019a unmanned aerial vehicle measurements yang et al 2018b image data uploaded by residents xi et al 2019 and real time traffic data provided by google maps honarvar and sami 2019 although the application of these new data in air quality forecasting with ai techniques is still in its infancy with the development of smart cities and the progress of big data techniques the data have a great potential to improve forecasting research and practical application in the future 4 3 2 the rise and fade of ai forecasting techniques ai techniques for air quality forecasting have evolved dynamically over the past 20 years with some methods increasingly used and others fading out of sight due to insufficient data scale and computing resources as well as the difficulty in setting the initial parameters and network structure the improvement of early neural network forecasting performance has been severely limited faganeli pucer et al 2018 after that support vector machines svm have gradually emerged due to their excellent learning ability on small sample datasets sun and sun 2017 because of its robustness to noise variables as well as its simplicity and insensitivity to parameter settings random forests are becoming increasingly popular especially in data intensive forecasting studies tang et al 2019 however such simple models ignore the relationships between different time steps of time series data li et al 2017b recurrent neural network rnn can efficiently process sequence inputs by setting memory capacity and the variants of rnn namely long short term memory network lstm and gated recurrent unit gru are reported to be more suitable for modelling long short temporal dependency by solving the exploding or vanishing gradient problems li et al 2017b lin et al 2021 moreover convolutional neural network cnn and graph neural network gnn have been examined to extract higher order spatial features of multiple sites qi et al 2019 wen et al 2019 these advanced deep learning models are composed of multiple processing layers to learn representations of data with multiple levels of abstraction they can achieve end to end learning without complex feature engineering sun et al 2019 the pioneering study on ai techniques performance comparison in air quality forecasting points out that no model can universally outperform all others in terms of forecasting accuracy watson et al 2019 xu et al 2018b the two major approaches to improve the forecasting performance of a single model are ensemble models and hybrid combination models considering that previous studies tend to confuse these two concepts we provide a detailed clarification here ensemble models which emphasize that multiple models can learn the inherent features of the targets from various perspectives can achieve better forecasting performance than a single model for example di et al 2019 develop a generalized additive model to ensemble the individual forecasting of gradient boosting neural network and random forests thus providing a superior model performance for pm2 5 forecasting of the entire contiguous united states hybrid models combine different models into a multi stage model to overcome their individual shortcomings the common hybrid model types are as follows 1 combining ai forecasting models with data pre processing approaches and or optimization methods such as various decomposition methods yuan et al 2019 outlier detection and correction wang et al 2020a clustering algorithms li et al 2019a and intelligent optimization algorithms xu et al 2017b 2 combining different ai forecasting models such as cnn lstm model wen et al 2019 lstm cnn model yang et al 2019 and gnn lstm model qi et al 2019 which are designed to extract and learn the spatiotemporal feature of air pollution concentration ensemble and hybrid models improve forecasting performance at the expense of computation this increased computing cost is negligible given the power of modern computers and cloud computing technology therefore we expect both ensemble and hybrid models to play an increasingly important role in air quality forecasting ensemble and hybrid models are also relevant to the recent requirement for reliable forecasting zhu et al 2018 one concern about ai techniques is the instability of forecasting performance ensemble models can effectively avoid the potentially unstable performance of individual models xiao et al 2018 hybrid models can incorporate multi objective optimization algorithms to improve forecasting accuracy and stability simultaneously du et al 2020 zhu et al 2018 another concern about whether ai techniques can make reliable forecasting is the interpretability of ai techniques there have been preliminary explainable attempts about variable importance measures in the air quality forecasting field such as random forests and neural networks with attention mechanisms grange et al 2018 lešnik et al 2019 in the future interpretable ai techniques will better quantify the impact of individual covariates on forecasting outcomes which allows for improved decision making when considering the implementation of prevention mechanisms 4 3 3 forecasting for better decisions emergent forecasting designs the primary objective of using ai techniques to forecast air quality is to improve the accuracy of forecasting air pollutant concentrations and this objective is further enhanced by powerful deep learning models however even though scholars claim that forecasts can help stakeholders such as authorities and the public make prevention decisions it is questionable whether these studies can actually be deployed to make a difference in the real world if only the average concentrations of air pollutants are predicted fortunately a variety of forecasting designs have emerged to help represent forecasting in a more comprehensible way so that authorities and citizens can make more beneficial use of the forecasts aznarte 2017 we can group these emerging research designs into the following two categories 1 develop probabilistic forecasting models that provide uncertain information compared with traditional point forecasting of air pollutant concentration values probabilistic forecasting focuses on the future probability distribution of concentrations abdullah and khalid 2012 in the field of air quality forecasting probabilistic forecasting has three main forms namely threshold forecasting interval forecasting and density forecasting concretely threshold forecasting aims to determine whether air pollution levels exceed air quality limits in the coming hours abdullah and khalid 2012 interval forecasting can provide the forecasting ranges for air pollutant concentrations at certain confidence levels which are also known as upper and lower bounds li and zhu 2018 wang et al 2020b by contrast density forecasting produces interval forecasts at any confidence level by obtaining continuous probability density functions faganeli pucer et al 2018 wang et al 2017b hence density forecasting can be easily transformed into threshold forecasting and interval forecasting these probabilistic forecasts can quantify the range of observed data changes caused by uncertain factors and provide more uncertain reference information for decision makers to help them formulate and implement air quality protection measures we hope that more air quality forecasting research will move into the fascinating and unexplored world of probabilistic forecasting 2 develop various air quality early warning monitoring systems including not only the forecasting module but also evaluations and other modules early warning monitoring systems can be roughly divided into two types according to the way in which different modules are combined the first type first evaluates the air quality degree to identify major pollutants followed by targeted forecasts li and zhu 2018 xu et al 2017a yang and wang 2017 the identification of major pollutants can greatly reduce the computing resources of subsequent forecasting without affecting the evaluation of air quality which is conducive to creating effective and practical air quality warning monitoring systems tailored for cities the second type evaluates the air quality degree based on all air pollutants forecasts li et al 2019b the most commonly used air quality evaluation tool is the air quality index aqi which simplifies the concentration of various air pollutants into a single conceptual numerical form and represents the air quality degree by grading wang et al 2017a zhu et al 2018 with the help of the aqi citizens without professional knowledge of atmospheric science can easily understand the current air quality nevertheless the aqi is calculated according to defined numerical rules while the air quality degree as a relatively vague concept lacks clear boundaries to grade wang et al 2019a as a result researchers have developed a variety of air quality evaluation methods based on fuzzy logic abdullah and khalid 2012 gorai et al 2015 li et al 2019b wang et al 2019a in addition we find that the authors in the field of air quality forecasting with ai techniques mostly come from the disciplines of statistics computer engineering and environmental geography at universities and institutes 4 4 the analysis of influential authors in section 3 also supports this finding in the future the field should attract more scholars from the disciplines of management and economics as well as from environmental or health authorities and associations who may provide better solutions for the design of practical forecasting studies 4 3 4 future research directions an important result revealed by the above analysis is that this research area has been driven largely by advances in forecasting data ai forecasting methods and forecasting designs in the foreseeable future with the rapid proliferation of ai techniques and the increased availability of air quality data there will be explosive growth in this field therefore combining the abovementioned driving factors for the development of this research area we recommend follow up studies considering these three aspects 1 combining air pollution expertise with ai techniques is expected to further drive the development of air quality forecasting as a data driven approach the forecasting performance of ai techniques relies heavily on the quality of input data considering this air pollution expertise will encourage modellers to explore new and effective data which not only enriches the research form of this field but also further contributes to the development of air pollution theory atmospheric environment a leading journal in the field of air quality forecasting has also expressed concern about the lack of in depth insight into forecasting in its recent guide for authors the studies that utilize novel data science tools that focus on forecasting and do not provide insight into atmospheric sources processes and impacts are not suitable for publication in atmospheric environment furthermore it is worth noting that some characteristic variables such as geographical location pollution source distribution and air mass trajectory have been introduced into ai forecasting models feng et al 2015 additionally given the requirement of being practical enough in the real world future modellers should focus more on how to make the models simple and easy to generalize without significantly reducing their forecasting power it might be a good option to draw on the relevant methods and ideas in other fields for example russo et al 2013 apply stochastic data analysis and statistical physics to considerably reduce the number of input variables 2 each step in the construction of the ai forecasting model should be formally and clearly stated the sparse cooperation network of authors in this field see fig 7 and previous review papers bai et al 2018 cabaneros et al 2019 suggest that the research paradigm has not yet been fully established although some modelling consensuses have been reached for example cabaneros et al 2019 systematically review ann models for ambient air quality forecasting and present the specification guidelines of ann forecasting models most of the specification guidelines also apply to other ai models for air quality forecasting such as the statement of the selected predictors utilized model structure and parameter settings the trade off between model complexity and model performance the statement of the computational penalty and running time and the corresponding computing environment in a word the current modelling process in this field is ambiguous and most of the key settings are in an ad hoc fashion cabaneros et al 2019 these guidelines are of great importance to improving the repeatability of the models applied in published papers thus contributing to comparability in particular for model driven forecasting papers the same data and computing environments should be chosen to validate the model performance to achieve comparability moreover for hybrid and ensemble models keep an eye on whether the running time meets the forecasting goals 3 future air quality forecasting research should better serve the decision making in terms of air pollution preventing and controlling wang et al 2019a point out that forecasting is meaningless if not applied to human life ai techniques can effectively process multisource and multimodal data such as images and satellite derived data which can help achieve accurate forecasts in areas with sparse monitoring sites thus promoting the joint development of regional and even global air pollution control geng et al 2018 xi et al 2019 although ai techniques have made great strides in air quality forecasting they have yet to be deployed on a large scale due to concerns about stability and interpretability lešnik et al 2019 we hope that more research can be done on the application of state of art ai models to make reliable forecasts in addition future forecasting designs should be more feasible practical and operational for decision makers including authorities and citizens using probability forecasting instead of point forecasting can provide more uncertain information and help authorities prepare prevention and control plans in advance air quality grading forecasting and evaluation are helpful for citizens to quickly understand the air quality and reasonably plan their future actions 5 conclusions and limitations accurate air quality forecasting is a prerequisite for controlling air pollution levels and reducing the exposure risk of residents this question has been extensively and continuously studied by scholars worldwide in the past two decades ai techniques represented by anns and other machine learning ml models have completely changed the form of research in this field due to their more accurate results and lower requirement for initial data with the help of scientometric and content analysis this study reviews the development and research hotspots of air quality forecasting with ai techniques from 2000 to 2019 the conclusions of this study are as follows first based on the review of previous search strategies this study innovatively proposes a set of effective search strategies according to the characteristics of this field we can improve the accuracy as much as possible while ensuring recall by combining four sets of topic keywords air quality forecasting ai techniques and exclusions the search strategy can provide directions for future scientometric research second this study provides a wide variety of statistics and visualizations of significant countries institutions authors journals and papers in the field in the form of specific analysis the methods used include cooperation network analysis co citation network analysis burst detection and basic statistical analysis this study presents the development process of air quality forecasting with ai techniques through several measurement indices such as frequency betweenness centrality burst strength and citations per year specifically the publications and citations in this field have grown steadily over the past two decades especially since 2014 more than 60 of papers in this field are published in environmental sciences wos category china has the largest number of publications and citations while the united states plays the most important structural role in the international cooperation network the most prolific institutions and authors are mainly from china and the united states in this field four journals including atmospheric environment environmental modelling software environmental science technology and science of the total environment outperform other journals in terms of citation per paper and citation per year finally this study explores the research hotspots and frontier evolution in this field by adopting cluster analysis of reference co citation network keyword co occurrence analysis and content analysis eight research topic clusters are automatically generated by the llr algorithm neural network model hourly o3 concentration new ensemble design cellular neural network artificial neural network land use regression model concentration forecasting and satellite data five keyword types are manually classified forecasting indicator forecasting method forecasting area forecasting data and health related words these analyses generally illustrate the important application of anns in air quality forecasting and the rapid proliferation of other machine learning algorithms in addition satellite data and hybrid and ensemble models are increasingly applied to the relevant studies in the content analysis section we further analyse 178 key studies from the perspectives of forecasting data forecasting techniques and forecasting designs based on this we emphasize the importance of air pollution expertise and the normative application of ai techniques and recommend future forecasting designs to better serve decision making this study is not exempt from limitations on the one hand due to statistical inconsistencies between citespace and wos some data may be lost or incorrect especially the authors full name it is therefore that we cross check the relevant data to reduce the error as much as possible on the other hand the constraint put on selecting papers from jcr q1 source journals for the content analysis also excludes certain papers that fit the inclusion criteria data availability the data are available and can be downloaded via web of science http webofscience com following the strategy presented in this study software availability the citespace software used in this study can be downloaded via the following website https sourceforge net projects citespace declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this research work was supported by the national natural science foundation of china under grant no 71774130 no 72101197 and no 71988101 the fundamental research funds for the central universities under grant no sk2021007 the authors thank the editor and the reviewers for invaluable comments and suggestions which have improved the quality of the paper immensely 
25634,solar radiation driving land atmosphere interactions and terrestrial carbon assimilation varies both temporally and spatially due to surface gradient slope aspect and vegetation shading here we present a double shading transposition model dst for simulating temporal and spatial distribution of downwelling shortwave solar radiation the dst model considers influences of both topographic and canopy structure on radiation transmission it simulates sub hourly radiation distribution on and beneath vegetation canopy for any weather conditions the model is tested against observations at two opposite facing slopes the simulated above and under canopy radiation compares well with the observed data the model provides a convenient and effective tool for simulating spatial and temporal variation of solar radiation on mountainous areas with vegetation cover and has great potential in ecohydrological modelling and catchment management keywords shortwave solar radiation topographic shading canopy transmission land surface energy balance 1 introduction solar radiation on earth s surface drives hydrological cycles powers meteorological systems and sustains ecosystems as a key variable in hydrological and meteorological models it largely determines the land surface energy balance and governs plenty of surface processes antonanzas torres et al 2019 sheng et al 2009 wild 2009 spatial variation of incident radiation also regulates vegetation spatial patterns especially in water limited areas fan et al 2019 gutiérrez jurado et al 2013 knowledge of temporal and spatial patterns of solar radiation is also essential for solar energy system designing lopez et al 2007 solar radiation is emitted from the sun with an average temperature of 6000 k over 95 of the solar energy is made up of shortwave radiation wavelength range of 0 29 3 μm which constitutes the largest component of the land surface energy balance duffie et al 2013 kumar et al 1997 for simplicity the term radiation in this paper denotes shortwave radiation when solar radiation penetrates through the atmosphere part of it that is not direct radiation is scattered in various directions and becomes diffuse radiation on a rugged surface radiation reflected from the surroundings can also contribute to the incident radiation at a surface point duffie et al 2013 thus the total solar radiation often referred to as global radiation reaching a point on the earth s surface consists of direct radiation diffuse radiation and in some case reflected radiation as well various models have been developed to simulate solar radiation on surfaces jeff dozier 1980 dubayah et al 1995 christian a gueymard et al 2008 kumar et al 1997 lefèvre et al 2013 liu et al 1960 radosevic et al 2020 swift jr 1976 j zhang et al 2017 s zhang et al 2020 one model type is based on extraterrestrial radiation which is referred to as a clear sky model this is the most fundamental model for estimating solar radiation on the earth surface antonanzas torres et al 2019 such a model calculates radiation under a clear sky condition which is the upper limit of real radiation on the earth surface lefèvre et al 2013 several factors are considered in a clear sky model first the extraterrestrial radiation is calculated for a location based on latitude and the time then the interactions between the atmosphere and solar radiation are considered mckenney 1999 most of the clear sky models simulate radiation transfer processes in the atmosphere and interactions with different atmospheric components various types of atmospheric molecules such as water oxygen and ozone can absorb solar radiation and turn it into heat while other gases and particulates in the atmosphere can scatter solar radiation in all directions kumar et al 1997 these effects are incorporated in the radiation transfer model to estimate shortwave radiation reaching a surface on earth various factors can limit the application of clear sky models for example one limitation is a lack of data to properly parameterize simulations of radiation transfer through the atmosphere badescu et al 2012 more importantly clear sky models do not consider cloud effects which significantly affect radiation transmission limiting their use to places or time periods with little cloud cover conditions depending on local climatic patterns there are situations where up to 80 of the down welling solar radiation can be intersected by clouds kambezidis et al 2016 serrano et al 2015 to overcome this shortcoming of clear sky radiation models attempts have been made to simulate the cloud effect on radiation transfer by quantifying the cloud optical depth effectively extending a clear sky model into an all condition model kambezidis et al 2016 serrano et al 2015 tang et al 2016 however it is difficult to monitor the spatial and temporal variations of cloud distribution and cloud properties due to the sparsity and heterogeneity of observation stations zajaczkowski et al 2013 to address these issues models based on point radiation measurements called transposition models have been developed liu et al 1963 yang 2016 a transposition model takes the observed radiation on a horizontal surface as an input to simulate radiation of a tilted surface over a certain area christian a gueymard et al 2008 in such a model geometry information of the surface and solar zenith angle are considered a common use of the transposition model is to optimize the position of flat plane photovoltaic arrays khatib et al 2015 yang 2016 for example it can be used to generate diagrams showing incident solar radiation on a surface for any time as a function of surface geometry additionally a transposition model requires fewer variables than an all condition model although it requires measured radiation on a horizontal plane at a vicinity point hereafter referred to as the reference radiation by using concurrent measured radiation a transposition model can be applied under any weather conditions another important phenomenon that can exert a strong influence on surface solar radiation is shading shading can produce significant temporal and spatial heterogeneity in radiation distribution especially in complex terrain marsh et al 2012 s zhang et al 2020 zou et al 2007 to accurately describe the heterogeneity of solar radiation a distributed solar radiation model is required most previous studies focus on the topography effect on the solar radiation and the models are constructed by deriving the landscape components and structures from digital elevation model dem jeff dozier 1980 dubayah et al 1995 kumar et al 1997 in recent years researchers have started to examine the spatial and temporal variation of solar radiation in urban environments for instance yu et al 2009 investigated the spatiotemporal variability of solar radiation in downtown houston texas and designed a solar flux model for urban vegetation planting and management liang et al 2014 proposed a 3 d method for computing and visualizing urban solar radiation based on image space data representation erdélyi et al 2014 proposed a numerical model to simulate distribution of direct and diffuse radiation in an urban environment these studies primarily addressed the shading effects from surrounding terrain or buildings on solar radiation distribution vegetation can partly intercept solar radiation which otherwise would cast on the ground thus modifying the patterns of incident radiation in forested and vegetated areas therefore knowing how much radiation on the top of vegetation and the ground surface below the tree canopy is essential in ecohydrological investigations in complex terrain where small variations in energy balance may sensibly modify carbon and water fluxes dynamics metzen et al 2019 seyednasrollah et al 2014 proposed a shortwave radiation model for snowmelt prediction which calculates radiation transmission as a function of the path length of the solar beam as it passes through the gap in the forest canopy however this model can only estimate the incident radiation for a single point with the advent of high resolution topography from novel photogrammetric and light detection and ranging lidar technology data showing the 3 d structure of landscapes is becoming more frequently available greatly expanding the potential to incorporate surface geometry and structure in earth system models crosby et al 2020 for example gutiérrez jurado et al 2013 explored the topography vegetation controls on annual and seasonal irradiance at opposite facing hillslopes in new mexico combining lidar derived digital surface models that include vegetation structures and bare earth surface and a distributed solar radiation model bode et al 2014 estimated sub canopy radiation transmittance based on a light penetration index lpi obtained from lidar peng et al 2014 proposed a spatiotemporally explicit 3d ray trace model to provide spatiotemporal patterns of understory light density nyman et al 2017 tested four sub canopy radiation models and found that the model based on path length simulation had the best performance the above mentioned methods take the lidar point cloud density data or the length of solar ray crossing the vegetation crown as inputs for canopy transmission calculation however characterizing the light attenuation effect based only on the path length attribute is insufficient since the light attenuation along its transmission path through the crown is also affected by the non random distribution of foliage elements zeng et al 2019 leaf area index lai a commonly available data quantifying the leaf density within the canopy can be used to parameterize the light attenuation effect through the canopy nevertheless few solar radiation models take advantage of lai to improve radiation estimates in vegetated areas moreover to our knowledge previous distributed solar radiation models either work for clear sky conditions or require cloudiness data in the study area a distributed 3 d solar radiation model for all weather conditions remains to be developed here we present the development of a solar radiation model for simulating the temporal and spatial distribution of incident radiation on and beneath a vegetated surface for any weather conditions based on a reference point shortwave irradiance measurement both the topographic shading and vegetation canopy shading are considered in order to simulate the canopy shading the dems including the digital terrain model dtm which is the elevation of bare earth and the digital surface model dsm which is the elevation of all objects including vegetation and the lai data are used one year of shortwave irradiance data above canopy and net radiation data below canopy on two opposite facing slopes north and south facing of a steep vegetated catchment at mount wilson south australia is used to evaluate the model performance 2 the double shading transposition model the model proposed here is a transposition model that incorporates the local topographic and canopy shading effects we refer to this model as the double shading transposition dst model the model is developed based on the conceptualization shown in fig 1 topographic shading can be caused by the surface itself self shading and the surrounding terrain the self shading results from surface orientation and it can be calculated with the solar altitude angle solar azimuth angle and slope and aspect derived from dtm duffie et al 2013 revfeim 1982 s zhang et al 2020 the surrounding terrain shading is complex since it results from multiple features interacting with and intercepting light from various directions to estimate when shading occurs as a result of the surrounding terrain horizon angles are calculated using the dems fig 1 at the solar azimuth and are compared with the solar altitude angle the horizon angle in a direction is defined as the angle between the horizontal plane and the upper view edge of elevated terrain in that direction ruiz arias et al 2010 if at the time of interest the terrain horizon angle at the solar azimuth direction is larger than the solar altitude angle the surface is shaded by the terrain and has no direct radiation if the solar altitude angle is between the terrain and vegetation horizon angles the surface is shaded by vegetation canopy which will be explained later different from direct radiation diffuse radiation comes from all directions from the sky usually the sky view factor is adopted to estimate the topography effect on diffuse radiation in solar radiation models antonić 1998 jeff dozier et al 1990 li et al 2016 however since the diffuse radiation does not distribute isotropically in the hemisphere it is inaccurate to simply use the sky view factor to calculate the diffuse radiation moreover the sky view factor only accounts for diffuse radiation reduction from the topography effect and is therefore not suitable for simulating the shading effect that is caused by penetrable objects such as vegetation canopy in the dst model we calculated the anisotropic diffuse radiation and the attenuation of diffuse radiation is estimated based on the canopy structure similar to topographic shading the incident solar radiation on the land surface may be obstructed by vegetation such that the radiation is only partially intercepted by the canopy in this process vegetation canopy structure plays an important role as can be seen in fig 1 the incident direct radiation on both above and under canopy can be obstructed by vegetation occurring in the beam direction to simulate the process of light traveling through the canopy the canopy is considered as a turbid medium da silva et al 2008 to detect the spatial variation of the forest structure the dsm is employed since it can capture the morphology of the vegetation above the land surface we combined it with lai to estimate the transmittance of solar ray crossing the canopy according to the beer lambert law ross 2012 for direct radiation at a certain point in time the canopy transmittance is calculated for the direction in which the solar ray comes in contact with the canopy surface at that moment the canopy transmittance of diffuse radiation is estimated according to the mean canopy transmittance value for all directions of the hemisphere a flowchart for the dst model is shown in fig 2 the model requires the following data 1 time and location information including the start and end dates and the latitude and longitude 2 topography and vegetation data including dems and lai and 3 reference radiation data on a horizontal surface free of the shading effects which shares the same sky condition with the area of interest with solar radiation to be simulated given that direct radiation and diffuse radiation behave differently in the two shading processes a decomposition model is adopted to partition the observed global radiation at the reference station into direct and diffuse components ridley et al 2010 the incident solar radiation on a surface at a given point in time is related to the solar geometric and the topographic factors of the receiving surface at that instance similar to other radiation models the dst model starts from calculating the solar geometry and topographic features kumar et al 1997 for an inclined surface the reflected radiation from surrounding terrain is also considered in the dst model 3 methodology 3 1 study area and data the study area is a relatively steep vegetated catchment 0 1 km2 located at mount wilson 138 64 e 35 21 s in south australia fig 3 most of the area is covered by native vegetation with an average tree height about 10 m the mean annual precipitation is 716 mm australia s bureau of meteorology station id 23906 http www bom gov au climate data stations xu et al 2019 this area is characterized by a mediterranean climate which experiences cold wet winters may july and hot dry summers dec feb resulting in large variations in solar radiation throughout the year the maximum irradiance radiation flux w m2 in summer can be up to 1200 w m2 but in winter it is about 600 w m2 based on a reference station detailed later in addition to this temporal variation spatial variation of solar irradiation is more complicated due to steep topography relief and slope induced variation in vegetation cover xu et al 2017 this spatial variability provides a range of conditions to test the proposed dst model three observation stations were installed to measure shortwave radiation one site was a weather station located in an open area at the top edge of the catchment which is hereafter referred to as the reference station this station measured the unshaded shortwave solar irradiance with a cm3 pyranometer kipp zonen the two other sites were located on two opposite facing hill slopes of the small catchment one with a north facing slope nfs and the other with a south facing slope sfs for these two sites a cnr4 net radiometer kipp zonen was installed at an elevation of 12 m above the ground and measured both upwelling and downwelling shortwave and longwave radiation i e above the tree canopy in addition a nr lite2 net radiometer kipp zonen was installed at 1 7 m above the ground under the tree canopy to measure understory net radiation at both sites irradiance data was recorded from 06 04 2013 to 05 04 2014 with a 30 min sample interval the quality control process includes the following rules 1 missing or incomplete data is excluded 2 recorded radiation exceeding extraterrestrial radiation is excluded 3 for the measured above canopy radiation a 3 point 1 h moving average is adopted for measured under canopy net radiation the negative values are eliminated and a 5 point 2 h moving average is adopted since a high frequency variation exists in the measured data due to temporal variation of canopy shadow the weather station data is taken as a reference station input for the model irradiance data of 361 days is available the data recorded at the nfs and sfs sites is used for model validation for measurements above the canopy there is irradiance data for 343 days at the nfs site and 208 days at the sfs site for measurements under the tree canopy half hourly net radiation data is available for 313 days at the nfs site and for 339 days at the sfs site the dtm and dsm data were derived from airborne lidar data obtained by airborne research australia in july 2013 with a point return density of 45 point m2 and have a spatial resolution of 0 5 m the catchment has a dtm range from 298 to 410 m and a dsm range from 298 to 427 m the slope of the study area ranges from 0 to 30 according to the dtm the lai data for the study area was estimated from the available ndvi data following the method described in tillack et al 2014 and is shown in fig 3 the ndvi data was obtained from the hyper spectral images collected by airborne research australia in november 2013 with a spatial resolution of 1 m it was resampled to 0 5 m for this study overall the lai of south facing slopes northern section is larger than north facing slopes southern section 3 2 basic equations 3 2 1 solar geometry the sun s position in the sky needs to be calculated for each time interval which can be expressed by solar altitude angle and solar azimuth angle solar altitude angle α s is the angle between a horizontal plane and the line to the sun and solar azimuth angle γ s is the angular displacement from south of the projection of direct radiation on the horizontal plane duffie et al 2013 1 sin α s cos φ cos δ cos ω sin φ sin δ 2 γ s sign ω π cos 1 sin α s sin φ sin δ cos α s cos φ π where φ is the latitude in radians δ is the solar declination equation 3 and ω is the hour angle which equals zero at the solar noon and varies at the rate of 15 per hour negative in the morning positive in the afternoon the sign function equals to 1 if ω is positive and is equal to 1 if ω is negative 3 δ 23 45 sin 360 284 n 365 where n is the day of year the solar azimuth angle γ s ranges from 0 to 2 π zero facing south and increasing in a counter clockwise direction 3 2 2 radiation partition in most cases only the global radiation is available boland et al 2001 proposed a method to estimate the diffuse fraction based on a logistical function this method establishes correlation among diffuse fraction clearness index and the solar time the clearness index is defined as the ratio of global radiation on a horizontal surface to the extraterrestrial radiation reindl et al 1990 with the solar constant time and location information the extraterrestrial radiation can be precisely calculated for any instantaneous time point duffie et al 2013 ridley et al 2010 improved the diffusion fraction method with more predictors which is referred to as the boland ridley lauret brl model the radiation partitioning model is 4 k d 1 1 exp β 0 β 1 k t β 2 a s t β 3 α s β 4 k t β 5 ψ where k d is the diffuse fraction k t and k t are the sub hourly and daily clearness index calculated based on the reference station data a s t is the apparent solar time α s is the solar altitude ψ is a persistence factor defined in ridley et al 2010 β 0 β 1 β 5 are the coefficients adopted from ridley et al 2010 3 2 3 topographic shading the terrain horizon angle is required to determine whether topographic shading occurs or not in this instance for a given point the horizon angle should be calculated for all possible solar azimuth angles which is computationally expensive to this end a rapid calculation algorithm is employed in this study after dozier et al 1981 the dtm data is transformed into matrix form so that it can be rotated the terrain horizon angle is calculated in forward and backward directions for each pixel element in the matrix the shading relationship is described using the discrete heaviside step function f x ruiz arias et al 2010 5 f x 0 x 0 1 x 0 the variable x in equation 5 is the difference between solar altitude angle and the horizon angle in the solar azimuth direction h γ s once the solar altitude angle α s and solar azimuth angle γ s are determined the estimated direct radiation is calculated as 6 r ˆ b f α s h γ s r b r b is the partitioned direct radiation 3 2 4 canopy shading when the dsm is used in the horizon angle calculation it results in a threshold for determining whether canopy shading may occur canopy shading on direct radiation occurs if the solar altitude angle falls between the terrain and vegetation horizon angles when this occurs the estimated direct radiation can be calculated as 7 r ˆ b τ b r b 8 τ b exp g l a d l where r ˆ b is the estimated direct radiation traveling through the canopy τ b is the canopy transmittance of beam radiation g is the extinction coefficient l a d is the leaf area density m2 m3 and l is the penetrating distance that light travels through the canopy the extinction coefficient g depends on light direction and leaf inclination distribution previous studies suggest that it is a function of solar altitude angle for a spherical leaf angle distribution campbell et al 1998 timlin et al 2014 9 g 0 5 sin α s the lad is defined as the ratio of leaf area to crown volume da silva et al 2012 in this study we evenly distribute leaves in a grid although canopy leaves tend to distribute at the upper part of a tree height at any given pixel given that a solar beam traveling through the canopy at different heights intercepts several pixels along its path fig 4 this approximation has potential to capture the canopy shading effect on beam radiation the leaf area density can then be expressed by lai 10 l a d l a i c a n o p y h e i g h t since the dems data include the height information of the land surface and canopy the canopy height for each grid cell can be estimated from the difference between the dtm and dsm to calculate the distance that the solar ray penetrates through the canopy a ray tracing method known as the voxel traverse algorithm is adopted amanatides et al 1987 musselman et al 2013 firstly a 3 d domain consisting of cubic volumes voxel is constructed according to the size of the study area having adequate voxel numbers ensures greater accuracy of distance estimation but it is computationally inefficient for a large number of voxels the resolution of the dems data in this study is 0 5 m so the height of each voxel is set to 0 5 m fig 4 the dems data include 730 998 pixels and the elevation difference between the highest surface and lowest terrain is 129 m thus the 3 d domain has 730 998 258 voxels according to the height information in the dems data the voxels are defined as three types above the canopy between the canopy and land surface and below the land surface once the point of interest is identified and light direction defined the ray tracing algorithm determines the positions of all voxels through which the light travels and the total distance for each time interval the solar altitude angle and solar azimuth angle are calculated for each grid cell and converted to a direction vector then the horizon angle is calculated based on the dsm and compared to the corresponding solar altitude angle to determine whether the grid cell is shaded by canopy only those shaded pixels are included in the transmittance calculation to ensure computational efficiency the averaged lad is calculated according to the voxel position and lai data the canopy transmittance of direct radiation for each shaded grid is calculated using equation 8 the canopy can also attenuate the diffuse radiation unlike the direct radiation diffuse radiation comes from all directions without a certain source two factors affect the intensity of the diffuse radiation from the sky 1 it has been found that the distribution of diffuse radiation is anisotropic especially in cloudy day kondratyev et al 1960 an anisotropic model proposed by hay 1993 is employed for its simplicity and accuracy hay 1993 yang 2016 the formulation gives by 11 d 1 a i cos 2 β 2 a i cos α sin α s where d is the diffuse factor ranging from 0 to 1 a i is the anisotropy index expressed as a i i i s c i is the normal incident direct radiation and i s c is the solar constant β is the surface slope the incident angle α between sun light and the normal to that surface can be calculated as duffie et al 2013 12 cos α sinδsin φ cosβ sinδcos φ sinβcosγ cosδcos φ cosβcosω cosδsin φ sinβcosγcosω cosδsinβsinγsinω where γ is the surface aspect other symbols are explained in section 3 2 1 2 the surrounding canopy attenuates the diffuse radiation from all directions in that case the canopy transmittance of diffuse radiation τ d is approximated by averaging the canopy transmittance over the whole hemisphere de castro et al 1998 in this research four azimuth west east north and south and three altitude angles 30 60 and 90 are selected for calculation the estimated diffuse radiation can be calculated as 13 r ˆ d d τ d r d where r d is the partitioned diffuse radiation 3 2 5 total radiation above and under the canopy the model is designed to estimate the incident solar radiation above and under the canopy for above canopy radiation the reference height is derived from the dsm data and the received surface is set to be horizontal the canopy shading mainly comes from surrounding canopy that is higher than itself 14 r ˆ a r ˆ b a r ˆ d a r ˆ b a and r ˆ d a are the estimated direct and diffuse radiation for above canopy condition for the under canopy radiation since the land surface is inclined the slope and aspect of land surface are considered in this study the reference radiation data from a weather station is received on a horizontal surface and therefore the direct radiation is restored to the correct solar direction at that point in time a geometric factor r cos α sin α s is applied for the direct radiation in complex terrain the radiation reflected from the surrounding terrain should be included in the calculation a common adopted formulation is employed here to describe the reflected radiation christian a gueymard 2009 15 r r ρ μ r r 16 μ 1 cosβ 2 where r r is the reflected radiation ρ is the foreground albedo in a forest area ρ assumes to be 0 2 μ is the reflection factor equation 16 and r r is the global radiation on a horizontal surface the under canopy radiation is calculated as follows 17 r ˆ u r r ˆ b u r ˆ d u r r r ˆ b u and r ˆ d u are the estimated direct and diffuse radiation for under canopy condition 3 3 model validation to validate the dst model in estimating the radiation transmittance the direct canopy transmittance of nfs and sfs sites are estimated based on the observation the dst model is compared with a point cloud based model in estimating transmittance musselman et al 2013 then the model simulations of incident radiation are compared with the observed radiation for above and under canopy conditions to evaluate the model s performance 3 3 1 estimates of direct canopy transmittance the direct shortwave canopy transmittance can be calculated as the ratio of the under canopy direct radiation and the above canopy direct radiation equation 18 milenković et al 2017 18 τ r b a r b u since the understory shortwave radiation was not measured in this study area an empirical equation considering both the surface temperature and earth sun distance is adopted to convert the understory net radiation to downwelling shortwave radiation equation 20 19 r n a 1 r s a 2 t a 3 d r a 4 where r n is the net radiation r s is the downwelling shortwave radiation t is air temperature d r is the inverse relative earth sun distance and a 1 a 2 a 3 and a 4 are coefficients the air temperature t was measured at both sites for more detail of this empirical method please refer to jiang et al 2015 to minimize the effect of diffuse radiation and topographic shading in the observation only data between the daylight hours of 10 00 and 14 00 local standard time and has the diffuse fraction k d less than 0 3 is considered then the daily average is calculated for evaluating the model performance 3 3 2 the point cloud based model musselman et al 2013 proposed a distributed solar radiation model based on lidar point cloud data the method consists of a voxel based canopy structure model and a ray trace model firstly the study area is assigned a certain vertical dimension defined by the elevation difference between the highest value of the dsm in the domain and the lowest value of the dtm the airborne scanning lidar data points are distributed in the voxel space according to the 3 d lidar laser return geographical coordinates each voxel is assigned a value corresponding to the number of laser point returns contained within that voxel extent voxels within which no returns are recorded are set a value of zero then a voxel traversal algorithm is applied to calculate the laser point returns along the direct solar ray direction for each surface grid the canopy transmittance for direct radiation τ i j t is computed as 20 τ i j t 1 min r e t u r n i j t p p where return i j t represents the accumulated lidar laser point returns along the solar ray path to surface point i j at time step t p is the maximum return parameter that represents the upper limit of accumulated point returns above which full canopy attenuation of the incoming solar direct beam is assumed in this study the vegetation type and the lidar point return density are different from musselman et al 2013 the p parameter is calibrated with the estimated direct canopy transmittance based on observation values of nfs and sfs the calibrated point cloud based model is then used as a benchmark for evaluating the dst model performance without a calibration 3 3 3 evaluation indexes several indexes are selected for model evaluation two metrics are evaluated for the transmittance estimate comparison the mean error me and mean absolute error mae the mae shows the absolute error between the model result and the observation the me determines whether the model result underestimates or overestimates the transmittance for the radiation simulation evaluation the correlation coefficient r between the model simulation and site observation both offset by the observed radiation at the reference station is calculated additionally the relative error of the sub hourly and daily radiation are evaluated by calculating the mean absolute percentage error mape and mean absolute percent deviation mapd jackson et al 2019 21 m a p e 1 n i 1 n y i x i r i 22 m a p d i 1 n y i x i i 1 n r i where n is the number of radiation data according to the simulated time interval in one day y i and x i are the simulated data and observed data r i is the radiation data of the reference station 4 evaluation and discussion 4 1 comparison of the transmittance estimates the me and mae of the direct radiation transmittance at the nfs and sfs sites are calculated to evaluate the dst model and the point cloud based model performance against the observation based transmittance table 1 the error metrics are calculated for four seasons the mae values of the dst model are smaller than the point cloud based model there are two improvements on the dst model which makes the dst estimated transmittance consistent with the observations firstly the dst model includes the light penetration distance among canopy for estimating the transmittance for the sfs site a significant difference is observed in which the me of the point cloud based model is larger than that of the dst model especially in summer the me positive values for summer suggest that the point cloud based model tends to overestimate the transmittance the airborne lidar only scans the forest canopy from above and therefore there is obstruction by higher canopy elements that reduces the probability for lower elements to be sampled popescu et al 2008 as a result the point cloud tends to have a larger distribution at the top of the canopy whilst the lower level canopy has less point samples even though it may have a dense leaf cover this leads to some apparent hollows in lidar point cloud data in the central and lower parts of the canopy the solar altitude angle is higher from late spring to early autumn which means the solar rays penetrate the canopy in more vertical direction at noon under this circumstance the dst model uses the light penetration distance from the incident point on the canopy to the land surface while the point cloud based model takes the point cloud information primarily representing the top of the canopy this explains why the point cloud based model tends to overestimate transmittance in summer secondly the dst model incorporates the information of lai to estimate the transmittance fig 3 shows that the vegetation cover in the south facing slope is much denser than the north facing slope as a result the direct shortwave canopy transmittance in the sfs is lower than the nfs not shown as can be seen in table 1 the point cloud based model performs differently for the two slopes in that the canopy transmittance tends to be underestimated for nfs and overestimated for sfs this result is likely due to the fact that a fixed maximum return parameter p in equation 20 was applied for a study area a need for spatially varying p parameter over complex terrain with variable vegetation conditions would increase the complexity of the point cloud model for complex terrain this level of error is not found in the dst model the better performance of the dst model in both the nfs and the sfs sites indicate that the lai information can improve the accuracy of transmittance estimation for the different facing slopes to further demonstrate the difference of the two models in simulating transmittance for different facing slopes fig 5 shows the direct radiation transmittance estimates of the two models for three days summer january 12 2014 spring april 10 2013 and winter july 11 2013 at 12 00 for the summer day in which the solar altitude angle is 76 5 the point cloud based model estimates is nearly evenly distributed for the entire study area while the dst model estimate shows a clear difference between the north facing south east section and south facing north west section slopes fig 5 since the north facing slope has less vegetation cover and small lai see in fig 3 the dst estimate captures the real situation while the point cloud based model fails in this regard for the spring day the solar altitude angle is 47 2 as the solar altitude angle is smaller the difference in the transmittance for the two opposite slopes is shown in the point cloud based model but not as much as that from the dst model in winter the solar altitude angle is 32 7 and there is an obvious difference in transmittance for the two opposite facing slopes estimated by the point cloud based model in summary the dst model tends to perform better than the point cloud based model with a smaller mae from the observation based estimated transmittance by using information of lai and the penetration distance the dst model can be used to estimate the canopy transmittance 4 2 above canopy radiation the 30 min radiation values are calculated for both nfs and sfs sites for eight representative clear and cloudy days in four seasons the results are compared to the site observations to evaluate the model performance in radiation simulation a 3 point equivalent to 1 h moving average is applied to observations and simulations at both sites to minimize the disturbance from temporal variation of sky conditions in order to remove the apparent effect of the daytime radiation cycle which is irrelevant of the model quality the offset radiation values are calculated the offset radiation values are the difference between the site observation model simulation and the reference station radiation fig 6 the correlation coefficient r between the offset values of observation and simulation is calculated for performance evaluation table 2 the mape and mapd are also calculated to examine the relative error at the nfs site the model simulations red solid line do not improve much as the offset values are not consistent with the observed one red dotted line from fig 6 a b we can find lower values appear at about 8 00 for the simulations while the observations close to zero that means it is shaded in the model at that moment which may not capture the real situation the r values between the offset irradiance of the observation and model simulation for nfs are relative low for nearly all examined clear and cloudy days suggesting that the shading situation is not well simulated for the nfs site there is very little shade above the canopy at the nfs site during the daytime at the top of the canopy the nfs site can receive almost same radiation as the reference station thus the dst model improvement is not clear at the sfs site a clear improvement is found in the model simulation the difference between the reference station and site observation blue dotted line shows significant variation with extreme low values occuring around 15 00 17 00 for most of the examined days negative offset values indicate that there are shading occurrences as can be seen in fig 6 the model simulation highly correlated with the observation the r values for the sfs site are high for most of the examined days table 2 the results show that the dst model can well capture the shading effects on the sfs site especially for the time with obvious shading occurrence compared to the nfs site there are more shading occurrences on the sfs site the results show that the dst model performs better on the sfs site while for the nfs site since there are fewer shading occurrences the above canopy radiation is consistent with the reference station data the model simulated radiation is also close to the real situation as the offset values are close to zero for most of the time intervals with the mape and mapd indexes being low for all the seasons and weather conditions suggesting that the relative error between the observed and simulated radiation is small in general the proposed dst model is suitable for simulating the above canopy radiation for both slopes 4 3 under canopy radiation the under canopy radiation values offset by the reference global radiation are calculated for the same days as in section 4 2 and compared with the independent shortwave radiation data section 3 3 the r value is calculated to evaluate the correlation between the model simulation and observation and the mape and mapd are calculated to evaluate the relative error table 3 the results show that the model simulation is well correlated with observed radiation at both the nfs and sfs sites as can be seen in fig 7 the shading exists for the entire daytime hours at both sites since the offset values are negative for selected days of different cloudiness in four seasons for the nfs site the offset values are less negative than the sfs site suggesting that the shading effect at the nfs site is weaker than in the sfs site this is consistent with the sparser vegetation cover on the nfs for both the nfs and sfs sites the r values are above 0 8 for almost all the examined days one exception is the clear day in summer where the model simulation on the nfs largely underestimates the under canopy radiation resulting in a lower r nevertheless the mape and mapd is 0 29 and 0 16 respectively this result indicates that the error is relatively small comparing to the total incident radiation comparing to the above canopy radiation a large improvement for the under canopy radiation simulation is found for the nfs site which demonstrates the capability of the dst model in simulating the canopy shading effect the mape and mapd are larger than the above canopy radiation simulation but it is still within an acceptable range mape less than 0 34 and mapd less than 0 18 4 4 annual under canopy insolation daily insolation mj m2 is the total amount of solar radiation energy received over a given surface area during a day in order to test the dst model performance one year of under canopy insolation data is simulated and compared with the converted understory shortwave radiation data fig 8 shows the daily insolation simulations of the nfs and sfs from 06 04 2013 to 05 04 2014 a seven day interval is used for the canopy transmittance calculation to reduce the computation cost i e data storage and processing time while still capturing the solar geometry variation the results of daily insolation over one year show a strong consistency between the simulation and measured data in comparison to the reference radiation grey line both simulated results of the nfs and sfs sites are close to the observation based estimated radiation at both sites in terms of the seasonal variation the simulated radiation is lower than observation from spring september 1 to the end of summer february 29 for the remaining days the simulated radiation is very close to the observation particularly for the sfs site the most possible reason for this seasonal variation of model performance is that the dems used in the model were obtained in june 2013 this could explain why the simulations for the days in a year close to the obtained date of the dems are more accurate if there were more representative data corresponding to the different seasons the model performance could be improved 4 5 demonstration of spatially distributed above and under canopy radiation fig 9 shows the simulated daily insolation of the whole study area it is not surprising that the top of canopy receives more radiation than the land surface in fig 9 we can see that the simulated radiation on the top of canopy is similar to that on a flat unshaded surface the difference in simulated insolation between different parts of the catchment is small for the under canopy radiation the amount of radiation in the northwest area of the catchment is significantly lower than in the southeast two factors may account for the difference 1 the topography effect influences the incident radiation the northwest area of the catchment is facing south east with a slope ranging from 10 to 30 while the southeast area of the catchment is facing north with a slope ranging from 20 to 30 since the study area is in south australia and the latitude is about 35 s the sun rises and moves in the northern sky of the area hence the incident angle of the northwest area of the catchment would be much higher than that of the southeast for example the impact of the incident angle to radiation is shown in the black circled area in fig 9 b it is an exposed area without much shading facing north with a slope of around 25 the radiation of this area would be the same as that in the northeast corner about 13 mj m2 if it is a flat surface however the simulated insolation is about 15 mj m2 consistent with the fact that the solar incident angle on this surface is smaller than that for a horizontal surface 2 the northwest section of the catchment is covered by more vegetation suggested by its larger lai than the southeast section fig 3 therefore there is more radiation being obstructed by the canopy resulting in lower simulated under canopy insolation to test the effects of spatial distribution and shading on simulated incident radiation 10 000 sample points for two different slope orientation sections composed of mostly north facing or south facing slopes with a 22 5 degree buffer either side are randomly selected fig 10 since the data do not meet the parametric statistic test assumptions of normality and homogeneity of variance the kruskal wallis test is conducted to compare the effect of canopy shading on the incident radiation there is a significant difference in incident radiation between the different facing slopes and positions p 0 001 the pairwise comparison results indicate that the average above canopy insolation for both north facing and south facing sections are significantly different from that of the under canopy insolation for the above canopy insolation the north facing section mean 11 14 mj m2 and south facing section mean 10 37 mj m2 are significantly different p 0 001 for under canopy insolation the average insolation for north facing section mean 6 28 mj m2 is significantly larger than that for the south facing section mean 2 35 mj m2 with p 0 001 the difference in mean value for the under canopy radiation between the two sites is larger than that of the above canopy radiation this large difference in under canopy radiation may help explain the contrasting understory vegetation between the two slopes in this dry summer climate site fig 11 for the simulated above canopy insolation the maximum and upper quartiles are close to each other between the two sections indicating that the upper limit of insolation has no difference for the opposite orientation slopes however the minimum and lower quartiles for south facing section are smaller than north facing section more canopy shading exists on the south facing section which results in the difference in minimum and lower quartiles the simulated under canopy insolation is generally lower than the above canopy insolation as expected however on the north facing section the maximum of the simulated under canopy insolation is even larger than the simulated above canopy insolation the slope orientation accounts for this discrepancy since the equator facing slope can receive more irradiance than a horizontal plane here the simulated above canopy insolation is meant for a horizontal plane above the canopy it may be different from the insolation the canopy actually receives due to a possible difference in solar incident angles to the canopy between the two slopes the current dst model does not simulate this difference altogether these results suggest that the dst model can well simulate the spatial variability of solar radiation above and below the canopy on complex terrain 4 6 model efficiency the computation efficiency is one aspect of users concern here is the relevant result based on the coded model in matlab 2020 platform running on a 4 cores 3 40 ghz inter r core tm i5 7500 cpu and 16 gb ram pc the computing time can be site specific which is outlined here the study area includes 611 481 grid cells and covers about 0 15 km2 the average computing time is listed in table 4 calculating the canopy transmittance is the most time consuming process compared to the global radiation calculation calculating the under canopy transmittance takes nearly twice as long as the above canopy transmittance the major computational process in calculating canopy transmittance is to estimate the distance that light penetrates the canopy the distance is estimated grid by grid using the voxel traverse algorithm although the calculation of canopy transmittance for a large study area can be computationally intensive some measures can be taken to simplify the process they include 1 since the solar geometry does not change significantly within a short period canopy transmittance can be estimated at a certain time interval e g a week and 2 once the canopy transmittance values are calculated for one year they can be used repetitively for other years if the canopy conditions do not vary much between years 5 conclusions a new distributed solar irradiation model the double shading transposition model which considers both topographic and canopy shading effects is developed and tested for complex terrain the dst model simulates radiation distribution in both above and under vegetation canopy in any weather conditions the model simulations in general agree well with the observations for the under canopy radiation simulation a significant difference in daily insolation is found for opposite north and south slope orientations these results indicate that the dst model is capable of capturing the shading effects caused by topography and vegetation canopy the spatial variation of solar radiation distribution is well simulated for the complex mountainous forested terrain of the study area the study provides researchers with an all condition model to generate solar radiation distribution in vegetated areas where solar radiation is expected to be significantly modulated by trees or other physical obstructions being able to simulate radiation fields in areas of complex terrain with variable vegetation cover opens a diverse set of possibilities for soil scientists ecologists hydrologists geomorphologists and other earth scientists for which accurate estimations of surface energy loads are important for example the dst model provides a key variable for hydrologist to examine evapotranspiration patterns for above and under canopy conditions in a distributed way other potential uses include insolation simulation to locate thermal shelters canopies that shelter an animal from microclimatic extremes in the forest which has been increasingly studied in biological investigations due to global warming melin et al 2014 future studies will focus on the dst model application to areas of different climatic and environmental conditions software and data availability the code used in this study is available at https au mathworks com matlabcentral fileexchange 103705 a distributed solar radiation model for complex terrain declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements collection of most field data for this study was funded by national centre for groundwater research and training australia sr08000001 wenjie liu acknowledges the support from china scholarship council x he s and his contribution was supported by the national natural science foundation of china grant number 41472238 thanks to hailong wang xiang xu zijuan deng and yuting yang for assisting in field data collection langdon badger kindly provided free access to the field site data and code requests can be addressed to the corresponding author huade guan 
25634,solar radiation driving land atmosphere interactions and terrestrial carbon assimilation varies both temporally and spatially due to surface gradient slope aspect and vegetation shading here we present a double shading transposition model dst for simulating temporal and spatial distribution of downwelling shortwave solar radiation the dst model considers influences of both topographic and canopy structure on radiation transmission it simulates sub hourly radiation distribution on and beneath vegetation canopy for any weather conditions the model is tested against observations at two opposite facing slopes the simulated above and under canopy radiation compares well with the observed data the model provides a convenient and effective tool for simulating spatial and temporal variation of solar radiation on mountainous areas with vegetation cover and has great potential in ecohydrological modelling and catchment management keywords shortwave solar radiation topographic shading canopy transmission land surface energy balance 1 introduction solar radiation on earth s surface drives hydrological cycles powers meteorological systems and sustains ecosystems as a key variable in hydrological and meteorological models it largely determines the land surface energy balance and governs plenty of surface processes antonanzas torres et al 2019 sheng et al 2009 wild 2009 spatial variation of incident radiation also regulates vegetation spatial patterns especially in water limited areas fan et al 2019 gutiérrez jurado et al 2013 knowledge of temporal and spatial patterns of solar radiation is also essential for solar energy system designing lopez et al 2007 solar radiation is emitted from the sun with an average temperature of 6000 k over 95 of the solar energy is made up of shortwave radiation wavelength range of 0 29 3 μm which constitutes the largest component of the land surface energy balance duffie et al 2013 kumar et al 1997 for simplicity the term radiation in this paper denotes shortwave radiation when solar radiation penetrates through the atmosphere part of it that is not direct radiation is scattered in various directions and becomes diffuse radiation on a rugged surface radiation reflected from the surroundings can also contribute to the incident radiation at a surface point duffie et al 2013 thus the total solar radiation often referred to as global radiation reaching a point on the earth s surface consists of direct radiation diffuse radiation and in some case reflected radiation as well various models have been developed to simulate solar radiation on surfaces jeff dozier 1980 dubayah et al 1995 christian a gueymard et al 2008 kumar et al 1997 lefèvre et al 2013 liu et al 1960 radosevic et al 2020 swift jr 1976 j zhang et al 2017 s zhang et al 2020 one model type is based on extraterrestrial radiation which is referred to as a clear sky model this is the most fundamental model for estimating solar radiation on the earth surface antonanzas torres et al 2019 such a model calculates radiation under a clear sky condition which is the upper limit of real radiation on the earth surface lefèvre et al 2013 several factors are considered in a clear sky model first the extraterrestrial radiation is calculated for a location based on latitude and the time then the interactions between the atmosphere and solar radiation are considered mckenney 1999 most of the clear sky models simulate radiation transfer processes in the atmosphere and interactions with different atmospheric components various types of atmospheric molecules such as water oxygen and ozone can absorb solar radiation and turn it into heat while other gases and particulates in the atmosphere can scatter solar radiation in all directions kumar et al 1997 these effects are incorporated in the radiation transfer model to estimate shortwave radiation reaching a surface on earth various factors can limit the application of clear sky models for example one limitation is a lack of data to properly parameterize simulations of radiation transfer through the atmosphere badescu et al 2012 more importantly clear sky models do not consider cloud effects which significantly affect radiation transmission limiting their use to places or time periods with little cloud cover conditions depending on local climatic patterns there are situations where up to 80 of the down welling solar radiation can be intersected by clouds kambezidis et al 2016 serrano et al 2015 to overcome this shortcoming of clear sky radiation models attempts have been made to simulate the cloud effect on radiation transfer by quantifying the cloud optical depth effectively extending a clear sky model into an all condition model kambezidis et al 2016 serrano et al 2015 tang et al 2016 however it is difficult to monitor the spatial and temporal variations of cloud distribution and cloud properties due to the sparsity and heterogeneity of observation stations zajaczkowski et al 2013 to address these issues models based on point radiation measurements called transposition models have been developed liu et al 1963 yang 2016 a transposition model takes the observed radiation on a horizontal surface as an input to simulate radiation of a tilted surface over a certain area christian a gueymard et al 2008 in such a model geometry information of the surface and solar zenith angle are considered a common use of the transposition model is to optimize the position of flat plane photovoltaic arrays khatib et al 2015 yang 2016 for example it can be used to generate diagrams showing incident solar radiation on a surface for any time as a function of surface geometry additionally a transposition model requires fewer variables than an all condition model although it requires measured radiation on a horizontal plane at a vicinity point hereafter referred to as the reference radiation by using concurrent measured radiation a transposition model can be applied under any weather conditions another important phenomenon that can exert a strong influence on surface solar radiation is shading shading can produce significant temporal and spatial heterogeneity in radiation distribution especially in complex terrain marsh et al 2012 s zhang et al 2020 zou et al 2007 to accurately describe the heterogeneity of solar radiation a distributed solar radiation model is required most previous studies focus on the topography effect on the solar radiation and the models are constructed by deriving the landscape components and structures from digital elevation model dem jeff dozier 1980 dubayah et al 1995 kumar et al 1997 in recent years researchers have started to examine the spatial and temporal variation of solar radiation in urban environments for instance yu et al 2009 investigated the spatiotemporal variability of solar radiation in downtown houston texas and designed a solar flux model for urban vegetation planting and management liang et al 2014 proposed a 3 d method for computing and visualizing urban solar radiation based on image space data representation erdélyi et al 2014 proposed a numerical model to simulate distribution of direct and diffuse radiation in an urban environment these studies primarily addressed the shading effects from surrounding terrain or buildings on solar radiation distribution vegetation can partly intercept solar radiation which otherwise would cast on the ground thus modifying the patterns of incident radiation in forested and vegetated areas therefore knowing how much radiation on the top of vegetation and the ground surface below the tree canopy is essential in ecohydrological investigations in complex terrain where small variations in energy balance may sensibly modify carbon and water fluxes dynamics metzen et al 2019 seyednasrollah et al 2014 proposed a shortwave radiation model for snowmelt prediction which calculates radiation transmission as a function of the path length of the solar beam as it passes through the gap in the forest canopy however this model can only estimate the incident radiation for a single point with the advent of high resolution topography from novel photogrammetric and light detection and ranging lidar technology data showing the 3 d structure of landscapes is becoming more frequently available greatly expanding the potential to incorporate surface geometry and structure in earth system models crosby et al 2020 for example gutiérrez jurado et al 2013 explored the topography vegetation controls on annual and seasonal irradiance at opposite facing hillslopes in new mexico combining lidar derived digital surface models that include vegetation structures and bare earth surface and a distributed solar radiation model bode et al 2014 estimated sub canopy radiation transmittance based on a light penetration index lpi obtained from lidar peng et al 2014 proposed a spatiotemporally explicit 3d ray trace model to provide spatiotemporal patterns of understory light density nyman et al 2017 tested four sub canopy radiation models and found that the model based on path length simulation had the best performance the above mentioned methods take the lidar point cloud density data or the length of solar ray crossing the vegetation crown as inputs for canopy transmission calculation however characterizing the light attenuation effect based only on the path length attribute is insufficient since the light attenuation along its transmission path through the crown is also affected by the non random distribution of foliage elements zeng et al 2019 leaf area index lai a commonly available data quantifying the leaf density within the canopy can be used to parameterize the light attenuation effect through the canopy nevertheless few solar radiation models take advantage of lai to improve radiation estimates in vegetated areas moreover to our knowledge previous distributed solar radiation models either work for clear sky conditions or require cloudiness data in the study area a distributed 3 d solar radiation model for all weather conditions remains to be developed here we present the development of a solar radiation model for simulating the temporal and spatial distribution of incident radiation on and beneath a vegetated surface for any weather conditions based on a reference point shortwave irradiance measurement both the topographic shading and vegetation canopy shading are considered in order to simulate the canopy shading the dems including the digital terrain model dtm which is the elevation of bare earth and the digital surface model dsm which is the elevation of all objects including vegetation and the lai data are used one year of shortwave irradiance data above canopy and net radiation data below canopy on two opposite facing slopes north and south facing of a steep vegetated catchment at mount wilson south australia is used to evaluate the model performance 2 the double shading transposition model the model proposed here is a transposition model that incorporates the local topographic and canopy shading effects we refer to this model as the double shading transposition dst model the model is developed based on the conceptualization shown in fig 1 topographic shading can be caused by the surface itself self shading and the surrounding terrain the self shading results from surface orientation and it can be calculated with the solar altitude angle solar azimuth angle and slope and aspect derived from dtm duffie et al 2013 revfeim 1982 s zhang et al 2020 the surrounding terrain shading is complex since it results from multiple features interacting with and intercepting light from various directions to estimate when shading occurs as a result of the surrounding terrain horizon angles are calculated using the dems fig 1 at the solar azimuth and are compared with the solar altitude angle the horizon angle in a direction is defined as the angle between the horizontal plane and the upper view edge of elevated terrain in that direction ruiz arias et al 2010 if at the time of interest the terrain horizon angle at the solar azimuth direction is larger than the solar altitude angle the surface is shaded by the terrain and has no direct radiation if the solar altitude angle is between the terrain and vegetation horizon angles the surface is shaded by vegetation canopy which will be explained later different from direct radiation diffuse radiation comes from all directions from the sky usually the sky view factor is adopted to estimate the topography effect on diffuse radiation in solar radiation models antonić 1998 jeff dozier et al 1990 li et al 2016 however since the diffuse radiation does not distribute isotropically in the hemisphere it is inaccurate to simply use the sky view factor to calculate the diffuse radiation moreover the sky view factor only accounts for diffuse radiation reduction from the topography effect and is therefore not suitable for simulating the shading effect that is caused by penetrable objects such as vegetation canopy in the dst model we calculated the anisotropic diffuse radiation and the attenuation of diffuse radiation is estimated based on the canopy structure similar to topographic shading the incident solar radiation on the land surface may be obstructed by vegetation such that the radiation is only partially intercepted by the canopy in this process vegetation canopy structure plays an important role as can be seen in fig 1 the incident direct radiation on both above and under canopy can be obstructed by vegetation occurring in the beam direction to simulate the process of light traveling through the canopy the canopy is considered as a turbid medium da silva et al 2008 to detect the spatial variation of the forest structure the dsm is employed since it can capture the morphology of the vegetation above the land surface we combined it with lai to estimate the transmittance of solar ray crossing the canopy according to the beer lambert law ross 2012 for direct radiation at a certain point in time the canopy transmittance is calculated for the direction in which the solar ray comes in contact with the canopy surface at that moment the canopy transmittance of diffuse radiation is estimated according to the mean canopy transmittance value for all directions of the hemisphere a flowchart for the dst model is shown in fig 2 the model requires the following data 1 time and location information including the start and end dates and the latitude and longitude 2 topography and vegetation data including dems and lai and 3 reference radiation data on a horizontal surface free of the shading effects which shares the same sky condition with the area of interest with solar radiation to be simulated given that direct radiation and diffuse radiation behave differently in the two shading processes a decomposition model is adopted to partition the observed global radiation at the reference station into direct and diffuse components ridley et al 2010 the incident solar radiation on a surface at a given point in time is related to the solar geometric and the topographic factors of the receiving surface at that instance similar to other radiation models the dst model starts from calculating the solar geometry and topographic features kumar et al 1997 for an inclined surface the reflected radiation from surrounding terrain is also considered in the dst model 3 methodology 3 1 study area and data the study area is a relatively steep vegetated catchment 0 1 km2 located at mount wilson 138 64 e 35 21 s in south australia fig 3 most of the area is covered by native vegetation with an average tree height about 10 m the mean annual precipitation is 716 mm australia s bureau of meteorology station id 23906 http www bom gov au climate data stations xu et al 2019 this area is characterized by a mediterranean climate which experiences cold wet winters may july and hot dry summers dec feb resulting in large variations in solar radiation throughout the year the maximum irradiance radiation flux w m2 in summer can be up to 1200 w m2 but in winter it is about 600 w m2 based on a reference station detailed later in addition to this temporal variation spatial variation of solar irradiation is more complicated due to steep topography relief and slope induced variation in vegetation cover xu et al 2017 this spatial variability provides a range of conditions to test the proposed dst model three observation stations were installed to measure shortwave radiation one site was a weather station located in an open area at the top edge of the catchment which is hereafter referred to as the reference station this station measured the unshaded shortwave solar irradiance with a cm3 pyranometer kipp zonen the two other sites were located on two opposite facing hill slopes of the small catchment one with a north facing slope nfs and the other with a south facing slope sfs for these two sites a cnr4 net radiometer kipp zonen was installed at an elevation of 12 m above the ground and measured both upwelling and downwelling shortwave and longwave radiation i e above the tree canopy in addition a nr lite2 net radiometer kipp zonen was installed at 1 7 m above the ground under the tree canopy to measure understory net radiation at both sites irradiance data was recorded from 06 04 2013 to 05 04 2014 with a 30 min sample interval the quality control process includes the following rules 1 missing or incomplete data is excluded 2 recorded radiation exceeding extraterrestrial radiation is excluded 3 for the measured above canopy radiation a 3 point 1 h moving average is adopted for measured under canopy net radiation the negative values are eliminated and a 5 point 2 h moving average is adopted since a high frequency variation exists in the measured data due to temporal variation of canopy shadow the weather station data is taken as a reference station input for the model irradiance data of 361 days is available the data recorded at the nfs and sfs sites is used for model validation for measurements above the canopy there is irradiance data for 343 days at the nfs site and 208 days at the sfs site for measurements under the tree canopy half hourly net radiation data is available for 313 days at the nfs site and for 339 days at the sfs site the dtm and dsm data were derived from airborne lidar data obtained by airborne research australia in july 2013 with a point return density of 45 point m2 and have a spatial resolution of 0 5 m the catchment has a dtm range from 298 to 410 m and a dsm range from 298 to 427 m the slope of the study area ranges from 0 to 30 according to the dtm the lai data for the study area was estimated from the available ndvi data following the method described in tillack et al 2014 and is shown in fig 3 the ndvi data was obtained from the hyper spectral images collected by airborne research australia in november 2013 with a spatial resolution of 1 m it was resampled to 0 5 m for this study overall the lai of south facing slopes northern section is larger than north facing slopes southern section 3 2 basic equations 3 2 1 solar geometry the sun s position in the sky needs to be calculated for each time interval which can be expressed by solar altitude angle and solar azimuth angle solar altitude angle α s is the angle between a horizontal plane and the line to the sun and solar azimuth angle γ s is the angular displacement from south of the projection of direct radiation on the horizontal plane duffie et al 2013 1 sin α s cos φ cos δ cos ω sin φ sin δ 2 γ s sign ω π cos 1 sin α s sin φ sin δ cos α s cos φ π where φ is the latitude in radians δ is the solar declination equation 3 and ω is the hour angle which equals zero at the solar noon and varies at the rate of 15 per hour negative in the morning positive in the afternoon the sign function equals to 1 if ω is positive and is equal to 1 if ω is negative 3 δ 23 45 sin 360 284 n 365 where n is the day of year the solar azimuth angle γ s ranges from 0 to 2 π zero facing south and increasing in a counter clockwise direction 3 2 2 radiation partition in most cases only the global radiation is available boland et al 2001 proposed a method to estimate the diffuse fraction based on a logistical function this method establishes correlation among diffuse fraction clearness index and the solar time the clearness index is defined as the ratio of global radiation on a horizontal surface to the extraterrestrial radiation reindl et al 1990 with the solar constant time and location information the extraterrestrial radiation can be precisely calculated for any instantaneous time point duffie et al 2013 ridley et al 2010 improved the diffusion fraction method with more predictors which is referred to as the boland ridley lauret brl model the radiation partitioning model is 4 k d 1 1 exp β 0 β 1 k t β 2 a s t β 3 α s β 4 k t β 5 ψ where k d is the diffuse fraction k t and k t are the sub hourly and daily clearness index calculated based on the reference station data a s t is the apparent solar time α s is the solar altitude ψ is a persistence factor defined in ridley et al 2010 β 0 β 1 β 5 are the coefficients adopted from ridley et al 2010 3 2 3 topographic shading the terrain horizon angle is required to determine whether topographic shading occurs or not in this instance for a given point the horizon angle should be calculated for all possible solar azimuth angles which is computationally expensive to this end a rapid calculation algorithm is employed in this study after dozier et al 1981 the dtm data is transformed into matrix form so that it can be rotated the terrain horizon angle is calculated in forward and backward directions for each pixel element in the matrix the shading relationship is described using the discrete heaviside step function f x ruiz arias et al 2010 5 f x 0 x 0 1 x 0 the variable x in equation 5 is the difference between solar altitude angle and the horizon angle in the solar azimuth direction h γ s once the solar altitude angle α s and solar azimuth angle γ s are determined the estimated direct radiation is calculated as 6 r ˆ b f α s h γ s r b r b is the partitioned direct radiation 3 2 4 canopy shading when the dsm is used in the horizon angle calculation it results in a threshold for determining whether canopy shading may occur canopy shading on direct radiation occurs if the solar altitude angle falls between the terrain and vegetation horizon angles when this occurs the estimated direct radiation can be calculated as 7 r ˆ b τ b r b 8 τ b exp g l a d l where r ˆ b is the estimated direct radiation traveling through the canopy τ b is the canopy transmittance of beam radiation g is the extinction coefficient l a d is the leaf area density m2 m3 and l is the penetrating distance that light travels through the canopy the extinction coefficient g depends on light direction and leaf inclination distribution previous studies suggest that it is a function of solar altitude angle for a spherical leaf angle distribution campbell et al 1998 timlin et al 2014 9 g 0 5 sin α s the lad is defined as the ratio of leaf area to crown volume da silva et al 2012 in this study we evenly distribute leaves in a grid although canopy leaves tend to distribute at the upper part of a tree height at any given pixel given that a solar beam traveling through the canopy at different heights intercepts several pixels along its path fig 4 this approximation has potential to capture the canopy shading effect on beam radiation the leaf area density can then be expressed by lai 10 l a d l a i c a n o p y h e i g h t since the dems data include the height information of the land surface and canopy the canopy height for each grid cell can be estimated from the difference between the dtm and dsm to calculate the distance that the solar ray penetrates through the canopy a ray tracing method known as the voxel traverse algorithm is adopted amanatides et al 1987 musselman et al 2013 firstly a 3 d domain consisting of cubic volumes voxel is constructed according to the size of the study area having adequate voxel numbers ensures greater accuracy of distance estimation but it is computationally inefficient for a large number of voxels the resolution of the dems data in this study is 0 5 m so the height of each voxel is set to 0 5 m fig 4 the dems data include 730 998 pixels and the elevation difference between the highest surface and lowest terrain is 129 m thus the 3 d domain has 730 998 258 voxels according to the height information in the dems data the voxels are defined as three types above the canopy between the canopy and land surface and below the land surface once the point of interest is identified and light direction defined the ray tracing algorithm determines the positions of all voxels through which the light travels and the total distance for each time interval the solar altitude angle and solar azimuth angle are calculated for each grid cell and converted to a direction vector then the horizon angle is calculated based on the dsm and compared to the corresponding solar altitude angle to determine whether the grid cell is shaded by canopy only those shaded pixels are included in the transmittance calculation to ensure computational efficiency the averaged lad is calculated according to the voxel position and lai data the canopy transmittance of direct radiation for each shaded grid is calculated using equation 8 the canopy can also attenuate the diffuse radiation unlike the direct radiation diffuse radiation comes from all directions without a certain source two factors affect the intensity of the diffuse radiation from the sky 1 it has been found that the distribution of diffuse radiation is anisotropic especially in cloudy day kondratyev et al 1960 an anisotropic model proposed by hay 1993 is employed for its simplicity and accuracy hay 1993 yang 2016 the formulation gives by 11 d 1 a i cos 2 β 2 a i cos α sin α s where d is the diffuse factor ranging from 0 to 1 a i is the anisotropy index expressed as a i i i s c i is the normal incident direct radiation and i s c is the solar constant β is the surface slope the incident angle α between sun light and the normal to that surface can be calculated as duffie et al 2013 12 cos α sinδsin φ cosβ sinδcos φ sinβcosγ cosδcos φ cosβcosω cosδsin φ sinβcosγcosω cosδsinβsinγsinω where γ is the surface aspect other symbols are explained in section 3 2 1 2 the surrounding canopy attenuates the diffuse radiation from all directions in that case the canopy transmittance of diffuse radiation τ d is approximated by averaging the canopy transmittance over the whole hemisphere de castro et al 1998 in this research four azimuth west east north and south and three altitude angles 30 60 and 90 are selected for calculation the estimated diffuse radiation can be calculated as 13 r ˆ d d τ d r d where r d is the partitioned diffuse radiation 3 2 5 total radiation above and under the canopy the model is designed to estimate the incident solar radiation above and under the canopy for above canopy radiation the reference height is derived from the dsm data and the received surface is set to be horizontal the canopy shading mainly comes from surrounding canopy that is higher than itself 14 r ˆ a r ˆ b a r ˆ d a r ˆ b a and r ˆ d a are the estimated direct and diffuse radiation for above canopy condition for the under canopy radiation since the land surface is inclined the slope and aspect of land surface are considered in this study the reference radiation data from a weather station is received on a horizontal surface and therefore the direct radiation is restored to the correct solar direction at that point in time a geometric factor r cos α sin α s is applied for the direct radiation in complex terrain the radiation reflected from the surrounding terrain should be included in the calculation a common adopted formulation is employed here to describe the reflected radiation christian a gueymard 2009 15 r r ρ μ r r 16 μ 1 cosβ 2 where r r is the reflected radiation ρ is the foreground albedo in a forest area ρ assumes to be 0 2 μ is the reflection factor equation 16 and r r is the global radiation on a horizontal surface the under canopy radiation is calculated as follows 17 r ˆ u r r ˆ b u r ˆ d u r r r ˆ b u and r ˆ d u are the estimated direct and diffuse radiation for under canopy condition 3 3 model validation to validate the dst model in estimating the radiation transmittance the direct canopy transmittance of nfs and sfs sites are estimated based on the observation the dst model is compared with a point cloud based model in estimating transmittance musselman et al 2013 then the model simulations of incident radiation are compared with the observed radiation for above and under canopy conditions to evaluate the model s performance 3 3 1 estimates of direct canopy transmittance the direct shortwave canopy transmittance can be calculated as the ratio of the under canopy direct radiation and the above canopy direct radiation equation 18 milenković et al 2017 18 τ r b a r b u since the understory shortwave radiation was not measured in this study area an empirical equation considering both the surface temperature and earth sun distance is adopted to convert the understory net radiation to downwelling shortwave radiation equation 20 19 r n a 1 r s a 2 t a 3 d r a 4 where r n is the net radiation r s is the downwelling shortwave radiation t is air temperature d r is the inverse relative earth sun distance and a 1 a 2 a 3 and a 4 are coefficients the air temperature t was measured at both sites for more detail of this empirical method please refer to jiang et al 2015 to minimize the effect of diffuse radiation and topographic shading in the observation only data between the daylight hours of 10 00 and 14 00 local standard time and has the diffuse fraction k d less than 0 3 is considered then the daily average is calculated for evaluating the model performance 3 3 2 the point cloud based model musselman et al 2013 proposed a distributed solar radiation model based on lidar point cloud data the method consists of a voxel based canopy structure model and a ray trace model firstly the study area is assigned a certain vertical dimension defined by the elevation difference between the highest value of the dsm in the domain and the lowest value of the dtm the airborne scanning lidar data points are distributed in the voxel space according to the 3 d lidar laser return geographical coordinates each voxel is assigned a value corresponding to the number of laser point returns contained within that voxel extent voxels within which no returns are recorded are set a value of zero then a voxel traversal algorithm is applied to calculate the laser point returns along the direct solar ray direction for each surface grid the canopy transmittance for direct radiation τ i j t is computed as 20 τ i j t 1 min r e t u r n i j t p p where return i j t represents the accumulated lidar laser point returns along the solar ray path to surface point i j at time step t p is the maximum return parameter that represents the upper limit of accumulated point returns above which full canopy attenuation of the incoming solar direct beam is assumed in this study the vegetation type and the lidar point return density are different from musselman et al 2013 the p parameter is calibrated with the estimated direct canopy transmittance based on observation values of nfs and sfs the calibrated point cloud based model is then used as a benchmark for evaluating the dst model performance without a calibration 3 3 3 evaluation indexes several indexes are selected for model evaluation two metrics are evaluated for the transmittance estimate comparison the mean error me and mean absolute error mae the mae shows the absolute error between the model result and the observation the me determines whether the model result underestimates or overestimates the transmittance for the radiation simulation evaluation the correlation coefficient r between the model simulation and site observation both offset by the observed radiation at the reference station is calculated additionally the relative error of the sub hourly and daily radiation are evaluated by calculating the mean absolute percentage error mape and mean absolute percent deviation mapd jackson et al 2019 21 m a p e 1 n i 1 n y i x i r i 22 m a p d i 1 n y i x i i 1 n r i where n is the number of radiation data according to the simulated time interval in one day y i and x i are the simulated data and observed data r i is the radiation data of the reference station 4 evaluation and discussion 4 1 comparison of the transmittance estimates the me and mae of the direct radiation transmittance at the nfs and sfs sites are calculated to evaluate the dst model and the point cloud based model performance against the observation based transmittance table 1 the error metrics are calculated for four seasons the mae values of the dst model are smaller than the point cloud based model there are two improvements on the dst model which makes the dst estimated transmittance consistent with the observations firstly the dst model includes the light penetration distance among canopy for estimating the transmittance for the sfs site a significant difference is observed in which the me of the point cloud based model is larger than that of the dst model especially in summer the me positive values for summer suggest that the point cloud based model tends to overestimate the transmittance the airborne lidar only scans the forest canopy from above and therefore there is obstruction by higher canopy elements that reduces the probability for lower elements to be sampled popescu et al 2008 as a result the point cloud tends to have a larger distribution at the top of the canopy whilst the lower level canopy has less point samples even though it may have a dense leaf cover this leads to some apparent hollows in lidar point cloud data in the central and lower parts of the canopy the solar altitude angle is higher from late spring to early autumn which means the solar rays penetrate the canopy in more vertical direction at noon under this circumstance the dst model uses the light penetration distance from the incident point on the canopy to the land surface while the point cloud based model takes the point cloud information primarily representing the top of the canopy this explains why the point cloud based model tends to overestimate transmittance in summer secondly the dst model incorporates the information of lai to estimate the transmittance fig 3 shows that the vegetation cover in the south facing slope is much denser than the north facing slope as a result the direct shortwave canopy transmittance in the sfs is lower than the nfs not shown as can be seen in table 1 the point cloud based model performs differently for the two slopes in that the canopy transmittance tends to be underestimated for nfs and overestimated for sfs this result is likely due to the fact that a fixed maximum return parameter p in equation 20 was applied for a study area a need for spatially varying p parameter over complex terrain with variable vegetation conditions would increase the complexity of the point cloud model for complex terrain this level of error is not found in the dst model the better performance of the dst model in both the nfs and the sfs sites indicate that the lai information can improve the accuracy of transmittance estimation for the different facing slopes to further demonstrate the difference of the two models in simulating transmittance for different facing slopes fig 5 shows the direct radiation transmittance estimates of the two models for three days summer january 12 2014 spring april 10 2013 and winter july 11 2013 at 12 00 for the summer day in which the solar altitude angle is 76 5 the point cloud based model estimates is nearly evenly distributed for the entire study area while the dst model estimate shows a clear difference between the north facing south east section and south facing north west section slopes fig 5 since the north facing slope has less vegetation cover and small lai see in fig 3 the dst estimate captures the real situation while the point cloud based model fails in this regard for the spring day the solar altitude angle is 47 2 as the solar altitude angle is smaller the difference in the transmittance for the two opposite slopes is shown in the point cloud based model but not as much as that from the dst model in winter the solar altitude angle is 32 7 and there is an obvious difference in transmittance for the two opposite facing slopes estimated by the point cloud based model in summary the dst model tends to perform better than the point cloud based model with a smaller mae from the observation based estimated transmittance by using information of lai and the penetration distance the dst model can be used to estimate the canopy transmittance 4 2 above canopy radiation the 30 min radiation values are calculated for both nfs and sfs sites for eight representative clear and cloudy days in four seasons the results are compared to the site observations to evaluate the model performance in radiation simulation a 3 point equivalent to 1 h moving average is applied to observations and simulations at both sites to minimize the disturbance from temporal variation of sky conditions in order to remove the apparent effect of the daytime radiation cycle which is irrelevant of the model quality the offset radiation values are calculated the offset radiation values are the difference between the site observation model simulation and the reference station radiation fig 6 the correlation coefficient r between the offset values of observation and simulation is calculated for performance evaluation table 2 the mape and mapd are also calculated to examine the relative error at the nfs site the model simulations red solid line do not improve much as the offset values are not consistent with the observed one red dotted line from fig 6 a b we can find lower values appear at about 8 00 for the simulations while the observations close to zero that means it is shaded in the model at that moment which may not capture the real situation the r values between the offset irradiance of the observation and model simulation for nfs are relative low for nearly all examined clear and cloudy days suggesting that the shading situation is not well simulated for the nfs site there is very little shade above the canopy at the nfs site during the daytime at the top of the canopy the nfs site can receive almost same radiation as the reference station thus the dst model improvement is not clear at the sfs site a clear improvement is found in the model simulation the difference between the reference station and site observation blue dotted line shows significant variation with extreme low values occuring around 15 00 17 00 for most of the examined days negative offset values indicate that there are shading occurrences as can be seen in fig 6 the model simulation highly correlated with the observation the r values for the sfs site are high for most of the examined days table 2 the results show that the dst model can well capture the shading effects on the sfs site especially for the time with obvious shading occurrence compared to the nfs site there are more shading occurrences on the sfs site the results show that the dst model performs better on the sfs site while for the nfs site since there are fewer shading occurrences the above canopy radiation is consistent with the reference station data the model simulated radiation is also close to the real situation as the offset values are close to zero for most of the time intervals with the mape and mapd indexes being low for all the seasons and weather conditions suggesting that the relative error between the observed and simulated radiation is small in general the proposed dst model is suitable for simulating the above canopy radiation for both slopes 4 3 under canopy radiation the under canopy radiation values offset by the reference global radiation are calculated for the same days as in section 4 2 and compared with the independent shortwave radiation data section 3 3 the r value is calculated to evaluate the correlation between the model simulation and observation and the mape and mapd are calculated to evaluate the relative error table 3 the results show that the model simulation is well correlated with observed radiation at both the nfs and sfs sites as can be seen in fig 7 the shading exists for the entire daytime hours at both sites since the offset values are negative for selected days of different cloudiness in four seasons for the nfs site the offset values are less negative than the sfs site suggesting that the shading effect at the nfs site is weaker than in the sfs site this is consistent with the sparser vegetation cover on the nfs for both the nfs and sfs sites the r values are above 0 8 for almost all the examined days one exception is the clear day in summer where the model simulation on the nfs largely underestimates the under canopy radiation resulting in a lower r nevertheless the mape and mapd is 0 29 and 0 16 respectively this result indicates that the error is relatively small comparing to the total incident radiation comparing to the above canopy radiation a large improvement for the under canopy radiation simulation is found for the nfs site which demonstrates the capability of the dst model in simulating the canopy shading effect the mape and mapd are larger than the above canopy radiation simulation but it is still within an acceptable range mape less than 0 34 and mapd less than 0 18 4 4 annual under canopy insolation daily insolation mj m2 is the total amount of solar radiation energy received over a given surface area during a day in order to test the dst model performance one year of under canopy insolation data is simulated and compared with the converted understory shortwave radiation data fig 8 shows the daily insolation simulations of the nfs and sfs from 06 04 2013 to 05 04 2014 a seven day interval is used for the canopy transmittance calculation to reduce the computation cost i e data storage and processing time while still capturing the solar geometry variation the results of daily insolation over one year show a strong consistency between the simulation and measured data in comparison to the reference radiation grey line both simulated results of the nfs and sfs sites are close to the observation based estimated radiation at both sites in terms of the seasonal variation the simulated radiation is lower than observation from spring september 1 to the end of summer february 29 for the remaining days the simulated radiation is very close to the observation particularly for the sfs site the most possible reason for this seasonal variation of model performance is that the dems used in the model were obtained in june 2013 this could explain why the simulations for the days in a year close to the obtained date of the dems are more accurate if there were more representative data corresponding to the different seasons the model performance could be improved 4 5 demonstration of spatially distributed above and under canopy radiation fig 9 shows the simulated daily insolation of the whole study area it is not surprising that the top of canopy receives more radiation than the land surface in fig 9 we can see that the simulated radiation on the top of canopy is similar to that on a flat unshaded surface the difference in simulated insolation between different parts of the catchment is small for the under canopy radiation the amount of radiation in the northwest area of the catchment is significantly lower than in the southeast two factors may account for the difference 1 the topography effect influences the incident radiation the northwest area of the catchment is facing south east with a slope ranging from 10 to 30 while the southeast area of the catchment is facing north with a slope ranging from 20 to 30 since the study area is in south australia and the latitude is about 35 s the sun rises and moves in the northern sky of the area hence the incident angle of the northwest area of the catchment would be much higher than that of the southeast for example the impact of the incident angle to radiation is shown in the black circled area in fig 9 b it is an exposed area without much shading facing north with a slope of around 25 the radiation of this area would be the same as that in the northeast corner about 13 mj m2 if it is a flat surface however the simulated insolation is about 15 mj m2 consistent with the fact that the solar incident angle on this surface is smaller than that for a horizontal surface 2 the northwest section of the catchment is covered by more vegetation suggested by its larger lai than the southeast section fig 3 therefore there is more radiation being obstructed by the canopy resulting in lower simulated under canopy insolation to test the effects of spatial distribution and shading on simulated incident radiation 10 000 sample points for two different slope orientation sections composed of mostly north facing or south facing slopes with a 22 5 degree buffer either side are randomly selected fig 10 since the data do not meet the parametric statistic test assumptions of normality and homogeneity of variance the kruskal wallis test is conducted to compare the effect of canopy shading on the incident radiation there is a significant difference in incident radiation between the different facing slopes and positions p 0 001 the pairwise comparison results indicate that the average above canopy insolation for both north facing and south facing sections are significantly different from that of the under canopy insolation for the above canopy insolation the north facing section mean 11 14 mj m2 and south facing section mean 10 37 mj m2 are significantly different p 0 001 for under canopy insolation the average insolation for north facing section mean 6 28 mj m2 is significantly larger than that for the south facing section mean 2 35 mj m2 with p 0 001 the difference in mean value for the under canopy radiation between the two sites is larger than that of the above canopy radiation this large difference in under canopy radiation may help explain the contrasting understory vegetation between the two slopes in this dry summer climate site fig 11 for the simulated above canopy insolation the maximum and upper quartiles are close to each other between the two sections indicating that the upper limit of insolation has no difference for the opposite orientation slopes however the minimum and lower quartiles for south facing section are smaller than north facing section more canopy shading exists on the south facing section which results in the difference in minimum and lower quartiles the simulated under canopy insolation is generally lower than the above canopy insolation as expected however on the north facing section the maximum of the simulated under canopy insolation is even larger than the simulated above canopy insolation the slope orientation accounts for this discrepancy since the equator facing slope can receive more irradiance than a horizontal plane here the simulated above canopy insolation is meant for a horizontal plane above the canopy it may be different from the insolation the canopy actually receives due to a possible difference in solar incident angles to the canopy between the two slopes the current dst model does not simulate this difference altogether these results suggest that the dst model can well simulate the spatial variability of solar radiation above and below the canopy on complex terrain 4 6 model efficiency the computation efficiency is one aspect of users concern here is the relevant result based on the coded model in matlab 2020 platform running on a 4 cores 3 40 ghz inter r core tm i5 7500 cpu and 16 gb ram pc the computing time can be site specific which is outlined here the study area includes 611 481 grid cells and covers about 0 15 km2 the average computing time is listed in table 4 calculating the canopy transmittance is the most time consuming process compared to the global radiation calculation calculating the under canopy transmittance takes nearly twice as long as the above canopy transmittance the major computational process in calculating canopy transmittance is to estimate the distance that light penetrates the canopy the distance is estimated grid by grid using the voxel traverse algorithm although the calculation of canopy transmittance for a large study area can be computationally intensive some measures can be taken to simplify the process they include 1 since the solar geometry does not change significantly within a short period canopy transmittance can be estimated at a certain time interval e g a week and 2 once the canopy transmittance values are calculated for one year they can be used repetitively for other years if the canopy conditions do not vary much between years 5 conclusions a new distributed solar irradiation model the double shading transposition model which considers both topographic and canopy shading effects is developed and tested for complex terrain the dst model simulates radiation distribution in both above and under vegetation canopy in any weather conditions the model simulations in general agree well with the observations for the under canopy radiation simulation a significant difference in daily insolation is found for opposite north and south slope orientations these results indicate that the dst model is capable of capturing the shading effects caused by topography and vegetation canopy the spatial variation of solar radiation distribution is well simulated for the complex mountainous forested terrain of the study area the study provides researchers with an all condition model to generate solar radiation distribution in vegetated areas where solar radiation is expected to be significantly modulated by trees or other physical obstructions being able to simulate radiation fields in areas of complex terrain with variable vegetation cover opens a diverse set of possibilities for soil scientists ecologists hydrologists geomorphologists and other earth scientists for which accurate estimations of surface energy loads are important for example the dst model provides a key variable for hydrologist to examine evapotranspiration patterns for above and under canopy conditions in a distributed way other potential uses include insolation simulation to locate thermal shelters canopies that shelter an animal from microclimatic extremes in the forest which has been increasingly studied in biological investigations due to global warming melin et al 2014 future studies will focus on the dst model application to areas of different climatic and environmental conditions software and data availability the code used in this study is available at https au mathworks com matlabcentral fileexchange 103705 a distributed solar radiation model for complex terrain declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements collection of most field data for this study was funded by national centre for groundwater research and training australia sr08000001 wenjie liu acknowledges the support from china scholarship council x he s and his contribution was supported by the national natural science foundation of china grant number 41472238 thanks to hailong wang xiang xu zijuan deng and yuting yang for assisting in field data collection langdon badger kindly provided free access to the field site data and code requests can be addressed to the corresponding author huade guan 
