index,text
25815,ocean waves are widely estimated using physics based computational models which predict how energy is transferred from the wind dissipated and transferred spatially across the ocean machine learning methods offer an opportunity to predict these data with significantly reduced data input and computational power this paper describes a novel surrogate model developed using the random forest method which replicates the spatial nearshore wave data estimated by a simulating waves nearshore swan numerical model by incorporating in situ buoy observations outputs were found to match observations at a test location more closely than the corresponding swan model furthermore the required computational time reduced by a factor of 100 this methodology can provide accurate spatial wave data in situations where computational power and transmission are limited such as autonomous marine vehicles or during coastal and offshore operations in remote areas this represents a significant supplementary service to existing physics based wave models keywords nearshore wave modelling random forest machine learning spatial prediction optimal gridding 1 introduction met ocean data play a significant role in the design and operation of offshore and coastal infrastructure wave conditions impact ship navigation and fuel efficient operation james 1957 mepc 2012 in particular the sea state is a key factor that determines vessel design and operational management strategies for autonomous marine systems johnston and poole 2017 for marine renewable energy offshore oil and gas and offshore aquaculture wave conditions influence activities across the full life cycle of the infrastructure cyclic wave loads impact fatigue reliability and performance of systems dnv 2014 whilst continuous wave data are key to determining the weather windows which govern the accessibility of renewable energy devices ardente et al 2008 balog et al 2016 gentry et al 2017 reikard et al 2017 virtually all forecasts and characterisations of wave conditions are currently based on deriving time series of spatial wave conditions using phase resolving physics based computational models a series of 3rd generation wave models such as wam wave modelling günther et al 1992 komen et al 1996 wavewatch iii tolman 2009 tolman et al 2002 and simulating waves nearshore swan booij et al 1999 ris et al 1999 have become universal numerical methods these models determine wave conditions based on the energy balance equations considering energy input from surface winds with processes dissipating wave energy by incorporating the propagation of waves across the model domain and modelling interaction with the bathymetry spatial wave data sets are created these models are widely used providing past wave climates and wave forecasts across the world berrisford et al 2011 chawla et al 2012 service c3s 2017 the spatial resolution of these global datasets range from 0 28 0 28 about 30 km to 1 1 about 111 km swan was designed as a tool for coastal modelling focusing more on wave propagation in shallow water booij et al 1999 it was designed for application in coastal regions around the world and has also been widely used to quantify wave conditions for offshore renewable energy sites e g ashton et al 2014 liang et al 2014 wu et al 2020 physics based models are commonly validated and calibrated with in situ measurements or remote sensing data presently global scale modelling assimilates satellite based remote sensing data e g the global data assimilation system gdas system noaa 2020 for nearshore areas waves observed by in situ buoy measurements have been used for validation of physics based models including the data used in this study van nieuwkoop et al 2013 combining measured time series of wave data with physics based models offers possibilities for deriving spatio temporal wave data in the past decade machine learning methods have demonstrated their accuracy in predicting various environmental variables research has explored forecasting of wave energy flux forecasts based on in situ measurement by machine learning algorithms and have achieved similar accuracy to physics based models in different forecast horizons specifically sánchez et al 2018 used an artificial neural network ann to estimate the wave height at a buoy station with a mean absolute percentage error mape of 5 27 while pirhooshyaran and snyder 2020 used long short term memory lstm and sequence to sequence networks to forecast significant wave height h s and power at multiple buoy stations their proposed networks can predict h s with mape of 18 2 which outperformed alternate networks and a random forest rf method a machine learning alternative the spatial correlations of environmental variables can be captured by machine learning methods oh and suh 2018 proposed a hybrid model combining empirical orthogonal function eof analysis and wavelet analysis with neural network eofwnn that can forecast wave heights for the following 24 h at multiple locations with values of normalized root mean squared error nrmse between 15 5 and 26 3 li et al 2011 compared the application of 23 methods including rf to the spatial interpolation of environmental variables their work confirmed both the effectiveness and sensitivity of rf to predict spatial patterns this suggests that it is an ideal candidate for application to ocean wave data some research has attempted to make grey box models combining a numerical model with a data driven approach ibarra berastegi et al 2015 serras et al 2019 these systems take output from a physical model e g european centre for medium range weather forecasts ecmwf and national centres for environmental prediction ncep as features in a machine learning model nencioli and quartly 2019 proposed a synergistic method to combine satellite and in situ observations to map an area of wave parameters validated by a global numerical wave model ibarra berastegi et al 2015 applied rf with a physics based model wam to issue short term forecasts of wave energy flux from 1h to 24 h at five buoys with mean absolute log differences of less than 20 60 serras et al 2019 has also combined rfs with physics based data from ecmwf to forecast wave energy flux at the mutriku wave farm up to 24 h ahead with 60 mape considering the computational requirements for coastal models such as swan surrogate models can reduce the necessary computational cost associated with modelling for example james et al 2018 generated a swan based machine learning framework model in which a multi layer perceptron method was used for wave height prediction while a svm method was used to predict wave period o donncha et al 2018b produced an ensemble model integrating ridge regression and exponentiated gradient algorithms as a surrogate of a swan model subsequently their research group aggregated their models to an ensemble computationally lightweight machine learning model applied to a site in monterey bay california o donncha et al 2018a their surrogate model showed good agreement with a physics based model and with a five thousand fold improvement in computational speed the rmse of the predicted significant wave height against their swan model averaged 9 cm and the predicted wave period had an rmse below 0 1 s the demonstrated accuracy and the low computational cost of relevant machine learning systems when compared to conventional physics based model outputs demonstrates an opportunity to improve accuracy and availability of wave data for a wide variety of applications this paper initiates that research by examining whether given sufficient data machine learning techniques can capture the spatial patterns derived by physics based models within a surrogate model acting as an addition to the physics based model such a system would have the potential to provide low computational cost estimates of wave conditions and effectively assimilate measured data in this study a rf algorithm was used to learn from an existing physics based swan wave model output in order to produce an operational surrogate model that can provide an immediate accurate estimate of wave conditions across a domain with this in mind the work presented in this paper addressed three principle objectives 1 generate a surrogate model that applied machine learning method on the physics based outputs to learn the spatial relationship between input buoy data at a few locations within the domain to the full spatially distributed wave conditions across the domain 2 run the surrogate model using input data from three locations within the domain 3 run the surrogate model using wave buoy measurements as input and validated against further buoy data measured within the domain this represents using the surrogate model and wave measurements for now casting wave conditions at any point in the domain without running a full numerical model such as swan 2 physics based wave model data a swan spectral wave model was developed for the south west uk longitude 4 w to 7 w and latitude 49 n to 51 n fig 1 and run for 23 years between 1989 and 2011 as described by van nieuwkoop et al 2013 this used 3 hourly gridded ecmwf era interim winds fields subjected to spatio temporal interpolation to 10 22 gridded data points which drove the swan wave model over a 1 1 km2 grid resolution i e 219 223 cells in the grid this work considers significant wave height h s mean wave direction m dir mean zero crossing period t z and peak wave period t p within this region the simulation time resolution was 1 h however due to storage constraints wave parameters were only recorded every 12 h the 12 h interval data from 1989 to 2011 were concatenated to build a single data structure in which the first three columns included time longitude position and latitude position and the remaining columns contained the wave parameters this dataset is henceforth referred to as the original dataset it includes the training dataset validation dataset and test dataset this swan model has previously been validated against a global wam model era interim at individual grid points but also with measurements at three buoy locations at looe bay penzance and perranporth the three buoy locations and corresponding information are shown in fig 1 and described in table 1 each of the buoys is within approximately 500 m of a swan grid point which is considered sufficiently close in the surrogate model the comparison between the numerical model results and measurement data can be found in the work of van nieuwkoop et al 2013 where the relative biases of h s energy period t m 1 0 at penzance buoy looe bay buoy and perranporth buoy remained 20 the rmse of mean direction remained 40 the comparison plots between numerical results at the perranporth buoy location are shown in fig 2 as an example of the validation process 3 methodology machine learning techniques for surrogate regression the high fidelity physics based model is governed by underlying nonlinear equations that relate the wave conditions throughout the domain a rf approach was implemented as a multivariate surrogate model to represent the spatial patterns in the wave field predicted by the physics based model in addition the rf model was benchmarked against a linear regression lr model section 5 1 developed based on methods in hutcheson 2011 3 1 multivariate random forest regression rfs are one of the most effective machine learning algorithms for predictive regression and classification purposes pedregosa et al 2011 it is an ensemble machine learning algorithm proposed by breiman 2001 as an ensemble approach it consists of multiple aggregated simpler machine learning constructs the rf therefore uses multiple parallel decision tree models to train and predict sample data each decision tree for regression was a non parametric supervised learning model that indicated a set of rules that were hierarchically structured to make decisions in forms of branches and to get real value consequences in forms of leaves from each node in this research binary decision trees were used splitting each node at most into two the ensemble models and random concepts in rf greatly reduce overfitting of individual tree models increase diversity in the forest and result in more robust overall predictions hastie et al 2008 the flow chart of rf algorithm is shown in fig 3 before building trees several iterations of bootstrap resampling random sampling with replacement from the training dataset were applied the bootstrapping process split each sample group into data for training trees called in bag and data not included in training trees are referred as out of bag oob for evaluation the objective of each tree model was to minimize the mean squared error mse of the oob sample the output included ensemble results of k trees because each tree is independent and identically distributed the regression result was the average of k trees hastie et al 2008 provide a more detailed discussion of the mathematical aspects of rf tsoumakas and katakis 2007 categorized the solutions to multi output or multivariate problems in two ways 1 problem transformation methods which transform the problem into several regular single output problems and 2 algorithm adaption methods which directly adapt algorithm into handling multiple outputs the multivariate rf mrf method can be treated as either a series of single output regression trees or as a multivariate model segal and xiao 2011 and the prediction scores of the two methods are similar in this paper the mrf regression used for each wave parameter prediction contains y different outputs each representing one of y different features grid points 3 2 training datasets to implement the machine learning techniques the original numerical results were formulated into a supervised learning framework this required the data to be structured as feature label pairs with a corresponding time index 3 2 1 input considering the training data set to be a two dimensional n x m matrix input features were represented by columns and time was represented by rows the input feature matrix was generated using 21 years of historical data january 01 1989 to december 31 2009 at the selected locations the historical data therefore consisted of 15 340 time samples at 12 h intervals the swan model in question was validated against three buoy locations looe bay penzance and perranporth van nieuwkoop et al 2013 correspondingly wave parameters at these three locations were used as input features to the surrogate model for each selected location time series of the four features of interest h s m m dir t z s and t p s are considered to train and validate the surrogate model the swan model is used exclusively with model results nearest the buoy locations used to represent synthetic buoy data inputs during the test phase of the model development however the synthetic data are substituted for real buoy measurements demonstrating how the surrogate model can initially be built in the absence of in situ data which can then be used in operation 3 2 2 correlation analysis prior to performing the regression analysis the correlations between feature variables were analysed using a heat map of the spearman s rank correlation coefficient matrix of the input feature variables which in this case were the wave parameters at the three buoy locations fig 4 for each wave parameter correlation between locations was observed the coefficient between the penzance buoy and the looe bay buoy was moderately higher these are along the same section of coastline and therefore more spatially correlated 3 2 3 output evgeniou and pontil 2004 suggested that for multivariate regression training a model on related features simultaneously rather than independently can improve predictive performance on the other hand if the output features are dissimilar training separate models independently for each feature can be more time efficient than taking a multi output approach faddoul et al 2010 in this case the model data showed spatial correlation across the measurement buoys fig 4 which indicated that multivariate regression would be of value correlation between the same parameters at different locations was greater than that between different parameters at the same location as such each state variable h s m dir t z t p was modelled separately while each spatially distributed variable was predicted simultaneously therefore the outputs for each wave parameter were also in the form of a two dimensional matrix defined as the y matrix in the y matrix the rows represent samples at different times while each column represents the result of a spatial grid point individually 4 model setup and application the application of the surrogate model including data processing and implementation used python 3 6 including the python toolkit scikit learn pedregosa et al 2011 4 1 pre processing swan model results were available in the network common data form netcdf during pre processing these were transformed to a matrix with three indexes time longitude and latitude these became the first three columns of the matrix and wave parameters h s m dir t z t p at a specific time and location corresponded to the remaining columns of the data matrix secondly invalid samples were removed from the data invalid samples in this model included grid points within the computational area that correspond to land these appeared as nan not a number values and were removed before generating the cleaned dataset the cleaned dataset for 21 years from 1989 to 2009 was randomly segmented into a training dataset 80 and validation dataset 20 fig 3 the data simulated in the year 2010 were held separately and processed as the test dataset normally machine learning models require cross validation to ensure a robust algorithm however for the rf algorithm the accuracy was evaluated on each oob sample which was equivalent to n fold cross validation and the results were obtained directly from the model in the rf model there is no requirement for feature engineering or transformation and normalisation of input features as an interpretable machine learning algorithm tree based algorithms can always compare prediction with what if scenarios which makes them work equally well with any monotonic transformation of a feature molnar 2020 4 2 evaluation criteria in this paper the accuracy of the surrogate model was quantified using the coefficient of determination r 2 rmse and nrmse as assessments of the uncertainty as well as the mean proportional differences to evaluate bias 1 coefficient of determination r 2 1 i 1 n y ˆ i y i 2 i 1 n y y i 2 2 root mean square error rmse 1 n i 1 n y ˆ i y i 2 3 normalized root mean square error nrmse r m s e y 4 mean proportional difference d 1 n i 1 n y ˆ i y i y i where n denotes the number of fitted samples y ˆ y and y represent the predicted value by the surrogate model actual value and mean of actual values respectively in the initial assessment of surrogate model performance over the study period based on the swan model the actual value refers to swan results while in the later model validation with measured data the actual value refers to the buoy observations 4 3 optimal gridding selection ideally the surrogate model would represent the same resolution as the swan model 219 223 points in this case representing each grid point as an output feature however the original swan resolution contained 42 500 valid points in the domain i e 42 500 output variables using this full resolution led to a requirement of over 2 tb of addressable memory for an 8 year training set to enable the approach using the full 21 years of data a dimension reduction process was implemented to reduce the spatial resolution the statistics of waves were considered stationary within each simulation and homogeneous area over the domain the surrogate model therefore required an evenly scaled resolution representing a smoothed version of the original model that captured the spatial distribution with a good agreement an effective resolution adjustment method was to use bilinear interpolation techniques accadia et al 2003 to transform the swan data to different grid resolutions in order to find an optimal gridding resolution for the surrogate model different scales of horizontal and vertical resolutions were assessed and compared the optimal resolution was affected by several factors including the computational cost and overall accuracy of the scaled resolution to represent the high resolution dataset to assess whether the low resolution data after the dimension reduction dr were accurately representing the original high resolution dataset dimension ascension da was applied to the adjusted low dimensional data using the same bilinear interpolation method the combined assessment which went through the dr da process took the following into consideration table 2 1 the minimum acceptable spatial resolution was set at 0 125 0 125 which resulted in 425 25 segments longitudinal x 17 segments latitudinal grid points in the area of study 2 the ratio of valid grid points non nan values after the dr da process selecting different resolutions generates a different number of nan values at the edges the ratio of non nan values was considered an important factor to the subsequent modelling and was used to evaluate the resolution selected 3 the mapping error after the dr da process the nrmses of four wave variables associated with each set of interpolated values were computed as the average of rmse at each valid point over the mean value of the wave parameter to avoid seasonal trends of wave parameter distributions 100 timestamps from the 21 years were sampled to execute the mapping nrmse assessment fig 5 4 the training time for the surrogate model in this comparison the time taken to train using 21 years of significant wave height with different resolutions was quantified 5 the accuracy of the surrogate model which was evaluated by the r 2 value in the test dataset the 100 sample averaged nrmse between interpolation from low resolution and the original data did not vary significantly fig 5 when varied between 1 2 and 1 8 the nrmse remained less than 2 of spatial average value for computational efficiency the scales from 1 4 to 1 8 were processed for training the surrogate model with results listed in table 2 the accuracy r 2 of the surrogate model against the scaled swan model remained stable around 0 957 but the training time dropped from 30 min to 6 min the combined evaluation considered the factors including non nan ratio mapping nrmse and training accuracy this indicated that the scale of 1 5 performed best the training time for a 1 5th scale surrogate model was around 17 min as a result the 1 5th scale was used in this work which transformed the original swan data to a grid with 43 longitude segments and 44 latitude segments the resolution change along the transformation is illustrated in fig 6 4 4 model hyper parameter setup the rf algorithm contains several hyper parameters including the number of estimators maximum tree depth maximum features at each split and maximum samples when looking for the best split all features were taken into consideration in each estimator the maximum tree depth was set to be default which means the nodes of each estimator were expanded until all leaves are pure the maximum sample was the length of the training dataset therefore the number of estimators was the key hyper parameter requiring tuning during model development generally the prediction accuracy improved with increasing numbers of estimators however increasing the number of estimators resulted in increased training time a parameter study of training the h s surrogate model showed diminishing returns when increasing the number of estimators fig 7 with greater than 200 estimators the r 2 curve and the rmse curve flattened and converged for both the training and test sets while the required computational time continued to increase linearly based on this with respect to both accuracy and training efficiency the number of estimators used in the present surrogate models was set to 200 5 results 5 1 accuracy of the surrogate model relative to swan model the surrogate outputs were compared to the equivalent swan estimates for each of the wave parameters studied r 2 values exceeded 0 9 with the exception of the mean wave direction table 3 and scatter plots demonstrate this strong correlation fig 8 the relative rmse of h s and t p are below 10 of each wave parameter s average value low rmse values within both the validation and test datasets indicated high confidence in the model s ability to replicate the swan results the prediction of peak period performed best among the four wave parameters with nrmse values 5 3 of the average peak period over the test data set among the target area the surrogate prediction results of mean wave direction m dir had relatively low prediction accuracy with the r 2 value below 0 9 in all of the datasets the nrmse accounted for 14 31 of the average direction the rf model outperformed the benchmark lr model reducing the nrmse by a factor of approximately 1 5 for all wave parameters table 3 indicating the efficacy of the rf algorithm the spatial distribution of the differences between the surrogate and swan models are illustrated in fig 9 the annual averaged proportional differences between the surrogate and swan model in 2010 were less than 1 8 of swan values in the northern area the surrogate model over estimated h s while in the southern area it underestimated the largest mean proportional difference within the domain was d h s 2 for the area close to the east of the isles of scilly however the annual averaged value from swan at that point was 0 66 m which resulted in a small actual difference of 0 0198 m areas closer to input positions were more likely to have lower rmse between the original swan simulation and the surrogate model in 2010 5 2 performance of the surrogate model to test the feasibility of the surrogate model for potential deployment applications a validation stage using measured data both as input and output was necessary the observation data of the three buoys from penzance looe bay and perranporth were used as input and the result at another buoy location was used for output validation this validation buoy close to the wave hub marine energy test centre site is representative of an offshore site with a valuable resource for marine renewable energy development ashton 2012 saulnier et al 2012 it is the primary resource for any real time decisions made for marine operations at this offshore renewable energy test site to compare the result from a more accurate location the da process was undertaken to generate a high resolution spatial data set the wave buoy data were then compared to the nearest high resolution grid point of the surrogate model for all four wave parameters studied the surrogate model consistently matched the real data better than the swan model table 4 fig 10 all r 2 and rmse values comparing the real data and surrogate model output were smaller than the equivalent statistics between real data and swan estimates in particular the surrogate model s prediction of zero crossing wave period had a dramatic improvement in accuracy compared to swan with an r 2 of 0 7205 and an rmse value half of the corresponding swan value table 4 the surrogate model estimated significant wave height r 2 value was 0 9067 with an rmse of 0 2556 m and nrmse of around 15 the nrmses of the surrogate model against measured data were also below 20 for both t z and t p the observations did not provide mean wave direction so the comparison of wave direction was possible 5 3 computational time and requirements training the model for each wave parameter for 21 years was completed in 17 min on a laptop with 16 0 gb ram and an i7 8550u processor using python 3 6 program in a windows environment including the steps for data processing feeding the surrogate model with one set of observation data yielding equivalent estimates of all wave parameters for the whole domain took less than 1 s on the same machine as the model architecture for the surrogate produced separate independent models for each wave parameter these can be run in parallel to make full use of available resources 6 discussion this paper has described a method for developing a surrogate wave model based on existing phase averaged spectral wave model output the surrogate model worked on the assumption that the spatial distribution of wave conditions created by the physical modelling process in this case swan was well defined and provides an additional service to immediately estimate wave conditions across the model domain from limited input values these results indicated that the rf based surrogate model represents an efficient and accurate method for predicting the spatial wave field when using solely physics based models forecasts and associated spatial estimates of current conditions rely on models updating every 6 12 h which require significant computing resources this system offers a low cost alternative to estimate spatial model outputs based on very limited input data and with significantly improved speed when deployed using data from 3 in situ wave buoys within the domain the outputs were more accurate than physical modelling equivalents furthermore the computational requirement was reduced by approximately a factor of 100 in the surrogate spatial estimation the most time consuming task was loading the machine learning model which took around 5 min for each wave parameter edge computing shi and dustdar 2016 technology would enable the model to be pre loaded into memory and give rise to nearly instantaneous spatial wave estimation as such this system that would potentially be accessible using a pc mobile phone vessel navigation system or autonomous vessel machine learning algorithms have been verified in the literature to solve the spatial regression problem for example james et al 2018 and o donncha et al 2018b 2018a used all wave boundary data to replicate swan outputs the implementation demonstrated in this paper is different to previous studies reducing the input data to three points within the domain and focusing on providing accurate now casting from measurement assets this makes direct comparison with previous studies difficult instead the work used benchmarking with a lr model and validation with measured data to assess the surrogate model benchmarking demonstrated that the extra complexity of rf algorithm improves accuracy verification showed that when in situ data were used to drive the surrogate model the results were improved when compared to physics based model estimates this is an important outcome as it showed that the combined surrogate model and in situ data have the potential to provide the most accurate description of current conditions across the model domain this makes the system highly suited to real time management for autonomous vehicles or marine operations for offshore infrastructure the surrogate modelling process methods demonstrated are additional to physics based models this system relies on an accurate spatial description of the wave conditions from which the surrogate model can learn the spatial distribution of conditions across the area as such accurate physical modelling remains central to this process this work shows that a surrogate model can offer real time estimates for wave conditions across a model domain the computational requirements and operation from limited real time measurements mean that higher resolution model output could now be implemented as a service but this could be considered as an additional service within the physical modelling architecture for some applications such a system based on global model data will be advantageous particularly where forecast data are available the spatial correlations quantified using this method could similarly be implemented to convert forecast from global models into higher resolution output allowing a forecast product to be defined with similar savings on data input and computational requirements further development in this area should consider how to effectively combine in situ measurements with this forecast activity taking advantage of the improved accuracy shown in this work has the potential to create an augmented forecast using a surrogate model procedure this work has demonstrated the surrogate system using wave buoy data these data were particularly suited to the application with accurate long term data sets however the buoys were not deployed for the purpose of this study and the system showed excellent results from measurements that were not optimally placed within the domain this highlights the potential for the surrogate approach to incorporate imperfect data further work should establish how such a system can work with other data sets satellite remote sensing offers global coverage while installed infrastructure vessels or autonomous systems all have the potential to gather data establishing how data that may be inaccurate or contain bias can be incorporated in this system has the potential to open up significant opportunity for revolutionising met ocean data provision and making real time access to accurate spatial data common in a marine setting 7 conclusion in this paper a novel method was proposed to derive an accurate spatial wave data set using in situ measured wave data from point locations using a machine learning approach based on a physics based wave model swan this approach used a rf algorithm to evaluate the spatial correlation of wave parameters within the computational domain this created a surrogate model which was an efficient method to replicate physical modelling without the undertaking computationally expensive calculations when using observations within the domain as inputs the surrogate model was more accurate than the corresponding estimates from the swan model drawing on global model data at the computational domain boundary this method supplements a combination of physics based modelling and in situ observations that form the most common approach to met ocean monitoring it combines the real time availability of in situ data with the spatial capabilities of physics based models and it is easily implemented with existing systems once developed the system required little computational power for implementation and as such it has the potential to provide real time spatial data coverage even in situations where data transmission or computational resources are limited this access to accurate real time spatial data has the potential to fundamentally change the way that met ocean data are used for the management and operation of marine infrastructure the system developed was highly flexible and has potential for implementation with other marine environmental parameters continued development will allow combined analysis of a range of in situ monitoring devices and can incorporate measurements of opportunity to create highly detailed and accurate data sets this will include mobile measurements from autonomous vehicles and a built in suitability to direct such measurements to improve accuracy and relevance of data sets for specific operations establishing this system with other data sets will create significant opportunity for making real time access to accurate spatial data the new normal in a marine setting declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements source data used in the preparation of this paper has been supplied by the channel coastal observatory uk funded under the regional coastal monitoring programmes by defra ongoing work is supported by the epsrc supergen offshore renewable energy hub grant no ep s000747 1 flexible fund support for malcom and collaboration with dr ed steele at the uk met office 
25815,ocean waves are widely estimated using physics based computational models which predict how energy is transferred from the wind dissipated and transferred spatially across the ocean machine learning methods offer an opportunity to predict these data with significantly reduced data input and computational power this paper describes a novel surrogate model developed using the random forest method which replicates the spatial nearshore wave data estimated by a simulating waves nearshore swan numerical model by incorporating in situ buoy observations outputs were found to match observations at a test location more closely than the corresponding swan model furthermore the required computational time reduced by a factor of 100 this methodology can provide accurate spatial wave data in situations where computational power and transmission are limited such as autonomous marine vehicles or during coastal and offshore operations in remote areas this represents a significant supplementary service to existing physics based wave models keywords nearshore wave modelling random forest machine learning spatial prediction optimal gridding 1 introduction met ocean data play a significant role in the design and operation of offshore and coastal infrastructure wave conditions impact ship navigation and fuel efficient operation james 1957 mepc 2012 in particular the sea state is a key factor that determines vessel design and operational management strategies for autonomous marine systems johnston and poole 2017 for marine renewable energy offshore oil and gas and offshore aquaculture wave conditions influence activities across the full life cycle of the infrastructure cyclic wave loads impact fatigue reliability and performance of systems dnv 2014 whilst continuous wave data are key to determining the weather windows which govern the accessibility of renewable energy devices ardente et al 2008 balog et al 2016 gentry et al 2017 reikard et al 2017 virtually all forecasts and characterisations of wave conditions are currently based on deriving time series of spatial wave conditions using phase resolving physics based computational models a series of 3rd generation wave models such as wam wave modelling günther et al 1992 komen et al 1996 wavewatch iii tolman 2009 tolman et al 2002 and simulating waves nearshore swan booij et al 1999 ris et al 1999 have become universal numerical methods these models determine wave conditions based on the energy balance equations considering energy input from surface winds with processes dissipating wave energy by incorporating the propagation of waves across the model domain and modelling interaction with the bathymetry spatial wave data sets are created these models are widely used providing past wave climates and wave forecasts across the world berrisford et al 2011 chawla et al 2012 service c3s 2017 the spatial resolution of these global datasets range from 0 28 0 28 about 30 km to 1 1 about 111 km swan was designed as a tool for coastal modelling focusing more on wave propagation in shallow water booij et al 1999 it was designed for application in coastal regions around the world and has also been widely used to quantify wave conditions for offshore renewable energy sites e g ashton et al 2014 liang et al 2014 wu et al 2020 physics based models are commonly validated and calibrated with in situ measurements or remote sensing data presently global scale modelling assimilates satellite based remote sensing data e g the global data assimilation system gdas system noaa 2020 for nearshore areas waves observed by in situ buoy measurements have been used for validation of physics based models including the data used in this study van nieuwkoop et al 2013 combining measured time series of wave data with physics based models offers possibilities for deriving spatio temporal wave data in the past decade machine learning methods have demonstrated their accuracy in predicting various environmental variables research has explored forecasting of wave energy flux forecasts based on in situ measurement by machine learning algorithms and have achieved similar accuracy to physics based models in different forecast horizons specifically sánchez et al 2018 used an artificial neural network ann to estimate the wave height at a buoy station with a mean absolute percentage error mape of 5 27 while pirhooshyaran and snyder 2020 used long short term memory lstm and sequence to sequence networks to forecast significant wave height h s and power at multiple buoy stations their proposed networks can predict h s with mape of 18 2 which outperformed alternate networks and a random forest rf method a machine learning alternative the spatial correlations of environmental variables can be captured by machine learning methods oh and suh 2018 proposed a hybrid model combining empirical orthogonal function eof analysis and wavelet analysis with neural network eofwnn that can forecast wave heights for the following 24 h at multiple locations with values of normalized root mean squared error nrmse between 15 5 and 26 3 li et al 2011 compared the application of 23 methods including rf to the spatial interpolation of environmental variables their work confirmed both the effectiveness and sensitivity of rf to predict spatial patterns this suggests that it is an ideal candidate for application to ocean wave data some research has attempted to make grey box models combining a numerical model with a data driven approach ibarra berastegi et al 2015 serras et al 2019 these systems take output from a physical model e g european centre for medium range weather forecasts ecmwf and national centres for environmental prediction ncep as features in a machine learning model nencioli and quartly 2019 proposed a synergistic method to combine satellite and in situ observations to map an area of wave parameters validated by a global numerical wave model ibarra berastegi et al 2015 applied rf with a physics based model wam to issue short term forecasts of wave energy flux from 1h to 24 h at five buoys with mean absolute log differences of less than 20 60 serras et al 2019 has also combined rfs with physics based data from ecmwf to forecast wave energy flux at the mutriku wave farm up to 24 h ahead with 60 mape considering the computational requirements for coastal models such as swan surrogate models can reduce the necessary computational cost associated with modelling for example james et al 2018 generated a swan based machine learning framework model in which a multi layer perceptron method was used for wave height prediction while a svm method was used to predict wave period o donncha et al 2018b produced an ensemble model integrating ridge regression and exponentiated gradient algorithms as a surrogate of a swan model subsequently their research group aggregated their models to an ensemble computationally lightweight machine learning model applied to a site in monterey bay california o donncha et al 2018a their surrogate model showed good agreement with a physics based model and with a five thousand fold improvement in computational speed the rmse of the predicted significant wave height against their swan model averaged 9 cm and the predicted wave period had an rmse below 0 1 s the demonstrated accuracy and the low computational cost of relevant machine learning systems when compared to conventional physics based model outputs demonstrates an opportunity to improve accuracy and availability of wave data for a wide variety of applications this paper initiates that research by examining whether given sufficient data machine learning techniques can capture the spatial patterns derived by physics based models within a surrogate model acting as an addition to the physics based model such a system would have the potential to provide low computational cost estimates of wave conditions and effectively assimilate measured data in this study a rf algorithm was used to learn from an existing physics based swan wave model output in order to produce an operational surrogate model that can provide an immediate accurate estimate of wave conditions across a domain with this in mind the work presented in this paper addressed three principle objectives 1 generate a surrogate model that applied machine learning method on the physics based outputs to learn the spatial relationship between input buoy data at a few locations within the domain to the full spatially distributed wave conditions across the domain 2 run the surrogate model using input data from three locations within the domain 3 run the surrogate model using wave buoy measurements as input and validated against further buoy data measured within the domain this represents using the surrogate model and wave measurements for now casting wave conditions at any point in the domain without running a full numerical model such as swan 2 physics based wave model data a swan spectral wave model was developed for the south west uk longitude 4 w to 7 w and latitude 49 n to 51 n fig 1 and run for 23 years between 1989 and 2011 as described by van nieuwkoop et al 2013 this used 3 hourly gridded ecmwf era interim winds fields subjected to spatio temporal interpolation to 10 22 gridded data points which drove the swan wave model over a 1 1 km2 grid resolution i e 219 223 cells in the grid this work considers significant wave height h s mean wave direction m dir mean zero crossing period t z and peak wave period t p within this region the simulation time resolution was 1 h however due to storage constraints wave parameters were only recorded every 12 h the 12 h interval data from 1989 to 2011 were concatenated to build a single data structure in which the first three columns included time longitude position and latitude position and the remaining columns contained the wave parameters this dataset is henceforth referred to as the original dataset it includes the training dataset validation dataset and test dataset this swan model has previously been validated against a global wam model era interim at individual grid points but also with measurements at three buoy locations at looe bay penzance and perranporth the three buoy locations and corresponding information are shown in fig 1 and described in table 1 each of the buoys is within approximately 500 m of a swan grid point which is considered sufficiently close in the surrogate model the comparison between the numerical model results and measurement data can be found in the work of van nieuwkoop et al 2013 where the relative biases of h s energy period t m 1 0 at penzance buoy looe bay buoy and perranporth buoy remained 20 the rmse of mean direction remained 40 the comparison plots between numerical results at the perranporth buoy location are shown in fig 2 as an example of the validation process 3 methodology machine learning techniques for surrogate regression the high fidelity physics based model is governed by underlying nonlinear equations that relate the wave conditions throughout the domain a rf approach was implemented as a multivariate surrogate model to represent the spatial patterns in the wave field predicted by the physics based model in addition the rf model was benchmarked against a linear regression lr model section 5 1 developed based on methods in hutcheson 2011 3 1 multivariate random forest regression rfs are one of the most effective machine learning algorithms for predictive regression and classification purposes pedregosa et al 2011 it is an ensemble machine learning algorithm proposed by breiman 2001 as an ensemble approach it consists of multiple aggregated simpler machine learning constructs the rf therefore uses multiple parallel decision tree models to train and predict sample data each decision tree for regression was a non parametric supervised learning model that indicated a set of rules that were hierarchically structured to make decisions in forms of branches and to get real value consequences in forms of leaves from each node in this research binary decision trees were used splitting each node at most into two the ensemble models and random concepts in rf greatly reduce overfitting of individual tree models increase diversity in the forest and result in more robust overall predictions hastie et al 2008 the flow chart of rf algorithm is shown in fig 3 before building trees several iterations of bootstrap resampling random sampling with replacement from the training dataset were applied the bootstrapping process split each sample group into data for training trees called in bag and data not included in training trees are referred as out of bag oob for evaluation the objective of each tree model was to minimize the mean squared error mse of the oob sample the output included ensemble results of k trees because each tree is independent and identically distributed the regression result was the average of k trees hastie et al 2008 provide a more detailed discussion of the mathematical aspects of rf tsoumakas and katakis 2007 categorized the solutions to multi output or multivariate problems in two ways 1 problem transformation methods which transform the problem into several regular single output problems and 2 algorithm adaption methods which directly adapt algorithm into handling multiple outputs the multivariate rf mrf method can be treated as either a series of single output regression trees or as a multivariate model segal and xiao 2011 and the prediction scores of the two methods are similar in this paper the mrf regression used for each wave parameter prediction contains y different outputs each representing one of y different features grid points 3 2 training datasets to implement the machine learning techniques the original numerical results were formulated into a supervised learning framework this required the data to be structured as feature label pairs with a corresponding time index 3 2 1 input considering the training data set to be a two dimensional n x m matrix input features were represented by columns and time was represented by rows the input feature matrix was generated using 21 years of historical data january 01 1989 to december 31 2009 at the selected locations the historical data therefore consisted of 15 340 time samples at 12 h intervals the swan model in question was validated against three buoy locations looe bay penzance and perranporth van nieuwkoop et al 2013 correspondingly wave parameters at these three locations were used as input features to the surrogate model for each selected location time series of the four features of interest h s m m dir t z s and t p s are considered to train and validate the surrogate model the swan model is used exclusively with model results nearest the buoy locations used to represent synthetic buoy data inputs during the test phase of the model development however the synthetic data are substituted for real buoy measurements demonstrating how the surrogate model can initially be built in the absence of in situ data which can then be used in operation 3 2 2 correlation analysis prior to performing the regression analysis the correlations between feature variables were analysed using a heat map of the spearman s rank correlation coefficient matrix of the input feature variables which in this case were the wave parameters at the three buoy locations fig 4 for each wave parameter correlation between locations was observed the coefficient between the penzance buoy and the looe bay buoy was moderately higher these are along the same section of coastline and therefore more spatially correlated 3 2 3 output evgeniou and pontil 2004 suggested that for multivariate regression training a model on related features simultaneously rather than independently can improve predictive performance on the other hand if the output features are dissimilar training separate models independently for each feature can be more time efficient than taking a multi output approach faddoul et al 2010 in this case the model data showed spatial correlation across the measurement buoys fig 4 which indicated that multivariate regression would be of value correlation between the same parameters at different locations was greater than that between different parameters at the same location as such each state variable h s m dir t z t p was modelled separately while each spatially distributed variable was predicted simultaneously therefore the outputs for each wave parameter were also in the form of a two dimensional matrix defined as the y matrix in the y matrix the rows represent samples at different times while each column represents the result of a spatial grid point individually 4 model setup and application the application of the surrogate model including data processing and implementation used python 3 6 including the python toolkit scikit learn pedregosa et al 2011 4 1 pre processing swan model results were available in the network common data form netcdf during pre processing these were transformed to a matrix with three indexes time longitude and latitude these became the first three columns of the matrix and wave parameters h s m dir t z t p at a specific time and location corresponded to the remaining columns of the data matrix secondly invalid samples were removed from the data invalid samples in this model included grid points within the computational area that correspond to land these appeared as nan not a number values and were removed before generating the cleaned dataset the cleaned dataset for 21 years from 1989 to 2009 was randomly segmented into a training dataset 80 and validation dataset 20 fig 3 the data simulated in the year 2010 were held separately and processed as the test dataset normally machine learning models require cross validation to ensure a robust algorithm however for the rf algorithm the accuracy was evaluated on each oob sample which was equivalent to n fold cross validation and the results were obtained directly from the model in the rf model there is no requirement for feature engineering or transformation and normalisation of input features as an interpretable machine learning algorithm tree based algorithms can always compare prediction with what if scenarios which makes them work equally well with any monotonic transformation of a feature molnar 2020 4 2 evaluation criteria in this paper the accuracy of the surrogate model was quantified using the coefficient of determination r 2 rmse and nrmse as assessments of the uncertainty as well as the mean proportional differences to evaluate bias 1 coefficient of determination r 2 1 i 1 n y ˆ i y i 2 i 1 n y y i 2 2 root mean square error rmse 1 n i 1 n y ˆ i y i 2 3 normalized root mean square error nrmse r m s e y 4 mean proportional difference d 1 n i 1 n y ˆ i y i y i where n denotes the number of fitted samples y ˆ y and y represent the predicted value by the surrogate model actual value and mean of actual values respectively in the initial assessment of surrogate model performance over the study period based on the swan model the actual value refers to swan results while in the later model validation with measured data the actual value refers to the buoy observations 4 3 optimal gridding selection ideally the surrogate model would represent the same resolution as the swan model 219 223 points in this case representing each grid point as an output feature however the original swan resolution contained 42 500 valid points in the domain i e 42 500 output variables using this full resolution led to a requirement of over 2 tb of addressable memory for an 8 year training set to enable the approach using the full 21 years of data a dimension reduction process was implemented to reduce the spatial resolution the statistics of waves were considered stationary within each simulation and homogeneous area over the domain the surrogate model therefore required an evenly scaled resolution representing a smoothed version of the original model that captured the spatial distribution with a good agreement an effective resolution adjustment method was to use bilinear interpolation techniques accadia et al 2003 to transform the swan data to different grid resolutions in order to find an optimal gridding resolution for the surrogate model different scales of horizontal and vertical resolutions were assessed and compared the optimal resolution was affected by several factors including the computational cost and overall accuracy of the scaled resolution to represent the high resolution dataset to assess whether the low resolution data after the dimension reduction dr were accurately representing the original high resolution dataset dimension ascension da was applied to the adjusted low dimensional data using the same bilinear interpolation method the combined assessment which went through the dr da process took the following into consideration table 2 1 the minimum acceptable spatial resolution was set at 0 125 0 125 which resulted in 425 25 segments longitudinal x 17 segments latitudinal grid points in the area of study 2 the ratio of valid grid points non nan values after the dr da process selecting different resolutions generates a different number of nan values at the edges the ratio of non nan values was considered an important factor to the subsequent modelling and was used to evaluate the resolution selected 3 the mapping error after the dr da process the nrmses of four wave variables associated with each set of interpolated values were computed as the average of rmse at each valid point over the mean value of the wave parameter to avoid seasonal trends of wave parameter distributions 100 timestamps from the 21 years were sampled to execute the mapping nrmse assessment fig 5 4 the training time for the surrogate model in this comparison the time taken to train using 21 years of significant wave height with different resolutions was quantified 5 the accuracy of the surrogate model which was evaluated by the r 2 value in the test dataset the 100 sample averaged nrmse between interpolation from low resolution and the original data did not vary significantly fig 5 when varied between 1 2 and 1 8 the nrmse remained less than 2 of spatial average value for computational efficiency the scales from 1 4 to 1 8 were processed for training the surrogate model with results listed in table 2 the accuracy r 2 of the surrogate model against the scaled swan model remained stable around 0 957 but the training time dropped from 30 min to 6 min the combined evaluation considered the factors including non nan ratio mapping nrmse and training accuracy this indicated that the scale of 1 5 performed best the training time for a 1 5th scale surrogate model was around 17 min as a result the 1 5th scale was used in this work which transformed the original swan data to a grid with 43 longitude segments and 44 latitude segments the resolution change along the transformation is illustrated in fig 6 4 4 model hyper parameter setup the rf algorithm contains several hyper parameters including the number of estimators maximum tree depth maximum features at each split and maximum samples when looking for the best split all features were taken into consideration in each estimator the maximum tree depth was set to be default which means the nodes of each estimator were expanded until all leaves are pure the maximum sample was the length of the training dataset therefore the number of estimators was the key hyper parameter requiring tuning during model development generally the prediction accuracy improved with increasing numbers of estimators however increasing the number of estimators resulted in increased training time a parameter study of training the h s surrogate model showed diminishing returns when increasing the number of estimators fig 7 with greater than 200 estimators the r 2 curve and the rmse curve flattened and converged for both the training and test sets while the required computational time continued to increase linearly based on this with respect to both accuracy and training efficiency the number of estimators used in the present surrogate models was set to 200 5 results 5 1 accuracy of the surrogate model relative to swan model the surrogate outputs were compared to the equivalent swan estimates for each of the wave parameters studied r 2 values exceeded 0 9 with the exception of the mean wave direction table 3 and scatter plots demonstrate this strong correlation fig 8 the relative rmse of h s and t p are below 10 of each wave parameter s average value low rmse values within both the validation and test datasets indicated high confidence in the model s ability to replicate the swan results the prediction of peak period performed best among the four wave parameters with nrmse values 5 3 of the average peak period over the test data set among the target area the surrogate prediction results of mean wave direction m dir had relatively low prediction accuracy with the r 2 value below 0 9 in all of the datasets the nrmse accounted for 14 31 of the average direction the rf model outperformed the benchmark lr model reducing the nrmse by a factor of approximately 1 5 for all wave parameters table 3 indicating the efficacy of the rf algorithm the spatial distribution of the differences between the surrogate and swan models are illustrated in fig 9 the annual averaged proportional differences between the surrogate and swan model in 2010 were less than 1 8 of swan values in the northern area the surrogate model over estimated h s while in the southern area it underestimated the largest mean proportional difference within the domain was d h s 2 for the area close to the east of the isles of scilly however the annual averaged value from swan at that point was 0 66 m which resulted in a small actual difference of 0 0198 m areas closer to input positions were more likely to have lower rmse between the original swan simulation and the surrogate model in 2010 5 2 performance of the surrogate model to test the feasibility of the surrogate model for potential deployment applications a validation stage using measured data both as input and output was necessary the observation data of the three buoys from penzance looe bay and perranporth were used as input and the result at another buoy location was used for output validation this validation buoy close to the wave hub marine energy test centre site is representative of an offshore site with a valuable resource for marine renewable energy development ashton 2012 saulnier et al 2012 it is the primary resource for any real time decisions made for marine operations at this offshore renewable energy test site to compare the result from a more accurate location the da process was undertaken to generate a high resolution spatial data set the wave buoy data were then compared to the nearest high resolution grid point of the surrogate model for all four wave parameters studied the surrogate model consistently matched the real data better than the swan model table 4 fig 10 all r 2 and rmse values comparing the real data and surrogate model output were smaller than the equivalent statistics between real data and swan estimates in particular the surrogate model s prediction of zero crossing wave period had a dramatic improvement in accuracy compared to swan with an r 2 of 0 7205 and an rmse value half of the corresponding swan value table 4 the surrogate model estimated significant wave height r 2 value was 0 9067 with an rmse of 0 2556 m and nrmse of around 15 the nrmses of the surrogate model against measured data were also below 20 for both t z and t p the observations did not provide mean wave direction so the comparison of wave direction was possible 5 3 computational time and requirements training the model for each wave parameter for 21 years was completed in 17 min on a laptop with 16 0 gb ram and an i7 8550u processor using python 3 6 program in a windows environment including the steps for data processing feeding the surrogate model with one set of observation data yielding equivalent estimates of all wave parameters for the whole domain took less than 1 s on the same machine as the model architecture for the surrogate produced separate independent models for each wave parameter these can be run in parallel to make full use of available resources 6 discussion this paper has described a method for developing a surrogate wave model based on existing phase averaged spectral wave model output the surrogate model worked on the assumption that the spatial distribution of wave conditions created by the physical modelling process in this case swan was well defined and provides an additional service to immediately estimate wave conditions across the model domain from limited input values these results indicated that the rf based surrogate model represents an efficient and accurate method for predicting the spatial wave field when using solely physics based models forecasts and associated spatial estimates of current conditions rely on models updating every 6 12 h which require significant computing resources this system offers a low cost alternative to estimate spatial model outputs based on very limited input data and with significantly improved speed when deployed using data from 3 in situ wave buoys within the domain the outputs were more accurate than physical modelling equivalents furthermore the computational requirement was reduced by approximately a factor of 100 in the surrogate spatial estimation the most time consuming task was loading the machine learning model which took around 5 min for each wave parameter edge computing shi and dustdar 2016 technology would enable the model to be pre loaded into memory and give rise to nearly instantaneous spatial wave estimation as such this system that would potentially be accessible using a pc mobile phone vessel navigation system or autonomous vessel machine learning algorithms have been verified in the literature to solve the spatial regression problem for example james et al 2018 and o donncha et al 2018b 2018a used all wave boundary data to replicate swan outputs the implementation demonstrated in this paper is different to previous studies reducing the input data to three points within the domain and focusing on providing accurate now casting from measurement assets this makes direct comparison with previous studies difficult instead the work used benchmarking with a lr model and validation with measured data to assess the surrogate model benchmarking demonstrated that the extra complexity of rf algorithm improves accuracy verification showed that when in situ data were used to drive the surrogate model the results were improved when compared to physics based model estimates this is an important outcome as it showed that the combined surrogate model and in situ data have the potential to provide the most accurate description of current conditions across the model domain this makes the system highly suited to real time management for autonomous vehicles or marine operations for offshore infrastructure the surrogate modelling process methods demonstrated are additional to physics based models this system relies on an accurate spatial description of the wave conditions from which the surrogate model can learn the spatial distribution of conditions across the area as such accurate physical modelling remains central to this process this work shows that a surrogate model can offer real time estimates for wave conditions across a model domain the computational requirements and operation from limited real time measurements mean that higher resolution model output could now be implemented as a service but this could be considered as an additional service within the physical modelling architecture for some applications such a system based on global model data will be advantageous particularly where forecast data are available the spatial correlations quantified using this method could similarly be implemented to convert forecast from global models into higher resolution output allowing a forecast product to be defined with similar savings on data input and computational requirements further development in this area should consider how to effectively combine in situ measurements with this forecast activity taking advantage of the improved accuracy shown in this work has the potential to create an augmented forecast using a surrogate model procedure this work has demonstrated the surrogate system using wave buoy data these data were particularly suited to the application with accurate long term data sets however the buoys were not deployed for the purpose of this study and the system showed excellent results from measurements that were not optimally placed within the domain this highlights the potential for the surrogate approach to incorporate imperfect data further work should establish how such a system can work with other data sets satellite remote sensing offers global coverage while installed infrastructure vessels or autonomous systems all have the potential to gather data establishing how data that may be inaccurate or contain bias can be incorporated in this system has the potential to open up significant opportunity for revolutionising met ocean data provision and making real time access to accurate spatial data common in a marine setting 7 conclusion in this paper a novel method was proposed to derive an accurate spatial wave data set using in situ measured wave data from point locations using a machine learning approach based on a physics based wave model swan this approach used a rf algorithm to evaluate the spatial correlation of wave parameters within the computational domain this created a surrogate model which was an efficient method to replicate physical modelling without the undertaking computationally expensive calculations when using observations within the domain as inputs the surrogate model was more accurate than the corresponding estimates from the swan model drawing on global model data at the computational domain boundary this method supplements a combination of physics based modelling and in situ observations that form the most common approach to met ocean monitoring it combines the real time availability of in situ data with the spatial capabilities of physics based models and it is easily implemented with existing systems once developed the system required little computational power for implementation and as such it has the potential to provide real time spatial data coverage even in situations where data transmission or computational resources are limited this access to accurate real time spatial data has the potential to fundamentally change the way that met ocean data are used for the management and operation of marine infrastructure the system developed was highly flexible and has potential for implementation with other marine environmental parameters continued development will allow combined analysis of a range of in situ monitoring devices and can incorporate measurements of opportunity to create highly detailed and accurate data sets this will include mobile measurements from autonomous vehicles and a built in suitability to direct such measurements to improve accuracy and relevance of data sets for specific operations establishing this system with other data sets will create significant opportunity for making real time access to accurate spatial data the new normal in a marine setting declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements source data used in the preparation of this paper has been supplied by the channel coastal observatory uk funded under the regional coastal monitoring programmes by defra ongoing work is supported by the epsrc supergen offshore renewable energy hub grant no ep s000747 1 flexible fund support for malcom and collaboration with dr ed steele at the uk met office 
25816,common methods for spatial distribution such as hydrologic response units are subjective time consuming and fail to capture the full range of basin attributes recent advances in statistical learning techniques allow for new approaches to this problem we propose the use of gaussian mixture models gmms for spatial distribution of hydrologic models gmms objectively select the set of modeling locations that best represent the distribution of watershed features relevant to the hydrologic cycle we demonstrate this method in two hydrologically distinct headwater catchments of the sierra nevada and show that it meets or exceeds the performance of traditionally distributed models for multiple metrics across the water balance at a fraction of the time cost finally we use univariate gmms to identify the most important drivers of hydrologic processes in a basin the gmm method allows for more robust objective and repeatable models which are critical for advancing hydrologic research and operational decision making keywords physically based hydrologic models spatial distribution gaussian mixture models statistical learning 1 introduction spatial heterogeneity of hydrologic processes within a watershed is fundamentally impacted by basin topography topographic variations affect vegetation characteristics directly via climatic controls and indirectly via impacts on soil profile and water and nutrient availability fan et al 2020 tian et al 2020 zhang et al 2011 qiu et al 2001 all three topography soil and vegetation combine to impact hydrologic processes including evapotranspiration infiltration runoff and interflow see e g ghestem et al 2011 wilcke et al 2011 obojes et al 2015 young et al 1997 both vegetation and topographic variations such as slope and aspect impact snow accumulation and ablation patterns through controls on short and longwave radiation wind and interception lundquist et al 2013 maxwell et al 2019 varhola et al 2010 in montane regions heterogeneity of the landscape can have profound implications for all portions of the water balance orographic effects can create dramatic differences in precipitation rates on either side of mountain ranges as well as influencing the phase rain versus snow of that precipitation roe 2005 landscape variability in these regions is of particular interest due to the role these river basins play in the waterscape connecting natural headwaters with human needs karpouzoglou and vij 2017 these water towers of the world supply water to over half of the human population partnership 2014 immerzeel et al 2020 viviroli et al 2007a understanding the variations of hydrologic processes that contribute to the timing and quantity of streamflow from these basins is a fundamental goal for both scientific researchers and increasingly operational forecasters in the water management sector these questions have become all the more pressing in regions where climate change is inducing shifts in the water balance not previously seen in order to meet these needs and spurred by increases in computational resources the use of physically based spatially distributed hydrologic models is becoming more common physical models indicate a bottom up approach in which mass and energy balances are resolved with a spatially distributed set up these simulations are performed at multiple points across a river basin and then aggregated models may be partially or semi distributed where some model components e g input data are varied across the landscape but others e g parameters are held constant or fully distributed all components are spatially variable in spatially lumped models on the other hand all hydrologic processes occurring within a basin are simulated at a single point and output is given as a single time series for the basin usually streamflow at the basin outlet much research has focused on calibration approaches for distributed models in an effort to address concerns of overparameterization non identifiability of parameters equifinality and scale consistency beven et al 1988 beven 1989 wood et al 1988 blöschl and sivapalan 1995 andréassian et al 2012 bai et al 2009 pianosi et al 2015 comparisons between lumped semi distributed and fully distributed calibration techniques e g khakbaz et al 2012 reed et al 2004 lobligeois et al 2014 carpenter and georgakakos 2006 have attempted to characterize the relationship between spatial distribution and model performance with mixed results while more modeling points can better capture variations in topography vegetation and climate they also introduce a greater number of free parameters which can contribute to overparameterization issues a high number of free parameters mean these models can often be calibrated to a baseline level of performance for average conditions regardless of the quality of the input data but the resulting parameters may have not be reflective of actual physical conditions as a result model performance will decline for previously unobserved conditions the extent to which distributed or lumped models are most appropriate may depend on the landscape and application of the model lobligeois et al 2014 research on scale consistency focuses on reconciling parameter values across scales in an effort to avoid sudden changes in results when spatial resolution is changed sivapalan and kalma 1995 these approaches may be top down calibrating a lumped model and disaggregating parameter values e g tran et al 2018 or bottom up regionalization e g blöschl and sivapalan 1995 arsenault and brissette 2014 hundecha et al 2016 samaniego et al 2010 this work on reconciling parameter values across scales largely focuses on the calibration step of model set up but less attention has been given to the prior step of selecting which and how many specific locations within the basin to include in the model this critical first step of spatially distributing a model impacts all subsequent set up including input data distribution across the basin and parameter definition and calibration methods proposed in the literature for selecting modeling locations or otherwise partitioning the basin include representative elementary areas reas an intermediate scale at which neither small nor large scale processes dominate wood et al 1988 representative elementary watersheds rews units derived based on the streamflow network and over which equations of mass and energy fluxes are integrated reggiani et al 2000 reggiani and rientjes 2005 and landform classes based on the upness index summerell et al 2005 roberts et al 1997 these methods showed promise in capturing spatial variability but were limited by detailed data or catchment monitoring requirements inability to simulate multiple hydrologic processes rather than runoff alone and or assumptions in the derivation process more recently pixel based distribution approaches have risen in popularity to be compatible with gridded remote sensing products though convenient this approach is disconnected from the physical characteristics of a basin pixels may straddle discontinuities in topography or land use introducing uncertainty into simulations and or runoff routing in addition pixel based approaches typically result in hundreds or even thousands of simulation points for a moderately sized basin see e g tran et al 2018 since model resolution is frequently dictated by input data resolution not only does the high number of modeling locations raise equifinality concerns these models often have higher simulation times and increase computational requirements this can be particularly problematic for time or resource constrained applications such as real time flood forecasting in montane regions elevation bands are sometimes used as a simple alternative to capture spatial variability e g bongio et al 2016 valéry et al 2014 but are also often arbitrarily defined and may not align with topographic features alongside pixel based methods the most widely used approach for spatially representing a basin is hydrologic response units hrus leavesley et al 1983 flügel 1994 1997 defined as areas of a basin that can be considered homogeneous in all respects influencing the water balance e g topography land cover and vegetation density and soil type conceptually simple hrus are favored by some modelers as having a stronger connection to physical basin characteristics than pixel based models hrus are the default distribution method in several major hydrologic models including the precipitation runoff modeling system prms markstrom et al 2015 the soil and water assessment tool swat see e g kalcic et al 2015 teshager et al 2016 qi et al 2017 precipitation runoff evapotranspiration hydrotope model prevah viviroli et al 2007b the sacramento soil accounting model sac sma national oceanic and atmospheric administration 2002 and the regional hydro ecological simulation system rhessys tague and band 2004 in addition hrus are used by many large water management agencies that rely on physical hydrologic models including california s department of water resources dwr and pacific gas electric pg e energy company despite their popularity hru based distribution presents both theoretical and practical problems there is inherent tension between having more smaller hrus that are more likely to conform to the assumption of homogeneity and the need to reduce unnecessary model complexity in addition though hrus are meant to represent a distributed sub area of a basin hydrologic processes are simulated at a particular point usually the geometric centroid of the hru this necessarily limits the points of the basin that can be simulated with hrus for example the geometric centroid will always be lower than a peak or ridge meaning that the model is likely to miss the highest elevations hrus are frequently delineated using a gis based approach starting with a digital elevation model and using topography including drainage divides slope and aspect to partition the study area see e g flügel 1994 1997 koczot et al 2005 though tools such as the arcmap based tool gis weasel viger and leavesley 2007 have been built to assist with this process this method of hru delineation involves significant subjective decision making such as selecting minimum hru size and stream segment resolution all of this can translate to multiple days of hands on work other methods have been proposed for hru delineation including khan et al 2013 and khan et al 2016 who overlaid soil and stream network data on a set of identified landform classes and fiddes and gruber 2012 who used a sub grid sampling method to include the effects of topography in a lumped model while promising these approaches both rely on assumptions that are not generalizable across catchments and or all aspects of the water balance ultimately hru delineation and by extension selection of modeling locations involves subjective decisions and significant time investment given these issues there is need for a simple rapid and objective approach to selecting modeling locations for spatially distributed hydrologic models recent advances in statistical learning algorithms have made possible alternative approaches to this problem such algorithms broadly speaking are used to identify and characterize patterns in data particularly those that are not obvious or that would be too labor intensive to test individually shen 2018 use of statistical learning is increasing in the field of hydrology see e g oroza et al 2018 avanzi et al 2019 schmidt et al 2020 kim et al 2020 but understanding which algorithms and for what applications it is most appropriate is an ongoing area of research shen 2018 kim et al 2020 for example how statistical models compare to and interact with traditional physically based hydrologic models has not been comprehensively tested oyebode and stretch 2019 physical models are usually mechanistic and process based by design since it is frequently important for hydrologists to understand the causes and relationships underlying an observed phenomena statistical learning on the other hand typically identifies correlations and associations between variables without suggesting causality the implications of this discrepancy and whether they matter for using these two model types together is an open research question oyebode and stretch 2019 schmidt et al 2020 here we use statistical learning to return to the question of selecting modeling locations for a physically based model and we propose a method that is grounded in physical properties and does not obscure process understanding given the relatively recent introduction of statistical learning to hydrology there are many possible approaches that have yet to be tested but as a first step we focus on mixture models a type of algorithm that has emerged as a way of optimally identifying a set of underlying components that best describes a population we propose the application of gaussian mixture models gmms as an objective efficient and physically based spatial distribution technique that addresses both the theoretical and practical shortcomings of existing methods for selecting modeling locations in a basin using basin characteristics that influence the water balance mixture models identify a set of modeling locations that optimally characterize the water balance throughout the basin gaussian mixture models have been successfully used to capture spatial patterns in other hydrologic contexts such as snow water equivalent swe distribution at a single site oroza et al 2016 but have not yet been tested in conjunction with a physically based hydrologic model this is also the first time it has been tested at landscape scale across the diverse topography of montane river basins we demonstrate the gmm based distribution method in two contrasting headwater catchments of the sierra nevada using the precipitation runoff modeling system prms a physically based rainfall runoff model commonly used in water management owing to their widespread use by researchers and forecasters we use a gis hru based prms model as a baseline to compare performance of the gmm based models in the research reported here we address the following 1 what is the measurable impact of a gmm based spatial distribution method versus an hru based method on predictive accuracy 2 are these spatial distribution methods robust to unobserved extreme hydrologic events which hydrologic process es drive improvements or declines in modeled performance 3 what attributes are the most important drivers of predictive accuracy in montane catchments 2 methods and data 2 1 study area we focus on almanor and the east branch two headwater catchments of the north fork of the feather river the northernmost basin of the california sierra nevada see fig 1 the feather river is important for water resources and energy production pacific gas electric pg e california s largest utility company operates a series of hydropower plants on the north fork totaling 740 mw of installed capacity about 19 of company s overall hydropower portfolio the basin also drains to lake oroville the primary storage reservoir for the state water project operated by the california dwr and serving drinking water and agricultural water needs in the central and southern parts of the state as a lower elevation sierra nevada basin peak elevation 2950 m the feather is susceptible to climate change effects as more precipitation falls as rain rather than snow the feather river can therefore be thought of as an early example of how other basins in the sierra nevada may change with rising temperatures freeman 2011 the main stem of the north fork of the feather originates in the almanor catchment to the northwest and is regulated at the outlet of lake almanor almanor drains an area of approximately 1150 km2 and contains mount lassen the highest and wettest point in the feather river at about 2900 m elevation and 3000 mm of annual precipitation koczot et al 2005 geologically almanor is part of the cascade mountain range rather than the sierra nevada making it distinct from the rest of the basin the subsurface is largely comprised of more permeable volcanic rocks and baseflow makes up a higher percentage of flow than in other subbasins freeman 2008 the east branch is a tributary of the north fork and drains an area of approximately 2650 km2 it meets the north fork south of lake almanor the east branch is rain shadowed due to the eastern ridge of the upper north fork canyon on its western edge and is thus considerably drier than almanor with an average annual precipitation of about 300 mm the subbasin has a largely granitic subsurface and low baseflow freeman 2008 it is also mostly unregulated 2 2 gaussian mixture models for spatial distribution gaussian mixture models are a statistical learning algorithm used to identify a subset of discrete points that best represent a feature space here feature is a measurable characteristic that describes a phenomenon being observed bishop 2006 for example in describing runoff from a basin a feature may be the basin s elevational distribution a feature space is the single if there is only one feature or multidimensional if there is more than one range collectively defined by the feature data for example in fig 2 there are two features elevation and slope creating a two dimensional feature space all data points fall shown as dots somewhere in the feature space features must be continuous numeric variables for use in a standard gmm but otherwise may be defined at the discretion of the modeler the gmm algorithm selects the optimal points across the feature space by assuming that the feature space can be represented by superimposing a finite number m of latent components which are normally distributed fig 2a shows three latent components each of which can be uniquely described by a mean expected value μ and covariance σ each is also assigned a mixing parameter π based on the prior probability of observing that component essentially a weighting factor thus the parameters that are defined in fitting a gmm to a particular dataset are the means covariances and mixing parameters for each latent component we take the expected values of the latent components as the set of points that optimally describes the feature space in concrete terms and relating the example schematic in fig 2a the three μ values shown as red x s are the points that best represent the distribution of elevation and slope in this hypothetical basin however a point that exists in feature space say for example an elevation of 2300 m and a slope of 85 may not exist physically in the basin thus once the means have been identified we use a nearest neighbors approach to find the physical location that is closest to the means of feature space fig 2b these locations define the spatial distribution of the gmm based prms models henceforth modeling locations and are analogous to the hru centroids that define the spatial distribution of traditional models fig 2c maps of the actual selected modeling locations for each subbasin are available in the supplementary information figs s1 and s2 formally the ability of a gmm latent component to represent the feature space is modeled as a multivariate normal distribution n with expected value μ and covariance σ applied to a d dimensional vector of empirical data x equation 1 1 n x μ σ 1 2 π d 2 1 σ 1 2 exp 1 2 x μ t σ 1 x μ the collective ability of the m components to reproduce the feature space is calculated by superimposing each n m weighted with its mixing parameter π m the number of features i e the length of x determines the dimension of each n m distribution the expected values covariance and mixing parameters that best represent the data are identified by maximizing the likelihood function of the superimposed multivariate normal distributions given by equation 2 in other words the gmm maximizes the following objective function 2 ln p x n π μ σ n 1 n ln m 1 m π m n x n μ m σ m subject to 3 m 1 m π m 1 in this study the number of dimensions was five and the features were basin elevation slope aspect vegetation coverage and soil hydraulic conductivity k s a t together these features capture the major drivers of the water balance endogenous to the basin i e not driven by climate or weather inputs including spatial distribution of the snowpack an important if not dominant component of the hydrologic cycle in the feather river evapotranspiration and infiltration characteristics elevation slope and aspect were defined using the usgs usgs national elevation dataset ned eros data center 1999 and vegetation data were obtained from the 2013 u s forest service landfire dataset landfire 2013b a respectively see supplementary information section s1 2 for full details on the landfire dataset topographic and vegetation data rasters were both at 30 m resolution these rasters were masked to the extent of the subbasins in the study and filtered to remove pixels with undefined values for example flat areas with undefined aspect and passed through a 1 in 2 resampling algorithm to make processing computationally feasible pandas dataframe sample the pandas development team 2019 subsampling was performed with a uniform distribution without replacement and with an initial seed for reproducibility soil hydraulic conductivity k s a t was derived from stago2 data soil survey staff natural resources conservation service 2019 which are available as shapefiles indicating the extent of different geologic groups i e clusters of one or more soil types each of which is associated with a set of unique soil properties properties were first depth integrated then spatially averaged using the percent of each soil type in a geologic group this averaged property i e k s a t was assumed to be spatially homogeneous across the geologic group the distribution of the averaged k s a t was used as the gmm input feature while this is relatively simple as a descriptor of soil type it demonstrates how soil properties may be incorporated into gmm modeling future work particularly in basins with large contributions of groundwater to streamflow could include more detailed assessment with multiple soil characteristics as gmm inputs using the five rasters as inputs the gmm algorithm was run using scikit learn s mixture gaussianmixture class pedregosa et al 2011a covariance parameters were trained using the spherical setting meaning that a single covariance value was calculated for each component this was selected to in order to simplify the analysis and to decrease the runtime of the algorithm but future work should explore the implications of other covariance options since many of the landscape features that control movement of water co vary but sometimes to differing degrees across the landscape beven et al 1988 it is possible that applying the different covariance parameters for each component would improve model performance the mixture gaussianmixture class uses the expectation maximization algorithm an iterative gradient descent method to optimize of equation 2 and identify the most likely mixing parameters covariance and means to explain the data mclachlan and peel 2004 pedregosa et al 2011b the optimization terminates when a maximization step no longer increases the log likelihood as noted the optimal expected values means of the latent components in feature space were translated to physical modeling locations using a nearest neighbors algorithm features were scaled with equal weight to prevent features with higher magnitude values from dominating the nearest neighbor search in addition to these five dimensional multivariate gmms we ran five additional gmms in each basin each driven by only one of the features univariate gmms this was done to assess the usefulness of each individual gmm input feature and we show that this can inform feature selection and their relevance to different hydrologic processes gmms fall into a general category of statistical learning models called clustering algorithms which aim to find areas of relative homogeneity of data in feature space while any number of clustering algorithms could also be effective for spatial distribution of modeling locations we used gmms for this analysis because every latent component in the gmm is affected by every data point this is not necessarily true of other clustering algorithms such as k means where means are calculated based only on the points assigned to a given cluster the former thus had a better potential to represent all points across the feature space 2 3 precipitation runoff modeling system the precipitation runoff modeling system prms is a distributed parameter hydrologic model developed by the u s geological survey markstrom et al 2015 the model runs on the daily time step taking as inputs daily precipitation and minimum and maximum temperature which are distributed to each hru either ahead of time by the modeler or through an interpolation scheme within the model prms simulates mass and energy balances beginning with calculation of solar radiation and precipitation phase partitioning and ending with computation of total streamflow intermediate processes include snow accumulation and ablation canopy interception and evapotranspiration infiltration surface runoff interflow and groundwater recharge a description of the major processes included in prms and their calculations can be found in the supplementary information section s1 5 prms is executed in a linear fashion at each time step in the simulation with each hydrologic process represented by a module of code for some processes users may specify a desired calculation method by selecting from multiple possible modules the spatial distribution in prms is achieved by partitioning the modeling area into hrus represented by a specific geographical point in the basin by default the geographic centroid koczot et al 2005 markstrom et al 2015 to which input data are distributed and at which the water balance is simulated the water balance is simulated separately at each hru and scaled according to the surface area of the hru outflow is aggregated across the basin based on the selected streamflow routing method prms is currently used throughout the california sierra nevada for streamflow modeling by pg e richards 2018 it is also being actively developed for new river basins by dwr see e g burley and fabbiani leon 2018 its widespread use for water resources planning as well as its commonalities with other distributed parameter models makes it ideal for this study the model is publicly available at https www usgs gov software precipitation runoff modeling system prms the latest release is version 5 but at the time of this research version 4 0 3 was the most updated available major changes between the two versions do not affect the modules used in this study 2 3 1 prms models used in this study required input data for prms are daily temperature range and precipitation amount which are spatially distributed based on a user selected method here both temperature and precipitation were pre distributed to each hru or modeling location before executing prms precipitation was distributed using an algorithm called draper in which spatially distributed long term average monthly precipitation surfaces from the parameter elevation regressions on independent slopes model prism dataset are adjusted using daily ground based values daly et al 2008 each day the prism surface is multiplied by the ground based measurements as a percent of long term daily average this effectively tilts the prism surface to reflect the daily observations for more details on the draper algorithm see donovan and koczot 2019 for a full discussion of the implementation of draper in this study see supplementary information section s1 4 daily minimum and maximum temperature were also distributed externally to prms this procedure was selected based on an analysis of temperature variability across the feather river basin that showed both temporal sub monthly and spatial intra subbasin variability in lapse rates avanzi et al 2020 internal prms distribution methods did not permit this variability to be accounted for instead temperature was distributed as a two step process in which we regressed elevation against minimum and maximum temperature for several training stations in each subbasin to establish daily basin wide lapse rates residuals between temperature predicted using these lapse rates and observed temperature at a set of evaluation stations were distributed using multilinear regression with elevation and the temperature at a designated seed station the final values of maximum and minimum temperature were obtained by subtracting the residual from the first guess temperature obtained using the lapse rates computed in the first step for more details on this process see supplementary information section s1 4 in prms some process calculations are pre determined while for others the user may select from a variety of options in this study solar radiation was calculated using a degree day approach which uses daily maximum air temperature to obtain actual daily solar radiation ddsolrad hru module evapotranspiration et was calculated using the jensen haise formulation potet jh prms also requires values for several dozen parameters that may be spatially and temporally global or may be set on a per month or per hru basis values based on topography and canopy cover were computed based on the usgs ned eros data center 1999 and the us forest service landfire dataset landfire 2013b a respectively remaining non calibration parameters were computed based on available data set to default values or retained from the original usgs version of prms on the feather river details of this process for both the gis and gmm versions can be found in the supplementary information section s1 in this study we use a traditionally designed prms model as a benchmark for model performance this model is henceforth referred to as the gis model in reference to the geographic information system framework typically used to delineate hrus the gis model was based on the feather river prms model version 2 designed by the usgs in the early 2000s in which hrus were delineated using standard methods see koczot et al 2005 for details we updated the model to prms version 4 0 3 as part of this process we made minor manual updates in the hru boundaries to better reflect drainage divides in addition prms version 4 allows for greater functionality in terms of streamflow routing which was introduced in lieu of straight summation of hru outflows at each time step as was done in version 2 details of the new model design including how non calibration parameter values were selected are given in the supplementary information section s1 other than the process for selecting modeling locations the prms set up for the gmm models was largely the same as for the gis model topographic and vegetation parameters were based on the values at the selected modeling locations unlike hrus which have each represent a different surface area each modeling location was assigned an equal portion of the headwater catchment based on the gmm feature rasters this decision was made in order to test the baseline effectiveness of the gmm but future work could test the effectiveness of scaling based on the surface areas that are represented by relatively homogeneous areas of feature space the gmm models did not employ a streamflow routing method since modeling locations are not coupled with drainage areas instead runoff from each modeling location was summed for each time step for details on the model design see supplementary information section s1 in order to explore the first two research questions assessing the accuracy of the distribution methods and robustness to unobserved events we trained four models for each subbasin with varying numbers of target modeling locations one lumped case for comparison 50 100 and 200 this process allowed us to identify how the number of modeling locations changes the performance of the model in order to address the third research question identifying important drivers of predictive accuracy we trained univariate gmm models i e each using only one of the five variables from the multivariate versions this allowed identification of the most important driving variables for each subbasin for this step models were run for the best performing number of modeling locations from the multivariate gmms 2 4 calibration and evaluation strategy we used a multi step multi objective method to calibrate the models in order to avoid the overfitting that is common when calibrating on streamflow alone hay et al 2006 gupta et al 1998 in order to isolate questions of basin partitioning from issues of distributed parameter calibration all calibration targets were basinwide in other words we did not calibrate to internal basin gauges or other internal targets based on the availability of data 20 year period of record and the dominant hydrologic processes in the feather river we chose to calibrate on swe et and basin outflow five calibration steps were used each with a specific target variable objective function and set of calibration parameters the order of the steps reflects the modeling order of hydrologic processes in prms the objective functions and target variables for each step are as follows daily rmse of swe cumulative annual bias of et daily weighted sum of kling gupta efficiency kge kling et al 2012 gupta et al 2009 and log nash sutcliffe efficiency lognse nash and sutcliffe 1970 of full natural flow fnf monthly weighted sum of kge and lognse of fnf and cumulative annual bias of fnf for any objective functions using the kge metric all rows with missing observations were removed before the calculation daily fnf values were weighted between two metrics in order to capture the performance for both high and low flows the objective functions for each step are listed in table 1 daily swe maps published by margulis et al 2016 were masked to the almanor and east branch subbasins and then aggregated to catchment wide values annual distributed evapotranspiration data were calculated following roche et al 2020 on a 30 m basis and were aggregated to catchment wide values finally daily runoff values for the outlet of each subbasin were computed from fnf values provided by pg e fnf is a reconstructed time series of runoff that would have occurred in the absence of diversions or other human activity uncertainties in sensor readings can result in negative fnf values so the values for the period of record were smoothed using a five day moving average window any fnf values that were still negative after the smoothing were masked to nan prms like many large scale hydrologic models has hundreds of parameters available for calibration some of which may be individually calibrated for different months of the year or on a per hru basis we selected calibration parameters on the basis of previous sensitivity analyses of prms markstrom et al 2016 and the availability of informative target data on which to calibrate them avanzi et al 2020 while some parameters are calibrated separately for different months all are spatially lumped the parameters calibrated at each step are presented in table 1 phase partitioning parameters tmax allsnow tmax allrain and adjmix rain which determine the percentages of precipitation that falls as rain and snow were calibrated to basin wide swe in addition we calibrated freeh2o cap the free water holding capacity of the snowpack subsurface parameters that are related to how much water is directed to the capillary soil layer were calibrated to et the capillary layer is the only soil layer from which et can occur so these parameters govern the largest source of et in the basin transpiration by plants though this neglects parameters related to evaporation from intercepted storage or sublimation from snow these contributors to et are much lower than transpiration by plants from soil storage finally other subsurface parameters governing overland flow interflow and groundwater flow were calibrated to streamflow at various time steps daily monthly and annual reflecting the timescales over which we expect these processes to occur for details on the use and physical meaning of these parameters see markstrom et al 2015 based on availability of input and calibration data the total calibration period included water years 1998 2016 instead of a traditional split sample approach using continuous calibration and validation periods a stochastic multi split process was employed to avoid biases that might arise from arbitrarily selecting a calibration period eleven of the nineteen water years in the period of record were randomly selected for calibration a process which was repeated to give five 70 30 calibration validation splits see table s2 in the supplementary information for specific years used for each split all calibrations were performed using the shuffled complex evolution sce algorithm a well established method that was specifically developed for large hydrologic models with many degrees of freedom duan et al 1993 1994 this algorithm was designed to handle arbitrary objective functions differences in parameter sensitivities and the presence of non optimal local minima in the feasible space in brief sce works as follows randomly selected sample points are partitioned into complexes which each evolve separately allowing the parameter space to be explored more efficiently periodically complexes are combined shuffled and re partitioned into new complexes this step allows for sharing of the information gained separately by each complex as it evolved the algorithm stops when one of several possible specified convergence criteria is met see lines marked with asterisks in table s1 in the supplementary information this process is performed sequentially for each of the calibration steps as listed in table 1 one iteration through all calibration steps is a calibration round users may set multiple calibration rounds here we used five thus calibration is both sequential and iterative while not guaranteed to reach a pareto front or other global optimum this accounts for trade offs between the objective functions and prevents overcalibration to any single objective function the various metaparameters of the sce algorithm including number of complexes number of calibration rounds and convergence criteria may be individually set for each step and were selected based on a combination of suggested default values hay and umemoto 2006 and trial and error table s1 in the supplementary information lists the metaparameters used for each step and a short explanation of how they were chosen more details of the sce algorithm can be found in duan et al 1992 1993 1994 each gis or gmm model was calibrated five times according to each of the splits and each calibration was assessed separately across its validation period the metrics used in model performance assessment were daily absolute bias of swe cumulative annual absolute bias of et and daily kge lognse and root mean squared error rmse of streamflow rmse gives more weight to accurately reproducing peaks in the time series while lognse gives more weight to baseflow periods it should be noted that swe bias was calculated across all pixels including those without snow to be consistent with the prms model s calculation of basin wide metrics the performance of the models relative to observed values may therefore appear artificially good but values are appropriate for comparison across models metrics and scores values are shown in figures in the main text as absolute values signed scores are reported in the supplementary information tables s3 and s4 to allow for comparison across metrics with different units all twenty five calibrations five for each of the five models were ranked for each metric then the average rank across all five metrics for each calibration was calculated and the median average rank was the score of that model unless otherwise noted all performance values reported are for the calibration with the median average rank other performance metrics focusing on conditions that are of particular interest to forecasters and modelers such as peak swe and flood periods are also presented but were not used in scoring model performance these include the relative error of high flows rehf silvestro et al 2018 to assess reproduction of peak flows peak swe error date of peak swe and baseflow error peak flows were identified as the top 5 of flows across the period of record and days of peak flow were the days these occurred only the peaks occurring within a calibration s validation period were used to calculate rehf all other metrics were calculated separately for each validation year and averaged 3 results this section presents results on gis and gmm model performance section 3 1 gives results under average conditions i e metrics computed across validation periods which were used to rank overall performance of the models it also discusses computational resources required to run the gmm algorithm next we present further analysis aiming to verify gmm performance during periods of extreme conditions section 3 2 finally we present the results of the univariate gmm models demonstrating how these can inform feature selection for different parts of the water balance section 3 3 3 1 gmm model set up and performance in both subbasins the gmm model performance is comparable to and in some cases better than the gis models with respect to all metrics calculated for the validation periods fig 3 the median average rank fig 3a shows that the gis model is best performing lowest ranked in almanor and the 200 location gmm model is best in the east branch the best performing gmm model in almanor is the 100 location one within a subbasin there is little variability in performance across models including both gis and gmm models swe and et bias in both catchments for example vary by less than 20 mm runoff rmse is variable between the two catchments reflecting the difference is average flow but models for a given catchment show similar performance however no single model consistently performs best for all components of the water balance performance rankings are more consistent in the east branch than almanor particularly with respect to streamflow metrics here the 200 location gmm generally performs best with the exception of swe in both subbasins lumped gmm model which represents a baseline from which to assess the improvement provided due to any type of spatial distribution ranks somewhere in the middle with respect to swe and et performance but consistently performs poorly with respect to streamflow in the east branch overall ranking consistently improves with higher numbers of gmm modeling locations but in almanor this is only true up to 100 modeling locations thus the best performing gmm uses half the number of modeling locations in almanor than the east branch fig 3a possibly reflecting the difference in catchment area the area of the east branch is approximately double that of almanor the optimal number of modeling locations may also be influenced by the input data raster resolutions further discussion of the implications of this result can be found in section 4 1 the computational times required to run the gmm algorithm were between 80 and 1500 s on a single core of a high performance computing cluster 3 9 ghz fig 4 this time includes all steps from raster sub sampling scaling fitting the gmm model and the nearest neighbor search for the physical modeling locations times varied by subbasin east branch models took longer to run than almanor due to the larger raster size and number of components 3 2 extreme and peak periods as with average condition metrics the gmm based approach yields comparable if not better performance than the gis method during extreme periods fig 3b in particular the gmm models that perform best in each subbasin under average conditions 100 location in almanor and 200 location in the east branch fig 3a also match or exceed gis performance during extreme periods we note that the lumped model performs worse in both subasins with regard to extreme periods than the average conditions which is further discussed in section 4 1 peak swe marks the transition from accumulation to ablation season in the sierra both timing and magnitude of peak swe are important seasonal benchmarks in snow dominated basins on average peak swe was better simulated in almanor but day of peak swe was better simulated in east branch peak swe tended to be overestimated in both basins see table s4 in the supplementary information day of peak swe was estimated later than observed in the east branch there was no consistent pattern in almanor july september flow was used to capture baseflow performance and rehf was used to assess peak flows like the swe metrics performance was better for both in almanor for baseflow in particular observed flows are lower on average in the east branch making the performance in almanor even more comparatively strong peak flows were generally challenging to capture in both subbasins with minimum rehf of 0 3 100 location gmm in almanor though there was little consistency in model rankings between baseflow and peak flows the best performing gmm models were again able to meet or exceed gis performance 3 3 univariate gmms univariate gmm performance was generally worse than multivariate models figs 3a and 5 though metrics fell within the same order of magnitude reflective of a baseline level of performance that can usually be achieved for average conditions by calibrating models with a high number of free parameters in the east branch the elevation driven gmm performed best as based on median average rank followed by in order the models driven by slope aspect saturated hydraulic conductivity and vegetation with median rank of slope driven model only slightly larger than elevation based model in almanor models in order of performance from best to worst were based on slope aspect saturated hydraulic conductivity elevation and vegetation slope performed particularly well with respect to runoff rmse and kge while aspect performed better with respect to swe bias and runoff lognse possible reasons for the differences between the two subbasins are discussed in section 4 4 4 discussion the gmm method has a number of significant logistical and theoretical advantages compared to gis models but its usefulness is still contingent on its ability to replicate the performance standards of traditional models in the following sections we first discuss both the performance of the gmm models and under what conditions they are able to meet or exceed the performance of traditional models section 4 1 next we describe the logistical advantages of gmm and the implications for users of hydrologic models section 4 2 in the final two sections of the discussion we look more specifically at the spatially distributed performance of the gmm models section 4 3 and the performance and use of univariate gmm models section 4 4 4 1 model performance the gmm method provides a sound objective basis for spatially distributing hydrologic models gmm based models match the performance of the gis models under both average and extreme conditions fig 3 they accurately simulate water balance components over time and provide more accurate spatial distribution of streamflow generation than do gis models we focus this discussion on the overall performance of the gmm models as indicated by the model score since in most cases users need and expect models to perform well across all components of the water balance in order to ensure that physical processes are being accurately simulated however there may be cases in which modelers would have more specific needs for which metrics related to a certain component like swe or et would be more informative the best performing gmm model based on model score exceeds the performance of the gis model in the east branch but not in almanor fig 3a thus the variables selected for gmm prediction may be more relevant for water balance partitioning in the east branch than almanor due to the particular hydrology of the catchments almanor is more subsurface dominated than the east branch and saturated hydraulic conductivity the only predictor used here related subsurface conditions is a relatively limited characterization of soil and groundwater flow thus the variables used in this study were likely a more complete characterization of hydrologic processes in the east branch another factor may be the relative importance of the gmm variables the most informative variables in almanor based on the performance of the univariate models were aspect and slope fig 5 the gis and gmm distributions of these factors were similar and largely consistent with the raster values see supplementary information fig s4 the most informative variable in the east branch was elevation here the gmm models span a greater range than the gis distributions which may have contributed to improved performance in addition the overall lower elevation of the east branch and the fact that its highest elevations are rain shadowed means that a greater proportion of the precipitation in east branch falls in the rain snow transition than in almanor 40 versus 33 where the rain snow transition is defined between 1300 and 2200 m cui et al 2020 this means that uncertainties in modeling precipitation phase well reported in the literature e g harpold et al 2017 jennings and molotch 2019 feiccabrino et al 2015 will affect the east branch more than almanor thus the east branch may be more sensitive to tuning the elevational distribution of modeling locations than almanor in other words there may be more potential for the gmm approach to improve results in addition to capturing temporally averaged metrics the gmm models demonstrated the ability to accurately reproduce periods of extreme or peak conditions good performance during extreme flow periods is particularly important for applications like flood forecasting but increasingly necessary for all streamflow modeling as climate change increases year to year variability and induces more severe weather events day of peak swe for example has traditionally been estimated as april 1st montoya et al 2014 this estimate has always been uncertain due to seasonal weather characteristics and elevation effects but is becoming increasingly inexact due to climate change induced shifts in precipitation margulis et al 2016 thus it is valuable for forecasters to be able to model the date of peak swe rather than relying on the april 1 estimate the ability to reproduce baseflows and correspondingly low flow periods is also of greater concern as length and severity of dry periods in arid regions are projected to increase williams et al 2020 woodhouse et al 2010 cayan et al 2010 in each of these cases the best performing gmm model is able to meet or exceed the performance of the gis model furthermore there was consistency between gmm models that performed well under average conditions and those that performed well under peak conditions meaning that forecasters would not need to rely on a separate model for extreme periods since models calibrated to average periods do not always work well under extreme conditions see e g vaze et al 2010 the consistent performance of the gmm based models is a significant advantage finally the relative performance of the gmm models varied between almanor and the east branch in both subbasins the lumped gmm model had the worst overall performance particularly with regards to streamflow but the best performing gmm models differed between the two catchments the uniformly poor performance of the lumped models reflects the added value of spatially distributed models in diverse topography where a single location is not sufficient to capture the variations across the landscape that impact the hydrologic process since lumped model performance is especially poor for streamflow metrics the variations the lumped models fail to capture may be related in particular to uncertainties in the simulation of subsurface processes e g model structure and lack of data moreover the lumped models perform even worse with respect to other models during extreme periods which may be reflective of the inability of these models to capture variations across the landscape that are especially important for extreme periods for example rapid snowmelt from high elevations that contributes to flooding for forecasters in particular accurate modeling of streamflow and the ability to capture both flood and drought conditions is imperative to optimize dam operations and to protect infrastructure and communities downstream our results are consistent with other studies e g lobligeois et al 2014 that have shown that spatially distributing hydrologic models can yield significant improvements over lumped models in basins with heterogeneities climatic inputs this an argument in favor of performing the additional steps required to run the gmm algorithm and so obtain a spatially distributed model for montane regions unlike the lumped models the spatially distributed models do not show this uniform drop in performance but there is evidence that adding more modeling locations may only be helpful to a point in almanor the 100 location gmm model outperforms the 200 location gmm which could be explained by equifinality problems in the 200 location mdoel outweighing the information gain from the additional modeling locations this pattern is not seen in the larger east branch where the 200 location gmm is the best performing model as noted in section 3 1 the optimal number of modeling locations at least among the models tested in this study represent approximately the same surface area in each subbasin since the east branch is approximately double the area of almanor thus it is possible that for this particular model and resolution of input data these catchments are essentially reaching a saturation point for modeling locations though more tests would need to be run on the east branch in particular to determine if performance drops with more locations the impact of number of modeling locations on model performance while not drastic is enough to support further attention being given to the distribution step i e selection of modeling locations of model set up 4 2 modeling set up a key advantage of the gmm method is its efficiency and repeatability especially when compared to traditional methods of hru delineation gmm requires only rasters of input variables thus combining the data processing advantages of pixel based models while still being based in physical basin characteristics once the rasters are prepared running the gmm algorithm from start to finish including subsampling and saving the outputs required less than half an hour on a high performance computing core as long as a seed is set in the random number generator for the gmm optimization the process is also repeatable while the gmm method does not address questions of scalability of parameters or automatically identify optimal resolutions this efficiency can be leveraged to test multiple spatial resolutions and allow modelers to understand how the resolution influences their results the ease of setup also allows modelers to test different combinations of input variables to their model and understand the drivers of hydrologic processes in their basin this can help inform what variables to use as inputs to the gmm the gmm algorithm requires no specific software and can be implemented through open source products as was done for this study there are no theoretical limits on modeling locations using gmm and all locations necessarily represent equal areas to comply with the multivariate selection process this removes questions of relative hru size and decisions about the maximum range of hru areas finally since the gmm method is separate from calibration it can be applied for any number of calibration designs including different algorithms single or multiobjective functions and semi or fully distributed parameters traditional hru delineation on the other hand is necessarily subjective delineation usually begins by identifying areas with similar topography using a dem but there are few norms or guidelines to selecting the number of hrus to use and by extension their average size other than the resolution of the input data and the computational power available to run the model once initial hrus are delineated smaller hrus are generally merged into neighboring larger ones so sizes fall within a similar range which hrus to merge and where is entirely subjective some common software tools including gis weasel automatically and randomly merge smaller hrus but do not contain the ability to set the seed of their random number generators making this process impossible to replicate viger and leavesley 2007 though the time required for gis based hru delineation is not consistent since it depends on the size and topography of each basins our experience in this study and conversations with modelers and forecasters suggest the process is on the order of days to weeks since the spatial distribution process is so labor intensive the assumptions made during this process cannot be easily tested by creating alternative versions of the model gis based hru delineation also presents theoretical problems including the fact that hydrologic processes are simulated at the geometric center of a supposedly homogeneous hru this means that extreme elevations will never be represented by the model potentially missing areas that are significant contributors to runoff production another issue is that if an hru is not convex its geometric center is not guaranteed to fall within the hru or even within the river basin itself given these issues more attention should be paid to novel methods of spatial distribution in hydrologic models though related to issues of scalability and overparameterization this has not received the same attention in the literature future work should focus on further exploring how gmms interacts with physically based models for example one area that was outside the scope of this study but would be highly useful for modelers is a method for model selection i e for determining a priori the optimal number of modeling locations to couple with gmm tools like information criterion may provide some insights but since these are usually calculated with respect to the statistical algorithm itself they may not capture the effects of calibration and equifinality in the physically based models see section 4 1 in addition as noted in section 2 2 the gmm method is only one of several clustering algorithms that may be appropriate for this so another next step is to explore the use of other related algorithms 4 3 spatially distributed performance optimized gmm modeling locations also lead to more realistic spatial representation of the basin in prms which has implications for model interpretation and distributed performance for example the extent of the elevations represented in the gis model is less than half the true range the gis model covers 1391 2155 m in almanor and 1103 2001 m in the east branch while the range of the dems is 1365 2950 m in almanor and 700 2550 m in the east branch due to this limited elevational range all hydrologic processes in the gis model can occur only up to 2200 m in almanor and between 1100 and 2100 m in the east branch compared to the gis models the best performing gmm models cover a 42 greater range in almanor 1406 2492 m and 38 greater range in the east branch 1016 2263 m though elevation gradients are only one of many types of spatial heterogeneity they are particularly relevant due to the strong orographic influence on precipitation in our study area roe 2005 roe and baker 2006 their greater range means the gmm models are better positioned to capture processes with strong elevational dependence including swe distribution vegetation and timing of runoff generation this finding applies to these study sites in particular but based on the theoretical limits on hru elevations as discussed in section 4 2 we expect the gmm method to give broader elevational representation than the gis method in any other montane catchment the implications of using gmm versus gis for spatial distribution are clear when we examine elevational trends in model performance figs 6 and 7 in fig 6 average daily bias shows how well the models match overall volume at different elevations while the pearson correlation coefficient shows how well temporal patterns are simulated bias here may be driven by two factors 1 errors in data or modeling assumptions or 2 biased elevational distribution of area in the model we see that the et correlation in almanor shows a clear u shaped pattern across all models while the et bias starts moderately negative decreases after 1500 m and rises again until about 2100 m where it becomes consistently positive since these general trends are common across all models it is likely that these errors are related to problems with input data and or to model structural error the better performance of all models at the lowest elevations may be related to vegetation patterns grass and bare lands are more common at low elevations while forests dominate the middle elevations since only forests intercept snow model structural errors in the snow interception and sublimation calculations would lead to errors in calculating et another possibility is that input climate data which also impact evapotranspiration calculations are more accurate at lower elevations where most data collections stations are located in particular the steady increase in r value and reduction in bias magnitude between 1500 and 2100 m suggests biases that are correlated with elevation input temperature data which was calculated using seed stations and lapse rates may show such a bias and would impact swe representation in the model swe in turn interacts with et by influencing rates of sublimation during winter and the timing and amount of water available for transpiration in the spring and summer growing season at the highest elevations above 2300 m the gmm models reveal a significant drop off in correlation values fig 6a notably this drop in correlation performance occurs at about the same elevation above which the et bias of the almanor models becomes consistently positive fig 6b we hypothesize that these patterns are related to et modeling above and below the tree line which since the highest portions of almanor including mount lassen above about 2400 m are largely free of vegetation elevations below the tree line are transpiration dominated while those above the tree line are evaporation dominated thus et calculations in prms appears to underestimate the transpiration component and overestimate the evaporation component in the evaporation dominated higher elevations structural issues may include estimating sublimation from the snowpack or evaporation from soil storage below the tree line bias may be related to underestimation of the depth of the root zone or other problems with subsurface modeling importantly this pattern at high elevations is not captured by the gis model which does not capture any location above the tree line nor is it seen in the east branch where elevations do not exceed 2300 m see supplementary information fig s3 simulation of the distribution of runoff production across elevations also benefits from the broader spatial range in the gmm models fig 7 due to misrepresentation of area per elevation band all models tend to over produce runoff at mid to low elevations 1200 1600 m in the east branch as compared to observed precipitation minus evapotranspiration p et is a first order estimate of runoff production which is not directly observable by elevation band most models show an overestimation at mid elevations that compensates for underestimations at higher elevations particularly elevations not represented at all where by default runoff production is zero in order to match overall runoff volume this error is greatest in the gis model which represents the narrowest range of elevations of any model the 200 component gmm model which performed best with respect to the spatially lumped metrics also shows the overall best match with observed p et and as such would be the best candidate to represent the spatial distribution of runoff production in prms this misrepresentation of contributing area may lead simulated runoff to interact with other water balance components in non physical ways over or under generation of runoff may lead to errors in partitioning infiltration versus runoff potentially impacting et simulation since the majority of et in vegetated areas is transpiration from soil storage and generally receives priority allocation of runoff over streamflow bales et al 2018 moreover misrepresentation of contributing area may lead to particularly poor representation during extreme periods like drought it has been shown that lower elevations of some northern sierra basins may become water limited during droughts even as the basin as a whole is energy limited thus failure to simulate these lower elevations may lead to the models overestimating runoff during droughts on the other hand failure to capture the higher elevations may mean the models miss an important drought mitigation factor bales et al 2018 4 4 univariate models univariate models performance metrics fell within the same order of magnitude as multivariate model metrics for average conditions again reflective of equifinality challenges in hydrologic models but their performance is still measurably worse this drop in performance is unsurprising since the multivariate models capture more the factors that influence the water balance in montane catchments fig 5 thus univariate models would be largely inappropriate for forecasting or process simulation but we propose that they can provide insights into the most important drivers for different hydrologic processes in the basins for example the top performing univariate model in the east branch is elevation based since the east branch sits largely in the rain snow transition zone elevation is a critical factor for determining runoff timing by way of precipitation phase however both the aspect and slope models performed better than the elevation model with regard to overall swe bias suggesting that the rain shadowed nature of the east branch and strong directional precipitation patterns are important factors in influencing accumulation and ablation in addition slope and aspect may influence the timing and shape of the swe ablation curve since they influence the amount of incident solar radiation a site will receive maxwell et al 2019 in almanor the top performing gmm models were based on slope and aspect followed by those based on saturated hydraulic conductivity elevation and vegetation almanor is a higher elevation basin than east branch with more area above the rain snow transition zone thus elevation may be less informative since precipitation phase is more consistent than in the east branch in these high elevation regions snowmelt starts later in the season and is radiation dominated bales et al 2006 so slope and aspect are greater controls on the timing of snow accumulation and melt and by extension runoff in addition baseflow fed by groundwater is a larger component of streamflow in almanor than the east branch so soil characteristics i e saturated hydraulic conductivity may be more relevant for determining streamflow the relatively good performance of the aspect driven models in both catchments may be due to rain shadowing effects direction of slope matters not only for snow ablation due to solar radiation but also for snow accumulation since the wettest parts of both basins are the western facing non rain shadowed portions along the main stem of the north fork the saturated hydraulic conductivity based model performed reasonably well in both subbasins but as expected topographic features were still overall most relevant for runoff generation the vegetation density model gave poor results across both basins indicating that subsurface conditions may be stronger drivers of et variation in the feather river since the majority of the land cover in both subbasins is forest vegetation density may be less informative due to relatively little variation across the landscape the univariate models and their differing performance in each subbasin demonstrate the physical basis of gmm based models showing how some factors exert more control on the hydrologic process than others depending on the subbasin we further show how these results can lead to greater processes understanding in headwater catchments we suggest that univariate gmms could be used in practice to assess the most relevant input features before running a multivariate gmm to distribute a new model this is relevant for both modelers and scientist seeking to improve forecasting performance prioritize data collection and better understand the hydrologic cycle at this stage selecting this initial set of input features to test is left to the expert knowledge of the modelers a more rigorous assessment is outside the scope of this study but options such as the use of an information content criterion should be the subject of future work 5 conclusion we introduce a new method for spatial distribution of hydrologic models using the gaussian mixture models gmm algorithm and demonstrate its use in two geologically distinct headwater catchments of the sierra nevada unlike traditional gis based methods the gmm method is objective repeatable and computationally fast on the order of minutes the method identifies the set of modeling locations that best represent the basin as a whole leveraging an efficient statistical learning tool while being grounded in physical basin properties analysis shows that gmm based models are able to match or exceed the performance of traditional gis based models with respect to both average and extreme conditions for both streamflow and other water balance components furthermore we show that the modeling locations selected using gmm better represent the geometry of the basin and thus more accurately reproduce the spatial distribution of processes such as runoff production thus gmm based models are closer to being right for the right reasons kirchner 2006 finally we show how the method can be adapted to test multiple feature combinations and identify the relative importance of a basin s hydrologic drivers an elevation based gmm model performed best in the study basin that sits primarily on the rain snow transition zone while slope and aspect based models performed best for the higher elevation catchment further research should investigate how different input components climates and topographies influence gmm performance the best model performance in the two headwater catchments where this method was tested was achieved with different numbers of modeling locations 100 in almanor and 200 in the east branch however these numbers are likely basin specific so future work should consider methods for identifying the optimal number of modeling locations the improved spatial representation of gmm based hydrologic models create a more robust decision making and process understanding tool for water supply agencies utility companies and flood control operators especially in topographically heterogeneous basins this can help mitigate risk and reduce costs to downstream users residents and infrastructure in addition to enhancing models directly the efficiency of the gmm method can facilitate improvements by encouraging more regular model upgrades this allows agencies to stay abreast of changes to their basins such as land use or vegetation coverage and advances model structure and data collection technology the resource and time intensive nature of updating hydrologic models mean that many agencies do so infrequently often with more than a decade between upgrades the rapid and repeatable nature of the gmm method would reduce time and labor associated with model updates overall the gmm method provides the basis for objective efficient process based model set up with the same capabilities as traditional semi distributed models leveraging advances in statistical learning it is a powerful and promising new tool for hydrologic modeling declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was partially supported by the california energy commission under contract epc 14 067 t m and f a and the national science foundation graduate research fellowship under grant dge 1106400 t m the authors would like to thank kevin richards joseph rungee and qin ma for their assistance with data collection and cleaning appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105076 
25816,common methods for spatial distribution such as hydrologic response units are subjective time consuming and fail to capture the full range of basin attributes recent advances in statistical learning techniques allow for new approaches to this problem we propose the use of gaussian mixture models gmms for spatial distribution of hydrologic models gmms objectively select the set of modeling locations that best represent the distribution of watershed features relevant to the hydrologic cycle we demonstrate this method in two hydrologically distinct headwater catchments of the sierra nevada and show that it meets or exceeds the performance of traditionally distributed models for multiple metrics across the water balance at a fraction of the time cost finally we use univariate gmms to identify the most important drivers of hydrologic processes in a basin the gmm method allows for more robust objective and repeatable models which are critical for advancing hydrologic research and operational decision making keywords physically based hydrologic models spatial distribution gaussian mixture models statistical learning 1 introduction spatial heterogeneity of hydrologic processes within a watershed is fundamentally impacted by basin topography topographic variations affect vegetation characteristics directly via climatic controls and indirectly via impacts on soil profile and water and nutrient availability fan et al 2020 tian et al 2020 zhang et al 2011 qiu et al 2001 all three topography soil and vegetation combine to impact hydrologic processes including evapotranspiration infiltration runoff and interflow see e g ghestem et al 2011 wilcke et al 2011 obojes et al 2015 young et al 1997 both vegetation and topographic variations such as slope and aspect impact snow accumulation and ablation patterns through controls on short and longwave radiation wind and interception lundquist et al 2013 maxwell et al 2019 varhola et al 2010 in montane regions heterogeneity of the landscape can have profound implications for all portions of the water balance orographic effects can create dramatic differences in precipitation rates on either side of mountain ranges as well as influencing the phase rain versus snow of that precipitation roe 2005 landscape variability in these regions is of particular interest due to the role these river basins play in the waterscape connecting natural headwaters with human needs karpouzoglou and vij 2017 these water towers of the world supply water to over half of the human population partnership 2014 immerzeel et al 2020 viviroli et al 2007a understanding the variations of hydrologic processes that contribute to the timing and quantity of streamflow from these basins is a fundamental goal for both scientific researchers and increasingly operational forecasters in the water management sector these questions have become all the more pressing in regions where climate change is inducing shifts in the water balance not previously seen in order to meet these needs and spurred by increases in computational resources the use of physically based spatially distributed hydrologic models is becoming more common physical models indicate a bottom up approach in which mass and energy balances are resolved with a spatially distributed set up these simulations are performed at multiple points across a river basin and then aggregated models may be partially or semi distributed where some model components e g input data are varied across the landscape but others e g parameters are held constant or fully distributed all components are spatially variable in spatially lumped models on the other hand all hydrologic processes occurring within a basin are simulated at a single point and output is given as a single time series for the basin usually streamflow at the basin outlet much research has focused on calibration approaches for distributed models in an effort to address concerns of overparameterization non identifiability of parameters equifinality and scale consistency beven et al 1988 beven 1989 wood et al 1988 blöschl and sivapalan 1995 andréassian et al 2012 bai et al 2009 pianosi et al 2015 comparisons between lumped semi distributed and fully distributed calibration techniques e g khakbaz et al 2012 reed et al 2004 lobligeois et al 2014 carpenter and georgakakos 2006 have attempted to characterize the relationship between spatial distribution and model performance with mixed results while more modeling points can better capture variations in topography vegetation and climate they also introduce a greater number of free parameters which can contribute to overparameterization issues a high number of free parameters mean these models can often be calibrated to a baseline level of performance for average conditions regardless of the quality of the input data but the resulting parameters may have not be reflective of actual physical conditions as a result model performance will decline for previously unobserved conditions the extent to which distributed or lumped models are most appropriate may depend on the landscape and application of the model lobligeois et al 2014 research on scale consistency focuses on reconciling parameter values across scales in an effort to avoid sudden changes in results when spatial resolution is changed sivapalan and kalma 1995 these approaches may be top down calibrating a lumped model and disaggregating parameter values e g tran et al 2018 or bottom up regionalization e g blöschl and sivapalan 1995 arsenault and brissette 2014 hundecha et al 2016 samaniego et al 2010 this work on reconciling parameter values across scales largely focuses on the calibration step of model set up but less attention has been given to the prior step of selecting which and how many specific locations within the basin to include in the model this critical first step of spatially distributing a model impacts all subsequent set up including input data distribution across the basin and parameter definition and calibration methods proposed in the literature for selecting modeling locations or otherwise partitioning the basin include representative elementary areas reas an intermediate scale at which neither small nor large scale processes dominate wood et al 1988 representative elementary watersheds rews units derived based on the streamflow network and over which equations of mass and energy fluxes are integrated reggiani et al 2000 reggiani and rientjes 2005 and landform classes based on the upness index summerell et al 2005 roberts et al 1997 these methods showed promise in capturing spatial variability but were limited by detailed data or catchment monitoring requirements inability to simulate multiple hydrologic processes rather than runoff alone and or assumptions in the derivation process more recently pixel based distribution approaches have risen in popularity to be compatible with gridded remote sensing products though convenient this approach is disconnected from the physical characteristics of a basin pixels may straddle discontinuities in topography or land use introducing uncertainty into simulations and or runoff routing in addition pixel based approaches typically result in hundreds or even thousands of simulation points for a moderately sized basin see e g tran et al 2018 since model resolution is frequently dictated by input data resolution not only does the high number of modeling locations raise equifinality concerns these models often have higher simulation times and increase computational requirements this can be particularly problematic for time or resource constrained applications such as real time flood forecasting in montane regions elevation bands are sometimes used as a simple alternative to capture spatial variability e g bongio et al 2016 valéry et al 2014 but are also often arbitrarily defined and may not align with topographic features alongside pixel based methods the most widely used approach for spatially representing a basin is hydrologic response units hrus leavesley et al 1983 flügel 1994 1997 defined as areas of a basin that can be considered homogeneous in all respects influencing the water balance e g topography land cover and vegetation density and soil type conceptually simple hrus are favored by some modelers as having a stronger connection to physical basin characteristics than pixel based models hrus are the default distribution method in several major hydrologic models including the precipitation runoff modeling system prms markstrom et al 2015 the soil and water assessment tool swat see e g kalcic et al 2015 teshager et al 2016 qi et al 2017 precipitation runoff evapotranspiration hydrotope model prevah viviroli et al 2007b the sacramento soil accounting model sac sma national oceanic and atmospheric administration 2002 and the regional hydro ecological simulation system rhessys tague and band 2004 in addition hrus are used by many large water management agencies that rely on physical hydrologic models including california s department of water resources dwr and pacific gas electric pg e energy company despite their popularity hru based distribution presents both theoretical and practical problems there is inherent tension between having more smaller hrus that are more likely to conform to the assumption of homogeneity and the need to reduce unnecessary model complexity in addition though hrus are meant to represent a distributed sub area of a basin hydrologic processes are simulated at a particular point usually the geometric centroid of the hru this necessarily limits the points of the basin that can be simulated with hrus for example the geometric centroid will always be lower than a peak or ridge meaning that the model is likely to miss the highest elevations hrus are frequently delineated using a gis based approach starting with a digital elevation model and using topography including drainage divides slope and aspect to partition the study area see e g flügel 1994 1997 koczot et al 2005 though tools such as the arcmap based tool gis weasel viger and leavesley 2007 have been built to assist with this process this method of hru delineation involves significant subjective decision making such as selecting minimum hru size and stream segment resolution all of this can translate to multiple days of hands on work other methods have been proposed for hru delineation including khan et al 2013 and khan et al 2016 who overlaid soil and stream network data on a set of identified landform classes and fiddes and gruber 2012 who used a sub grid sampling method to include the effects of topography in a lumped model while promising these approaches both rely on assumptions that are not generalizable across catchments and or all aspects of the water balance ultimately hru delineation and by extension selection of modeling locations involves subjective decisions and significant time investment given these issues there is need for a simple rapid and objective approach to selecting modeling locations for spatially distributed hydrologic models recent advances in statistical learning algorithms have made possible alternative approaches to this problem such algorithms broadly speaking are used to identify and characterize patterns in data particularly those that are not obvious or that would be too labor intensive to test individually shen 2018 use of statistical learning is increasing in the field of hydrology see e g oroza et al 2018 avanzi et al 2019 schmidt et al 2020 kim et al 2020 but understanding which algorithms and for what applications it is most appropriate is an ongoing area of research shen 2018 kim et al 2020 for example how statistical models compare to and interact with traditional physically based hydrologic models has not been comprehensively tested oyebode and stretch 2019 physical models are usually mechanistic and process based by design since it is frequently important for hydrologists to understand the causes and relationships underlying an observed phenomena statistical learning on the other hand typically identifies correlations and associations between variables without suggesting causality the implications of this discrepancy and whether they matter for using these two model types together is an open research question oyebode and stretch 2019 schmidt et al 2020 here we use statistical learning to return to the question of selecting modeling locations for a physically based model and we propose a method that is grounded in physical properties and does not obscure process understanding given the relatively recent introduction of statistical learning to hydrology there are many possible approaches that have yet to be tested but as a first step we focus on mixture models a type of algorithm that has emerged as a way of optimally identifying a set of underlying components that best describes a population we propose the application of gaussian mixture models gmms as an objective efficient and physically based spatial distribution technique that addresses both the theoretical and practical shortcomings of existing methods for selecting modeling locations in a basin using basin characteristics that influence the water balance mixture models identify a set of modeling locations that optimally characterize the water balance throughout the basin gaussian mixture models have been successfully used to capture spatial patterns in other hydrologic contexts such as snow water equivalent swe distribution at a single site oroza et al 2016 but have not yet been tested in conjunction with a physically based hydrologic model this is also the first time it has been tested at landscape scale across the diverse topography of montane river basins we demonstrate the gmm based distribution method in two contrasting headwater catchments of the sierra nevada using the precipitation runoff modeling system prms a physically based rainfall runoff model commonly used in water management owing to their widespread use by researchers and forecasters we use a gis hru based prms model as a baseline to compare performance of the gmm based models in the research reported here we address the following 1 what is the measurable impact of a gmm based spatial distribution method versus an hru based method on predictive accuracy 2 are these spatial distribution methods robust to unobserved extreme hydrologic events which hydrologic process es drive improvements or declines in modeled performance 3 what attributes are the most important drivers of predictive accuracy in montane catchments 2 methods and data 2 1 study area we focus on almanor and the east branch two headwater catchments of the north fork of the feather river the northernmost basin of the california sierra nevada see fig 1 the feather river is important for water resources and energy production pacific gas electric pg e california s largest utility company operates a series of hydropower plants on the north fork totaling 740 mw of installed capacity about 19 of company s overall hydropower portfolio the basin also drains to lake oroville the primary storage reservoir for the state water project operated by the california dwr and serving drinking water and agricultural water needs in the central and southern parts of the state as a lower elevation sierra nevada basin peak elevation 2950 m the feather is susceptible to climate change effects as more precipitation falls as rain rather than snow the feather river can therefore be thought of as an early example of how other basins in the sierra nevada may change with rising temperatures freeman 2011 the main stem of the north fork of the feather originates in the almanor catchment to the northwest and is regulated at the outlet of lake almanor almanor drains an area of approximately 1150 km2 and contains mount lassen the highest and wettest point in the feather river at about 2900 m elevation and 3000 mm of annual precipitation koczot et al 2005 geologically almanor is part of the cascade mountain range rather than the sierra nevada making it distinct from the rest of the basin the subsurface is largely comprised of more permeable volcanic rocks and baseflow makes up a higher percentage of flow than in other subbasins freeman 2008 the east branch is a tributary of the north fork and drains an area of approximately 2650 km2 it meets the north fork south of lake almanor the east branch is rain shadowed due to the eastern ridge of the upper north fork canyon on its western edge and is thus considerably drier than almanor with an average annual precipitation of about 300 mm the subbasin has a largely granitic subsurface and low baseflow freeman 2008 it is also mostly unregulated 2 2 gaussian mixture models for spatial distribution gaussian mixture models are a statistical learning algorithm used to identify a subset of discrete points that best represent a feature space here feature is a measurable characteristic that describes a phenomenon being observed bishop 2006 for example in describing runoff from a basin a feature may be the basin s elevational distribution a feature space is the single if there is only one feature or multidimensional if there is more than one range collectively defined by the feature data for example in fig 2 there are two features elevation and slope creating a two dimensional feature space all data points fall shown as dots somewhere in the feature space features must be continuous numeric variables for use in a standard gmm but otherwise may be defined at the discretion of the modeler the gmm algorithm selects the optimal points across the feature space by assuming that the feature space can be represented by superimposing a finite number m of latent components which are normally distributed fig 2a shows three latent components each of which can be uniquely described by a mean expected value μ and covariance σ each is also assigned a mixing parameter π based on the prior probability of observing that component essentially a weighting factor thus the parameters that are defined in fitting a gmm to a particular dataset are the means covariances and mixing parameters for each latent component we take the expected values of the latent components as the set of points that optimally describes the feature space in concrete terms and relating the example schematic in fig 2a the three μ values shown as red x s are the points that best represent the distribution of elevation and slope in this hypothetical basin however a point that exists in feature space say for example an elevation of 2300 m and a slope of 85 may not exist physically in the basin thus once the means have been identified we use a nearest neighbors approach to find the physical location that is closest to the means of feature space fig 2b these locations define the spatial distribution of the gmm based prms models henceforth modeling locations and are analogous to the hru centroids that define the spatial distribution of traditional models fig 2c maps of the actual selected modeling locations for each subbasin are available in the supplementary information figs s1 and s2 formally the ability of a gmm latent component to represent the feature space is modeled as a multivariate normal distribution n with expected value μ and covariance σ applied to a d dimensional vector of empirical data x equation 1 1 n x μ σ 1 2 π d 2 1 σ 1 2 exp 1 2 x μ t σ 1 x μ the collective ability of the m components to reproduce the feature space is calculated by superimposing each n m weighted with its mixing parameter π m the number of features i e the length of x determines the dimension of each n m distribution the expected values covariance and mixing parameters that best represent the data are identified by maximizing the likelihood function of the superimposed multivariate normal distributions given by equation 2 in other words the gmm maximizes the following objective function 2 ln p x n π μ σ n 1 n ln m 1 m π m n x n μ m σ m subject to 3 m 1 m π m 1 in this study the number of dimensions was five and the features were basin elevation slope aspect vegetation coverage and soil hydraulic conductivity k s a t together these features capture the major drivers of the water balance endogenous to the basin i e not driven by climate or weather inputs including spatial distribution of the snowpack an important if not dominant component of the hydrologic cycle in the feather river evapotranspiration and infiltration characteristics elevation slope and aspect were defined using the usgs usgs national elevation dataset ned eros data center 1999 and vegetation data were obtained from the 2013 u s forest service landfire dataset landfire 2013b a respectively see supplementary information section s1 2 for full details on the landfire dataset topographic and vegetation data rasters were both at 30 m resolution these rasters were masked to the extent of the subbasins in the study and filtered to remove pixels with undefined values for example flat areas with undefined aspect and passed through a 1 in 2 resampling algorithm to make processing computationally feasible pandas dataframe sample the pandas development team 2019 subsampling was performed with a uniform distribution without replacement and with an initial seed for reproducibility soil hydraulic conductivity k s a t was derived from stago2 data soil survey staff natural resources conservation service 2019 which are available as shapefiles indicating the extent of different geologic groups i e clusters of one or more soil types each of which is associated with a set of unique soil properties properties were first depth integrated then spatially averaged using the percent of each soil type in a geologic group this averaged property i e k s a t was assumed to be spatially homogeneous across the geologic group the distribution of the averaged k s a t was used as the gmm input feature while this is relatively simple as a descriptor of soil type it demonstrates how soil properties may be incorporated into gmm modeling future work particularly in basins with large contributions of groundwater to streamflow could include more detailed assessment with multiple soil characteristics as gmm inputs using the five rasters as inputs the gmm algorithm was run using scikit learn s mixture gaussianmixture class pedregosa et al 2011a covariance parameters were trained using the spherical setting meaning that a single covariance value was calculated for each component this was selected to in order to simplify the analysis and to decrease the runtime of the algorithm but future work should explore the implications of other covariance options since many of the landscape features that control movement of water co vary but sometimes to differing degrees across the landscape beven et al 1988 it is possible that applying the different covariance parameters for each component would improve model performance the mixture gaussianmixture class uses the expectation maximization algorithm an iterative gradient descent method to optimize of equation 2 and identify the most likely mixing parameters covariance and means to explain the data mclachlan and peel 2004 pedregosa et al 2011b the optimization terminates when a maximization step no longer increases the log likelihood as noted the optimal expected values means of the latent components in feature space were translated to physical modeling locations using a nearest neighbors algorithm features were scaled with equal weight to prevent features with higher magnitude values from dominating the nearest neighbor search in addition to these five dimensional multivariate gmms we ran five additional gmms in each basin each driven by only one of the features univariate gmms this was done to assess the usefulness of each individual gmm input feature and we show that this can inform feature selection and their relevance to different hydrologic processes gmms fall into a general category of statistical learning models called clustering algorithms which aim to find areas of relative homogeneity of data in feature space while any number of clustering algorithms could also be effective for spatial distribution of modeling locations we used gmms for this analysis because every latent component in the gmm is affected by every data point this is not necessarily true of other clustering algorithms such as k means where means are calculated based only on the points assigned to a given cluster the former thus had a better potential to represent all points across the feature space 2 3 precipitation runoff modeling system the precipitation runoff modeling system prms is a distributed parameter hydrologic model developed by the u s geological survey markstrom et al 2015 the model runs on the daily time step taking as inputs daily precipitation and minimum and maximum temperature which are distributed to each hru either ahead of time by the modeler or through an interpolation scheme within the model prms simulates mass and energy balances beginning with calculation of solar radiation and precipitation phase partitioning and ending with computation of total streamflow intermediate processes include snow accumulation and ablation canopy interception and evapotranspiration infiltration surface runoff interflow and groundwater recharge a description of the major processes included in prms and their calculations can be found in the supplementary information section s1 5 prms is executed in a linear fashion at each time step in the simulation with each hydrologic process represented by a module of code for some processes users may specify a desired calculation method by selecting from multiple possible modules the spatial distribution in prms is achieved by partitioning the modeling area into hrus represented by a specific geographical point in the basin by default the geographic centroid koczot et al 2005 markstrom et al 2015 to which input data are distributed and at which the water balance is simulated the water balance is simulated separately at each hru and scaled according to the surface area of the hru outflow is aggregated across the basin based on the selected streamflow routing method prms is currently used throughout the california sierra nevada for streamflow modeling by pg e richards 2018 it is also being actively developed for new river basins by dwr see e g burley and fabbiani leon 2018 its widespread use for water resources planning as well as its commonalities with other distributed parameter models makes it ideal for this study the model is publicly available at https www usgs gov software precipitation runoff modeling system prms the latest release is version 5 but at the time of this research version 4 0 3 was the most updated available major changes between the two versions do not affect the modules used in this study 2 3 1 prms models used in this study required input data for prms are daily temperature range and precipitation amount which are spatially distributed based on a user selected method here both temperature and precipitation were pre distributed to each hru or modeling location before executing prms precipitation was distributed using an algorithm called draper in which spatially distributed long term average monthly precipitation surfaces from the parameter elevation regressions on independent slopes model prism dataset are adjusted using daily ground based values daly et al 2008 each day the prism surface is multiplied by the ground based measurements as a percent of long term daily average this effectively tilts the prism surface to reflect the daily observations for more details on the draper algorithm see donovan and koczot 2019 for a full discussion of the implementation of draper in this study see supplementary information section s1 4 daily minimum and maximum temperature were also distributed externally to prms this procedure was selected based on an analysis of temperature variability across the feather river basin that showed both temporal sub monthly and spatial intra subbasin variability in lapse rates avanzi et al 2020 internal prms distribution methods did not permit this variability to be accounted for instead temperature was distributed as a two step process in which we regressed elevation against minimum and maximum temperature for several training stations in each subbasin to establish daily basin wide lapse rates residuals between temperature predicted using these lapse rates and observed temperature at a set of evaluation stations were distributed using multilinear regression with elevation and the temperature at a designated seed station the final values of maximum and minimum temperature were obtained by subtracting the residual from the first guess temperature obtained using the lapse rates computed in the first step for more details on this process see supplementary information section s1 4 in prms some process calculations are pre determined while for others the user may select from a variety of options in this study solar radiation was calculated using a degree day approach which uses daily maximum air temperature to obtain actual daily solar radiation ddsolrad hru module evapotranspiration et was calculated using the jensen haise formulation potet jh prms also requires values for several dozen parameters that may be spatially and temporally global or may be set on a per month or per hru basis values based on topography and canopy cover were computed based on the usgs ned eros data center 1999 and the us forest service landfire dataset landfire 2013b a respectively remaining non calibration parameters were computed based on available data set to default values or retained from the original usgs version of prms on the feather river details of this process for both the gis and gmm versions can be found in the supplementary information section s1 in this study we use a traditionally designed prms model as a benchmark for model performance this model is henceforth referred to as the gis model in reference to the geographic information system framework typically used to delineate hrus the gis model was based on the feather river prms model version 2 designed by the usgs in the early 2000s in which hrus were delineated using standard methods see koczot et al 2005 for details we updated the model to prms version 4 0 3 as part of this process we made minor manual updates in the hru boundaries to better reflect drainage divides in addition prms version 4 allows for greater functionality in terms of streamflow routing which was introduced in lieu of straight summation of hru outflows at each time step as was done in version 2 details of the new model design including how non calibration parameter values were selected are given in the supplementary information section s1 other than the process for selecting modeling locations the prms set up for the gmm models was largely the same as for the gis model topographic and vegetation parameters were based on the values at the selected modeling locations unlike hrus which have each represent a different surface area each modeling location was assigned an equal portion of the headwater catchment based on the gmm feature rasters this decision was made in order to test the baseline effectiveness of the gmm but future work could test the effectiveness of scaling based on the surface areas that are represented by relatively homogeneous areas of feature space the gmm models did not employ a streamflow routing method since modeling locations are not coupled with drainage areas instead runoff from each modeling location was summed for each time step for details on the model design see supplementary information section s1 in order to explore the first two research questions assessing the accuracy of the distribution methods and robustness to unobserved events we trained four models for each subbasin with varying numbers of target modeling locations one lumped case for comparison 50 100 and 200 this process allowed us to identify how the number of modeling locations changes the performance of the model in order to address the third research question identifying important drivers of predictive accuracy we trained univariate gmm models i e each using only one of the five variables from the multivariate versions this allowed identification of the most important driving variables for each subbasin for this step models were run for the best performing number of modeling locations from the multivariate gmms 2 4 calibration and evaluation strategy we used a multi step multi objective method to calibrate the models in order to avoid the overfitting that is common when calibrating on streamflow alone hay et al 2006 gupta et al 1998 in order to isolate questions of basin partitioning from issues of distributed parameter calibration all calibration targets were basinwide in other words we did not calibrate to internal basin gauges or other internal targets based on the availability of data 20 year period of record and the dominant hydrologic processes in the feather river we chose to calibrate on swe et and basin outflow five calibration steps were used each with a specific target variable objective function and set of calibration parameters the order of the steps reflects the modeling order of hydrologic processes in prms the objective functions and target variables for each step are as follows daily rmse of swe cumulative annual bias of et daily weighted sum of kling gupta efficiency kge kling et al 2012 gupta et al 2009 and log nash sutcliffe efficiency lognse nash and sutcliffe 1970 of full natural flow fnf monthly weighted sum of kge and lognse of fnf and cumulative annual bias of fnf for any objective functions using the kge metric all rows with missing observations were removed before the calculation daily fnf values were weighted between two metrics in order to capture the performance for both high and low flows the objective functions for each step are listed in table 1 daily swe maps published by margulis et al 2016 were masked to the almanor and east branch subbasins and then aggregated to catchment wide values annual distributed evapotranspiration data were calculated following roche et al 2020 on a 30 m basis and were aggregated to catchment wide values finally daily runoff values for the outlet of each subbasin were computed from fnf values provided by pg e fnf is a reconstructed time series of runoff that would have occurred in the absence of diversions or other human activity uncertainties in sensor readings can result in negative fnf values so the values for the period of record were smoothed using a five day moving average window any fnf values that were still negative after the smoothing were masked to nan prms like many large scale hydrologic models has hundreds of parameters available for calibration some of which may be individually calibrated for different months of the year or on a per hru basis we selected calibration parameters on the basis of previous sensitivity analyses of prms markstrom et al 2016 and the availability of informative target data on which to calibrate them avanzi et al 2020 while some parameters are calibrated separately for different months all are spatially lumped the parameters calibrated at each step are presented in table 1 phase partitioning parameters tmax allsnow tmax allrain and adjmix rain which determine the percentages of precipitation that falls as rain and snow were calibrated to basin wide swe in addition we calibrated freeh2o cap the free water holding capacity of the snowpack subsurface parameters that are related to how much water is directed to the capillary soil layer were calibrated to et the capillary layer is the only soil layer from which et can occur so these parameters govern the largest source of et in the basin transpiration by plants though this neglects parameters related to evaporation from intercepted storage or sublimation from snow these contributors to et are much lower than transpiration by plants from soil storage finally other subsurface parameters governing overland flow interflow and groundwater flow were calibrated to streamflow at various time steps daily monthly and annual reflecting the timescales over which we expect these processes to occur for details on the use and physical meaning of these parameters see markstrom et al 2015 based on availability of input and calibration data the total calibration period included water years 1998 2016 instead of a traditional split sample approach using continuous calibration and validation periods a stochastic multi split process was employed to avoid biases that might arise from arbitrarily selecting a calibration period eleven of the nineteen water years in the period of record were randomly selected for calibration a process which was repeated to give five 70 30 calibration validation splits see table s2 in the supplementary information for specific years used for each split all calibrations were performed using the shuffled complex evolution sce algorithm a well established method that was specifically developed for large hydrologic models with many degrees of freedom duan et al 1993 1994 this algorithm was designed to handle arbitrary objective functions differences in parameter sensitivities and the presence of non optimal local minima in the feasible space in brief sce works as follows randomly selected sample points are partitioned into complexes which each evolve separately allowing the parameter space to be explored more efficiently periodically complexes are combined shuffled and re partitioned into new complexes this step allows for sharing of the information gained separately by each complex as it evolved the algorithm stops when one of several possible specified convergence criteria is met see lines marked with asterisks in table s1 in the supplementary information this process is performed sequentially for each of the calibration steps as listed in table 1 one iteration through all calibration steps is a calibration round users may set multiple calibration rounds here we used five thus calibration is both sequential and iterative while not guaranteed to reach a pareto front or other global optimum this accounts for trade offs between the objective functions and prevents overcalibration to any single objective function the various metaparameters of the sce algorithm including number of complexes number of calibration rounds and convergence criteria may be individually set for each step and were selected based on a combination of suggested default values hay and umemoto 2006 and trial and error table s1 in the supplementary information lists the metaparameters used for each step and a short explanation of how they were chosen more details of the sce algorithm can be found in duan et al 1992 1993 1994 each gis or gmm model was calibrated five times according to each of the splits and each calibration was assessed separately across its validation period the metrics used in model performance assessment were daily absolute bias of swe cumulative annual absolute bias of et and daily kge lognse and root mean squared error rmse of streamflow rmse gives more weight to accurately reproducing peaks in the time series while lognse gives more weight to baseflow periods it should be noted that swe bias was calculated across all pixels including those without snow to be consistent with the prms model s calculation of basin wide metrics the performance of the models relative to observed values may therefore appear artificially good but values are appropriate for comparison across models metrics and scores values are shown in figures in the main text as absolute values signed scores are reported in the supplementary information tables s3 and s4 to allow for comparison across metrics with different units all twenty five calibrations five for each of the five models were ranked for each metric then the average rank across all five metrics for each calibration was calculated and the median average rank was the score of that model unless otherwise noted all performance values reported are for the calibration with the median average rank other performance metrics focusing on conditions that are of particular interest to forecasters and modelers such as peak swe and flood periods are also presented but were not used in scoring model performance these include the relative error of high flows rehf silvestro et al 2018 to assess reproduction of peak flows peak swe error date of peak swe and baseflow error peak flows were identified as the top 5 of flows across the period of record and days of peak flow were the days these occurred only the peaks occurring within a calibration s validation period were used to calculate rehf all other metrics were calculated separately for each validation year and averaged 3 results this section presents results on gis and gmm model performance section 3 1 gives results under average conditions i e metrics computed across validation periods which were used to rank overall performance of the models it also discusses computational resources required to run the gmm algorithm next we present further analysis aiming to verify gmm performance during periods of extreme conditions section 3 2 finally we present the results of the univariate gmm models demonstrating how these can inform feature selection for different parts of the water balance section 3 3 3 1 gmm model set up and performance in both subbasins the gmm model performance is comparable to and in some cases better than the gis models with respect to all metrics calculated for the validation periods fig 3 the median average rank fig 3a shows that the gis model is best performing lowest ranked in almanor and the 200 location gmm model is best in the east branch the best performing gmm model in almanor is the 100 location one within a subbasin there is little variability in performance across models including both gis and gmm models swe and et bias in both catchments for example vary by less than 20 mm runoff rmse is variable between the two catchments reflecting the difference is average flow but models for a given catchment show similar performance however no single model consistently performs best for all components of the water balance performance rankings are more consistent in the east branch than almanor particularly with respect to streamflow metrics here the 200 location gmm generally performs best with the exception of swe in both subbasins lumped gmm model which represents a baseline from which to assess the improvement provided due to any type of spatial distribution ranks somewhere in the middle with respect to swe and et performance but consistently performs poorly with respect to streamflow in the east branch overall ranking consistently improves with higher numbers of gmm modeling locations but in almanor this is only true up to 100 modeling locations thus the best performing gmm uses half the number of modeling locations in almanor than the east branch fig 3a possibly reflecting the difference in catchment area the area of the east branch is approximately double that of almanor the optimal number of modeling locations may also be influenced by the input data raster resolutions further discussion of the implications of this result can be found in section 4 1 the computational times required to run the gmm algorithm were between 80 and 1500 s on a single core of a high performance computing cluster 3 9 ghz fig 4 this time includes all steps from raster sub sampling scaling fitting the gmm model and the nearest neighbor search for the physical modeling locations times varied by subbasin east branch models took longer to run than almanor due to the larger raster size and number of components 3 2 extreme and peak periods as with average condition metrics the gmm based approach yields comparable if not better performance than the gis method during extreme periods fig 3b in particular the gmm models that perform best in each subbasin under average conditions 100 location in almanor and 200 location in the east branch fig 3a also match or exceed gis performance during extreme periods we note that the lumped model performs worse in both subasins with regard to extreme periods than the average conditions which is further discussed in section 4 1 peak swe marks the transition from accumulation to ablation season in the sierra both timing and magnitude of peak swe are important seasonal benchmarks in snow dominated basins on average peak swe was better simulated in almanor but day of peak swe was better simulated in east branch peak swe tended to be overestimated in both basins see table s4 in the supplementary information day of peak swe was estimated later than observed in the east branch there was no consistent pattern in almanor july september flow was used to capture baseflow performance and rehf was used to assess peak flows like the swe metrics performance was better for both in almanor for baseflow in particular observed flows are lower on average in the east branch making the performance in almanor even more comparatively strong peak flows were generally challenging to capture in both subbasins with minimum rehf of 0 3 100 location gmm in almanor though there was little consistency in model rankings between baseflow and peak flows the best performing gmm models were again able to meet or exceed gis performance 3 3 univariate gmms univariate gmm performance was generally worse than multivariate models figs 3a and 5 though metrics fell within the same order of magnitude reflective of a baseline level of performance that can usually be achieved for average conditions by calibrating models with a high number of free parameters in the east branch the elevation driven gmm performed best as based on median average rank followed by in order the models driven by slope aspect saturated hydraulic conductivity and vegetation with median rank of slope driven model only slightly larger than elevation based model in almanor models in order of performance from best to worst were based on slope aspect saturated hydraulic conductivity elevation and vegetation slope performed particularly well with respect to runoff rmse and kge while aspect performed better with respect to swe bias and runoff lognse possible reasons for the differences between the two subbasins are discussed in section 4 4 4 discussion the gmm method has a number of significant logistical and theoretical advantages compared to gis models but its usefulness is still contingent on its ability to replicate the performance standards of traditional models in the following sections we first discuss both the performance of the gmm models and under what conditions they are able to meet or exceed the performance of traditional models section 4 1 next we describe the logistical advantages of gmm and the implications for users of hydrologic models section 4 2 in the final two sections of the discussion we look more specifically at the spatially distributed performance of the gmm models section 4 3 and the performance and use of univariate gmm models section 4 4 4 1 model performance the gmm method provides a sound objective basis for spatially distributing hydrologic models gmm based models match the performance of the gis models under both average and extreme conditions fig 3 they accurately simulate water balance components over time and provide more accurate spatial distribution of streamflow generation than do gis models we focus this discussion on the overall performance of the gmm models as indicated by the model score since in most cases users need and expect models to perform well across all components of the water balance in order to ensure that physical processes are being accurately simulated however there may be cases in which modelers would have more specific needs for which metrics related to a certain component like swe or et would be more informative the best performing gmm model based on model score exceeds the performance of the gis model in the east branch but not in almanor fig 3a thus the variables selected for gmm prediction may be more relevant for water balance partitioning in the east branch than almanor due to the particular hydrology of the catchments almanor is more subsurface dominated than the east branch and saturated hydraulic conductivity the only predictor used here related subsurface conditions is a relatively limited characterization of soil and groundwater flow thus the variables used in this study were likely a more complete characterization of hydrologic processes in the east branch another factor may be the relative importance of the gmm variables the most informative variables in almanor based on the performance of the univariate models were aspect and slope fig 5 the gis and gmm distributions of these factors were similar and largely consistent with the raster values see supplementary information fig s4 the most informative variable in the east branch was elevation here the gmm models span a greater range than the gis distributions which may have contributed to improved performance in addition the overall lower elevation of the east branch and the fact that its highest elevations are rain shadowed means that a greater proportion of the precipitation in east branch falls in the rain snow transition than in almanor 40 versus 33 where the rain snow transition is defined between 1300 and 2200 m cui et al 2020 this means that uncertainties in modeling precipitation phase well reported in the literature e g harpold et al 2017 jennings and molotch 2019 feiccabrino et al 2015 will affect the east branch more than almanor thus the east branch may be more sensitive to tuning the elevational distribution of modeling locations than almanor in other words there may be more potential for the gmm approach to improve results in addition to capturing temporally averaged metrics the gmm models demonstrated the ability to accurately reproduce periods of extreme or peak conditions good performance during extreme flow periods is particularly important for applications like flood forecasting but increasingly necessary for all streamflow modeling as climate change increases year to year variability and induces more severe weather events day of peak swe for example has traditionally been estimated as april 1st montoya et al 2014 this estimate has always been uncertain due to seasonal weather characteristics and elevation effects but is becoming increasingly inexact due to climate change induced shifts in precipitation margulis et al 2016 thus it is valuable for forecasters to be able to model the date of peak swe rather than relying on the april 1 estimate the ability to reproduce baseflows and correspondingly low flow periods is also of greater concern as length and severity of dry periods in arid regions are projected to increase williams et al 2020 woodhouse et al 2010 cayan et al 2010 in each of these cases the best performing gmm model is able to meet or exceed the performance of the gis model furthermore there was consistency between gmm models that performed well under average conditions and those that performed well under peak conditions meaning that forecasters would not need to rely on a separate model for extreme periods since models calibrated to average periods do not always work well under extreme conditions see e g vaze et al 2010 the consistent performance of the gmm based models is a significant advantage finally the relative performance of the gmm models varied between almanor and the east branch in both subbasins the lumped gmm model had the worst overall performance particularly with regards to streamflow but the best performing gmm models differed between the two catchments the uniformly poor performance of the lumped models reflects the added value of spatially distributed models in diverse topography where a single location is not sufficient to capture the variations across the landscape that impact the hydrologic process since lumped model performance is especially poor for streamflow metrics the variations the lumped models fail to capture may be related in particular to uncertainties in the simulation of subsurface processes e g model structure and lack of data moreover the lumped models perform even worse with respect to other models during extreme periods which may be reflective of the inability of these models to capture variations across the landscape that are especially important for extreme periods for example rapid snowmelt from high elevations that contributes to flooding for forecasters in particular accurate modeling of streamflow and the ability to capture both flood and drought conditions is imperative to optimize dam operations and to protect infrastructure and communities downstream our results are consistent with other studies e g lobligeois et al 2014 that have shown that spatially distributing hydrologic models can yield significant improvements over lumped models in basins with heterogeneities climatic inputs this an argument in favor of performing the additional steps required to run the gmm algorithm and so obtain a spatially distributed model for montane regions unlike the lumped models the spatially distributed models do not show this uniform drop in performance but there is evidence that adding more modeling locations may only be helpful to a point in almanor the 100 location gmm model outperforms the 200 location gmm which could be explained by equifinality problems in the 200 location mdoel outweighing the information gain from the additional modeling locations this pattern is not seen in the larger east branch where the 200 location gmm is the best performing model as noted in section 3 1 the optimal number of modeling locations at least among the models tested in this study represent approximately the same surface area in each subbasin since the east branch is approximately double the area of almanor thus it is possible that for this particular model and resolution of input data these catchments are essentially reaching a saturation point for modeling locations though more tests would need to be run on the east branch in particular to determine if performance drops with more locations the impact of number of modeling locations on model performance while not drastic is enough to support further attention being given to the distribution step i e selection of modeling locations of model set up 4 2 modeling set up a key advantage of the gmm method is its efficiency and repeatability especially when compared to traditional methods of hru delineation gmm requires only rasters of input variables thus combining the data processing advantages of pixel based models while still being based in physical basin characteristics once the rasters are prepared running the gmm algorithm from start to finish including subsampling and saving the outputs required less than half an hour on a high performance computing core as long as a seed is set in the random number generator for the gmm optimization the process is also repeatable while the gmm method does not address questions of scalability of parameters or automatically identify optimal resolutions this efficiency can be leveraged to test multiple spatial resolutions and allow modelers to understand how the resolution influences their results the ease of setup also allows modelers to test different combinations of input variables to their model and understand the drivers of hydrologic processes in their basin this can help inform what variables to use as inputs to the gmm the gmm algorithm requires no specific software and can be implemented through open source products as was done for this study there are no theoretical limits on modeling locations using gmm and all locations necessarily represent equal areas to comply with the multivariate selection process this removes questions of relative hru size and decisions about the maximum range of hru areas finally since the gmm method is separate from calibration it can be applied for any number of calibration designs including different algorithms single or multiobjective functions and semi or fully distributed parameters traditional hru delineation on the other hand is necessarily subjective delineation usually begins by identifying areas with similar topography using a dem but there are few norms or guidelines to selecting the number of hrus to use and by extension their average size other than the resolution of the input data and the computational power available to run the model once initial hrus are delineated smaller hrus are generally merged into neighboring larger ones so sizes fall within a similar range which hrus to merge and where is entirely subjective some common software tools including gis weasel automatically and randomly merge smaller hrus but do not contain the ability to set the seed of their random number generators making this process impossible to replicate viger and leavesley 2007 though the time required for gis based hru delineation is not consistent since it depends on the size and topography of each basins our experience in this study and conversations with modelers and forecasters suggest the process is on the order of days to weeks since the spatial distribution process is so labor intensive the assumptions made during this process cannot be easily tested by creating alternative versions of the model gis based hru delineation also presents theoretical problems including the fact that hydrologic processes are simulated at the geometric center of a supposedly homogeneous hru this means that extreme elevations will never be represented by the model potentially missing areas that are significant contributors to runoff production another issue is that if an hru is not convex its geometric center is not guaranteed to fall within the hru or even within the river basin itself given these issues more attention should be paid to novel methods of spatial distribution in hydrologic models though related to issues of scalability and overparameterization this has not received the same attention in the literature future work should focus on further exploring how gmms interacts with physically based models for example one area that was outside the scope of this study but would be highly useful for modelers is a method for model selection i e for determining a priori the optimal number of modeling locations to couple with gmm tools like information criterion may provide some insights but since these are usually calculated with respect to the statistical algorithm itself they may not capture the effects of calibration and equifinality in the physically based models see section 4 1 in addition as noted in section 2 2 the gmm method is only one of several clustering algorithms that may be appropriate for this so another next step is to explore the use of other related algorithms 4 3 spatially distributed performance optimized gmm modeling locations also lead to more realistic spatial representation of the basin in prms which has implications for model interpretation and distributed performance for example the extent of the elevations represented in the gis model is less than half the true range the gis model covers 1391 2155 m in almanor and 1103 2001 m in the east branch while the range of the dems is 1365 2950 m in almanor and 700 2550 m in the east branch due to this limited elevational range all hydrologic processes in the gis model can occur only up to 2200 m in almanor and between 1100 and 2100 m in the east branch compared to the gis models the best performing gmm models cover a 42 greater range in almanor 1406 2492 m and 38 greater range in the east branch 1016 2263 m though elevation gradients are only one of many types of spatial heterogeneity they are particularly relevant due to the strong orographic influence on precipitation in our study area roe 2005 roe and baker 2006 their greater range means the gmm models are better positioned to capture processes with strong elevational dependence including swe distribution vegetation and timing of runoff generation this finding applies to these study sites in particular but based on the theoretical limits on hru elevations as discussed in section 4 2 we expect the gmm method to give broader elevational representation than the gis method in any other montane catchment the implications of using gmm versus gis for spatial distribution are clear when we examine elevational trends in model performance figs 6 and 7 in fig 6 average daily bias shows how well the models match overall volume at different elevations while the pearson correlation coefficient shows how well temporal patterns are simulated bias here may be driven by two factors 1 errors in data or modeling assumptions or 2 biased elevational distribution of area in the model we see that the et correlation in almanor shows a clear u shaped pattern across all models while the et bias starts moderately negative decreases after 1500 m and rises again until about 2100 m where it becomes consistently positive since these general trends are common across all models it is likely that these errors are related to problems with input data and or to model structural error the better performance of all models at the lowest elevations may be related to vegetation patterns grass and bare lands are more common at low elevations while forests dominate the middle elevations since only forests intercept snow model structural errors in the snow interception and sublimation calculations would lead to errors in calculating et another possibility is that input climate data which also impact evapotranspiration calculations are more accurate at lower elevations where most data collections stations are located in particular the steady increase in r value and reduction in bias magnitude between 1500 and 2100 m suggests biases that are correlated with elevation input temperature data which was calculated using seed stations and lapse rates may show such a bias and would impact swe representation in the model swe in turn interacts with et by influencing rates of sublimation during winter and the timing and amount of water available for transpiration in the spring and summer growing season at the highest elevations above 2300 m the gmm models reveal a significant drop off in correlation values fig 6a notably this drop in correlation performance occurs at about the same elevation above which the et bias of the almanor models becomes consistently positive fig 6b we hypothesize that these patterns are related to et modeling above and below the tree line which since the highest portions of almanor including mount lassen above about 2400 m are largely free of vegetation elevations below the tree line are transpiration dominated while those above the tree line are evaporation dominated thus et calculations in prms appears to underestimate the transpiration component and overestimate the evaporation component in the evaporation dominated higher elevations structural issues may include estimating sublimation from the snowpack or evaporation from soil storage below the tree line bias may be related to underestimation of the depth of the root zone or other problems with subsurface modeling importantly this pattern at high elevations is not captured by the gis model which does not capture any location above the tree line nor is it seen in the east branch where elevations do not exceed 2300 m see supplementary information fig s3 simulation of the distribution of runoff production across elevations also benefits from the broader spatial range in the gmm models fig 7 due to misrepresentation of area per elevation band all models tend to over produce runoff at mid to low elevations 1200 1600 m in the east branch as compared to observed precipitation minus evapotranspiration p et is a first order estimate of runoff production which is not directly observable by elevation band most models show an overestimation at mid elevations that compensates for underestimations at higher elevations particularly elevations not represented at all where by default runoff production is zero in order to match overall runoff volume this error is greatest in the gis model which represents the narrowest range of elevations of any model the 200 component gmm model which performed best with respect to the spatially lumped metrics also shows the overall best match with observed p et and as such would be the best candidate to represent the spatial distribution of runoff production in prms this misrepresentation of contributing area may lead simulated runoff to interact with other water balance components in non physical ways over or under generation of runoff may lead to errors in partitioning infiltration versus runoff potentially impacting et simulation since the majority of et in vegetated areas is transpiration from soil storage and generally receives priority allocation of runoff over streamflow bales et al 2018 moreover misrepresentation of contributing area may lead to particularly poor representation during extreme periods like drought it has been shown that lower elevations of some northern sierra basins may become water limited during droughts even as the basin as a whole is energy limited thus failure to simulate these lower elevations may lead to the models overestimating runoff during droughts on the other hand failure to capture the higher elevations may mean the models miss an important drought mitigation factor bales et al 2018 4 4 univariate models univariate models performance metrics fell within the same order of magnitude as multivariate model metrics for average conditions again reflective of equifinality challenges in hydrologic models but their performance is still measurably worse this drop in performance is unsurprising since the multivariate models capture more the factors that influence the water balance in montane catchments fig 5 thus univariate models would be largely inappropriate for forecasting or process simulation but we propose that they can provide insights into the most important drivers for different hydrologic processes in the basins for example the top performing univariate model in the east branch is elevation based since the east branch sits largely in the rain snow transition zone elevation is a critical factor for determining runoff timing by way of precipitation phase however both the aspect and slope models performed better than the elevation model with regard to overall swe bias suggesting that the rain shadowed nature of the east branch and strong directional precipitation patterns are important factors in influencing accumulation and ablation in addition slope and aspect may influence the timing and shape of the swe ablation curve since they influence the amount of incident solar radiation a site will receive maxwell et al 2019 in almanor the top performing gmm models were based on slope and aspect followed by those based on saturated hydraulic conductivity elevation and vegetation almanor is a higher elevation basin than east branch with more area above the rain snow transition zone thus elevation may be less informative since precipitation phase is more consistent than in the east branch in these high elevation regions snowmelt starts later in the season and is radiation dominated bales et al 2006 so slope and aspect are greater controls on the timing of snow accumulation and melt and by extension runoff in addition baseflow fed by groundwater is a larger component of streamflow in almanor than the east branch so soil characteristics i e saturated hydraulic conductivity may be more relevant for determining streamflow the relatively good performance of the aspect driven models in both catchments may be due to rain shadowing effects direction of slope matters not only for snow ablation due to solar radiation but also for snow accumulation since the wettest parts of both basins are the western facing non rain shadowed portions along the main stem of the north fork the saturated hydraulic conductivity based model performed reasonably well in both subbasins but as expected topographic features were still overall most relevant for runoff generation the vegetation density model gave poor results across both basins indicating that subsurface conditions may be stronger drivers of et variation in the feather river since the majority of the land cover in both subbasins is forest vegetation density may be less informative due to relatively little variation across the landscape the univariate models and their differing performance in each subbasin demonstrate the physical basis of gmm based models showing how some factors exert more control on the hydrologic process than others depending on the subbasin we further show how these results can lead to greater processes understanding in headwater catchments we suggest that univariate gmms could be used in practice to assess the most relevant input features before running a multivariate gmm to distribute a new model this is relevant for both modelers and scientist seeking to improve forecasting performance prioritize data collection and better understand the hydrologic cycle at this stage selecting this initial set of input features to test is left to the expert knowledge of the modelers a more rigorous assessment is outside the scope of this study but options such as the use of an information content criterion should be the subject of future work 5 conclusion we introduce a new method for spatial distribution of hydrologic models using the gaussian mixture models gmm algorithm and demonstrate its use in two geologically distinct headwater catchments of the sierra nevada unlike traditional gis based methods the gmm method is objective repeatable and computationally fast on the order of minutes the method identifies the set of modeling locations that best represent the basin as a whole leveraging an efficient statistical learning tool while being grounded in physical basin properties analysis shows that gmm based models are able to match or exceed the performance of traditional gis based models with respect to both average and extreme conditions for both streamflow and other water balance components furthermore we show that the modeling locations selected using gmm better represent the geometry of the basin and thus more accurately reproduce the spatial distribution of processes such as runoff production thus gmm based models are closer to being right for the right reasons kirchner 2006 finally we show how the method can be adapted to test multiple feature combinations and identify the relative importance of a basin s hydrologic drivers an elevation based gmm model performed best in the study basin that sits primarily on the rain snow transition zone while slope and aspect based models performed best for the higher elevation catchment further research should investigate how different input components climates and topographies influence gmm performance the best model performance in the two headwater catchments where this method was tested was achieved with different numbers of modeling locations 100 in almanor and 200 in the east branch however these numbers are likely basin specific so future work should consider methods for identifying the optimal number of modeling locations the improved spatial representation of gmm based hydrologic models create a more robust decision making and process understanding tool for water supply agencies utility companies and flood control operators especially in topographically heterogeneous basins this can help mitigate risk and reduce costs to downstream users residents and infrastructure in addition to enhancing models directly the efficiency of the gmm method can facilitate improvements by encouraging more regular model upgrades this allows agencies to stay abreast of changes to their basins such as land use or vegetation coverage and advances model structure and data collection technology the resource and time intensive nature of updating hydrologic models mean that many agencies do so infrequently often with more than a decade between upgrades the rapid and repeatable nature of the gmm method would reduce time and labor associated with model updates overall the gmm method provides the basis for objective efficient process based model set up with the same capabilities as traditional semi distributed models leveraging advances in statistical learning it is a powerful and promising new tool for hydrologic modeling declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was partially supported by the california energy commission under contract epc 14 067 t m and f a and the national science foundation graduate research fellowship under grant dge 1106400 t m the authors would like to thank kevin richards joseph rungee and qin ma for their assistance with data collection and cleaning appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105076 
25817,reservoir control policies provide a flexible option to adapt to the uncertain hydrologic impacts of climate change this challenge requires robust policies capable of navigating scenarios that are wetter drier or more variable than anticipated while a number of prior studies have trained robust policies using large scenario ensembles there remains a need to understand how the properties of training scenarios impact policy robustness specifically this study investigates scenario properties including annual runoff snowpack and baseline regret the difference between baseline policy and perfect foresight performance in an individual scenario results indicate that policies trained to scenario subsets with high baseline regret outperform those generated with other training sets in both wetter and drier futures largely by adopting an intra annual hedging strategy the approach highlights the potential to improve the efficiency and robustness of policy training by considering both the hydrologic properties and baseline regret of the training ensemble keywords policy search reservoir operations climate adaptation robustness scenario selection 1 introduction adaptation to the multi scale impacts of climate change in water resources systems is challenged by substantial uncertainty in future hydrologic projections particularly in flood and drought risks wilby and dessai 2010 asadieh and krakauer 2017 dottori et al 2018 this hinders the ability to use traditional prediction based planning methods and has resulted in the recent consensus toward robust planning dessai and hulme 2004 wilby and dessai 2010 robust and adaptive planning have been widely considered for both expansion of water resources infrastructure e g haasnoot et al 2013 beh et al 2015 zeff et al 2016 kwakkel et al 2016 maier et al 2016 trindade et al 2017 as well as changes to reservoir control policies e g giuliani et al 2014 quinn et al 2018 herman and giuliani 2018 of the two alternatives control policies provide a more flexible soft path approach as they can be reversed if the future unfolds differently than predicted gleick 2002 fletcher et al 2017 in this case the physical constraints of the existing system establish the range of uncertain scenarios that can be adapted to before new infrastructure is needed e g culley et al 2016 robust planning of reservoir control policies generally consists of two phases that have been studied using a variety of different approaches policy design and robustness analysis a number of studies have focused on the robustness of current system operations to a range of future climate changes represented either by downscaled global circulation model gcm scenarios e g brekke et al 2009 karamouz et al 2013 knowles et al 2018 or synthetically generated scenarios based on perturbed statistics of hydrologic timeseries e g prudhomme et al 2010 brown et al 2012 weaver et al 2013 turner et al 2014 both approaches often serve as precursors to adaptation studies in which a discrete set of proposed management alternatives are tested to mitigate vulnerabilities in future scenarios e g groves et al 2013 steinschneider et al 2015a mateus and tullos 2017 in this case the policies are not trained or optimized to a particular set of scenarios but instead arise from stakeholder expertise and negotiations an alternative approach is to generate candidate alternatives via optimization approaches kasprzyk et al 2013 in this case policy design and robustness analysis are analogous to the train test terminology often used in machine learning e g russell and norvig 2002 and recently in the water resources field e g brodeur et al 2020 policy design training involves optimizing learned policy parameters to a specific set of input data the training set in robustness analysis testing a test set consisting of input data separate from the training set are used to assess the performance of an optimized policy in the context of reservoir control under climate change the most relevant optimization approach is policy search in which parameterized operating rules are optimized for system performance objectives under a set of training scenarios koutsoyiannis and economou 2003 giuliani et al 2015a 2017 this heuristic approach functions as both a simulation optimization problem salazar et al 2016 and an information selection problem for the policy inputs giuliani et al 2015b nayak et al 2018 while the training performance of a policy on an individual scenario represents a best case outcome with perfect foresight the key challenge is whether the policy can generalize to a different test set which is also the case for any optimization method applied in the context of climate adaptation test sets often include additional stochastic realizations of the same uncertainties used in training i e to obtain a thorough representation of sampling uncertainty e g quinn et al 2017 trindade et al 2017 they may also include scenarios representing a different characterization of uncertainty altogether watson and kasprzyk 2017 eker and kwakkel 2018 the robustness of alternatives generated by policy search therefore strongly depends on the choice of scenarios used for training and testing both in terms of coarse scale statistics wet vs dry and the realizations of natural variability that lead to extreme events herman et al 2020 this includes the case where the training data represent a baseline or historical scenario e g kasprzyk et al 2013 giuliani and castelletti 2016 quinn et al 2018 these studies evaluate resulting alternatives over test scenarios spanning a wide range of potential future hydrology but without analyzing the influence of the training scenarios on robustness robust optimization studies overcome this by optimizing robustness metrics over many samples of uncertain scenarios e g hamarat et al 2014 kwakkel et al 2015 while they optimize over a large range of training scenarios studies using robust optimization generally have not considered how well solutions can generalize out of sample robustness measures will typically include either regret which quantifies the cost of choosing an incorrect solution or satisficing which calculates the fraction of scenarios in which a policy meets a set of performance criteria lempert and collins 2007 herman et al 2015 while both of these methods are effective in evaluating the performance of individual solutions there is also the consideration of the robustness of the pareto set as a whole to quantify deviations in multi objective performance while the properties of the test scenarios have been the focus of many prior studies using scenario discovery and related methods the properties of the training scenarios and their influence on policy robustness have received relatively less attention however several recent studies have begun to analyze the effect of the choice of training scenarios in optimization problems for example watson and kasprzyk 2017 extend many objective robust decision making by optimizing to several different sets of scenarios with varying properties they then re evaluate solutions in out of sample scenarios quantifying robustness for individual solutions using the satisficing metric eker and kwakkel 2018 optimize to scenarios with maximum diversity and policy relevance and re evaluate solutions under the same uncertainty characterization used in training giudici et al 2020 propose an algorithm to select the smallest subset of training scenarios which can be used to generate robust solutions when re evaluated against the full set to minimize computational cost these studies all effectively aim to find training scenarios scenario selection that lead to robust out of sample performance however studies to date have not attributed multi objective policy robustness to the hydrologic properties of the training scenarios which holds significant implications for the design of robust policies under climate uncertainty this study proposes an experimental design to determine how the properties of forcing scenarios influence the robustness of multi objective policy alternatives across several combinations of test scenarios this framework is generalizable to any environmental planning problem that includes a no action case an optimization component and an ensemble of forcing scenarios exhibiting uncertainty for the initial application we focus specifically on how the hydrologic properties of climate scenarios influence reservoir policy alternatives one additional scenario property is the baseline regret which quantifies the extent to which policy search can improve upon the status quo based on a perfect foresight optimization for an individual scenario scenarios are clustered into groups with similar hydrology via unsupervised learning and split into different combinations of training and test sets the robustness of the resulting policies is quantified relative to the perfect foresight and baseline solutions to ensure that at a minimum all solutions outperform the baseline no action policy this is done with a normalized hypervolume metric to represent the robustness of the pareto set as a whole which simultaneously quantifies the changes in the performance of the solutions as well as their diversity when re evaluated on a given test set finally we perform hypothesis tests on several iterations of the train test split to identify the properties of training scenarios that lead to the most robust results for each test set with a particular focus on training policies to scenarios which have high baseline regret we demonstrate this approach using a simulation model of the northern california reservoir system coupled with an ensemble of transient downscaled climate scenarios 2 case study 2 1 northern california reservoir system to support urban and agricultural growth amid intense intra and inter annual variability in hydrology california has built a complex system of water resources infrastructure reservoirs in the foothills of the sierra nevada range capture winter and spring flood season flows to be delivered for agriculture and municipal supply particularly during summer months the state water project swp and federal central valley project cvp consist of a number of reservoirs and aqueducts throughout the sacramento san joaquin river basin the terminal delta of this system is the site of pumped water exports from north to south which support agriculture and municipal supply in the southern portion of the state critical environmental requirements related to the salinity of outflows from the delta are a major constraint on these exports delta exports are a key metric for water supply reliability in the state and have been found vulnerable to climate change due to both changes in precipitation levels and seasonal runoff timing anderson et al 2008 ray et al 2020 in the sacramento river basin three of the largest sierra foothill reservoirs by volume shasta oroville and folsom combine to a total of 9 million acre feet 11 1 km 3 of storage in parallel these reservoirs play a major role in balancing the state s human and environmental water needs carryover storage in these reservoirs measured at the end of the water year september 30 is a strong indicator of overall system performance and potential economic vulnerabilities draper and lund 2004 uncertainty in changing inter annual precipitation patterns and reduced snowpack levels has the potential to be detrimental to carryover storage levels and their economic benefits medellín azuara et al 2008 under a variety of projected changes to the hydrologic regime operational adaptations are needed to maintain carryover storage levels to support multiple environmental and water supply related objectives while continuing to provide adequate flood control functions cohen et al 2020 2 2 simulation model orca we use the open source model operation of reservoirs in california orca to simulate the northern california reservoir system https github com jscohen4 orca tree cohen 2021 properties training scenarios orca is a simulation model that runs on a daily timestep and accurately reproduces historical operations cohen et al 2020 the operating rules that drive orca are used as the baseline policy in the current study the model simulates the major components of the california system north of the delta including the shasta oroville and folsom reservoirs and delta water supply exports via the harvey o banks swp and tracy cvp pumping plants fig 1 a and b while not as spatially comprehensive as several other statewide models orca is a pure simulation model which allows for flexible adjustments to operating rules and straightforward evaluation of alternative hydrologic scenarios as required by the proposed set of policy search experiments orca is driven by a basic mass balance update for each reservoir based on timestep t storage s t r in reservoir r is updated based on inflows q r t evaporative losses l t r and a release u t r 1 s t r s t 1 r q t r u t r l t r a target release r t t r is determined by the greatest of three minimum operating requirements that must be satisfied for each reservoir 2 r t t r max u t environment r u t flood r u t demand r c t r the first is a minimum environmental flow requirement u t environment r that varies based on the time of year and water year type the second is a flood control release target u t flood r the flood control release depends on a dynamic flood control rule curve which is determined by a flood control index based on the previous day s precipitation and current reservoir storage finally the minimum demand release u t demand r consists of water supply demands north of the delta south of delta demands to be delivered by banks and tracy pumping plants and a delta outflow demand for environmental benefits and salinity control these water demands are also partially controlled by current and projected reservoir storage creating a feedback between reservoir operations and downstream delta management a snowpack to streamflow forecast enables projections of reservoir inflows throughout the irrigation season this forecast also determines the water year type classification which influences both environmental flow requirements and water supply demands along with other operational parameters the snowpack to streamflow forecast is altered by an exceedance level z w y i for the water year type prediction and z w y t r for individual reservoir inflows the exceedance represents the confidence in the forecast a lower exceedance level indicates a more conservative forecast resulting in lower inflow forecasts and drier water year type classifications and likely hedged reservoir releases the forecast is updated each day in the simulation via equation 3 3 q f t r β d w t r s w e t r α d w t r z w y t r σ d w t r where q f t is the forecasted inflow for the remainder of the water year at reservoir r s w e t is the snow water equivalent at day t β d w t k and α d w t k are regression coefficients for day of water year d w t based on historical streamflow records and σ d w t k is the standard deviation of remaining streamflow on d w t also based on the historical record a curtailment multiplier c t r can hedge releases in cases where the system is not projected to meet a carryover storage target c w y t r at the end of the water year the forecasted flow and current reservoir storage are used to determine what curtailment multiplier would be necessary to meet the carryover target equation 4 the curtailment multiplier is also constrained by a maximum curtailment allowance c max w y t r a higher maximum curtailment will allow for lower releases to occur in the irrigation season to maintain the cold pool carryover storage the daily curtailment multiplier during between may and september 5 m 9 is determined at each timestep via equation 4 4 c t 5 m 9 r min 1 max q f t r s t 1 r d w t 365 r t t c w y t r c max w y t r the release for each reservoir is then equal to the target release r t t k times the curtailment factor c t k 5 u t r r t t r c t r further details concerning operations modeled in orca are described in cohen et al 2020 2 3 data sources several hydroclimatic time series are used as inputs for the simulation model these include daily streamflows spatially gridded and site specific precipitation and air temperature along with spatially averaged and site specific monthly spatial snow water equivalent swe for simulating historical operations these data are obtained from the california data exchange center cdec 2018 downscaled cmip5 climate and hydrology projections are obtained from the united states bureau of reclamation usbr reclamation 2013 brekke et al 2014 these consist of 31 gcms simulated for various emissions scenarios to generate 97 scenarios of precipitation and temperature on a daily timestep through 2100 see section 1 in the supplementary material for a list of institutions providing gcm projections in the usbr study outputs from these gcm simulations were routed through the variable infiltration capacity vic model liang et al 1994 calibrated for each basin yielding additional streamflow and swe projections to serve as model inputs the choice to use a gcm ensemble in this study reflects several considerations first it provides the best available representation of physically based transient changes to hydrology including extreme events despite the known limitations of gcm projections herman et al 2020 second it provides an accurate link between hydrologic variables across space and time linking precipitation streamflow temperature and snowpack in multiple basins this is difficult to achieve with synthetic generators though these are rapidly improving for this purpose e g steinschneider et al 2019 this ensemble exhibits the high degree of uncertainty associated with future precipitation and to a lesser extent temperature in fig 2 a the trajectories of annual streamflow show an end of century average annual flow ranging from 50 of historical values this creates a challenge for how to best adapt operations to balance the tradeoff between flood control and water supply herman and giuliani 2018 nayak et al 2018 all scenarios in the ensemble show a decline in snowpack ranging from 20 to 90 of the historical average this is one of the best predicted aspects of climate change although uncertainties exist in the extent and severity of this thermodynamic hydrologic change cayan et al 2001 klos et al 2014 this can be particularly impactful in mountainous regions where snowpack has historically functioned as a natural reservoir rhoades et al 2018 as a result the primary impact of rising temperatures in the region is earlier spring snowmelt timing knowles et al 2006 kapnick and hall 2010 these intra annual streamflow shifts are predicted throughout the cmip5 ensemble based on the water year centroid a representation of the center of mass of the annual hydrograph fig 2c the ensemble also shows uncertainty in the extent of flood risk changes fig 2d based on both uncertain dynamic climate changes as well as the potential increases given a shift from snow to rain and more rain on snow events mccabe et al 2007 surfleet and tullos 2013 huang et al 2018 lastly the ensemble shows severity in uncertainty related to changes in drought patterns fig 1e neither changes in drought nor flood statistics show a reliable relationship to the overall changes in total annual streamflow overall the downscaled projection ensemble exhibits the significant uncertainty typical of climate adaptation studies requiring careful attention to the choice of scenarios under which reservoir control policies are trained and tested 3 methods the proposed experiments aim to analyze reservoir policy performance on held out climate projections by selecting different subsets of training scenarios based on their hydrologic properties and a baseline regret property the experiments require several components fig 3 1 policy search which is used at several steps throughout the experiment 2 multi objective baseline regret which uses perfect foresight optimization to determine the upper bound of system performance in each scenario 3 unsupervised clustering of the scenario ensemble taking into account baseline regret as well as hydrologic properties to determine train test splits and 4 the robustness of policies trained to one set of scenarios when evaluated on another set finally we analyze the decision variables and dynamics of several robust policies identified using different training sets in the policy search 3 1 policy search we employ multi objective policy search throughout this study to identify operational adaptations by parameterizing the structure of existing rules in this study policy search aims to solve the optimization problem 6 θ argmax θ 1 n s j θ s where s s are the training scenarios over which the particular optimization occurs and n is the number of scenarios in set s θ is the vector of decision variables representing the parameters of the operating policy and j is the vector of objective functions θ is the set of policies which correspond to the pareto optimal solutions thus the policy search attempts to maximize the expected value of objectives j in scenarios s across set s in the specific case of a perfect foresight optimization s includes only one training scenario the alternatives represented by the decision variables include a revised snowpack to streamflow forecasting method updated release curtailment rules and changes in the timing of a dynamic flood control curve specifically 7 θ z w y i z w y t r r w y t c max w y t r r w y t f s r r where z w y i is the forecast exceedance level to determine water year indices and z w y t r is the rest of year inflow forecast exceedance level for each reservoir r in each water year type w y t c max w y t r is the maximum curtailment ratio and f s r represents a shift of the reservoir flood control refill period earlier in the water year given the n r 3 reservoirs and n w y t 5 water year types this leads to a total of 1 n r 1 2 n w y t 34 decision variables for each optimization in prior work these individual actions have been shown to improve system performance by enumeration as snowpack decline continues later in the century cohen et al 2020 but their effect in tandem has not yet been analyzed as a policy search problem the decision variables are optimized using a normalized set with 60 discrete values in order for consistency with alternatives defined in cohen et al 2020 values from the normalized set are transformed in the model to reflect the actual bounds for each variable from the previous study z w y i z w y t r 3 6 3 6 c max w y t r 0 1 and f s r 0 60 the choice of decision variables reflecting system parameters rather than a universal approximator function such as a neural network e g salazar et al 2016 giuliani et al 2017 is intended to support the interpretability of the resulting policies as well as their compatibility with already in place system operations however there are currently efforts to formulate methods that improve the interpretability of neural network based policies for example via sensitivity analysis quinn et al 2019 the objectives j contain five performance metrics including flood control reservoir carryover storage at the end of the water year delta outflow representing salinity control and environmental benefits delta exports for water supply and hydropower generation the expectation of each objective across a scenario set is to be maximized equation 6 the objective values over each scenario are calculated according to 8 j flood θ t 1 t r 1 3 max u t r d q r 0 2 9 j carryover θ y 1 n r 1 3 c r y r 10 j outflow θ t 1 t q i n t t r p t h r o t 11 j exports θ t 1 t t r p t h r o t 12 j hydro θ t 1 t r 1 3 h p t r where t is the number of days t in the simulation period while n is the number of water years y in the flooding objective d q r is the downstream levee capacity of reservoir r note in equation 8 we maximize negative flooding for consistency with maximization of the other objectives c r y r is the carryover storage in reservoir r at the end of water year y q i n t is the delta inflow on day t while t r p t tracy pumping plant and h r p t harvey o banks pumping plant are exports from the delta to the central valley project and state water project respectively lastly h p t r represents the hydropower production from reservoir r on day t these objective functions are intended to capture the necessary balance between key aspects of system performance the optimization is performed using the non dominated genetic sorting algorithm nsgaiii deb and jain 2013 via the open source platypus library hadka 2015 to support this choice algorithm performance was tested over 70 scenarios with three random trials of 50 000 maximum number of function evaluations nfe each obtaining similar results for each trial the results were compared with alternative moeas including ε moea nsgaii and spea2 which showed no significant improvement over nsgaiii for this problem in further instances where this problem is solved 10 000 nfe are used when optimizing to a single scenario perfect foresight while all other optimizations with various train test splits use 50 000 nfe as the number of scenarios increases the optimization is slower to converge visually based on hypervolume influencing the choice of 10 000 vs 50 000 nfe all optimization runs were performed on the hpc1 cluster at uc davis which includes 60 nodes with 16 cores each running at 2 4 ghz 3 2 baseline regret multi objective baseline regret quantifies the maximum level to which system performance can be improved for a particular scenario within the constraints imposed by the policy function and existing infrastructure this property is scenario specific each time it is calculated the baseline policy is held constant while the scenario differs this concept draws from the expected value of perfect information evpi metric proposed by giuliani et al 2015b for multi objective problems by incorporating the results of a perfect foresight optimization it couples the evpi approach with a regret metric savage 1951 which describes the performance of a policy based on its distance from the best possible alternative traditional decision making under uncertainty problems often use the minimax regret approach in which the goal is to choose the alternative that minimizes the maximum regret across all scenarios e g giuliani and castelletti 2016 the baseline regret metric differs from minimax regret because it applies only to the no action case of an individual scenario rather than policy alternatives reflecting the evpi approach it is based on the performance of both a baseline solution and perfect foresight solution set for each scenario as the performance of any other effective policy solution is expected to be bounded by these two as a result the baseline regret partially depends on the suitability of the baseline policy for each climate scenario however this still reflects the ability of the system to adapt to future change even if it is starting from a poor baseline 3 2 1 baseline policy performance and perfect foresight optimization the baseline policy simulation uses parameters θ b to best represent the dynamics of the system shown in historical observations the resulting baseline policy solution performance is denoted as j b s j θ b s a one dimensional vector containing a single value for each objective rather than a full pareto set under the baseline policy this performance is not optimized it should be viewed as a simplified representation of the several performance considerations of a real world system operator the upper bound performance is established by a perfect foresight optimization in this case the policy search is performed over each scenario individually to determine the objective values if the future were known exactly we define the perfect foresight performance metric as j p s j p θ s where the optimized parameters θ are specific to the training set consisting of the single scenario s 3 2 2 hypervolume metric we use a hypervolume metric to quantify the baseline regret of each scenario s in the ensemble the hypervolume is defined as the volume in the objective space between the perfect foresight pareto set j p s and the baseline policy performance j b which is used as a reference point while baseline regret is calculated in a five dimensional objective space for this application the hypervolume concept is illustrated in two dimensions in the top row of fig 3 solutions in the perfect foresight pareto front j p s are anticipated to dominate the baseline policy performance j b s the rare solutions for which this does not occur are not considered in the remainder of the calculations in general a larger hypervolume value indicates improvement over the baseline policy as well as a higher variety in alternatives among the pareto set initially disregarding the baseline solution we normalize all objective values in j p s 0 1 to reduce scaling issues between the objectives the baseline policy performance is normalized accordingly to j b 0 to allow for consistent comparison of baseline regret across scenarios we then calculate the baseline regret r s based on the hypervolume h between the perfect foresight pareto set and the baseline policy reference point such that 13 r s h j p s j b s the baseline regret describes the performance of a perfect foresight optimization relative to the baseline for each climate scenario because the objective values are normalized it provides an upper bound performance metric that can be directly compared across scenarios for a given policy 3 3 scenario clustering unsupervised clustering provides the basis for separating climate projections into training and test sets for the policy search the clustering is based on three features averaged annual streamflow averaged peak annual snow water equivalent and the baseline regret metric described above these features are calculated on the time horizon 2070 2100 which is chosen as the period of analysis due to its large deviation from historical hydrologic conditions and thus high regret fig 2 in the supplementary material this time period also contains much more variability in hydrologic properties than do earlier periods in the projected time horizon fig 2 the three features are calculated for each of the 97 scenarios in the ensemble and clustered using the k means algorithm with k 3 equal to the number of features this allows for a minimally complex characterization of scenario properties based on cluster centroids the resulting clusters are denoted as c 1 c 2 and c 3 3 4 training and test sets we first split each cluster c i randomly into roughly equal training and test subsets s and s t respectively various combinations of these training and test subsets make up the overall training and test sets s i and s t j respectively table 1 while the goal of this division is to ensure that test information is never used in training we acknowledge the possibility for interdependence among the ensemble of climate scenarios for example using the same model or emissions scenario or different models relying on the same components steinschneider et al 2015b 3 4 1 training and testing policy search runs separately for each training set to identify the pareto set of policies θ s i corresponding to each training set of scenarios s i 14 θ s i argmax θ j θ s i in order to increase the extent and continuity of the pareto optimal solutions three trials of each optimization are run using varying random seeds the use of baseline regret as a scenario property links the perfect foresight optimization to various combinations of training scenarios without explicitly using perfect foresight to inform the choice of all training scenarios we next re evaluate the policies trained to set s i over each scenario in the test set s t j resulting in a set of objectives j s i s for each test scenario 15 j s i s j θ s i s s s t j we consider only solutions that also dominate the baseline policy for all scenarios in the test set this is achieved via a filtering step which identifies the solutions that will at a minimum outperform the status quo in all re evaluations we identify these particular policies and solutions as policy set θ i j and solution set j i j this process results in a total of 28 pairwise combinations of training and test scenarios 3 4 2 set diversity while the training and test sets are delineated via k means clustering the diversity of each set should also be considered for the analysis this can help determine if policy performance across train test set combinations is influenced by the scenario diversity in each set diversity is determined via equations 16 and 17 adapted from carlsen et al 2016 and eker and kwakkel 2018 in equation 16 d s represents the diversity of set s w the weight assigned to the extent the mean distance is 0 5 in this case d l k is the euclidean distance based on the m 3 features fnf swe baseline regret for scenarios l and k in equation 17 f m l and f m k are the values for these features to ensure equal weighting of all features f m l and f m k are normalized from 0 to 1 across all scenarios 16 d s 1 w min l k s d l k w mean l k s d l k 17 d l k m f m l f m k 2 3 5 policy robustness we would like to evaluate the robustness of policies trained to set s i when re evaluated over each scenario s in set s t j for all combinations of i j because the performance is multi objective across a range of hydrologic scenarios we use a hypervolume metric normalized by the baseline regret section 3 2 to represent the robustness of the policy set as a whole 3 5 1 hypervolume robustness metric robustness is represented by a normalized hypervolume metric for each solution set the hypervolume for a solution set of train test set combination i j applied to scenario s is defined as that between the baseline reference point j b s and solution set j i j h j i j s j b s this is normalized by the baseline regret r s giving the hypervolume robustnesss metric h r i j s 18 h r i j s h j i j s j b s r s thus the hypervolume robustness metric will always be a fraction of the baseline regret r s ensuring that it can be appropriately compared across train test combinations see fourth row in fig 3 a higher normalized hypervolume metric denotes a more robust policy set with a value of 1 equaling the performance of perfect foresight policies this ensures that the robustness of a policy set is not measured only by its ability to improve system performance relative to the baseline but also the extent to which the policies are able to reach the maximum attainable level of system performance for each train test combination we can obtain a set of hypervolume robustness metric values h r t i j where 19 h r t i j h r i j s 1 h r i j s 2 h r i j s n s s t j in general larger hypervolume robustness metrics will indicate two properties of the objective outputs the first is that as hypervolumes increase the distance between the baseline policy j b s and policy performance set j i j s will increase indicating higher performance improvements compared with the baseline policy additionally a larger hypervolume indicates a higher diversity of solutions across the pareto front 3 5 2 rank sum tests for each pair of train test combinations with identical test sets we perform a one sided mann whitney u test mann and whitney 1947 to determine if the hypervolume of a given training set exceeds that of a second training set when evaluated on the same test set this test aims to determine if policies trained to a test set with particular properties are significantly more robust with p 0 05 we reject the null hypothesis and conclude that the distribution of hypervolume across test scenarios s t j in sample h r t 1 j is greater than that in sample h r t 2 j with statistical significance 3 6 policy analysis finally we analyze individual policies chosen from the most robust training sets by considering tradeoffs between the objective values the decision variables of these policies are compared to the baseline policy to understand what combinations of adaptations to system operations could be promising under a range of future climates we then compare the dynamics in terms of reservoir storage and water supply exports to those obtained by simulating the baseline policy on the same hydrologic inputs and then relate key differences to the decision variables interpreted in the context of the system 4 results and discussion 4 1 scenario clusters scenarios are divided into three clusters based on their streamflow snowpack and baseline regret as shown in fig 4 based on the cluster centroids we define them as high regret low regret wet and low regret dry the high regret scenarios contain a mix of streamflow and snowpack values distributed throughout their respective ranges indicating that baseline regret does not solely depend on annual hydrologic properties the clear separation between the high regret and low regret clusters suggests the possible utility of this metric in determining combinations of training scenarios in policy search experiments the low regret scenarios occur in both wet and dry clusters however the ranges of streamflow and snowpack values overlap across these two clusters fig 4a and b specifically this occurs in two cases first some wetter scenarios may also show high levels of snowpack decline due to severely warmer temperatures second there exist dry scenarios with relatively higher snowpack values than other low flow scenarios due to less warming this overlap along with the much clearer separation between high regret and low regret clusters supports the choice of k 3 clusters to minimize complexity 4 2 training set robustness comparison the three clusters each split randomly into training and test subsets are combined to create different train test splits the overall proportion of training to test sets out of all available scenarios is 50 47 table 1 from all possible combinations combined to create different train test splits created in this process a total of seven training sets and four test sets are chosen to demonstrate the training testing process these are described in table 1 these sets are used to determine the performance of policies optimized to each training set when re evaluated in each test set measured according to the hypervolume robustness metric fig 5 shows the distributions of the resulting hypervolume metric for each train test split plotted as cumulative distributions distributions shifted further right indicate higher robustness of the policy sets over the test set while these distributions support the interpretation of the performance differences between policy sets trained on different scenarios the rankings of policy sets must be shown to be statistically significant these conclusions are made using the mann whitney u test between each pair of train sets over each test set with results shown in fig 6 for the high regret test set fig 6a the most robust policies are those optimized to the high regret and all scenario training sets where the latter contains the former neither of these significantly outperforms the other this finding is not surprising as the policies trained to scenarios with similar properties demonstrate the best out of sample performance however this result does not always hold for the other test sets for example in the test set consisting of low regret wet scenarios fig 6b the best performing set of policies are those trained to a mix of high regret and low regret wet scenarios s 4 which ranks higher than every other training set the training sets containing dry scenarios and lacking wet scenarios s 3 and s 5 perform worst for the high regret test set set s 1 consisting of only high regret scenarios outperforms set s 6 which consists of wet and dry low regret scenarios this indicates that training to only high regret scenarios may be more effective than training to low regret scenarios regardless of the variability in scenarios hydrologic properties this result shows that adding high regret scenarios to the training set whether they are wet or dry improves the robustness of the optimized policies when tested in out of sample wet scenarios additionally including low regret dry scenarios in training sets for policies tested on low regret wet scenarios degrades policy performance similar results are shown for the low regret dry test set fig 6c where the highest ranking training set is again not only the dry scenarios s 3 but also a mix of high regret and low regret dry s 5 scenarios in addition training sets including low regret wet scenarios s 2 and s 4 have the lowest ranking when their corresponding policies are simulated over the low regret dry test set lastly the high regret training set continues to outperform the low regret wet dry training set for s t 4 further highlighting the good training value of high regret scenarios for the final test set s t 4 which includes all testing scenarios fig 6d the majority of mann whitney u tests fail to reject the null hypothesis however results indicate that the high regret training set s 1 outperforms the low regret training sets s 2 s 3 and s 6 as does the all scenarios training set s 7 especially notable is that the high regret training set s 1 outperforms the combined wet dry low regret training set s 6 when testing to all scenarios since both of these training sets have wide ranges for the hydrologic properties this further highlights the benefit of high regret training scenarios over low regret scenarios the diversity of the training sets can be analyzed in tandem with these results the high regret s 1 and all training scenario s 7 sets are the most diverse table 1 based on our specific quantification of set diversity this is an artifact specifically of the high regret values which contain more outliers and a more skewed distribution across all scenarios fig 4a and b the high mean distances that occur from this cause the diversity values to be larger whenever the high regret scenarios are included in a set this leads to the fact that the three sets which do not contain the high regret scenarios s 2 s 3 and s 6 are the least diverse of the sets it could then be concluded that the larger diversity of the high regret set influences its good performance however since this value is skewed by just a few outliers it should not be considered the only reason for the effective training value of the high regret sets training sets s 1 high regret only and s 6 low regret wet dry have similar ranges across both hydrologic properties the lower diversity of set s 6 is influenced by its small range in baseline regret values as well as the fact that is has several scenarios in close proximity in terms of hydrologic properties fig 4c leading to a skewed minimum distance value in the diversity calculation set s 6 has almost three times the number of scenarios as set s 1 which contributes to its low diversity calculation several close proximity scenarios could be omitted to make set s 6 more diverse this would not improve the performance as the set would lose valuable training data and potential for overfitting would increase therefore the high diversity of set s 1 is not the only factor controlling the set s good performance its high baseline regret values will enable the policy search to find solutions more robust to vulnerable conditions additionally there may be many other scenario properties that are not examined in this study which contribute to set performance and scenario training value these include hydroclimatic properties such as temperature rise flood frequencies flow timing precipitation drought patterns soil moisture and evapotranspiration because the high regret training set performs no worse than training to all scenarios the strategy of designing a training set around scenarios with high baseline regret may serve to reduce the computational cost of policy search for large ensemble cases and or to reserve more scenarios for testing to support this point table 2 compares the computational cost for different aspects of policy training in this study training to scenarios with high baseline regret which includes the perfect foresight optimizations required 9733 computing hours roughly three times less than training to all scenarios training to scenarios with high baseline regret improves the efficiency of policy search without sacrificing robustness relative to the case of training to all scenarios this denotes the benefit of analyzing the hydrology and baseline regret of scenarios before a train test split is determined thus it is also possible to determine the conditions under which a high baseline regret set will give computational benefits by generalizing the requirements outlined in table 2 this condition is described as 20 f p ρ p η f r ρ f a ρ a where η represents fraction of overall scenarios which are in the high regret set f p f r and f a denote the number of function evaluations and ρ p ρ r and ρ a denote the number of random seeds for each of the perfect foresight high regret only and all training scenario sets respectively this generalization can potentially be applied to other planning problems in which the baseline regret is determined a priori and where there is a choice about how many high regret solutions to include in the training set this analysis has important implications for the generalizability of this approach several variables may be degrees of freedom for instance numbers of random seeds ρ and function evaluations f necessary for convergence to diverse and near optimal pareto solutions sets will vary across models the fraction of high regret scenarios η may differ based on the number of clusters chosen in some instances if the level of baseline regret is not a significant source of variation among scenarios it may not provide a way of separating different training sets using a clustering approach furthermore differences in performance among training sets may be due to confounding factors not reflected in the abstracted scenario properties especially for hydrologic timeseries which can be summarized in a number of different ways however the proposed clustering and train test methodology is still generalizable across environmental planning applications to pinpoint the most important scenario properties for policy training and out of sample performance therefore discovering conditions for computational benefits furthermore results presented in figs 5 and 6 must be interpreted in light of the fact that the future climate trajectory is uncertain it is likely that more information about future hydrology will be collected over time and this process could complement policy search methods in the context of dynamic planning e g hui et al 2018 fletcher et al 2019 therefore in this study the methodology aims to identify a training strategy that leads to robust outcomes to both uncertain and clustered future climate measured according to multi objective performance bounded by the baseline policy and perfect foresight cases we find that training to scenarios with high baseline regret is competitive with training to all scenarios across a range of future climates and often leads to the best out of sample performance this is likely due to higher inter annual variability in these scenarios based on a higher diversity of extreme events across individual scenarios and potential poor baseline performance in the high regret cluster solutions will give both a wider variety of tradeoffs in objectives and improvements relative to baseline policy performance these findings extend to both wet and dry futures where the inclusion of high regret scenarios in the training set outperforms using exclusively either wet or dry training scenarios this result links to the importance of evaluating perfect foresight policies in individual scenarios when designing the training set to establish an upper bound for system performance 4 3 policy analysis the final step of the analysis is to determine what specific adaptations are implemented by the robust policies this analysis focuses on six specific train test splits chosen based on their high ranking performance 1 policies trained on set s 4 and tested on set s t 2 2 policies trained on the set s 5 tested on set s t 3 3 4 policies trained on set s 1 tested on sets s t 2 and s t 3 and 5 6 policies trained on set s 6 tested on sets s t 2 and s t 3 the average performance measures across all scenarios for these sets are shown by the highlighted solutions on the parallel axis plots in fig 7 while the expected value of all highlighted solutions dominates the baseline policy there are still several significant tradeoffs between the objectives indicated by their nonlinear correlations see section 3 in supplementary material for the s 4 s t 2 train test combination these include statistically significant tradeoffs between hydropower and flooding ρ 0 53 and water supply and delta outflow ρ 0 97 the same tradeoffs exist in the dry test scenarios which also exhibit tradeoffs between carryover storage and flooding ρ 0 49 in general these relationships reflect the fact that higher storage levels benefit several of the proposed objectives although they can be detrimental to the flooding objective which is to be minimized while this high water elevation benefits the hydropower and carryover storage objectives it can induce larger releases if large storms occur later in the spring we have shown that training set s 4 high regret low regret wet will yield the best performing policies for the low regret wet test set s t 2 fig 6b likewise training set s 5 high regret low regret dry will yield the best performing policies for the low regret wet test set s t 3 fig 6b this is reflected in fig 7 a b c e f g where the highlighted pareto solutions for s 4 and s 5 are shifted higher than s 1 and s 6 over their particular test sets as shown by the higher maximum percent of baseline values for the flood objective lower minimum in fig 7a e these ranges in fig 7 a b e f also reflect the better performance of the high regret training set s 1 over the low regret wet dry training set s 6 for both test sets we next examine the four compromise policies that balance the tradeoffs in performance measures denoted as the s 4 s 5 s 1 and s 6 policies in fig 7 the s 4 and s 5 policies coming from the most robust training sets for the respective test sets also give the best performance on the individual scenarios fig 7d h the alternatives that these policies employ are shown in fig 8 along with a comparison to the decision variables of the baseline policy each column in the tables represents the decision variable which occurs for that specific water year type in the s 4 policy shasta and folsom reservoirs have higher maximum allowable curtailments c m a x w y t r than in the baseline policy these higher maximum curtailment levels will allow for increased hedging of releases the curtailments for oroville reservoir are higher in wet above and below normal years but lower in dry and critical years all three reservoirs also have a flood pool shift of at least 10 days forward in the water year for the s 4 policy in wet and above normal years shasta and oroville use low z w y t r values indicating a very conservative forecast with a high exceedance level in drier water year types the z w y t r values are generally close to or greater than the baseline exceedance levels for folsom reservoir these values vary much more across water year types the differences between operational adaptations at each reservoir highlight the complexity of managing the multi reservoir system and the potential to design adaptations for system wide benefit fig 9 shows the system dynamics of the baseline policy compared to the compromise policies in a time series over one scenario from each corresponding test set an rcp 8 5 scenario cnrm cm5 for low regret wet and an rcp 6 0 miroc5 scenario for low regret dry under the baseline policy reservoir storage levels are vulnerable to snowmelt loss regardless of water year type evidenced by low storage levels in the irrigation season even in wetter years the s 4 policy mitigates this vulnerability via an intra annual hedging resulting in higher reservoir storage during the early irrigation season may june the s 5 policy functions similarly for both policies this intra annual hedging dynamic is supported by the adapted snowpack to streamflow forecasts where underpredictions will cause some release curtailments to conserve for potential low inflows later in the season however curtailments can be partially avoided with higher carryover storage due to the flood pool shift this seasonal shift is also reflected in the delta exports fig 9d h which maximize total volume by shifting throughout the year the remaining s 6 and s 1 policies also exhibit the intra annual hedging strategy see section 4 in supplementary material for these policies decision variables however given that reservoir storage becomes higher in the flood season and carryover storage drops lower when these policies are deployed they are slightly less effective fig 9b c f g additionally they often will have periods of low delta exports fig 9d h this highlights that a policy from the best performing training set for a particular test set may be more likely to give better performance for scenarios in that test set there are two major differences between the s 4 and s 5 policies stemming from the hydrologic properties of their respective training scenarios the first is that the s 5 policy tends to curtail releases more during dry and critical years reflected in its conservative forecasts and high maximum curtailment allowances the s 4 policy hedges less during dry and critical years and instead relies on larger storage brought about by intra annual hedging this is further driven by the low maximum curtailment allowances for oroville during these water year types the second difference is that the s 4 policy tends to hold less storage during the flood season than the baseline policy while the s 5 policy does not for the s 4 policy this makes curtailment less necessary later in the mid to late summer and reduces flood vulnerabilities the fact that policies exist that can improve upon both of these objectives via the same policy parameters is the main reason why flood control and carryover storage do not have a significant tradeoff in the wet test set in summary analysis of these two compromise policies shows how training to scenarios with high baseline regret can yield policies with improved performance on out of sample hydrology to balance conflicting objectives 5 conclusions this study advances the design and testing of robust control policies as an adaptation to uncertainty in environmental planning problems contributing an experimental design to better understand the influence of the forcing scenario properties and baseline regret of training scenarios on the robustness of resulting policies we demonstrate this approach for the northern california reservoir system to determine how transient downscaled climate scenarios impact tradeoffs between water supply flood control environmental flows and hydropower generation results indicate that policies trained to scenario sets with high baseline regret tend to outperform those generated with other training sets in both wetter and drier futures additionally the policies adapted under these conditions develop an intra annual hedging strategy to mitigate the effects of snowpack decline under rising temperatures the approach highlights the general importance of considering the specific properties of training scenarios in the design of robust control policies beyond the pairwise comparison of train test splits this analysis also highlights the general difficulty of maintaining out of sample performance for reservoir control policies this is driven primarily by extreme events that occur infrequently by definition and which may be the result of natural variability rather than anthropogenic change creating a risk of overfitting to the training set the baseline regret based on perfect foresight optimization provides a measure of regret to place this performance degradation in context unlike the traditional minimax regret strategy where the alternative that minimizes the maximum regret across all scenarios is chosen our approach uses a regret metric to choose training scenarios rather than optimal alternatives we show that optimal policies benefit from training to sets of scenarios with a high regret for the baseline solution our methodology also provides a way to group ensembles of scenarios using an unsupervised learning approach along with other hydrologic properties including streamflow and snowpack to create an experiment which maps the relationship between training and test scenarios to the outcome of policy robustness considering both the performance and diversity of solutions the latter is particularly important given the concern with reversible adaptations to operations which can be changed over time herman et al 2020 while this study considers uncertainty in hydrology due to climate change across downscaled model projections it could further test the robustness of the resulting policies against more realizations of sampling variability from a synthetic generator or supplement the training set with the same increasing the number of scenario realizations would allow for additional hydrologic variables to be included in clustering such as changes in flood and drought frequencies and intra annual streamflow shifts additionally policy training might be improved with a more flexible policy structure beyond parameterizing the existing system such as a neural network though this may also increase the potential for overfitting due to increased degrees of freedom policy training can also be coupled with infrastructure design e g bertoni et al 2020 which in many regions will be required to cope with the more extreme projections of hydrologic change lastly while our approach is demonstrated with an exampled from the water resources management field it can generalize to any environmental natural resources or infrastructure planning problem which includes a no action case an optimization component and a forcing scenario ensemble future work should explore the impacts of these additional experimental components in combination with the analysis of the training scenarios properties presented here to further improve robust policy search under uncertainty software availability code for operations of reservoir in california orca a python simulation model is available at https github com jscohen4 orca simulation code data analysis and figure scripts for this manuscripts are available at https github com jscohen4 orca tree cohen 2021 properties training scenarios processed cmip5 climate projection data files used in this study are available at https github com jscohen4 orca cmip5 inputs declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was partially supported by the u s national science foundation grant cbet 1803589 and infews grant cns 1639268 any opinions findings and conclusions are those of the authors and do not necessarily reflect the views or policies of the nsf we further acknowledge the world climate research program s working group on coupled modeling and the climate modeling groups listed in the supplement of this paper for producing and making available their model output appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105047 
25817,reservoir control policies provide a flexible option to adapt to the uncertain hydrologic impacts of climate change this challenge requires robust policies capable of navigating scenarios that are wetter drier or more variable than anticipated while a number of prior studies have trained robust policies using large scenario ensembles there remains a need to understand how the properties of training scenarios impact policy robustness specifically this study investigates scenario properties including annual runoff snowpack and baseline regret the difference between baseline policy and perfect foresight performance in an individual scenario results indicate that policies trained to scenario subsets with high baseline regret outperform those generated with other training sets in both wetter and drier futures largely by adopting an intra annual hedging strategy the approach highlights the potential to improve the efficiency and robustness of policy training by considering both the hydrologic properties and baseline regret of the training ensemble keywords policy search reservoir operations climate adaptation robustness scenario selection 1 introduction adaptation to the multi scale impacts of climate change in water resources systems is challenged by substantial uncertainty in future hydrologic projections particularly in flood and drought risks wilby and dessai 2010 asadieh and krakauer 2017 dottori et al 2018 this hinders the ability to use traditional prediction based planning methods and has resulted in the recent consensus toward robust planning dessai and hulme 2004 wilby and dessai 2010 robust and adaptive planning have been widely considered for both expansion of water resources infrastructure e g haasnoot et al 2013 beh et al 2015 zeff et al 2016 kwakkel et al 2016 maier et al 2016 trindade et al 2017 as well as changes to reservoir control policies e g giuliani et al 2014 quinn et al 2018 herman and giuliani 2018 of the two alternatives control policies provide a more flexible soft path approach as they can be reversed if the future unfolds differently than predicted gleick 2002 fletcher et al 2017 in this case the physical constraints of the existing system establish the range of uncertain scenarios that can be adapted to before new infrastructure is needed e g culley et al 2016 robust planning of reservoir control policies generally consists of two phases that have been studied using a variety of different approaches policy design and robustness analysis a number of studies have focused on the robustness of current system operations to a range of future climate changes represented either by downscaled global circulation model gcm scenarios e g brekke et al 2009 karamouz et al 2013 knowles et al 2018 or synthetically generated scenarios based on perturbed statistics of hydrologic timeseries e g prudhomme et al 2010 brown et al 2012 weaver et al 2013 turner et al 2014 both approaches often serve as precursors to adaptation studies in which a discrete set of proposed management alternatives are tested to mitigate vulnerabilities in future scenarios e g groves et al 2013 steinschneider et al 2015a mateus and tullos 2017 in this case the policies are not trained or optimized to a particular set of scenarios but instead arise from stakeholder expertise and negotiations an alternative approach is to generate candidate alternatives via optimization approaches kasprzyk et al 2013 in this case policy design and robustness analysis are analogous to the train test terminology often used in machine learning e g russell and norvig 2002 and recently in the water resources field e g brodeur et al 2020 policy design training involves optimizing learned policy parameters to a specific set of input data the training set in robustness analysis testing a test set consisting of input data separate from the training set are used to assess the performance of an optimized policy in the context of reservoir control under climate change the most relevant optimization approach is policy search in which parameterized operating rules are optimized for system performance objectives under a set of training scenarios koutsoyiannis and economou 2003 giuliani et al 2015a 2017 this heuristic approach functions as both a simulation optimization problem salazar et al 2016 and an information selection problem for the policy inputs giuliani et al 2015b nayak et al 2018 while the training performance of a policy on an individual scenario represents a best case outcome with perfect foresight the key challenge is whether the policy can generalize to a different test set which is also the case for any optimization method applied in the context of climate adaptation test sets often include additional stochastic realizations of the same uncertainties used in training i e to obtain a thorough representation of sampling uncertainty e g quinn et al 2017 trindade et al 2017 they may also include scenarios representing a different characterization of uncertainty altogether watson and kasprzyk 2017 eker and kwakkel 2018 the robustness of alternatives generated by policy search therefore strongly depends on the choice of scenarios used for training and testing both in terms of coarse scale statistics wet vs dry and the realizations of natural variability that lead to extreme events herman et al 2020 this includes the case where the training data represent a baseline or historical scenario e g kasprzyk et al 2013 giuliani and castelletti 2016 quinn et al 2018 these studies evaluate resulting alternatives over test scenarios spanning a wide range of potential future hydrology but without analyzing the influence of the training scenarios on robustness robust optimization studies overcome this by optimizing robustness metrics over many samples of uncertain scenarios e g hamarat et al 2014 kwakkel et al 2015 while they optimize over a large range of training scenarios studies using robust optimization generally have not considered how well solutions can generalize out of sample robustness measures will typically include either regret which quantifies the cost of choosing an incorrect solution or satisficing which calculates the fraction of scenarios in which a policy meets a set of performance criteria lempert and collins 2007 herman et al 2015 while both of these methods are effective in evaluating the performance of individual solutions there is also the consideration of the robustness of the pareto set as a whole to quantify deviations in multi objective performance while the properties of the test scenarios have been the focus of many prior studies using scenario discovery and related methods the properties of the training scenarios and their influence on policy robustness have received relatively less attention however several recent studies have begun to analyze the effect of the choice of training scenarios in optimization problems for example watson and kasprzyk 2017 extend many objective robust decision making by optimizing to several different sets of scenarios with varying properties they then re evaluate solutions in out of sample scenarios quantifying robustness for individual solutions using the satisficing metric eker and kwakkel 2018 optimize to scenarios with maximum diversity and policy relevance and re evaluate solutions under the same uncertainty characterization used in training giudici et al 2020 propose an algorithm to select the smallest subset of training scenarios which can be used to generate robust solutions when re evaluated against the full set to minimize computational cost these studies all effectively aim to find training scenarios scenario selection that lead to robust out of sample performance however studies to date have not attributed multi objective policy robustness to the hydrologic properties of the training scenarios which holds significant implications for the design of robust policies under climate uncertainty this study proposes an experimental design to determine how the properties of forcing scenarios influence the robustness of multi objective policy alternatives across several combinations of test scenarios this framework is generalizable to any environmental planning problem that includes a no action case an optimization component and an ensemble of forcing scenarios exhibiting uncertainty for the initial application we focus specifically on how the hydrologic properties of climate scenarios influence reservoir policy alternatives one additional scenario property is the baseline regret which quantifies the extent to which policy search can improve upon the status quo based on a perfect foresight optimization for an individual scenario scenarios are clustered into groups with similar hydrology via unsupervised learning and split into different combinations of training and test sets the robustness of the resulting policies is quantified relative to the perfect foresight and baseline solutions to ensure that at a minimum all solutions outperform the baseline no action policy this is done with a normalized hypervolume metric to represent the robustness of the pareto set as a whole which simultaneously quantifies the changes in the performance of the solutions as well as their diversity when re evaluated on a given test set finally we perform hypothesis tests on several iterations of the train test split to identify the properties of training scenarios that lead to the most robust results for each test set with a particular focus on training policies to scenarios which have high baseline regret we demonstrate this approach using a simulation model of the northern california reservoir system coupled with an ensemble of transient downscaled climate scenarios 2 case study 2 1 northern california reservoir system to support urban and agricultural growth amid intense intra and inter annual variability in hydrology california has built a complex system of water resources infrastructure reservoirs in the foothills of the sierra nevada range capture winter and spring flood season flows to be delivered for agriculture and municipal supply particularly during summer months the state water project swp and federal central valley project cvp consist of a number of reservoirs and aqueducts throughout the sacramento san joaquin river basin the terminal delta of this system is the site of pumped water exports from north to south which support agriculture and municipal supply in the southern portion of the state critical environmental requirements related to the salinity of outflows from the delta are a major constraint on these exports delta exports are a key metric for water supply reliability in the state and have been found vulnerable to climate change due to both changes in precipitation levels and seasonal runoff timing anderson et al 2008 ray et al 2020 in the sacramento river basin three of the largest sierra foothill reservoirs by volume shasta oroville and folsom combine to a total of 9 million acre feet 11 1 km 3 of storage in parallel these reservoirs play a major role in balancing the state s human and environmental water needs carryover storage in these reservoirs measured at the end of the water year september 30 is a strong indicator of overall system performance and potential economic vulnerabilities draper and lund 2004 uncertainty in changing inter annual precipitation patterns and reduced snowpack levels has the potential to be detrimental to carryover storage levels and their economic benefits medellín azuara et al 2008 under a variety of projected changes to the hydrologic regime operational adaptations are needed to maintain carryover storage levels to support multiple environmental and water supply related objectives while continuing to provide adequate flood control functions cohen et al 2020 2 2 simulation model orca we use the open source model operation of reservoirs in california orca to simulate the northern california reservoir system https github com jscohen4 orca tree cohen 2021 properties training scenarios orca is a simulation model that runs on a daily timestep and accurately reproduces historical operations cohen et al 2020 the operating rules that drive orca are used as the baseline policy in the current study the model simulates the major components of the california system north of the delta including the shasta oroville and folsom reservoirs and delta water supply exports via the harvey o banks swp and tracy cvp pumping plants fig 1 a and b while not as spatially comprehensive as several other statewide models orca is a pure simulation model which allows for flexible adjustments to operating rules and straightforward evaluation of alternative hydrologic scenarios as required by the proposed set of policy search experiments orca is driven by a basic mass balance update for each reservoir based on timestep t storage s t r in reservoir r is updated based on inflows q r t evaporative losses l t r and a release u t r 1 s t r s t 1 r q t r u t r l t r a target release r t t r is determined by the greatest of three minimum operating requirements that must be satisfied for each reservoir 2 r t t r max u t environment r u t flood r u t demand r c t r the first is a minimum environmental flow requirement u t environment r that varies based on the time of year and water year type the second is a flood control release target u t flood r the flood control release depends on a dynamic flood control rule curve which is determined by a flood control index based on the previous day s precipitation and current reservoir storage finally the minimum demand release u t demand r consists of water supply demands north of the delta south of delta demands to be delivered by banks and tracy pumping plants and a delta outflow demand for environmental benefits and salinity control these water demands are also partially controlled by current and projected reservoir storage creating a feedback between reservoir operations and downstream delta management a snowpack to streamflow forecast enables projections of reservoir inflows throughout the irrigation season this forecast also determines the water year type classification which influences both environmental flow requirements and water supply demands along with other operational parameters the snowpack to streamflow forecast is altered by an exceedance level z w y i for the water year type prediction and z w y t r for individual reservoir inflows the exceedance represents the confidence in the forecast a lower exceedance level indicates a more conservative forecast resulting in lower inflow forecasts and drier water year type classifications and likely hedged reservoir releases the forecast is updated each day in the simulation via equation 3 3 q f t r β d w t r s w e t r α d w t r z w y t r σ d w t r where q f t is the forecasted inflow for the remainder of the water year at reservoir r s w e t is the snow water equivalent at day t β d w t k and α d w t k are regression coefficients for day of water year d w t based on historical streamflow records and σ d w t k is the standard deviation of remaining streamflow on d w t also based on the historical record a curtailment multiplier c t r can hedge releases in cases where the system is not projected to meet a carryover storage target c w y t r at the end of the water year the forecasted flow and current reservoir storage are used to determine what curtailment multiplier would be necessary to meet the carryover target equation 4 the curtailment multiplier is also constrained by a maximum curtailment allowance c max w y t r a higher maximum curtailment will allow for lower releases to occur in the irrigation season to maintain the cold pool carryover storage the daily curtailment multiplier during between may and september 5 m 9 is determined at each timestep via equation 4 4 c t 5 m 9 r min 1 max q f t r s t 1 r d w t 365 r t t c w y t r c max w y t r the release for each reservoir is then equal to the target release r t t k times the curtailment factor c t k 5 u t r r t t r c t r further details concerning operations modeled in orca are described in cohen et al 2020 2 3 data sources several hydroclimatic time series are used as inputs for the simulation model these include daily streamflows spatially gridded and site specific precipitation and air temperature along with spatially averaged and site specific monthly spatial snow water equivalent swe for simulating historical operations these data are obtained from the california data exchange center cdec 2018 downscaled cmip5 climate and hydrology projections are obtained from the united states bureau of reclamation usbr reclamation 2013 brekke et al 2014 these consist of 31 gcms simulated for various emissions scenarios to generate 97 scenarios of precipitation and temperature on a daily timestep through 2100 see section 1 in the supplementary material for a list of institutions providing gcm projections in the usbr study outputs from these gcm simulations were routed through the variable infiltration capacity vic model liang et al 1994 calibrated for each basin yielding additional streamflow and swe projections to serve as model inputs the choice to use a gcm ensemble in this study reflects several considerations first it provides the best available representation of physically based transient changes to hydrology including extreme events despite the known limitations of gcm projections herman et al 2020 second it provides an accurate link between hydrologic variables across space and time linking precipitation streamflow temperature and snowpack in multiple basins this is difficult to achieve with synthetic generators though these are rapidly improving for this purpose e g steinschneider et al 2019 this ensemble exhibits the high degree of uncertainty associated with future precipitation and to a lesser extent temperature in fig 2 a the trajectories of annual streamflow show an end of century average annual flow ranging from 50 of historical values this creates a challenge for how to best adapt operations to balance the tradeoff between flood control and water supply herman and giuliani 2018 nayak et al 2018 all scenarios in the ensemble show a decline in snowpack ranging from 20 to 90 of the historical average this is one of the best predicted aspects of climate change although uncertainties exist in the extent and severity of this thermodynamic hydrologic change cayan et al 2001 klos et al 2014 this can be particularly impactful in mountainous regions where snowpack has historically functioned as a natural reservoir rhoades et al 2018 as a result the primary impact of rising temperatures in the region is earlier spring snowmelt timing knowles et al 2006 kapnick and hall 2010 these intra annual streamflow shifts are predicted throughout the cmip5 ensemble based on the water year centroid a representation of the center of mass of the annual hydrograph fig 2c the ensemble also shows uncertainty in the extent of flood risk changes fig 2d based on both uncertain dynamic climate changes as well as the potential increases given a shift from snow to rain and more rain on snow events mccabe et al 2007 surfleet and tullos 2013 huang et al 2018 lastly the ensemble shows severity in uncertainty related to changes in drought patterns fig 1e neither changes in drought nor flood statistics show a reliable relationship to the overall changes in total annual streamflow overall the downscaled projection ensemble exhibits the significant uncertainty typical of climate adaptation studies requiring careful attention to the choice of scenarios under which reservoir control policies are trained and tested 3 methods the proposed experiments aim to analyze reservoir policy performance on held out climate projections by selecting different subsets of training scenarios based on their hydrologic properties and a baseline regret property the experiments require several components fig 3 1 policy search which is used at several steps throughout the experiment 2 multi objective baseline regret which uses perfect foresight optimization to determine the upper bound of system performance in each scenario 3 unsupervised clustering of the scenario ensemble taking into account baseline regret as well as hydrologic properties to determine train test splits and 4 the robustness of policies trained to one set of scenarios when evaluated on another set finally we analyze the decision variables and dynamics of several robust policies identified using different training sets in the policy search 3 1 policy search we employ multi objective policy search throughout this study to identify operational adaptations by parameterizing the structure of existing rules in this study policy search aims to solve the optimization problem 6 θ argmax θ 1 n s j θ s where s s are the training scenarios over which the particular optimization occurs and n is the number of scenarios in set s θ is the vector of decision variables representing the parameters of the operating policy and j is the vector of objective functions θ is the set of policies which correspond to the pareto optimal solutions thus the policy search attempts to maximize the expected value of objectives j in scenarios s across set s in the specific case of a perfect foresight optimization s includes only one training scenario the alternatives represented by the decision variables include a revised snowpack to streamflow forecasting method updated release curtailment rules and changes in the timing of a dynamic flood control curve specifically 7 θ z w y i z w y t r r w y t c max w y t r r w y t f s r r where z w y i is the forecast exceedance level to determine water year indices and z w y t r is the rest of year inflow forecast exceedance level for each reservoir r in each water year type w y t c max w y t r is the maximum curtailment ratio and f s r represents a shift of the reservoir flood control refill period earlier in the water year given the n r 3 reservoirs and n w y t 5 water year types this leads to a total of 1 n r 1 2 n w y t 34 decision variables for each optimization in prior work these individual actions have been shown to improve system performance by enumeration as snowpack decline continues later in the century cohen et al 2020 but their effect in tandem has not yet been analyzed as a policy search problem the decision variables are optimized using a normalized set with 60 discrete values in order for consistency with alternatives defined in cohen et al 2020 values from the normalized set are transformed in the model to reflect the actual bounds for each variable from the previous study z w y i z w y t r 3 6 3 6 c max w y t r 0 1 and f s r 0 60 the choice of decision variables reflecting system parameters rather than a universal approximator function such as a neural network e g salazar et al 2016 giuliani et al 2017 is intended to support the interpretability of the resulting policies as well as their compatibility with already in place system operations however there are currently efforts to formulate methods that improve the interpretability of neural network based policies for example via sensitivity analysis quinn et al 2019 the objectives j contain five performance metrics including flood control reservoir carryover storage at the end of the water year delta outflow representing salinity control and environmental benefits delta exports for water supply and hydropower generation the expectation of each objective across a scenario set is to be maximized equation 6 the objective values over each scenario are calculated according to 8 j flood θ t 1 t r 1 3 max u t r d q r 0 2 9 j carryover θ y 1 n r 1 3 c r y r 10 j outflow θ t 1 t q i n t t r p t h r o t 11 j exports θ t 1 t t r p t h r o t 12 j hydro θ t 1 t r 1 3 h p t r where t is the number of days t in the simulation period while n is the number of water years y in the flooding objective d q r is the downstream levee capacity of reservoir r note in equation 8 we maximize negative flooding for consistency with maximization of the other objectives c r y r is the carryover storage in reservoir r at the end of water year y q i n t is the delta inflow on day t while t r p t tracy pumping plant and h r p t harvey o banks pumping plant are exports from the delta to the central valley project and state water project respectively lastly h p t r represents the hydropower production from reservoir r on day t these objective functions are intended to capture the necessary balance between key aspects of system performance the optimization is performed using the non dominated genetic sorting algorithm nsgaiii deb and jain 2013 via the open source platypus library hadka 2015 to support this choice algorithm performance was tested over 70 scenarios with three random trials of 50 000 maximum number of function evaluations nfe each obtaining similar results for each trial the results were compared with alternative moeas including ε moea nsgaii and spea2 which showed no significant improvement over nsgaiii for this problem in further instances where this problem is solved 10 000 nfe are used when optimizing to a single scenario perfect foresight while all other optimizations with various train test splits use 50 000 nfe as the number of scenarios increases the optimization is slower to converge visually based on hypervolume influencing the choice of 10 000 vs 50 000 nfe all optimization runs were performed on the hpc1 cluster at uc davis which includes 60 nodes with 16 cores each running at 2 4 ghz 3 2 baseline regret multi objective baseline regret quantifies the maximum level to which system performance can be improved for a particular scenario within the constraints imposed by the policy function and existing infrastructure this property is scenario specific each time it is calculated the baseline policy is held constant while the scenario differs this concept draws from the expected value of perfect information evpi metric proposed by giuliani et al 2015b for multi objective problems by incorporating the results of a perfect foresight optimization it couples the evpi approach with a regret metric savage 1951 which describes the performance of a policy based on its distance from the best possible alternative traditional decision making under uncertainty problems often use the minimax regret approach in which the goal is to choose the alternative that minimizes the maximum regret across all scenarios e g giuliani and castelletti 2016 the baseline regret metric differs from minimax regret because it applies only to the no action case of an individual scenario rather than policy alternatives reflecting the evpi approach it is based on the performance of both a baseline solution and perfect foresight solution set for each scenario as the performance of any other effective policy solution is expected to be bounded by these two as a result the baseline regret partially depends on the suitability of the baseline policy for each climate scenario however this still reflects the ability of the system to adapt to future change even if it is starting from a poor baseline 3 2 1 baseline policy performance and perfect foresight optimization the baseline policy simulation uses parameters θ b to best represent the dynamics of the system shown in historical observations the resulting baseline policy solution performance is denoted as j b s j θ b s a one dimensional vector containing a single value for each objective rather than a full pareto set under the baseline policy this performance is not optimized it should be viewed as a simplified representation of the several performance considerations of a real world system operator the upper bound performance is established by a perfect foresight optimization in this case the policy search is performed over each scenario individually to determine the objective values if the future were known exactly we define the perfect foresight performance metric as j p s j p θ s where the optimized parameters θ are specific to the training set consisting of the single scenario s 3 2 2 hypervolume metric we use a hypervolume metric to quantify the baseline regret of each scenario s in the ensemble the hypervolume is defined as the volume in the objective space between the perfect foresight pareto set j p s and the baseline policy performance j b which is used as a reference point while baseline regret is calculated in a five dimensional objective space for this application the hypervolume concept is illustrated in two dimensions in the top row of fig 3 solutions in the perfect foresight pareto front j p s are anticipated to dominate the baseline policy performance j b s the rare solutions for which this does not occur are not considered in the remainder of the calculations in general a larger hypervolume value indicates improvement over the baseline policy as well as a higher variety in alternatives among the pareto set initially disregarding the baseline solution we normalize all objective values in j p s 0 1 to reduce scaling issues between the objectives the baseline policy performance is normalized accordingly to j b 0 to allow for consistent comparison of baseline regret across scenarios we then calculate the baseline regret r s based on the hypervolume h between the perfect foresight pareto set and the baseline policy reference point such that 13 r s h j p s j b s the baseline regret describes the performance of a perfect foresight optimization relative to the baseline for each climate scenario because the objective values are normalized it provides an upper bound performance metric that can be directly compared across scenarios for a given policy 3 3 scenario clustering unsupervised clustering provides the basis for separating climate projections into training and test sets for the policy search the clustering is based on three features averaged annual streamflow averaged peak annual snow water equivalent and the baseline regret metric described above these features are calculated on the time horizon 2070 2100 which is chosen as the period of analysis due to its large deviation from historical hydrologic conditions and thus high regret fig 2 in the supplementary material this time period also contains much more variability in hydrologic properties than do earlier periods in the projected time horizon fig 2 the three features are calculated for each of the 97 scenarios in the ensemble and clustered using the k means algorithm with k 3 equal to the number of features this allows for a minimally complex characterization of scenario properties based on cluster centroids the resulting clusters are denoted as c 1 c 2 and c 3 3 4 training and test sets we first split each cluster c i randomly into roughly equal training and test subsets s and s t respectively various combinations of these training and test subsets make up the overall training and test sets s i and s t j respectively table 1 while the goal of this division is to ensure that test information is never used in training we acknowledge the possibility for interdependence among the ensemble of climate scenarios for example using the same model or emissions scenario or different models relying on the same components steinschneider et al 2015b 3 4 1 training and testing policy search runs separately for each training set to identify the pareto set of policies θ s i corresponding to each training set of scenarios s i 14 θ s i argmax θ j θ s i in order to increase the extent and continuity of the pareto optimal solutions three trials of each optimization are run using varying random seeds the use of baseline regret as a scenario property links the perfect foresight optimization to various combinations of training scenarios without explicitly using perfect foresight to inform the choice of all training scenarios we next re evaluate the policies trained to set s i over each scenario in the test set s t j resulting in a set of objectives j s i s for each test scenario 15 j s i s j θ s i s s s t j we consider only solutions that also dominate the baseline policy for all scenarios in the test set this is achieved via a filtering step which identifies the solutions that will at a minimum outperform the status quo in all re evaluations we identify these particular policies and solutions as policy set θ i j and solution set j i j this process results in a total of 28 pairwise combinations of training and test scenarios 3 4 2 set diversity while the training and test sets are delineated via k means clustering the diversity of each set should also be considered for the analysis this can help determine if policy performance across train test set combinations is influenced by the scenario diversity in each set diversity is determined via equations 16 and 17 adapted from carlsen et al 2016 and eker and kwakkel 2018 in equation 16 d s represents the diversity of set s w the weight assigned to the extent the mean distance is 0 5 in this case d l k is the euclidean distance based on the m 3 features fnf swe baseline regret for scenarios l and k in equation 17 f m l and f m k are the values for these features to ensure equal weighting of all features f m l and f m k are normalized from 0 to 1 across all scenarios 16 d s 1 w min l k s d l k w mean l k s d l k 17 d l k m f m l f m k 2 3 5 policy robustness we would like to evaluate the robustness of policies trained to set s i when re evaluated over each scenario s in set s t j for all combinations of i j because the performance is multi objective across a range of hydrologic scenarios we use a hypervolume metric normalized by the baseline regret section 3 2 to represent the robustness of the policy set as a whole 3 5 1 hypervolume robustness metric robustness is represented by a normalized hypervolume metric for each solution set the hypervolume for a solution set of train test set combination i j applied to scenario s is defined as that between the baseline reference point j b s and solution set j i j h j i j s j b s this is normalized by the baseline regret r s giving the hypervolume robustnesss metric h r i j s 18 h r i j s h j i j s j b s r s thus the hypervolume robustness metric will always be a fraction of the baseline regret r s ensuring that it can be appropriately compared across train test combinations see fourth row in fig 3 a higher normalized hypervolume metric denotes a more robust policy set with a value of 1 equaling the performance of perfect foresight policies this ensures that the robustness of a policy set is not measured only by its ability to improve system performance relative to the baseline but also the extent to which the policies are able to reach the maximum attainable level of system performance for each train test combination we can obtain a set of hypervolume robustness metric values h r t i j where 19 h r t i j h r i j s 1 h r i j s 2 h r i j s n s s t j in general larger hypervolume robustness metrics will indicate two properties of the objective outputs the first is that as hypervolumes increase the distance between the baseline policy j b s and policy performance set j i j s will increase indicating higher performance improvements compared with the baseline policy additionally a larger hypervolume indicates a higher diversity of solutions across the pareto front 3 5 2 rank sum tests for each pair of train test combinations with identical test sets we perform a one sided mann whitney u test mann and whitney 1947 to determine if the hypervolume of a given training set exceeds that of a second training set when evaluated on the same test set this test aims to determine if policies trained to a test set with particular properties are significantly more robust with p 0 05 we reject the null hypothesis and conclude that the distribution of hypervolume across test scenarios s t j in sample h r t 1 j is greater than that in sample h r t 2 j with statistical significance 3 6 policy analysis finally we analyze individual policies chosen from the most robust training sets by considering tradeoffs between the objective values the decision variables of these policies are compared to the baseline policy to understand what combinations of adaptations to system operations could be promising under a range of future climates we then compare the dynamics in terms of reservoir storage and water supply exports to those obtained by simulating the baseline policy on the same hydrologic inputs and then relate key differences to the decision variables interpreted in the context of the system 4 results and discussion 4 1 scenario clusters scenarios are divided into three clusters based on their streamflow snowpack and baseline regret as shown in fig 4 based on the cluster centroids we define them as high regret low regret wet and low regret dry the high regret scenarios contain a mix of streamflow and snowpack values distributed throughout their respective ranges indicating that baseline regret does not solely depend on annual hydrologic properties the clear separation between the high regret and low regret clusters suggests the possible utility of this metric in determining combinations of training scenarios in policy search experiments the low regret scenarios occur in both wet and dry clusters however the ranges of streamflow and snowpack values overlap across these two clusters fig 4a and b specifically this occurs in two cases first some wetter scenarios may also show high levels of snowpack decline due to severely warmer temperatures second there exist dry scenarios with relatively higher snowpack values than other low flow scenarios due to less warming this overlap along with the much clearer separation between high regret and low regret clusters supports the choice of k 3 clusters to minimize complexity 4 2 training set robustness comparison the three clusters each split randomly into training and test subsets are combined to create different train test splits the overall proportion of training to test sets out of all available scenarios is 50 47 table 1 from all possible combinations combined to create different train test splits created in this process a total of seven training sets and four test sets are chosen to demonstrate the training testing process these are described in table 1 these sets are used to determine the performance of policies optimized to each training set when re evaluated in each test set measured according to the hypervolume robustness metric fig 5 shows the distributions of the resulting hypervolume metric for each train test split plotted as cumulative distributions distributions shifted further right indicate higher robustness of the policy sets over the test set while these distributions support the interpretation of the performance differences between policy sets trained on different scenarios the rankings of policy sets must be shown to be statistically significant these conclusions are made using the mann whitney u test between each pair of train sets over each test set with results shown in fig 6 for the high regret test set fig 6a the most robust policies are those optimized to the high regret and all scenario training sets where the latter contains the former neither of these significantly outperforms the other this finding is not surprising as the policies trained to scenarios with similar properties demonstrate the best out of sample performance however this result does not always hold for the other test sets for example in the test set consisting of low regret wet scenarios fig 6b the best performing set of policies are those trained to a mix of high regret and low regret wet scenarios s 4 which ranks higher than every other training set the training sets containing dry scenarios and lacking wet scenarios s 3 and s 5 perform worst for the high regret test set set s 1 consisting of only high regret scenarios outperforms set s 6 which consists of wet and dry low regret scenarios this indicates that training to only high regret scenarios may be more effective than training to low regret scenarios regardless of the variability in scenarios hydrologic properties this result shows that adding high regret scenarios to the training set whether they are wet or dry improves the robustness of the optimized policies when tested in out of sample wet scenarios additionally including low regret dry scenarios in training sets for policies tested on low regret wet scenarios degrades policy performance similar results are shown for the low regret dry test set fig 6c where the highest ranking training set is again not only the dry scenarios s 3 but also a mix of high regret and low regret dry s 5 scenarios in addition training sets including low regret wet scenarios s 2 and s 4 have the lowest ranking when their corresponding policies are simulated over the low regret dry test set lastly the high regret training set continues to outperform the low regret wet dry training set for s t 4 further highlighting the good training value of high regret scenarios for the final test set s t 4 which includes all testing scenarios fig 6d the majority of mann whitney u tests fail to reject the null hypothesis however results indicate that the high regret training set s 1 outperforms the low regret training sets s 2 s 3 and s 6 as does the all scenarios training set s 7 especially notable is that the high regret training set s 1 outperforms the combined wet dry low regret training set s 6 when testing to all scenarios since both of these training sets have wide ranges for the hydrologic properties this further highlights the benefit of high regret training scenarios over low regret scenarios the diversity of the training sets can be analyzed in tandem with these results the high regret s 1 and all training scenario s 7 sets are the most diverse table 1 based on our specific quantification of set diversity this is an artifact specifically of the high regret values which contain more outliers and a more skewed distribution across all scenarios fig 4a and b the high mean distances that occur from this cause the diversity values to be larger whenever the high regret scenarios are included in a set this leads to the fact that the three sets which do not contain the high regret scenarios s 2 s 3 and s 6 are the least diverse of the sets it could then be concluded that the larger diversity of the high regret set influences its good performance however since this value is skewed by just a few outliers it should not be considered the only reason for the effective training value of the high regret sets training sets s 1 high regret only and s 6 low regret wet dry have similar ranges across both hydrologic properties the lower diversity of set s 6 is influenced by its small range in baseline regret values as well as the fact that is has several scenarios in close proximity in terms of hydrologic properties fig 4c leading to a skewed minimum distance value in the diversity calculation set s 6 has almost three times the number of scenarios as set s 1 which contributes to its low diversity calculation several close proximity scenarios could be omitted to make set s 6 more diverse this would not improve the performance as the set would lose valuable training data and potential for overfitting would increase therefore the high diversity of set s 1 is not the only factor controlling the set s good performance its high baseline regret values will enable the policy search to find solutions more robust to vulnerable conditions additionally there may be many other scenario properties that are not examined in this study which contribute to set performance and scenario training value these include hydroclimatic properties such as temperature rise flood frequencies flow timing precipitation drought patterns soil moisture and evapotranspiration because the high regret training set performs no worse than training to all scenarios the strategy of designing a training set around scenarios with high baseline regret may serve to reduce the computational cost of policy search for large ensemble cases and or to reserve more scenarios for testing to support this point table 2 compares the computational cost for different aspects of policy training in this study training to scenarios with high baseline regret which includes the perfect foresight optimizations required 9733 computing hours roughly three times less than training to all scenarios training to scenarios with high baseline regret improves the efficiency of policy search without sacrificing robustness relative to the case of training to all scenarios this denotes the benefit of analyzing the hydrology and baseline regret of scenarios before a train test split is determined thus it is also possible to determine the conditions under which a high baseline regret set will give computational benefits by generalizing the requirements outlined in table 2 this condition is described as 20 f p ρ p η f r ρ f a ρ a where η represents fraction of overall scenarios which are in the high regret set f p f r and f a denote the number of function evaluations and ρ p ρ r and ρ a denote the number of random seeds for each of the perfect foresight high regret only and all training scenario sets respectively this generalization can potentially be applied to other planning problems in which the baseline regret is determined a priori and where there is a choice about how many high regret solutions to include in the training set this analysis has important implications for the generalizability of this approach several variables may be degrees of freedom for instance numbers of random seeds ρ and function evaluations f necessary for convergence to diverse and near optimal pareto solutions sets will vary across models the fraction of high regret scenarios η may differ based on the number of clusters chosen in some instances if the level of baseline regret is not a significant source of variation among scenarios it may not provide a way of separating different training sets using a clustering approach furthermore differences in performance among training sets may be due to confounding factors not reflected in the abstracted scenario properties especially for hydrologic timeseries which can be summarized in a number of different ways however the proposed clustering and train test methodology is still generalizable across environmental planning applications to pinpoint the most important scenario properties for policy training and out of sample performance therefore discovering conditions for computational benefits furthermore results presented in figs 5 and 6 must be interpreted in light of the fact that the future climate trajectory is uncertain it is likely that more information about future hydrology will be collected over time and this process could complement policy search methods in the context of dynamic planning e g hui et al 2018 fletcher et al 2019 therefore in this study the methodology aims to identify a training strategy that leads to robust outcomes to both uncertain and clustered future climate measured according to multi objective performance bounded by the baseline policy and perfect foresight cases we find that training to scenarios with high baseline regret is competitive with training to all scenarios across a range of future climates and often leads to the best out of sample performance this is likely due to higher inter annual variability in these scenarios based on a higher diversity of extreme events across individual scenarios and potential poor baseline performance in the high regret cluster solutions will give both a wider variety of tradeoffs in objectives and improvements relative to baseline policy performance these findings extend to both wet and dry futures where the inclusion of high regret scenarios in the training set outperforms using exclusively either wet or dry training scenarios this result links to the importance of evaluating perfect foresight policies in individual scenarios when designing the training set to establish an upper bound for system performance 4 3 policy analysis the final step of the analysis is to determine what specific adaptations are implemented by the robust policies this analysis focuses on six specific train test splits chosen based on their high ranking performance 1 policies trained on set s 4 and tested on set s t 2 2 policies trained on the set s 5 tested on set s t 3 3 4 policies trained on set s 1 tested on sets s t 2 and s t 3 and 5 6 policies trained on set s 6 tested on sets s t 2 and s t 3 the average performance measures across all scenarios for these sets are shown by the highlighted solutions on the parallel axis plots in fig 7 while the expected value of all highlighted solutions dominates the baseline policy there are still several significant tradeoffs between the objectives indicated by their nonlinear correlations see section 3 in supplementary material for the s 4 s t 2 train test combination these include statistically significant tradeoffs between hydropower and flooding ρ 0 53 and water supply and delta outflow ρ 0 97 the same tradeoffs exist in the dry test scenarios which also exhibit tradeoffs between carryover storage and flooding ρ 0 49 in general these relationships reflect the fact that higher storage levels benefit several of the proposed objectives although they can be detrimental to the flooding objective which is to be minimized while this high water elevation benefits the hydropower and carryover storage objectives it can induce larger releases if large storms occur later in the spring we have shown that training set s 4 high regret low regret wet will yield the best performing policies for the low regret wet test set s t 2 fig 6b likewise training set s 5 high regret low regret dry will yield the best performing policies for the low regret wet test set s t 3 fig 6b this is reflected in fig 7 a b c e f g where the highlighted pareto solutions for s 4 and s 5 are shifted higher than s 1 and s 6 over their particular test sets as shown by the higher maximum percent of baseline values for the flood objective lower minimum in fig 7a e these ranges in fig 7 a b e f also reflect the better performance of the high regret training set s 1 over the low regret wet dry training set s 6 for both test sets we next examine the four compromise policies that balance the tradeoffs in performance measures denoted as the s 4 s 5 s 1 and s 6 policies in fig 7 the s 4 and s 5 policies coming from the most robust training sets for the respective test sets also give the best performance on the individual scenarios fig 7d h the alternatives that these policies employ are shown in fig 8 along with a comparison to the decision variables of the baseline policy each column in the tables represents the decision variable which occurs for that specific water year type in the s 4 policy shasta and folsom reservoirs have higher maximum allowable curtailments c m a x w y t r than in the baseline policy these higher maximum curtailment levels will allow for increased hedging of releases the curtailments for oroville reservoir are higher in wet above and below normal years but lower in dry and critical years all three reservoirs also have a flood pool shift of at least 10 days forward in the water year for the s 4 policy in wet and above normal years shasta and oroville use low z w y t r values indicating a very conservative forecast with a high exceedance level in drier water year types the z w y t r values are generally close to or greater than the baseline exceedance levels for folsom reservoir these values vary much more across water year types the differences between operational adaptations at each reservoir highlight the complexity of managing the multi reservoir system and the potential to design adaptations for system wide benefit fig 9 shows the system dynamics of the baseline policy compared to the compromise policies in a time series over one scenario from each corresponding test set an rcp 8 5 scenario cnrm cm5 for low regret wet and an rcp 6 0 miroc5 scenario for low regret dry under the baseline policy reservoir storage levels are vulnerable to snowmelt loss regardless of water year type evidenced by low storage levels in the irrigation season even in wetter years the s 4 policy mitigates this vulnerability via an intra annual hedging resulting in higher reservoir storage during the early irrigation season may june the s 5 policy functions similarly for both policies this intra annual hedging dynamic is supported by the adapted snowpack to streamflow forecasts where underpredictions will cause some release curtailments to conserve for potential low inflows later in the season however curtailments can be partially avoided with higher carryover storage due to the flood pool shift this seasonal shift is also reflected in the delta exports fig 9d h which maximize total volume by shifting throughout the year the remaining s 6 and s 1 policies also exhibit the intra annual hedging strategy see section 4 in supplementary material for these policies decision variables however given that reservoir storage becomes higher in the flood season and carryover storage drops lower when these policies are deployed they are slightly less effective fig 9b c f g additionally they often will have periods of low delta exports fig 9d h this highlights that a policy from the best performing training set for a particular test set may be more likely to give better performance for scenarios in that test set there are two major differences between the s 4 and s 5 policies stemming from the hydrologic properties of their respective training scenarios the first is that the s 5 policy tends to curtail releases more during dry and critical years reflected in its conservative forecasts and high maximum curtailment allowances the s 4 policy hedges less during dry and critical years and instead relies on larger storage brought about by intra annual hedging this is further driven by the low maximum curtailment allowances for oroville during these water year types the second difference is that the s 4 policy tends to hold less storage during the flood season than the baseline policy while the s 5 policy does not for the s 4 policy this makes curtailment less necessary later in the mid to late summer and reduces flood vulnerabilities the fact that policies exist that can improve upon both of these objectives via the same policy parameters is the main reason why flood control and carryover storage do not have a significant tradeoff in the wet test set in summary analysis of these two compromise policies shows how training to scenarios with high baseline regret can yield policies with improved performance on out of sample hydrology to balance conflicting objectives 5 conclusions this study advances the design and testing of robust control policies as an adaptation to uncertainty in environmental planning problems contributing an experimental design to better understand the influence of the forcing scenario properties and baseline regret of training scenarios on the robustness of resulting policies we demonstrate this approach for the northern california reservoir system to determine how transient downscaled climate scenarios impact tradeoffs between water supply flood control environmental flows and hydropower generation results indicate that policies trained to scenario sets with high baseline regret tend to outperform those generated with other training sets in both wetter and drier futures additionally the policies adapted under these conditions develop an intra annual hedging strategy to mitigate the effects of snowpack decline under rising temperatures the approach highlights the general importance of considering the specific properties of training scenarios in the design of robust control policies beyond the pairwise comparison of train test splits this analysis also highlights the general difficulty of maintaining out of sample performance for reservoir control policies this is driven primarily by extreme events that occur infrequently by definition and which may be the result of natural variability rather than anthropogenic change creating a risk of overfitting to the training set the baseline regret based on perfect foresight optimization provides a measure of regret to place this performance degradation in context unlike the traditional minimax regret strategy where the alternative that minimizes the maximum regret across all scenarios is chosen our approach uses a regret metric to choose training scenarios rather than optimal alternatives we show that optimal policies benefit from training to sets of scenarios with a high regret for the baseline solution our methodology also provides a way to group ensembles of scenarios using an unsupervised learning approach along with other hydrologic properties including streamflow and snowpack to create an experiment which maps the relationship between training and test scenarios to the outcome of policy robustness considering both the performance and diversity of solutions the latter is particularly important given the concern with reversible adaptations to operations which can be changed over time herman et al 2020 while this study considers uncertainty in hydrology due to climate change across downscaled model projections it could further test the robustness of the resulting policies against more realizations of sampling variability from a synthetic generator or supplement the training set with the same increasing the number of scenario realizations would allow for additional hydrologic variables to be included in clustering such as changes in flood and drought frequencies and intra annual streamflow shifts additionally policy training might be improved with a more flexible policy structure beyond parameterizing the existing system such as a neural network though this may also increase the potential for overfitting due to increased degrees of freedom policy training can also be coupled with infrastructure design e g bertoni et al 2020 which in many regions will be required to cope with the more extreme projections of hydrologic change lastly while our approach is demonstrated with an exampled from the water resources management field it can generalize to any environmental natural resources or infrastructure planning problem which includes a no action case an optimization component and a forcing scenario ensemble future work should explore the impacts of these additional experimental components in combination with the analysis of the training scenarios properties presented here to further improve robust policy search under uncertainty software availability code for operations of reservoir in california orca a python simulation model is available at https github com jscohen4 orca simulation code data analysis and figure scripts for this manuscripts are available at https github com jscohen4 orca tree cohen 2021 properties training scenarios processed cmip5 climate projection data files used in this study are available at https github com jscohen4 orca cmip5 inputs declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was partially supported by the u s national science foundation grant cbet 1803589 and infews grant cns 1639268 any opinions findings and conclusions are those of the authors and do not necessarily reflect the views or policies of the nsf we further acknowledge the world climate research program s working group on coupled modeling and the climate modeling groups listed in the supplement of this paper for producing and making available their model output appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105047 
25818,a new open source multi gpu 2d flood model called triton is presented in this work the model solves the 2d shallow water equations with source terms using a time explicit first order upwind scheme based on an augmented roe s solver that incorporates a careful estimation of bed strengths and a local implicit formulation of friction terms the scheme is demonstrated to be first order accurate robust and able to solve for flows under various conditions triton is implemented such that the model effectively utilizes heterogeneous architectures from single to multiple cpus and gpus different test cases are shown to illustrate the capabilities and performance of the model showing promising runtimes for large spatial and temporal scales when leveraging the computer power of gpus under this hardware configuration communication and input output subroutines may impact the scalability the code is developed under an open source license and can be freely downloaded in https code ornl gov hydro triton keywords 2d flood model open source multi gpu high resolution shallow water equations 1 software availability name of the software triton two dimensional runoff inundation toolkit for operational needs contact address oak ridge national laboratory 1 bethel valley road tn 37830 usa tennessee technological university 1 william l jones dr cookeville tn 38505 usa email moraleshern2 ornl gov akalyanapu tntech edu language cuda c hardware desktop laptop or clusters of cpus gpus software nvidia cuda toolkit netcdf optional availability https code ornl gov hydro triton year first available 2020 2 introduction with the increasing frequency and intensity of extreme hydrologic events in a changing environment madsen et al 2014 faster and more accurate inundation models are particularly important tools for flood risk management when translating the amount of rainfall either from weather forecasting models or from rain gauge observations all the way to surface inundation coupled hydrologic hydraulic models should be used where the flood inundation model represents the last mile that simulates flood wave propagation across high resolution terrain based on simulated runoff and streamflow from hydrologic models xia et al 2019 flood inundation models can provide not just streamflow discharge but the temporal evolution of flood location depth and movement however this information is not always directly translated into practical instructions to better support operators and emergency responders for more rapid decision making spiekermann et al 2015 in this regard collaborative flood modeling is a great example for overcoming this challenge where end users stakeholders and scientific knowledge meet and interact evers et al 2012 maskrey et al 2016 specifically the use of high resolution maps built from flood inundation models together with non technical terminology have proven to benefit decision making for end users sanders et al 2020 beyond this methodology integrated platforms using hydraulic modeling linked with live geospatial information remote sensing flood visualization and community intercommunication xie et al 2017 wang et al 2019 and other strategies such as probabilistic flood maps based on flood models and the value of information voi alfonso et al 2016 have been also demonstrated to reduce the uncertainty and assess the consequences of actions taken by decision makers despite the integrated flood risk management tools large scale operational hydrodynamics models are still rarely used mainly because of their large computational cost simulation models must be particularly fast on the order of minutes to meet decision making needs resulting in the reliance on simplified and analytical models one example is the integrated version of height above nearest drainage hand within the us national water model nwm liu et al 2016 2018 hand is based on manning s equation some geometric extrapolations and precomputed rasters that are used to generate national flood maps at 10 m resolution its european equivalent within the efas european flood awareness system is lisflood fp bates et al 2010 a spatially distributed rainfall runoff routing model its operational version neal et al 2012 wing et al 2018 uses the runoff generated at a 5 km grid spatial resolution as an input for a simplified shallow water model in which the convective term is neglected flooding maps are derived from a catalog of flood hazard maps made of precalculated runs of the lisflood fp model the main limitation of these models is the range of applicability for all types of flows and situations which are restricted by the simplification hypothesis assumed and consequently the validity and accuracy of the results one of the most complete inundation frameworks is the 2d full shallow water system garcía navarro et al 2019 which solves mass and momentum equations and provides water depth distribution and an accurate surface velocity field the resolution of the 2d shallow water equations is nevertheless computationally demanding at fine resolutions and there is no obvious path for them to scale up to continental or global scales for operational purposes two main efforts are highlighted in the last decades to overcome this constraint efficient numerical techniques and the adaptation to high performance computing hpc first numerical improvements are of crucial importance to reduce the computational burden in the context of augmented riemann solvers a careful numerical estimation of topography and friction terms has been demonstrated to improve the performance and accuracy of the solution murillo and garcía navarro 2010 2012 murillo and navas montilla 2016 local time step methods sanders 2008 dazzi et al 2018 are considered as a worthwhile solution to improve the computational efficiency at the extra cost of having to deal with different stages for flux and source computations depending on the time level as for the spatial discretization leaving aside adaptive mesh refinement techniques discontinous galerkin dg schemes as the one proposed in kesserwani et al 2018 seem to deliver high quality solutions with desired scalability properties on the same page a well balanced no neighbor method has recently been proposed for the 1d saint venant equations hodges and liu 2019 although the extension to the 2d framework is not clear this family of schemes could open the door to new efficient algorithms on the other side parallel implementations and the use of hpc on new architectures of modern supercomputers have become a fundamental requirement to study increasingly complex problems at large spatial scale and high temporal resolution on water resources hydrodynamics morales hernández et al 2020 neal et al 2010 explored different parallel strategies of lisflood fp across various types of architectures stressing the efficiency difficulty and estimated development time of each implementation the great majority of models are nevertheless implemented on a single hardware type one of the best exponents is primo sanders and schubert 2019 a raster based subgrid flood model able to run on clusters of central processing units cpus using both shared and distributed memory however the use of graphical processing units gpus has become a new trend in the recent decade sætra et al 2012 castro et al 2011 kalyanapu et al 2011 brodtkorb et al 2012 in the gpu context the spatial discretization and efficiency have been studied for regular vacondio et al 2014 and irregular grids petaccia et al 2016 garcía navarro et al 2019 echeverribar et al 2019a seeking efficiency with i different solvers de la asunción et al 2013 ii different memory access patterns and mesh ordering algorithms for unstructured grids lacasta et al 2014 iii block uniform quadtree and adaptive mesh refinement on nested rectangular meshes vacondio et al 2017 and iv a local time stepping scheme to improve performance dazzi et al 2018 additional capabilities have been also reported and accelerated with the use of gpus rainfall runoff applications lacasta et al 2015b aureli et al 2020 sediment transport and erosion processes juez et al 2016 caviedes voullieme et al 2017 dazzi et al 2019 landslides lacasta et al 2015a hydraulic structures modeling echeverribar et al 2019b dazzi et al 2020 and transport of contaminants and water quality models viñas et al 2013 garcía feal et al 2020 multi gpu models could be the solution to improve the tradeoff among accuracy speed and large scale domains the initial work by sætra and brodtkorb sætra et al 2012 studied both weak and strong scaling and the effect of synchronizations of 2d shallow water equations using a 4 gpu machine xia et al 2019 used a framework called hipims to model a storm in a 2500 k m 2 catchment using 8 gpus and 100 million grid cells around 2 5 times faster than real time in turchetto et al 2018 2020 the equations are discretized in a buq quadtree grid following the proposed scheme of vacondio et al 2017 although this type of spatial discretization could be convenient to improve performance with respect to regular cartesian grids and to address high resolution problems the domain decomposition increases complexity and could add an overhead in large scale simulations in sharif et al 2020 the authors compare two versions of solvers for the shallow water equations finite difference versus finite volume weak and strong scaling up to 272 million grid cells are analyzed together with the cuda aware mpi feature designed to optimize communications between the different sub domains triton is presented here as the first to date multi architecture multiple cpus and gpus open source 2d hydrodynamic flood model based on the resolution of full shallow water equations with source terms different free or open source 2d models can be found in the literature besides the lisflood model bates et al 2010 neal et al 2012 and the well known hec ras model 2d version released in 2016 other cpu based or multi cpu models such as fullswof delestre et al 2017 and brezo sanders et al 2010 solve the 2d shallow water equations in structured and unstructured triangular grids respectively delft3d deltares 2014 also permits 2d flow computations for different applications ranging from hydrodynamics to sediment transport and water quality few gpu models are freely available geoclaw berger et al 2011 for certain applications qin et al 2019 iber for 2d flood modeling garcía feal et al 2018 and anuga enabling the gpu offloading using pycuda weng and strazdins 2014 triton has been nevertheless designed for a multi architecture paradigm and is able to run on several configurations including single or multiple cpus and single or multiple gpus using a combination of open multi processing openmp compute unified device architecture cuda and message passing interface mpi in addition to this architectural flexibility all computing subroutines are programmed only once regardless of the hardware type minimizing error sources and bolstering the software portability finally a simple input output configuration is implemented that would avoid significant geographic information system gis pre and post processing as an example a digital elevation model dem is directly used as the computational mesh circumventing the necessity of site specific mesh building as with most existing models these features would eventually enable the use of 2d hydrodynamic models for operational purposes and other applications that were not feasible before in addition to introducing this triton as a fast and flexible open source suite to simulate both pluvial and fluvial flood events together with some freely available pre and post processing tools this work aims to answer the following research questions 1 is it possible to simulate large temporal and spatial scales in the order of minutes using a hydrodynamic model based on the solution to the full 2d shallow water equations which are the most appropriate architectural and parallelization strategies required to achieve this 2 does spatially distributed rainfall runoff have an impact on the results of predictive hydrodynamic models 3 could communication and i o times represent a bottleneck for large scales the paper is organized as follows after describing triton including the equations numerical scheme and the hpc implementation the software features are presented three test cases are included to demonstrate the capabilities of the model showing the accuracy and performance of triton on different configurations and architectures including the simulation of the multi day event of hurricane harvey over harris county in texas on 384 gpus using summit supercomputer at oak ridge national laboratory the trade offs among parallel computation communication and input output are also analyzed revealing the importance of the latter two for large temporal and spatial scales 3 the triton model triton the two dimensional runoff inundation toolkit for operational needs is a physically based hydrodynamic model that solves the 2d shallow water equations on a structured cartesian grid based on the initial gpu model developed by kalyanapu et al 2011 a new conservative numerical scheme has been implemented and integrated in an updated framework able run on multiple architectures 3 1 governing equations the 2d shallow water equations express the depth averaged conservation of mass and momentum in x and y directions of the space they can be written in a compact differential conservative form as presented in eq 1 1 u t f x g y s r s b s f u h q x q y f q x q x 2 h 1 2 g h 2 q x q y h g q y q x q y h q y 2 h 1 2 g h 2 s r r 0 0 s b 0 g h z x g h z y s f 0 g n 2 h 7 3 q x q x 2 q y 2 g n 2 h 7 3 q y q x 2 q y 2 the vector u represents the conserved variables i e the unknowns of the system and includes the water depth h l and the x and y unit discharges called q x l 2 t 1 and q y l 2 t 1 respectively eq 1 also contains the fluxes of these conserved variables f and g being g l t 2 the gravity acceleration and the source terms the latter encompass runoff terms s r expressed according to the runoff rate r l t 1 bed slope terms s b accounting for the gradient of the elevation z l and friction terms s f modeled by means of gauckler manning s law in terms of the manning s roughness coefficient n t l 1 3 here runoff refers to the effective rainfall i e total rainfall minus losses due to infiltration abstraction and evapotranspiration plus baseflow that are typical outputs from hydrologic or land surface models 3 2 numerical scheme a finite volume upwind explicit scheme is used to solve eq 1 in a squared cartesian mesh of grid spacing δ x an augmented roe aroe solver is implemented based on murillo and garcía navarro 2010 echeverribar et al 2019a for the fluxes and bed slope source terms nevertheless a different estimation of bed slope source terms at each edge are proposed in this work this treatment also ensures the positivity of the solution without reducing the time step size for the sake of clarity the derivation of this part of the scheme can be found in appendix a friction terms are discretized using a local implicit formulation xia and liang 2018 that does not alter the explicitness of the scheme accordingly a two step algorithm is proposed for the update of a cell i from time t n to time t n 1 t n δ t 2 u i u i n δ t δ x k 1 4 m 1 3 λ λ λ α β b e m k n u i n 1 f u i n u i r i n δ t where at each interface k α and β b are the fluxes and bed slope source term linearizations minus superscript accounts for the upwind discretization and λ and e are the eigenvalues and eigenvectors of the system of equations respectively see appendix a the localized runoff rate is denoted by r i while function f stands for the friction discretization written as 3 f 1 h f 2 q x 1 1 4 s f 2 s f f 3 q y 1 1 4 s f 2 s f where 4 s f δ t g n 2 q x 2 q y 2 h n 7 3 the explicit character of the scheme restricts the time step size according to the courant friedrich lewy cfl condition 5 δ t cfl min i δ x q x h i g h i q y h i g h i cfl 0 5 where index i loops over the number of grid cells note that although formally the maximum wave speed should be estimated at the interfaces the cell values are used instead in this work notwithstanding this approach does not compromise the stability of the scheme in fact a value equal to or less than the size of the time step is selected using this formula but it simplifies and ultimately accelerates the computations providing simultaneously a new way to estimate some corrections for the source terms see appendix a 3 3 hpc implementation increased problem complexity motivates heterogeneous hpc for hydrodynamics codes in the new era of parallel computing morales hernández et al 2020 computation time can be reduced effectively with the use of clusters of cpus and gpus on demand cloud workstations are also becoming more popular and affordable allowing the simulation of larger spatial and temporal domains at finer scales varied programming paradigms have arisen as a consequence of this heterogeneity open multi processing openmp for multicore cpus message passing interface mpi for clusters compute unified device architecture cuda or open computing language opencl for gpus for that reason triton has been designed as a multi architecture single code base able to run on the following platforms through specific compilation instructions 1 multi core shared memory platform using openmp 2 multi node cluster using mpi or mpi openmp 3 single node gpu machine using cuda 4 multi node gpu cluster using mpi cuda the simplified flowchart for the current implementation is depicted in fig 1 a after reading and parsing the input data see section 4 1 the domain is decomposed into different subdomains according to the desired number of mpi sub tasks then the simulation starts and runs until the time reaches the final simulation time writing the output information described in section 4 2 each output interval at each time state the time step size is first computed according to eq 5 then the computing kernels gpu or subroutines cpu are executed merely accounting for the numerical scheme in eq 2 finally the information is exchanged between the corresponding subdomains details of the later three processes along with the mpi decomposition are explained below 3 3 1 domain decomposition and halo exchange domain decomposition is an important factor for large scale parallelization using mpi libraries or similar in design of portable and scalable communication between subdomains although there are different ways to partition the information a 1d row wise decomposition is applied here for simplicity morales hernández et al 2020 as the stencil for the numerical scheme eq 2 involves neighbouring data see fig 9 a appendix a the information has to be exchanged each time step and overlap computations are performed fig 1 b shows a sketch of the row wise approach where north south communication is required and the halo size is the number of columns of the whole domain a two step non blocking algorithm is implemented using mpi isend and mpi irecv with the aid of mpi wait to wait for an mpi request let n be the number of ranks e g the number of partitions of the domain first all subdomains n n will send the halo data to the subdomain n 1 and receive from subdomain n 1 then vice versa each subdomain n 1 will receive information from subdomain n 1 and send to subdomain n 1 the subdomain exchange imposes a slightly greater level of complexity when dealing with a gpu implementation due to memory allocation as computations are performed by a gpu data are fully allocated in the device gpu memory however the regular mpi calls require pointers to host cpu memory which requires an additional data copy between host and device in addition to an extra host memory allocation this might decrease the performance of the model the cuda aware mpi is used for halo exchange to overcome this challenge of possible performance degradation cuda aware mpi allows gpu to gpu direct communication via network bypassing the cpu if underlying hardware supports this technology specifically summit supercomputer has gpu direct communication support and the use of cuda aware mpi has resulted in improved triton s performance the impact of using cuda aware mpi versus the conventional approach has been already studied in sharif et al 2020 see results fvs and fvg for a primitive version of triton for the sake of flexibility triton supports both approaches allowing users to choose a preferred implementation according to their system requirements 3 3 2 time step size computation in order to guarantee a stable and reliable solution at every time stage δ t is limited according to eq 5 which requires the computation of a global minimum time step size that will be imposed to evolve the solution in time to do that each subdomain computes first its own local minimum time step size for all its cells again two different implementations can be distinguished here depending on the architecture the openmp approach is rather simple since it only consists of the computation of a reduction operation the equivalent reduction operation in gpu is more complicated although some cuda standard libraries cublas thrust or third party cub libraries can be used a reduction ad hoc function has been implemented in cuda in contrast to the existing libraries in which a global array of size the total number of cells is required and then perform the reduction operation we make use of shared memory and thread synchronization at each kernel where we calculate the time step size to launch multiple instances using a reduced global array of size thread block times smaller than the original size then the reduction operation is done over the global reduced array diminishing the global memory use improving memory coalescing and providing a better performance than the existing libraries for large scale problems once the local minimum time step size is computed the global minimum is found using mpi allreduce across all the processes sub tasks triton also offers the possibility of a constant time step size this implementation does not require any reduction operation making it easier nevertheless the accuracy and robustness of the results are not ensured since the maximum allowed time step size governed by eq 5 might be violated 3 3 3 kernel subroutine execution triton is written using c and cuda each computing module has been implemented as a cpu subroutine as well as a cuda kernel during compilation based on the computing platform appropriate computing modules are compiled a set of arrays with the same structure is allocated either in the cpu or in the gpu so that the difference between cpu and gpu execution only consists of the memory where the data is defined and the kernel subroutine calls each kernel is then only programmed once avoiding duplicated information this fact improves the readability of the code at the same time as reproducibility and trustfulness between different architectures minimizing or almost eliminating eventual human mistakes usually made when porting the code from one to another architecture fig 2 depicts the source code for the wet dry kernel subroutine showing both the function call and declaration as shown the number of arguments required by the cpu and gpu versions are exactly the same although arrays reside either in the device or the host memory according to the chosen architecture some ifdef else and endif directives allow to switch between cpu and gpu in compilation time 4 software features triton is a unix based model targeting laptops desktops and optimized for supercomputers leveraging the current power of workstations the code pre post processing tools and some samples can be found in https code ornl gov hydro triton the main features and tools are explained below 4 1 input data the mandatory optional triton input files include configuration mandatory a text file containing the path of all input files output interval and format and all parameters and constants needed for simulation e g number of streamflow sources external boundary conditions initial and final time cfl number switches to enable disable observation point and checkpointing etc it can be configured either manually or with the aid of a config file tool generator topography mandatory triton is a dem based code consequently the mesh used for computation is a cartesian square grid obtained directly from the dem file i e avoiding the ad hoc and site specific task of computational mesh building dems follow the esri raster file format both ascii and binary formats are allowed although binary format is recommended for large spatial domains that contains a header section with the number of columns rows origin coordinates and cell size as well as a matrix of elevation values nodata values are not allowed in the current version of triton streamflow hydrograph optional streamflow hydrograph is one of two possible hydrologic inputs to triton streamflow here refers to the point discharge typically from upstream incoming river channels sources when selecting streamflow hydrograph two files are required an x y coordinate list with the location of all inflow sources and a streamflow hydrograph table including the time in hours and the timeseries of discharge in cubic meters per second at each source j q j t l 3 t 1 the discharge is introduced in triton as a single mass release runoff hydrograph optional runoff hydrograph is another possible hydrologic input to triton it is a common output from various hydrologic and land surface models immediately before such information is used for streamflow routing by involving runoff in triton the model can simulate local pluvial floods and hence increases its functionality when selecting runoff hydrograph two files are also required the runoff regions corresponding to the coarser hydrologic model grids are defined in the form of a matrix raster map with the same format with the dem file but without the header each distinct area is labeled with a non negative integer number that serves as a unique region identifier links to the runoff hydrograph table as in the streamflow hydrograph table the runoff hydrograph table contains the time in hours and timeseries of runoff rate r in mm per hour from all declared runoff regions a sketch of the rainfall runoff input files is depicted in fig 3 roughness optional the surface friction is represented by manning s roughness coefficient n it can be provided either as a constant number specified in the configuration file and applied globally to the whole domain or in the form of a matrix raster map without header matching the number of dem grid cells the latter choice allows the user to define spatially varied roughness coefficients to represent the site specific land use and land cover conditions external boundary conditions optional the boundaries of the domain north east south and west can be provided with five type of external boundary conditions by default all boundaries are closed i e water flux is prevented from exiting the domain four alternative flow conditions can be then be imposed type 0 zero gradient the original setup in kalyanapu et al 2011 type 1 level versus time an additional file containing a table with the time and the timeseries of water level is mandatory type 2 normal slope the desired slope is required type 3 froude number the froude number defined as f r q h g h is needed to be imposed across the external boundary being q q x q y more than one external boundary condition can be defined at each boundary edge the user only has to provide the initial ending x y coordinates of the boundary and its corresponding boundary condition parameters according to the boundary type the external boundary conditions are imposed at the ghost cells surrounding the whole domain this implementation ensures the scalability of the solution for domain decomposition since no additional information has to be exchanged between the partitioned subdomains initial conditions optional a dry domain is the default initial condition however the user can also specify an initial condition for each conserved variable h q x or q y in the configuration file in the form of a matrix type file without header furthermore checkpointing hotstart is also allowed backup files are written during the computation to retrieve the simulation from the last state in case it is necessary point output optional in addition to the default matrix output at user specified time intervals triton also supports output of timeseries at user specified locations to avoid data processing for known points of interest such as locations with gauge observations an x y coordinate list file containing the location of the desired points is therefore required 4 2 output data the output from triton is intended to be easy to post process by standard graphing and gis tools two types of output data spatial and temporal are generated in a separate folder at each user specified interval matrix data spatial a snapshot of water depth and unit discharge is written in the form of a matrix type file without header either in ascii or in binary format the latter is recommended for large scale domains morales hernández et al 2020 additionally if the execution is run using domain decomposition either multi cpu or multi gpu a switch in the configuration file allows the user to choose the mode in which the data is written 1 sequential that gathers all subdomain information in a single file during the computation or 2 parallel that directly outputs subdomain data as separate files for the latter a separate script can then be used to combine all subdomain information into a single file during post processing water depth and unit discharge profiles temporal the timeseries of water depth and unit discharge at the specified point locations defined in the input files can be outputted in a single file for each variable fig 4 displays a style of output information that can be obtained it corresponds to the hurricane harvey test case studied in section 5 3 and condenses in a single panel the stage hydrograph for each predefined observation point as well as a 2d view of the whole spatial domain showing the topography and the maximum flooded area note that to be consistent with eq 1 triton outputs unit discharge rather than velocity the user should conduct proper conversion to estimate velocity based on unit discharge and water depth 4 3 other tools a suite of tools is provided with triton in order to simplify some pre and post processing tasks first some bash scripts are included for the conversion between binary ascii formats and sequential parallel mode they can be used for both input and output data additionally two programs containing a gui targeting windows and linux are included in the repository the first one is a netcdf converter that takes both binary and ascii files outputted from triton as input data and converts them into netcdf format the second gui software called config file tool generator allows the user to generate the configuration file mandatory input file for triton 5 test cases a set of test cases are proposed here to demonstrate certain desired characteristics such as consistency stability convergence and robustness and model capabilities as triton is intended to be a reference software for the computation of large scale flood problems reliability on the output results is mandatory the first test case is therefore included as a verification and to show how the model behaves on a classic literature test case that involves all kind of flows subcritical supercritical and sonic transitions and to test the accuracy and grid convergence of the model once the model is able to provide a robust and trustworthy solution the second test is designed to test the capability of a model to accept runoff hydrograph as a hydrologic input and illustrate its importance this feature also allows users to simulate the effects of local pluvial floods that occur due to highly intense local precipitation in a non floodplain region the final test case is oriented to evaluate the performance of the model on different architectures multiple cpus and gpus analyzing the possible bottlenecks in large temporal and spatial scales all test cases introduced below can be found in the repository additional test cases will be included as they are developed to provide users with further examples 5 1 test case 1 paraboloid bed topography this test case consists of a square domain 0 4 0 4 with a frictionless paraboloid topography the initial water depth condition is a planar surface with velocity in the y direction the domain boundaries are closed and after one period t 1t the numerical solution should recover the initial condition due to the rotating velocity field this configuration is considered to be a challenge test where some numerical methods fail when trying to reproduce it mainly due to an incorrect treatment of wet dry interfaces or a non balanced source terms fluxes implementation more details about the analytical periodic solution are described in delestre et al 2013 the package provided there is used to generate four different resolution grids δ x 0 04 0 02 0 01 0 005 m the tolerance h t o l see appendix a is set to 10 4 in this particular case without friction and cfl 0 45 three periods t 3t are simulated and the numerical results obtained by each grid resolution are compared to the exact solution understood as the projection of the analytical solution on each computational grid fig 5 depicts those comparisons at t 1t t 2t and t 3t accuracy decreases as time advances especially with lower resolution this is a consequence of the first order only numerical diffusion which is resolution dependent that said the use of higher order schemes is not recommended since both pressure and dissipative terms usually dominate over the convective terms with the roughness of realistic applications also the presence of wet dry discontinuities would downgrade the accuracy to first order so that the cost of the implementation is not justified nonetheless a correct estimation of the source strengths is mandatory to avoid dramatic reductions in the time step size and to ensure a correct well balanced implementation murillo and garcía navarro 2010 murillo and navas montilla 2016 echeverribar et al 2019a to provide a quantitative measure of the error made for each resolution and to check the convergence rate l 1 l 2 and l error norms are computed with respect to the analytical solution at t 3t table 1 contains those error norms computed as 6 l 1 y δ x 1 n i 1 n y n y e i l 2 y δ x 1 n i 1 n y n y e i 2 l y δ x max i y n y e i where n is the number of grid cells and y n and y e are the numerical and exact solutions respectively these errors confirm the consistency of the scheme for a first order method even though this test case involves water depth values on the order of centimeters millimeters or below together with strong velocities the scheme is demonstrated to be robust and accurate enough providing a solution free of oscillations which is particularly of interest when dealing with wet dry boundaries a key factor in 2d flood models 5 2 test case 2 runoff capability triton admits runoff hydrographs to provide spatiotemporally distributed runoff as an input typically a hydraulic model is driven by providing streamflow hydrographs at user specified locations to simulate riverine fluvial floods these locations can be where the observations of streamflow were made or can match a river routing model however the lack of sufficient streamflow source locations due to hydrologic model limitations or other factors may lead to the underestimation of flood extents especially at smaller tributaries to overcome this issue users can utilize the runoff function within triton as an example to illustrate this capability we select an area located upstream of allatoona reservoir in the northwest georgia us the region was previously modeled by gangrade et al 2018 gangrade et al 2019 to study probable maximum flood using the distributed hydrologic soil vegetation model dhsvm and flood2d gpu kalyanapu et al 2011 the 100 year peak streamflow was estimated at the outlet of the computational domain following the guidelines of bulletin 17b on water data 1982 and then used to prepare 100 year return level streamflow hydrographs for a detailed description readers are referred to section 3 1 of gangrade et al 2019 in triton the 100 year flood is simulated with two different model configurations scenario a no runoff using 13 streamflow source locations along the river network fig 6 a and scenario b with runoff using 2 upstream streamflow source locations and runoff simulated at various catchments fig 6 b the computational domain spans 358 sq km and consists of 400 000 grid cells at a 30m dem resolution in both scenarios a 5 day simulation is conducted with peak streamflow occurring at day 3 the water depth is written in raster maps every 30 min the maximum flood inundation extents for both scenarios compared against a benchmark dataset from fema 100 year flood zones are presented in fig 6 c and d respectively a qualitative comparison reveals that just using streamflow inputs scenario a fig 6 c may lead to the underestimation of flood extents especially in tributaries where no upstream streamflow inputs are specified in scenario b since the model is driven by runoff hydrograph across the entire domain it can better capture the flood extents especially in the tributary areas to evaluate the two scenarios quantitatively we use a metric called hit rate which is a measure of model tendency to accurately predict the benchmark flood extents gangrade et al 2019 wing et al 2017 using fema 100 year flood plain as a benchmark we obtain a hit rate of 0 50 for scenario a and an improved rate of 0 68 for scenario b not only demonstrating a better floodplain simulation local pluvial floods can also be simulated in fig 6 d the results showcase the advantage of using runoff for better and easier flood regime simulation 5 3 test case 3 hurricane harvey the last test case is intended to evaluate the performance and applicability of triton using different architectures multiple cpus and gpus in a realistic configuration in this scenario we simulate the massive flood that hurricane harvey caused in the summer of 2017 along the us gulf coast the spatial domain encompasses around 6800 k m 2 of harris county texas us the simulation covers ten days with the heaviest rainfall occurring during day 7 to day 9 the runoff data are generated using the variable infiltration capacity vic hydrologic model liang et al 1994 driven by hourly 4 km radar based stage iv quantitative precipitation estimate this runoff is then routed using the routing application for parallel computation of discharge rapid model david et al 2011 through the river network to generate streamflow input at 69 inflow source locations information from the us geological survey national elevation dataset is used to build a corrected dem in which in we included addicks and barker s reservoir capacities incorporated the bathymetry of the main river segments and eliminated some noise present in the original data the dem resolution is 10m with a total of around 68 million grid cells and the manning s roughness coefficient is set to a constant value of n 0 035 s m 1 3 the output is configured in sequential binary format with an output interval of 1800s more information about this test case can be found in dullo et al 2021 the purpose of this test case is to evaluate the efficiency and scalability of the model on different hardware configurations simulations are carried out on summit supercomputer at oak ridge national laboratory summit is comprised of 4608 compute nodes each of them containing six nvidia volta v100 gpus and 42 physical cores 168 hardware cores using simultaneous multithreading in order to make a fair comparison between multiple architectures a summit compute node is chosen and seven numerical experiments are performed using up to 64 nodes two triton hardware configurations are used multiple cpus using mpi openmp and multiple gpus using mpi cuda each of them with the aim of minimizing the number of mpi tasks per configuration consequently the multi cpu version uses one mpi task per node while the multi gpu version uses one mpi tasks per gpu cuda aware mpi is used for the multi gpu simulations table 2 condenses both hardware configurations as well as displays the runtimes following sharif et al 2020 we also define the following metrics billion lattice updates per second blups and speed up 7 blups n c n t s t n 10 9 speed up t 1 t n where n c is the number of grid cells n t s is the number of time steps done and t n is the runtime achieved using n computing nodes the speed up in this case measures how fast the simulation is compared to the runtime using 1 node fig 7 left depicts the runtime in log scale y axis and the number of blups x axis achieved by each hardware configuration plotted in circles of different sizes the main result is that a very large problem for a serial code can be computed more reasonably with a multi gpu system with runtimes of less than 30 min even with the lowest hardware configuration 1 node 6 gpus the multi gpu version is able to achieve a lower runtime than the most demanding multi cpu hardware configuration 64 nodes 10752 openmp threads accordingly the maximum blups achieved by the multi cpu system is still lower than the 1 node multi gpu configuration additionally given a fixed number of summit nodes the average blups ratio between the multi gpu and multi cpu versions is 25x with a maximum of 45x for 1 node revealing the convenience of using this architecture the circle series turns horizontal when the strong scaling limit is reached and the asymptote is representative of the absolute fastest runtime expected for each configuration in this problem with this the multi cpu system has not reach its maximum using 64 nodes while the multi gpu version tails off significantly beyond 8 or 16 nodes fig 7 right shows in log log scale the speed up relative to the simulation using 1 node y axis as a function of node count x axis compared with the perfect scaling as shown the multi cpu version is able to achieve a satisfactory scaling factor up to 32 nodes 32 mpi tasks although a performance hit is observed for 64 nodes this might be due to the row wise 1d mpi partitioning on the other hand the multi gpu speed up does not scale acceptably although 6 mpi tasks are used per summit node 1 per gpu a lower speed up is observed even for 32 mpi tasks extrapolation between 4 and 8 nodes where adequate speed ups were achieved for the multi cpu version many factors are responsible for this first this test case does not have runoff as an inflow and only around 20 as an average of the domain is wet a loss of efficiency is therefore caused by thread divergence due to an if statement implemented in the code to avoid the computations on dry cells this is an issue reported for cuda and it is not the case with openmp since each cpu core runs like a single threaded subroutine executing its own independent set of instructions additionally a static mpi subdomain partitioning aggravates this fact since some subdomains could not have a significant computational burden resulting in an imbalance among mpi processes morales hernández et al 2020 to investigate other sources of scaling limitations in the multi gpu system the computation time gpu the mpi communication time the i o time and the rest are plotted in fig 8 two additional simulations are carried out using 1 and 3 gpus the left panel shows in log log scale the absolute number in minutes while the right plot displays the percentage of the time consumed by each process against the gpu count note that memory copies between the host and the device are counted within the gpu time and the other time stands for subroutine calls and basic operations run on the cpu as observed on the left plot the gpu time is reduced at almost the same rate for each configuration meaning that it actually scales according to the number of gpus a slightly lower rate is detected for the last two values 192 and 384 gpus due to the low number of grid cells per gpu each gpu does not have enough work to leverage its computing power i o and other cpu time remain almost constant for all configurations as expected communication time using a blocking algorithm and cuda aware mpi does not increase noticeably as the number of gpus grows indicating that communicating every time step carries significantly more weight than the number of gpus the right panel demonstrates that mpi and i o times govern this problem for large gpu count in particular from 40 to 66 of the time in this test case is consumed by these processes at 96 gpus and beyond this fact suggests that the operational scale bottleneck is not with computation but on communication and i o tasks parallel i o should improve these results and a more efficient communication strategy should be designed these results motivate even larger test cases to leverage the massive parallelization in a substantial number of gpus paving the way to even larger temporal and spatial scales 6 conclusions and perspectives a new open source 2d flood model triton is available to run on heterogeneous architectures using single and multiple cpus and gpus to enable hydraulic computations at large temporal and spatial scales it consists of a simple input file structure meshless standard gis formats and a configuration text file and provides spatiotemporal information of water depths and velocities as output information three test cases have been provided to demonstrate the triton capabilities the accuracy consistency and robustness of the scheme has been proved by means of the paraboloid test case the importance of runoff capability in triton has been highlighted using a flood test case in northwest georgia us results with and without runoff have been compared against the 100 year flood extension provided by fema revealing the benefit of considering the runoff capability for this sort of studies finally the hurricane harvey test case has been used to show the performance on the scheme on different architectures multi cpu using openmp mpi and multi gpu with cuda mpi the numerical experiments highlight the convenience of using the multi gpu version against the multi cpu achieving a low runtime less than 30 min for real world configurations a large spatial domain at 10m resolution and a 10 day hydrograph and unlocking operational purposes at even larger spatial and temporal scales although the runtimes are promising the scalability of the multi gpu version is nevertheless unsatisfactory for large number of gpus many aspects are responsible for this thread divergence for dry cells poor load balancing between mpi ranks with a static decomposition and principally the execution time consumed by i 0 and mpi communication for a large scale problem with respect to the computation time although these times in the order of minutes each are almost constant and does not depend on the number of gpus they represent a great percentage of the total runtime when trying to achieve operational purposes future perspectives are therefore aimed at designing optimized i o parallel algorithms and exploring new communication techniques such as a 2d mpi decomposition or overlapping strategies particularly the latter would considerably improve the performance as communication between subdomains would not be required to be every time step triton is under continuous development future planned improvement includes besides new efficient i o and communication algorithms support for other gpu libraries such as openacc all of these efforts will be regularly documented in the repository we provide this baseline to enable the use of a fully 2d hydraulic model for new science questions such as uncertainty quantification or climate change problems beyond the studied temporal and spatial scales heretofore the extension to very high resolution continental global scale flood modeling will require multi gpu computations so new algorithms are obviously planned to extend these capabilities to operational scales declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this research was supported by the us air force numerical weather modeling program this research used resources of the oak ridge leadership computing facility at the oak ridge national laboratory which is a us department of energy doe office of science user facility some of the co authors are employees of ut battelle llc under contract de ac05 00or22725 with the us department of energy accordingly the us government retains and the publisher by accepting the article for publication acknowledges that the us government retains a nonexclusive paid up irrevocable worldwide license to publish or reproduce the published form of this manuscript or allow others to do so for us government purposes doe will provide public access to these results of federally sponsored research in accordance with the doe public access plan http energy gov downloads doe public access plan appendix a numerical scheme the derivation of the numerical scheme from 1 to 2 is detailed here first integrating the hyperbolic system of equation 1 in ω i t n t where ω i is referred to the i th computational cell of the domain and t t n δ t and applying the gauss theorem 8 t n t ω i u t d s d t t n t ω i e d s d t t n t ω i s b d s d t where e f g applying the gauss theorem and replacing the contour integral by the sum across its four edges cartesian square grid of size δ x 9 t n t ω i u t d s d t t n t k 1 4 e n δ x d t t n t ω i s b d s d t with n n x n y the outward normal direction assuming a piecewise discretization and a roe s solver with an upwind discretization of fluxes and source terms the updating of the conserved variables can be written in flux difference splitting form as hubbard and garcia navarro 2000 10 u i u i n δ t δ x k 1 4 δ e n h b n k n δ e n k p λ a k h b n k p λ λ b b k the meaning of this expression is simple the conserved variables u h q x q y at each grid cell i will be updated according to the in going contributions that come from its four edges k east north west and south shared by the four neighbouring cells j a sketch of the scheme is shown in figure a 9 a fig 9 stencil for the proposed numerical scheme a and approximate solution for λ 1 0 λ 2 λ 3 0 b fig 9 equation a 3 includes the matrices and vectors coming from roe s linearization p is the matrix containing the three right eigenvectors e k of the jacobian matrix λ represents the diagonal matrix of the three eigenvalues λ k the minus superscript denotes the upwind discretization and a k and b b k account for the three wave and source strengths respectively with this it is easy to derive equation 2 from 10 the expressions for those matrices and vectors are detailed here 11 p k e 1 e 2 e 3 k 1 0 1 u c n x c n y u c n x v c n y c n x v c n y k λ k λ 1 0 0 0 λ 2 0 0 0 λ 3 k u n c 0 0 0 u n 0 0 0 u n c k λ k λ k λ k 2 a k p 1 δ u k α 1 α 2 α 3 k δ h 2 δ q n u n δ h 2 c 1 c δ q y v δ h n x δ q x u δ h n y δ h 2 δ q n u n δ h 2 c k u k u v k u i h i u j h j h i h j v i h i v j h j h i h j k c g h i h j 2 δ h k h j h i δ q k δ q x δ q y k q x j q x i q y j q y i k where u u v q x h q y h is the flow velocity the term b b k accounts for the bed slope source estimation in this work the integral formulation proposed in murillo and garcía navarro 2010 is adopted leading to the following definition of b b k 12 b b k β b 1 β b 2 β b 3 k g 2 c h p δ z 2 δ z 0 g 2 c h p δ z 2 δ z h p h i δ z 0 h j δ z 0 δ z h i δ z 0 and h i z i z j h j δ z 0 and h j z j z i z j z i otherwise although this estimation is more accurate than the conventional finite difference like approach it can still lead to non physical solutions when dealing when complex flows when these problems arise the conventional solution consists in reducing cfl and consequently the time step size impacting directly on the performance of the computation another approach is adopted here based on the augmented approach which allows us to reconstruct the cell averaged approximate solution with the objective of avoiding negative water depth values at time t this can be particularly useful for the treatment of wet dry fronts a key factor for reliable 2d hydraulic models in this work the wet dry treatment detailed in echeverribar et al 2019a murillo and garcía navarro 2010 is implemented following this approach the intermediate states of the riemann problem are analyzed and depending on their sign the information is sent to the left or the right side of the discontinuity this has been proved to be mass conservative and robust for any kind of flows echeverribar et al 2019a murillo and garcía navarro 2010 besides the wet dry fronts stiff source terms can lead to negative water depth values even in the presence of wet wet problems i e thin layers of water with strong bed discontinuities and high friction terms in order to consider these situations in our numerical scheme a similar technique of that followed in murillo and garcía navarro 2010 is adopted here however as the time step is computed before the source terms estimation a new limitation based on the final integration rather than zero the intermediate states is proposed in this work as detailed in murillo and garcía navarro 2010 only wet wet subcritical cases are analyzed let us denote l and r the left and right states of a discontinuity and assume λ 2 0 the derivation does not change for λ 2 0 due to the properties of the intermediate states for the water depth in the augmented approach the numerical solution at time t should guarantee h l 0 and h r 0 then according to figure a9 b the following inequations must be fulfilled 13 h l δ x 2 λ 1 δ t λ 1 δ t h 1 0 λ 2 δ t h 2 λ 3 λ 2 δ t h 3 h r δ x 2 λ 3 δ t 0 in agreement with the definitions of the intermediate states toro 2013 murillo and garcía navarro 2010 14 h 1 h l α 1 β b 1 λ 1 h 2 h 3 h r α 3 β b 1 λ 3 and doing some simple algebraic manipulations the following restrictions are derived for β b 1 and consequently for β b 3 15 β m i n β b 1 β m a x β m i n h l δ x 2 δ t α 1 λ 1 β m a x δ x 2 δ t h r λ 3 α 3 β b 3 β b 1 these conditions are implemented in triton minimizing the appearance of negative water depths additionally this scheme needs an entropy correction for transonic rarefactions hence the harten hyman entropy fix is used here as this approach is based on decomposing the existing jump into two new jumps a special emphasis should be put in the source term split murillo and garcía navarro 2010 two additional assumptions are considered first a cell is considered dry if its water depth is below 10 12 second velocities are set to zero for water depths below a tolerance this value depends on the characteristics of the problem study spatial scales and roughness mainly as a physical explanation for this parameter dissipation terms in practical applications with realistic roughness values dominate at this scale over convective and inertial terms therefore it is not arbitrary to model this phenomenon as a tolerance under which velocities are considered null a value of 10 3 is suggested for real world scenarios as a general recommendation although it can be also modified in triton for every configuration the scheme has been proved to be well balanced robust and reliable for flows under various conditions according to section 5 
25818,a new open source multi gpu 2d flood model called triton is presented in this work the model solves the 2d shallow water equations with source terms using a time explicit first order upwind scheme based on an augmented roe s solver that incorporates a careful estimation of bed strengths and a local implicit formulation of friction terms the scheme is demonstrated to be first order accurate robust and able to solve for flows under various conditions triton is implemented such that the model effectively utilizes heterogeneous architectures from single to multiple cpus and gpus different test cases are shown to illustrate the capabilities and performance of the model showing promising runtimes for large spatial and temporal scales when leveraging the computer power of gpus under this hardware configuration communication and input output subroutines may impact the scalability the code is developed under an open source license and can be freely downloaded in https code ornl gov hydro triton keywords 2d flood model open source multi gpu high resolution shallow water equations 1 software availability name of the software triton two dimensional runoff inundation toolkit for operational needs contact address oak ridge national laboratory 1 bethel valley road tn 37830 usa tennessee technological university 1 william l jones dr cookeville tn 38505 usa email moraleshern2 ornl gov akalyanapu tntech edu language cuda c hardware desktop laptop or clusters of cpus gpus software nvidia cuda toolkit netcdf optional availability https code ornl gov hydro triton year first available 2020 2 introduction with the increasing frequency and intensity of extreme hydrologic events in a changing environment madsen et al 2014 faster and more accurate inundation models are particularly important tools for flood risk management when translating the amount of rainfall either from weather forecasting models or from rain gauge observations all the way to surface inundation coupled hydrologic hydraulic models should be used where the flood inundation model represents the last mile that simulates flood wave propagation across high resolution terrain based on simulated runoff and streamflow from hydrologic models xia et al 2019 flood inundation models can provide not just streamflow discharge but the temporal evolution of flood location depth and movement however this information is not always directly translated into practical instructions to better support operators and emergency responders for more rapid decision making spiekermann et al 2015 in this regard collaborative flood modeling is a great example for overcoming this challenge where end users stakeholders and scientific knowledge meet and interact evers et al 2012 maskrey et al 2016 specifically the use of high resolution maps built from flood inundation models together with non technical terminology have proven to benefit decision making for end users sanders et al 2020 beyond this methodology integrated platforms using hydraulic modeling linked with live geospatial information remote sensing flood visualization and community intercommunication xie et al 2017 wang et al 2019 and other strategies such as probabilistic flood maps based on flood models and the value of information voi alfonso et al 2016 have been also demonstrated to reduce the uncertainty and assess the consequences of actions taken by decision makers despite the integrated flood risk management tools large scale operational hydrodynamics models are still rarely used mainly because of their large computational cost simulation models must be particularly fast on the order of minutes to meet decision making needs resulting in the reliance on simplified and analytical models one example is the integrated version of height above nearest drainage hand within the us national water model nwm liu et al 2016 2018 hand is based on manning s equation some geometric extrapolations and precomputed rasters that are used to generate national flood maps at 10 m resolution its european equivalent within the efas european flood awareness system is lisflood fp bates et al 2010 a spatially distributed rainfall runoff routing model its operational version neal et al 2012 wing et al 2018 uses the runoff generated at a 5 km grid spatial resolution as an input for a simplified shallow water model in which the convective term is neglected flooding maps are derived from a catalog of flood hazard maps made of precalculated runs of the lisflood fp model the main limitation of these models is the range of applicability for all types of flows and situations which are restricted by the simplification hypothesis assumed and consequently the validity and accuracy of the results one of the most complete inundation frameworks is the 2d full shallow water system garcía navarro et al 2019 which solves mass and momentum equations and provides water depth distribution and an accurate surface velocity field the resolution of the 2d shallow water equations is nevertheless computationally demanding at fine resolutions and there is no obvious path for them to scale up to continental or global scales for operational purposes two main efforts are highlighted in the last decades to overcome this constraint efficient numerical techniques and the adaptation to high performance computing hpc first numerical improvements are of crucial importance to reduce the computational burden in the context of augmented riemann solvers a careful numerical estimation of topography and friction terms has been demonstrated to improve the performance and accuracy of the solution murillo and garcía navarro 2010 2012 murillo and navas montilla 2016 local time step methods sanders 2008 dazzi et al 2018 are considered as a worthwhile solution to improve the computational efficiency at the extra cost of having to deal with different stages for flux and source computations depending on the time level as for the spatial discretization leaving aside adaptive mesh refinement techniques discontinous galerkin dg schemes as the one proposed in kesserwani et al 2018 seem to deliver high quality solutions with desired scalability properties on the same page a well balanced no neighbor method has recently been proposed for the 1d saint venant equations hodges and liu 2019 although the extension to the 2d framework is not clear this family of schemes could open the door to new efficient algorithms on the other side parallel implementations and the use of hpc on new architectures of modern supercomputers have become a fundamental requirement to study increasingly complex problems at large spatial scale and high temporal resolution on water resources hydrodynamics morales hernández et al 2020 neal et al 2010 explored different parallel strategies of lisflood fp across various types of architectures stressing the efficiency difficulty and estimated development time of each implementation the great majority of models are nevertheless implemented on a single hardware type one of the best exponents is primo sanders and schubert 2019 a raster based subgrid flood model able to run on clusters of central processing units cpus using both shared and distributed memory however the use of graphical processing units gpus has become a new trend in the recent decade sætra et al 2012 castro et al 2011 kalyanapu et al 2011 brodtkorb et al 2012 in the gpu context the spatial discretization and efficiency have been studied for regular vacondio et al 2014 and irregular grids petaccia et al 2016 garcía navarro et al 2019 echeverribar et al 2019a seeking efficiency with i different solvers de la asunción et al 2013 ii different memory access patterns and mesh ordering algorithms for unstructured grids lacasta et al 2014 iii block uniform quadtree and adaptive mesh refinement on nested rectangular meshes vacondio et al 2017 and iv a local time stepping scheme to improve performance dazzi et al 2018 additional capabilities have been also reported and accelerated with the use of gpus rainfall runoff applications lacasta et al 2015b aureli et al 2020 sediment transport and erosion processes juez et al 2016 caviedes voullieme et al 2017 dazzi et al 2019 landslides lacasta et al 2015a hydraulic structures modeling echeverribar et al 2019b dazzi et al 2020 and transport of contaminants and water quality models viñas et al 2013 garcía feal et al 2020 multi gpu models could be the solution to improve the tradeoff among accuracy speed and large scale domains the initial work by sætra and brodtkorb sætra et al 2012 studied both weak and strong scaling and the effect of synchronizations of 2d shallow water equations using a 4 gpu machine xia et al 2019 used a framework called hipims to model a storm in a 2500 k m 2 catchment using 8 gpus and 100 million grid cells around 2 5 times faster than real time in turchetto et al 2018 2020 the equations are discretized in a buq quadtree grid following the proposed scheme of vacondio et al 2017 although this type of spatial discretization could be convenient to improve performance with respect to regular cartesian grids and to address high resolution problems the domain decomposition increases complexity and could add an overhead in large scale simulations in sharif et al 2020 the authors compare two versions of solvers for the shallow water equations finite difference versus finite volume weak and strong scaling up to 272 million grid cells are analyzed together with the cuda aware mpi feature designed to optimize communications between the different sub domains triton is presented here as the first to date multi architecture multiple cpus and gpus open source 2d hydrodynamic flood model based on the resolution of full shallow water equations with source terms different free or open source 2d models can be found in the literature besides the lisflood model bates et al 2010 neal et al 2012 and the well known hec ras model 2d version released in 2016 other cpu based or multi cpu models such as fullswof delestre et al 2017 and brezo sanders et al 2010 solve the 2d shallow water equations in structured and unstructured triangular grids respectively delft3d deltares 2014 also permits 2d flow computations for different applications ranging from hydrodynamics to sediment transport and water quality few gpu models are freely available geoclaw berger et al 2011 for certain applications qin et al 2019 iber for 2d flood modeling garcía feal et al 2018 and anuga enabling the gpu offloading using pycuda weng and strazdins 2014 triton has been nevertheless designed for a multi architecture paradigm and is able to run on several configurations including single or multiple cpus and single or multiple gpus using a combination of open multi processing openmp compute unified device architecture cuda and message passing interface mpi in addition to this architectural flexibility all computing subroutines are programmed only once regardless of the hardware type minimizing error sources and bolstering the software portability finally a simple input output configuration is implemented that would avoid significant geographic information system gis pre and post processing as an example a digital elevation model dem is directly used as the computational mesh circumventing the necessity of site specific mesh building as with most existing models these features would eventually enable the use of 2d hydrodynamic models for operational purposes and other applications that were not feasible before in addition to introducing this triton as a fast and flexible open source suite to simulate both pluvial and fluvial flood events together with some freely available pre and post processing tools this work aims to answer the following research questions 1 is it possible to simulate large temporal and spatial scales in the order of minutes using a hydrodynamic model based on the solution to the full 2d shallow water equations which are the most appropriate architectural and parallelization strategies required to achieve this 2 does spatially distributed rainfall runoff have an impact on the results of predictive hydrodynamic models 3 could communication and i o times represent a bottleneck for large scales the paper is organized as follows after describing triton including the equations numerical scheme and the hpc implementation the software features are presented three test cases are included to demonstrate the capabilities of the model showing the accuracy and performance of triton on different configurations and architectures including the simulation of the multi day event of hurricane harvey over harris county in texas on 384 gpus using summit supercomputer at oak ridge national laboratory the trade offs among parallel computation communication and input output are also analyzed revealing the importance of the latter two for large temporal and spatial scales 3 the triton model triton the two dimensional runoff inundation toolkit for operational needs is a physically based hydrodynamic model that solves the 2d shallow water equations on a structured cartesian grid based on the initial gpu model developed by kalyanapu et al 2011 a new conservative numerical scheme has been implemented and integrated in an updated framework able run on multiple architectures 3 1 governing equations the 2d shallow water equations express the depth averaged conservation of mass and momentum in x and y directions of the space they can be written in a compact differential conservative form as presented in eq 1 1 u t f x g y s r s b s f u h q x q y f q x q x 2 h 1 2 g h 2 q x q y h g q y q x q y h q y 2 h 1 2 g h 2 s r r 0 0 s b 0 g h z x g h z y s f 0 g n 2 h 7 3 q x q x 2 q y 2 g n 2 h 7 3 q y q x 2 q y 2 the vector u represents the conserved variables i e the unknowns of the system and includes the water depth h l and the x and y unit discharges called q x l 2 t 1 and q y l 2 t 1 respectively eq 1 also contains the fluxes of these conserved variables f and g being g l t 2 the gravity acceleration and the source terms the latter encompass runoff terms s r expressed according to the runoff rate r l t 1 bed slope terms s b accounting for the gradient of the elevation z l and friction terms s f modeled by means of gauckler manning s law in terms of the manning s roughness coefficient n t l 1 3 here runoff refers to the effective rainfall i e total rainfall minus losses due to infiltration abstraction and evapotranspiration plus baseflow that are typical outputs from hydrologic or land surface models 3 2 numerical scheme a finite volume upwind explicit scheme is used to solve eq 1 in a squared cartesian mesh of grid spacing δ x an augmented roe aroe solver is implemented based on murillo and garcía navarro 2010 echeverribar et al 2019a for the fluxes and bed slope source terms nevertheless a different estimation of bed slope source terms at each edge are proposed in this work this treatment also ensures the positivity of the solution without reducing the time step size for the sake of clarity the derivation of this part of the scheme can be found in appendix a friction terms are discretized using a local implicit formulation xia and liang 2018 that does not alter the explicitness of the scheme accordingly a two step algorithm is proposed for the update of a cell i from time t n to time t n 1 t n δ t 2 u i u i n δ t δ x k 1 4 m 1 3 λ λ λ α β b e m k n u i n 1 f u i n u i r i n δ t where at each interface k α and β b are the fluxes and bed slope source term linearizations minus superscript accounts for the upwind discretization and λ and e are the eigenvalues and eigenvectors of the system of equations respectively see appendix a the localized runoff rate is denoted by r i while function f stands for the friction discretization written as 3 f 1 h f 2 q x 1 1 4 s f 2 s f f 3 q y 1 1 4 s f 2 s f where 4 s f δ t g n 2 q x 2 q y 2 h n 7 3 the explicit character of the scheme restricts the time step size according to the courant friedrich lewy cfl condition 5 δ t cfl min i δ x q x h i g h i q y h i g h i cfl 0 5 where index i loops over the number of grid cells note that although formally the maximum wave speed should be estimated at the interfaces the cell values are used instead in this work notwithstanding this approach does not compromise the stability of the scheme in fact a value equal to or less than the size of the time step is selected using this formula but it simplifies and ultimately accelerates the computations providing simultaneously a new way to estimate some corrections for the source terms see appendix a 3 3 hpc implementation increased problem complexity motivates heterogeneous hpc for hydrodynamics codes in the new era of parallel computing morales hernández et al 2020 computation time can be reduced effectively with the use of clusters of cpus and gpus on demand cloud workstations are also becoming more popular and affordable allowing the simulation of larger spatial and temporal domains at finer scales varied programming paradigms have arisen as a consequence of this heterogeneity open multi processing openmp for multicore cpus message passing interface mpi for clusters compute unified device architecture cuda or open computing language opencl for gpus for that reason triton has been designed as a multi architecture single code base able to run on the following platforms through specific compilation instructions 1 multi core shared memory platform using openmp 2 multi node cluster using mpi or mpi openmp 3 single node gpu machine using cuda 4 multi node gpu cluster using mpi cuda the simplified flowchart for the current implementation is depicted in fig 1 a after reading and parsing the input data see section 4 1 the domain is decomposed into different subdomains according to the desired number of mpi sub tasks then the simulation starts and runs until the time reaches the final simulation time writing the output information described in section 4 2 each output interval at each time state the time step size is first computed according to eq 5 then the computing kernels gpu or subroutines cpu are executed merely accounting for the numerical scheme in eq 2 finally the information is exchanged between the corresponding subdomains details of the later three processes along with the mpi decomposition are explained below 3 3 1 domain decomposition and halo exchange domain decomposition is an important factor for large scale parallelization using mpi libraries or similar in design of portable and scalable communication between subdomains although there are different ways to partition the information a 1d row wise decomposition is applied here for simplicity morales hernández et al 2020 as the stencil for the numerical scheme eq 2 involves neighbouring data see fig 9 a appendix a the information has to be exchanged each time step and overlap computations are performed fig 1 b shows a sketch of the row wise approach where north south communication is required and the halo size is the number of columns of the whole domain a two step non blocking algorithm is implemented using mpi isend and mpi irecv with the aid of mpi wait to wait for an mpi request let n be the number of ranks e g the number of partitions of the domain first all subdomains n n will send the halo data to the subdomain n 1 and receive from subdomain n 1 then vice versa each subdomain n 1 will receive information from subdomain n 1 and send to subdomain n 1 the subdomain exchange imposes a slightly greater level of complexity when dealing with a gpu implementation due to memory allocation as computations are performed by a gpu data are fully allocated in the device gpu memory however the regular mpi calls require pointers to host cpu memory which requires an additional data copy between host and device in addition to an extra host memory allocation this might decrease the performance of the model the cuda aware mpi is used for halo exchange to overcome this challenge of possible performance degradation cuda aware mpi allows gpu to gpu direct communication via network bypassing the cpu if underlying hardware supports this technology specifically summit supercomputer has gpu direct communication support and the use of cuda aware mpi has resulted in improved triton s performance the impact of using cuda aware mpi versus the conventional approach has been already studied in sharif et al 2020 see results fvs and fvg for a primitive version of triton for the sake of flexibility triton supports both approaches allowing users to choose a preferred implementation according to their system requirements 3 3 2 time step size computation in order to guarantee a stable and reliable solution at every time stage δ t is limited according to eq 5 which requires the computation of a global minimum time step size that will be imposed to evolve the solution in time to do that each subdomain computes first its own local minimum time step size for all its cells again two different implementations can be distinguished here depending on the architecture the openmp approach is rather simple since it only consists of the computation of a reduction operation the equivalent reduction operation in gpu is more complicated although some cuda standard libraries cublas thrust or third party cub libraries can be used a reduction ad hoc function has been implemented in cuda in contrast to the existing libraries in which a global array of size the total number of cells is required and then perform the reduction operation we make use of shared memory and thread synchronization at each kernel where we calculate the time step size to launch multiple instances using a reduced global array of size thread block times smaller than the original size then the reduction operation is done over the global reduced array diminishing the global memory use improving memory coalescing and providing a better performance than the existing libraries for large scale problems once the local minimum time step size is computed the global minimum is found using mpi allreduce across all the processes sub tasks triton also offers the possibility of a constant time step size this implementation does not require any reduction operation making it easier nevertheless the accuracy and robustness of the results are not ensured since the maximum allowed time step size governed by eq 5 might be violated 3 3 3 kernel subroutine execution triton is written using c and cuda each computing module has been implemented as a cpu subroutine as well as a cuda kernel during compilation based on the computing platform appropriate computing modules are compiled a set of arrays with the same structure is allocated either in the cpu or in the gpu so that the difference between cpu and gpu execution only consists of the memory where the data is defined and the kernel subroutine calls each kernel is then only programmed once avoiding duplicated information this fact improves the readability of the code at the same time as reproducibility and trustfulness between different architectures minimizing or almost eliminating eventual human mistakes usually made when porting the code from one to another architecture fig 2 depicts the source code for the wet dry kernel subroutine showing both the function call and declaration as shown the number of arguments required by the cpu and gpu versions are exactly the same although arrays reside either in the device or the host memory according to the chosen architecture some ifdef else and endif directives allow to switch between cpu and gpu in compilation time 4 software features triton is a unix based model targeting laptops desktops and optimized for supercomputers leveraging the current power of workstations the code pre post processing tools and some samples can be found in https code ornl gov hydro triton the main features and tools are explained below 4 1 input data the mandatory optional triton input files include configuration mandatory a text file containing the path of all input files output interval and format and all parameters and constants needed for simulation e g number of streamflow sources external boundary conditions initial and final time cfl number switches to enable disable observation point and checkpointing etc it can be configured either manually or with the aid of a config file tool generator topography mandatory triton is a dem based code consequently the mesh used for computation is a cartesian square grid obtained directly from the dem file i e avoiding the ad hoc and site specific task of computational mesh building dems follow the esri raster file format both ascii and binary formats are allowed although binary format is recommended for large spatial domains that contains a header section with the number of columns rows origin coordinates and cell size as well as a matrix of elevation values nodata values are not allowed in the current version of triton streamflow hydrograph optional streamflow hydrograph is one of two possible hydrologic inputs to triton streamflow here refers to the point discharge typically from upstream incoming river channels sources when selecting streamflow hydrograph two files are required an x y coordinate list with the location of all inflow sources and a streamflow hydrograph table including the time in hours and the timeseries of discharge in cubic meters per second at each source j q j t l 3 t 1 the discharge is introduced in triton as a single mass release runoff hydrograph optional runoff hydrograph is another possible hydrologic input to triton it is a common output from various hydrologic and land surface models immediately before such information is used for streamflow routing by involving runoff in triton the model can simulate local pluvial floods and hence increases its functionality when selecting runoff hydrograph two files are also required the runoff regions corresponding to the coarser hydrologic model grids are defined in the form of a matrix raster map with the same format with the dem file but without the header each distinct area is labeled with a non negative integer number that serves as a unique region identifier links to the runoff hydrograph table as in the streamflow hydrograph table the runoff hydrograph table contains the time in hours and timeseries of runoff rate r in mm per hour from all declared runoff regions a sketch of the rainfall runoff input files is depicted in fig 3 roughness optional the surface friction is represented by manning s roughness coefficient n it can be provided either as a constant number specified in the configuration file and applied globally to the whole domain or in the form of a matrix raster map without header matching the number of dem grid cells the latter choice allows the user to define spatially varied roughness coefficients to represent the site specific land use and land cover conditions external boundary conditions optional the boundaries of the domain north east south and west can be provided with five type of external boundary conditions by default all boundaries are closed i e water flux is prevented from exiting the domain four alternative flow conditions can be then be imposed type 0 zero gradient the original setup in kalyanapu et al 2011 type 1 level versus time an additional file containing a table with the time and the timeseries of water level is mandatory type 2 normal slope the desired slope is required type 3 froude number the froude number defined as f r q h g h is needed to be imposed across the external boundary being q q x q y more than one external boundary condition can be defined at each boundary edge the user only has to provide the initial ending x y coordinates of the boundary and its corresponding boundary condition parameters according to the boundary type the external boundary conditions are imposed at the ghost cells surrounding the whole domain this implementation ensures the scalability of the solution for domain decomposition since no additional information has to be exchanged between the partitioned subdomains initial conditions optional a dry domain is the default initial condition however the user can also specify an initial condition for each conserved variable h q x or q y in the configuration file in the form of a matrix type file without header furthermore checkpointing hotstart is also allowed backup files are written during the computation to retrieve the simulation from the last state in case it is necessary point output optional in addition to the default matrix output at user specified time intervals triton also supports output of timeseries at user specified locations to avoid data processing for known points of interest such as locations with gauge observations an x y coordinate list file containing the location of the desired points is therefore required 4 2 output data the output from triton is intended to be easy to post process by standard graphing and gis tools two types of output data spatial and temporal are generated in a separate folder at each user specified interval matrix data spatial a snapshot of water depth and unit discharge is written in the form of a matrix type file without header either in ascii or in binary format the latter is recommended for large scale domains morales hernández et al 2020 additionally if the execution is run using domain decomposition either multi cpu or multi gpu a switch in the configuration file allows the user to choose the mode in which the data is written 1 sequential that gathers all subdomain information in a single file during the computation or 2 parallel that directly outputs subdomain data as separate files for the latter a separate script can then be used to combine all subdomain information into a single file during post processing water depth and unit discharge profiles temporal the timeseries of water depth and unit discharge at the specified point locations defined in the input files can be outputted in a single file for each variable fig 4 displays a style of output information that can be obtained it corresponds to the hurricane harvey test case studied in section 5 3 and condenses in a single panel the stage hydrograph for each predefined observation point as well as a 2d view of the whole spatial domain showing the topography and the maximum flooded area note that to be consistent with eq 1 triton outputs unit discharge rather than velocity the user should conduct proper conversion to estimate velocity based on unit discharge and water depth 4 3 other tools a suite of tools is provided with triton in order to simplify some pre and post processing tasks first some bash scripts are included for the conversion between binary ascii formats and sequential parallel mode they can be used for both input and output data additionally two programs containing a gui targeting windows and linux are included in the repository the first one is a netcdf converter that takes both binary and ascii files outputted from triton as input data and converts them into netcdf format the second gui software called config file tool generator allows the user to generate the configuration file mandatory input file for triton 5 test cases a set of test cases are proposed here to demonstrate certain desired characteristics such as consistency stability convergence and robustness and model capabilities as triton is intended to be a reference software for the computation of large scale flood problems reliability on the output results is mandatory the first test case is therefore included as a verification and to show how the model behaves on a classic literature test case that involves all kind of flows subcritical supercritical and sonic transitions and to test the accuracy and grid convergence of the model once the model is able to provide a robust and trustworthy solution the second test is designed to test the capability of a model to accept runoff hydrograph as a hydrologic input and illustrate its importance this feature also allows users to simulate the effects of local pluvial floods that occur due to highly intense local precipitation in a non floodplain region the final test case is oriented to evaluate the performance of the model on different architectures multiple cpus and gpus analyzing the possible bottlenecks in large temporal and spatial scales all test cases introduced below can be found in the repository additional test cases will be included as they are developed to provide users with further examples 5 1 test case 1 paraboloid bed topography this test case consists of a square domain 0 4 0 4 with a frictionless paraboloid topography the initial water depth condition is a planar surface with velocity in the y direction the domain boundaries are closed and after one period t 1t the numerical solution should recover the initial condition due to the rotating velocity field this configuration is considered to be a challenge test where some numerical methods fail when trying to reproduce it mainly due to an incorrect treatment of wet dry interfaces or a non balanced source terms fluxes implementation more details about the analytical periodic solution are described in delestre et al 2013 the package provided there is used to generate four different resolution grids δ x 0 04 0 02 0 01 0 005 m the tolerance h t o l see appendix a is set to 10 4 in this particular case without friction and cfl 0 45 three periods t 3t are simulated and the numerical results obtained by each grid resolution are compared to the exact solution understood as the projection of the analytical solution on each computational grid fig 5 depicts those comparisons at t 1t t 2t and t 3t accuracy decreases as time advances especially with lower resolution this is a consequence of the first order only numerical diffusion which is resolution dependent that said the use of higher order schemes is not recommended since both pressure and dissipative terms usually dominate over the convective terms with the roughness of realistic applications also the presence of wet dry discontinuities would downgrade the accuracy to first order so that the cost of the implementation is not justified nonetheless a correct estimation of the source strengths is mandatory to avoid dramatic reductions in the time step size and to ensure a correct well balanced implementation murillo and garcía navarro 2010 murillo and navas montilla 2016 echeverribar et al 2019a to provide a quantitative measure of the error made for each resolution and to check the convergence rate l 1 l 2 and l error norms are computed with respect to the analytical solution at t 3t table 1 contains those error norms computed as 6 l 1 y δ x 1 n i 1 n y n y e i l 2 y δ x 1 n i 1 n y n y e i 2 l y δ x max i y n y e i where n is the number of grid cells and y n and y e are the numerical and exact solutions respectively these errors confirm the consistency of the scheme for a first order method even though this test case involves water depth values on the order of centimeters millimeters or below together with strong velocities the scheme is demonstrated to be robust and accurate enough providing a solution free of oscillations which is particularly of interest when dealing with wet dry boundaries a key factor in 2d flood models 5 2 test case 2 runoff capability triton admits runoff hydrographs to provide spatiotemporally distributed runoff as an input typically a hydraulic model is driven by providing streamflow hydrographs at user specified locations to simulate riverine fluvial floods these locations can be where the observations of streamflow were made or can match a river routing model however the lack of sufficient streamflow source locations due to hydrologic model limitations or other factors may lead to the underestimation of flood extents especially at smaller tributaries to overcome this issue users can utilize the runoff function within triton as an example to illustrate this capability we select an area located upstream of allatoona reservoir in the northwest georgia us the region was previously modeled by gangrade et al 2018 gangrade et al 2019 to study probable maximum flood using the distributed hydrologic soil vegetation model dhsvm and flood2d gpu kalyanapu et al 2011 the 100 year peak streamflow was estimated at the outlet of the computational domain following the guidelines of bulletin 17b on water data 1982 and then used to prepare 100 year return level streamflow hydrographs for a detailed description readers are referred to section 3 1 of gangrade et al 2019 in triton the 100 year flood is simulated with two different model configurations scenario a no runoff using 13 streamflow source locations along the river network fig 6 a and scenario b with runoff using 2 upstream streamflow source locations and runoff simulated at various catchments fig 6 b the computational domain spans 358 sq km and consists of 400 000 grid cells at a 30m dem resolution in both scenarios a 5 day simulation is conducted with peak streamflow occurring at day 3 the water depth is written in raster maps every 30 min the maximum flood inundation extents for both scenarios compared against a benchmark dataset from fema 100 year flood zones are presented in fig 6 c and d respectively a qualitative comparison reveals that just using streamflow inputs scenario a fig 6 c may lead to the underestimation of flood extents especially in tributaries where no upstream streamflow inputs are specified in scenario b since the model is driven by runoff hydrograph across the entire domain it can better capture the flood extents especially in the tributary areas to evaluate the two scenarios quantitatively we use a metric called hit rate which is a measure of model tendency to accurately predict the benchmark flood extents gangrade et al 2019 wing et al 2017 using fema 100 year flood plain as a benchmark we obtain a hit rate of 0 50 for scenario a and an improved rate of 0 68 for scenario b not only demonstrating a better floodplain simulation local pluvial floods can also be simulated in fig 6 d the results showcase the advantage of using runoff for better and easier flood regime simulation 5 3 test case 3 hurricane harvey the last test case is intended to evaluate the performance and applicability of triton using different architectures multiple cpus and gpus in a realistic configuration in this scenario we simulate the massive flood that hurricane harvey caused in the summer of 2017 along the us gulf coast the spatial domain encompasses around 6800 k m 2 of harris county texas us the simulation covers ten days with the heaviest rainfall occurring during day 7 to day 9 the runoff data are generated using the variable infiltration capacity vic hydrologic model liang et al 1994 driven by hourly 4 km radar based stage iv quantitative precipitation estimate this runoff is then routed using the routing application for parallel computation of discharge rapid model david et al 2011 through the river network to generate streamflow input at 69 inflow source locations information from the us geological survey national elevation dataset is used to build a corrected dem in which in we included addicks and barker s reservoir capacities incorporated the bathymetry of the main river segments and eliminated some noise present in the original data the dem resolution is 10m with a total of around 68 million grid cells and the manning s roughness coefficient is set to a constant value of n 0 035 s m 1 3 the output is configured in sequential binary format with an output interval of 1800s more information about this test case can be found in dullo et al 2021 the purpose of this test case is to evaluate the efficiency and scalability of the model on different hardware configurations simulations are carried out on summit supercomputer at oak ridge national laboratory summit is comprised of 4608 compute nodes each of them containing six nvidia volta v100 gpus and 42 physical cores 168 hardware cores using simultaneous multithreading in order to make a fair comparison between multiple architectures a summit compute node is chosen and seven numerical experiments are performed using up to 64 nodes two triton hardware configurations are used multiple cpus using mpi openmp and multiple gpus using mpi cuda each of them with the aim of minimizing the number of mpi tasks per configuration consequently the multi cpu version uses one mpi task per node while the multi gpu version uses one mpi tasks per gpu cuda aware mpi is used for the multi gpu simulations table 2 condenses both hardware configurations as well as displays the runtimes following sharif et al 2020 we also define the following metrics billion lattice updates per second blups and speed up 7 blups n c n t s t n 10 9 speed up t 1 t n where n c is the number of grid cells n t s is the number of time steps done and t n is the runtime achieved using n computing nodes the speed up in this case measures how fast the simulation is compared to the runtime using 1 node fig 7 left depicts the runtime in log scale y axis and the number of blups x axis achieved by each hardware configuration plotted in circles of different sizes the main result is that a very large problem for a serial code can be computed more reasonably with a multi gpu system with runtimes of less than 30 min even with the lowest hardware configuration 1 node 6 gpus the multi gpu version is able to achieve a lower runtime than the most demanding multi cpu hardware configuration 64 nodes 10752 openmp threads accordingly the maximum blups achieved by the multi cpu system is still lower than the 1 node multi gpu configuration additionally given a fixed number of summit nodes the average blups ratio between the multi gpu and multi cpu versions is 25x with a maximum of 45x for 1 node revealing the convenience of using this architecture the circle series turns horizontal when the strong scaling limit is reached and the asymptote is representative of the absolute fastest runtime expected for each configuration in this problem with this the multi cpu system has not reach its maximum using 64 nodes while the multi gpu version tails off significantly beyond 8 or 16 nodes fig 7 right shows in log log scale the speed up relative to the simulation using 1 node y axis as a function of node count x axis compared with the perfect scaling as shown the multi cpu version is able to achieve a satisfactory scaling factor up to 32 nodes 32 mpi tasks although a performance hit is observed for 64 nodes this might be due to the row wise 1d mpi partitioning on the other hand the multi gpu speed up does not scale acceptably although 6 mpi tasks are used per summit node 1 per gpu a lower speed up is observed even for 32 mpi tasks extrapolation between 4 and 8 nodes where adequate speed ups were achieved for the multi cpu version many factors are responsible for this first this test case does not have runoff as an inflow and only around 20 as an average of the domain is wet a loss of efficiency is therefore caused by thread divergence due to an if statement implemented in the code to avoid the computations on dry cells this is an issue reported for cuda and it is not the case with openmp since each cpu core runs like a single threaded subroutine executing its own independent set of instructions additionally a static mpi subdomain partitioning aggravates this fact since some subdomains could not have a significant computational burden resulting in an imbalance among mpi processes morales hernández et al 2020 to investigate other sources of scaling limitations in the multi gpu system the computation time gpu the mpi communication time the i o time and the rest are plotted in fig 8 two additional simulations are carried out using 1 and 3 gpus the left panel shows in log log scale the absolute number in minutes while the right plot displays the percentage of the time consumed by each process against the gpu count note that memory copies between the host and the device are counted within the gpu time and the other time stands for subroutine calls and basic operations run on the cpu as observed on the left plot the gpu time is reduced at almost the same rate for each configuration meaning that it actually scales according to the number of gpus a slightly lower rate is detected for the last two values 192 and 384 gpus due to the low number of grid cells per gpu each gpu does not have enough work to leverage its computing power i o and other cpu time remain almost constant for all configurations as expected communication time using a blocking algorithm and cuda aware mpi does not increase noticeably as the number of gpus grows indicating that communicating every time step carries significantly more weight than the number of gpus the right panel demonstrates that mpi and i o times govern this problem for large gpu count in particular from 40 to 66 of the time in this test case is consumed by these processes at 96 gpus and beyond this fact suggests that the operational scale bottleneck is not with computation but on communication and i o tasks parallel i o should improve these results and a more efficient communication strategy should be designed these results motivate even larger test cases to leverage the massive parallelization in a substantial number of gpus paving the way to even larger temporal and spatial scales 6 conclusions and perspectives a new open source 2d flood model triton is available to run on heterogeneous architectures using single and multiple cpus and gpus to enable hydraulic computations at large temporal and spatial scales it consists of a simple input file structure meshless standard gis formats and a configuration text file and provides spatiotemporal information of water depths and velocities as output information three test cases have been provided to demonstrate the triton capabilities the accuracy consistency and robustness of the scheme has been proved by means of the paraboloid test case the importance of runoff capability in triton has been highlighted using a flood test case in northwest georgia us results with and without runoff have been compared against the 100 year flood extension provided by fema revealing the benefit of considering the runoff capability for this sort of studies finally the hurricane harvey test case has been used to show the performance on the scheme on different architectures multi cpu using openmp mpi and multi gpu with cuda mpi the numerical experiments highlight the convenience of using the multi gpu version against the multi cpu achieving a low runtime less than 30 min for real world configurations a large spatial domain at 10m resolution and a 10 day hydrograph and unlocking operational purposes at even larger spatial and temporal scales although the runtimes are promising the scalability of the multi gpu version is nevertheless unsatisfactory for large number of gpus many aspects are responsible for this thread divergence for dry cells poor load balancing between mpi ranks with a static decomposition and principally the execution time consumed by i 0 and mpi communication for a large scale problem with respect to the computation time although these times in the order of minutes each are almost constant and does not depend on the number of gpus they represent a great percentage of the total runtime when trying to achieve operational purposes future perspectives are therefore aimed at designing optimized i o parallel algorithms and exploring new communication techniques such as a 2d mpi decomposition or overlapping strategies particularly the latter would considerably improve the performance as communication between subdomains would not be required to be every time step triton is under continuous development future planned improvement includes besides new efficient i o and communication algorithms support for other gpu libraries such as openacc all of these efforts will be regularly documented in the repository we provide this baseline to enable the use of a fully 2d hydraulic model for new science questions such as uncertainty quantification or climate change problems beyond the studied temporal and spatial scales heretofore the extension to very high resolution continental global scale flood modeling will require multi gpu computations so new algorithms are obviously planned to extend these capabilities to operational scales declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this research was supported by the us air force numerical weather modeling program this research used resources of the oak ridge leadership computing facility at the oak ridge national laboratory which is a us department of energy doe office of science user facility some of the co authors are employees of ut battelle llc under contract de ac05 00or22725 with the us department of energy accordingly the us government retains and the publisher by accepting the article for publication acknowledges that the us government retains a nonexclusive paid up irrevocable worldwide license to publish or reproduce the published form of this manuscript or allow others to do so for us government purposes doe will provide public access to these results of federally sponsored research in accordance with the doe public access plan http energy gov downloads doe public access plan appendix a numerical scheme the derivation of the numerical scheme from 1 to 2 is detailed here first integrating the hyperbolic system of equation 1 in ω i t n t where ω i is referred to the i th computational cell of the domain and t t n δ t and applying the gauss theorem 8 t n t ω i u t d s d t t n t ω i e d s d t t n t ω i s b d s d t where e f g applying the gauss theorem and replacing the contour integral by the sum across its four edges cartesian square grid of size δ x 9 t n t ω i u t d s d t t n t k 1 4 e n δ x d t t n t ω i s b d s d t with n n x n y the outward normal direction assuming a piecewise discretization and a roe s solver with an upwind discretization of fluxes and source terms the updating of the conserved variables can be written in flux difference splitting form as hubbard and garcia navarro 2000 10 u i u i n δ t δ x k 1 4 δ e n h b n k n δ e n k p λ a k h b n k p λ λ b b k the meaning of this expression is simple the conserved variables u h q x q y at each grid cell i will be updated according to the in going contributions that come from its four edges k east north west and south shared by the four neighbouring cells j a sketch of the scheme is shown in figure a 9 a fig 9 stencil for the proposed numerical scheme a and approximate solution for λ 1 0 λ 2 λ 3 0 b fig 9 equation a 3 includes the matrices and vectors coming from roe s linearization p is the matrix containing the three right eigenvectors e k of the jacobian matrix λ represents the diagonal matrix of the three eigenvalues λ k the minus superscript denotes the upwind discretization and a k and b b k account for the three wave and source strengths respectively with this it is easy to derive equation 2 from 10 the expressions for those matrices and vectors are detailed here 11 p k e 1 e 2 e 3 k 1 0 1 u c n x c n y u c n x v c n y c n x v c n y k λ k λ 1 0 0 0 λ 2 0 0 0 λ 3 k u n c 0 0 0 u n 0 0 0 u n c k λ k λ k λ k 2 a k p 1 δ u k α 1 α 2 α 3 k δ h 2 δ q n u n δ h 2 c 1 c δ q y v δ h n x δ q x u δ h n y δ h 2 δ q n u n δ h 2 c k u k u v k u i h i u j h j h i h j v i h i v j h j h i h j k c g h i h j 2 δ h k h j h i δ q k δ q x δ q y k q x j q x i q y j q y i k where u u v q x h q y h is the flow velocity the term b b k accounts for the bed slope source estimation in this work the integral formulation proposed in murillo and garcía navarro 2010 is adopted leading to the following definition of b b k 12 b b k β b 1 β b 2 β b 3 k g 2 c h p δ z 2 δ z 0 g 2 c h p δ z 2 δ z h p h i δ z 0 h j δ z 0 δ z h i δ z 0 and h i z i z j h j δ z 0 and h j z j z i z j z i otherwise although this estimation is more accurate than the conventional finite difference like approach it can still lead to non physical solutions when dealing when complex flows when these problems arise the conventional solution consists in reducing cfl and consequently the time step size impacting directly on the performance of the computation another approach is adopted here based on the augmented approach which allows us to reconstruct the cell averaged approximate solution with the objective of avoiding negative water depth values at time t this can be particularly useful for the treatment of wet dry fronts a key factor for reliable 2d hydraulic models in this work the wet dry treatment detailed in echeverribar et al 2019a murillo and garcía navarro 2010 is implemented following this approach the intermediate states of the riemann problem are analyzed and depending on their sign the information is sent to the left or the right side of the discontinuity this has been proved to be mass conservative and robust for any kind of flows echeverribar et al 2019a murillo and garcía navarro 2010 besides the wet dry fronts stiff source terms can lead to negative water depth values even in the presence of wet wet problems i e thin layers of water with strong bed discontinuities and high friction terms in order to consider these situations in our numerical scheme a similar technique of that followed in murillo and garcía navarro 2010 is adopted here however as the time step is computed before the source terms estimation a new limitation based on the final integration rather than zero the intermediate states is proposed in this work as detailed in murillo and garcía navarro 2010 only wet wet subcritical cases are analyzed let us denote l and r the left and right states of a discontinuity and assume λ 2 0 the derivation does not change for λ 2 0 due to the properties of the intermediate states for the water depth in the augmented approach the numerical solution at time t should guarantee h l 0 and h r 0 then according to figure a9 b the following inequations must be fulfilled 13 h l δ x 2 λ 1 δ t λ 1 δ t h 1 0 λ 2 δ t h 2 λ 3 λ 2 δ t h 3 h r δ x 2 λ 3 δ t 0 in agreement with the definitions of the intermediate states toro 2013 murillo and garcía navarro 2010 14 h 1 h l α 1 β b 1 λ 1 h 2 h 3 h r α 3 β b 1 λ 3 and doing some simple algebraic manipulations the following restrictions are derived for β b 1 and consequently for β b 3 15 β m i n β b 1 β m a x β m i n h l δ x 2 δ t α 1 λ 1 β m a x δ x 2 δ t h r λ 3 α 3 β b 3 β b 1 these conditions are implemented in triton minimizing the appearance of negative water depths additionally this scheme needs an entropy correction for transonic rarefactions hence the harten hyman entropy fix is used here as this approach is based on decomposing the existing jump into two new jumps a special emphasis should be put in the source term split murillo and garcía navarro 2010 two additional assumptions are considered first a cell is considered dry if its water depth is below 10 12 second velocities are set to zero for water depths below a tolerance this value depends on the characteristics of the problem study spatial scales and roughness mainly as a physical explanation for this parameter dissipation terms in practical applications with realistic roughness values dominate at this scale over convective and inertial terms therefore it is not arbitrary to model this phenomenon as a tolerance under which velocities are considered null a value of 10 3 is suggested for real world scenarios as a general recommendation although it can be also modified in triton for every configuration the scheme has been proved to be well balanced robust and reliable for flows under various conditions according to section 5 
25819,data driven modelling with machine learning ml is already being used for predictions in environmental science however it is less clear to what extent data driven models that successfully predict a phenomenon are representationally accurate and thus increase our understanding of the phenomenon besides empirical accuracy we propose three criteria to indirectly assess the relationships learned by the ml algorithms and how they relate to a phenomenon under investigation first consistency of the outcomes with background knowledge second the adequacy of the measurements datasets and methods used to construct a data driven model third the robustness of interpretable machine learning analyses across different ml algorithms we apply the three criteria with a case study modelling of the effect of different urban green infrastructure types on temperature and show that our approach improves the assessment of representational accuracy and reduces representational uncertainty which can improve the understanding of modelled phenomena keywords urban heat machine learning representational accuracy interpretable machine learning data driven modelling 1 introduction a data driven model is a model that detects associations in data using machine learning ml algorithms knüsel and baumberger 2020 such models have been applied successfully in environmental data science for many prediction tasks for example zumwald et al 2021 predicted urban temperature distributions for zürich from citizen weather stations cws satellite and open government data the engineering and selection of features is aided by a good understanding of the basic mechanisms that govern the temperature distribution in urban areas oke 1982 oke et al 2017 nevertheless in urban planning for heat mitigation scientific knowledge is mostly still used to propose general practical heuristics skelton 2020 such as increase and preserve the amount of urban greenery thorsson et al 2017 or large wooded parks within a city and large trees scattered across residential areas are needed to best mitigate the urban heat island effect davis et al 2016 such general guidelines may be helpful but urban planning would greatly benefit from a better understanding of how different urban green infrastructure types interact with each other and influence air or ambient temperature despite sufficiently high predictive accuracy in complex modelling tasks using many features and flexible algorithms it often remains unclear how to make use of data driven modelling approaches to increase our understanding of the mechanisms that lead to a specific urban temperature distribution pattern because the model is based only on associations in the data which do not per se represent causal relationships while there are methods that aim at extracting causal relationships from data see pearl 2009 applications for complex spatiotemporal learning problems are still largely lacking here we investigate how one can assess the representational accuracy given predictive accuracy in order to make use of data driven models for improving our understanding of environmental phenomena zumwald et al 2021 use citizen weather stations and machine learning to predict the temperature at high spatial and temporal resolutions for the city of zürich switzerland based on this case study we investigate how supervised machine learning can be used to increase our theoretical understanding of urban temperature distribution even though the basic mechanisms that govern the temperature distributions in urban areas are well understood the precise way that different surfaces and structures such as streets buildings and green infrastructure influence urban temperature is very complex and not sufficiently understood for example we do not have a very good understanding of the effect of the geometry of buildings on temperature which depends on the time of the day the season and the weather conditions konarska holmer et al 2016 here we focus on a better understanding of the effect of different urban green infrastructure types such as green areas street trees park trees and green roofs on urban temperature distribution methods that aim at understanding the learned relationships of a supervised machine learning algorithm are called interpretable machine learning 1 1 iml methods are often categorized into inherently interpretable methods and model agnostic post hoc analysis methods du et al 2019 inherent interpretability refers to methods that interpret based on the structure of the ml algorithm post hoc interpretability primarily refers to often model agnostic methods that interpret the trained model without assessing the model structure relying only on the relationship between feature values and predictions interpretation in the context of iml is understood as being able to explain how the algorithm derives the prediction doshi velez and kim 2018 with the aim of learning something about the target system murdoch et al 2019 some argue that interpretability is different for different agents and to different beliefs and goals of different agents tomsett et al 2018 hence not applying a general epistemic perspective others argue that what interpretability refers to is generally not clear lipton 2017 nevertheless there are approaches to increase the use of iml methods for example by formalizing what interpretability means dhurandhar et al 2017 or by showing how interactive visualizations tools can be used in a structured workflow approach baniecki and biecek 2020 iml murdoch et al 2019 here we use model intrinsic feature importance metrics and the post hoc model agnostic accumulated local effects ale method apley and zhu 2020 making use of a model with low representational accuracy is a source of uncertainty whether a data driven model can be used to increase our understanding of a phenomenon depends on how accurately the model represents it understanding requires that the model represent the phenomenon sufficiently accurately for a specific purpose how well a data driven model purely based on associations represents a target is not easy to assess here we propose a structured approach that aims to assess the representational accuracy of a ml model that predicts successfully a representationally accurate model has learned the relationships and processes that lead to the emergence of the phenomenon of interest however directly evaluating the representational accuracy of a data driven model is not possible empirical accuracy provides evidence that a data driven model is representationally accurate but is not sufficient to exclude the possibility that the model predicts correctly for the wrong reasons here we use three additional criteria to assess the representational accuracy first coherence with background knowledge second robustness i e the similarity of model outputs given different models for data driven models see knüsel and baumberger 2020 we then propose a further criterion the adequacy of the model set up concerning the first criterion we specifically investigate to what extent the model s predictions and the iml outputs are consistent with background knowledge for the second criterion we analyze the adequacy of the measurements datasets and methods i e the model setup which includes selecting datasets and algorithms feature engineering training and testing procedures finally for the third criterion the robustness of the model s predictions and the iml outputs is investigated using two ensemble algorithms a generalized random forest rf athey et al 2016 and xgboost chen and guestrin 2016 in section 2 we introduce the case study and the model used in this study in section 3 we present the results in section 4 we introduce our three criteria for assessing the representational accuracy of the model and use them to discuss the results critically we conclude in section 5 by drawing the implications of our case study for the more general question of the potentials and limitations of data driven models for understanding phenomena 2 case study 2 1 feature engineering the basic mechanisms that govern the temperature distribution in urban areas are well understood oke 1982 oke et al 2017 green roofs can reduce thermal energy entering the indoor environment by about half bevilacqua et al 2016 they can lower the outdoor roof surface and ambient temperature for a review see besir and cuce 2018 but the magnitude and spatial scale of changes are less clear trees are also an important measure to reduce urban heat the cooling effect of trees can be mainly attributed to shading but evapotranspiration contributes to cooling mainly around and shortly after sunset konarska uddling et al 2016 furthermore natural ground surface materials such as grass or cobblestone reduce immediate mean radiant temperature although less than direct shading by trees or buildings lindberg et al 2016 lindberg and grimmond 2011 to predict urban temperature distribution zumwald et al 2021 use building volume water forest rail roads urban green areas and altitude as geographic feature groups in this study we also use the land use classes buildings rail roads green areas forest and water as the foundation for spatial feature engineering see fig 1 the normalized differentiated vegetation index ndvi derived from sentinel 2 satellite data is used as a proxy for different vegetation types we furthermore differentiate four urban greening categories namely urban green areas green roofs park trees and street trees to identify green areas we overlay the ndvi layer with the green areas geographic features and set the ndvi threshold to 0 4 similarly for green roofs we overlay the ndvi with a building raster and apply the same threshold for the trees we use a vegetation height model for switzerland developed by ginzler and hobi 2015 reclassify it to four classes and overlay it with a street a rail and a green areas layer which results in a park and street tree layer we apply a gaussian filter 2 2 we also tested a sobel filter used for edge detection however it did not improve the predictive performance in the used modelling setup and for the land classes count grid cells with a certain land use type within a 10 30 50 100 250 or 500 m radius which leads to six features per feature group altitude is an important predictor as zürich s complex topography ranges from 406 m a s l to 670 m a s l as we do not know the exact position above ground of the sensors ground elevation is used as a predictor besides the spatial predictors we also use 35 meteorological predictors from a meteoswiss weather station in zürich in total 92 features were used for a full list of the used predictors see tables s1 and s2 in the si the inclusion of further predictors might improve accuracy of the model but the aim of this study is not to move beyond the state of the art in data driven urban temperature modelling but to test the conceptual framework and illustrate it with a case study 2 2 random forest and xgboost random forest rf and xgboost extreme gradient boosting are both based on decision tree ensembles the two algorithms differ in how they grow and how they combine the individual decision trees rf relies on a procedure called bagging bagging stands for bootstrap aggregation and describes the process of growing individual decision trees from a random bootstrap drawing with replacement sample and the subsequent aggregation of the individual trees to an ensemble in addition to pure bagging the rf algorithm also draws subsamples of the features when growing a tree this method leads to the desirable property of decorrelated features making the application of additional cross validation unnecessary since bagging is already a form of internal cross validation except for fitting hyperparameters the model can be evaluated repeatedly using the out of bag sample here we use the implementation from the generalized rf framework athey et al 2016 with bagging the individual decision trees are grown independently of each other in contrast xgboost chen and guestrin 2016 is based on a procedure called boosting here each tree relies on information from previous trees and the individual trees are not independent of each other specifically data tuples that yielded a bad performance have an increased chance of being drawn again the xgboost algorithm is a computationally highly optimized and regularized stochastic gradient boosting algorithm allowing for regularization has the advantage of preventing overfitting plus the stochastic gradient optimization computes second order partial derivatives which often improves the performance of the model 2 3 accumulated local effects ale approach in iml partial dependence plots pdps are a model agnostic methods to visualize the average partial relationship between the target variable and a selected feature friedman 2001 however in the case of correlated features pdps do not lead to reliable results because the method assumes independent features furthermore since pdps only give an average estimate heterogeneous effects are not visible individual conditional expectation ice plots aim to overcome this limitation and allow for investigation of heterogeneity across data goldstein et al 2014 but still assuming independent features here we use the accumulated local effects ale to understand the effect of individual features and their interaction on the target variable apley and zhu 2020 in contrast to pdps and ices ale also work in the case of strongly correlated covariates since their estimation is based on the conditional distribution between features the algorithm sets the bin width according to the data density as the bins are flexible also the rate of change is variable ale plots are displayed as line graphs in which each line of each interval represents the change in the model prediction when the selected feature has the given value compared to the average temperature prediction the averaged prediction differences per interval are summed up along the x axis hence the ale of a feature value that lies in the nth interval is the sum of the effects of the first through the nth interval finally the accumulated effects at each interval is centered such that the mean effect is zero also the extended approach using it in a spatial explicit way easily visualizes the effect of all features of one feature group furthermore the ale methodology allows the investigation of the interaction of two features which is visualized as a heat map showing the effect in addition to the individual effects 3 results the rf has a root mean square error rmse of 1 91 c and a mean average error mae of 1 58 c on the test data whereas the xgboost has a rmse of 1 37 c and mae of 1 08 c 3 3 an in depth validation of a similar modelling approach was performed by zumwald et al 2021 in section 3 1 the predicted temperature maps and predictions conditional on different surface types are shown in section 3 2 the results of the feature importance metrics are shown in section 3 3 the results of the iml method of ale are presented 3 1 conditional predictions fig 2 shows that when the model is applied to predict the temperature for the 30th of june 2019 at 15 00 a hot day for the district of the city of zürich the rf and xgboost predict the same general patterns over the city s area the rf has a mean of 31 5 c and xgboost of 32 8 c looking only at urban green areas rf has 31 8 c and xgboost 32 8 c temperatures in areas with buildings are 32 2 c and 33 5 c respectively and thus 0 4 c and 0 7 c higher than in green areas similar results hold for roads railways where the temperature is 32 3 c 0 5 c and 33 6 c 0 8 c respectively compared to green areas park trees are marginally cooler for rf with 31 7 c and don t change temperature for xgboost with 32 8 c street trees have an average temperature of 31 5 c with the rf which is 0 8 c lower compared to the average of railways and roads for xgboost the situation is similar with 32 6 c which is 1 c cooler than for the railways and roads fig s1 the si shows the temperature distribution conditional on the different surface categories 3 2 feature importance in tree based methods a frequency based feature importance metric counts the number of times a feature is used to split across all forests the frequency is normalized representing the proportion that a particular feature is used to split in the trees of the model in rf the spatial predictors sum to 4 2 whereas for xgboost the feature importance sums to 11 5 which is nearly three times higher both feature importance results indicate that altitude rail road and features evaluated across larger radii are more important fig 3 shows how the feature importance compares across the two algorithms for altitude building volume street trees park trees green roofs and green areas summed over all features that use them altitude is almost equally important for both algorithms in absolute terms but relative to the total importance of spatial variables altitude is more important for the rf algorithm similarly rf assigns relatively more importance to the building volume than xgboost for street trees park trees and green roofs the rf importance is only a fraction of the xgboost importance green areas are the third most important geographic feature type for rf another type of analysis aims at understanding the importance of different features of the same radius used when creating the variable this clearly shows for both algorithms an increase in importance with an increase in radius with the basic raster having a resolution of 10 x 10 m the variable importance for the individual spatial features can be found in fig s3 in the si the feature importance was investigated as sum of one class and xgboost is better in detecting signals that can be attributed to spatial factors also explaining the better overarching performance xgboost also attributes greater importance to all greening related variables than to altitude the investigation across the spatial extent clearly shows that accounting for larger areas of influence is important to prediction generally the xgboost assigns importance to variables at all radii compared to rf where mostly 250 and 500 m radii are assigned greater importance in the case of building volume even the basic raster and 10 m raster have large importance to xgboost 3 3 accumulated local effects ale to test for the strength of a monotonic non linear relationship between the ale and the individual features we use the kendall rank correlation coefficient see abdi 2007 fig 4 shows the absolute kendall correlation coefficient for all variables with more than two levels of the respective algorithm used variables with a high absolute value can be considered as having a clear increasing or decreasing ordinal trend and low absolute values are referring to a noisy signal as a working hypothesis we assume that any absolute value larger than 0 5 is referring to an ordinal trend and values lower than this threshold are shown gray a negative value blue refers to a reduction effect on temperature and a positive red to an increasing effect on temperature however the kendall rank correlation coefficient does not say anything about the magnitude of this relationship which needs to be assessed with the feature importance see fig s3 in si in fig 4 we see that for the rf the green areas feature all exhibit a trend which does not hold true for the xgboost results for park trees there are 4 5 features below an absolute kendall correlation coefficient of 0 5 in both algorithms some examples of the individual ale plots can be found in the si figs s4 and s5 to make use of the ale in a spatial setting we developed spatial ale plots fig 5 shows spatial ale plots for the green areas feature for both algorithms spatial ale plots sum up ale values for all radii per feature group per raster cell where a feature group is defined as the combined set of features with a common geographical category and varying evaluation radii hence they show the spatially explicit effect of a feature group on ale in c it is clearly visible that for instance urban parks and greener residential areas have a cooling effect the other spatial ale plots can be found in figs s6 and s7 in the si 4 understanding and reducing uncertainty due to unknown representational accuracy the extent to which a trained ml algorithm can be used to understand a phenomenon depends on the representational accuracy 4 4 the measurements and the features can also be inaccurate however we do not discuss this in this paper zumwald et al 2021 have shown that the measurements in this study are sufficiently accurate for many purposes and that the features rely on well curated open satellite data and hence a sufficiently high accuracy can be assumed of the model i e on how well the relationships learned by the model represent processes leading to a specific phenomenon moreover using a data driven model that has a low representational accuracy is a source of uncertainty assessing the representational accuracy of a model is thus of key importance however in contrast to its predictive accuracy the representational accuracy of a model cannot be assessed directly since it is impossible to directly compare the relationships learned by the model with the physical processes that lead to a phenomenon for this reason we suggest three criteria that allow us to assess the representational accuracy and thus reduce the representational uncertainties of data driven models representational uncertainty is the lack of knowledge about how well a model represents a phenomenon we can investigate the outputs in the results section the conditional prediction the feature importance and the ale plots on two different aggregation levels i e as individual features and feature groups it is important to note that in environmental science process based models representing causal processes directly e g via stocks and flows as in the case of climate models do not have exactly the same challenges to interpretability as data driven models process based models can be opaque to an epistemic agent because of their computational complexity or because data that is used in model development or evaluation is itself uncertain data driven models do not presuppose any structure and learn purely from uncertain data while uncertainty in data is relevant for all types of models for data driven models it is especially severe because erroneous data might lead an algorithm to learn incorrect structures this has important implications concerning the interpretability of data driven models first the uncertainty arising from datasets e g measurement uncertainty and the prediction uncertainty of the trained data driven model are harder to separate second many ml algorithms cannot optimize globally as for example in the case of an rf breiman 2001 hence data driven models might miss a global optimum third while induced randomness makes the algorithm more robust and performant in the case of the rf it blurs the tractability compared to single decision trees in section 4 1 we investigate whether the model outputs and the learned relations via iml methods are consistent with background knowledge however consistency with background knowledge is not sufficient to separate models that accurately represent causal relations in the real world from those that are based on spurious correlations as there are different levels of confidence of background knowledge and scientists want to remain open to counterintuitive findings a reason for counterintuitive findings can be non adequate elements in the model setup the engineered features for example might not be adequate for the intended purpose hence secondly in section 4 2 we analyze whether the measurements datasets and methods are adequate to construct a representationally accurate data driven model thirdly in section 4 3 robustness considerations are used to investigate whether a finding is an artefact of the ml algorithm or a spurious relationship between the data which reduces the representational accuracy of the model we present the analysis sequentially but in a real case the criteria should be applied in an iterative manner and not necessarily in any particular sequence 4 1 consistency with background knowledge background knowledge can be used to increase confidence that the trained data driven model correctly predicts for the right reasons which increases our confidence that the model is representationally accurate to assess consistency with background knowledge we first check whether the predictions conditional on different surface types are physically consistent hence plausible the models predict a temperature difference between green areas and roads and railways of 0 5 0 8 c in the afternoon in the literature similar results can be found for instance daytime air temperature in parks is 1 2 c cooler than surrounding areas depending on the time of the day bowler et al 2010 zhang et al 2019 aminipouri et al 2019 have shown that planting additional urban street trees can reduce average mean radiant temperature up to 1 3 c and hence have the potential to mitigate climate change locally under an rcp 4 5 scenario because our conditional predictions are consistent with background knowledge this increases our confidence in the representational accuracy of the models this predictive plausibility is used to assess the emerging behavior of the model but does not per se increase our understanding of the underlying learned relationships a model here can be consistent on the conditional predictive level but might behave as expected by chance or because of the wrong reasons hence consistency considerations regarding the relationships between the features need to be investigated using iml methods to rule out such behavior second we assess whether the feature importance metric results are consistent with background knowledge generally based on background knowledge it is difficult to form expectations about feature importance we know that the average environmental lapse rate is 0 65 k per 100 m altitude hence we expect that the feature altitude is of large importance fig 3 we also expect the feature group rail roads to have a comparably large importance the importance of different greening types is much more contested third we can check whether the ale plots are consistent with the behavior that is expected based on background knowledge concerning the ale we use knowledge about physical processes to build hypotheses about how specific features influence temperature for example for the building volume features over most radii show a positive kendall correlation coefficient as one would expect since building materials are absorbing much of incoming radiation the basic raster and the 10 m feature do not confirm the expectation however there is a potential explanation for this building volume is also associated with shading the more voluminous a building the larger a shadow it casts in our model we have not included this explicitly in any feature hence it might be that the shade offsets the expected temperature increase close to buildings this is a plausible explanation since temperature measurements probably represent conditions close to building walls because many cws are placed on balconies concerning the spatial ale plots figs s6 and s7 in the si the green areas green roofs and building volume feature groups are in line with our background knowledge street and park trees do not show a clear cooling effect on streets and parks these second and third points refer to the plausibility of the learned relations in addition the strength of evidence that informs any expectation needs to be assessed often background knowledge is not available or is less reliable for more specific expectations for example at the level of explaining the behavior of individual features 4 2 adequacy of the model setup one of the reasons for implausible findings can be an inadequate model setup a signal that is detected by an iml method can be an artefact of the iml method of the ml algorithm or it can be based on a spurious relationship between the measurement data and some features it is therefore important to disentangle the adequacy of the used model setup i e the measurements and features the ml algorithm and the iml methods and understand how they depend on each other a better understanding of the model setup allows us to better assess representational accuracy and reduce representational uncertainty fig 6 shows the relationship between the target phenomenon the measurement data the features the ml algorithm and the iml and how they depend on each other in our case study the crowd sensed temperature measurements 1 are representing air temperature distribution which is our target phenomenon however a more specific understanding of the measurement conditions helps to better understand the adequacy of the cws measurements for instance we know that there is uncertainty concerning the height of the sensor above ground see zumwald et al 2021 furthermore many cws measurements are close to building walls hence conditions with strong back radiation from walls are overrepresented the model features 2 are designed based on background knowledge see section 2 1 and our study s key influence factors for which we want to learn a relationship with temperature the features represent factors in the real world that influence the target phenomenon such as urban green infrastructure buildings and roads we know that features affect temperature distribution on different spatial scales konarska holmer et al 2016 hence we designed variations of the features taking into account different radii of influence in total we have 92 predictors 35 meteorological and 57 spatial ones the feature engineering is limited by our available background knowledge and pragmatic considerations such as available data an example is the rail roads feature group with the largest feature importance the spatial ale of the rail roads see fig s6 indicates that this feature group seems to represent rather a general urban heat island effect rather than the expected effect of sealed surfaces such as asphalt an ml algorithm 3 is chosen to learn the relationship between the measurement and the features an adequate ml algorithm needs to be sufficiently flexible previous studies have shown that the relationship between influence factors and urban temperature distribution do not have a high predictive accuracy when linear relationships are assumed voelkel and shandas 2017 and ensemble methods such as rf and xgboost are more appropriate finally iml approaches are used either to investigate the inner workings of the algorithm 4a e g via feature importance metrics or using model agnostic methods in post hoc analysis 4b in our case using ale plots from theory we know that ale plots should be able to deal with correlated features apley and zhu 2020 which is important in the present case 4 3 robustness if background knowledge can explain the model s behavior this increases confidence in the representational accuracy of the trained model robustness considerations can be used to further investigate the representational accuracy robustness indicates that a result is insensitive to different assumptions be it parameter values in a model structural elements or using different kinds of representations weisberg and reisman 2008 here we analyze the predictions of the different ensemble based ml algorithms rf and xgboost which can be thought of as between an examination of structural and representational robustness generally robustness increases the confidence that the ml algorithm is adequate to model the system under investigation and a contradicting iml output of two different algorithms hence a non robust finding means our used algorithm may not be adequate we investigate the robustness of the model s mapping and conditional predictions feature importance metrics and ale plots for individual features and aggregated spatially explicit features fig 2 shows the predictions for both algorithms used and while the granularity seems different the hotspots and patterns are robust across both algorithms furthermore in fig s1 in the si we see that the conditional predictions are also producing robust results concerning the feature importance we see in fig 3 that the importance of different feature categories especially those concerning green infrastructure is not robust the same holds true for features grouped with 10 and 30 m radii generally the xgboost attributes more importance to the urban green infrastructure and as well as to lower radii for the individual features we define robustness as the kendall correlation between the ale of both methods and as a working hypothesis we assume that values below 0 5 indicate a non robust behavior and that values larger than 0 5 indicate a robust behavior fig 7 for the spatial ale plots we define robustness as the pearson correlation between the two predicted maps and say correlations larger than 0 5 are robust here we choose the pearson over the kendall correlation coefficient because we aim at testing the linear relationship between the spatial ale plots robustness is always interpreted together with the plausibility of a result which is assessed using background knowledge in fig 7 the robustness is shown by the monotonic non linear association between ale values of the rf and the xgboost algorithms and non robust relationships are shaded gray a cross implies a both plausible and robust result and we find this holds true for a minority of features for the building volume and rail road feature groups a higher temperature with larger values of the features is expected red in fig 4 for the remaining features a decreasing effect is expected blue in fig 4 for the building volume feature group we see that the behavior of all features can be considered robust this might increase trust in the potential explanation of the behavior of the building volume basic raster and 10 m features which was discussed in section 4 1 however the signal could still be an artefact in the data although there is robustness in the general signal for green areas the comparison indicates that mainly the 100 m and 50 m features are robust the effects of green roofs and green areas are plausible and robust for smaller and mid sized radii altitude and the majority of building volume features are robust and consistent with background knowledge rail road only has one consistent feature although the most important one is robust and confirms expectations for the forest feature group all robust features also confirm expectations as discussed in section 4 2 when analyzing the robustness as pearson correlation between the spatial ale maps the green area feature group has a pearson correlation of 0 96 street trees of 0 57 park trees of 0 70 and green roofs of 0 74 hence all can be considered robust there is a trade off between confidence in the plausibility of an effect and the adequacy of the model setup a non robust result containing plausible and implausible effects decreases confidence in the adequacy of the model setup more when the confidence in the plausibility assessment is high and vice versa for example the modelled effect of green areas within 100 m is a robust and plausible result and increases the confidence in both the model setup and the plausibility assessment however if a robust result contradicts the expected behavior then the confidence in the expectation needs to be assessed the higher the confidence the more likely it is that the model setup is not adequate when the detected signal is robustly noisy then this is likely because the model setup is not suited to detect the signal or because there is no such assumed relationship also if the relationship between a feature and the target phenomenon is noisy indicated by kendall value close to zero in fig 4 and this is robust across both ml algorithms it increases confidence that the finding is an artefact of the data either the measurement or features rather than the ml algorithm and no real world signal could be detected 5 conclusion increasingly high resolution data is available for urban areas see creutzig et al 2019 and the potential applications of data driven modeling are increasing data driven modeling is for example used to emulate spatiotemporal climate model outputs beusch et al 2020 to better understand marine liquid water clouds andersen et al 2017 or to predict the dynamics of a simple general circulation model as used in numerical weather prediction models scher 2018 predictive accuracy is a good indicator that a model represents the target phenomenon accurately however it is not a sufficient condition as the model might predict correctly for the wrong reasons for climate models see baumberger et al 2017 and overfit to noise and artefacts in the data to increase process understanding in earth science by deep learning methods reichstein et al 2019 argue that model interpretability and physical consistency of predictions are crucial for the adoption of deep learning methods for this purpose knowledge about the underlying causal structure background knowledge and appropriate methods for visualizations are also required zhao and hastie 2019 5 1 aiming at a better understanding of representational accuracy we have shown that the presented approach using the criteria of consistency with background knowledge adequacy of model setup and robustness allows us to understand the representational accuracy of the model for example in the presented modelling approach altitude is a single feature with no derived features and is representationally accurate 5 5 when we refer to a feature or feature group being representationally accurate our meaning is that the relationship between this feature and the predicted temperature is representationally accurate the building volume features also prove to be representationally accurate however rail road might as discussed instead represent a general urban heat island effect and cannot be considered representationally accurate green areas and green roofs are representationally accurate on the aggregated level but not on the level of individual features that is we have several features e g representing green roofs with different radii of influence and fail to extract a plausible signal form the individual signals while the aggregation over all features can show some signs of representational accuracy in contrast park trees and street trees features are not representationally accurate individually or in aggregate considering green areas a potential reason for the poor representational accuracy of individual features might be that we do not systematically investigate second order effects between features for example we would expect the green area 500 m feature to exhibit a temperature reduction effect but see the opposite fig 4 to better understand the result of the implausible 500 m feature we plotted the interaction of the 250 and 500 m features fig s2 in the si we see that high values of the 250 m feature and values in the lower range of the 500 m feature lead to lower temperature values with an effect of up to 0 5 c for the xgboost this is an example where it is unclear whether the models overfit to an artefact in the data or whether there is an interesting relationship between green areas on different scales that needs further investigation here we have investigated the robustness only on the level of different ml algorithms however this idea is much more generally applicable for instance different predictions from the same algorithms can also be used in a robustness analysis the uncertainty of iml outputs if the methods allow could also be included in the robustness analysis analogously to extrapolation uncertainty iml estimates are also less reliable in data sparse regions in a more systematic way different elements of the model setup and not just the ml algorithm could be varied to create ensembles to more systematically investigate and reduce uncertainty the three criteria presented here aim to indirectly assess the representational accuracy of a trained data driven model and reduce representational uncertainty while these criteria have proven to be successful for the presented case study they can just as well be applied to other investigations in climate and environmental science 5 2 required representational accuracy depends on adequacy for purpose while we cannot say that a model is representationally accurate in its entirety it can be accurate enough for certain purposes e g to answer certain questions depending on the features and how they are represented models that are representationally inaccurate can also be useful for other reasons for instance if the aim is only short term prediction then representational accuracy is not needed as long as we can guarantee that the model has predicted well before and the system we predict has not changed knüsel et al 2019 but as soon as we aim at understanding the underlying phenomenon a certain representational accuracy becomes necessary for example our analysis has shown that not all individual features contribute to predictions in a way consistent with background knowledge the green areas feature group created plausible average temperature differences between green areas and streets in contrast we can conclude that our model cannot be used to investigate the effect of street trees which did not show a clear temperature signal so besides correct conditional predictions our model also helps to gain insights concerning certain individual features and of feature groups representing different urban greening infrastructure types 5 3 combining data driven and process based models an approach to increase the likelihood of a representationally accurate data driven model would be to constrain the data driven model with process based understanding in so called hybrid models in climate science methods have recently been developed that can physically constrain the machine learning of evapotranspiration zhao et al 2019 and achieve conservation of energy when using neural network emulators for global climate model emulators beucler et al 2019 using conditional generative adversarial networks it has recently been shown that urban land use can be predicted using physical constraints albert et al 2019 hence constraining data driven models e g by energy conservation approaches could in principle be applied to urban climate modelling however applications in this domain are to the best of our knowledge still lacking knowledge about physical processes might also be used to further improve the feature engineering and increase the representational accuracy of models one possible example is to explicitly use shading patterns of buildings dependent on the time of the day as additional features 5 4 final remarks it is not guaranteed that plausible results emerge just because a model represents relationships between individual features and the target variable robustly and plausibly this potential discrepancy cannot be understood by the means of iml methods alone we proposed three criteria to guide the assessment how accurate the data driven model represents relationships of the phenomenon of interest coherence with background knowledge robustness and the adequacy of the model set up by evaluating the representational accuracy via those criteria representational uncertainty is reduced and one gets a better understanding for which purpose the data driven model at hand might be adequate this improves the value of applying data driven models in the environmental science declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we acknowledge the helpful thoughts and comments by gertrude hirsch hadorn and christopher fairless this study was funded by the swiss national science foundation national research programme 75 big data project number 167215 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105048 
25819,data driven modelling with machine learning ml is already being used for predictions in environmental science however it is less clear to what extent data driven models that successfully predict a phenomenon are representationally accurate and thus increase our understanding of the phenomenon besides empirical accuracy we propose three criteria to indirectly assess the relationships learned by the ml algorithms and how they relate to a phenomenon under investigation first consistency of the outcomes with background knowledge second the adequacy of the measurements datasets and methods used to construct a data driven model third the robustness of interpretable machine learning analyses across different ml algorithms we apply the three criteria with a case study modelling of the effect of different urban green infrastructure types on temperature and show that our approach improves the assessment of representational accuracy and reduces representational uncertainty which can improve the understanding of modelled phenomena keywords urban heat machine learning representational accuracy interpretable machine learning data driven modelling 1 introduction a data driven model is a model that detects associations in data using machine learning ml algorithms knüsel and baumberger 2020 such models have been applied successfully in environmental data science for many prediction tasks for example zumwald et al 2021 predicted urban temperature distributions for zürich from citizen weather stations cws satellite and open government data the engineering and selection of features is aided by a good understanding of the basic mechanisms that govern the temperature distribution in urban areas oke 1982 oke et al 2017 nevertheless in urban planning for heat mitigation scientific knowledge is mostly still used to propose general practical heuristics skelton 2020 such as increase and preserve the amount of urban greenery thorsson et al 2017 or large wooded parks within a city and large trees scattered across residential areas are needed to best mitigate the urban heat island effect davis et al 2016 such general guidelines may be helpful but urban planning would greatly benefit from a better understanding of how different urban green infrastructure types interact with each other and influence air or ambient temperature despite sufficiently high predictive accuracy in complex modelling tasks using many features and flexible algorithms it often remains unclear how to make use of data driven modelling approaches to increase our understanding of the mechanisms that lead to a specific urban temperature distribution pattern because the model is based only on associations in the data which do not per se represent causal relationships while there are methods that aim at extracting causal relationships from data see pearl 2009 applications for complex spatiotemporal learning problems are still largely lacking here we investigate how one can assess the representational accuracy given predictive accuracy in order to make use of data driven models for improving our understanding of environmental phenomena zumwald et al 2021 use citizen weather stations and machine learning to predict the temperature at high spatial and temporal resolutions for the city of zürich switzerland based on this case study we investigate how supervised machine learning can be used to increase our theoretical understanding of urban temperature distribution even though the basic mechanisms that govern the temperature distributions in urban areas are well understood the precise way that different surfaces and structures such as streets buildings and green infrastructure influence urban temperature is very complex and not sufficiently understood for example we do not have a very good understanding of the effect of the geometry of buildings on temperature which depends on the time of the day the season and the weather conditions konarska holmer et al 2016 here we focus on a better understanding of the effect of different urban green infrastructure types such as green areas street trees park trees and green roofs on urban temperature distribution methods that aim at understanding the learned relationships of a supervised machine learning algorithm are called interpretable machine learning 1 1 iml methods are often categorized into inherently interpretable methods and model agnostic post hoc analysis methods du et al 2019 inherent interpretability refers to methods that interpret based on the structure of the ml algorithm post hoc interpretability primarily refers to often model agnostic methods that interpret the trained model without assessing the model structure relying only on the relationship between feature values and predictions interpretation in the context of iml is understood as being able to explain how the algorithm derives the prediction doshi velez and kim 2018 with the aim of learning something about the target system murdoch et al 2019 some argue that interpretability is different for different agents and to different beliefs and goals of different agents tomsett et al 2018 hence not applying a general epistemic perspective others argue that what interpretability refers to is generally not clear lipton 2017 nevertheless there are approaches to increase the use of iml methods for example by formalizing what interpretability means dhurandhar et al 2017 or by showing how interactive visualizations tools can be used in a structured workflow approach baniecki and biecek 2020 iml murdoch et al 2019 here we use model intrinsic feature importance metrics and the post hoc model agnostic accumulated local effects ale method apley and zhu 2020 making use of a model with low representational accuracy is a source of uncertainty whether a data driven model can be used to increase our understanding of a phenomenon depends on how accurately the model represents it understanding requires that the model represent the phenomenon sufficiently accurately for a specific purpose how well a data driven model purely based on associations represents a target is not easy to assess here we propose a structured approach that aims to assess the representational accuracy of a ml model that predicts successfully a representationally accurate model has learned the relationships and processes that lead to the emergence of the phenomenon of interest however directly evaluating the representational accuracy of a data driven model is not possible empirical accuracy provides evidence that a data driven model is representationally accurate but is not sufficient to exclude the possibility that the model predicts correctly for the wrong reasons here we use three additional criteria to assess the representational accuracy first coherence with background knowledge second robustness i e the similarity of model outputs given different models for data driven models see knüsel and baumberger 2020 we then propose a further criterion the adequacy of the model set up concerning the first criterion we specifically investigate to what extent the model s predictions and the iml outputs are consistent with background knowledge for the second criterion we analyze the adequacy of the measurements datasets and methods i e the model setup which includes selecting datasets and algorithms feature engineering training and testing procedures finally for the third criterion the robustness of the model s predictions and the iml outputs is investigated using two ensemble algorithms a generalized random forest rf athey et al 2016 and xgboost chen and guestrin 2016 in section 2 we introduce the case study and the model used in this study in section 3 we present the results in section 4 we introduce our three criteria for assessing the representational accuracy of the model and use them to discuss the results critically we conclude in section 5 by drawing the implications of our case study for the more general question of the potentials and limitations of data driven models for understanding phenomena 2 case study 2 1 feature engineering the basic mechanisms that govern the temperature distribution in urban areas are well understood oke 1982 oke et al 2017 green roofs can reduce thermal energy entering the indoor environment by about half bevilacqua et al 2016 they can lower the outdoor roof surface and ambient temperature for a review see besir and cuce 2018 but the magnitude and spatial scale of changes are less clear trees are also an important measure to reduce urban heat the cooling effect of trees can be mainly attributed to shading but evapotranspiration contributes to cooling mainly around and shortly after sunset konarska uddling et al 2016 furthermore natural ground surface materials such as grass or cobblestone reduce immediate mean radiant temperature although less than direct shading by trees or buildings lindberg et al 2016 lindberg and grimmond 2011 to predict urban temperature distribution zumwald et al 2021 use building volume water forest rail roads urban green areas and altitude as geographic feature groups in this study we also use the land use classes buildings rail roads green areas forest and water as the foundation for spatial feature engineering see fig 1 the normalized differentiated vegetation index ndvi derived from sentinel 2 satellite data is used as a proxy for different vegetation types we furthermore differentiate four urban greening categories namely urban green areas green roofs park trees and street trees to identify green areas we overlay the ndvi layer with the green areas geographic features and set the ndvi threshold to 0 4 similarly for green roofs we overlay the ndvi with a building raster and apply the same threshold for the trees we use a vegetation height model for switzerland developed by ginzler and hobi 2015 reclassify it to four classes and overlay it with a street a rail and a green areas layer which results in a park and street tree layer we apply a gaussian filter 2 2 we also tested a sobel filter used for edge detection however it did not improve the predictive performance in the used modelling setup and for the land classes count grid cells with a certain land use type within a 10 30 50 100 250 or 500 m radius which leads to six features per feature group altitude is an important predictor as zürich s complex topography ranges from 406 m a s l to 670 m a s l as we do not know the exact position above ground of the sensors ground elevation is used as a predictor besides the spatial predictors we also use 35 meteorological predictors from a meteoswiss weather station in zürich in total 92 features were used for a full list of the used predictors see tables s1 and s2 in the si the inclusion of further predictors might improve accuracy of the model but the aim of this study is not to move beyond the state of the art in data driven urban temperature modelling but to test the conceptual framework and illustrate it with a case study 2 2 random forest and xgboost random forest rf and xgboost extreme gradient boosting are both based on decision tree ensembles the two algorithms differ in how they grow and how they combine the individual decision trees rf relies on a procedure called bagging bagging stands for bootstrap aggregation and describes the process of growing individual decision trees from a random bootstrap drawing with replacement sample and the subsequent aggregation of the individual trees to an ensemble in addition to pure bagging the rf algorithm also draws subsamples of the features when growing a tree this method leads to the desirable property of decorrelated features making the application of additional cross validation unnecessary since bagging is already a form of internal cross validation except for fitting hyperparameters the model can be evaluated repeatedly using the out of bag sample here we use the implementation from the generalized rf framework athey et al 2016 with bagging the individual decision trees are grown independently of each other in contrast xgboost chen and guestrin 2016 is based on a procedure called boosting here each tree relies on information from previous trees and the individual trees are not independent of each other specifically data tuples that yielded a bad performance have an increased chance of being drawn again the xgboost algorithm is a computationally highly optimized and regularized stochastic gradient boosting algorithm allowing for regularization has the advantage of preventing overfitting plus the stochastic gradient optimization computes second order partial derivatives which often improves the performance of the model 2 3 accumulated local effects ale approach in iml partial dependence plots pdps are a model agnostic methods to visualize the average partial relationship between the target variable and a selected feature friedman 2001 however in the case of correlated features pdps do not lead to reliable results because the method assumes independent features furthermore since pdps only give an average estimate heterogeneous effects are not visible individual conditional expectation ice plots aim to overcome this limitation and allow for investigation of heterogeneity across data goldstein et al 2014 but still assuming independent features here we use the accumulated local effects ale to understand the effect of individual features and their interaction on the target variable apley and zhu 2020 in contrast to pdps and ices ale also work in the case of strongly correlated covariates since their estimation is based on the conditional distribution between features the algorithm sets the bin width according to the data density as the bins are flexible also the rate of change is variable ale plots are displayed as line graphs in which each line of each interval represents the change in the model prediction when the selected feature has the given value compared to the average temperature prediction the averaged prediction differences per interval are summed up along the x axis hence the ale of a feature value that lies in the nth interval is the sum of the effects of the first through the nth interval finally the accumulated effects at each interval is centered such that the mean effect is zero also the extended approach using it in a spatial explicit way easily visualizes the effect of all features of one feature group furthermore the ale methodology allows the investigation of the interaction of two features which is visualized as a heat map showing the effect in addition to the individual effects 3 results the rf has a root mean square error rmse of 1 91 c and a mean average error mae of 1 58 c on the test data whereas the xgboost has a rmse of 1 37 c and mae of 1 08 c 3 3 an in depth validation of a similar modelling approach was performed by zumwald et al 2021 in section 3 1 the predicted temperature maps and predictions conditional on different surface types are shown in section 3 2 the results of the feature importance metrics are shown in section 3 3 the results of the iml method of ale are presented 3 1 conditional predictions fig 2 shows that when the model is applied to predict the temperature for the 30th of june 2019 at 15 00 a hot day for the district of the city of zürich the rf and xgboost predict the same general patterns over the city s area the rf has a mean of 31 5 c and xgboost of 32 8 c looking only at urban green areas rf has 31 8 c and xgboost 32 8 c temperatures in areas with buildings are 32 2 c and 33 5 c respectively and thus 0 4 c and 0 7 c higher than in green areas similar results hold for roads railways where the temperature is 32 3 c 0 5 c and 33 6 c 0 8 c respectively compared to green areas park trees are marginally cooler for rf with 31 7 c and don t change temperature for xgboost with 32 8 c street trees have an average temperature of 31 5 c with the rf which is 0 8 c lower compared to the average of railways and roads for xgboost the situation is similar with 32 6 c which is 1 c cooler than for the railways and roads fig s1 the si shows the temperature distribution conditional on the different surface categories 3 2 feature importance in tree based methods a frequency based feature importance metric counts the number of times a feature is used to split across all forests the frequency is normalized representing the proportion that a particular feature is used to split in the trees of the model in rf the spatial predictors sum to 4 2 whereas for xgboost the feature importance sums to 11 5 which is nearly three times higher both feature importance results indicate that altitude rail road and features evaluated across larger radii are more important fig 3 shows how the feature importance compares across the two algorithms for altitude building volume street trees park trees green roofs and green areas summed over all features that use them altitude is almost equally important for both algorithms in absolute terms but relative to the total importance of spatial variables altitude is more important for the rf algorithm similarly rf assigns relatively more importance to the building volume than xgboost for street trees park trees and green roofs the rf importance is only a fraction of the xgboost importance green areas are the third most important geographic feature type for rf another type of analysis aims at understanding the importance of different features of the same radius used when creating the variable this clearly shows for both algorithms an increase in importance with an increase in radius with the basic raster having a resolution of 10 x 10 m the variable importance for the individual spatial features can be found in fig s3 in the si the feature importance was investigated as sum of one class and xgboost is better in detecting signals that can be attributed to spatial factors also explaining the better overarching performance xgboost also attributes greater importance to all greening related variables than to altitude the investigation across the spatial extent clearly shows that accounting for larger areas of influence is important to prediction generally the xgboost assigns importance to variables at all radii compared to rf where mostly 250 and 500 m radii are assigned greater importance in the case of building volume even the basic raster and 10 m raster have large importance to xgboost 3 3 accumulated local effects ale to test for the strength of a monotonic non linear relationship between the ale and the individual features we use the kendall rank correlation coefficient see abdi 2007 fig 4 shows the absolute kendall correlation coefficient for all variables with more than two levels of the respective algorithm used variables with a high absolute value can be considered as having a clear increasing or decreasing ordinal trend and low absolute values are referring to a noisy signal as a working hypothesis we assume that any absolute value larger than 0 5 is referring to an ordinal trend and values lower than this threshold are shown gray a negative value blue refers to a reduction effect on temperature and a positive red to an increasing effect on temperature however the kendall rank correlation coefficient does not say anything about the magnitude of this relationship which needs to be assessed with the feature importance see fig s3 in si in fig 4 we see that for the rf the green areas feature all exhibit a trend which does not hold true for the xgboost results for park trees there are 4 5 features below an absolute kendall correlation coefficient of 0 5 in both algorithms some examples of the individual ale plots can be found in the si figs s4 and s5 to make use of the ale in a spatial setting we developed spatial ale plots fig 5 shows spatial ale plots for the green areas feature for both algorithms spatial ale plots sum up ale values for all radii per feature group per raster cell where a feature group is defined as the combined set of features with a common geographical category and varying evaluation radii hence they show the spatially explicit effect of a feature group on ale in c it is clearly visible that for instance urban parks and greener residential areas have a cooling effect the other spatial ale plots can be found in figs s6 and s7 in the si 4 understanding and reducing uncertainty due to unknown representational accuracy the extent to which a trained ml algorithm can be used to understand a phenomenon depends on the representational accuracy 4 4 the measurements and the features can also be inaccurate however we do not discuss this in this paper zumwald et al 2021 have shown that the measurements in this study are sufficiently accurate for many purposes and that the features rely on well curated open satellite data and hence a sufficiently high accuracy can be assumed of the model i e on how well the relationships learned by the model represent processes leading to a specific phenomenon moreover using a data driven model that has a low representational accuracy is a source of uncertainty assessing the representational accuracy of a model is thus of key importance however in contrast to its predictive accuracy the representational accuracy of a model cannot be assessed directly since it is impossible to directly compare the relationships learned by the model with the physical processes that lead to a phenomenon for this reason we suggest three criteria that allow us to assess the representational accuracy and thus reduce the representational uncertainties of data driven models representational uncertainty is the lack of knowledge about how well a model represents a phenomenon we can investigate the outputs in the results section the conditional prediction the feature importance and the ale plots on two different aggregation levels i e as individual features and feature groups it is important to note that in environmental science process based models representing causal processes directly e g via stocks and flows as in the case of climate models do not have exactly the same challenges to interpretability as data driven models process based models can be opaque to an epistemic agent because of their computational complexity or because data that is used in model development or evaluation is itself uncertain data driven models do not presuppose any structure and learn purely from uncertain data while uncertainty in data is relevant for all types of models for data driven models it is especially severe because erroneous data might lead an algorithm to learn incorrect structures this has important implications concerning the interpretability of data driven models first the uncertainty arising from datasets e g measurement uncertainty and the prediction uncertainty of the trained data driven model are harder to separate second many ml algorithms cannot optimize globally as for example in the case of an rf breiman 2001 hence data driven models might miss a global optimum third while induced randomness makes the algorithm more robust and performant in the case of the rf it blurs the tractability compared to single decision trees in section 4 1 we investigate whether the model outputs and the learned relations via iml methods are consistent with background knowledge however consistency with background knowledge is not sufficient to separate models that accurately represent causal relations in the real world from those that are based on spurious correlations as there are different levels of confidence of background knowledge and scientists want to remain open to counterintuitive findings a reason for counterintuitive findings can be non adequate elements in the model setup the engineered features for example might not be adequate for the intended purpose hence secondly in section 4 2 we analyze whether the measurements datasets and methods are adequate to construct a representationally accurate data driven model thirdly in section 4 3 robustness considerations are used to investigate whether a finding is an artefact of the ml algorithm or a spurious relationship between the data which reduces the representational accuracy of the model we present the analysis sequentially but in a real case the criteria should be applied in an iterative manner and not necessarily in any particular sequence 4 1 consistency with background knowledge background knowledge can be used to increase confidence that the trained data driven model correctly predicts for the right reasons which increases our confidence that the model is representationally accurate to assess consistency with background knowledge we first check whether the predictions conditional on different surface types are physically consistent hence plausible the models predict a temperature difference between green areas and roads and railways of 0 5 0 8 c in the afternoon in the literature similar results can be found for instance daytime air temperature in parks is 1 2 c cooler than surrounding areas depending on the time of the day bowler et al 2010 zhang et al 2019 aminipouri et al 2019 have shown that planting additional urban street trees can reduce average mean radiant temperature up to 1 3 c and hence have the potential to mitigate climate change locally under an rcp 4 5 scenario because our conditional predictions are consistent with background knowledge this increases our confidence in the representational accuracy of the models this predictive plausibility is used to assess the emerging behavior of the model but does not per se increase our understanding of the underlying learned relationships a model here can be consistent on the conditional predictive level but might behave as expected by chance or because of the wrong reasons hence consistency considerations regarding the relationships between the features need to be investigated using iml methods to rule out such behavior second we assess whether the feature importance metric results are consistent with background knowledge generally based on background knowledge it is difficult to form expectations about feature importance we know that the average environmental lapse rate is 0 65 k per 100 m altitude hence we expect that the feature altitude is of large importance fig 3 we also expect the feature group rail roads to have a comparably large importance the importance of different greening types is much more contested third we can check whether the ale plots are consistent with the behavior that is expected based on background knowledge concerning the ale we use knowledge about physical processes to build hypotheses about how specific features influence temperature for example for the building volume features over most radii show a positive kendall correlation coefficient as one would expect since building materials are absorbing much of incoming radiation the basic raster and the 10 m feature do not confirm the expectation however there is a potential explanation for this building volume is also associated with shading the more voluminous a building the larger a shadow it casts in our model we have not included this explicitly in any feature hence it might be that the shade offsets the expected temperature increase close to buildings this is a plausible explanation since temperature measurements probably represent conditions close to building walls because many cws are placed on balconies concerning the spatial ale plots figs s6 and s7 in the si the green areas green roofs and building volume feature groups are in line with our background knowledge street and park trees do not show a clear cooling effect on streets and parks these second and third points refer to the plausibility of the learned relations in addition the strength of evidence that informs any expectation needs to be assessed often background knowledge is not available or is less reliable for more specific expectations for example at the level of explaining the behavior of individual features 4 2 adequacy of the model setup one of the reasons for implausible findings can be an inadequate model setup a signal that is detected by an iml method can be an artefact of the iml method of the ml algorithm or it can be based on a spurious relationship between the measurement data and some features it is therefore important to disentangle the adequacy of the used model setup i e the measurements and features the ml algorithm and the iml methods and understand how they depend on each other a better understanding of the model setup allows us to better assess representational accuracy and reduce representational uncertainty fig 6 shows the relationship between the target phenomenon the measurement data the features the ml algorithm and the iml and how they depend on each other in our case study the crowd sensed temperature measurements 1 are representing air temperature distribution which is our target phenomenon however a more specific understanding of the measurement conditions helps to better understand the adequacy of the cws measurements for instance we know that there is uncertainty concerning the height of the sensor above ground see zumwald et al 2021 furthermore many cws measurements are close to building walls hence conditions with strong back radiation from walls are overrepresented the model features 2 are designed based on background knowledge see section 2 1 and our study s key influence factors for which we want to learn a relationship with temperature the features represent factors in the real world that influence the target phenomenon such as urban green infrastructure buildings and roads we know that features affect temperature distribution on different spatial scales konarska holmer et al 2016 hence we designed variations of the features taking into account different radii of influence in total we have 92 predictors 35 meteorological and 57 spatial ones the feature engineering is limited by our available background knowledge and pragmatic considerations such as available data an example is the rail roads feature group with the largest feature importance the spatial ale of the rail roads see fig s6 indicates that this feature group seems to represent rather a general urban heat island effect rather than the expected effect of sealed surfaces such as asphalt an ml algorithm 3 is chosen to learn the relationship between the measurement and the features an adequate ml algorithm needs to be sufficiently flexible previous studies have shown that the relationship between influence factors and urban temperature distribution do not have a high predictive accuracy when linear relationships are assumed voelkel and shandas 2017 and ensemble methods such as rf and xgboost are more appropriate finally iml approaches are used either to investigate the inner workings of the algorithm 4a e g via feature importance metrics or using model agnostic methods in post hoc analysis 4b in our case using ale plots from theory we know that ale plots should be able to deal with correlated features apley and zhu 2020 which is important in the present case 4 3 robustness if background knowledge can explain the model s behavior this increases confidence in the representational accuracy of the trained model robustness considerations can be used to further investigate the representational accuracy robustness indicates that a result is insensitive to different assumptions be it parameter values in a model structural elements or using different kinds of representations weisberg and reisman 2008 here we analyze the predictions of the different ensemble based ml algorithms rf and xgboost which can be thought of as between an examination of structural and representational robustness generally robustness increases the confidence that the ml algorithm is adequate to model the system under investigation and a contradicting iml output of two different algorithms hence a non robust finding means our used algorithm may not be adequate we investigate the robustness of the model s mapping and conditional predictions feature importance metrics and ale plots for individual features and aggregated spatially explicit features fig 2 shows the predictions for both algorithms used and while the granularity seems different the hotspots and patterns are robust across both algorithms furthermore in fig s1 in the si we see that the conditional predictions are also producing robust results concerning the feature importance we see in fig 3 that the importance of different feature categories especially those concerning green infrastructure is not robust the same holds true for features grouped with 10 and 30 m radii generally the xgboost attributes more importance to the urban green infrastructure and as well as to lower radii for the individual features we define robustness as the kendall correlation between the ale of both methods and as a working hypothesis we assume that values below 0 5 indicate a non robust behavior and that values larger than 0 5 indicate a robust behavior fig 7 for the spatial ale plots we define robustness as the pearson correlation between the two predicted maps and say correlations larger than 0 5 are robust here we choose the pearson over the kendall correlation coefficient because we aim at testing the linear relationship between the spatial ale plots robustness is always interpreted together with the plausibility of a result which is assessed using background knowledge in fig 7 the robustness is shown by the monotonic non linear association between ale values of the rf and the xgboost algorithms and non robust relationships are shaded gray a cross implies a both plausible and robust result and we find this holds true for a minority of features for the building volume and rail road feature groups a higher temperature with larger values of the features is expected red in fig 4 for the remaining features a decreasing effect is expected blue in fig 4 for the building volume feature group we see that the behavior of all features can be considered robust this might increase trust in the potential explanation of the behavior of the building volume basic raster and 10 m features which was discussed in section 4 1 however the signal could still be an artefact in the data although there is robustness in the general signal for green areas the comparison indicates that mainly the 100 m and 50 m features are robust the effects of green roofs and green areas are plausible and robust for smaller and mid sized radii altitude and the majority of building volume features are robust and consistent with background knowledge rail road only has one consistent feature although the most important one is robust and confirms expectations for the forest feature group all robust features also confirm expectations as discussed in section 4 2 when analyzing the robustness as pearson correlation between the spatial ale maps the green area feature group has a pearson correlation of 0 96 street trees of 0 57 park trees of 0 70 and green roofs of 0 74 hence all can be considered robust there is a trade off between confidence in the plausibility of an effect and the adequacy of the model setup a non robust result containing plausible and implausible effects decreases confidence in the adequacy of the model setup more when the confidence in the plausibility assessment is high and vice versa for example the modelled effect of green areas within 100 m is a robust and plausible result and increases the confidence in both the model setup and the plausibility assessment however if a robust result contradicts the expected behavior then the confidence in the expectation needs to be assessed the higher the confidence the more likely it is that the model setup is not adequate when the detected signal is robustly noisy then this is likely because the model setup is not suited to detect the signal or because there is no such assumed relationship also if the relationship between a feature and the target phenomenon is noisy indicated by kendall value close to zero in fig 4 and this is robust across both ml algorithms it increases confidence that the finding is an artefact of the data either the measurement or features rather than the ml algorithm and no real world signal could be detected 5 conclusion increasingly high resolution data is available for urban areas see creutzig et al 2019 and the potential applications of data driven modeling are increasing data driven modeling is for example used to emulate spatiotemporal climate model outputs beusch et al 2020 to better understand marine liquid water clouds andersen et al 2017 or to predict the dynamics of a simple general circulation model as used in numerical weather prediction models scher 2018 predictive accuracy is a good indicator that a model represents the target phenomenon accurately however it is not a sufficient condition as the model might predict correctly for the wrong reasons for climate models see baumberger et al 2017 and overfit to noise and artefacts in the data to increase process understanding in earth science by deep learning methods reichstein et al 2019 argue that model interpretability and physical consistency of predictions are crucial for the adoption of deep learning methods for this purpose knowledge about the underlying causal structure background knowledge and appropriate methods for visualizations are also required zhao and hastie 2019 5 1 aiming at a better understanding of representational accuracy we have shown that the presented approach using the criteria of consistency with background knowledge adequacy of model setup and robustness allows us to understand the representational accuracy of the model for example in the presented modelling approach altitude is a single feature with no derived features and is representationally accurate 5 5 when we refer to a feature or feature group being representationally accurate our meaning is that the relationship between this feature and the predicted temperature is representationally accurate the building volume features also prove to be representationally accurate however rail road might as discussed instead represent a general urban heat island effect and cannot be considered representationally accurate green areas and green roofs are representationally accurate on the aggregated level but not on the level of individual features that is we have several features e g representing green roofs with different radii of influence and fail to extract a plausible signal form the individual signals while the aggregation over all features can show some signs of representational accuracy in contrast park trees and street trees features are not representationally accurate individually or in aggregate considering green areas a potential reason for the poor representational accuracy of individual features might be that we do not systematically investigate second order effects between features for example we would expect the green area 500 m feature to exhibit a temperature reduction effect but see the opposite fig 4 to better understand the result of the implausible 500 m feature we plotted the interaction of the 250 and 500 m features fig s2 in the si we see that high values of the 250 m feature and values in the lower range of the 500 m feature lead to lower temperature values with an effect of up to 0 5 c for the xgboost this is an example where it is unclear whether the models overfit to an artefact in the data or whether there is an interesting relationship between green areas on different scales that needs further investigation here we have investigated the robustness only on the level of different ml algorithms however this idea is much more generally applicable for instance different predictions from the same algorithms can also be used in a robustness analysis the uncertainty of iml outputs if the methods allow could also be included in the robustness analysis analogously to extrapolation uncertainty iml estimates are also less reliable in data sparse regions in a more systematic way different elements of the model setup and not just the ml algorithm could be varied to create ensembles to more systematically investigate and reduce uncertainty the three criteria presented here aim to indirectly assess the representational accuracy of a trained data driven model and reduce representational uncertainty while these criteria have proven to be successful for the presented case study they can just as well be applied to other investigations in climate and environmental science 5 2 required representational accuracy depends on adequacy for purpose while we cannot say that a model is representationally accurate in its entirety it can be accurate enough for certain purposes e g to answer certain questions depending on the features and how they are represented models that are representationally inaccurate can also be useful for other reasons for instance if the aim is only short term prediction then representational accuracy is not needed as long as we can guarantee that the model has predicted well before and the system we predict has not changed knüsel et al 2019 but as soon as we aim at understanding the underlying phenomenon a certain representational accuracy becomes necessary for example our analysis has shown that not all individual features contribute to predictions in a way consistent with background knowledge the green areas feature group created plausible average temperature differences between green areas and streets in contrast we can conclude that our model cannot be used to investigate the effect of street trees which did not show a clear temperature signal so besides correct conditional predictions our model also helps to gain insights concerning certain individual features and of feature groups representing different urban greening infrastructure types 5 3 combining data driven and process based models an approach to increase the likelihood of a representationally accurate data driven model would be to constrain the data driven model with process based understanding in so called hybrid models in climate science methods have recently been developed that can physically constrain the machine learning of evapotranspiration zhao et al 2019 and achieve conservation of energy when using neural network emulators for global climate model emulators beucler et al 2019 using conditional generative adversarial networks it has recently been shown that urban land use can be predicted using physical constraints albert et al 2019 hence constraining data driven models e g by energy conservation approaches could in principle be applied to urban climate modelling however applications in this domain are to the best of our knowledge still lacking knowledge about physical processes might also be used to further improve the feature engineering and increase the representational accuracy of models one possible example is to explicitly use shading patterns of buildings dependent on the time of the day as additional features 5 4 final remarks it is not guaranteed that plausible results emerge just because a model represents relationships between individual features and the target variable robustly and plausibly this potential discrepancy cannot be understood by the means of iml methods alone we proposed three criteria to guide the assessment how accurate the data driven model represents relationships of the phenomenon of interest coherence with background knowledge robustness and the adequacy of the model set up by evaluating the representational accuracy via those criteria representational uncertainty is reduced and one gets a better understanding for which purpose the data driven model at hand might be adequate this improves the value of applying data driven models in the environmental science declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we acknowledge the helpful thoughts and comments by gertrude hirsch hadorn and christopher fairless this study was funded by the swiss national science foundation national research programme 75 big data project number 167215 appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105048 
