index,text
25830,a novel multi source data fusion method based on bayesian inference for accurate estimation of chlorophyll a concentration over eutrophic lakes cheng chen a b c qiuwen chen a c gang li c mengnan he c jianwei dong a c hanlu yan a c zhiyuan wang a c zheng duan d a state key laboratory of hydrology water resources and hydraulic engineering nanjing hydraulic research institute nanjing 210029 china state key laboratory of hydrology water resources and hydraulic engineering nanjing hydraulic research institute nanjing 210029 china state key laboratory of hydrology water resources and hydraulic engineering nanjing hydraulic research institute nanjing 210029 china b college of water conservancy and hydroelectric power hohai university nanjing 210098 china college of water conservancy and hydroelectric power hohai university nanjing 210098 china college of water conservancy and hydroelectric power hohai university nanjing 210098 china c center for eco environmental research nanjing hydraulic research institute nanjing 210029 china center for eco environmental research nanjing hydraulic research institute nanjing 210029 china center for eco environmental research nanjing hydraulic research institute nanjing 210029 china d department of physical geography and ecosystem science lund university sölvegatan 12 se 223 62 lund sweden department of physical geography and ecosystem science lund university sölvegatan 12 se 223 62 lund sweden department of physical geography and ecosystem science lund university solvegatan 12 se 223 62 lund sweden corresponding author hujuguan 34 nanjing 210029 china hujuguan 34 nanjing 210029 china a novel multi source data fusion method based on bayesian inference bif was proposed in this study to blend the advantages of in situ observations and remote sensing estimations for obtaining accurate chlorophyll a chla concentration in lake taihu china two error models additive and multiplicative were adopted to construct the likelihood function in bif the bif method was also compared with three commonly used data fusion algorithms including linear and nonlinear regression data fusion lrf and nlrf and cumulative distribution function matching data fusion cdff the results showed the multiplicative error model had small normalized residual errors and was a more suitable choice the bif method largely outperformed the data fusion algorithms of cdff nlrf and lrf with the largest correlation coefficients and smallest root mean square error moreover the bif results can capture the high chla concentrations in the northwest and the low chla concentrations in the east of lake taihu graphical abstract image 1 keywords chlorophyll a multi source data fusion eutrophic lake bayesian inference multiplicative error model lake taihu 1 introduction with the increase of human activities and economic developments in the past few decades eutrophication of lakes due to the increasing nutrient inputs has become a serious environmental ecological and social problem all over the world duan et al 2009 schindler 2012 shi et al 2017 in the meantime elevated water temperature caused by climate change is expected to further reinforce the eutrophication problem moe et al 2016 eutrophication and the consequent harmful algal blooms can lead to consumption of dissolved oxygen the death of higher organisms and poisoning of humans chen et al 2019 so far many eutrophic lakes have been suffering from severe algal blooms including lake erie in the united states michalak et al 2013 lake garda in italy brivio et al 2001 lake bogoria in kenya tebbs et al 2013 lake chaohu duan et al 2017 and lake taihu in china shi et al 2017 especially in lake taihu the algal blooms in 2007 caused drinking water crisis and millions of people in wuxi city were exposed to health hazards which in turn posed a great threat to the social security yang et al 2008 the quantitative characterization of the spatiotemporal variability of algal blooms can provide technical guidance for its prediction and early warning which is crucial to risk management in eutrophic lakes chlorophyll a chla concentration is a commonly used indicator for assessing eutrophication and algal blooms due to its close relationship to the abundance and biomass of aquatic phytoplankton ha et al 2014 the conventional ship borne or laboratory borne measurements of chla concentration are usually regarded as the most accurate data shang et al 2017 however due to the sparse sampling points and the coarse sampling frequencies the in situ sampling data is insufficient to accurately describe the distribution of chla concentration in time and space especially lakes such as lake taihu where the chla concentration has large spatial variations due to the varied nutrient concentrations and hydrodynamic conditions in the entire lake chen et al 2019 remote sensing can provide high frequency chla estimation data with large space coverage it therefore has been frequently used for monitoring chla concentration to assess the status of algal blooms in lakes zhang et al 2014 shi et al 2015 duan et al 2017 remote sensing based chla estimation is built on the relationship between the remote sensing reflectance and the inherent optical properties absorption coefficient and backscattering coefficient gitelson et al 2011 a large number of sensors have been widely used to assess the chla concentration in ocean and inland water bodies including the sea viewing wide field of view sensor seawifs the moderate resolution imaging spectroradiometer modis the medium resolution imaging spectrometer meris landsat series sensors and sentinel series sensors based on these satellite sensors numerous retrieval algorithms have been proposed over the past years to quantify the chla concentration these algorithms can be mainly divided into two categories semi analytical methods mishra and mishra 2012 and empirical methods such as two band algorithms shi et al 2015 three or four band algorithms le et al 2009 as well as maximum chlorophyll index mci algorithms matthews and odermatt 2015 however accurate remote sensing estimation of chla concentration is still a challenge because of the presence of optically active constituents whose spectral features overlap with chla moreover the visible and near infrared wavelengths of the electromagnetic spectrum of satellite sensors is susceptible to cloud cover and other weather conditions in the complex coastal and inland waters and the spatial distribution of chla concentration is typically influenced by water temperature as well as hydrodynamic characteristics mixing and drifting beck et al 2016 all of these will lead to uncertainty in satellite assessments of chla concentration in summary in situ sampling method and satellite remote sensing have their own advantages therefore how to effectively integrate multisource observation data for obtaining more reliable results of algal bloom measurements at desired spatial and temporal scales in eutrophic lakes is an urgent need data fusion is an emerging research discipline of data processing which can intelligently combine data and knowledge from diverse sources castanedo 2013 and can effectively process homogeneous and heterogeneous multi source data from multiple sensor systems multi source data fusion has been widely applied in many fields to improve the observation accuracy including rainfall hu et al 2019 land surface temperature wu et al 2015 soil moisture park et al 2017 and so on great attempts have been made to blend multi source observations of chla concentration to improve its spatiotemporal accuracy pottier et al 2006 used the objective analysis and the error weighted averaging to merge the seawifs data with modis data for obtaining daily maps of ocean chla the validation results using the in situ data showed that both methods yielded similar errors but the combined data by the objective analysis showed improved spatial distribution when compared to the error weighted averaging saulquin et al 2010 developed the multi sensor analysis meris seawifs and modis based on the kriging method to produce a better spatiotemporal distribution of the ocean chla concentration for the period of 1998 2008 these studies mainly focus on the fusion of multiple ocean color satellite data due to a relatively large number of ocean color satellite sensors and their mature chla inversion algorithms in open ocean waters zhang et al 2019 specifically based on the assumption that in situ measured chla data is accurate within measurement error gregg and conkright 2001 combined in situ measured chla data with the coastal zone color scanner czcs data to obtain enhanced seasonal representation of global ocean chla by the blended analysis it was shown that the concentration of seasonal chla estimates increased 8 35 in the blended analysis by integrating the in situ measured data similarly wilkie et al 2015 proposed a data fusion method to combine the remote sensing and in situ chla measured data using a spatially varying coefficient regression however these methods also have limitations since they consider the in situ observations as true values and cannot identify more effective information from multiple measurements li 2015 bayesian inference method has a complete theoretical basis which is commonly used for uncertainty analysis and multiscale data fusion in water resources field kavetski et al 2006 wang et al 2019 in bayesian inference the prior distribution and the likelihood function are constructed to establish the joint posterior distribution chen 2005 wang et al 2019 developed a bayesian assimilation method to provide the daily and monthly precipitation based on satellite precipitation data and rain gauge observations over a mountainous region results showed that the bayesian assimilation method could effectively utilize the spatially distributed precipitation information and also significantly reduce the associated uncertainty however to our best knowledge the bayesian inference based data fusion method has been scarcely investigated for the estimation of chla concentration to fill the aforementioned research gaps a joint posterior distribution between the true chla and remote sensing chla estimation was established in the study considering that the remote sensing chla estimation has relatively good spatiotemporal coverage and the in situ measured chla data can provide the prior knowledge for true chla a multi source bayesian inference data fusion method bif based on the numerical solution of the joint posterior distribution was proposed to effectively blend the advantages of remote sensing estimation data and in situ measured data for accurate monitoring of chla concentration in lake taihu from 2014 to 2016 in addition the additive and multiplicative error models in likelihood function of the bif method were compared to quantify the errors between remote sensing chla data and true chla three commonly used data fusion algorithms including linear regression data fusion lrf nonlinear regression data fusion nlrf and cumulative distribution function data fusion cdff were compared to demonstrate the superiority of the proposed bif method this study will contribute to the accurate characterization on spatiotemporal chla concentration and provide valuable guidance for risk management of algal blooms in eutrophic lakes 2 study area and datasets 2 1 study area lake taihu is located in the south of the yangtze river delta 119 52 32 120 36 10 e 30 55 40 31 32 58 n which is the third largest freshwater lakes in china it is generally divided into eight sub lakes zhushan bay gonghu bay meiliang bay xuhu bay central lake sw lake nw lake and east lake as shown in fig 1 lake taihu is featured by the large water surface area 2338 km2 and the shallow water depth 1 9 m the annually average air temperature of lake taihu is 16 c in summer the water temperature in lake taihu can reach about 30 c due to the sufficient sunlight which along with the low flow velocity is suitable for the growth of algae chen et al 2019 it had been widely reported that high algae contents in lake taihu usually appear from may to october and the chla concentration in the west and north of lake taihu meiliang bay zhushan bay and gonghu bay is higher than in the south and center in all seasons due to a large number of nutrient inputs from the rivers in the west and north of lake taihu and the suitable climatic conditions hu et al 2010 shi et al 2017 algal blooms usually outbreak in the northern bays of lake taihu in summer while they occur in the sw lake in early summer autumn and early winter zhang et al 2011 lake taihu is the important source of water supply for the cities of suzhou and wuxi with high density of population during the last 40 years with the industrialization and urbanization a large amount of waste water and sewage without effective treatments were discharged into the northern bays of lake taihu gonghu bay meiliang bay and zhushan bay resulting in the perennial eutrophication since the late 1980s and early 1990s algal blooms began to occur in lake taihu the algal blooms have been frequently reported in recent years posing serious threats to the environment and humans qi et al 2014 2 2 datasets 2 2 1 in situ chla observation data a total of 30 observation stations were sampled on the lake taihu as shown in fig 1 the water samples were collected once a month from january 2014 to december 2016 by the taihu ecosystem research and field observation station of the nanjing institute of geography and limnology surface water samples were collected at a depth of 0 5 m below the surface stored in a dark refrigerator and then taken back to the laboratory for chla concentration analysis the collected samples were filtered on gf c filters the chla was extracted at 80 90 c with ethanol 90 and analyzed spectrophotometrically at 665 nm and 750 nm zhang et al 2011 it should be noted that this dataset is the best in situ chla observation of the study area with long time series of all the samples 5 sites 1 7 15 24 and 30 located in five different lake regions each region has more than 3 sample points were selected as the independent validation data and the other 25 sites were used as the training data the 5 validation points are evenly distributed in the lake to make the evaluation comprehensive and objective 2 2 2 remote sensing chla data remote sensing estimated chla data was obtained from the modis aqua level 0 data in this study the modis aqua level 0 data from 2014 to 2016 were obtained from the nasa website http oceancolor gsfc nasa gov there are seven spectral bands in the modis instruments which is designed for the monitoring of land and atmosphere the spatial resolution of the bands at 645 nm and 859 nm is 250 m and it is 500 m for the bands at 469 nm 555 nm 1240 nm 1640 nm and 2130 nm two bands at 645 nm and 859 nm of 250 m spatial resolution were used in this study because of the sufficient spatiotemporal coverage of lake taihu the empirical relationship between modis estimated chla data and in situ measured chla data in lake taihu which was proposed by shi et al 2015 is adopted in this study 1 r r c π l t f 0 cos θ 0 r r 2 c h l a 1588 3 exp r r c 645 exp r r c 859 exp r r c 645 exp r r c 859 72 6 where l t is the calibrated sensor radiance θ 0 is the solar zenith angle f 0 is the extraterrestrial solar irradiance at the time of data acquisition r r is the rayleigh reflectance r r c is the atmospheric rayleigh corrected reflectance and r rc 645 and r rc 859 are the atmospheric rayleigh corrected modis data at 645 nm and 859 nm respectively according to eq 1 and eq 2 the chla concentration in lake taihu can be extracted from the modis data a total of 36 synchronized modis images at a 250 m spatial resolution from january 1st 2014 to december 31st 2016 with the corresponding observation time to the in situ measurements were selected for chla concentration estimation two commonly used grid to point extraction techniques including nearest neighbor nn and bilinear weight interpolation bwi wells 2005 were applied to calculate the remote sensing retrieved chla value at any in situ observation position within remote sensing pixels to assess the spatial mismatch error between in situ chla observations and remote sensing chla estimations details about the nn and bwi methods can be found in support information 3 methodology 3 1 algorithms for multi source chla data fusion based on the multi source observed data of chla concentration in situ observations and remote sensing estimations a multi source data fusion method based on bayesian inference bif was proposed to obtain more accurate chla concentration data in lake taihu the proposed bif algorithm was also compared with three commonly used data fusion algorithms which were linear regression data fusion lrf nonlinear regression data fusion nlrf and cumulative distribution function data fusion cdff 3 1 1 bayesian inference based chla data fusion bif bayesian inference is an alternative method of statistical inference in which the bayesian theory is used to update the probability for a hypothesis as more evidence becomes available bayesian inference derives the posterior probability as a consequence by the likelihood function and a prior distribution chen 2005 in this study considering that the remote sensing chla estimation has relatively good spatiotemporal coverage and the in situ measured chla data can provide the prior knowledge for true chla we developed the bif to estimate the spatial distribution of true chla by blending the advantages of in situ measured chla and remote sensing chla data we assumed the in situ measured chla data as the true values therefore the objective of bif was to estimate the true chla data in the locations without in situ observations specifically the spatial interpolation procedure was first adopted to obtain pseudo in situ chla measurements to the locations where real in situ measurements are not available the interpolated results were then used to define a prior probability distribution which would be updated by using the corresponding remote sensing chla data c r s u i let c d f u i represents the estimated chla also known as data fusion result of chla concentration at any location u i the basic formula of bayesian inference can be expressed as 3 p c d f u i c r s u i p c r s u i c d f u i p c d f u i p c r s u i where p c d f u i c r s u i represents the posterior probability of the estimated chla p c d f u i represents the prior probability of estimated chla p c r s u i c d f u i represents the posterior probability of remote sensing chla estimation also known as the likelihood function p c r s u i represents the marginal likelihood which is a constant chen 2005 thus the posterior probability distribution function pdf of eq 3 can be simplified and calculated as follows 4 f c d f u i c r s u i l c r s u i c d f u i f c d f u i where l c r s u i c d f u i represents the likelihood function f c d f u i c r s u i and f c d f u i represent the posterior and prior probability pdfs respectively as long as the specific mathematical forms of likelihood function and prior pdf are known the posterior pdf can be deduced it was assumed that the prior pdf of the true chla data obeys to gaussian distribution and thus the prior pdf can be obtained by the spatial interpolation of the in situ chla observations in the study the ordinary kriging ok interpolation method which is typically used for spatial autocorrelation data and proved to be effective for mapping the spatial distribution of the chla observation data ha et al 2014 saulquin et al 2010 was used to interpolate the in situ chla observations to each grid cell that does not have direct observations the ok interpolation is the most commonly used univariate unbiased method to quantify the spatial correlation of a localized variable by using the semivariogram more details about the ok interpolation method can be found in saulquin et al 2010 by the ok interpolation method the expectation and variance of the gaussian distribution can be represented by the estimated chla concentration data c o b s o k u i and estimated standard variance σ o k u i at each grid cell u i in lake taihu therefore the prior pdf f c d f u i can be calculated as follows 5 f c d f u i 1 2 π σ o k u i exp 1 2 σ o k 2 u i c d f u i c o b s o k u i 2 likelihood function plays an important role in the bif in this study the likelihood function can be inferred from the in situ chla observations and remote sensing chla estimations and the additive and multiplicative error models are more widely used statistical models to describe the relationships between true chla and remote sensing chla data cuo and zhang 2017 wang et al 2019 the additive error model eq 6 and the multiplicative error model eq 7 can be expressed as 6 c r s u i a 1 b 1 c d f u i ε 7 c r s u i a 2 c d f b 2 u i e ε where a 1 and b 1 are the intercept and slope respectively a 2 and b 2 represent the fitted parameters in the multiplicative error model these four parameters can be inferred from in situ chla observations and remote sensing estimated chla data the parameter ε represents the fitted model error which can be assumed to have a gaussian distribution with the mean of zero and the standard deviation of σ e r r u i which is ε n 0 σ e r r u i the multiplicative error model can be converted to eq 8 by the logarithm operation 8 ln c r s u i ln a 2 b 2 ln c d f u i ε by the logarithmic transformation it can be found that eq 6 and eq 8 had the similar error function the a 1 in the additive error model corresponds to ln a 2 in the multiplicative error model the random variables of c r s u i and c d f u i in the multiplicative error model are the logarithmic form of those in the additive error model then the likelihood function for the additive error model can be calculated as eq 9 9 l c r s u i c d f u i 1 2 π σ e r r u i exp 1 σ e r r 2 u i c r s u i a 1 b 1 c d f u i 2 it should be noted that in this paper we took the additive error model as an example to solve the likelihood function similarly for the multiplicative error model we can obtain a similar likelihood function by the logarithmic transformation in eq 8 the normalized residual errors were calculated to verify the normality of errors and select the best error model based on eq 5 and eq 9 the posterior pdf of estimated chla can be expressed as follows 10 f c d f u i c r s u i 1 2 π σ o k u i exp 1 2 σ o k 2 u i c d f u i c o b s o k u i 2 1 2 π σ e r r u i exp 1 2 σ e r r 2 u i c r s u i a b c d f u i 2 according to eq 10 it can be deduced the posterior pdf obeys to gaussian distribution the detailed derivation of posterior pdf can be found in support information therefore the posterior pdf of estimated chla at any location u i can be obtained by the remote sensing chla data and the interpolated in situ chla observations 3 1 2 linear regression data fusion lrf in the lrf method the in situ chla observation is assumed to approximately represent the true chla let vector u i represents the coordinates at ith grid location in lake taihu the in situ measured chla data c o b s u i is fitted linearly with remote sensing chla data c r s u i to obtain the linear regression equation the formula is as follows 11 c o b s u i f c r s u i ε i where f represent the linear fitting function ε i represents the fitted error based on f and c r s u i the data fusion result of chla concentration can be obtained by the lrf method at any location u i in lake taihu especially for the location without in situ sampling sites 3 1 3 nonlinear regression data fusion nlrf the data fusion process of the nlrf method is similar to that of the lrf method the c o b s u i and c r s u i are fitted by various nonlinear regression methods such as polynomial regression logarithm regression and exponential regression the fitted results are then compared to choose the best nonlinear fitting function the data fusion result of chla concentration by nlrf can be calculated by the best nonlinear fitting function and c r s u i 3 1 4 cumulative distribution function matching data fusion cdff by matching the cumulative distribution functions cdfs site based analysis and satellite estimates can be effectively combined to provide more accurate results liu et al 2011 mishra et al 2017 in the study the cdff method was also employed to blend the distribution of c o b s u i and c r s u i by matching their cdfs assuming that c o b s u i and c r s u i are subject to gaussian distribution the cdff method can be expressed as follows 12 c d f c u i 1 2 π σ exp 1 2 σ 2 c u i μ 2 13 c d f u i c d f o b s 1 c d f r s c r s u i where c d f is the cumulative distribution function c u i represents the chla concentration data obtained by in situ observation or remote sensing c d f r s and c d f o b s are the cumulative distribution function of the remote sensing chla data and the in situ observation chla data respectively μ and σ are the mean and standard variance of c u i respectively c d f o b s 1 is the inverse function of c d f o b s 3 2 evaluation metrics four commonly used evaluation indicators pearson correlation coefficients cc relative bias rb root mean square error rmse relative root mean square error rrmse were used in this study to assess the model accuracy table 1 in addition the kullback leibler divergence kld was also calculated to measure the difference in probability density functions pdfs between the in situ chla observations and data fusion chla result from each method yang et al 2019 4 results 4 1 spatial mismatch errors assessment by two grid to point extraction methods fig 2 shows the scatterplots between in situ chla observations and remote sensing chla estimations extracted by the nn and bwi methods it can be seen that the values of chla concentration were mainly in the range of 0 40 μg l the extracted chla concentration based on the two grid to point extraction methods both showed good agreement with in situ chla observations especially when the in situ chla observations were below 40 μg l table 2 shows the evaluation metrics of the two grid to point extraction methods and it can be found that both methods had relatively large cc 0 653 and 0 712 for nn and bwi respectively negative values of rb of two grid to point extraction methods indicated that remote sensing chla estimations underestimated the in situ chla observations especially for the large values the bwi method had a better performance with larger cc while smaller rmse and rrmse than the nn method it was demonstrated that the bwi method can reduce the space mismatch errors and is more suitable for the grid to point extraction therefore the bwi method was used for further analyses 4 2 optimal fitting of error distribution in likelihood function of the bif model fig 3 shows the scatterplots and the smoothed mean values red line of the normalized residual error residual error normalized by their standard deviation of the additive and multiplicative models in likelihood function of the bif model it can be found that the mean values of the normalized residual error had large oscillations as the increase of chla concentration however the mean values of the normalized residual error showed small oscillations when the chla concentration was below 40 μg l compared with the normalized residual error in the additive model the mean values of the normalized residual error in the multiplicative model had a steady oscillation and were closer to zero especially when the sample size of chla concentration in the range of 0 40 μg l was large therefore considering its better performance in the verification result of the normality of errors when the sample size of chla concentration was sufficient the multiplicative model was adopted in this study to describe the relationships between true chla and remote sensing chla data in likelihood function of the bif model 4 3 fitting results of the lrf nlrf and cdff methods in the training locations fig 4 a b present the fitting results of lrf and nlrf using the training data from 25 sites respectively the lrf method had a better fitting result cc 0 715 in terms of the nlrf method the logarithm regression was selected as the nonlinear regression function because of the largest cc 0 614 fig 4 b fig 4 c shows the cdf curves of cdff using the training data from 25 sites the cdf curve of the chla concentration data fusion result lime color was between the two cdf curves of the site data blue color and remote sensing data red color which indicates that data fusion results of chla concentration obtained by the cdff method were closer to the site observations when compared to the original remote sensing estimations as a whole the lrf method had the best fitting result while the cdff method had the worst result 4 4 comparative evaluation on the four data fusion algorithms for chla concentration fig 5 a d show the comparative evaluation in the validation locations using 5 independent validation points from 2014 to 2016 between in situ chla measurements and the corresponding chla estimations from four data fusion algorithms lrf nlrf cdff and bif respectively table 3 shows the evaluation metrics of four data fusion algorithms in terms of cc the cdff method had the worst result and the bif method had the best result the data fusion results by lrf and cdff tended to overestimate the in situ chla observations with positive rb while the negative rb values for nlrf and bif indicated there were underestimations compared with the nlrf method the lrf method had better performance with larger cc as well as smaller rrmse and rmse which was consistent with the results in the training locations the cdff method had the largest rrmse 0 727 and rmse 18 584 μg l whereas the bif method had the smallest rrmse 0 487 and rmse 12 433 μg l compared with the other three data fusion methods the bif method had the best performance in the evaluation metrics in addition the chla concentrations by bif showed good consistent with the in situ chla observations in summer as shown in fig 5 d these results demonstrated that the proposed bif method is effective for accurate monitoring of chla concentration in lake taihu fig 6 shows the pdfs of in situ chla observation remote sensing chla estimation and four data fusion results of chla concentration by lrf nlrf cdff and bif the kld values between the pdfs of in situ chla observation and each data fusion chla result are also displayed the pdf peak values of in situ observations and remote sensing estimations occurred at the chla concentration of 13 0 μg l and 19 0 μg l respectively fig 6 b c show that the chla concentration based on nlrf and cdff were mainly concentrated around 23 0 μg l and 27 0 μg l respectively which is obviously larger than the results of in situ observations and remote sensing estimations in addition the kld values of nlrf and cdff 0 170 and 0 224 respectively were larger than those of lrf and bif the horizontal axis of the pdf peak values of lrf 18 0 μg l and bif 15 0 μg l lay between those of in situ observation and remote sensing estimation which implies the good performances of data fusion of chla concentration as expected the pdf curve of bif was much closer to that of in situ chla observation and had the smallest kld value 0 105 which indicated the proposed bif algorithm can obtain more realistic results fig 7 presents the statistic results of monthly chla concentrations from 5 validation points 1 7 15 24 and 30 and their mean values the chla concentrations in each month from site interpolation remote sensing estimation and four data fusion methods lrf nlrf cdff and bif were calculated during 2014 2016 to assess the performance of data fusion at monthly scale it can be seen that in situ chla observations were close to remote sensing chla estimations but considerable differences between the two different sources of chla concentration data were found when the chla concentration was large fig 7 a e the data fusion methods could effectively reduce the large differences and improve the monitoring results of chla concentration as shown in august and september in fig 7 f the data fusion results based on the bif method had better agreement with the in situ chla observations compared with the other three data fusion methods however the data fusion methods did not have much improvement when the differences between in situ chla observations and remote sensing chla estimations were small fig 8 presents the spatial distribution of chla concentration in aug 14 2014 from interpolation of in situ chla observations remote sensing chla estimations and data fusion results of chla concentration obtained by the lrf nlrf cdff and bif methods the data in aug 14 2014 was selected because the chla concentration in august was the highest worst condition for water quality and the remote sensing data had good quality in this day fig s1 overall both in situ chla observations fig 8 a and remote sensing chla estimations fig 8 b had a similar spatial distribution where the chla concentration was high in the north and west of lake taihu meiliang bay zhushan bay gonghu bay and nw lake the remote sensing chla estimations in the east of lake taihu were relatively large however the in situ chla observations showed small values compared to the remote sensing estimations the spatial distribution of chla concentration based on the lrf method showed a general overestimation in chla concentration as seen in the north and east of lake taihu in fig 8 c the spatial distribution results based on the nlrf method in fig 8 d show that the high values of chla concentration were underestimated but the low values of chla concentration were overestimated the cdff results in fig 8 e show a significant overestimation in high values of chla concentration while no significant difference existed for the low values of chla concentration in terms of the spatial distribution of chla concentration by bif fig 8 f the chla concentration was significantly overestimated especially in meiliang bay zhushan bay gonghu bay and nw lake however the chla concentration in the east of lake taihu was close to the in situ chla observations comparatively the proposed bif algorithm could more effectively blend site observations and multi source remote sensing estimations to obtain accurate more chla concentration in lake taihu 5 discussion a number of studies have pointed out that there is a typical spatial and temporal mismatch issue between satellite pixels and point measurements brewin et al 2017 freitas and dierssen 2019 the high uncertainties from two different sources of data may impose significant influences on a data fusion scheme and thus lead to failure in validation since the modis remote sensing chla estimation is a kind of homogenization data of the 250 m 250 m grid and the in situ chla observation data is a point based observation this study adopted the nn and bwi techniques for grid to point extraction to assess the spatial mismatch errors between the two different sources of chla concentration data it is demonstrated that the nn method is more sensitive to the spatial mismatch errors with large rrmse and rmse while the bwi method calculated the average of nearby satellite pixel values to extract the remote sensing chla data for a specific in situ location wells 2005 which could effectively reduce the spatial mismatch errors this study suggests the adopted bwi method is effective for the grid to point extraction of chla concentration in lake taihu however an overall underestimation of the high chla values and an overestimation of the low chla values were found in the modis remote sensing estimates it was mainly because the coarse spatial resolution makes modis remote sensing data have a weak ability to capture extreme values of chla beck et al 2016 in addition modis has wide bands that do not target the chla reflectance peak well and has lower performance because of the optical interference of dissolved organic and suspended inorganic matter in the complex coastal and inland waters garcia et al 2006 beck et al 2019 the optical properties of the water in lake taihu are very complex which are dominated by a combination of chromophoric dissolved organic matter phytoplankton and non algal particulate matter zhang et al 2019 especially in summer the complex composition of water bodies and the wide range of chla concentration in lake taihu can cause large deviations in remote sensing chla retrieval algorithm this uncertainty could be reduced by using high resolution remote sensing data e g landsat series data with a 30 m resolution and sentinel series data with a 10 m resolution and developing more suitable remote sensing retrieval algorithms of chla in future studies uncertainty definition representation and quantification are usually determined by the selected error model tian et al 2013 this study used the additive and multiplicative models to quantify the errors between true chla concentration and remote sensing chla concentration it was found that the normalized residual errors of the two error models had large oscillations with the increase in chla concentration showing obvious heteroscedasticity limited by the sparse sampling points and the coarse sampling frequencies the number of samples used for analyses in this study was small and the majority of chla concentration was less than 40 μg l this could lead to the large oscillations of the mean normalized residual errors when the chla concentration was high in addition the weak capability of modis to capture extreme values may be the cause of heteroscedasticity for large chla values this study demonstrated that the multiplicative error model is more suitable because the mean values of the normalized residual errors in the multiplicative model had a steady oscillation and were close to zero especially when the sample size of chla concentration was large in fact the essence of the multiplicative error model is the logarithmic transforms of chla concentration data which has been proved to be able to reduce the effects derived from extreme data ranges to prevent high impact of large values of in situ observations gregg and conkright 2001 pottier et al 2006 it should be noted that both in situ chla data and remote sensing chla data have their own advantages and disadvantages the in situ measured data has high accuracy and reliability but has insufficient spatial representation ability whereas the remote sensing data has good spatial and temporal continuity but has poor accuracy it is recommended to combine the traditional in situ sampling and remote sensing methods to monitor the algal blooms paerl and huisman 2009 the three commonly used data fusion algorithms lrf nlrf and cdff showed good consistency in the training and validation periods and had an overall improvement in chla concentration in the west and north of lake taihu compared to the original remote sensing estimations however the spatial distribution of chla concentration based on the lrf method had a slight overestimation in the entire lake taihu which is attributed to the linear lifting of lrf the data fusion results by nlrf showed that the high values of chla concentration were underestimated but the low values of chla concentration were overestimated it is related to the logarithm regression which reduces the large variations of measurements with high concentration levels and narrows the numerical range of the data gallie and murtha 1992 among the three methods the cdff method had the worst performance this was mainly because the cdf matching method does not correct the mean of the chla data therefore this method cannot correct the systematic bias of the remote sensing chla data which led to limit improvement liu et al 2011 in particular the data fusion results of chla concentration based on the bif method had the best performance with the largest cc and the smallest rmse as well as kld the bif results had good agreement with in situ chla observations in the east of lake taihu where the other three data fusion methods made obvious overestimations the east part of lake taihu is typically covered by aquatic plants reeds weeds and other macrophytes which makes chla not to be accurately derived from remote sensing because the surface reflectance characteristics of lake can be significantly changed by the aquatic plants shi et al 2015 the proposed bif method is based on the numerical solution of a joint posterior distribution and the joint posterior distribution can be obtained by a prior distribution and the likelihood function chen 2005 wang et al 2019 since remote sensing chla estimations have relatively good spatiotemporal coverage and in situ chla data can provide the prior knowledge of true chla the bif method is believed to be able to effectively blend multi source data and provide more accurate results of chla concentration in lake taihu by blending the chla observations from multiple sources the proposed bif algorithm can better capture the high chla concentration in the west and north of lake taihu as well as the low chla concentration in the east of lake taihu in summer months since large values of chla in summer are more important to the risk management of algal blooms the improved performance of the proposed bif method is more meaningful the primary purpose of data fusion in the study is to reduce biases in the satellite chla estimates while retain the spatial variations of the satellite data by using the higher accurate in situ data the proposed bif method proved to be an effective approach to take the advantages of multi source data and is expected to be transferable and applicable to other eutrophic lakes to provide more accurate monitoring of chla concentration however it should be noted that it was assumed that the prior and likelihood distributions in bif obey to the gaussian distribution to simplify the computation this assumption could bring some uncertainties in addition it has been well recognized that wind driven circulation plays a key role in the spatial variations of chla concentration this is particularly true for lake taihu because it is a typical shallow lake with a large water surface area where the spatial distribution of materials such as suspended solids nutrients and algae is strongly affected by wind induced flow and wind drifting zhang et al 2011 huang et al 2014 temperature and light are also important factors to the growth of phytoplankton huang et al 2015 therefore more essential factors e g water temperature light and hydrodynamic characteristics should be fused to improve the accuracy of chla concentration measurements in future using more high frequency data through this way the dynamic features of chla concentration and algal blooms can be well captured due to the limitation of in situ observations we only selected 5 sites in 5 lake regions relatively uniformly distributed in space for validation in the study cross validation methods should be applied by obtaining denser and long time series data to avoid over fitting and provide more reliable verification results in further studies 6 conclusions a multi source data fusion method based on bayesian inference bif was proposed in this study to effectively blend the advantages of in situ measurements and remote sensing data for obtaining accurate chla concentration in a typical eutrophic lake lake taihu china from 2014 to 2016 the extracted chla concentration based on both nn and bwi methods showed good agreement with in situ chla observations the bwi method which has larger cc and smaller rmse and rrmse is more effective for the extraction of chla concentration compared to the additive error model in likelihood function of the bif model the mean values of the normalized residual errors in the multiplicative model had a steady oscillation and were close to zero especially when the sample size of chla concentration 0 40 μg l was large demonstrating that the multiplicative error model is clearly a more suitable choice four data fusion methods could effectively reduce the large differences between in situ chla observations and remote sensing chla estimations and the performance of the four algorithms were in the order cdff worst nlrf lrf bif best the proposed bif method had the largest cc 0 843 and the smallest kld 0 105 and rmse 12 433 μg l and the spatial distribution of the data fusion result by the bif method can capture the high chla concentration in the west and north of lake taihu as well as the low chla concentration in the east of lake taihu the proposed bif is expected to be transferable and applicable to other lakes to provide more accurate monitoring data of chla concentration future studies should focus on fusing data from higher frequencies higher temporal scales and more sources especially on other important influencing factors e g water temperature and hydrodynamic characteristics to improve the accuracy of large chla concentration measurements and characterization of chla dynamic processes in eutrophic lakes declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is supported by the national key research and development program no 2018yfc0830800 the national nature science foundation of china no 51709179 the water conservancy science and technology project of jiangsu province no 2018007 and the innovation cluster fund of nanjing hydraulic research institute no y917020 qiuwen chen acknowledges the support from the xplorer prize appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105057 
25830,a novel multi source data fusion method based on bayesian inference for accurate estimation of chlorophyll a concentration over eutrophic lakes cheng chen a b c qiuwen chen a c gang li c mengnan he c jianwei dong a c hanlu yan a c zhiyuan wang a c zheng duan d a state key laboratory of hydrology water resources and hydraulic engineering nanjing hydraulic research institute nanjing 210029 china state key laboratory of hydrology water resources and hydraulic engineering nanjing hydraulic research institute nanjing 210029 china state key laboratory of hydrology water resources and hydraulic engineering nanjing hydraulic research institute nanjing 210029 china b college of water conservancy and hydroelectric power hohai university nanjing 210098 china college of water conservancy and hydroelectric power hohai university nanjing 210098 china college of water conservancy and hydroelectric power hohai university nanjing 210098 china c center for eco environmental research nanjing hydraulic research institute nanjing 210029 china center for eco environmental research nanjing hydraulic research institute nanjing 210029 china center for eco environmental research nanjing hydraulic research institute nanjing 210029 china d department of physical geography and ecosystem science lund university sölvegatan 12 se 223 62 lund sweden department of physical geography and ecosystem science lund university sölvegatan 12 se 223 62 lund sweden department of physical geography and ecosystem science lund university solvegatan 12 se 223 62 lund sweden corresponding author hujuguan 34 nanjing 210029 china hujuguan 34 nanjing 210029 china a novel multi source data fusion method based on bayesian inference bif was proposed in this study to blend the advantages of in situ observations and remote sensing estimations for obtaining accurate chlorophyll a chla concentration in lake taihu china two error models additive and multiplicative were adopted to construct the likelihood function in bif the bif method was also compared with three commonly used data fusion algorithms including linear and nonlinear regression data fusion lrf and nlrf and cumulative distribution function matching data fusion cdff the results showed the multiplicative error model had small normalized residual errors and was a more suitable choice the bif method largely outperformed the data fusion algorithms of cdff nlrf and lrf with the largest correlation coefficients and smallest root mean square error moreover the bif results can capture the high chla concentrations in the northwest and the low chla concentrations in the east of lake taihu graphical abstract image 1 keywords chlorophyll a multi source data fusion eutrophic lake bayesian inference multiplicative error model lake taihu 1 introduction with the increase of human activities and economic developments in the past few decades eutrophication of lakes due to the increasing nutrient inputs has become a serious environmental ecological and social problem all over the world duan et al 2009 schindler 2012 shi et al 2017 in the meantime elevated water temperature caused by climate change is expected to further reinforce the eutrophication problem moe et al 2016 eutrophication and the consequent harmful algal blooms can lead to consumption of dissolved oxygen the death of higher organisms and poisoning of humans chen et al 2019 so far many eutrophic lakes have been suffering from severe algal blooms including lake erie in the united states michalak et al 2013 lake garda in italy brivio et al 2001 lake bogoria in kenya tebbs et al 2013 lake chaohu duan et al 2017 and lake taihu in china shi et al 2017 especially in lake taihu the algal blooms in 2007 caused drinking water crisis and millions of people in wuxi city were exposed to health hazards which in turn posed a great threat to the social security yang et al 2008 the quantitative characterization of the spatiotemporal variability of algal blooms can provide technical guidance for its prediction and early warning which is crucial to risk management in eutrophic lakes chlorophyll a chla concentration is a commonly used indicator for assessing eutrophication and algal blooms due to its close relationship to the abundance and biomass of aquatic phytoplankton ha et al 2014 the conventional ship borne or laboratory borne measurements of chla concentration are usually regarded as the most accurate data shang et al 2017 however due to the sparse sampling points and the coarse sampling frequencies the in situ sampling data is insufficient to accurately describe the distribution of chla concentration in time and space especially lakes such as lake taihu where the chla concentration has large spatial variations due to the varied nutrient concentrations and hydrodynamic conditions in the entire lake chen et al 2019 remote sensing can provide high frequency chla estimation data with large space coverage it therefore has been frequently used for monitoring chla concentration to assess the status of algal blooms in lakes zhang et al 2014 shi et al 2015 duan et al 2017 remote sensing based chla estimation is built on the relationship between the remote sensing reflectance and the inherent optical properties absorption coefficient and backscattering coefficient gitelson et al 2011 a large number of sensors have been widely used to assess the chla concentration in ocean and inland water bodies including the sea viewing wide field of view sensor seawifs the moderate resolution imaging spectroradiometer modis the medium resolution imaging spectrometer meris landsat series sensors and sentinel series sensors based on these satellite sensors numerous retrieval algorithms have been proposed over the past years to quantify the chla concentration these algorithms can be mainly divided into two categories semi analytical methods mishra and mishra 2012 and empirical methods such as two band algorithms shi et al 2015 three or four band algorithms le et al 2009 as well as maximum chlorophyll index mci algorithms matthews and odermatt 2015 however accurate remote sensing estimation of chla concentration is still a challenge because of the presence of optically active constituents whose spectral features overlap with chla moreover the visible and near infrared wavelengths of the electromagnetic spectrum of satellite sensors is susceptible to cloud cover and other weather conditions in the complex coastal and inland waters and the spatial distribution of chla concentration is typically influenced by water temperature as well as hydrodynamic characteristics mixing and drifting beck et al 2016 all of these will lead to uncertainty in satellite assessments of chla concentration in summary in situ sampling method and satellite remote sensing have their own advantages therefore how to effectively integrate multisource observation data for obtaining more reliable results of algal bloom measurements at desired spatial and temporal scales in eutrophic lakes is an urgent need data fusion is an emerging research discipline of data processing which can intelligently combine data and knowledge from diverse sources castanedo 2013 and can effectively process homogeneous and heterogeneous multi source data from multiple sensor systems multi source data fusion has been widely applied in many fields to improve the observation accuracy including rainfall hu et al 2019 land surface temperature wu et al 2015 soil moisture park et al 2017 and so on great attempts have been made to blend multi source observations of chla concentration to improve its spatiotemporal accuracy pottier et al 2006 used the objective analysis and the error weighted averaging to merge the seawifs data with modis data for obtaining daily maps of ocean chla the validation results using the in situ data showed that both methods yielded similar errors but the combined data by the objective analysis showed improved spatial distribution when compared to the error weighted averaging saulquin et al 2010 developed the multi sensor analysis meris seawifs and modis based on the kriging method to produce a better spatiotemporal distribution of the ocean chla concentration for the period of 1998 2008 these studies mainly focus on the fusion of multiple ocean color satellite data due to a relatively large number of ocean color satellite sensors and their mature chla inversion algorithms in open ocean waters zhang et al 2019 specifically based on the assumption that in situ measured chla data is accurate within measurement error gregg and conkright 2001 combined in situ measured chla data with the coastal zone color scanner czcs data to obtain enhanced seasonal representation of global ocean chla by the blended analysis it was shown that the concentration of seasonal chla estimates increased 8 35 in the blended analysis by integrating the in situ measured data similarly wilkie et al 2015 proposed a data fusion method to combine the remote sensing and in situ chla measured data using a spatially varying coefficient regression however these methods also have limitations since they consider the in situ observations as true values and cannot identify more effective information from multiple measurements li 2015 bayesian inference method has a complete theoretical basis which is commonly used for uncertainty analysis and multiscale data fusion in water resources field kavetski et al 2006 wang et al 2019 in bayesian inference the prior distribution and the likelihood function are constructed to establish the joint posterior distribution chen 2005 wang et al 2019 developed a bayesian assimilation method to provide the daily and monthly precipitation based on satellite precipitation data and rain gauge observations over a mountainous region results showed that the bayesian assimilation method could effectively utilize the spatially distributed precipitation information and also significantly reduce the associated uncertainty however to our best knowledge the bayesian inference based data fusion method has been scarcely investigated for the estimation of chla concentration to fill the aforementioned research gaps a joint posterior distribution between the true chla and remote sensing chla estimation was established in the study considering that the remote sensing chla estimation has relatively good spatiotemporal coverage and the in situ measured chla data can provide the prior knowledge for true chla a multi source bayesian inference data fusion method bif based on the numerical solution of the joint posterior distribution was proposed to effectively blend the advantages of remote sensing estimation data and in situ measured data for accurate monitoring of chla concentration in lake taihu from 2014 to 2016 in addition the additive and multiplicative error models in likelihood function of the bif method were compared to quantify the errors between remote sensing chla data and true chla three commonly used data fusion algorithms including linear regression data fusion lrf nonlinear regression data fusion nlrf and cumulative distribution function data fusion cdff were compared to demonstrate the superiority of the proposed bif method this study will contribute to the accurate characterization on spatiotemporal chla concentration and provide valuable guidance for risk management of algal blooms in eutrophic lakes 2 study area and datasets 2 1 study area lake taihu is located in the south of the yangtze river delta 119 52 32 120 36 10 e 30 55 40 31 32 58 n which is the third largest freshwater lakes in china it is generally divided into eight sub lakes zhushan bay gonghu bay meiliang bay xuhu bay central lake sw lake nw lake and east lake as shown in fig 1 lake taihu is featured by the large water surface area 2338 km2 and the shallow water depth 1 9 m the annually average air temperature of lake taihu is 16 c in summer the water temperature in lake taihu can reach about 30 c due to the sufficient sunlight which along with the low flow velocity is suitable for the growth of algae chen et al 2019 it had been widely reported that high algae contents in lake taihu usually appear from may to october and the chla concentration in the west and north of lake taihu meiliang bay zhushan bay and gonghu bay is higher than in the south and center in all seasons due to a large number of nutrient inputs from the rivers in the west and north of lake taihu and the suitable climatic conditions hu et al 2010 shi et al 2017 algal blooms usually outbreak in the northern bays of lake taihu in summer while they occur in the sw lake in early summer autumn and early winter zhang et al 2011 lake taihu is the important source of water supply for the cities of suzhou and wuxi with high density of population during the last 40 years with the industrialization and urbanization a large amount of waste water and sewage without effective treatments were discharged into the northern bays of lake taihu gonghu bay meiliang bay and zhushan bay resulting in the perennial eutrophication since the late 1980s and early 1990s algal blooms began to occur in lake taihu the algal blooms have been frequently reported in recent years posing serious threats to the environment and humans qi et al 2014 2 2 datasets 2 2 1 in situ chla observation data a total of 30 observation stations were sampled on the lake taihu as shown in fig 1 the water samples were collected once a month from january 2014 to december 2016 by the taihu ecosystem research and field observation station of the nanjing institute of geography and limnology surface water samples were collected at a depth of 0 5 m below the surface stored in a dark refrigerator and then taken back to the laboratory for chla concentration analysis the collected samples were filtered on gf c filters the chla was extracted at 80 90 c with ethanol 90 and analyzed spectrophotometrically at 665 nm and 750 nm zhang et al 2011 it should be noted that this dataset is the best in situ chla observation of the study area with long time series of all the samples 5 sites 1 7 15 24 and 30 located in five different lake regions each region has more than 3 sample points were selected as the independent validation data and the other 25 sites were used as the training data the 5 validation points are evenly distributed in the lake to make the evaluation comprehensive and objective 2 2 2 remote sensing chla data remote sensing estimated chla data was obtained from the modis aqua level 0 data in this study the modis aqua level 0 data from 2014 to 2016 were obtained from the nasa website http oceancolor gsfc nasa gov there are seven spectral bands in the modis instruments which is designed for the monitoring of land and atmosphere the spatial resolution of the bands at 645 nm and 859 nm is 250 m and it is 500 m for the bands at 469 nm 555 nm 1240 nm 1640 nm and 2130 nm two bands at 645 nm and 859 nm of 250 m spatial resolution were used in this study because of the sufficient spatiotemporal coverage of lake taihu the empirical relationship between modis estimated chla data and in situ measured chla data in lake taihu which was proposed by shi et al 2015 is adopted in this study 1 r r c π l t f 0 cos θ 0 r r 2 c h l a 1588 3 exp r r c 645 exp r r c 859 exp r r c 645 exp r r c 859 72 6 where l t is the calibrated sensor radiance θ 0 is the solar zenith angle f 0 is the extraterrestrial solar irradiance at the time of data acquisition r r is the rayleigh reflectance r r c is the atmospheric rayleigh corrected reflectance and r rc 645 and r rc 859 are the atmospheric rayleigh corrected modis data at 645 nm and 859 nm respectively according to eq 1 and eq 2 the chla concentration in lake taihu can be extracted from the modis data a total of 36 synchronized modis images at a 250 m spatial resolution from january 1st 2014 to december 31st 2016 with the corresponding observation time to the in situ measurements were selected for chla concentration estimation two commonly used grid to point extraction techniques including nearest neighbor nn and bilinear weight interpolation bwi wells 2005 were applied to calculate the remote sensing retrieved chla value at any in situ observation position within remote sensing pixels to assess the spatial mismatch error between in situ chla observations and remote sensing chla estimations details about the nn and bwi methods can be found in support information 3 methodology 3 1 algorithms for multi source chla data fusion based on the multi source observed data of chla concentration in situ observations and remote sensing estimations a multi source data fusion method based on bayesian inference bif was proposed to obtain more accurate chla concentration data in lake taihu the proposed bif algorithm was also compared with three commonly used data fusion algorithms which were linear regression data fusion lrf nonlinear regression data fusion nlrf and cumulative distribution function data fusion cdff 3 1 1 bayesian inference based chla data fusion bif bayesian inference is an alternative method of statistical inference in which the bayesian theory is used to update the probability for a hypothesis as more evidence becomes available bayesian inference derives the posterior probability as a consequence by the likelihood function and a prior distribution chen 2005 in this study considering that the remote sensing chla estimation has relatively good spatiotemporal coverage and the in situ measured chla data can provide the prior knowledge for true chla we developed the bif to estimate the spatial distribution of true chla by blending the advantages of in situ measured chla and remote sensing chla data we assumed the in situ measured chla data as the true values therefore the objective of bif was to estimate the true chla data in the locations without in situ observations specifically the spatial interpolation procedure was first adopted to obtain pseudo in situ chla measurements to the locations where real in situ measurements are not available the interpolated results were then used to define a prior probability distribution which would be updated by using the corresponding remote sensing chla data c r s u i let c d f u i represents the estimated chla also known as data fusion result of chla concentration at any location u i the basic formula of bayesian inference can be expressed as 3 p c d f u i c r s u i p c r s u i c d f u i p c d f u i p c r s u i where p c d f u i c r s u i represents the posterior probability of the estimated chla p c d f u i represents the prior probability of estimated chla p c r s u i c d f u i represents the posterior probability of remote sensing chla estimation also known as the likelihood function p c r s u i represents the marginal likelihood which is a constant chen 2005 thus the posterior probability distribution function pdf of eq 3 can be simplified and calculated as follows 4 f c d f u i c r s u i l c r s u i c d f u i f c d f u i where l c r s u i c d f u i represents the likelihood function f c d f u i c r s u i and f c d f u i represent the posterior and prior probability pdfs respectively as long as the specific mathematical forms of likelihood function and prior pdf are known the posterior pdf can be deduced it was assumed that the prior pdf of the true chla data obeys to gaussian distribution and thus the prior pdf can be obtained by the spatial interpolation of the in situ chla observations in the study the ordinary kriging ok interpolation method which is typically used for spatial autocorrelation data and proved to be effective for mapping the spatial distribution of the chla observation data ha et al 2014 saulquin et al 2010 was used to interpolate the in situ chla observations to each grid cell that does not have direct observations the ok interpolation is the most commonly used univariate unbiased method to quantify the spatial correlation of a localized variable by using the semivariogram more details about the ok interpolation method can be found in saulquin et al 2010 by the ok interpolation method the expectation and variance of the gaussian distribution can be represented by the estimated chla concentration data c o b s o k u i and estimated standard variance σ o k u i at each grid cell u i in lake taihu therefore the prior pdf f c d f u i can be calculated as follows 5 f c d f u i 1 2 π σ o k u i exp 1 2 σ o k 2 u i c d f u i c o b s o k u i 2 likelihood function plays an important role in the bif in this study the likelihood function can be inferred from the in situ chla observations and remote sensing chla estimations and the additive and multiplicative error models are more widely used statistical models to describe the relationships between true chla and remote sensing chla data cuo and zhang 2017 wang et al 2019 the additive error model eq 6 and the multiplicative error model eq 7 can be expressed as 6 c r s u i a 1 b 1 c d f u i ε 7 c r s u i a 2 c d f b 2 u i e ε where a 1 and b 1 are the intercept and slope respectively a 2 and b 2 represent the fitted parameters in the multiplicative error model these four parameters can be inferred from in situ chla observations and remote sensing estimated chla data the parameter ε represents the fitted model error which can be assumed to have a gaussian distribution with the mean of zero and the standard deviation of σ e r r u i which is ε n 0 σ e r r u i the multiplicative error model can be converted to eq 8 by the logarithm operation 8 ln c r s u i ln a 2 b 2 ln c d f u i ε by the logarithmic transformation it can be found that eq 6 and eq 8 had the similar error function the a 1 in the additive error model corresponds to ln a 2 in the multiplicative error model the random variables of c r s u i and c d f u i in the multiplicative error model are the logarithmic form of those in the additive error model then the likelihood function for the additive error model can be calculated as eq 9 9 l c r s u i c d f u i 1 2 π σ e r r u i exp 1 σ e r r 2 u i c r s u i a 1 b 1 c d f u i 2 it should be noted that in this paper we took the additive error model as an example to solve the likelihood function similarly for the multiplicative error model we can obtain a similar likelihood function by the logarithmic transformation in eq 8 the normalized residual errors were calculated to verify the normality of errors and select the best error model based on eq 5 and eq 9 the posterior pdf of estimated chla can be expressed as follows 10 f c d f u i c r s u i 1 2 π σ o k u i exp 1 2 σ o k 2 u i c d f u i c o b s o k u i 2 1 2 π σ e r r u i exp 1 2 σ e r r 2 u i c r s u i a b c d f u i 2 according to eq 10 it can be deduced the posterior pdf obeys to gaussian distribution the detailed derivation of posterior pdf can be found in support information therefore the posterior pdf of estimated chla at any location u i can be obtained by the remote sensing chla data and the interpolated in situ chla observations 3 1 2 linear regression data fusion lrf in the lrf method the in situ chla observation is assumed to approximately represent the true chla let vector u i represents the coordinates at ith grid location in lake taihu the in situ measured chla data c o b s u i is fitted linearly with remote sensing chla data c r s u i to obtain the linear regression equation the formula is as follows 11 c o b s u i f c r s u i ε i where f represent the linear fitting function ε i represents the fitted error based on f and c r s u i the data fusion result of chla concentration can be obtained by the lrf method at any location u i in lake taihu especially for the location without in situ sampling sites 3 1 3 nonlinear regression data fusion nlrf the data fusion process of the nlrf method is similar to that of the lrf method the c o b s u i and c r s u i are fitted by various nonlinear regression methods such as polynomial regression logarithm regression and exponential regression the fitted results are then compared to choose the best nonlinear fitting function the data fusion result of chla concentration by nlrf can be calculated by the best nonlinear fitting function and c r s u i 3 1 4 cumulative distribution function matching data fusion cdff by matching the cumulative distribution functions cdfs site based analysis and satellite estimates can be effectively combined to provide more accurate results liu et al 2011 mishra et al 2017 in the study the cdff method was also employed to blend the distribution of c o b s u i and c r s u i by matching their cdfs assuming that c o b s u i and c r s u i are subject to gaussian distribution the cdff method can be expressed as follows 12 c d f c u i 1 2 π σ exp 1 2 σ 2 c u i μ 2 13 c d f u i c d f o b s 1 c d f r s c r s u i where c d f is the cumulative distribution function c u i represents the chla concentration data obtained by in situ observation or remote sensing c d f r s and c d f o b s are the cumulative distribution function of the remote sensing chla data and the in situ observation chla data respectively μ and σ are the mean and standard variance of c u i respectively c d f o b s 1 is the inverse function of c d f o b s 3 2 evaluation metrics four commonly used evaluation indicators pearson correlation coefficients cc relative bias rb root mean square error rmse relative root mean square error rrmse were used in this study to assess the model accuracy table 1 in addition the kullback leibler divergence kld was also calculated to measure the difference in probability density functions pdfs between the in situ chla observations and data fusion chla result from each method yang et al 2019 4 results 4 1 spatial mismatch errors assessment by two grid to point extraction methods fig 2 shows the scatterplots between in situ chla observations and remote sensing chla estimations extracted by the nn and bwi methods it can be seen that the values of chla concentration were mainly in the range of 0 40 μg l the extracted chla concentration based on the two grid to point extraction methods both showed good agreement with in situ chla observations especially when the in situ chla observations were below 40 μg l table 2 shows the evaluation metrics of the two grid to point extraction methods and it can be found that both methods had relatively large cc 0 653 and 0 712 for nn and bwi respectively negative values of rb of two grid to point extraction methods indicated that remote sensing chla estimations underestimated the in situ chla observations especially for the large values the bwi method had a better performance with larger cc while smaller rmse and rrmse than the nn method it was demonstrated that the bwi method can reduce the space mismatch errors and is more suitable for the grid to point extraction therefore the bwi method was used for further analyses 4 2 optimal fitting of error distribution in likelihood function of the bif model fig 3 shows the scatterplots and the smoothed mean values red line of the normalized residual error residual error normalized by their standard deviation of the additive and multiplicative models in likelihood function of the bif model it can be found that the mean values of the normalized residual error had large oscillations as the increase of chla concentration however the mean values of the normalized residual error showed small oscillations when the chla concentration was below 40 μg l compared with the normalized residual error in the additive model the mean values of the normalized residual error in the multiplicative model had a steady oscillation and were closer to zero especially when the sample size of chla concentration in the range of 0 40 μg l was large therefore considering its better performance in the verification result of the normality of errors when the sample size of chla concentration was sufficient the multiplicative model was adopted in this study to describe the relationships between true chla and remote sensing chla data in likelihood function of the bif model 4 3 fitting results of the lrf nlrf and cdff methods in the training locations fig 4 a b present the fitting results of lrf and nlrf using the training data from 25 sites respectively the lrf method had a better fitting result cc 0 715 in terms of the nlrf method the logarithm regression was selected as the nonlinear regression function because of the largest cc 0 614 fig 4 b fig 4 c shows the cdf curves of cdff using the training data from 25 sites the cdf curve of the chla concentration data fusion result lime color was between the two cdf curves of the site data blue color and remote sensing data red color which indicates that data fusion results of chla concentration obtained by the cdff method were closer to the site observations when compared to the original remote sensing estimations as a whole the lrf method had the best fitting result while the cdff method had the worst result 4 4 comparative evaluation on the four data fusion algorithms for chla concentration fig 5 a d show the comparative evaluation in the validation locations using 5 independent validation points from 2014 to 2016 between in situ chla measurements and the corresponding chla estimations from four data fusion algorithms lrf nlrf cdff and bif respectively table 3 shows the evaluation metrics of four data fusion algorithms in terms of cc the cdff method had the worst result and the bif method had the best result the data fusion results by lrf and cdff tended to overestimate the in situ chla observations with positive rb while the negative rb values for nlrf and bif indicated there were underestimations compared with the nlrf method the lrf method had better performance with larger cc as well as smaller rrmse and rmse which was consistent with the results in the training locations the cdff method had the largest rrmse 0 727 and rmse 18 584 μg l whereas the bif method had the smallest rrmse 0 487 and rmse 12 433 μg l compared with the other three data fusion methods the bif method had the best performance in the evaluation metrics in addition the chla concentrations by bif showed good consistent with the in situ chla observations in summer as shown in fig 5 d these results demonstrated that the proposed bif method is effective for accurate monitoring of chla concentration in lake taihu fig 6 shows the pdfs of in situ chla observation remote sensing chla estimation and four data fusion results of chla concentration by lrf nlrf cdff and bif the kld values between the pdfs of in situ chla observation and each data fusion chla result are also displayed the pdf peak values of in situ observations and remote sensing estimations occurred at the chla concentration of 13 0 μg l and 19 0 μg l respectively fig 6 b c show that the chla concentration based on nlrf and cdff were mainly concentrated around 23 0 μg l and 27 0 μg l respectively which is obviously larger than the results of in situ observations and remote sensing estimations in addition the kld values of nlrf and cdff 0 170 and 0 224 respectively were larger than those of lrf and bif the horizontal axis of the pdf peak values of lrf 18 0 μg l and bif 15 0 μg l lay between those of in situ observation and remote sensing estimation which implies the good performances of data fusion of chla concentration as expected the pdf curve of bif was much closer to that of in situ chla observation and had the smallest kld value 0 105 which indicated the proposed bif algorithm can obtain more realistic results fig 7 presents the statistic results of monthly chla concentrations from 5 validation points 1 7 15 24 and 30 and their mean values the chla concentrations in each month from site interpolation remote sensing estimation and four data fusion methods lrf nlrf cdff and bif were calculated during 2014 2016 to assess the performance of data fusion at monthly scale it can be seen that in situ chla observations were close to remote sensing chla estimations but considerable differences between the two different sources of chla concentration data were found when the chla concentration was large fig 7 a e the data fusion methods could effectively reduce the large differences and improve the monitoring results of chla concentration as shown in august and september in fig 7 f the data fusion results based on the bif method had better agreement with the in situ chla observations compared with the other three data fusion methods however the data fusion methods did not have much improvement when the differences between in situ chla observations and remote sensing chla estimations were small fig 8 presents the spatial distribution of chla concentration in aug 14 2014 from interpolation of in situ chla observations remote sensing chla estimations and data fusion results of chla concentration obtained by the lrf nlrf cdff and bif methods the data in aug 14 2014 was selected because the chla concentration in august was the highest worst condition for water quality and the remote sensing data had good quality in this day fig s1 overall both in situ chla observations fig 8 a and remote sensing chla estimations fig 8 b had a similar spatial distribution where the chla concentration was high in the north and west of lake taihu meiliang bay zhushan bay gonghu bay and nw lake the remote sensing chla estimations in the east of lake taihu were relatively large however the in situ chla observations showed small values compared to the remote sensing estimations the spatial distribution of chla concentration based on the lrf method showed a general overestimation in chla concentration as seen in the north and east of lake taihu in fig 8 c the spatial distribution results based on the nlrf method in fig 8 d show that the high values of chla concentration were underestimated but the low values of chla concentration were overestimated the cdff results in fig 8 e show a significant overestimation in high values of chla concentration while no significant difference existed for the low values of chla concentration in terms of the spatial distribution of chla concentration by bif fig 8 f the chla concentration was significantly overestimated especially in meiliang bay zhushan bay gonghu bay and nw lake however the chla concentration in the east of lake taihu was close to the in situ chla observations comparatively the proposed bif algorithm could more effectively blend site observations and multi source remote sensing estimations to obtain accurate more chla concentration in lake taihu 5 discussion a number of studies have pointed out that there is a typical spatial and temporal mismatch issue between satellite pixels and point measurements brewin et al 2017 freitas and dierssen 2019 the high uncertainties from two different sources of data may impose significant influences on a data fusion scheme and thus lead to failure in validation since the modis remote sensing chla estimation is a kind of homogenization data of the 250 m 250 m grid and the in situ chla observation data is a point based observation this study adopted the nn and bwi techniques for grid to point extraction to assess the spatial mismatch errors between the two different sources of chla concentration data it is demonstrated that the nn method is more sensitive to the spatial mismatch errors with large rrmse and rmse while the bwi method calculated the average of nearby satellite pixel values to extract the remote sensing chla data for a specific in situ location wells 2005 which could effectively reduce the spatial mismatch errors this study suggests the adopted bwi method is effective for the grid to point extraction of chla concentration in lake taihu however an overall underestimation of the high chla values and an overestimation of the low chla values were found in the modis remote sensing estimates it was mainly because the coarse spatial resolution makes modis remote sensing data have a weak ability to capture extreme values of chla beck et al 2016 in addition modis has wide bands that do not target the chla reflectance peak well and has lower performance because of the optical interference of dissolved organic and suspended inorganic matter in the complex coastal and inland waters garcia et al 2006 beck et al 2019 the optical properties of the water in lake taihu are very complex which are dominated by a combination of chromophoric dissolved organic matter phytoplankton and non algal particulate matter zhang et al 2019 especially in summer the complex composition of water bodies and the wide range of chla concentration in lake taihu can cause large deviations in remote sensing chla retrieval algorithm this uncertainty could be reduced by using high resolution remote sensing data e g landsat series data with a 30 m resolution and sentinel series data with a 10 m resolution and developing more suitable remote sensing retrieval algorithms of chla in future studies uncertainty definition representation and quantification are usually determined by the selected error model tian et al 2013 this study used the additive and multiplicative models to quantify the errors between true chla concentration and remote sensing chla concentration it was found that the normalized residual errors of the two error models had large oscillations with the increase in chla concentration showing obvious heteroscedasticity limited by the sparse sampling points and the coarse sampling frequencies the number of samples used for analyses in this study was small and the majority of chla concentration was less than 40 μg l this could lead to the large oscillations of the mean normalized residual errors when the chla concentration was high in addition the weak capability of modis to capture extreme values may be the cause of heteroscedasticity for large chla values this study demonstrated that the multiplicative error model is more suitable because the mean values of the normalized residual errors in the multiplicative model had a steady oscillation and were close to zero especially when the sample size of chla concentration was large in fact the essence of the multiplicative error model is the logarithmic transforms of chla concentration data which has been proved to be able to reduce the effects derived from extreme data ranges to prevent high impact of large values of in situ observations gregg and conkright 2001 pottier et al 2006 it should be noted that both in situ chla data and remote sensing chla data have their own advantages and disadvantages the in situ measured data has high accuracy and reliability but has insufficient spatial representation ability whereas the remote sensing data has good spatial and temporal continuity but has poor accuracy it is recommended to combine the traditional in situ sampling and remote sensing methods to monitor the algal blooms paerl and huisman 2009 the three commonly used data fusion algorithms lrf nlrf and cdff showed good consistency in the training and validation periods and had an overall improvement in chla concentration in the west and north of lake taihu compared to the original remote sensing estimations however the spatial distribution of chla concentration based on the lrf method had a slight overestimation in the entire lake taihu which is attributed to the linear lifting of lrf the data fusion results by nlrf showed that the high values of chla concentration were underestimated but the low values of chla concentration were overestimated it is related to the logarithm regression which reduces the large variations of measurements with high concentration levels and narrows the numerical range of the data gallie and murtha 1992 among the three methods the cdff method had the worst performance this was mainly because the cdf matching method does not correct the mean of the chla data therefore this method cannot correct the systematic bias of the remote sensing chla data which led to limit improvement liu et al 2011 in particular the data fusion results of chla concentration based on the bif method had the best performance with the largest cc and the smallest rmse as well as kld the bif results had good agreement with in situ chla observations in the east of lake taihu where the other three data fusion methods made obvious overestimations the east part of lake taihu is typically covered by aquatic plants reeds weeds and other macrophytes which makes chla not to be accurately derived from remote sensing because the surface reflectance characteristics of lake can be significantly changed by the aquatic plants shi et al 2015 the proposed bif method is based on the numerical solution of a joint posterior distribution and the joint posterior distribution can be obtained by a prior distribution and the likelihood function chen 2005 wang et al 2019 since remote sensing chla estimations have relatively good spatiotemporal coverage and in situ chla data can provide the prior knowledge of true chla the bif method is believed to be able to effectively blend multi source data and provide more accurate results of chla concentration in lake taihu by blending the chla observations from multiple sources the proposed bif algorithm can better capture the high chla concentration in the west and north of lake taihu as well as the low chla concentration in the east of lake taihu in summer months since large values of chla in summer are more important to the risk management of algal blooms the improved performance of the proposed bif method is more meaningful the primary purpose of data fusion in the study is to reduce biases in the satellite chla estimates while retain the spatial variations of the satellite data by using the higher accurate in situ data the proposed bif method proved to be an effective approach to take the advantages of multi source data and is expected to be transferable and applicable to other eutrophic lakes to provide more accurate monitoring of chla concentration however it should be noted that it was assumed that the prior and likelihood distributions in bif obey to the gaussian distribution to simplify the computation this assumption could bring some uncertainties in addition it has been well recognized that wind driven circulation plays a key role in the spatial variations of chla concentration this is particularly true for lake taihu because it is a typical shallow lake with a large water surface area where the spatial distribution of materials such as suspended solids nutrients and algae is strongly affected by wind induced flow and wind drifting zhang et al 2011 huang et al 2014 temperature and light are also important factors to the growth of phytoplankton huang et al 2015 therefore more essential factors e g water temperature light and hydrodynamic characteristics should be fused to improve the accuracy of chla concentration measurements in future using more high frequency data through this way the dynamic features of chla concentration and algal blooms can be well captured due to the limitation of in situ observations we only selected 5 sites in 5 lake regions relatively uniformly distributed in space for validation in the study cross validation methods should be applied by obtaining denser and long time series data to avoid over fitting and provide more reliable verification results in further studies 6 conclusions a multi source data fusion method based on bayesian inference bif was proposed in this study to effectively blend the advantages of in situ measurements and remote sensing data for obtaining accurate chla concentration in a typical eutrophic lake lake taihu china from 2014 to 2016 the extracted chla concentration based on both nn and bwi methods showed good agreement with in situ chla observations the bwi method which has larger cc and smaller rmse and rrmse is more effective for the extraction of chla concentration compared to the additive error model in likelihood function of the bif model the mean values of the normalized residual errors in the multiplicative model had a steady oscillation and were close to zero especially when the sample size of chla concentration 0 40 μg l was large demonstrating that the multiplicative error model is clearly a more suitable choice four data fusion methods could effectively reduce the large differences between in situ chla observations and remote sensing chla estimations and the performance of the four algorithms were in the order cdff worst nlrf lrf bif best the proposed bif method had the largest cc 0 843 and the smallest kld 0 105 and rmse 12 433 μg l and the spatial distribution of the data fusion result by the bif method can capture the high chla concentration in the west and north of lake taihu as well as the low chla concentration in the east of lake taihu the proposed bif is expected to be transferable and applicable to other lakes to provide more accurate monitoring data of chla concentration future studies should focus on fusing data from higher frequencies higher temporal scales and more sources especially on other important influencing factors e g water temperature and hydrodynamic characteristics to improve the accuracy of large chla concentration measurements and characterization of chla dynamic processes in eutrophic lakes declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is supported by the national key research and development program no 2018yfc0830800 the national nature science foundation of china no 51709179 the water conservancy science and technology project of jiangsu province no 2018007 and the innovation cluster fund of nanjing hydraulic research institute no y917020 qiuwen chen acknowledges the support from the xplorer prize appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105057 
25831,network analysis of complex systems is a rapidly growing field both theoretical and empirical network studies have permeated many different ecological biological social and economic fields investigating the interrelationships between nodes as structural and functional attributes in static time dynamic or spatially explicit formats we consider the network construction phase as a vital but neglected component and therefore provide recommended guidelines describe how to evaluate the resulting network model quality and highlight tools to assess their plausibility thereby we stress the importance of constructing multiple plausible networks to comply with basic scientific standards and to pave the way for better informed evaluations finally we provide recommendations for the management and policy arena where we advocate a thorough interrogation of network analyses outcomes metrics especially with regard to their sensitivity to the construction process and a focus on relative changes between and within systems e g as indication of vulnerability rather than strict benchmarks keywords weighted networks ecosystem socio economic best practice plausibility sensitivity 1 introduction network modelling and analyses are tools to investigate the complexity of biological ecological social and economic structures as integral systems capra and luisi 2014 estrada 2012 newman 2003 a core advantage of an integral view is that it provides context for individual species in ecosystems for individuals in societies or individual economic activities in cities countries and the globe context renders a comprehensive view of the system that is inclusive of the wider impacts and roles of system components whole system properties can emerge from the system components interactions and these emergent properties are therefore only understood from analyses at this level capra and luisi 2014 fath and patten 1998 jørgensen 2012 ulanowicz 1986 2009a the many different types of interactions within systems trophic behavioural energy water money etc facilitate the co existence of several networks operating in parallel within a system golubski et al 2016 olff et al 2009 treml et al 2015 zand et al 2017 all networks however consist of nodes e g vertices compartments that are linked by edges that can be weighted and directed the construction and analysis of network models is a growing approach to study many types of complex systems identified as network science brandes et al 2013 newman 2010 network ecology is the use of network models to investigate ecological and evolutionary questions and it is a proper subset of network science fig 1 network ecology is rapidly expanding into many different fields in ecology and socio economics borrett et al 2014 as well as socio ecology e g sayles and baggio 2017 treml et al 2015 in the wider ecological field it has found applications to ecosystem service assessments dee et al 2017 to habitat connectivity in the life history of single species buddendorf et al 2017 the interplay between human consumption and environmental issues dai et al 2012 the importance of functional traits in ecosystems gravel et al 2016 and landscape connectivity fletcher et al 2011 a key challenge for the success of network science lies in the model construction and evaluation steps model quality is essential the challenge of quality network construction is a theme apparent from fields as diverse as archaeology e g groenhuijzen and verhagen 2017 waste water treatment works martin and vanrolleghem 2014 epidemiology eames et al 2014 pellis et al 2015 neural networks peng et al 2006 tsoulos et al 2008 geomorphic systems phillips 2012 healthcare systems zand et al 2017 or effects of natural disasters zheng et al 2017 for example in the social sciences the construction process has explored methodology regarding peer reputation networks azzedin and ridha 2008 pujol et al 2002 or how the behaviour of social network site users generates certain types of networks krasnova et al 2010 the definition and measurement of interactions and how to manage data gaps has also received attention in social networks eames et al 2014 the dramatic recent increase in the amount of genetic sequencing data has opened opportunities for big data analyses necessitating considerable attention also in the field on network construction methodologies and evaluation for instance in genealogy e g cassens et al 2005 for gene regulatory networks e g chai et al 2014 chen and vanburen 2012 and co expression networks e g kumari et al 2012 tang et al 2011 while network construction is a widely discussed topic in different disciplines in this paper we focus on directed weighted networks which have been the subject of analyses by methods collectively known as ecological network analysis ena borrett et al 2018 lindeman 1942 patten et al 1976 scharler and fath 2009 ulanowicz 1986 figs 1 and 2 two types of directed weighted networks have been most prominent the construction of trophic ecosystem networks energy and nutrient networks and socio economic networks e g urban metabolism sectorial water use monetary exchanges fig 3 though we believe that much of the methodology may be more broadly applicable application of ena has brought new insights into ecosystem functioning and hypothesized emergent properties over the past decades for instance the understanding that overfishing decreases the mean trophic level of the fishery as large predators are overfished first pauly et al 1998 was revealed by calculating trophic levels of target fish species from the analyses of trophic networks the concept of fishing down the food web is now firmly embedded in the literature explaining the diminished resource of sought after higher trophic level predatory fish e g tuna in fisheries catches other longer established ecological theory on community motifs holt 1997 have been investigated in the big cypress preserve ecosystem florida usa and revealed beneficial relations between species by examining both direct and indirect effects ulanowicz and puccia 1990 that were not apparent from the known biology and ecology of the involved species alone bondavalli and ulanowicz 1999 as an example an intraguild predation configuration alligators and snakes feed on frogs alligators also feed on snakes revealed mutualistic relations between predators alligators and their prey frog this motif of trophic interaction is only a particular portion of their feeding interaction within the food web and alligators had further net positive indirect effects on 11 of their prey groups although direct effects of predators on their prey are negative the consequences of these deeper insights into relations between species is a changed perception of importance of this species within the context of the trophic web this has added information on the ecosystem next to other important concepts such as species diversity predation or habitat modification methods and metrics emerging from ecological networks have been applied to socio economic systems with increasing enthusiasm tang et al 2021 for instance a study on carbon emissions sequestrations and fluxes of the beijing metropolitan area revealed positive and negative direct and indirect relations between the city s components xia et al 2016 one of the main findings were that change in land use and economic sectors over 20 years highlighted that urban expansion caused a decline in mutualistic relations between metropolitan sectors overall even though ena is used widely in multiple different fields borrett et al 2018 there are as yet insufficient guidelines on how to generate input data the construction process itself and how to evaluate the quality of the network model it is essential for practitioners to recognize that the construction of network models is fundamentally a modelling step and as such the process should follow best ecological modelling practices this is reminiscent of efforts in other areas of ecological modelling e g grimm et al 2010 jakeman et al 2006 parrott 2017 schmolke et al 2010 hipsey et al 2020 which provide guidelines for model development evaluation model description validation and documentation amongst others and are applicable to decision support models individual and agent based models and more generally environmental models unfortunately the direct application of these existing guidelines to specifically network models is not always clear due to model differences thus this paper provides an overview of adapted and additional guidelines and principles to the construction of network models of ecosystems and socio economic systems there are several existing recommendations but little overarching consensus on what makes a quality ecosystem network model instead as we already know from other modelling studies the sufficiency or success of the model is dependent on research questions and hypotheses the system and data availability such ambiguity is in conspicuous contrast to more established guidelines for generating ecological datasets e g underwood 1997 at present the available guidelines are few and are a loose conglomerate of descriptions of data required how to construct the network from the data and how to generate possible network solutions of the available data ayers and scharler 2011 dame and christian 2006 fath et al 2007 heymans et al 2016 lassalle et al 2014 link 2010 van oevelen et al 2010 ulanowicz 1986 ulanowicz and scharler 2008 all of this documentation describes specific possible steps within the process of generating networks but it does not give complete guidance on multiple critical topics including how to design fieldwork to obtain data appropriate for constructing networks how to transform the data into the correct format conceptually and practically how to identify links how to deal with missing data and lastly but perhaps most importantly how to evaluate whether a network model is a sufficient or plausible representation of the system in multilayer networks multi and hypergraphs delmas et al 2018 golubski et al 2016 lin and sutherland 2013 pilosof et al 2017 increased types of interactions can be modelled within the same system a guided construction process is thus valuable for the current mainstream of network models as well as for networks expanded into different dimensions which we will see increasing in future the last publication explicitly focused on the construction of weighted ecological network models and specifically for ecosystems was published more than 10 years ago fath et al 2007 since then considerable progress has been made to reveal and solve flaws in network construction processes with many useful developments in the field this paper seeks to capture these developments and common practices to provide guidance during the network construction process and present a more unified approach it is divided into four sections that each deal with a different aspect of this process fig 4 and includes 1 network construction 2 evaluation 3 documentation and 4 utility of network models including interpretation of ena results this paper is intended as a reference and for first entry researchers and more senior students to become acquainted with concepts of ecosystem and socio economic network construction methodology 2 network construction data requirements for network construction are generally high to start the investigator has to identify the system components and represent them as compartments or nodes and then determine how the nodes interrelate with others in the system to map the network edges or links fig 4 this requires knowledge on the inputs into and outputs from each node in ecosystem networks it is thus imperative to know the intake consumption and the proportions to which the intake is divided into outputs of production e g somatic production reproduction natural mortality unassimilated consumption faeces and metabolic cost respiration odum 1971 the outputs that are useable in the system production and unassimilated consumption are furthermore divided along links to various other nodes in the system representing their consumers and detritus nodes respectively this concept is similarly applied to socio economic networks where inputs into and outputs from nodes may be virtual water carbon sequestration and emission in urban environments money or commodity trade flows ecosystems are thermodynamically open jørgensen et al 1999 and therefore receive and produce boundary flows as imports and exports in food webs gross production of primary producers is often treated as an import and respiratory losses as boundary losses boundary flows can also include migratory movements or long distance transport processes socio economic network models do not always incorporate boundary flows e g raw materials feeding into a trade network of the product the inclusion of natural resources as boundary flows could certainly be added to highlight resource dependency applicable to much of the global economic activity to move from a single network representing a temporal or spatial snapshot towards dynamic networks over time and space data requirements increase according to the extent of the temporal and spatial frame it is no surprise that historically network construction has been guided by data availability some of the first available ecological networks silver springs odum 1957 or cone springs tilly 1968 are therefore rather small with a total of five highly aggregated nodes e g multiple species and resources grouped together these early networks illustrate perfectly the ambition at the time to characterise ecosystem processes at a level beyond that of species and communities even if comprehensive datasets were not available although present day ecosystem networks are better resolved with a larger number of nodes that may reach 120 this is likely far less than the number of species present availability of suitable data for network construction remains an issue as it can influence how well the network model represents the system and has consequences for analyses outcomes e g abarca arenas and ulanowicz 2002 allesina et al 2005 baird et al 2009 gauzens et al 2013 johnson et al 2009 jordán and osváth 2009 continuous datasets in time and over spatial extents are especially rare for ecosystems but not necessarily for socio economic systems fang et al 2014 kharrazi et al 2017 zhang et al 2017 therefore few time series and spatially explicit networks have found their way into the ecological literature but see christian and thomas 2003 chrystal and scharler 2014 haraldsson et al 2018 scharler 2012 steenbeek et al 2013 de la vega et al 2018 2 1 existing network construction processes and software several network construction techniques for ecosystems became mainstream in the 1980s ecosystem and socio economic networks in the early decades were constructed mainly by arranging gathered data into spreadsheets that allowed the verification of mass balance by comparing inputs and outputs e g ulanowicz 1986 for the early small networks silver springs odum 1957 cone springs tilly 1968 this was manageable but far less practical for larger networks a plain text format scor comprised the input structure for the netwrk software facilitating the network analysis ulanowicz and kay 1991 this was later translated into an excel front end and the calculations to the gui operated software wand allesina and bondavalli 2004 used by ecologists but did not change the way networks were constructed subsequently a more objective method was developed which systematically assigns weights to interactions within the constraints of node consumption and production matlod ulanowicz and scharler 2008 building partially on this methodology an expanded version constructing stoichiometric multitrophic networks has recently been applied to semi terrestrial and marine environments scharler et al 2015 scharler and ayers 2019 a similar bioenergetics food web modelling approach ecopath was conceived in the early 1980s polovina 1984 subsequently it was developed into the software package ecopath with ecosim ewe that also enabled a network construction process christensen and pauly 1992 the construction process in ewe is facilitated by a user friendly interface where standardized and available data are entered and the software will calculate missing data through mass balance and other modelling assumptions recently the mass balance algorithms have been translated into r r core team 2018 which makes this approach further accessible through the package rpath lucey et al 2020 the direct incorporation of additional information on diet mccormack et al 2019 and diet uncertainty analysis bentley et al 2019 promise to advance realistic representations of trophic webs an alternative network construction methodology is that of linear inverse modelling lim niquil et al 1998 van oevelen et al 2010 vézina and platt 1988 here networks flows are inferred from equalities and inequalities describing stocks and flows of typically undersampled food webs the method provides either a single network solution or a range of solutions presented as an ensemble of possible networks within the range of the input data in addition stoichiometric and isotope data can be used in this approach van oevelen et al 2010 this method was recently advanced by generating multiple plausible networks to certain specifications regarding the flow ranges hines et al 2018 waspe et al 2018 all of the described approaches result in networks with weighted links that represent how much energy or material depending on model currency is transferred from one node to another in the selected model time step with the exception of the multi solution linear inverse modelling approach their common disadvantage is that the variability of data used to construct the networks is largely lost because the result is a single network per point in time or space when networks represent only a single version of a system statistical analyses are largely infeasible but see kones et al 2009 there have been few attempts to reflect this variability of input data e g ayers and scharler 2011 but hines et al 2018 and waspe et al 2018 started to address this issue systematically within the lim framework this framework already provides the possibility to produce an ensemble of plausible network models based on known data and hines et al 2018 illustrate how they can be constructed from already existing single networks for instance networks can be replicated retrospectively by applying a user defined range of flow values e g 50 of a nominal estimated value or a user can specify a range defined by empirically known data variability hines et al 2015 2018 de la vega 2018 bentley et al 2019 an ensemble of plausible networks is then generated based on one or more new flow ranges and the group of networks is returned to enar for subsequent analysis borrett and lau 2014 lau et al 2017b as each model parameter is being randomly sampled from the empirically estimated range it appears that these plausible models act like replicate samples of the system as a result network analysis results can be compared more rigorously by comparing their density distributions that result from a given uncertainty or range of flow values which lets the user draw more robust conclusions for example hines et al 2015 applied this uncertainty technique in an ena application to determine the impact of sea water intrusion on microbially mediated nitrogen removal pathways in the cape fear estuary nc usa this work was able to robustly conclude that while the sites experiencing different salinity regimes had the same n2 removal capacity the same coupled steps in the nitrogen cycle used to achieve the removal were utilised in substantially different magnitudes more generally these uncertainty analyses allow for a deeper understanding of networks in terms of their function and behaviour ma et al 2018 this constitutes a large and absolutely necessary step towards a more comprehensive representation of systems as networks waspe et al 2018 expanded the methodology of van oevelen 2010 and hines et al 2018 to preserve the entire empirically measured range of the input data during the initial network construction phase in the r package flowcar the networks generated in this way are diagnosed to be representative of the entire range of empirically measured input data they are subsequently packed into a format readable by enar for network analysis and the calculated ena metric distributions plausibly represent the range of empirical measurements another way of constructing networks and especially for time series networks is to extract snapshots from time dynamic simulations that are conducted in software such as stella isee systems econet kazanci 2007 ewe christensen et al 2005 vensim ventana systems inc and others care should be taken that all data needed for an ena are able to be extracted from networks generated this way fath et al 2007 as an example econet kazanci 2007 schramski et al 2011 uses basic input data stocks flows which are converted into differential equations the goal is to arrive at a steady state system to be used subsequently for analyses although simulated ecosystem networks have played a prominent part in the network literature e g the cascade model cohen and newman 1985 niche model williams and martinez 2000 allesina et al 2008 there are few similar approaches to construct weighted versions of networks a notable exception is the method introduced by fath 2004 on cyber ecosystem assembly then how to start the network construction process regardless of which methodology is used to construct networks the basic data needs are very similar two fundamental steps of the process are the system conceptualization referring to structure purpose and ecosystem boundaries and the subsequent data requirements and parameterisation 2 2 conceptualization 2 2 1 system boundary as with most modelling one of the first steps to constructing a network model is to conceptualize the system which starts by identifying its boundaries haefner 2005 fath et al 2007 this process begins with deciding or inferring from data what system elements are inside the system of interest and what elements fall outside the system in this process the modeller will determine 1 the spatial scale and resolution of the system and 2 the timescale it represents day season year for instance should the spatial resolution adhere to natural boundaries such as watersheds or to political boundaries should the network model include features such as the littoral zone of lakes and estuaries or the benthic environment for an open ocean ecosystem or lake sometimes the system boundary is determined by data availability which may lead one to restrict its physical or temporal dimension for instance to the pelagic or benthic realm to municipalities or certain economic sectors or a particular season or decade 2 2 2 structure of networks nodes links resolution the next step in system conceptualization is the definition of the basic structure of a network created by deciding on the network nodes and edges fig 4 for example how many nodes will be used to represent the ecosystem elements e g species functional groups non living resources and what are the interlinkages or edges among the nodes in network models used for ena a single directed edge between two nodes represents the energy or matter transfer even if it arose from multiple ecological processes again the node and edge conceptualization should be guided by the research questions and data availability and overall lead towards an appropriate representation of the system some system components represent highly important functions e g primary producers in ecosystem or water sources in virtual water networks that it would be unreasonable to dismiss them this may require efforts to close the data gap from direct e g fieldwork data banks or indirect sources e g literature expert opinion once constructed scientists should use sensitivity and uncertainty analyses to judge the model sufficiency and analytical consequences of the heterogeneity of data abundance and quality in terms of sensitivities of model output uncertainties to those of the input data increasing the model resolution resolution refers to the degree of aggregation or disaggregation by increasing the number of nodes and links is only feasible if data are available for parameterisation the resolution of networks has received considerable attention see section 1 as network structure and thus analysis outcomes are generally sensitive to the number and proportional weight of links haller bull and rovenskaya 2019 socio economic network models are usually more aggregated compared to those of ecosystems due to the more intense aggregation of data sources in economic systems the reporting of trade flows is more readily available as aggregated datasets with nodes representing sectors incorporating many different types of factories or economic activities e g transport agriculture annual trade volume this results in a higher degree of aggregation of commodity flows and therefore a higher connectivity of the network in ecology the recognized importance of taxonomic and functional biodiversity lead to a tendency to disaggregate nodes where possible overall it is essential to have all critical components of the system represented in the model even if in more aggregated nodes than to completely omit key elements the structure of network models can differ due to underlying system differences and also due to differences in research questions and system conceptualization a particular system can thus be represented by different network models this has made comparisons challenging in the past researchers have tried to standardise network structure by resolving different networks to the same number of nodes even going to the extreme of featuring nodes as placeholders for temporarily absent species e g baird et al 2011 however such practice may conceal real differences between networks for instance the absence and presence of migratory species horn et al 2019 or of seasonally active economic sectors in trophic webs direct flows between nodes are largely unidirectional and not reciprocated by return flows exchanges between highly aggregated nodes nodes to represent groupings of functionally similar species may appear bidirectional represented by two single direction edges in opposite direction for instance exchanges between living nodes and non living nutrient pools or detritus may be bidirectional in economic networks bidirectional flows between any two particular nodes are more common this observation may be a consequence of a relatively high degree of aggregation in currently available empirical economic and socio economic network models or it could reflect a higher interaction incidence in comparison to trophic webs economic networks structures are subject to and therefore a result of anthropogenic concepts some of which may differ substantially from that of ecosystems e g fang et al 2014 huang and ulanowicz 2014 xia et al 2016 for instance they lack the strong metabolic constraints of ecosystems also trading networks seldom incorporate the commodity source as opposed to ecosystem networks which feature energy or nutrient imports across the system boundary to depict their connectedness to other environments and external sources in reality this is an important feature for certain economic sectors and for those warrants consideration 2 3 basic data requirements and parameterisation node and link weights efforts put into data gathering will be reflected in the outcomes of model analyses and both small and large links deserve attention large links usually emanate from high biomass e g detritus trees or high turnover e g phytoplankton bacteria nodes and a high variability in their value will result in the same for calculated ena metrics e g ludovisi and scharler 2017 it is important to consider which weak or small weight links of a system to include as their absence or presence change the network structure and therefore can change system function and the analytic results for example the number and magnitude of weak links in networks has an influence on network metrics as they are calculated from the flow distribution within weighted directed networks incomplete system specific data availability for nodes and links can be supplemented depending on the type of missing information if only some part of the data requirements are missing they can often be estimated by mass balance equations so that inputs equal outputs of energy material trade flows wealth accumulation etc from expert opinion or information from the literature a balance of node inputs and outputs i e sum of inputs sum of outputs is required for many different types of network analyses and can be balanced by using the equations in box 1 historically model balance was achieved manually e g by assessing and comparing inputs and outputs on a spreadsheet this approach is cumbersome but has the advantage that experts can apply their system knowledge to determine the reasonableness of the necessary changes several automated procedures are available to balance all nodes simultaneously for the entire network e g allesina and bondavalli 2003 christensen et al 2005 lau et al 2017a van oevelen et al 2010 however one should be cognisant that these balancing methods tend to change all of the original flow values in the model some values can exceed what is biologically reasonable to achieve such balance and they typically disregard the underlying data certainty or quality practitioners should compare the balanced and original network model to assess the algorithmic changes and changes to the original input data some of the changes could result in ratios of stock to flow that turn out to be unreasonable for certain nodes another solution therefore is to add an in or decrease of stocks to assist in the balancing procedure ulanowicz 2004 suggested this could take the form of adding a vector each that represent the stock increase as an added input and the stock decrease as an added output box 1 more information on model evaluation is provided in the following section section 3 evaluation network data can be represented as matrices and vectors box 2 a flow matrix indicates not only the presence or absence of a flow edge from source node i to receiver node j but in weighted networks the amount transferred via a particular link per unit time is stated for clarity care should be taken to specify the flow direction as both row to column and column to row orientations are used in the literature see e g scharler and fath 2009 for certain links databases and online tools are available to identify feeding links and sometimes estimate flow values e g brey 2001 froese and pauly 2000 gray et al 2015 pasquaud et al 2007 poelen et al 2014 short of direct measurements respiration rates in ecosystems can be estimated from body size and environmental data e g brey 2001 brown et al 2004 and a popular estimate of production values are production biomass ratios from the literature whenever literature data are used data sources for the same species in the same environment e g biogeographic region type of habitat temperature are preferable information on the presence and absence of trophic links is also often gathered from the literature e g stomach contents feeding experiments isotope data and measured data include observations or the analyses of stomach content isotopes or fatty acids although literature data can give an indication of the feeding guild of a species they might not be an accurate representation of the feeding interaction of every system the species occurs in overall it is imperative to keep in mind that literature data are estimates and may therefore introduce bias into the network construction process this applies to both ecosystem and socio economic networks usually a combination of methods to parameterise the network are applied and for socio economic networks existing data recorded in open access databases for instance municipal records or trade relations for certain sectors are readily available data sources an empirical sampling design that connects directly to network construction is a preferable approach to generating data in practice however this is not always feasible and networks are constructed from measured and literature data and expert opinion to fill gaps for ecosystems important data sources are those obtained by continuous data recorders long term government datasets e g fisheries records or open access databases when data are gathered specifically for constructing networks such actions may be classified according to their resource investment these include 1 extreme investment when data are actually measured for nodes and links for which no data are available or 2 moderate investment that includes estimating some data that are missing we speak of 3 minimal investment when balance adjustments within the range of input data from which the networks were constructed are sufficient a recently developed methodology assists in the decision making for diverting resources to measure certain links over others depending on their importance kazanci et al 2020 this is a useful guide applicable especially in resource constrained or data constrained environments for network science to move from largely descriptive studies to hypothesis driven research adequate initial data gathering that enables researchers to pose and answer hypotheses is a requirement and thus combined efforts by data scientists and empirical scientists are essential e g delmas et al 2018 a step by step summary of network construction guidelines is provided in box 3 3 evaluation evaluation of constructed network models has historically been a neglected topic this is in part because the most commonly used model verification and validation methods in ecological modelling compare the temporal output of dynamic models to observed field data but these methods do not apply to static models like most of the ena network models this is further complicated by a lack of data for validation especially for early networks and lack of evaluation on how network analyses outcomes are affected by the network construction process today the desire to use network models and ena in system management and policy making dame and christian 2008 fath et al 2019 goerner et al 2009 heymans et al 2016 kharrazi et al 2013 safi et al 2019 xia et al 2016 makes model evaluation essential the evaluation should be conducted on both the input data and the network analyses outcomes and can be treated as input data and analyses output centered validations such evaluations have been described for various software and different modelling frameworks e g bennett et al 2013 costanza et al 1992 grimm et al 2010 heymans et al 2016 jakeman et al 2006 schmolke et al 2010 here we add points that are specific to system network models 3 1 input data centered the field or literature data used for network construction should always be evaluated for their fit for the model purpose and especially when the goal is management or policy relevant research costanza et al 1992 when considering the relevance of the data for node and flow values of the network modelers should keep in mind that the input data themselves may not be a good representation of the system e g bennett et al 2013 due to inadequately describing the system when data are used from studies not designed for this purpose for instance literature metabolic ratios for fauna specific to certain latitudes are less likely to be representative for those occurring at different latitudes or between vastly different phyla or feeding guilds e g detritivores and carnivores should it not be possible to construct replicate plausible network models of a system to reflect the system variability itself researchers can conduct extensive sensitivity and uncertainty analyses to judge the network model quality and the dependency of the ena results on less well known inputs the final network model parameterisation can be compared to the original input data using various descriptive and statistical methods to evaluate differences of single node and flow values an automated assessment of the amount of system specific data used for network construction is incorporated in the software ecopath christensen et al 2005 through a measure called pedigree that represents the proportion of system specific input data in the network model even though the amount of system specific data used for network construction is not a full guarantee for a good quality network it serves as a broad gauge of how well the network model could represent the system also incorporated into the ewe software is a data check before mass balancing for the entire network called prebal link 2010 with this tool certain attributes across nodes and trophic levels can be checked for their correspondence with known general ecosystem attributes at all times and notwithstanding the software used an assessment of correspondence between balanced networks with original input data of node and flow data and data variability should be conducted 3 2 analyses outcome centered once ena has been applied to the network the network analyses outputs the analytic results need evaluation for example practitioners should consider if the results show artefacts from an ill defined network because the network construction process is influenced by data availability it may become apparent only after analyses that the degree of node aggregation prevented detection of a system feature of interest in this case the network topology may have to be re evaluated for its fitness for purpose such considerations are important also when comparing networks while it is important to maintain the same model assumptions and approach e g node aggregation decisions real differences between the systems should not be masked by trying at all costs to keep the structure the same here again multiple plausible networks representing the system are useful and a knowledge of the system by experts in different fields may provide deeper insights into the accuracy of results how convincingly results can be communicated to people outside the field e g stakeholders is another desirable check it requires a deep understanding of the intricacies of the network and how its quantitative structure resulted in certain metric values a second powerful approach is to validate selected network analysis results with an independent method for example deehr et al 2014 validated their fisheries models of core sound north carolina by comparing the ena predicted node trophic levels calculated by ecopath with an independent estimate of the node trophic levels from isotope analysis conducting internal checks by using different types of analyses on the same dataset to calculate nutrient limitations of nodes in several networks was applied to a mangrove ecosystem by scharler et al 2015 strong agreement between the independent methods was used as convincing evidence of the trophic models quality we suggest the guidelines listed in box 4 to evaluate the representativeness of constructed networks 4 model reporting documentation scientific reporting aspires to be replicable transparent and accessible to achieve this scientists working with network analysis must 1 document the network construction itself including adjustments to primary data and evaluations of input data and analyses outcomes and 2 consider the publication and accessibility of the network models scientific writing conventions require methodologies to be described in enough detail to be replicated by other investigators this is true for modelling studies as much as it is for experimental work the documentation of modelling decisions the use of large amounts of data and clarifications on any necessary data transformations and calculations are often voluminous but necessary it is difficult to identify a single common documentation format because networks may have very different structures data sources and purposes however this does not preclude the documentation of network construction and evaluation to be presented in detail and to standardise documentation where possible e g ayers and scharler 2011 bonet et al 2014 grüss et al 2017 gurney et al 2014 hoch et al 1998 schmolke et al 2010 articles that present new network models must include a full description of the network construction to enable peer review and evaluation beyond individual publications there are several collections of previously published networks that are available for additional research these network data can be stored in different formats for instance ulanowicz maintained a collection in a data format referred to as scor formatting ulanowicz and kay 1991 a collection of over 100 ecosystem networks which partially overlap with ulanowicz s set are distributed with enar in the r network data format borrett and lau 2014 lau et al 2017a https github com seelab enar ecopath with ecosim networks are stored in ecobase http sirs agrocampus ouest fr ecobase allowing for submission of new networks and extraction of existing ones another more recent development is that of mangle a development and storage for binary food webs in r poisot et al 2016 such repositories are useful for cross system research network construction and common storage format however the network model information may differ within and between databases according to purpose a list of commonly used network construction methodologies model databases and software are provided in box 6 in addition to evaluation of the science clear documentation of the network construction process may ultimately benefit the research field to enable new participants and the emergence of stronger community standards this is especially important for addressing common data challenges including that of missing data applying conversion factors and how to distinguish between plausible and possible network models above all documenting the methods used during the entire network construction process adds credibility to the process and final network structure and increases the potential applications of networks and their analyses in management and policy making costanza et al 1992 we recommend that clear documentation of the network construction and evaluation process should be included in ena publications as part of a best practice it will allow the approach to become more rigorous and the results to be better interpretable the communication of the variability and uncertainty of the analysis outcomes plays an important role in providing realistic recommendations to stakeholders and facilitate application saltelli et al 2020 box 5 provides basic guidelines we can provide for information on the accuracy validation variability and uncertainty of the network construction phase 5 networks as response variables how scientists construct evaluate and document network models is critical to create a successful ecological network science once networks are constructed and deemed sufficiently representative of the system researchers can use them as experimental response variables providing information at system components and whole ecosystem levels see memmott 2009 network metrics describe various features of the system in the ena framework these metrics are frequently labelled as either structural e g the number of nodes edge density or functional e g total system throughflow finn cycling index nevertheless the functional metrics often incorporate both structure and function e g bersier et al 2002 delmas et al 2018 kazanci and ma 2015 ulanowicz 1986 weighted networks are used to calculate the latter because a considerable amount of information is inherent in the distribution of link weights within networks e g allesina et al 2009 bersier et al 2002 comparing two or more models of a system under different conditions is one way network models can function as response variables in this application analyses are focused on the whole system response to the conditional change for example deehr et al 2014 compared trophic network models parameterized with data from sites where shrimping was allowed and sites where shrimping was excluded to identify the ecosystem impact of the fishing activity de la vega et al 2018 compared network models of the sylt romo bight ecosystem parameterized with data from different seasons by applying the uncertainty analyses to generate multiple plausible model given the data uncertainty the authors discovered that some whole system network metrics such as flow diversity and effective link density varied seasonally as expected while other indices such as the average mutual information showed no significant seasonal variations multiple recent applications of the uncertainty analysis hines et al 2015 de la vega et al 2018 bentley et al 2019 have directly compared the distribution of analytical results statistical difference was inferred if the 95 confidence intervals did not overlap this is a useful but conservative approach in some cases it might be possible to use a nonparametric statistical approach to compare the distributions we suggest a nonparametric test because the observed distributions of network metrics from flows of multiple plausible networks in general often do not follow a simple distribution pattern whereas for decades theory has far outrun applications in the network analysis field many efforts have arisen in recent years to use ecosystem and socio economic networks in management and to investigate how they could shape policy making fath et al 2019 heleno et al 2014 pincetl et al 2012 safi et al 2019 zhang et al 2013 longo et al 2015 parallel to this is an increased effort to understand how input data and ena output relate and therefore how network analysis metrics can be meaningfully applied borrett and osidele 2007 christian et al 2009 kaufman and borrett 2010 ludovisi and scharler 2017 although the ena metrics generally have thorough theoretical underpinnings one challenge to their use in management is that the correspondence between any metric value and desired system states e g healthy sustainable resilient is not always immediately apparent a key challenge for using ena metrics for ecosystem and economic system management is to determine which ena metric values are most sensitive to certain system states fath et al 2019 few ena metrics are well benchmarked and interpretations largely remain relative among networks this relative comparison however is a powerful tool for example it can be used to track system function over time christian and thomas 2003 schückel et al 2015 luong et al 2014 and compare differences among systems or subsystems baird et al 2011 pezy et al 2017 scharler and baird 2005 such relative indicators may overall be a better guide for change than metrics benchmarked against absolute values because they track trajectories of individual systems one example of metric benchmarking has been applied to the metric termed system s robustness ulanowicz 2009b that considers the information inherent in the weighted flow structure of networks this has been adopted by ecologists and economists alike e g goerner et al 2009 kharrazi et al 2017 mukherjee et al 2015 scharler et al 2018 it has been proposed as a way of identifying the optimal functioning state as a dynamic tension between system needs both for efficiency and resilience the latter of which is expressed as redundant or parallel transfers within the network ecosystem networks constructed from empirical data have been shown to congregate at a point that is indicative of high robustness signifying an advantage for ecosystems with a considerable proportion of redundant flows in addition to favouring efficiency ulanowicz 2009b economic networks seem to feature many more redundant pathways and thus have their highest robustness at different proportions of flow redundancy and efficiency compared to ecosystem networks this however may be an artefact as a result of high node aggregation the interpretation of the network metrics can depend on model characteristics as well as management goals this further implies that scientists and managers applying ena need to carefully consider their results and cannot always depend on previous ecological interpretations of the metrics context dependency of a desired state is an important consideration when using ena metrics many of the metrics characterise the state of the system in a driver pressure state impact response dpsir frame burkhard and mueller 2008 lewison et al 2016 and do not characterise the desirability of the state for example an increase in the node or total system throughflow might be desirable in marine fisheries models tracing carbon or energy deehr et al 2014 but it would be undesirable in models tracing toxins in the food web taffi et al 2015 6 conclusions the overarching goal of this paper is to improve the development of weighted network models of ecosystems and socio economic systems the application of ecological network analysis and its use for system management by considering guidelines for best practices specifically we considered key elements of network construction model evaluation documentation and scientific applications we highlighted challenges in the network construction process and identified points that might result in unrealistic or not for purpose networks more broadly it is useful to recognize that network construction is a specific form of modelling and that it should thus adhere to the best practices prescribed for general modelling activities including robust forms of model evaluation e g verification validation sensitivity and uncertainty analysis the construction process is time consuming and yet critical as a strong foundation for credible network analysis results and for the field to move into more rigorous hypothesis testing and policy and decision making realms as we seek greater management applications of ecological network analysis the critical gap between theoretically oriented network analysis and its application becomes more apparent this highlights a key direction for future research and development a closer collaboration between theorists and empirical scientists can alleviate some of the remaining challenges especially for checking the quality of constructed networks filling crucial data gaps and the interpretation of calculated metrics against a backdrop of empirical knowledge of the system overall adhering to basic scientific standards of assessing data quality reporting methodology hypothesis testing and more thorough interpretation of metrics will already make the field more appealing to stakeholders standardized protocols provide for a certain efficiency and accessibility the network model databases mentioned are highly useful to access a large number of networks especially for assessments across different systems in the absence of a single agreed upon standard format for model presentation it is useful to create interchangeable formats in various databases that allow access by different users for different types of analyses in different software besides such technical issues the most critical challenges at present are the data verification of constructed networks and the interpretation of metrics in ways that increase our understanding of system function for management actions and eventually for policy making declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was partially funded by an nrf knowledge interchange and collaboration fund kic160211157827 award to u s 
25831,network analysis of complex systems is a rapidly growing field both theoretical and empirical network studies have permeated many different ecological biological social and economic fields investigating the interrelationships between nodes as structural and functional attributes in static time dynamic or spatially explicit formats we consider the network construction phase as a vital but neglected component and therefore provide recommended guidelines describe how to evaluate the resulting network model quality and highlight tools to assess their plausibility thereby we stress the importance of constructing multiple plausible networks to comply with basic scientific standards and to pave the way for better informed evaluations finally we provide recommendations for the management and policy arena where we advocate a thorough interrogation of network analyses outcomes metrics especially with regard to their sensitivity to the construction process and a focus on relative changes between and within systems e g as indication of vulnerability rather than strict benchmarks keywords weighted networks ecosystem socio economic best practice plausibility sensitivity 1 introduction network modelling and analyses are tools to investigate the complexity of biological ecological social and economic structures as integral systems capra and luisi 2014 estrada 2012 newman 2003 a core advantage of an integral view is that it provides context for individual species in ecosystems for individuals in societies or individual economic activities in cities countries and the globe context renders a comprehensive view of the system that is inclusive of the wider impacts and roles of system components whole system properties can emerge from the system components interactions and these emergent properties are therefore only understood from analyses at this level capra and luisi 2014 fath and patten 1998 jørgensen 2012 ulanowicz 1986 2009a the many different types of interactions within systems trophic behavioural energy water money etc facilitate the co existence of several networks operating in parallel within a system golubski et al 2016 olff et al 2009 treml et al 2015 zand et al 2017 all networks however consist of nodes e g vertices compartments that are linked by edges that can be weighted and directed the construction and analysis of network models is a growing approach to study many types of complex systems identified as network science brandes et al 2013 newman 2010 network ecology is the use of network models to investigate ecological and evolutionary questions and it is a proper subset of network science fig 1 network ecology is rapidly expanding into many different fields in ecology and socio economics borrett et al 2014 as well as socio ecology e g sayles and baggio 2017 treml et al 2015 in the wider ecological field it has found applications to ecosystem service assessments dee et al 2017 to habitat connectivity in the life history of single species buddendorf et al 2017 the interplay between human consumption and environmental issues dai et al 2012 the importance of functional traits in ecosystems gravel et al 2016 and landscape connectivity fletcher et al 2011 a key challenge for the success of network science lies in the model construction and evaluation steps model quality is essential the challenge of quality network construction is a theme apparent from fields as diverse as archaeology e g groenhuijzen and verhagen 2017 waste water treatment works martin and vanrolleghem 2014 epidemiology eames et al 2014 pellis et al 2015 neural networks peng et al 2006 tsoulos et al 2008 geomorphic systems phillips 2012 healthcare systems zand et al 2017 or effects of natural disasters zheng et al 2017 for example in the social sciences the construction process has explored methodology regarding peer reputation networks azzedin and ridha 2008 pujol et al 2002 or how the behaviour of social network site users generates certain types of networks krasnova et al 2010 the definition and measurement of interactions and how to manage data gaps has also received attention in social networks eames et al 2014 the dramatic recent increase in the amount of genetic sequencing data has opened opportunities for big data analyses necessitating considerable attention also in the field on network construction methodologies and evaluation for instance in genealogy e g cassens et al 2005 for gene regulatory networks e g chai et al 2014 chen and vanburen 2012 and co expression networks e g kumari et al 2012 tang et al 2011 while network construction is a widely discussed topic in different disciplines in this paper we focus on directed weighted networks which have been the subject of analyses by methods collectively known as ecological network analysis ena borrett et al 2018 lindeman 1942 patten et al 1976 scharler and fath 2009 ulanowicz 1986 figs 1 and 2 two types of directed weighted networks have been most prominent the construction of trophic ecosystem networks energy and nutrient networks and socio economic networks e g urban metabolism sectorial water use monetary exchanges fig 3 though we believe that much of the methodology may be more broadly applicable application of ena has brought new insights into ecosystem functioning and hypothesized emergent properties over the past decades for instance the understanding that overfishing decreases the mean trophic level of the fishery as large predators are overfished first pauly et al 1998 was revealed by calculating trophic levels of target fish species from the analyses of trophic networks the concept of fishing down the food web is now firmly embedded in the literature explaining the diminished resource of sought after higher trophic level predatory fish e g tuna in fisheries catches other longer established ecological theory on community motifs holt 1997 have been investigated in the big cypress preserve ecosystem florida usa and revealed beneficial relations between species by examining both direct and indirect effects ulanowicz and puccia 1990 that were not apparent from the known biology and ecology of the involved species alone bondavalli and ulanowicz 1999 as an example an intraguild predation configuration alligators and snakes feed on frogs alligators also feed on snakes revealed mutualistic relations between predators alligators and their prey frog this motif of trophic interaction is only a particular portion of their feeding interaction within the food web and alligators had further net positive indirect effects on 11 of their prey groups although direct effects of predators on their prey are negative the consequences of these deeper insights into relations between species is a changed perception of importance of this species within the context of the trophic web this has added information on the ecosystem next to other important concepts such as species diversity predation or habitat modification methods and metrics emerging from ecological networks have been applied to socio economic systems with increasing enthusiasm tang et al 2021 for instance a study on carbon emissions sequestrations and fluxes of the beijing metropolitan area revealed positive and negative direct and indirect relations between the city s components xia et al 2016 one of the main findings were that change in land use and economic sectors over 20 years highlighted that urban expansion caused a decline in mutualistic relations between metropolitan sectors overall even though ena is used widely in multiple different fields borrett et al 2018 there are as yet insufficient guidelines on how to generate input data the construction process itself and how to evaluate the quality of the network model it is essential for practitioners to recognize that the construction of network models is fundamentally a modelling step and as such the process should follow best ecological modelling practices this is reminiscent of efforts in other areas of ecological modelling e g grimm et al 2010 jakeman et al 2006 parrott 2017 schmolke et al 2010 hipsey et al 2020 which provide guidelines for model development evaluation model description validation and documentation amongst others and are applicable to decision support models individual and agent based models and more generally environmental models unfortunately the direct application of these existing guidelines to specifically network models is not always clear due to model differences thus this paper provides an overview of adapted and additional guidelines and principles to the construction of network models of ecosystems and socio economic systems there are several existing recommendations but little overarching consensus on what makes a quality ecosystem network model instead as we already know from other modelling studies the sufficiency or success of the model is dependent on research questions and hypotheses the system and data availability such ambiguity is in conspicuous contrast to more established guidelines for generating ecological datasets e g underwood 1997 at present the available guidelines are few and are a loose conglomerate of descriptions of data required how to construct the network from the data and how to generate possible network solutions of the available data ayers and scharler 2011 dame and christian 2006 fath et al 2007 heymans et al 2016 lassalle et al 2014 link 2010 van oevelen et al 2010 ulanowicz 1986 ulanowicz and scharler 2008 all of this documentation describes specific possible steps within the process of generating networks but it does not give complete guidance on multiple critical topics including how to design fieldwork to obtain data appropriate for constructing networks how to transform the data into the correct format conceptually and practically how to identify links how to deal with missing data and lastly but perhaps most importantly how to evaluate whether a network model is a sufficient or plausible representation of the system in multilayer networks multi and hypergraphs delmas et al 2018 golubski et al 2016 lin and sutherland 2013 pilosof et al 2017 increased types of interactions can be modelled within the same system a guided construction process is thus valuable for the current mainstream of network models as well as for networks expanded into different dimensions which we will see increasing in future the last publication explicitly focused on the construction of weighted ecological network models and specifically for ecosystems was published more than 10 years ago fath et al 2007 since then considerable progress has been made to reveal and solve flaws in network construction processes with many useful developments in the field this paper seeks to capture these developments and common practices to provide guidance during the network construction process and present a more unified approach it is divided into four sections that each deal with a different aspect of this process fig 4 and includes 1 network construction 2 evaluation 3 documentation and 4 utility of network models including interpretation of ena results this paper is intended as a reference and for first entry researchers and more senior students to become acquainted with concepts of ecosystem and socio economic network construction methodology 2 network construction data requirements for network construction are generally high to start the investigator has to identify the system components and represent them as compartments or nodes and then determine how the nodes interrelate with others in the system to map the network edges or links fig 4 this requires knowledge on the inputs into and outputs from each node in ecosystem networks it is thus imperative to know the intake consumption and the proportions to which the intake is divided into outputs of production e g somatic production reproduction natural mortality unassimilated consumption faeces and metabolic cost respiration odum 1971 the outputs that are useable in the system production and unassimilated consumption are furthermore divided along links to various other nodes in the system representing their consumers and detritus nodes respectively this concept is similarly applied to socio economic networks where inputs into and outputs from nodes may be virtual water carbon sequestration and emission in urban environments money or commodity trade flows ecosystems are thermodynamically open jørgensen et al 1999 and therefore receive and produce boundary flows as imports and exports in food webs gross production of primary producers is often treated as an import and respiratory losses as boundary losses boundary flows can also include migratory movements or long distance transport processes socio economic network models do not always incorporate boundary flows e g raw materials feeding into a trade network of the product the inclusion of natural resources as boundary flows could certainly be added to highlight resource dependency applicable to much of the global economic activity to move from a single network representing a temporal or spatial snapshot towards dynamic networks over time and space data requirements increase according to the extent of the temporal and spatial frame it is no surprise that historically network construction has been guided by data availability some of the first available ecological networks silver springs odum 1957 or cone springs tilly 1968 are therefore rather small with a total of five highly aggregated nodes e g multiple species and resources grouped together these early networks illustrate perfectly the ambition at the time to characterise ecosystem processes at a level beyond that of species and communities even if comprehensive datasets were not available although present day ecosystem networks are better resolved with a larger number of nodes that may reach 120 this is likely far less than the number of species present availability of suitable data for network construction remains an issue as it can influence how well the network model represents the system and has consequences for analyses outcomes e g abarca arenas and ulanowicz 2002 allesina et al 2005 baird et al 2009 gauzens et al 2013 johnson et al 2009 jordán and osváth 2009 continuous datasets in time and over spatial extents are especially rare for ecosystems but not necessarily for socio economic systems fang et al 2014 kharrazi et al 2017 zhang et al 2017 therefore few time series and spatially explicit networks have found their way into the ecological literature but see christian and thomas 2003 chrystal and scharler 2014 haraldsson et al 2018 scharler 2012 steenbeek et al 2013 de la vega et al 2018 2 1 existing network construction processes and software several network construction techniques for ecosystems became mainstream in the 1980s ecosystem and socio economic networks in the early decades were constructed mainly by arranging gathered data into spreadsheets that allowed the verification of mass balance by comparing inputs and outputs e g ulanowicz 1986 for the early small networks silver springs odum 1957 cone springs tilly 1968 this was manageable but far less practical for larger networks a plain text format scor comprised the input structure for the netwrk software facilitating the network analysis ulanowicz and kay 1991 this was later translated into an excel front end and the calculations to the gui operated software wand allesina and bondavalli 2004 used by ecologists but did not change the way networks were constructed subsequently a more objective method was developed which systematically assigns weights to interactions within the constraints of node consumption and production matlod ulanowicz and scharler 2008 building partially on this methodology an expanded version constructing stoichiometric multitrophic networks has recently been applied to semi terrestrial and marine environments scharler et al 2015 scharler and ayers 2019 a similar bioenergetics food web modelling approach ecopath was conceived in the early 1980s polovina 1984 subsequently it was developed into the software package ecopath with ecosim ewe that also enabled a network construction process christensen and pauly 1992 the construction process in ewe is facilitated by a user friendly interface where standardized and available data are entered and the software will calculate missing data through mass balance and other modelling assumptions recently the mass balance algorithms have been translated into r r core team 2018 which makes this approach further accessible through the package rpath lucey et al 2020 the direct incorporation of additional information on diet mccormack et al 2019 and diet uncertainty analysis bentley et al 2019 promise to advance realistic representations of trophic webs an alternative network construction methodology is that of linear inverse modelling lim niquil et al 1998 van oevelen et al 2010 vézina and platt 1988 here networks flows are inferred from equalities and inequalities describing stocks and flows of typically undersampled food webs the method provides either a single network solution or a range of solutions presented as an ensemble of possible networks within the range of the input data in addition stoichiometric and isotope data can be used in this approach van oevelen et al 2010 this method was recently advanced by generating multiple plausible networks to certain specifications regarding the flow ranges hines et al 2018 waspe et al 2018 all of the described approaches result in networks with weighted links that represent how much energy or material depending on model currency is transferred from one node to another in the selected model time step with the exception of the multi solution linear inverse modelling approach their common disadvantage is that the variability of data used to construct the networks is largely lost because the result is a single network per point in time or space when networks represent only a single version of a system statistical analyses are largely infeasible but see kones et al 2009 there have been few attempts to reflect this variability of input data e g ayers and scharler 2011 but hines et al 2018 and waspe et al 2018 started to address this issue systematically within the lim framework this framework already provides the possibility to produce an ensemble of plausible network models based on known data and hines et al 2018 illustrate how they can be constructed from already existing single networks for instance networks can be replicated retrospectively by applying a user defined range of flow values e g 50 of a nominal estimated value or a user can specify a range defined by empirically known data variability hines et al 2015 2018 de la vega 2018 bentley et al 2019 an ensemble of plausible networks is then generated based on one or more new flow ranges and the group of networks is returned to enar for subsequent analysis borrett and lau 2014 lau et al 2017b as each model parameter is being randomly sampled from the empirically estimated range it appears that these plausible models act like replicate samples of the system as a result network analysis results can be compared more rigorously by comparing their density distributions that result from a given uncertainty or range of flow values which lets the user draw more robust conclusions for example hines et al 2015 applied this uncertainty technique in an ena application to determine the impact of sea water intrusion on microbially mediated nitrogen removal pathways in the cape fear estuary nc usa this work was able to robustly conclude that while the sites experiencing different salinity regimes had the same n2 removal capacity the same coupled steps in the nitrogen cycle used to achieve the removal were utilised in substantially different magnitudes more generally these uncertainty analyses allow for a deeper understanding of networks in terms of their function and behaviour ma et al 2018 this constitutes a large and absolutely necessary step towards a more comprehensive representation of systems as networks waspe et al 2018 expanded the methodology of van oevelen 2010 and hines et al 2018 to preserve the entire empirically measured range of the input data during the initial network construction phase in the r package flowcar the networks generated in this way are diagnosed to be representative of the entire range of empirically measured input data they are subsequently packed into a format readable by enar for network analysis and the calculated ena metric distributions plausibly represent the range of empirical measurements another way of constructing networks and especially for time series networks is to extract snapshots from time dynamic simulations that are conducted in software such as stella isee systems econet kazanci 2007 ewe christensen et al 2005 vensim ventana systems inc and others care should be taken that all data needed for an ena are able to be extracted from networks generated this way fath et al 2007 as an example econet kazanci 2007 schramski et al 2011 uses basic input data stocks flows which are converted into differential equations the goal is to arrive at a steady state system to be used subsequently for analyses although simulated ecosystem networks have played a prominent part in the network literature e g the cascade model cohen and newman 1985 niche model williams and martinez 2000 allesina et al 2008 there are few similar approaches to construct weighted versions of networks a notable exception is the method introduced by fath 2004 on cyber ecosystem assembly then how to start the network construction process regardless of which methodology is used to construct networks the basic data needs are very similar two fundamental steps of the process are the system conceptualization referring to structure purpose and ecosystem boundaries and the subsequent data requirements and parameterisation 2 2 conceptualization 2 2 1 system boundary as with most modelling one of the first steps to constructing a network model is to conceptualize the system which starts by identifying its boundaries haefner 2005 fath et al 2007 this process begins with deciding or inferring from data what system elements are inside the system of interest and what elements fall outside the system in this process the modeller will determine 1 the spatial scale and resolution of the system and 2 the timescale it represents day season year for instance should the spatial resolution adhere to natural boundaries such as watersheds or to political boundaries should the network model include features such as the littoral zone of lakes and estuaries or the benthic environment for an open ocean ecosystem or lake sometimes the system boundary is determined by data availability which may lead one to restrict its physical or temporal dimension for instance to the pelagic or benthic realm to municipalities or certain economic sectors or a particular season or decade 2 2 2 structure of networks nodes links resolution the next step in system conceptualization is the definition of the basic structure of a network created by deciding on the network nodes and edges fig 4 for example how many nodes will be used to represent the ecosystem elements e g species functional groups non living resources and what are the interlinkages or edges among the nodes in network models used for ena a single directed edge between two nodes represents the energy or matter transfer even if it arose from multiple ecological processes again the node and edge conceptualization should be guided by the research questions and data availability and overall lead towards an appropriate representation of the system some system components represent highly important functions e g primary producers in ecosystem or water sources in virtual water networks that it would be unreasonable to dismiss them this may require efforts to close the data gap from direct e g fieldwork data banks or indirect sources e g literature expert opinion once constructed scientists should use sensitivity and uncertainty analyses to judge the model sufficiency and analytical consequences of the heterogeneity of data abundance and quality in terms of sensitivities of model output uncertainties to those of the input data increasing the model resolution resolution refers to the degree of aggregation or disaggregation by increasing the number of nodes and links is only feasible if data are available for parameterisation the resolution of networks has received considerable attention see section 1 as network structure and thus analysis outcomes are generally sensitive to the number and proportional weight of links haller bull and rovenskaya 2019 socio economic network models are usually more aggregated compared to those of ecosystems due to the more intense aggregation of data sources in economic systems the reporting of trade flows is more readily available as aggregated datasets with nodes representing sectors incorporating many different types of factories or economic activities e g transport agriculture annual trade volume this results in a higher degree of aggregation of commodity flows and therefore a higher connectivity of the network in ecology the recognized importance of taxonomic and functional biodiversity lead to a tendency to disaggregate nodes where possible overall it is essential to have all critical components of the system represented in the model even if in more aggregated nodes than to completely omit key elements the structure of network models can differ due to underlying system differences and also due to differences in research questions and system conceptualization a particular system can thus be represented by different network models this has made comparisons challenging in the past researchers have tried to standardise network structure by resolving different networks to the same number of nodes even going to the extreme of featuring nodes as placeholders for temporarily absent species e g baird et al 2011 however such practice may conceal real differences between networks for instance the absence and presence of migratory species horn et al 2019 or of seasonally active economic sectors in trophic webs direct flows between nodes are largely unidirectional and not reciprocated by return flows exchanges between highly aggregated nodes nodes to represent groupings of functionally similar species may appear bidirectional represented by two single direction edges in opposite direction for instance exchanges between living nodes and non living nutrient pools or detritus may be bidirectional in economic networks bidirectional flows between any two particular nodes are more common this observation may be a consequence of a relatively high degree of aggregation in currently available empirical economic and socio economic network models or it could reflect a higher interaction incidence in comparison to trophic webs economic networks structures are subject to and therefore a result of anthropogenic concepts some of which may differ substantially from that of ecosystems e g fang et al 2014 huang and ulanowicz 2014 xia et al 2016 for instance they lack the strong metabolic constraints of ecosystems also trading networks seldom incorporate the commodity source as opposed to ecosystem networks which feature energy or nutrient imports across the system boundary to depict their connectedness to other environments and external sources in reality this is an important feature for certain economic sectors and for those warrants consideration 2 3 basic data requirements and parameterisation node and link weights efforts put into data gathering will be reflected in the outcomes of model analyses and both small and large links deserve attention large links usually emanate from high biomass e g detritus trees or high turnover e g phytoplankton bacteria nodes and a high variability in their value will result in the same for calculated ena metrics e g ludovisi and scharler 2017 it is important to consider which weak or small weight links of a system to include as their absence or presence change the network structure and therefore can change system function and the analytic results for example the number and magnitude of weak links in networks has an influence on network metrics as they are calculated from the flow distribution within weighted directed networks incomplete system specific data availability for nodes and links can be supplemented depending on the type of missing information if only some part of the data requirements are missing they can often be estimated by mass balance equations so that inputs equal outputs of energy material trade flows wealth accumulation etc from expert opinion or information from the literature a balance of node inputs and outputs i e sum of inputs sum of outputs is required for many different types of network analyses and can be balanced by using the equations in box 1 historically model balance was achieved manually e g by assessing and comparing inputs and outputs on a spreadsheet this approach is cumbersome but has the advantage that experts can apply their system knowledge to determine the reasonableness of the necessary changes several automated procedures are available to balance all nodes simultaneously for the entire network e g allesina and bondavalli 2003 christensen et al 2005 lau et al 2017a van oevelen et al 2010 however one should be cognisant that these balancing methods tend to change all of the original flow values in the model some values can exceed what is biologically reasonable to achieve such balance and they typically disregard the underlying data certainty or quality practitioners should compare the balanced and original network model to assess the algorithmic changes and changes to the original input data some of the changes could result in ratios of stock to flow that turn out to be unreasonable for certain nodes another solution therefore is to add an in or decrease of stocks to assist in the balancing procedure ulanowicz 2004 suggested this could take the form of adding a vector each that represent the stock increase as an added input and the stock decrease as an added output box 1 more information on model evaluation is provided in the following section section 3 evaluation network data can be represented as matrices and vectors box 2 a flow matrix indicates not only the presence or absence of a flow edge from source node i to receiver node j but in weighted networks the amount transferred via a particular link per unit time is stated for clarity care should be taken to specify the flow direction as both row to column and column to row orientations are used in the literature see e g scharler and fath 2009 for certain links databases and online tools are available to identify feeding links and sometimes estimate flow values e g brey 2001 froese and pauly 2000 gray et al 2015 pasquaud et al 2007 poelen et al 2014 short of direct measurements respiration rates in ecosystems can be estimated from body size and environmental data e g brey 2001 brown et al 2004 and a popular estimate of production values are production biomass ratios from the literature whenever literature data are used data sources for the same species in the same environment e g biogeographic region type of habitat temperature are preferable information on the presence and absence of trophic links is also often gathered from the literature e g stomach contents feeding experiments isotope data and measured data include observations or the analyses of stomach content isotopes or fatty acids although literature data can give an indication of the feeding guild of a species they might not be an accurate representation of the feeding interaction of every system the species occurs in overall it is imperative to keep in mind that literature data are estimates and may therefore introduce bias into the network construction process this applies to both ecosystem and socio economic networks usually a combination of methods to parameterise the network are applied and for socio economic networks existing data recorded in open access databases for instance municipal records or trade relations for certain sectors are readily available data sources an empirical sampling design that connects directly to network construction is a preferable approach to generating data in practice however this is not always feasible and networks are constructed from measured and literature data and expert opinion to fill gaps for ecosystems important data sources are those obtained by continuous data recorders long term government datasets e g fisheries records or open access databases when data are gathered specifically for constructing networks such actions may be classified according to their resource investment these include 1 extreme investment when data are actually measured for nodes and links for which no data are available or 2 moderate investment that includes estimating some data that are missing we speak of 3 minimal investment when balance adjustments within the range of input data from which the networks were constructed are sufficient a recently developed methodology assists in the decision making for diverting resources to measure certain links over others depending on their importance kazanci et al 2020 this is a useful guide applicable especially in resource constrained or data constrained environments for network science to move from largely descriptive studies to hypothesis driven research adequate initial data gathering that enables researchers to pose and answer hypotheses is a requirement and thus combined efforts by data scientists and empirical scientists are essential e g delmas et al 2018 a step by step summary of network construction guidelines is provided in box 3 3 evaluation evaluation of constructed network models has historically been a neglected topic this is in part because the most commonly used model verification and validation methods in ecological modelling compare the temporal output of dynamic models to observed field data but these methods do not apply to static models like most of the ena network models this is further complicated by a lack of data for validation especially for early networks and lack of evaluation on how network analyses outcomes are affected by the network construction process today the desire to use network models and ena in system management and policy making dame and christian 2008 fath et al 2019 goerner et al 2009 heymans et al 2016 kharrazi et al 2013 safi et al 2019 xia et al 2016 makes model evaluation essential the evaluation should be conducted on both the input data and the network analyses outcomes and can be treated as input data and analyses output centered validations such evaluations have been described for various software and different modelling frameworks e g bennett et al 2013 costanza et al 1992 grimm et al 2010 heymans et al 2016 jakeman et al 2006 schmolke et al 2010 here we add points that are specific to system network models 3 1 input data centered the field or literature data used for network construction should always be evaluated for their fit for the model purpose and especially when the goal is management or policy relevant research costanza et al 1992 when considering the relevance of the data for node and flow values of the network modelers should keep in mind that the input data themselves may not be a good representation of the system e g bennett et al 2013 due to inadequately describing the system when data are used from studies not designed for this purpose for instance literature metabolic ratios for fauna specific to certain latitudes are less likely to be representative for those occurring at different latitudes or between vastly different phyla or feeding guilds e g detritivores and carnivores should it not be possible to construct replicate plausible network models of a system to reflect the system variability itself researchers can conduct extensive sensitivity and uncertainty analyses to judge the network model quality and the dependency of the ena results on less well known inputs the final network model parameterisation can be compared to the original input data using various descriptive and statistical methods to evaluate differences of single node and flow values an automated assessment of the amount of system specific data used for network construction is incorporated in the software ecopath christensen et al 2005 through a measure called pedigree that represents the proportion of system specific input data in the network model even though the amount of system specific data used for network construction is not a full guarantee for a good quality network it serves as a broad gauge of how well the network model could represent the system also incorporated into the ewe software is a data check before mass balancing for the entire network called prebal link 2010 with this tool certain attributes across nodes and trophic levels can be checked for their correspondence with known general ecosystem attributes at all times and notwithstanding the software used an assessment of correspondence between balanced networks with original input data of node and flow data and data variability should be conducted 3 2 analyses outcome centered once ena has been applied to the network the network analyses outputs the analytic results need evaluation for example practitioners should consider if the results show artefacts from an ill defined network because the network construction process is influenced by data availability it may become apparent only after analyses that the degree of node aggregation prevented detection of a system feature of interest in this case the network topology may have to be re evaluated for its fitness for purpose such considerations are important also when comparing networks while it is important to maintain the same model assumptions and approach e g node aggregation decisions real differences between the systems should not be masked by trying at all costs to keep the structure the same here again multiple plausible networks representing the system are useful and a knowledge of the system by experts in different fields may provide deeper insights into the accuracy of results how convincingly results can be communicated to people outside the field e g stakeholders is another desirable check it requires a deep understanding of the intricacies of the network and how its quantitative structure resulted in certain metric values a second powerful approach is to validate selected network analysis results with an independent method for example deehr et al 2014 validated their fisheries models of core sound north carolina by comparing the ena predicted node trophic levels calculated by ecopath with an independent estimate of the node trophic levels from isotope analysis conducting internal checks by using different types of analyses on the same dataset to calculate nutrient limitations of nodes in several networks was applied to a mangrove ecosystem by scharler et al 2015 strong agreement between the independent methods was used as convincing evidence of the trophic models quality we suggest the guidelines listed in box 4 to evaluate the representativeness of constructed networks 4 model reporting documentation scientific reporting aspires to be replicable transparent and accessible to achieve this scientists working with network analysis must 1 document the network construction itself including adjustments to primary data and evaluations of input data and analyses outcomes and 2 consider the publication and accessibility of the network models scientific writing conventions require methodologies to be described in enough detail to be replicated by other investigators this is true for modelling studies as much as it is for experimental work the documentation of modelling decisions the use of large amounts of data and clarifications on any necessary data transformations and calculations are often voluminous but necessary it is difficult to identify a single common documentation format because networks may have very different structures data sources and purposes however this does not preclude the documentation of network construction and evaluation to be presented in detail and to standardise documentation where possible e g ayers and scharler 2011 bonet et al 2014 grüss et al 2017 gurney et al 2014 hoch et al 1998 schmolke et al 2010 articles that present new network models must include a full description of the network construction to enable peer review and evaluation beyond individual publications there are several collections of previously published networks that are available for additional research these network data can be stored in different formats for instance ulanowicz maintained a collection in a data format referred to as scor formatting ulanowicz and kay 1991 a collection of over 100 ecosystem networks which partially overlap with ulanowicz s set are distributed with enar in the r network data format borrett and lau 2014 lau et al 2017a https github com seelab enar ecopath with ecosim networks are stored in ecobase http sirs agrocampus ouest fr ecobase allowing for submission of new networks and extraction of existing ones another more recent development is that of mangle a development and storage for binary food webs in r poisot et al 2016 such repositories are useful for cross system research network construction and common storage format however the network model information may differ within and between databases according to purpose a list of commonly used network construction methodologies model databases and software are provided in box 6 in addition to evaluation of the science clear documentation of the network construction process may ultimately benefit the research field to enable new participants and the emergence of stronger community standards this is especially important for addressing common data challenges including that of missing data applying conversion factors and how to distinguish between plausible and possible network models above all documenting the methods used during the entire network construction process adds credibility to the process and final network structure and increases the potential applications of networks and their analyses in management and policy making costanza et al 1992 we recommend that clear documentation of the network construction and evaluation process should be included in ena publications as part of a best practice it will allow the approach to become more rigorous and the results to be better interpretable the communication of the variability and uncertainty of the analysis outcomes plays an important role in providing realistic recommendations to stakeholders and facilitate application saltelli et al 2020 box 5 provides basic guidelines we can provide for information on the accuracy validation variability and uncertainty of the network construction phase 5 networks as response variables how scientists construct evaluate and document network models is critical to create a successful ecological network science once networks are constructed and deemed sufficiently representative of the system researchers can use them as experimental response variables providing information at system components and whole ecosystem levels see memmott 2009 network metrics describe various features of the system in the ena framework these metrics are frequently labelled as either structural e g the number of nodes edge density or functional e g total system throughflow finn cycling index nevertheless the functional metrics often incorporate both structure and function e g bersier et al 2002 delmas et al 2018 kazanci and ma 2015 ulanowicz 1986 weighted networks are used to calculate the latter because a considerable amount of information is inherent in the distribution of link weights within networks e g allesina et al 2009 bersier et al 2002 comparing two or more models of a system under different conditions is one way network models can function as response variables in this application analyses are focused on the whole system response to the conditional change for example deehr et al 2014 compared trophic network models parameterized with data from sites where shrimping was allowed and sites where shrimping was excluded to identify the ecosystem impact of the fishing activity de la vega et al 2018 compared network models of the sylt romo bight ecosystem parameterized with data from different seasons by applying the uncertainty analyses to generate multiple plausible model given the data uncertainty the authors discovered that some whole system network metrics such as flow diversity and effective link density varied seasonally as expected while other indices such as the average mutual information showed no significant seasonal variations multiple recent applications of the uncertainty analysis hines et al 2015 de la vega et al 2018 bentley et al 2019 have directly compared the distribution of analytical results statistical difference was inferred if the 95 confidence intervals did not overlap this is a useful but conservative approach in some cases it might be possible to use a nonparametric statistical approach to compare the distributions we suggest a nonparametric test because the observed distributions of network metrics from flows of multiple plausible networks in general often do not follow a simple distribution pattern whereas for decades theory has far outrun applications in the network analysis field many efforts have arisen in recent years to use ecosystem and socio economic networks in management and to investigate how they could shape policy making fath et al 2019 heleno et al 2014 pincetl et al 2012 safi et al 2019 zhang et al 2013 longo et al 2015 parallel to this is an increased effort to understand how input data and ena output relate and therefore how network analysis metrics can be meaningfully applied borrett and osidele 2007 christian et al 2009 kaufman and borrett 2010 ludovisi and scharler 2017 although the ena metrics generally have thorough theoretical underpinnings one challenge to their use in management is that the correspondence between any metric value and desired system states e g healthy sustainable resilient is not always immediately apparent a key challenge for using ena metrics for ecosystem and economic system management is to determine which ena metric values are most sensitive to certain system states fath et al 2019 few ena metrics are well benchmarked and interpretations largely remain relative among networks this relative comparison however is a powerful tool for example it can be used to track system function over time christian and thomas 2003 schückel et al 2015 luong et al 2014 and compare differences among systems or subsystems baird et al 2011 pezy et al 2017 scharler and baird 2005 such relative indicators may overall be a better guide for change than metrics benchmarked against absolute values because they track trajectories of individual systems one example of metric benchmarking has been applied to the metric termed system s robustness ulanowicz 2009b that considers the information inherent in the weighted flow structure of networks this has been adopted by ecologists and economists alike e g goerner et al 2009 kharrazi et al 2017 mukherjee et al 2015 scharler et al 2018 it has been proposed as a way of identifying the optimal functioning state as a dynamic tension between system needs both for efficiency and resilience the latter of which is expressed as redundant or parallel transfers within the network ecosystem networks constructed from empirical data have been shown to congregate at a point that is indicative of high robustness signifying an advantage for ecosystems with a considerable proportion of redundant flows in addition to favouring efficiency ulanowicz 2009b economic networks seem to feature many more redundant pathways and thus have their highest robustness at different proportions of flow redundancy and efficiency compared to ecosystem networks this however may be an artefact as a result of high node aggregation the interpretation of the network metrics can depend on model characteristics as well as management goals this further implies that scientists and managers applying ena need to carefully consider their results and cannot always depend on previous ecological interpretations of the metrics context dependency of a desired state is an important consideration when using ena metrics many of the metrics characterise the state of the system in a driver pressure state impact response dpsir frame burkhard and mueller 2008 lewison et al 2016 and do not characterise the desirability of the state for example an increase in the node or total system throughflow might be desirable in marine fisheries models tracing carbon or energy deehr et al 2014 but it would be undesirable in models tracing toxins in the food web taffi et al 2015 6 conclusions the overarching goal of this paper is to improve the development of weighted network models of ecosystems and socio economic systems the application of ecological network analysis and its use for system management by considering guidelines for best practices specifically we considered key elements of network construction model evaluation documentation and scientific applications we highlighted challenges in the network construction process and identified points that might result in unrealistic or not for purpose networks more broadly it is useful to recognize that network construction is a specific form of modelling and that it should thus adhere to the best practices prescribed for general modelling activities including robust forms of model evaluation e g verification validation sensitivity and uncertainty analysis the construction process is time consuming and yet critical as a strong foundation for credible network analysis results and for the field to move into more rigorous hypothesis testing and policy and decision making realms as we seek greater management applications of ecological network analysis the critical gap between theoretically oriented network analysis and its application becomes more apparent this highlights a key direction for future research and development a closer collaboration between theorists and empirical scientists can alleviate some of the remaining challenges especially for checking the quality of constructed networks filling crucial data gaps and the interpretation of calculated metrics against a backdrop of empirical knowledge of the system overall adhering to basic scientific standards of assessing data quality reporting methodology hypothesis testing and more thorough interpretation of metrics will already make the field more appealing to stakeholders standardized protocols provide for a certain efficiency and accessibility the network model databases mentioned are highly useful to access a large number of networks especially for assessments across different systems in the absence of a single agreed upon standard format for model presentation it is useful to create interchangeable formats in various databases that allow access by different users for different types of analyses in different software besides such technical issues the most critical challenges at present are the data verification of constructed networks and the interpretation of metrics in ways that increase our understanding of system function for management actions and eventually for policy making declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was partially funded by an nrf knowledge interchange and collaboration fund kic160211157827 award to u s 
25832,assessing and modelling the coastal plume dispersion of nuclearized rivers is strategic in case of accidental releases but taking into account the variation of main hydrodynamic forcings is challenging this study uses fuzzy c mean clustering of a 10 years series of discharge and wind speed at the rhone river estuary france in order to explain the variability of its plume the method allows to classify the data into 6 scenarios of hydrodynamic forcings that were related to different spatial extensions of the plume as well as to surface currents measured in situ these scenarios were used to simulate the extension and dilution of a radioactive release issued from the river based on threshold values of the forcings a decisional tree is proposed to provide a quick decision tool identifying in real time which climatological scenario occurs at the river mouth and the potential plume pattern keywords coastal plume fuzzy c mean clustering accidental release scenarios rhone river radioprotection coastal management 1 introduction the rhone river catchment extends over 98 000 km2 and covers one fifth of the french metropolitan territory it is the main source of particles and freshwater for the gulf of lion in the north western mediterranean sea durrieu de madron et al 2000 and all together one of the most important input to the mediterranean sea ludwig et al 2009 the rhone valley also hosts the largest concentration of nuclear power plants in europe with 4 nuclear power plants in process and a spent fuel reprocessing center under dismantlement since 1997 eyrolle et al 2020 recently synthesized the studies showing that this river carries artificial radionuclides from decades resulting from authorized releases of low level radioactive liquid wastes and from the export of atmospheric deposits on watersheds consequently to nuclear weapons testing and chernobyl accident france is presently ranked second in the world for the production of nuclear energy and the total electricity production in the combined regions of northern western and southern europe is projected to increase by 2050 iaea 2019 also the risk of incident on any kind of nuclear installations is still of concern in france and must be taken into account as for any river the transport of artificial radionuclides in case of accidental release occurs both in dissolved and particulate form depending on the amount of suspended particulate matter and on the chemical properties of the radionuclides and particularly their distribution coefficient tomczak et al 2019 for the rhone river the prediction of dissolved vs particulate fluxes and the associated time scale for transit can be evaluated through numerical modeling launay et al 2019 but the behavior of radionuclides once at sea is clearly less constrained because it will primarily depend on the forcings governing the shape of the rhone river plume the area of the rhone river mouth is characterized by a very small tidal amplitude about 30 cm inducing the formation of a sedimentary delta as usual in this case the freshwater input forms a thin stratified plume of low salinity water and higher turbidity overlying the seawater and extending between 4 and 1000 km2 estournel et al 1997 gangloff et al 2017 with a thickness decreasing seaward pairaud et al 2011 gangloff et al 2017 it is preferentially deflected westward in a clockwise orientation running east to west reffray et al 2004 due to the general circulation induced by the northern current along the continental slope under north northwest winds the plume extends offshore towards the southwest whereas it is pushed to the coast west of the river inlet in case of southeastern winds satellite and modelling results have also shown that the plume size increases with river discharge fraysse et al 2014 gangloff et al 2017 more episodic processes impact the plume pattern such as dense water formation and cascading ulses et al 2008 upwelling cells and marine storms millot 1990 1999 as a result this plume extends far beyond the coastal areas and may covers a large area in the gol extending from the vicinity of the mouth up to the cap de creus at the french spanish border sanchez cabeza et al 1992 it can also reach the gulf of fos gontier et al 1992 charmasson et al 1999 or the bay of marseille pairaud et al 2011 fraysse et al 2014 on the eastern side of river mouth the gulf of fos is an important economic area with one of the biggest commercial port in europe and a large shellfish area and marseille is one of the biggest mediterranean coastal city with one million inhabitants due to the oligotrophic nature of the mediterranean sea the region of freshwater influence rofi of the rhone river has a major influence on the distribution of plankton groups diaz et al 2019 and thus pelagic catches on the gol obviously the inputs of chemical contaminants from the rhone river can greatly affect the fishery activity the combination of meteorological and hydrodynamic forcings with the dynamics of the rhone river discharge results in a large spatio temporal variability in freshwater and associated pollutants delivery to the gol martin et al 2019 if an accidental release occurs in the rhone river the dissolved radionuclides may reach the estuary within 48 h h to few days depending on the source location and water discharge unpublished results once at sea the different shapes that the plume may present will depend on hydrodynamic and weather conditions and will lead to contaminate different areas since one goal of radioprotection is to predict the transfer of radionuclides in the environment there is a need to anticipate their dispersion at any time and in any kind of meteorological and hydrodynamical conditions different numerical hydrodynamic models have been set up in the gol including the river mouth pairaud et al 2011 duffa et al 2016 and they could be used actually in case of accidental release in order to predict the behavior of the freshwater input however the delay necessary for their implementation will range from few hours to few days whereas very quick and concise information should be provided to experts and decision makers as a first picture of the local issues alsothe potentially impacted zones will be better defined by performing a fine spatial scale simulation adequately centered compared to a large scale simulation as a result a preliminary study embraces all possible plume patterns is necessary and a first step is to target the general behavior of the estuarine plume system bárcena et al 2015 explained that two approaches may be conducted for that simulating several scenarios using constant conditions of hydrodynamic forcings or simulating few scenarios using the most frequent or extreme real hydrodynamic forcings during short medium term periods month to year these authors demonstrated that the first approach is not complete because real forcings cannot be deduced from the combination of simple idealized scenarios the second approach relies on a subjective selection of scenarios by an expert and it will have an expensive computational cost for simulations if the need is to get on overview of the different kind of realistic responses of the estuarine plume mean behavior in this case and to minimize subjectivity a methodology based on data mining should be able to select the most relevant condensed hydrodynamic scenarios taking into account the time evolution and the occurrence probability of the forcings plume classifications based on satellite observations or hydrodynamic model output have been defined in several river sea systems using empirical orthogonal function or self organizing map falcieri et al 2014 xu et al 2019 such classification method deals with large spatial scale but implies a heavy data pretreatment like masking to treat the satellite data or for the computation of the model in addition the need of long term environmental databases e g 10 20 years to assess probabilities implies significant computational costs as well as long and multiple series of data to be used as boundary conditions and climatic forcings another approach is to classify the main hydrodynamics drivers by looking for example at the catchment discharge and the winds intensities and directions kaufmann and whiteman 1999 zhang et al 2011 since the plume response to these forcings can be longer than 24h demarcq and wald 1984 estournel et al 1997 the classification should work observation by observation but must also keep consistency over longer temporal scales of few days in order to be accurate clustering performed on temporal series helps to assess the consistency of a trend over time and a fuzzy clustering algorithm provides a continuous cluster membership function allowing to spot significant trend changes in this context this paper presents a methodology based on statistical analysis and numerical modelling that was developed to address the limitations of the previously mentioned approaches firstly we used a fuzzy c mean algorithm to identify and classify combinations of winds and discharge at the mouth of the rhone river in order to define model scenarios of realistic forcings secondly the consequences for sea surface currents will be assessed and the resulting plume pattern will be modelled for each scenario as well as the distribution of dissolved radionuclides due to a hypothetical and episodic release on the rhone river these plumes scenario can be used as a support for operational tools improvement and decision 2 material and methods 2 1 field study and data the rhone river hourly discharges have been provided by the c n r compagnie nationale du rhône thanks to the rhone sediment observatory osr program they were measured at the sora station in the city of arles located 47 km upstream of rhone river mouth fig 1 it must be noted that the rhone river splits in two branches upstream of this station the grand rhone and petit rhone the station reports the discharge for the grand rhone river only which represents about 90 of the total rhone river discharge boudet et al 2017 in our case we focus only on the river plume at the grand rhone outlet weather data and subsurface marine currents data are issued from the mesurho station pairaud et al 2016 operational since june 2009 and located at the buoy float immersed bfi maritime buoyage roustan east 43 19 2 n 4 52 e on the rhone prodelta 20 m water depth it is about 1 mile southeast of the mouth and was configured to collect physico chemical data in near real time and at high frequency about 30 min in the fresh marine waters transition zone it is equipped with a weather station at 10 m height and an acoustic current doppler profiler adcp the instrumentation is connected by a cable to a controller located above the sea surface and powered by solar panels the measurements are transmitted to the coriolis data center via gprs about 1 transmission every 12h since 2015 4h before weather variables used are the average wind speed over 30 min and the gust wind speed gust wind speed is the maximal mean wind speed over 0 5 s observed during a period of 30 min the observations used were registered between 2009 and 2019 and result in a total of 128 262 data the subsurface currents maximum depth of 1 5 m from 2010 to 2019 were also used when available and after quality control validation leading to a total of 31 826 observations in order to perform multivariate analysis and regression wind and currents variables expressed in terms of velocities u and directions θ are described by an eastward and a northward component x and y and calculated as follow 1 x u c o s θ y u s i n θ the corresponding hourly discharges in arles 64 131 obs were shifted with a 24 h delay which corresponds to the transit time between arles and the river mouth for a mean liquid discharge 2 2 principal component analysis principal component analysis pca has been widely used in environmental sciences including hydrologic and hydrodynamics e g hannah et al 2000 pairaud et al 2008 the common goal to all principal component methods is to describe a data set x with i individuals or observations and w variables using a small number p w of uncorrelated variables while retaining as much information variance as possible the reduction is achieved by transforming the data into a new set of continuous variables named the principal components the reduction of dimensionality provides a framework to visualize data which is especially important for large datasets husson et al 2010 this facilitates the analyses based on geometrical criteria such as separate observations into k distinct sub groups clustering or determination of extreme points renner 1993 napoleon and pavalakodi 2011 using pca as a pre processing tool in order to cluster presents two additional advantages the reduction of dimensionality speeds up the convergence of classification algorithms which usually depends on the square of p and i ben dor et al 2004 and it reduces the noise the essential of the information being on the first components whereas the noise is on the lasting ones husson et al 2010 pca has been performed using the r package factominer lê et al 2008 2 3 fuzzy c mean algorithm clustering is a usual method for data mining when it comes to identify groups and classify individuals but many algorithms exist and present different results and convergence speeds jain et al 1999 the first goal is to find an algorithm based on geometrical criteria as simple as possible for a more realistic interpretability and the second one is to find a fast convergence algorithm in order to treat the important dataset the most usual method is the c mean or k mean macqueen 1965 yadav and sharma 2013 and its fuzzy alternative bezdek 1981 fu lai and tong 1994 c means are iterative algorithms that classify individuals of a dataset into c groups the algorithm allows to randomly define c centroids in the same coordinate systems as the individuals each individual x total of k is then assigned to the closest centroid center ci the barycenter of each subgroup is then calculated and becomes the new centroid again individuals are reassigned to the closest centroid this iterative procedure minimizes the objective function j and the procedure ends when j reaches an inferior threshold in eq 2 2 j i 1 c x k c i x k c i 2 this method is defined as crisp which means that each observation is set to belong to its closest centroid cluster consequences are that observations with different distances from the nearest cluster are classified into this cluster without degree of uncertainty and the ambiguity of the data is eliminated cluster boundaries are usually not sharp in environmental sciences zadeh et al 1965 especially when ambiguous data exist and membership degrees are more realistic than crisp assignments klawonn and höppner 2003 a priori we do not expect a crisp classification and it is important to have feedback on the confidence of classification for each individual as a result the ambiguity of the data can be preserved and his probability can be used later for post treatments kim et al 2011 the fuzzy alternative introduces two new parameters the first one is the membership coefficient μik the coefficient of the kth observation to the ith cluster this membership represents how closely the kth data object xk is located from the ith cluster center it varies from 0 to 1 depending on the distance x k c i 2 and a higher membership coefficient indicates stronger association between the kth data object to the ith cluster 3 μ i k j 1 c x k c i 2 x k c j 2 2 m 1 1 the second parameter m is the fuzziness coefficient it is greater than 1 and usually dependent on the dataset structure because it represents the degree of overlap of the clusters klawonn and höppner 2003 if we set m to a smaller value more less weight is given to the objects that are located closer to farther from a cluster center as m is close to 1 μik converges to 0 for the objects that are far from a cluster center or 1 for those close to a cluster center which implies less fuzziness i e clearer cut the symbol denotes any vector norm that represents the distance between the data object and the cluster center here we use the 2 norm euclidean norm which is widely used in the fcm the new c mean function to minimize becomes 4 j i 1 c k 1 k μ i k m x k c i 2 the robustness brought by the fuzzy approach over the crisp classification is a significant improvement in term of efficiency and convergence because each individual observations has a probability to belong to each center centers are adjusted faster and the algorithm converges faster fu lai and tong 1994 ferraro and giordani 2015 also without any prior information on the cluster structure sphericity of clusters possible overlap the fuzzy c mean provides better results than its crisp counterpart selim and kamel 1992 as a result hydrologic and climatologic combinations can be identified by fuzzy cmean kim et al 2011 zhang et al 2011 bárcena et al 2015 in this study the fuzzy cmean algorithm is performed using the e1071 package from r software meyer et al 2019 2 4 choice of the number of clusters c and the coefficient of fuzziness m fuzzy c mean algorithm needs to be initialized with the number of clusters c and the coefficient of fuzziness m the best combination of these parameters is not determined by the algorithm one approach is to run different simulations with different c m pairs and to check the efficiency of clustering with a quality criteria ramze rezaee et al 1998 setnes and babuška 1999 many criteria and their efficiency are available in wang and zhang 2007 and liu et al 2010 some have fast calculation like partition coefficient pc or partition entropy pe but they monotonously decrease with the number of clusters and the lack of direct connection to the geometry of the dataset others are more complete but computationally expensive such as the dunn index dunn 1974 or the fuzzy silhouette campello and hruschka 2006 and they could not be calculated with this dataset the xie and beni index xie and beni 1991 could be calculated based on eq 5 xb has a direct connection to the geometrical property of dataset because it takes into account both compacity and separation of the clusters it deals correctly with noisy datasets size or density variations liu et al 2010 5 x b i 1 c k 1 k μ i k m x k c i 2 k m i n i j c j c i 2 calculation of xb is also fast for our dataset from 4 to 10 s depending on c m pair c could be any integer number between 2 and 358 the last one being theorically the square root of the dataset length chaimontree et al 2010 the fuzzifier m can be in theory any real number between 1 and in our case the interval of c m simulations has been restrained based on the following depending on the river sea system involved a different number of plume patterns exist in literature we found that a river plume can present up to 8 patterns xu et al 2019 as a result we do not expect our number of cluster to exceed 8 and the number of clusters c was set between 2 and 8 previous studies report that values of m can range from 1 to 4 most of them use m ε 1 5 2 5 and as result m is usually set to 2 by default klawonn and höppner 2003 overall m is lower for large datasets klawonn and höppner 2003 and the lower limit will be fix in our case to 1 using the empirical threshold equation based on the length and dimensions of the dataset proposed by schwämmle and jensen 2010 we found that the superior threshold value of m for our dataset is around 2 5 by safety this threshold value is increased by 0 25 as a result parameter m will be tested in the interval 1 2 75 and c in the interval 2 8 3 results and discussion 3 1 principal component analysis pca successfully reduced the five original variables wind speed toward north and east gust wind speed toward north and east and rhone discharge into three components and gave a summed variance of 96 4 fig 1 supplementary material this is not surprising since the gust wind speed and mean wind speed are correlated due to same direction fig 2 supplementary material the first axis contains 51 of variability with the information on wind direction the second axis with 25 of variability contains the information on wind speed and the last one 20 variability corresponds to the rhone discharge the elbow criteria the kaiser rule and the interpretation of the components confirm without ambiguity these three components fig 1 supplementary material the lasting 3 6 carried by the two remaining components concern really specific and scarce interactions like the anticorrelation between mean wind speed and gust wind speed as a result 80 and 20 of the variability are due to the variations of winds and liquid discharge respectively 3 2 clustering results and performances fuzzy c mean clustering was performed on the 128 262 observations and the three main dimensions resulting from pca a summary on classification performances based on xb index is shown fig 2 all configurations performs reasonably well except the one with 3 clusters an interesting result is that the 2 clusters configuration performed reasonably well which confirms that the plume dynamics can be described as a first approach by considering only the wind direction that is south east against north west winds this is in agreement with the 50 of variability held by the wind direction discussed hereunder however the configuration selected is the one giving the better result for xb with 6 clusters optimized at m 2 45 xb 0 19 3 3 characterization of the scenarios a cluster gathers observations having close values for one or more variables these properties on variables are specific to each cluster and are then interpreted hereunder as a scenario in order to interpret the clustering and to characterize the resulting scenarios we present the distribution of winds and discharges in figs 3 and 4 whereas fig 5 shows the percentage of occurrence of these scenarios for each month the discharge distribution in each cluster was significantly different from the global distribution of rhone discharge in arles based on the kolmogorov smirnov test see fig 4 in the description below a flood event for the rhone river refers to a discharge above a threshold set at 3900 m3 s in arles boudet et al 2017 a storm criteria is usually the significant wave height but this parameter showed too many breaks in the time series transmitted in near real time by the buoy over the 2009 2019 period also we defined sea storm here by using as a threshold the quantile 98 of our offshore gust wind speed dataset 50 3 of total dataset which is 27 8 m s 100 km h klawa and ulbrich 2003 cluster 1 gathers south east winds paragon 126 with 9 2 m s mean wind speed and 16 m s mean gust it contains 86 of all observed sea storm events with the highest intensity and 19 of the flood events the distribution of the hourly water discharge does not characterize this cluster observations belonging to this cluster have less than 6 occurrence in july september rising up to 23 in october and november this cluster can be interpreted as moderate to high waves scenario resulting from fresh breeze to violent storm south east marine winds observations in cluster 2 are winds with velocities around 4 5 m s and 9 2 m s gust fully coming from the south 171 it contains 12 of all observed sea storm events it gathers discharges values under 2500 m3 s with a median at 960 m3 s the observations mainly occur in august september with 23 occurrence this cluster can be interpreted as a rhone river low flow scenario mainly associated with south east marine breeze or sometimes a sirocco wind coming from the south reiter 1975 wind observations in cluster 3 show an important variability and are superimposed with clusters 4 and 5 most representative winds present a mean speed of 8 5 m s and gust speed of 13 1 m s the rhone discharge distribution for cluster 3 is very different from the reference distribution highest kolmogorov s d statistic it gathers discharges higher than 2000 m3 s and contains most of the flood events 79 of them this is also the cluster showing the highest contrast in seasonality with an occurrence up to 33 from november to february decreasing to 0 2 during the july october period cluster 3 can be interpreted as the high river flow scenario with a combination of different winds coming from the north west in cluster 4 observations are usually winds with 7 2 m s mean wind speed and 9 1 m s mean gust coming from the west 272 it contains 2 of all observed sea storm events discharges are below 2500 m3 s with a median around 1070 m3 s these observations mainly occur in july august september with 33 of occurrence a specific point is that their occurrence increases during the afternoon with a peak around 1h am fig 3 supplementary material interpretation of this cluster is a rhone river low flow scenario gathering moderate sea breeze coming from the south west cros et al 2004 with sometimes a strong onshore gale from west cluster 5 corresponds to winds with 12 3 m s average speed and 18 6 m s gust coming from a restricted area in the north 325 the corresponding water discharge distribution is on the lower part of the global distribution median of 1140 m3 s and discharges are always below 3000 m3 s the monthly occurrence is stable 15 with a peak in february at 25 the strong average wind intensities and gust speeds highest at 340 combined with the restricted wind direction parallel to the rhone valley stand for the characteristics of the mistral wind reiter 1975 as a result cluster 5 can be interpreted as a mistral wind scenario dry and strong breeze to strong gale associated with low to moderate discharges cluster 6 usually gathers winds with 5 7 m s average speed and gusts of 12 5 m s coming from the north east 11 however winds coming from 340 to 360 north north west are also observed the related hourly water discharges distribution is in the lower part of the global one median of 1230 m3 s and few discharges higher than 3000 m3 s are observed occurrence of cluster 6 observations is very stable all along the year ranging between 14 and 18 this cluster presents the largest gap between the wind speed average and the gust wind speed and shows an increasing occurrence in the early morning the highest gusts reach 50 m s and occur episodically in winter with an origin from 10 to 60 north east these are the strongest gusts observed among all scenarios we interpret cluster 6 as a scenario gathering land breeze or valley flow during summer and winds channeled by pre alps moutains cros et al 2004 duine et al 2017 which become stronger in winter orsure according reiter 1975 3 4 consequences for surface currents six clear wind discharge patterns have been identified but did they correspond or induce different hydrodynamics responses of the surface currents in the vicinity of the rhone river mouth consequences for subsurface currents observations issued from the adcp on the mesurho station are investigated through a least squares multiple regression observation membership to clusters ci are the explanatory variables and currents in eastward and northward directions are the response variables estimators xi and yi are then used to calculate current orientation θ rad and speed u m s observed on each cluster with equation 1 confidence intervals are calculated with the robust white standard errors with lmtest package white 1980 hothorn et al 2019 to avoid heteroskedasticity and underestimation of confidence intervals the main current direction for each cluster obtained by least squares regression on memberships is presented on fig 6 right along with the global current rose left currents oriented at 280 correspond to scenario 1 and are in agreement with the more general modelling and satellite observations during similar south easterlies wind conditions showing the plume tackled to the camargue coast marsaleix et al 1998 gangloff et al 2017 scenario 2 presents small currents not related to the wind direction in this case winds are probably too low and currents are driven by the general circulation which has a current speed similar to those of this scenario 10 cm s in scenario 3 the current direction correspond to those at the rhone river mouth meaning that during high water events discharges superior to 2500 m3 s the river influence becomes significant scenario 4 5 and 6 seem to follow the surface ekman transport with a deflection to the left relative to the wind direction scenario 5 is the one presenting the largest interval of confidence despite having the straightest wind distribution a closer look at the data shows that in this scenario the currents deeper than 1 2 m present an important heterogeneity in their direction however for wind average speeds superior to 15 m s and gust wind speeds over 25 m s this heterogeneity does no longer exist and all currents are oriented in a 150 direction for comparison scenario 5 paragon is an average wind speed of 12 3 m s and gust wind speed of 18 6 m s values which areinferior to the two thresholds and may explain the currents discrepancies associated with this scenario to conclude each scenario has its own current direction and intensity statistically different and significant 3 5 application the main objective of this work is to define the general trends of dispersal in the gol that can be expected in the case of artificial radionuclides release within the river since releases may occur at any time in a year we modelled the dispersion of a radioactive plume in the gol for each of the previous hydrodynamic scenario in order to get an overview of the potential impacts whatever the hydrodynamic and climatological conditions the simulation code used at irsn for the marine area is sterne simulation du transport et du transfert d eléments radioactifs dans l environnement marin or simulation of radionuclide transport and transfer in marine environments it was designed to assess the radiological impact of accidental releases affecting the marine environment eulerian radionuclide dispersion is calculated using a tracer advection diffusion equation more details on the code can be found in duffa et al 2016 we use the 2010 hydrodynamic outputs provided by ifremer with its mars3d model implemented on the north western mediterranean sea nicolle et al 2009 the simulations assumed a release of 1 tbq of 137cs dissolved activity in the river over a temporal window of 48 h 137cs was chosen because this radionuclide is released at each nuclear accident and is also found in authorized releases from nuclear powerplants this radionuclide presents a high radiotoxicity garnier laplace et al 2011 and is relatively soluble in seawater with a kd ranging from 450 to 2000 l kg delaval et al 2020 for comparison the estimated average direct discharge of 137cs to the ocean during the fukushima daiichi nuclear power plants was around 5000 tbq buesseler et al 2017 the simulations were done for each hydro meteorological scenario defined on the basis of the 2009 2019 dataset and the plume extension in the gol was modelled for each scenario since the hydrodynamic inputs are only available for the year 2010 we selected in this input the most representative temporal window for each scenario simulation by integrating observations memberships over a sliding window of 48h the temporal window presenting the highest summed membership values were selected for each cluster one exception was done for the sixth scenario for which the temporal window with the second highest membership was chosen due to the non homogeneity between wind observations at the buoy and wind restitution in the hydrodynamic model implications for this are discussed further the results of the 6 simulations are very different in terms of plume shape and thus affected areas fig 7 mean winds and discharges conditions over 48 h h are indicated in the figure it must be noted that these values are specific to the chosen temporal windows and thus can be different from the parangons presented in the previous chapter in scenario 1 strong marine wind with moderate discharge conditions the plume is constrained to the coast and extends west in agreement with the currents at the buoy this scenario has already been highlighted by demarcq wald 1984 and many et al 2018 or modelled by estournel et al 1997 a part of the activity remains blocked in the estuary due to winds in opposition with its flowing path and an increase in sea level at the mouth limiting the power of the jet the lowest expansion of the plume is observed with scenario 2 weak wind with discharge slightly under the annual mean the plume has the lowest surface spatial expansion among the 6 plumes and is nearly stagnant and remains with a high activity this is in agreement with the currents observed at the buoy showing really low speeds this scenario appears mostly during summer fig 4 and satellite images confirmed that the turbid plume present effectively its smallest area at this period gangloff et al 2017 scenario 3 corresponds to a high rhone river discharge and northwest winds conditions the plume presents a large area but the northwesterly wind is powerful enough to carry the plume offshore for different wind stress simulations marsaleix et al 1998 showed that winds around 30 km h were sufficient to detach the plume from the coast according to this author this threshold is independent of rhone river discharge in scenario 4 strong westerly wind and low discharge conditions the plume extends over a large area favored by the presence of a summer stratification at low discharge a part of the plume at the latitude of the buoy can be deflected eastwards in the gulf of fos in agreement with current data at the buoy according to fraysse et al 2014 this plume shape is the first step toward an intrusion in the bay of marseille if this scenario is followed by south east winds conditions to note such intrusions occurred in summer fraysse et al 2014 when the probability of occurrence of this scenario are the highest in scenario 5 with a strong northwest wind mistral and low discharge conditions the plume stands out from the coast as shown by demarcq wald 1984 and gangloff et al 2017 and modelled by estournel et al 1997 this case is favorable to an export of the plume far away from the coast even at low discharge conditions and moderate mistral but the wind speed are however above the 30 km h threshold proposed by marsaleix et al 1998 finally the northeast wind and moderate discharge conditions of scenario 6 induces a plume over a small area which does not affect the gulf of fos this is confirmed by gangloff et al 2017 who showed that for the most northern winds above 340 the plume barycenter does not go further east than the rhône river left bank however this scenario is really transitory and should be considered with caution indeed the climatologic model indicated persistence of a south wind similar to scenario 2 during the 48 h duration while according to buoy data this wind lasted 6 hours this implies that winds in this scenario are sometimes too transitory or weak to be correctly captured by the actual hydrodynamic model all these simulations shown that the activity plume may be maintained along the western coast of the rhone river outlet scenario 1 its eastern side scenario 4 or can extend far away from the coast scenario 5 plume of different extents can also remain nearly stable scenarios 2 3 and 6 until a change of hydrodynamic conditions and thus scenario most likely shifts from 2 to 4 5 6 and from 3 to 5 6 1 each cluster of hydrodynamic forcings has thus its own patterns for rhone river plume spreading in the gol their hydrodynamic variables and resulting trends were presented in the previous chapters and these results now allow to better evaluate the risk of propagation of 137cs activity however it is also interesting to define thresholds values for these variables in order to be able to select the most appropriate scenario to apply in case of alert on accidental release of radionuclides or any kind of chemical contaminants to summarize the differences obtained between the scenarios a simplified decision tree has been constructed fig 8 from top to bottom it allows to outline practical separation criteria fig 8 the classification of these 6 scenarios is based on the wind and gust direction wind speed and water discharge these in situ conditions can thus be associated with a scenario in near real time as they are available at this time scale from the websites of coriolis cotier wind and vigicrues river flow the tree reproduces the classification using 80 of the hydrodynamic raw data without pca treatment as training and 20 as a validation set it allows a fast crisp classification into one of the 6 established scenario with 83 of accuracy on both trained and tested data as an example in case of wind direction of 270 and a river discharge of 2500 m3 s the shape of the plume will correspond to scenario 3 currents directions at the buoy coriolis cotier can also be used as an additional verification 4 conclusion in this paper a 10 year period was considered in order to identify the main combinations of hydrodynamic forcings wind and rhone river discharge using a fuzzy c mean clustering these combinations called scenario summarized mean shelf behavior providing a very important information to estimate and understand the rhone plume patterns in case of accidental release in addition existence of observations memberships allowed to spot the best temporal windows to run simulations covering all possible patterns 6 scenarios have been identified and simulations showed that the plume behavior was different for each of them these plume patterns are more or less critical in terms of radiologic risks regarding the areas affected and the dilution of the activity if necessary wind speed measured on one point are sufficient to extrapolate plume shape for 48 h on this zone and the surface currents measured at the roustan buoy will give a first idea of plume orientation however transitory aspects of the last scenario shows that reliability of this method could fade after 48 h this illustrates also the need to increase climatological model precision in coastal hydrodynamic modelling this study provided a first global picture of main rhone river plume patterns and consequences for radionuclides accidental releases but the methodology may be applied to other estuaries declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors are indebted to the institute for radiological protection and nuclear safety irsn and to region sud provence alpes côte d azur authorities for the phd funding this study was conducted within the rhône sediment observatory osr program a multi partner research program funded through plan rhône of the european regional developmentfund erdf agence de l eau rhône méditérranée corse cnr edf and three regional councils région auvergne rhône alpes paca and occitanie the mesurho station is part of the coast hf network and associates ifremer irsn cnrs cetmef and cerema appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105005 
25832,assessing and modelling the coastal plume dispersion of nuclearized rivers is strategic in case of accidental releases but taking into account the variation of main hydrodynamic forcings is challenging this study uses fuzzy c mean clustering of a 10 years series of discharge and wind speed at the rhone river estuary france in order to explain the variability of its plume the method allows to classify the data into 6 scenarios of hydrodynamic forcings that were related to different spatial extensions of the plume as well as to surface currents measured in situ these scenarios were used to simulate the extension and dilution of a radioactive release issued from the river based on threshold values of the forcings a decisional tree is proposed to provide a quick decision tool identifying in real time which climatological scenario occurs at the river mouth and the potential plume pattern keywords coastal plume fuzzy c mean clustering accidental release scenarios rhone river radioprotection coastal management 1 introduction the rhone river catchment extends over 98 000 km2 and covers one fifth of the french metropolitan territory it is the main source of particles and freshwater for the gulf of lion in the north western mediterranean sea durrieu de madron et al 2000 and all together one of the most important input to the mediterranean sea ludwig et al 2009 the rhone valley also hosts the largest concentration of nuclear power plants in europe with 4 nuclear power plants in process and a spent fuel reprocessing center under dismantlement since 1997 eyrolle et al 2020 recently synthesized the studies showing that this river carries artificial radionuclides from decades resulting from authorized releases of low level radioactive liquid wastes and from the export of atmospheric deposits on watersheds consequently to nuclear weapons testing and chernobyl accident france is presently ranked second in the world for the production of nuclear energy and the total electricity production in the combined regions of northern western and southern europe is projected to increase by 2050 iaea 2019 also the risk of incident on any kind of nuclear installations is still of concern in france and must be taken into account as for any river the transport of artificial radionuclides in case of accidental release occurs both in dissolved and particulate form depending on the amount of suspended particulate matter and on the chemical properties of the radionuclides and particularly their distribution coefficient tomczak et al 2019 for the rhone river the prediction of dissolved vs particulate fluxes and the associated time scale for transit can be evaluated through numerical modeling launay et al 2019 but the behavior of radionuclides once at sea is clearly less constrained because it will primarily depend on the forcings governing the shape of the rhone river plume the area of the rhone river mouth is characterized by a very small tidal amplitude about 30 cm inducing the formation of a sedimentary delta as usual in this case the freshwater input forms a thin stratified plume of low salinity water and higher turbidity overlying the seawater and extending between 4 and 1000 km2 estournel et al 1997 gangloff et al 2017 with a thickness decreasing seaward pairaud et al 2011 gangloff et al 2017 it is preferentially deflected westward in a clockwise orientation running east to west reffray et al 2004 due to the general circulation induced by the northern current along the continental slope under north northwest winds the plume extends offshore towards the southwest whereas it is pushed to the coast west of the river inlet in case of southeastern winds satellite and modelling results have also shown that the plume size increases with river discharge fraysse et al 2014 gangloff et al 2017 more episodic processes impact the plume pattern such as dense water formation and cascading ulses et al 2008 upwelling cells and marine storms millot 1990 1999 as a result this plume extends far beyond the coastal areas and may covers a large area in the gol extending from the vicinity of the mouth up to the cap de creus at the french spanish border sanchez cabeza et al 1992 it can also reach the gulf of fos gontier et al 1992 charmasson et al 1999 or the bay of marseille pairaud et al 2011 fraysse et al 2014 on the eastern side of river mouth the gulf of fos is an important economic area with one of the biggest commercial port in europe and a large shellfish area and marseille is one of the biggest mediterranean coastal city with one million inhabitants due to the oligotrophic nature of the mediterranean sea the region of freshwater influence rofi of the rhone river has a major influence on the distribution of plankton groups diaz et al 2019 and thus pelagic catches on the gol obviously the inputs of chemical contaminants from the rhone river can greatly affect the fishery activity the combination of meteorological and hydrodynamic forcings with the dynamics of the rhone river discharge results in a large spatio temporal variability in freshwater and associated pollutants delivery to the gol martin et al 2019 if an accidental release occurs in the rhone river the dissolved radionuclides may reach the estuary within 48 h h to few days depending on the source location and water discharge unpublished results once at sea the different shapes that the plume may present will depend on hydrodynamic and weather conditions and will lead to contaminate different areas since one goal of radioprotection is to predict the transfer of radionuclides in the environment there is a need to anticipate their dispersion at any time and in any kind of meteorological and hydrodynamical conditions different numerical hydrodynamic models have been set up in the gol including the river mouth pairaud et al 2011 duffa et al 2016 and they could be used actually in case of accidental release in order to predict the behavior of the freshwater input however the delay necessary for their implementation will range from few hours to few days whereas very quick and concise information should be provided to experts and decision makers as a first picture of the local issues alsothe potentially impacted zones will be better defined by performing a fine spatial scale simulation adequately centered compared to a large scale simulation as a result a preliminary study embraces all possible plume patterns is necessary and a first step is to target the general behavior of the estuarine plume system bárcena et al 2015 explained that two approaches may be conducted for that simulating several scenarios using constant conditions of hydrodynamic forcings or simulating few scenarios using the most frequent or extreme real hydrodynamic forcings during short medium term periods month to year these authors demonstrated that the first approach is not complete because real forcings cannot be deduced from the combination of simple idealized scenarios the second approach relies on a subjective selection of scenarios by an expert and it will have an expensive computational cost for simulations if the need is to get on overview of the different kind of realistic responses of the estuarine plume mean behavior in this case and to minimize subjectivity a methodology based on data mining should be able to select the most relevant condensed hydrodynamic scenarios taking into account the time evolution and the occurrence probability of the forcings plume classifications based on satellite observations or hydrodynamic model output have been defined in several river sea systems using empirical orthogonal function or self organizing map falcieri et al 2014 xu et al 2019 such classification method deals with large spatial scale but implies a heavy data pretreatment like masking to treat the satellite data or for the computation of the model in addition the need of long term environmental databases e g 10 20 years to assess probabilities implies significant computational costs as well as long and multiple series of data to be used as boundary conditions and climatic forcings another approach is to classify the main hydrodynamics drivers by looking for example at the catchment discharge and the winds intensities and directions kaufmann and whiteman 1999 zhang et al 2011 since the plume response to these forcings can be longer than 24h demarcq and wald 1984 estournel et al 1997 the classification should work observation by observation but must also keep consistency over longer temporal scales of few days in order to be accurate clustering performed on temporal series helps to assess the consistency of a trend over time and a fuzzy clustering algorithm provides a continuous cluster membership function allowing to spot significant trend changes in this context this paper presents a methodology based on statistical analysis and numerical modelling that was developed to address the limitations of the previously mentioned approaches firstly we used a fuzzy c mean algorithm to identify and classify combinations of winds and discharge at the mouth of the rhone river in order to define model scenarios of realistic forcings secondly the consequences for sea surface currents will be assessed and the resulting plume pattern will be modelled for each scenario as well as the distribution of dissolved radionuclides due to a hypothetical and episodic release on the rhone river these plumes scenario can be used as a support for operational tools improvement and decision 2 material and methods 2 1 field study and data the rhone river hourly discharges have been provided by the c n r compagnie nationale du rhône thanks to the rhone sediment observatory osr program they were measured at the sora station in the city of arles located 47 km upstream of rhone river mouth fig 1 it must be noted that the rhone river splits in two branches upstream of this station the grand rhone and petit rhone the station reports the discharge for the grand rhone river only which represents about 90 of the total rhone river discharge boudet et al 2017 in our case we focus only on the river plume at the grand rhone outlet weather data and subsurface marine currents data are issued from the mesurho station pairaud et al 2016 operational since june 2009 and located at the buoy float immersed bfi maritime buoyage roustan east 43 19 2 n 4 52 e on the rhone prodelta 20 m water depth it is about 1 mile southeast of the mouth and was configured to collect physico chemical data in near real time and at high frequency about 30 min in the fresh marine waters transition zone it is equipped with a weather station at 10 m height and an acoustic current doppler profiler adcp the instrumentation is connected by a cable to a controller located above the sea surface and powered by solar panels the measurements are transmitted to the coriolis data center via gprs about 1 transmission every 12h since 2015 4h before weather variables used are the average wind speed over 30 min and the gust wind speed gust wind speed is the maximal mean wind speed over 0 5 s observed during a period of 30 min the observations used were registered between 2009 and 2019 and result in a total of 128 262 data the subsurface currents maximum depth of 1 5 m from 2010 to 2019 were also used when available and after quality control validation leading to a total of 31 826 observations in order to perform multivariate analysis and regression wind and currents variables expressed in terms of velocities u and directions θ are described by an eastward and a northward component x and y and calculated as follow 1 x u c o s θ y u s i n θ the corresponding hourly discharges in arles 64 131 obs were shifted with a 24 h delay which corresponds to the transit time between arles and the river mouth for a mean liquid discharge 2 2 principal component analysis principal component analysis pca has been widely used in environmental sciences including hydrologic and hydrodynamics e g hannah et al 2000 pairaud et al 2008 the common goal to all principal component methods is to describe a data set x with i individuals or observations and w variables using a small number p w of uncorrelated variables while retaining as much information variance as possible the reduction is achieved by transforming the data into a new set of continuous variables named the principal components the reduction of dimensionality provides a framework to visualize data which is especially important for large datasets husson et al 2010 this facilitates the analyses based on geometrical criteria such as separate observations into k distinct sub groups clustering or determination of extreme points renner 1993 napoleon and pavalakodi 2011 using pca as a pre processing tool in order to cluster presents two additional advantages the reduction of dimensionality speeds up the convergence of classification algorithms which usually depends on the square of p and i ben dor et al 2004 and it reduces the noise the essential of the information being on the first components whereas the noise is on the lasting ones husson et al 2010 pca has been performed using the r package factominer lê et al 2008 2 3 fuzzy c mean algorithm clustering is a usual method for data mining when it comes to identify groups and classify individuals but many algorithms exist and present different results and convergence speeds jain et al 1999 the first goal is to find an algorithm based on geometrical criteria as simple as possible for a more realistic interpretability and the second one is to find a fast convergence algorithm in order to treat the important dataset the most usual method is the c mean or k mean macqueen 1965 yadav and sharma 2013 and its fuzzy alternative bezdek 1981 fu lai and tong 1994 c means are iterative algorithms that classify individuals of a dataset into c groups the algorithm allows to randomly define c centroids in the same coordinate systems as the individuals each individual x total of k is then assigned to the closest centroid center ci the barycenter of each subgroup is then calculated and becomes the new centroid again individuals are reassigned to the closest centroid this iterative procedure minimizes the objective function j and the procedure ends when j reaches an inferior threshold in eq 2 2 j i 1 c x k c i x k c i 2 this method is defined as crisp which means that each observation is set to belong to its closest centroid cluster consequences are that observations with different distances from the nearest cluster are classified into this cluster without degree of uncertainty and the ambiguity of the data is eliminated cluster boundaries are usually not sharp in environmental sciences zadeh et al 1965 especially when ambiguous data exist and membership degrees are more realistic than crisp assignments klawonn and höppner 2003 a priori we do not expect a crisp classification and it is important to have feedback on the confidence of classification for each individual as a result the ambiguity of the data can be preserved and his probability can be used later for post treatments kim et al 2011 the fuzzy alternative introduces two new parameters the first one is the membership coefficient μik the coefficient of the kth observation to the ith cluster this membership represents how closely the kth data object xk is located from the ith cluster center it varies from 0 to 1 depending on the distance x k c i 2 and a higher membership coefficient indicates stronger association between the kth data object to the ith cluster 3 μ i k j 1 c x k c i 2 x k c j 2 2 m 1 1 the second parameter m is the fuzziness coefficient it is greater than 1 and usually dependent on the dataset structure because it represents the degree of overlap of the clusters klawonn and höppner 2003 if we set m to a smaller value more less weight is given to the objects that are located closer to farther from a cluster center as m is close to 1 μik converges to 0 for the objects that are far from a cluster center or 1 for those close to a cluster center which implies less fuzziness i e clearer cut the symbol denotes any vector norm that represents the distance between the data object and the cluster center here we use the 2 norm euclidean norm which is widely used in the fcm the new c mean function to minimize becomes 4 j i 1 c k 1 k μ i k m x k c i 2 the robustness brought by the fuzzy approach over the crisp classification is a significant improvement in term of efficiency and convergence because each individual observations has a probability to belong to each center centers are adjusted faster and the algorithm converges faster fu lai and tong 1994 ferraro and giordani 2015 also without any prior information on the cluster structure sphericity of clusters possible overlap the fuzzy c mean provides better results than its crisp counterpart selim and kamel 1992 as a result hydrologic and climatologic combinations can be identified by fuzzy cmean kim et al 2011 zhang et al 2011 bárcena et al 2015 in this study the fuzzy cmean algorithm is performed using the e1071 package from r software meyer et al 2019 2 4 choice of the number of clusters c and the coefficient of fuzziness m fuzzy c mean algorithm needs to be initialized with the number of clusters c and the coefficient of fuzziness m the best combination of these parameters is not determined by the algorithm one approach is to run different simulations with different c m pairs and to check the efficiency of clustering with a quality criteria ramze rezaee et al 1998 setnes and babuška 1999 many criteria and their efficiency are available in wang and zhang 2007 and liu et al 2010 some have fast calculation like partition coefficient pc or partition entropy pe but they monotonously decrease with the number of clusters and the lack of direct connection to the geometry of the dataset others are more complete but computationally expensive such as the dunn index dunn 1974 or the fuzzy silhouette campello and hruschka 2006 and they could not be calculated with this dataset the xie and beni index xie and beni 1991 could be calculated based on eq 5 xb has a direct connection to the geometrical property of dataset because it takes into account both compacity and separation of the clusters it deals correctly with noisy datasets size or density variations liu et al 2010 5 x b i 1 c k 1 k μ i k m x k c i 2 k m i n i j c j c i 2 calculation of xb is also fast for our dataset from 4 to 10 s depending on c m pair c could be any integer number between 2 and 358 the last one being theorically the square root of the dataset length chaimontree et al 2010 the fuzzifier m can be in theory any real number between 1 and in our case the interval of c m simulations has been restrained based on the following depending on the river sea system involved a different number of plume patterns exist in literature we found that a river plume can present up to 8 patterns xu et al 2019 as a result we do not expect our number of cluster to exceed 8 and the number of clusters c was set between 2 and 8 previous studies report that values of m can range from 1 to 4 most of them use m ε 1 5 2 5 and as result m is usually set to 2 by default klawonn and höppner 2003 overall m is lower for large datasets klawonn and höppner 2003 and the lower limit will be fix in our case to 1 using the empirical threshold equation based on the length and dimensions of the dataset proposed by schwämmle and jensen 2010 we found that the superior threshold value of m for our dataset is around 2 5 by safety this threshold value is increased by 0 25 as a result parameter m will be tested in the interval 1 2 75 and c in the interval 2 8 3 results and discussion 3 1 principal component analysis pca successfully reduced the five original variables wind speed toward north and east gust wind speed toward north and east and rhone discharge into three components and gave a summed variance of 96 4 fig 1 supplementary material this is not surprising since the gust wind speed and mean wind speed are correlated due to same direction fig 2 supplementary material the first axis contains 51 of variability with the information on wind direction the second axis with 25 of variability contains the information on wind speed and the last one 20 variability corresponds to the rhone discharge the elbow criteria the kaiser rule and the interpretation of the components confirm without ambiguity these three components fig 1 supplementary material the lasting 3 6 carried by the two remaining components concern really specific and scarce interactions like the anticorrelation between mean wind speed and gust wind speed as a result 80 and 20 of the variability are due to the variations of winds and liquid discharge respectively 3 2 clustering results and performances fuzzy c mean clustering was performed on the 128 262 observations and the three main dimensions resulting from pca a summary on classification performances based on xb index is shown fig 2 all configurations performs reasonably well except the one with 3 clusters an interesting result is that the 2 clusters configuration performed reasonably well which confirms that the plume dynamics can be described as a first approach by considering only the wind direction that is south east against north west winds this is in agreement with the 50 of variability held by the wind direction discussed hereunder however the configuration selected is the one giving the better result for xb with 6 clusters optimized at m 2 45 xb 0 19 3 3 characterization of the scenarios a cluster gathers observations having close values for one or more variables these properties on variables are specific to each cluster and are then interpreted hereunder as a scenario in order to interpret the clustering and to characterize the resulting scenarios we present the distribution of winds and discharges in figs 3 and 4 whereas fig 5 shows the percentage of occurrence of these scenarios for each month the discharge distribution in each cluster was significantly different from the global distribution of rhone discharge in arles based on the kolmogorov smirnov test see fig 4 in the description below a flood event for the rhone river refers to a discharge above a threshold set at 3900 m3 s in arles boudet et al 2017 a storm criteria is usually the significant wave height but this parameter showed too many breaks in the time series transmitted in near real time by the buoy over the 2009 2019 period also we defined sea storm here by using as a threshold the quantile 98 of our offshore gust wind speed dataset 50 3 of total dataset which is 27 8 m s 100 km h klawa and ulbrich 2003 cluster 1 gathers south east winds paragon 126 with 9 2 m s mean wind speed and 16 m s mean gust it contains 86 of all observed sea storm events with the highest intensity and 19 of the flood events the distribution of the hourly water discharge does not characterize this cluster observations belonging to this cluster have less than 6 occurrence in july september rising up to 23 in october and november this cluster can be interpreted as moderate to high waves scenario resulting from fresh breeze to violent storm south east marine winds observations in cluster 2 are winds with velocities around 4 5 m s and 9 2 m s gust fully coming from the south 171 it contains 12 of all observed sea storm events it gathers discharges values under 2500 m3 s with a median at 960 m3 s the observations mainly occur in august september with 23 occurrence this cluster can be interpreted as a rhone river low flow scenario mainly associated with south east marine breeze or sometimes a sirocco wind coming from the south reiter 1975 wind observations in cluster 3 show an important variability and are superimposed with clusters 4 and 5 most representative winds present a mean speed of 8 5 m s and gust speed of 13 1 m s the rhone discharge distribution for cluster 3 is very different from the reference distribution highest kolmogorov s d statistic it gathers discharges higher than 2000 m3 s and contains most of the flood events 79 of them this is also the cluster showing the highest contrast in seasonality with an occurrence up to 33 from november to february decreasing to 0 2 during the july october period cluster 3 can be interpreted as the high river flow scenario with a combination of different winds coming from the north west in cluster 4 observations are usually winds with 7 2 m s mean wind speed and 9 1 m s mean gust coming from the west 272 it contains 2 of all observed sea storm events discharges are below 2500 m3 s with a median around 1070 m3 s these observations mainly occur in july august september with 33 of occurrence a specific point is that their occurrence increases during the afternoon with a peak around 1h am fig 3 supplementary material interpretation of this cluster is a rhone river low flow scenario gathering moderate sea breeze coming from the south west cros et al 2004 with sometimes a strong onshore gale from west cluster 5 corresponds to winds with 12 3 m s average speed and 18 6 m s gust coming from a restricted area in the north 325 the corresponding water discharge distribution is on the lower part of the global distribution median of 1140 m3 s and discharges are always below 3000 m3 s the monthly occurrence is stable 15 with a peak in february at 25 the strong average wind intensities and gust speeds highest at 340 combined with the restricted wind direction parallel to the rhone valley stand for the characteristics of the mistral wind reiter 1975 as a result cluster 5 can be interpreted as a mistral wind scenario dry and strong breeze to strong gale associated with low to moderate discharges cluster 6 usually gathers winds with 5 7 m s average speed and gusts of 12 5 m s coming from the north east 11 however winds coming from 340 to 360 north north west are also observed the related hourly water discharges distribution is in the lower part of the global one median of 1230 m3 s and few discharges higher than 3000 m3 s are observed occurrence of cluster 6 observations is very stable all along the year ranging between 14 and 18 this cluster presents the largest gap between the wind speed average and the gust wind speed and shows an increasing occurrence in the early morning the highest gusts reach 50 m s and occur episodically in winter with an origin from 10 to 60 north east these are the strongest gusts observed among all scenarios we interpret cluster 6 as a scenario gathering land breeze or valley flow during summer and winds channeled by pre alps moutains cros et al 2004 duine et al 2017 which become stronger in winter orsure according reiter 1975 3 4 consequences for surface currents six clear wind discharge patterns have been identified but did they correspond or induce different hydrodynamics responses of the surface currents in the vicinity of the rhone river mouth consequences for subsurface currents observations issued from the adcp on the mesurho station are investigated through a least squares multiple regression observation membership to clusters ci are the explanatory variables and currents in eastward and northward directions are the response variables estimators xi and yi are then used to calculate current orientation θ rad and speed u m s observed on each cluster with equation 1 confidence intervals are calculated with the robust white standard errors with lmtest package white 1980 hothorn et al 2019 to avoid heteroskedasticity and underestimation of confidence intervals the main current direction for each cluster obtained by least squares regression on memberships is presented on fig 6 right along with the global current rose left currents oriented at 280 correspond to scenario 1 and are in agreement with the more general modelling and satellite observations during similar south easterlies wind conditions showing the plume tackled to the camargue coast marsaleix et al 1998 gangloff et al 2017 scenario 2 presents small currents not related to the wind direction in this case winds are probably too low and currents are driven by the general circulation which has a current speed similar to those of this scenario 10 cm s in scenario 3 the current direction correspond to those at the rhone river mouth meaning that during high water events discharges superior to 2500 m3 s the river influence becomes significant scenario 4 5 and 6 seem to follow the surface ekman transport with a deflection to the left relative to the wind direction scenario 5 is the one presenting the largest interval of confidence despite having the straightest wind distribution a closer look at the data shows that in this scenario the currents deeper than 1 2 m present an important heterogeneity in their direction however for wind average speeds superior to 15 m s and gust wind speeds over 25 m s this heterogeneity does no longer exist and all currents are oriented in a 150 direction for comparison scenario 5 paragon is an average wind speed of 12 3 m s and gust wind speed of 18 6 m s values which areinferior to the two thresholds and may explain the currents discrepancies associated with this scenario to conclude each scenario has its own current direction and intensity statistically different and significant 3 5 application the main objective of this work is to define the general trends of dispersal in the gol that can be expected in the case of artificial radionuclides release within the river since releases may occur at any time in a year we modelled the dispersion of a radioactive plume in the gol for each of the previous hydrodynamic scenario in order to get an overview of the potential impacts whatever the hydrodynamic and climatological conditions the simulation code used at irsn for the marine area is sterne simulation du transport et du transfert d eléments radioactifs dans l environnement marin or simulation of radionuclide transport and transfer in marine environments it was designed to assess the radiological impact of accidental releases affecting the marine environment eulerian radionuclide dispersion is calculated using a tracer advection diffusion equation more details on the code can be found in duffa et al 2016 we use the 2010 hydrodynamic outputs provided by ifremer with its mars3d model implemented on the north western mediterranean sea nicolle et al 2009 the simulations assumed a release of 1 tbq of 137cs dissolved activity in the river over a temporal window of 48 h 137cs was chosen because this radionuclide is released at each nuclear accident and is also found in authorized releases from nuclear powerplants this radionuclide presents a high radiotoxicity garnier laplace et al 2011 and is relatively soluble in seawater with a kd ranging from 450 to 2000 l kg delaval et al 2020 for comparison the estimated average direct discharge of 137cs to the ocean during the fukushima daiichi nuclear power plants was around 5000 tbq buesseler et al 2017 the simulations were done for each hydro meteorological scenario defined on the basis of the 2009 2019 dataset and the plume extension in the gol was modelled for each scenario since the hydrodynamic inputs are only available for the year 2010 we selected in this input the most representative temporal window for each scenario simulation by integrating observations memberships over a sliding window of 48h the temporal window presenting the highest summed membership values were selected for each cluster one exception was done for the sixth scenario for which the temporal window with the second highest membership was chosen due to the non homogeneity between wind observations at the buoy and wind restitution in the hydrodynamic model implications for this are discussed further the results of the 6 simulations are very different in terms of plume shape and thus affected areas fig 7 mean winds and discharges conditions over 48 h h are indicated in the figure it must be noted that these values are specific to the chosen temporal windows and thus can be different from the parangons presented in the previous chapter in scenario 1 strong marine wind with moderate discharge conditions the plume is constrained to the coast and extends west in agreement with the currents at the buoy this scenario has already been highlighted by demarcq wald 1984 and many et al 2018 or modelled by estournel et al 1997 a part of the activity remains blocked in the estuary due to winds in opposition with its flowing path and an increase in sea level at the mouth limiting the power of the jet the lowest expansion of the plume is observed with scenario 2 weak wind with discharge slightly under the annual mean the plume has the lowest surface spatial expansion among the 6 plumes and is nearly stagnant and remains with a high activity this is in agreement with the currents observed at the buoy showing really low speeds this scenario appears mostly during summer fig 4 and satellite images confirmed that the turbid plume present effectively its smallest area at this period gangloff et al 2017 scenario 3 corresponds to a high rhone river discharge and northwest winds conditions the plume presents a large area but the northwesterly wind is powerful enough to carry the plume offshore for different wind stress simulations marsaleix et al 1998 showed that winds around 30 km h were sufficient to detach the plume from the coast according to this author this threshold is independent of rhone river discharge in scenario 4 strong westerly wind and low discharge conditions the plume extends over a large area favored by the presence of a summer stratification at low discharge a part of the plume at the latitude of the buoy can be deflected eastwards in the gulf of fos in agreement with current data at the buoy according to fraysse et al 2014 this plume shape is the first step toward an intrusion in the bay of marseille if this scenario is followed by south east winds conditions to note such intrusions occurred in summer fraysse et al 2014 when the probability of occurrence of this scenario are the highest in scenario 5 with a strong northwest wind mistral and low discharge conditions the plume stands out from the coast as shown by demarcq wald 1984 and gangloff et al 2017 and modelled by estournel et al 1997 this case is favorable to an export of the plume far away from the coast even at low discharge conditions and moderate mistral but the wind speed are however above the 30 km h threshold proposed by marsaleix et al 1998 finally the northeast wind and moderate discharge conditions of scenario 6 induces a plume over a small area which does not affect the gulf of fos this is confirmed by gangloff et al 2017 who showed that for the most northern winds above 340 the plume barycenter does not go further east than the rhône river left bank however this scenario is really transitory and should be considered with caution indeed the climatologic model indicated persistence of a south wind similar to scenario 2 during the 48 h duration while according to buoy data this wind lasted 6 hours this implies that winds in this scenario are sometimes too transitory or weak to be correctly captured by the actual hydrodynamic model all these simulations shown that the activity plume may be maintained along the western coast of the rhone river outlet scenario 1 its eastern side scenario 4 or can extend far away from the coast scenario 5 plume of different extents can also remain nearly stable scenarios 2 3 and 6 until a change of hydrodynamic conditions and thus scenario most likely shifts from 2 to 4 5 6 and from 3 to 5 6 1 each cluster of hydrodynamic forcings has thus its own patterns for rhone river plume spreading in the gol their hydrodynamic variables and resulting trends were presented in the previous chapters and these results now allow to better evaluate the risk of propagation of 137cs activity however it is also interesting to define thresholds values for these variables in order to be able to select the most appropriate scenario to apply in case of alert on accidental release of radionuclides or any kind of chemical contaminants to summarize the differences obtained between the scenarios a simplified decision tree has been constructed fig 8 from top to bottom it allows to outline practical separation criteria fig 8 the classification of these 6 scenarios is based on the wind and gust direction wind speed and water discharge these in situ conditions can thus be associated with a scenario in near real time as they are available at this time scale from the websites of coriolis cotier wind and vigicrues river flow the tree reproduces the classification using 80 of the hydrodynamic raw data without pca treatment as training and 20 as a validation set it allows a fast crisp classification into one of the 6 established scenario with 83 of accuracy on both trained and tested data as an example in case of wind direction of 270 and a river discharge of 2500 m3 s the shape of the plume will correspond to scenario 3 currents directions at the buoy coriolis cotier can also be used as an additional verification 4 conclusion in this paper a 10 year period was considered in order to identify the main combinations of hydrodynamic forcings wind and rhone river discharge using a fuzzy c mean clustering these combinations called scenario summarized mean shelf behavior providing a very important information to estimate and understand the rhone plume patterns in case of accidental release in addition existence of observations memberships allowed to spot the best temporal windows to run simulations covering all possible patterns 6 scenarios have been identified and simulations showed that the plume behavior was different for each of them these plume patterns are more or less critical in terms of radiologic risks regarding the areas affected and the dilution of the activity if necessary wind speed measured on one point are sufficient to extrapolate plume shape for 48 h on this zone and the surface currents measured at the roustan buoy will give a first idea of plume orientation however transitory aspects of the last scenario shows that reliability of this method could fade after 48 h this illustrates also the need to increase climatological model precision in coastal hydrodynamic modelling this study provided a first global picture of main rhone river plume patterns and consequences for radionuclides accidental releases but the methodology may be applied to other estuaries declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors are indebted to the institute for radiological protection and nuclear safety irsn and to region sud provence alpes côte d azur authorities for the phd funding this study was conducted within the rhône sediment observatory osr program a multi partner research program funded through plan rhône of the european regional developmentfund erdf agence de l eau rhône méditérranée corse cnr edf and three regional councils région auvergne rhône alpes paca and occitanie the mesurho station is part of the coast hf network and associates ifremer irsn cnrs cetmef and cerema appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105005 
25833,quantarctica https www npolar no quantarctica is a geospatial data package analysis environment and visualization platform for the antarctic continent southern ocean 40os and sub antarctic islands quantarctica works with the free cross platform geographical information system gis software qgis and can run without an internet connection making it a viable tool for fieldwork in remote areas the data package includes basemaps satellite imagery terrain models and scientific data in nine disciplines including physical and biological sciences environmental management and social science to provide a clear and responsive user experience cartography and rendering settings are carefully prepared using colour sets that work well for typical data combinations and with consideration of users with common colour vision deficiencies metadata included in each dataset provides brief abstracts for non specialists and references to the original data sources thus quantarctica provides an integrated environment to view and analyse multiple antarctic datasets together conveniently and with a low entry barrier keywords geographical information system regional analytical tool polar regions product availability the product is released under the cc by4 0 license with doi https doi org 10 21334 npolar 2018 8516e961 quantarctica is downloadable from the norwegian polar institute ftp ftp quantarctica npolar no as well as from global mirrors generously provided by the tasmanian partnership for advanced computing at the university of tasmania ftp quantarctica tpac org au the arctic and antarctic data archive system at japan s national institute of polar research https ads nipr ac jp gis quantarctica india s national center for polar and ocean research ftp ftp ncaor gov in and the polar geospatial center at the university of minnesota ftp ftp data pgc umn edu gis packages quantarctica all download sites and quantarctica friendly datasets see section 4 2 are listed at quantarctica s project web page at the norwegian polar institute https www npolar no quantarctica 1 introduction the antarctic ice sheet and southern ocean are major thermal reservoirs in the earth system that significantly influence climate change kennicutt et al 2015 2019 as the amount and availability of scientific data in this region grows and the needs of more interdisciplinary research are realized kennicutt et al 2014 researchers and students increasingly require an easy to use unified platform to import display and analyse geospatial datasets for work involving this region educators and logistics operators also need a convenient platform with low entry barrier to get a complete picture of these remote areas as well as develop practical tools for their work many such users develop their own dedicated analysis environments for individual purposes bringing together basic data such as terrain models satellite imagery and scientific data from different disciplines unfortunately this entails significant start up cost and duplicated effort basic antarctic datasets such as the antarctic digital database add https www add scar org and the southern ocean observation system map soosmap http www soosmap aq exist as online portals one can download satellite data for antarctica from the earliest such data decades ago through to the present e g nasa s world view https worldview earthdata nasa gov polar view https www polarview aq antarctic and the us geological survey s earthexplorer https earthexplorer usgs gov in addition the antarctic environmental portal https www environments aq provides an important link between antarctic science and antarctic policy particularly in environmental management however being online portals all these data sources are not immediately helpful during remote field work or a scientific cruise one exception is antarctic mapping tools greene et al 2017 which provides antarctic data that can be operated offline however it is limited mostly to physical science data and its analysis functions require proprietary matlab computing software thus instead of having to gather antarctic scientific data from numerous online data portals in many file formats and geographic projections a user friendly system would provide pre compiled basic antarctic datasets in multiple disciplines and load consistently both in the office and in the field for this purpose the base software platform should be low cost and multiple platform operational with little or no license restrictions to maximize benefits for the rapidly growing antarctic research community in 2011 we began developing a gis data package that can operate with modest computer capabilities so that the system is useful at remote field camps and for a wide range of users we selected quantum gis renamed to qgis in 2013 https qgis org for the software platform and named the data package quantarctica hereafter qa version 1 of qa was released in 2012 for version 2 released in 2014 we improved the glaciology and geophysics data coverage and data visualization the scientific committee on antarctic research scar a thematic organization of the international science council selected qa version 2 as a scar map product in 2014 https scar org resources maps to respond to the antarctic scientific community s increased interest in qa we decided to develop version 3 qa3 with scientific data from other disciplines and expand its geographical coverage from 50os to 40os to include research in the southern ocean and sub antarctic islands qa3 was released in february 2018 for use with qgis version 2 as our integration tests were completed we updated qa3 to version 3 2 in 2021 which runs on qgis version 3 16 fig 1 here we describe methods used to develop qa3 present the current contents of qa3 demonstrate user applications and finally present its outlook 2 methods 2 1 choice of software to choose the base gis software for which our data package is optimized the following criteria were used the software should 1 be free of charge or at least low cost 2 work on the most common computer operating systems 3 be able to function without an internet connection offline readiness 4 be relatively easy to use 5 have a rich toolset 6 be able to read and write common geographical data file formats 7 allow advanced cartography 8 allow good map figure production 9 be actively maintained and in development 10 have a process that allows transparent bug fixing processes 11 have an active user community and rich knowledge resources and 12 be available for bundling with the qa package e g in a usb stick for standalone offline use we found quantum gis renamed to qgis in 2013 https qgis org to satisfy these requirements best 2 2 selection of data to select data used for basemaps miscellaneous base layers satellite data and terrain models we considered antarctic map products widely available as best suited for the purpose in the combined terms of coverage completeness and detail sections 3 1 3 4 to select the scientific data we first surveyed the relevant scientific community in 2016 to identify community priorities for included datasets this input was considered by one or two editors for each discipline and then the recommended datasets were further screened by the core development team at the norwegian polar institute qa3 includes 265 data layers of which 164 layers represent scientific datasets tables 1 and 2 these selections will be re visited when qa version 4 is developed in the future 2 3 mapping projection the antarctic polar stereographic projection epsg 3031 was chosen because it is both antarctic centric and the most commonly used projection for antarctic geographical data already some datasets are originally in other projections qgis can handle multiple data layers projected to different systems nonetheless on the fly reprojection hampers fast rendering and demands cpu work therefore all data in qa are projected to the single projection epsg 3031 2 4 file formats compression and data re sampling individual datasets are originally in various file formats for consistency we convert all vector data to esri shapefile format and raster data to geotiff format most raster files are compressed with lossless lzw and deflate algorithms these file formats are commonly used in the gis community and are supported in most other gis systems however including high resolution imagery in a lossless format largely decreases qa s portability so we converted them to the jpeg format with associated georeferenced files or to the jpeg2000 format with a compression level that provides a hardly noticeable quality reduction section 3 3 these formats are less common in the gis community but still supported widely in this way datasets in qa can be used in other platforms as well except for re projection the original data are not altered 2 5 cartography the presentation of data layers is carefully adjusted to be intuitive readable and visually pleasant at all map scales the scientific datasets are presented such as to stand out when rendered on the top of basemaps and satellite imagery the presentation is also made so that datasets often viewed together are easily distinguishable from one another for colour deficient users if the predefined cartography is not fully suited for a specific purpose qa is fully customizable so users can change the appearance many data layers particularly the detailed basemaps use scale dependent features so that these layers are rendered to an appropriate level of details at a given scale fig 1 most data layers constituting detailed basemaps are stored at high medium and low resolutions for high performance rendering and for best cartographic expression 2 6 meta data metadata includes a brief description of the dataset for general use also included are original data location citation and the editor responsible for the data layer we urge all users to cite this original data source not qa when they use the dataset in their work in qa version 3 2 all metadata are stored as a part of the qgis project file qgs file format and in each data folder associated with individual data layers in the qmd file format the abstract section of the metadata brief description of the data citation and handling editor is stored as plain text txt files for quick reference the latter is useful when users are interested in using specific data layers on different platforms such as arcgis and non gis platforms such as computing software r https www r project org 3 results qa3 includes datasets categorized as 1 simple and detailed basemaps 2 miscellaneous base layers 3 satellite imagery 4 terrain models and 5 a range of scientific data table 1 users can display and rearrange the order of data layers that lay on top of basemaps satellite imagery and terrain models at a user chosen continuously adjustable map scale fig 1 scientific data are categorized into nine disciplines table 2 all data layers are stored with a folder tree structure so that individual data files and associated metadata can be easily found table 3 lists all data included in qa3 the total size of qa3 s data files is 7 53 gb with the satellite imagery and terrain models constituting about 4 13 gb table 1 our distribution package with qgis version 3 16 1 is 8 65 gb we provide a start up manual to adjust qgis to the quantarctica workspace ftp ftp quantarctica npolar no quantarctica3 quantarctica getstarted pdf which typically takes 0 5 1 h for first time users to follow after qgis and quantarctica are installed 3 1 simple and detailed basemaps simple basemaps allow users to quickly render oceans ice sheet ice shelves and other continents primarily composed of the add data layers fig 2b inset detailed basemaps are a composite of vector layers of ice and rock surface elevation contours outcrops moraines lakes and streams ice shelves calving front as well as raster layers of both bathymetry and topographic hillshade these basemaps use the add and several terrain models figs 1 2b and 2c and 2d as users zoom in features appear with a higher resolution to balance the level of detail necessary for a given scale and rendering speed 3 2 miscellaneous base layers additional base layers show other scale dependent features such as the 37 628 place names south of 60os registered in the scar composite gazetteer of antarctica https data aad gov au aadc gaz scar further north the names come from other reliable resources and shown in the language of the sovereignty some features are left unlabelled when the sovereignty might be disputed other layers show latitude longitude lines the antarctic circle south pole universal transverse mercator utm zones and locations and facility details of 108 research stations from a list of antarctic facilities maintained by the council of managers of national antarctic programs comnap fig 1 users can filter and search for place names and features using attribute tables an overview place name layer gives a quick reference to large regions in a small map scale 3 3 satellite imagery qa3 includes three sets of satellite imagery one for the landsat images users can view the 240 m resolution landsat image mosaic of antarctica lima taken in 1999 2002 that covers the entire ice sheet and ice shelves bindschadler et al 2008 or view the 15 m resolution landsat image tiles taken in 2013 2017 over islands outcrops ice shelves and the ice sheet except for regions 82 7os poleward two the radarsat antarctic mapping project s ramp 100 m resolution mosaic covers the entire ice sheet and ice shelves taken in 1997 jezek et al 2013 three is the moderate resolution image spectroradiometer modis mosaic of antarctica moa 125 m resolution image mosaic taken in 2003 2004 that covers the entire ice sheet and ice shelves haran et al 2014 scambos et al 2007 these specific satellite products allow users to optimize the display of overall topographic features lima ramp moa areas of blue ice and outcrops lima and sub surface features such as crevasses under snow detected using ice penetrating microwaves ramp to balance image resolutions and portability of qa we converted lima to the jpeg format with associated georeferenced files and ramp and individual landsat tiles to the jpeg2000 format with a compression level that provides a hardly noticeable quality reduction to increase the rendering performance and user friendliness more than 654 individual landsat image tiles are combined to a single virtual mosaic file with image pyramids so that lower resolution versions are rendered when zoomed out 3 4 terrain models six terrain models are included with spatial resolutions ranging from 0 2 to 2 0 km specifically the ramp2 liu et al 2015 and cryosat 2 helm et al 2014 terrain models as well as the add elevation contours represent the surface of the outcrops ice sheet and ice shelves figs 2a and 3b the bedmap2 compilation represents the ice thickness and bed topography under the ice sheet fretwell et al 2013 bedmap2 shows uncertainties in ice thickness and bed elevation and geographic coverage and corresponding data sparse regions of the ice penetrating radar data with which ice thickness and bed elevation were determined over the ice sheet the international bathymetry chart of the southern ocean ibcso shows the bathymetry of the southern ocean at latitudes south of 60os as well as metadata on the availability and type of data used for the compilation e g multibeam or single beam fig 2c arndt et al 2013 finally a global etopo1 relief model applies to latitudes north of 60os noaa national geophysical data center 2009 elevation references differ between the terrain models cryosat 2 and ramp2 refer to the wgs84 ellipsoid whereas the others refer to the mean sea level using different geoid models similarly each terrain model has different features with different strengths and weaknesses so users can choose the dataset that best matches their needs all models are displayed as rasters derived hillshades and elevation contours contour intervals and hillshade parameters can be customized in qgis to better view highly variable terrains such as continental shelf breaks abyssal plains steep mountainous regions near the coast and virtually flat and featureless inland ice sheet over a range of spatial scales 3 5 atmosphere data most atmospheric data provided in qa are outputs from racmo2 a regional atmospheric climate model van wessem et al 2014a 2014b these include near surface data such as 2 m high temperature and 10 m high wind speeds over the ice sheet and ocean fig 2a as well as surface mass balance over the ice sheet and ice shelves all annual values averaged between 1971 and 2011 the 27 km resolution racmo2 raster datasets are originally projected in a rotated polar coordinate system which cannot easily be converted to epsg 3031 however each model cell has epsg 4326 coordinate values as well which we used to build up epsg 3031 rasters at 35 km resolution the best achievable without creating void data cells anywhere in between atmospheric data also include a dataset of satellite observed wind scour zones over the ice sheet fig 2a das et al 2013 these near surface and surface data have been selected over middle and upper atmosphere data because of their stronger influences on processes in the other disciplines 3 6 biology data biology datasets for the southern ocean include summer chlorophyll a density near the ocean surface fig 2b johnson et al 2013 johnson et al 2017 20 pelagic regions based on seawater temperature and sea ice distribution raymond 2014 and 29 benthic regions douglass et al 2014 on top of these raster or polygon data layers users can plot locations of available vertical profiles of temperature and salinity collected by the marine mammals exploring the ocean pole to pole project meop fig 2b treasure et al 2017 as well as population densities of antarctic krill and salps surveyed in 1926 2016 by 10 countries krillbase atkinson et al 2017 for the antarctic coast data include 204 areas classified as important bird areas iba harris et al 2016 and populations of 46 major emperor penguin colonies identified using satellite imagery fig 2b fretwell et al 2012 3 7 environmental management data this discipline focuses on specially designated areas for environmental protection purposes included are the antarctic specially protected areas aspas terauds 2016 and antarctic specially managed areas asmas https www ats aq devph en apa database search apa results adopted by the parties to the antarctic treaty under the provisions of annex v to the protocol on environmental protection to the antarctic treaty also included are the convention for the conservation of antarctic marine living resources ccamlr s marine protected areas mpas small scale management units ssmus and small scale research units ssrus all available from https gis ccamlr org home ccamlrgis as well as 16 antarctic conservation biogeographic regions acbrs that cover most ice free areas in antarctica terauds et al 2012 terauds and lee 2016 3 8 geology data geological data layers include add s high resolution rock outcrop burton johnson et al 2016 undersea geomorphic features post et al 2014 a schematic geological map tingey 1991 tectonic plate boundaries fig 2c bird 2003 and epicentres of earthquakes since 1900 fig 2c https earthquake usgs gov earthquakes search this discipline has three more layers the first layer has attributes of the data source at each location of multibeam seafloor survey data included in ibcso s bathymetry terrain model arndt et al 2013 the second layer is locations of rock and sediment samples available from the byrd polar and climate research center s rock repository at ohio state university https research bpcrc osu edu rr users can learn about the sample by clicking a feature and can request a physical sample the third layer shows the coverage of geological maps used for scar s ongoing geomap project to compile numerous geological maps cox et al 2019 these three layers provide additional use to geologists rather than presenting established knowledge for a wide audience nonetheless these layers are most useful when displayed together with other datasets and act as important tools in qa to explore a diverse array of potential projects 3 9 geophysics data geophysics data include two gravity field and one magnetic field datasets the first gravity field one gives 10 km resolution free air and bouguer gravity anomalies it was developed by the international association of geodesy s iag subcommission 2 4f gravity and geoid in antarctica antgg using ground based airborne and shipborne data fig 3a scheinert et al 2016 above the anomalies a semi transparent layer gives the accuracy estimates as the transparency is assigned in terms of accuracy anomalies are less visible at lower accuracy the second one is a satellite based eigen 6c4 gravity field geoid model available for 60os poleward förste et al 2014 which has lower resolution yet greater geographical coverage than the antgg compilation fig 3a finally the magnetic field dataset is a satellite measured magnetic field with 5 km resolution for 59 90os from scar s antarctic digital magnetic anomaly project admap golynsky et al 2013 golynsky et al 2001 also included are magnetic pole locations since 1590 and present day magnetic declination contours which may be used in fieldwork chulliat et al 2014 for sub surface geothermal flux the geophysics discipline includes one dataset inferred from seismic velocity an et al 2015 which adds to the two datasets in glaciology s albmap compilation section 3 10 3 10 glaciology and ice core data glaciological and ice core data have the largest data volume in qa3 data are provided on grounding lines bindschadler et al 2011 hydrostatic lines bindschadler et al 2011 two sets of major drainage boundaries http icesat4 gsfc nasa gov cryo data ant grn drainage systems php and rignot et al 2013 and an inventory of coastal ice rises and rumples matsuoka et al 2015 all of which give the basic form of the antarctic ice sheet for the grounding and hydrostatic lines colours indicate the level of confidence in their locations looking to the past paleo ice sheet extents are shown at four epochs since the last glacial maximum together with three levels of confidence bentley et al 2014 a 450 m resolution surface ice flow field derived from satellite data is shown in a raster and also as flow vectors fig 3b mouginot et al 2012 rignot et al 2011 flow speed uncertainty appears in a semi transparent layer consistent with those given for the gravity layers section 3 9 differential roles of ice shelves in stabilizing the grounding line and inland ice sheet are modelled at the 1 km resolution fig 2d furst et al 2016 melting at the base of the ice sheet is represented by about 130 satellite detected active subglacial lakes which are filled and drained repeatedly over months to years smith et al 2009 and about 250 radar detected subglacial lakes fig 2d bell et al 2007 carter et al 2007 studinger et al 2003 wright and siegert 2012 at some locations the modelled subglacial water flux le brocq et al 2013 shows the meltwater network that connects these lakes and drains to the ocean other data layers are modelled surface firn density with depths at which the firn density reaches about 60 and 90 of the pure ice density ligtenberg et al 2011 satellite observed 5 km resolution surface melt flux averaged between 1999 and 2009 trusel et al 2013 and satellite observed blue ice areas hui et al 2014 albmap is a compilation of glaciological datasets le brocq et al 2010 that include satellite observed and modelled surface mass balance arthern et al 2006 van den broeke et al 2006 modelled surface temperature comiso 2000 and firn thickness van den broeke 2008 two geothermal flux datasets inferred each from global seismic models shapiro and ritzwoller 2004 and satellite measured magnetic fields fox maule et al 2005 and newly generated ice and bed topography datasets these individual datasets originally have different spatial resolutions extents and geographic projections albmap modifies these original datasets and provides them with 5 km resolution to be used as boundary conditions for ice flow modelling we include albmap in qa as a grouped set of layers qa3 has an ice core database that includes the location depth and reported literature of 241 ice cores around antarctica which were reported by itase icereader http www icereader org icereader listdata jsp climate change institute antarctic ice core data http cci icecoredata org antarctica html and wais divide ice core project fudge et al 2013 also included is a dataset of surface mass balance measured at 3236 sites together with metadata for each site showing methods and the year of ice core retrieval favier et al 2013 this measured surface mass balance dataset complements model output layers included in the atmosphere discipline section 3 5 isotopic compositions of snow with a complete attribute table are reported at 1279 locations which are presented using progressive symbol colours for δ18o of water stable isotope ratios a proxy for the local temperature touzeau et al 2016 3 11 oceanography data qa3 includes oceanography data to 40os so that the subantarctic ocean front and even most of the subtropical ocean front are included most oceanography data layers are taken from the world ocean atlas 2013 woa including temperature locarnini et al 2013 salinity zweng et al 2013 and concentrations of oxygen garcia et al 2014a silicate phosphate and nitrate garcia et al 2014b at the surface and three depths 50 200 and 500 m both in the summer and winter fig 1 all woa data layers are gridded every 25 km other oceanography datasets include five ocean fronts southern antarctic circumpolar current front southern boundary of the antarctic circumpolar current polar front subantarctic front and subtropical front and a grid of mean surface current speed with the 16 km resolution that is obtained using observation data assimilation techniques mazloff et al 2010 3 12 sea ice data for sea ice data qa3 has satellite observed monthly median sea ice extent between 1981 and 2010 fetterer et al 2017 to indicate historical changes and seasonal variability satellite observed 25 km resolution sea ice concentrations in september maximum coverage in early austral summer and february minimum coverage are included for each year from 2007 to 2017 fetterer et al 2016 the proportion of time the ocean is covered by sea ice with a concentration of 85 or higher is mapped at 6 km resolution using satellite data from 2002 to 2011 spreen et al 2008 3 13 social sciences data data here include human footprints in antarctica since the early stage of expeditions routes and tracks of six historic antarctic expeditions are presented including the first circumnavigation of antarctica by fabian gottlieb von bellingshausen in 1819 1821 tracks to the south pole by roald amundsen and robert falcon scott in early 1910s and the first trans antarctic flight by lincoln ellsworth in 1935 dater 1975 historic stations headland 2009 as well as historic sites and monuments included in add are presented these include 103 stations on the continent and 36 more on surrounding islands developed by 24 nations between the first international polar year 1882 1883 and its fourth one in 2007 2009 human activities in antarctica have increased dramatically in recent decades through research expeditions and tourism consequently non native species were brought to terrestrial antarctica locations and brief descriptions of 39 documented biological invasion cases in terrestrial antarctica are presented here hughes and pertierra 2016 4 discussion 4 1 user applications quantarctica is a multidisciplinary knowledge base for antarctica the southern ocean and sub antarctic islands it has given researchers students educators and logistics operators a single easy to use platform to view analyse and synthesize antarctic datasets perkel 2018 qa3 is distributed with the cc by4 0 license so users can develop their own gis environment using qa as its basis and visually present their own data with qa as the graphical basis in many scientific publications over various disciplines such as quaternary science andersen et al 2020 oceanography schiaparelli and aliani 2019 behavioural ecology schiaparelli and aliani 2019 and microorganisms hirose et al 2020 cautions are needed however because some datasets have different licenses terms of use and attribution requirements which are documented in the metadata qa can be freely distributed and thus it is hard to know the actual number of users but we know of users from all scar member countries including those currently in the early stages of developing full scale antarctic programs the scar expert group on antarctic biodiversity informatics has developed a r package quantarcticr https github com scar quantarcticr that provides access to qa datasets for r users without needing qgis to be installed with the success of qa a similar gis data package is being developed for the greenland ice sheet qgreenland https qgreenland org many users import non qa datasets to develop their own gis workspaces based on qa qgis provides numerous tools and plugins to help analyse qa datasets the identify features tool provides detailed information on selected vector data points e g red profile in fig 2b or cell values of selected raster layers the attribute tables list information that can be sorted or filtered by field properties some commonly used analytical tools include polygonization of raster vector datasets contour extractions terrain slope and aspect analysis and raster calculation using multiple scientific datasets and terrain models as well as interpolation smoothing and merging of multiple raster datasets e g continental data in qa and user s own data in a smaller region the profile tool displays the transect of raster values along custom line segments fig 3a qgis also allows 3d viewing fig 3b selected datasets can be draped over three dimensional representations of a terrain model with various angles and vertical exaggerations qgis has many other tools and plugins due to its modest computational requirements qa can be easily used on laptops during antarctic fieldwork before deploying to the field qa s data layers satellite imagery and basemaps can be used for the first order safety assessments in addition users can import recent satellite imagery and analyse them on the qa workspace to define hazardous crevasse areas and design traverse routes fig 3c qgis also works to plot the user s current location using a handheld gps receiver this capability increases safety and speed during field traverses during fieldwork researchers can gain a deeper understanding of a region by displaying qa s included package of scientific data and satellite imagery 4 2 quantarctica friendly datasets the number of data layers and the data volume in the qa package are limited to those with a large spatial coverage and useful for multidisciplinary users so that users can easily browse data outside of their own disciplines however individual users working in a specific region or specific discipline need more detailed data to improve user experience and promote data sharing we encourage data providers to follow our recommendations and mandatory guidelines ftp ftp quantarctica npolar no quantarctica3 making 20quantarctica friendly 20datasets pdf those datasets that meet these criteria called quantarctica friendly datasets are hosted by the data providers and download links and short descriptions are listed at qa s project web site https www npolar no quantarctica mandatory guidelines are file formats esri shapefile for vector and geotiff or if needed jpeg2000 or jpeg for raster layer style file qml format metadata file qmd format and txt file with only the meta data abstract for convenience and clarity and projection epsg 3031 also we recommend the following 1 specific layer styles and labelling to visualize the data compatible with qa s datasets 2 image pyramids of large raster data for faster rendering and 3 raster data compression preferably using lossless lzw or deflate compression but also if needed using lossy compression such as jpeg2000 for large volume data datasets currently listed as quantarctica friendly datasets include region specific bed topography data in the weddell sea jeofry et al 2018 electronic navigational chart coverage and tide records as a part of gis service maintained by the hydrographic commission of antarctica under the international hydrographic organization https data iho opendata arcgis com and modelled permafrost temperature validated with observations in 2000 2017 obu et al 2020 4 3 qgis functionality in polar regions in our development of qa we contributed to the development of the following three functions of qgis that benefit qa users as well as qgis users in the arctic 1 print layout can now display the north arrow pointing to true north not to the grid north in polar coordinate systems https www qgis org en site forusers visualchangelog218 index html feature true north arrows this is particularly useful in maps of small regions 2 qgis can now render select and edit on the fly projected vector data when on the fly reprojection is enabled for epsg 3031 and other polar stereographic projections https issues qgis org issues 7596 change 64937 this problem occurred when users loaded their own data not projected to epsg 3031 to qa s workplace as a test before re projecting their data to epsg 3031 3 the garmincustommap plugin is now ported from qgis2 to qgis3 https plugins qgis org plugins garmincustommap version 3 0 this plugin enables the user to export what is visible on the map canvas in qgis as a raster image file to the kmz file format which can be loaded on most modern garmin handheld gps units this function is commonly used during fieldwork 4 4 outlook the scientific disciplines included in qa are not comprehensive for example data for upper atmosphere research and astronomy are absent in the current version as are antarctic ice sheet mass balance estimates and trends as new datasets with higher precision and resolution become available adding new datasets and replacing obsolete datasets are necessary to provide a state of the art knowledge base to the entire antarctic and southern ocean community we seek to maintain qa as a balanced product in terms of portability coverage and user friendliness 5 conclusions quantarctica https www npolar no quantarctica is an integrated mapping environment for antarctica the southern ocean and sub antarctic islands freely available with the cc by4 0 license with doi https doi org 10 21334 npolar 2018 8516e961 quantarctica works with qgis software version 3 16 on multiple platforms without an internet connection it is composed of 265 data layers in simple and detailed basemaps satellite imagery terrain models and scientific data in nine disciplines of atmosphere biology environment management geology geophysics glaciology and ice cores oceanography sea ice and social science giving the total data size of 7 53 gb we will maintain quantarctica by adding new datasets and replacing obsolete datasets to provide a state of the art knowledge base in a balanced form in terms of portability coverage and user friendliness author contributions kenichi matsuoka and anders skoglund co founded quantarctica and have led its development from the beginning george roth was a project coordinator for qa version 3 assistance with the development of other versions was provided by angela von deschwanden version 1 césar deschamps berger version 2 and brice van liefferinge version 3 2 stein tronstad and yngve melvær promoted synergies with scar s data management committee scadm and scar s geographic information scagi respectively the other authors are editorial board members for version 3 acting to recommend scientific data for each discipline and enhancing the overall quality of the product jean de pomereu social science huw griffiths biology robert headland social science brad herried miscellaneous base layers katsuro katsumata oceanography anne le brocq glaciology kathy licht geology fraser morgan environment management peter d neff ice cores catherine ritz glaciology mirko scheinert geophysics takeshi tamura sea ice anton van de putte biology and michiel van den broeke atmospheric science all authors except for the deceased angela von deschwanden contributed to the development of this paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements development of quantarctica version 3 was funded by the norwegian ministry of foreign affairs grant number qza 15 0332 quantarctica became highly visible and widely used in the antarctic community thanks to scar s endorsement of qa as a scar product and promotions made by association of polar early career scientists apecs and scar s scientific research programs action groups and expert groups as well as scadm and scagi the norwegian environment agency tasmanian partnership for advanced computing arctic and antarctic data archive system at the national institute of polar research national center for polar and ocean research and polar geospatial center provide distributed global download capabilities the radarsat 2 image used in fig 3c provided by nsc ksat under the norwegian canadian radarsat agreement we acknowledge all data providers who supported quantarctica 
25833,quantarctica https www npolar no quantarctica is a geospatial data package analysis environment and visualization platform for the antarctic continent southern ocean 40os and sub antarctic islands quantarctica works with the free cross platform geographical information system gis software qgis and can run without an internet connection making it a viable tool for fieldwork in remote areas the data package includes basemaps satellite imagery terrain models and scientific data in nine disciplines including physical and biological sciences environmental management and social science to provide a clear and responsive user experience cartography and rendering settings are carefully prepared using colour sets that work well for typical data combinations and with consideration of users with common colour vision deficiencies metadata included in each dataset provides brief abstracts for non specialists and references to the original data sources thus quantarctica provides an integrated environment to view and analyse multiple antarctic datasets together conveniently and with a low entry barrier keywords geographical information system regional analytical tool polar regions product availability the product is released under the cc by4 0 license with doi https doi org 10 21334 npolar 2018 8516e961 quantarctica is downloadable from the norwegian polar institute ftp ftp quantarctica npolar no as well as from global mirrors generously provided by the tasmanian partnership for advanced computing at the university of tasmania ftp quantarctica tpac org au the arctic and antarctic data archive system at japan s national institute of polar research https ads nipr ac jp gis quantarctica india s national center for polar and ocean research ftp ftp ncaor gov in and the polar geospatial center at the university of minnesota ftp ftp data pgc umn edu gis packages quantarctica all download sites and quantarctica friendly datasets see section 4 2 are listed at quantarctica s project web page at the norwegian polar institute https www npolar no quantarctica 1 introduction the antarctic ice sheet and southern ocean are major thermal reservoirs in the earth system that significantly influence climate change kennicutt et al 2015 2019 as the amount and availability of scientific data in this region grows and the needs of more interdisciplinary research are realized kennicutt et al 2014 researchers and students increasingly require an easy to use unified platform to import display and analyse geospatial datasets for work involving this region educators and logistics operators also need a convenient platform with low entry barrier to get a complete picture of these remote areas as well as develop practical tools for their work many such users develop their own dedicated analysis environments for individual purposes bringing together basic data such as terrain models satellite imagery and scientific data from different disciplines unfortunately this entails significant start up cost and duplicated effort basic antarctic datasets such as the antarctic digital database add https www add scar org and the southern ocean observation system map soosmap http www soosmap aq exist as online portals one can download satellite data for antarctica from the earliest such data decades ago through to the present e g nasa s world view https worldview earthdata nasa gov polar view https www polarview aq antarctic and the us geological survey s earthexplorer https earthexplorer usgs gov in addition the antarctic environmental portal https www environments aq provides an important link between antarctic science and antarctic policy particularly in environmental management however being online portals all these data sources are not immediately helpful during remote field work or a scientific cruise one exception is antarctic mapping tools greene et al 2017 which provides antarctic data that can be operated offline however it is limited mostly to physical science data and its analysis functions require proprietary matlab computing software thus instead of having to gather antarctic scientific data from numerous online data portals in many file formats and geographic projections a user friendly system would provide pre compiled basic antarctic datasets in multiple disciplines and load consistently both in the office and in the field for this purpose the base software platform should be low cost and multiple platform operational with little or no license restrictions to maximize benefits for the rapidly growing antarctic research community in 2011 we began developing a gis data package that can operate with modest computer capabilities so that the system is useful at remote field camps and for a wide range of users we selected quantum gis renamed to qgis in 2013 https qgis org for the software platform and named the data package quantarctica hereafter qa version 1 of qa was released in 2012 for version 2 released in 2014 we improved the glaciology and geophysics data coverage and data visualization the scientific committee on antarctic research scar a thematic organization of the international science council selected qa version 2 as a scar map product in 2014 https scar org resources maps to respond to the antarctic scientific community s increased interest in qa we decided to develop version 3 qa3 with scientific data from other disciplines and expand its geographical coverage from 50os to 40os to include research in the southern ocean and sub antarctic islands qa3 was released in february 2018 for use with qgis version 2 as our integration tests were completed we updated qa3 to version 3 2 in 2021 which runs on qgis version 3 16 fig 1 here we describe methods used to develop qa3 present the current contents of qa3 demonstrate user applications and finally present its outlook 2 methods 2 1 choice of software to choose the base gis software for which our data package is optimized the following criteria were used the software should 1 be free of charge or at least low cost 2 work on the most common computer operating systems 3 be able to function without an internet connection offline readiness 4 be relatively easy to use 5 have a rich toolset 6 be able to read and write common geographical data file formats 7 allow advanced cartography 8 allow good map figure production 9 be actively maintained and in development 10 have a process that allows transparent bug fixing processes 11 have an active user community and rich knowledge resources and 12 be available for bundling with the qa package e g in a usb stick for standalone offline use we found quantum gis renamed to qgis in 2013 https qgis org to satisfy these requirements best 2 2 selection of data to select data used for basemaps miscellaneous base layers satellite data and terrain models we considered antarctic map products widely available as best suited for the purpose in the combined terms of coverage completeness and detail sections 3 1 3 4 to select the scientific data we first surveyed the relevant scientific community in 2016 to identify community priorities for included datasets this input was considered by one or two editors for each discipline and then the recommended datasets were further screened by the core development team at the norwegian polar institute qa3 includes 265 data layers of which 164 layers represent scientific datasets tables 1 and 2 these selections will be re visited when qa version 4 is developed in the future 2 3 mapping projection the antarctic polar stereographic projection epsg 3031 was chosen because it is both antarctic centric and the most commonly used projection for antarctic geographical data already some datasets are originally in other projections qgis can handle multiple data layers projected to different systems nonetheless on the fly reprojection hampers fast rendering and demands cpu work therefore all data in qa are projected to the single projection epsg 3031 2 4 file formats compression and data re sampling individual datasets are originally in various file formats for consistency we convert all vector data to esri shapefile format and raster data to geotiff format most raster files are compressed with lossless lzw and deflate algorithms these file formats are commonly used in the gis community and are supported in most other gis systems however including high resolution imagery in a lossless format largely decreases qa s portability so we converted them to the jpeg format with associated georeferenced files or to the jpeg2000 format with a compression level that provides a hardly noticeable quality reduction section 3 3 these formats are less common in the gis community but still supported widely in this way datasets in qa can be used in other platforms as well except for re projection the original data are not altered 2 5 cartography the presentation of data layers is carefully adjusted to be intuitive readable and visually pleasant at all map scales the scientific datasets are presented such as to stand out when rendered on the top of basemaps and satellite imagery the presentation is also made so that datasets often viewed together are easily distinguishable from one another for colour deficient users if the predefined cartography is not fully suited for a specific purpose qa is fully customizable so users can change the appearance many data layers particularly the detailed basemaps use scale dependent features so that these layers are rendered to an appropriate level of details at a given scale fig 1 most data layers constituting detailed basemaps are stored at high medium and low resolutions for high performance rendering and for best cartographic expression 2 6 meta data metadata includes a brief description of the dataset for general use also included are original data location citation and the editor responsible for the data layer we urge all users to cite this original data source not qa when they use the dataset in their work in qa version 3 2 all metadata are stored as a part of the qgis project file qgs file format and in each data folder associated with individual data layers in the qmd file format the abstract section of the metadata brief description of the data citation and handling editor is stored as plain text txt files for quick reference the latter is useful when users are interested in using specific data layers on different platforms such as arcgis and non gis platforms such as computing software r https www r project org 3 results qa3 includes datasets categorized as 1 simple and detailed basemaps 2 miscellaneous base layers 3 satellite imagery 4 terrain models and 5 a range of scientific data table 1 users can display and rearrange the order of data layers that lay on top of basemaps satellite imagery and terrain models at a user chosen continuously adjustable map scale fig 1 scientific data are categorized into nine disciplines table 2 all data layers are stored with a folder tree structure so that individual data files and associated metadata can be easily found table 3 lists all data included in qa3 the total size of qa3 s data files is 7 53 gb with the satellite imagery and terrain models constituting about 4 13 gb table 1 our distribution package with qgis version 3 16 1 is 8 65 gb we provide a start up manual to adjust qgis to the quantarctica workspace ftp ftp quantarctica npolar no quantarctica3 quantarctica getstarted pdf which typically takes 0 5 1 h for first time users to follow after qgis and quantarctica are installed 3 1 simple and detailed basemaps simple basemaps allow users to quickly render oceans ice sheet ice shelves and other continents primarily composed of the add data layers fig 2b inset detailed basemaps are a composite of vector layers of ice and rock surface elevation contours outcrops moraines lakes and streams ice shelves calving front as well as raster layers of both bathymetry and topographic hillshade these basemaps use the add and several terrain models figs 1 2b and 2c and 2d as users zoom in features appear with a higher resolution to balance the level of detail necessary for a given scale and rendering speed 3 2 miscellaneous base layers additional base layers show other scale dependent features such as the 37 628 place names south of 60os registered in the scar composite gazetteer of antarctica https data aad gov au aadc gaz scar further north the names come from other reliable resources and shown in the language of the sovereignty some features are left unlabelled when the sovereignty might be disputed other layers show latitude longitude lines the antarctic circle south pole universal transverse mercator utm zones and locations and facility details of 108 research stations from a list of antarctic facilities maintained by the council of managers of national antarctic programs comnap fig 1 users can filter and search for place names and features using attribute tables an overview place name layer gives a quick reference to large regions in a small map scale 3 3 satellite imagery qa3 includes three sets of satellite imagery one for the landsat images users can view the 240 m resolution landsat image mosaic of antarctica lima taken in 1999 2002 that covers the entire ice sheet and ice shelves bindschadler et al 2008 or view the 15 m resolution landsat image tiles taken in 2013 2017 over islands outcrops ice shelves and the ice sheet except for regions 82 7os poleward two the radarsat antarctic mapping project s ramp 100 m resolution mosaic covers the entire ice sheet and ice shelves taken in 1997 jezek et al 2013 three is the moderate resolution image spectroradiometer modis mosaic of antarctica moa 125 m resolution image mosaic taken in 2003 2004 that covers the entire ice sheet and ice shelves haran et al 2014 scambos et al 2007 these specific satellite products allow users to optimize the display of overall topographic features lima ramp moa areas of blue ice and outcrops lima and sub surface features such as crevasses under snow detected using ice penetrating microwaves ramp to balance image resolutions and portability of qa we converted lima to the jpeg format with associated georeferenced files and ramp and individual landsat tiles to the jpeg2000 format with a compression level that provides a hardly noticeable quality reduction to increase the rendering performance and user friendliness more than 654 individual landsat image tiles are combined to a single virtual mosaic file with image pyramids so that lower resolution versions are rendered when zoomed out 3 4 terrain models six terrain models are included with spatial resolutions ranging from 0 2 to 2 0 km specifically the ramp2 liu et al 2015 and cryosat 2 helm et al 2014 terrain models as well as the add elevation contours represent the surface of the outcrops ice sheet and ice shelves figs 2a and 3b the bedmap2 compilation represents the ice thickness and bed topography under the ice sheet fretwell et al 2013 bedmap2 shows uncertainties in ice thickness and bed elevation and geographic coverage and corresponding data sparse regions of the ice penetrating radar data with which ice thickness and bed elevation were determined over the ice sheet the international bathymetry chart of the southern ocean ibcso shows the bathymetry of the southern ocean at latitudes south of 60os as well as metadata on the availability and type of data used for the compilation e g multibeam or single beam fig 2c arndt et al 2013 finally a global etopo1 relief model applies to latitudes north of 60os noaa national geophysical data center 2009 elevation references differ between the terrain models cryosat 2 and ramp2 refer to the wgs84 ellipsoid whereas the others refer to the mean sea level using different geoid models similarly each terrain model has different features with different strengths and weaknesses so users can choose the dataset that best matches their needs all models are displayed as rasters derived hillshades and elevation contours contour intervals and hillshade parameters can be customized in qgis to better view highly variable terrains such as continental shelf breaks abyssal plains steep mountainous regions near the coast and virtually flat and featureless inland ice sheet over a range of spatial scales 3 5 atmosphere data most atmospheric data provided in qa are outputs from racmo2 a regional atmospheric climate model van wessem et al 2014a 2014b these include near surface data such as 2 m high temperature and 10 m high wind speeds over the ice sheet and ocean fig 2a as well as surface mass balance over the ice sheet and ice shelves all annual values averaged between 1971 and 2011 the 27 km resolution racmo2 raster datasets are originally projected in a rotated polar coordinate system which cannot easily be converted to epsg 3031 however each model cell has epsg 4326 coordinate values as well which we used to build up epsg 3031 rasters at 35 km resolution the best achievable without creating void data cells anywhere in between atmospheric data also include a dataset of satellite observed wind scour zones over the ice sheet fig 2a das et al 2013 these near surface and surface data have been selected over middle and upper atmosphere data because of their stronger influences on processes in the other disciplines 3 6 biology data biology datasets for the southern ocean include summer chlorophyll a density near the ocean surface fig 2b johnson et al 2013 johnson et al 2017 20 pelagic regions based on seawater temperature and sea ice distribution raymond 2014 and 29 benthic regions douglass et al 2014 on top of these raster or polygon data layers users can plot locations of available vertical profiles of temperature and salinity collected by the marine mammals exploring the ocean pole to pole project meop fig 2b treasure et al 2017 as well as population densities of antarctic krill and salps surveyed in 1926 2016 by 10 countries krillbase atkinson et al 2017 for the antarctic coast data include 204 areas classified as important bird areas iba harris et al 2016 and populations of 46 major emperor penguin colonies identified using satellite imagery fig 2b fretwell et al 2012 3 7 environmental management data this discipline focuses on specially designated areas for environmental protection purposes included are the antarctic specially protected areas aspas terauds 2016 and antarctic specially managed areas asmas https www ats aq devph en apa database search apa results adopted by the parties to the antarctic treaty under the provisions of annex v to the protocol on environmental protection to the antarctic treaty also included are the convention for the conservation of antarctic marine living resources ccamlr s marine protected areas mpas small scale management units ssmus and small scale research units ssrus all available from https gis ccamlr org home ccamlrgis as well as 16 antarctic conservation biogeographic regions acbrs that cover most ice free areas in antarctica terauds et al 2012 terauds and lee 2016 3 8 geology data geological data layers include add s high resolution rock outcrop burton johnson et al 2016 undersea geomorphic features post et al 2014 a schematic geological map tingey 1991 tectonic plate boundaries fig 2c bird 2003 and epicentres of earthquakes since 1900 fig 2c https earthquake usgs gov earthquakes search this discipline has three more layers the first layer has attributes of the data source at each location of multibeam seafloor survey data included in ibcso s bathymetry terrain model arndt et al 2013 the second layer is locations of rock and sediment samples available from the byrd polar and climate research center s rock repository at ohio state university https research bpcrc osu edu rr users can learn about the sample by clicking a feature and can request a physical sample the third layer shows the coverage of geological maps used for scar s ongoing geomap project to compile numerous geological maps cox et al 2019 these three layers provide additional use to geologists rather than presenting established knowledge for a wide audience nonetheless these layers are most useful when displayed together with other datasets and act as important tools in qa to explore a diverse array of potential projects 3 9 geophysics data geophysics data include two gravity field and one magnetic field datasets the first gravity field one gives 10 km resolution free air and bouguer gravity anomalies it was developed by the international association of geodesy s iag subcommission 2 4f gravity and geoid in antarctica antgg using ground based airborne and shipborne data fig 3a scheinert et al 2016 above the anomalies a semi transparent layer gives the accuracy estimates as the transparency is assigned in terms of accuracy anomalies are less visible at lower accuracy the second one is a satellite based eigen 6c4 gravity field geoid model available for 60os poleward förste et al 2014 which has lower resolution yet greater geographical coverage than the antgg compilation fig 3a finally the magnetic field dataset is a satellite measured magnetic field with 5 km resolution for 59 90os from scar s antarctic digital magnetic anomaly project admap golynsky et al 2013 golynsky et al 2001 also included are magnetic pole locations since 1590 and present day magnetic declination contours which may be used in fieldwork chulliat et al 2014 for sub surface geothermal flux the geophysics discipline includes one dataset inferred from seismic velocity an et al 2015 which adds to the two datasets in glaciology s albmap compilation section 3 10 3 10 glaciology and ice core data glaciological and ice core data have the largest data volume in qa3 data are provided on grounding lines bindschadler et al 2011 hydrostatic lines bindschadler et al 2011 two sets of major drainage boundaries http icesat4 gsfc nasa gov cryo data ant grn drainage systems php and rignot et al 2013 and an inventory of coastal ice rises and rumples matsuoka et al 2015 all of which give the basic form of the antarctic ice sheet for the grounding and hydrostatic lines colours indicate the level of confidence in their locations looking to the past paleo ice sheet extents are shown at four epochs since the last glacial maximum together with three levels of confidence bentley et al 2014 a 450 m resolution surface ice flow field derived from satellite data is shown in a raster and also as flow vectors fig 3b mouginot et al 2012 rignot et al 2011 flow speed uncertainty appears in a semi transparent layer consistent with those given for the gravity layers section 3 9 differential roles of ice shelves in stabilizing the grounding line and inland ice sheet are modelled at the 1 km resolution fig 2d furst et al 2016 melting at the base of the ice sheet is represented by about 130 satellite detected active subglacial lakes which are filled and drained repeatedly over months to years smith et al 2009 and about 250 radar detected subglacial lakes fig 2d bell et al 2007 carter et al 2007 studinger et al 2003 wright and siegert 2012 at some locations the modelled subglacial water flux le brocq et al 2013 shows the meltwater network that connects these lakes and drains to the ocean other data layers are modelled surface firn density with depths at which the firn density reaches about 60 and 90 of the pure ice density ligtenberg et al 2011 satellite observed 5 km resolution surface melt flux averaged between 1999 and 2009 trusel et al 2013 and satellite observed blue ice areas hui et al 2014 albmap is a compilation of glaciological datasets le brocq et al 2010 that include satellite observed and modelled surface mass balance arthern et al 2006 van den broeke et al 2006 modelled surface temperature comiso 2000 and firn thickness van den broeke 2008 two geothermal flux datasets inferred each from global seismic models shapiro and ritzwoller 2004 and satellite measured magnetic fields fox maule et al 2005 and newly generated ice and bed topography datasets these individual datasets originally have different spatial resolutions extents and geographic projections albmap modifies these original datasets and provides them with 5 km resolution to be used as boundary conditions for ice flow modelling we include albmap in qa as a grouped set of layers qa3 has an ice core database that includes the location depth and reported literature of 241 ice cores around antarctica which were reported by itase icereader http www icereader org icereader listdata jsp climate change institute antarctic ice core data http cci icecoredata org antarctica html and wais divide ice core project fudge et al 2013 also included is a dataset of surface mass balance measured at 3236 sites together with metadata for each site showing methods and the year of ice core retrieval favier et al 2013 this measured surface mass balance dataset complements model output layers included in the atmosphere discipline section 3 5 isotopic compositions of snow with a complete attribute table are reported at 1279 locations which are presented using progressive symbol colours for δ18o of water stable isotope ratios a proxy for the local temperature touzeau et al 2016 3 11 oceanography data qa3 includes oceanography data to 40os so that the subantarctic ocean front and even most of the subtropical ocean front are included most oceanography data layers are taken from the world ocean atlas 2013 woa including temperature locarnini et al 2013 salinity zweng et al 2013 and concentrations of oxygen garcia et al 2014a silicate phosphate and nitrate garcia et al 2014b at the surface and three depths 50 200 and 500 m both in the summer and winter fig 1 all woa data layers are gridded every 25 km other oceanography datasets include five ocean fronts southern antarctic circumpolar current front southern boundary of the antarctic circumpolar current polar front subantarctic front and subtropical front and a grid of mean surface current speed with the 16 km resolution that is obtained using observation data assimilation techniques mazloff et al 2010 3 12 sea ice data for sea ice data qa3 has satellite observed monthly median sea ice extent between 1981 and 2010 fetterer et al 2017 to indicate historical changes and seasonal variability satellite observed 25 km resolution sea ice concentrations in september maximum coverage in early austral summer and february minimum coverage are included for each year from 2007 to 2017 fetterer et al 2016 the proportion of time the ocean is covered by sea ice with a concentration of 85 or higher is mapped at 6 km resolution using satellite data from 2002 to 2011 spreen et al 2008 3 13 social sciences data data here include human footprints in antarctica since the early stage of expeditions routes and tracks of six historic antarctic expeditions are presented including the first circumnavigation of antarctica by fabian gottlieb von bellingshausen in 1819 1821 tracks to the south pole by roald amundsen and robert falcon scott in early 1910s and the first trans antarctic flight by lincoln ellsworth in 1935 dater 1975 historic stations headland 2009 as well as historic sites and monuments included in add are presented these include 103 stations on the continent and 36 more on surrounding islands developed by 24 nations between the first international polar year 1882 1883 and its fourth one in 2007 2009 human activities in antarctica have increased dramatically in recent decades through research expeditions and tourism consequently non native species were brought to terrestrial antarctica locations and brief descriptions of 39 documented biological invasion cases in terrestrial antarctica are presented here hughes and pertierra 2016 4 discussion 4 1 user applications quantarctica is a multidisciplinary knowledge base for antarctica the southern ocean and sub antarctic islands it has given researchers students educators and logistics operators a single easy to use platform to view analyse and synthesize antarctic datasets perkel 2018 qa3 is distributed with the cc by4 0 license so users can develop their own gis environment using qa as its basis and visually present their own data with qa as the graphical basis in many scientific publications over various disciplines such as quaternary science andersen et al 2020 oceanography schiaparelli and aliani 2019 behavioural ecology schiaparelli and aliani 2019 and microorganisms hirose et al 2020 cautions are needed however because some datasets have different licenses terms of use and attribution requirements which are documented in the metadata qa can be freely distributed and thus it is hard to know the actual number of users but we know of users from all scar member countries including those currently in the early stages of developing full scale antarctic programs the scar expert group on antarctic biodiversity informatics has developed a r package quantarcticr https github com scar quantarcticr that provides access to qa datasets for r users without needing qgis to be installed with the success of qa a similar gis data package is being developed for the greenland ice sheet qgreenland https qgreenland org many users import non qa datasets to develop their own gis workspaces based on qa qgis provides numerous tools and plugins to help analyse qa datasets the identify features tool provides detailed information on selected vector data points e g red profile in fig 2b or cell values of selected raster layers the attribute tables list information that can be sorted or filtered by field properties some commonly used analytical tools include polygonization of raster vector datasets contour extractions terrain slope and aspect analysis and raster calculation using multiple scientific datasets and terrain models as well as interpolation smoothing and merging of multiple raster datasets e g continental data in qa and user s own data in a smaller region the profile tool displays the transect of raster values along custom line segments fig 3a qgis also allows 3d viewing fig 3b selected datasets can be draped over three dimensional representations of a terrain model with various angles and vertical exaggerations qgis has many other tools and plugins due to its modest computational requirements qa can be easily used on laptops during antarctic fieldwork before deploying to the field qa s data layers satellite imagery and basemaps can be used for the first order safety assessments in addition users can import recent satellite imagery and analyse them on the qa workspace to define hazardous crevasse areas and design traverse routes fig 3c qgis also works to plot the user s current location using a handheld gps receiver this capability increases safety and speed during field traverses during fieldwork researchers can gain a deeper understanding of a region by displaying qa s included package of scientific data and satellite imagery 4 2 quantarctica friendly datasets the number of data layers and the data volume in the qa package are limited to those with a large spatial coverage and useful for multidisciplinary users so that users can easily browse data outside of their own disciplines however individual users working in a specific region or specific discipline need more detailed data to improve user experience and promote data sharing we encourage data providers to follow our recommendations and mandatory guidelines ftp ftp quantarctica npolar no quantarctica3 making 20quantarctica friendly 20datasets pdf those datasets that meet these criteria called quantarctica friendly datasets are hosted by the data providers and download links and short descriptions are listed at qa s project web site https www npolar no quantarctica mandatory guidelines are file formats esri shapefile for vector and geotiff or if needed jpeg2000 or jpeg for raster layer style file qml format metadata file qmd format and txt file with only the meta data abstract for convenience and clarity and projection epsg 3031 also we recommend the following 1 specific layer styles and labelling to visualize the data compatible with qa s datasets 2 image pyramids of large raster data for faster rendering and 3 raster data compression preferably using lossless lzw or deflate compression but also if needed using lossy compression such as jpeg2000 for large volume data datasets currently listed as quantarctica friendly datasets include region specific bed topography data in the weddell sea jeofry et al 2018 electronic navigational chart coverage and tide records as a part of gis service maintained by the hydrographic commission of antarctica under the international hydrographic organization https data iho opendata arcgis com and modelled permafrost temperature validated with observations in 2000 2017 obu et al 2020 4 3 qgis functionality in polar regions in our development of qa we contributed to the development of the following three functions of qgis that benefit qa users as well as qgis users in the arctic 1 print layout can now display the north arrow pointing to true north not to the grid north in polar coordinate systems https www qgis org en site forusers visualchangelog218 index html feature true north arrows this is particularly useful in maps of small regions 2 qgis can now render select and edit on the fly projected vector data when on the fly reprojection is enabled for epsg 3031 and other polar stereographic projections https issues qgis org issues 7596 change 64937 this problem occurred when users loaded their own data not projected to epsg 3031 to qa s workplace as a test before re projecting their data to epsg 3031 3 the garmincustommap plugin is now ported from qgis2 to qgis3 https plugins qgis org plugins garmincustommap version 3 0 this plugin enables the user to export what is visible on the map canvas in qgis as a raster image file to the kmz file format which can be loaded on most modern garmin handheld gps units this function is commonly used during fieldwork 4 4 outlook the scientific disciplines included in qa are not comprehensive for example data for upper atmosphere research and astronomy are absent in the current version as are antarctic ice sheet mass balance estimates and trends as new datasets with higher precision and resolution become available adding new datasets and replacing obsolete datasets are necessary to provide a state of the art knowledge base to the entire antarctic and southern ocean community we seek to maintain qa as a balanced product in terms of portability coverage and user friendliness 5 conclusions quantarctica https www npolar no quantarctica is an integrated mapping environment for antarctica the southern ocean and sub antarctic islands freely available with the cc by4 0 license with doi https doi org 10 21334 npolar 2018 8516e961 quantarctica works with qgis software version 3 16 on multiple platforms without an internet connection it is composed of 265 data layers in simple and detailed basemaps satellite imagery terrain models and scientific data in nine disciplines of atmosphere biology environment management geology geophysics glaciology and ice cores oceanography sea ice and social science giving the total data size of 7 53 gb we will maintain quantarctica by adding new datasets and replacing obsolete datasets to provide a state of the art knowledge base in a balanced form in terms of portability coverage and user friendliness author contributions kenichi matsuoka and anders skoglund co founded quantarctica and have led its development from the beginning george roth was a project coordinator for qa version 3 assistance with the development of other versions was provided by angela von deschwanden version 1 césar deschamps berger version 2 and brice van liefferinge version 3 2 stein tronstad and yngve melvær promoted synergies with scar s data management committee scadm and scar s geographic information scagi respectively the other authors are editorial board members for version 3 acting to recommend scientific data for each discipline and enhancing the overall quality of the product jean de pomereu social science huw griffiths biology robert headland social science brad herried miscellaneous base layers katsuro katsumata oceanography anne le brocq glaciology kathy licht geology fraser morgan environment management peter d neff ice cores catherine ritz glaciology mirko scheinert geophysics takeshi tamura sea ice anton van de putte biology and michiel van den broeke atmospheric science all authors except for the deceased angela von deschwanden contributed to the development of this paper declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements development of quantarctica version 3 was funded by the norwegian ministry of foreign affairs grant number qza 15 0332 quantarctica became highly visible and widely used in the antarctic community thanks to scar s endorsement of qa as a scar product and promotions made by association of polar early career scientists apecs and scar s scientific research programs action groups and expert groups as well as scadm and scagi the norwegian environment agency tasmanian partnership for advanced computing arctic and antarctic data archive system at the national institute of polar research national center for polar and ocean research and polar geospatial center provide distributed global download capabilities the radarsat 2 image used in fig 3c provided by nsc ksat under the norwegian canadian radarsat agreement we acknowledge all data providers who supported quantarctica 
25834,surface water is an irreplaceable resource for human survival and environmental sustainability accurate finely detailed cartographic representations of hydrologic streamlines are critically important in various scientific domains such as assessing the quantity and quality of present and future water resources modeling climate changes evaluating agricultural suitability mapping flood inundation and monitoring environmental changes conventional approaches to detecting such streamlines cannot adequately incorporate information from the complex three dimensional 3d environment of streams and land surface features such information is vital to accurately delineate streamlines in recent years high accuracy lidar data has become increasingly available for deriving both 3d information and terrestrial surface reflectance this study develops an attention u net model to take advantage of high accuracy lidar data for finely detailed streamline detection and evaluates model results against a baseline of multiple traditional machine learning methods the evaluation shows that the attention u net model outperforms the best baseline machine learning method by an average f1 score of 11 25 and achieves significantly better smoothness and connectivity between classified streamline channels these findings suggest that our deep learning approach can harness high accuracy lidar data for fine scale hydrologic streamline detection and in turn produce desirable benefits for many scientific domains keywords cybergis deep learning hydrologic streamlines hydrography lidar data analysis 1 introduction interactions of water within earth s systems have been studied extensively yet increased demand for this vital resource has expanded interest in monitoring and management of water resources accurate finely detailed delineation of surface hydrologic features is crucial for various scientific investigations and water resource applications such as agricultural suitability river dynamics flood mapping landslide risk analysis wetland inventory watershed analysis environmental monitoring and climate modeling to name just a few maidment 2017 poppenga and gesch 2013 schultz et al 2017 simley and carswell 2009 terziotti 2018 wright and nielsen 2012 while other terrain conditions have a role the spatial pattern of a surface water drainage network is largely a reflection of the type and arrangement of subsurface bedrock which can assist with classification and management of land resources clubb and bookhagen 2019 muller and oberlander 1976 therefore the key objective of this research is to understand how to advance machine intelligence for automatic extraction of detailed hydrologic features from high resolution elevation data and other open geospatial datasets yielding important data that can be used for this type of scientific work the national hydrography dataset nhd is a digital database of surface water features of the united states that is managed by the u s geological survey usgs and partner organizations sheng and wilson 2007 simley and carswell 2009 it provides a common reference for regulation research and modeling noaa 2016 the nhd high resolution hr is a multi scale dataset compiled from the best available data sources having scales of 1 24 000 or larger finer detail except in alaska where 1 63 360 or larger scales finer details are used however the quality of hydrographic data that has been compiled from topographic maps which include the nhd is not suitable for certain hydrologic regulatory and engineering purposes because of inconsistent drainage density and missing headwater content caruso 2014 chorley and dale 1972 colson and gregory 2008 colson 2006 fritz et al 2013 russell 2008 headwaters are small streams formed at the upstream extent of a watershed and comprise more than 50 percent of the stream network by length in the united states nadeau and rains 2007 to overcome these issues since 2009 nhd hr is being updated with more detailed hydrography derived from finer scale source information up to 1 2400 simley and carswell 2009 stanislawski 2009 economically these enhanced hydrographic data are expected to generate over 600 million dollars per year in potential benefits to water resource and emergency response managers in addition to the 500 million dollars in annual benefits already being generated from the existing program hoegberg 2016 updating the nhd hr applies the best available digital elevation model dem data which should use quality level 2 ql2 lidar data or better in the conterminous united states heidemann 2018 since 2014 the usgs 3d elevation program 3dep has been coordinating the collection of ql2 or better lidar point cloud data for the united states except for alaska where cloud penetrating interferometric synthetic aperture radar ifsar is being acquired to simplify collection in remote areas lukas et al 2015 ql2 lidar provides an aggregate nominal pulse spacing of less than 1 m m for first returns heidemann 2018 which supports derivation of a 1 meter m resolution dem the detail inherent to this high resolution dem data enables modeling of surface water dynamics from the continental scale to catchment and headwater scales although some methods to improve the nhd hr have been studied lopez torrijos 2018 poppenga et al 2013 sheng et al 2007 stanislawski and survila 2018 extracting accurate and fine scale hydrography from high resolution dem data using traditional flow accumulation methods is a costly and laborious process depending on the selected workflow various sophisticated issues must be handled which include conditioning the dem for flow modeling estimating flow accumulation weights and a minimum contributing area for stream formation along with tailoring solutions to diverse environmental conditions coupled with the fact that multiple methods are available solutions can vary and assessing the accuracy of extracted drainage lines is further complicated by temporal environmental variations procedures generally involve well known automated methods to derive drainage lines from dems anderson 2012 jenson and domingue 1988 maidment and morehouse 2002 metz and mitasova 2011 montgomery and foufoula georgiou 1993 o callaghan and mark 1984 passalacqua and belmont 2012 poppenga et al 2013 tarboton and bras 1991 with subsequent manual editing to adjust drainage lines and collect waterbodies from high resolution orthorectified images remotely sensed information at high spatial and temporal resolutions such as repeat lidar can facilitate automated analysis and extraction of hydrographic features saving time and increasing the accuracy and consistency of extracted features sharma and xu 2016 advanced computationally intensive machine learning approaches integrated with cybergis cyber geospatial information science and systems for the resolution of computational and data intensive geospatial analyses wang 2010 wang and goodchild 2019 wang and liu 2016 represent an exciting frontier for extracting accurate and fine scale hydrography from lidar to improve the nhd hr recent rapid advances in deep learning have been widely acknowledged and adopted in many challenging pattern recognition and object detection tasks kampffmeyer and salberg 2016 lecun and bengio 2015 maggiori and tarabalka 2017 reichstein et al 2019 schmidhuber 2015 sun and zhang 2018 xu and guan 2018 zhu et al 2017 compared to the traditional or hand crafted feature engineering deep learning has demonstrated advances in accuracy and efficiency for complex feature learning in various application domains liang and sun 2017 lin and tegmark 2017 lin and nie 2017 xu and mountrakis 2017 while such strategies promise a new way for hydrologic feature extraction from geospatial big data limited effort has taken advantage of deep learning for accurate efficient and fine scale delineation of hydrologic features moreover the full utilization of the most recent technology of geiger mode lidar could significantly improve high quality delineation of natural features clifton et al 2015 stoker and abdullah 2016 this research develops a deep learning model based on the u net structure ronneberger and fischer 2015 and attention mechanism oktay et al 2018 vaswani et al 2017 which consists of a contractive path and an expanding path for segmenting streamlines from input feature maps the contractive path is comprised of six triple convolutional layers plus five pooling layers for accurate extraction of global features and reduction of spatial redundancy while the expanding path is comprised of five transposed convolutional layers plus five triple convolutional layers for projecting the extracted global feature content to original locations in the prediction map the number of layers is chosen to reduce contractive path and upsample expanding path the x y dimension of feature maps from 224 by 224 to 7 by 7 or from 7 by 7 to 224 by 224 with a stride of two the patch size of 224 influences the model accuracy and efficiency small patch sizes cause poor accuracies due to the lack of context information while large patch sizes add extra computational burden without particular benefits to model performance in this research patch sizes of 64 112 224 448 and 512 were tested and 224 was chosen over smaller or larger ones based on evaluation of model accuracy and efficiency meanwhile feature concatenation is used to combine the extracted local and global information at different levels from the contractive path to its corresponding locations in the expanding path to enhance the expressivity of the model during the convolution and transposed convolutional processes computationally we use gpu processing to speed up model training that is based on keras and tensorflow two types of benchmark methods are adopted for model comparisons the first type includes two traditional pixel based classification methods a support vector machine svm and an artificial neural network ann model the other types include the nhd hr data compiled from topographic maps and orthophotography and elevation derived drainage lines generated from geonet tools sangireddy and stark 2016 the comparison shows that our method based on the attention u net model outperforms the best benchmark method by 8 61 9 39 13 68 and 13 31 in four different scenarios the resulting streamline map also indicates that the attention u net model generates smoother and more topologically connected features than the benchmark methods which is significant for hydrologic applications the major contributions of this research are two fold 1 a novel application of the attention u net for accurate and fine scale hydrologic streamline detection and 2 an effective streamline detection method that fully utilizes the geometric and intensity information from high resolution lidar data 2 study area and input dataset development our study area is a watershed in rowan county which is located in west central north carolina fig 1 this area encompasses a set of tributaries that flow into second creek which is the primary flowline feature of 12 digit nhd watershed 030401020504 the study area is 6 3 km2 and has a humid subtropical climate winters are short and mild while summers are usually hot and humid spring and fall are distinct and refreshing periods of transition temperature ranges between 100 f 38 c to 10 f 12 c within a year the watershed lies in the central interior and appalachian ecological division comer et al 2003 in terms of land cover types forest dominates the area and most of the stream channels are underneath closed canopy the lidar dataset used in this research is small footprint discrete return geiger mode lidar that was collected by the state government of north carolina in the fall of 2016 the lidar dataset requires about 21 gb of disk storage and the projection coordinate system is the 2011 state plane of north carolina this area has elevations ranging from 194 to 256 m because a geiger mode lidar sensor was used the point density of all returns reaches 43 returns per square meter a field validated set of intermittent stream heads surveyed between 2013 and 2014 along with on screen editing was used to generate reference data shavers and stanislawski 2018 a 3 m buffer was generated along the reference streamline to simulate the width of stream channels eight co registered 1 m resolution raster data layers were derived from the lidar point cloud data and were used for training validation and testing in the research the layers were selected through extensive comparison of elevation derivatives and optical imagery having national coverage with validated surface hydrography in diverse landscapes the raster layers include 1 a 1 m resolution digital elevation model dem derived from the ground return points 2 geometric curvature determined from the dem 3 a topographic position index tpi derived from the dem using a 3 cell by 3 cell window 4 a tpi derived from the dem using a 21 cell by 21 cell window 5 zenith angle positive openness derived from the dem using a 10 cell radius with 32 directions doneus 2013 6 return intensity determined from the lidar ground points averaged with inverse distance weighting using 10 nearest points 7 point density for return points between zero and 1 foot above ground and 8 point density for return points between 0 and 3 feet above ground geometric curvature is determined using geonet software sangireddy et al 2016 the software applies the non linear diffusion perona malik filter on the dem to remove noise and sharpen the localization of channels passalacqua et al 2010 geometric curvature which sums curvature in the x and y directions is then determined for the filtered dem the tpi value of a cell is the difference between the cell elevation and the local average elevation within a specific radius or within a surrounding window of cells de reu et al 2013 as noted average values for tpi layer 3 and 4 are computed based on 3 3 and 21 21 surrounding cell windows respectively the tpi exaggerates local lows and highs in a dem relative to the nearby topographic features accentuating ridges and valleys zenith angle positive opennes 5 with 10 m radius can enhance drainage and small stream channels doneus 2013 lidar return intensity 6 is usually lower for water surfaces and wet areas than for dry areas because of energy absorption by water hooshyar et al 2015 return point density layers 7 and 8 respectively estimate the density of land surface features such as shrubs and tree limbs up to 1 and 3 feet above ground which is most likely vegetation under the forest canopy shavers and stanislawski 2018 suggest vegetation density structure in the riparian zones may be reflected in these layers the eight raster layers are shown in fig 2 with summary statistics presented in table 1 inputs to our model are individual image patches sampled from the eight different feature maps derived from the lidar data at a resolution of 1 m the feature maps are normalized versions of the eight raster data layers normalizing each of the floating point datasets within the study area to a corresponding unsigned integer feature map to effectively test our method we created four different classification scenarios by splitting the research area into upper lower and left right portions when conducting our experiments we used one of the portions for generating training validation patches and the other portion to generate testing patches for accuracy assessment in order to evaluate model generalizability sample patches for training and validation were generated based on a random process that ensures no overlap between training and validation patches a visualization of the locations of our generated training and validation patches is shown in fig 3 to further enhance our training data we applied image augmentation by randomly rotating each training patch by 30 150 and 210 330 rescaling each sample by 0 5 0 8 and 1 5 2 0 shearing each sample by random ranges from 30 to 30 and mirroring each sample horizontally to create six augmented samples for each training sample finally 200 training 1400 after data augmentation and 30 validation patches were selected 3 methods 3 1 benchmark methods to evaluate the performance of our method it is compared to existing hydrography data elevation derived drainage lines and hydrography predicted from two machine learning methods we set up four baseline benchmarks which include nhd hr elevation derived drainage lines from geonet svm and an ann model the high resolution nhd hr was retrieved from the usgs nhd website https www usgs gov core science systems ngp national hydrography nhd data for the study area were compiled from 1 24 000 scale digital line graph data in 2001 with waterbodies and associated features manually adjusted in 2013 to fit national agriculture imagery program naip 1 m resolution color infrared digital orthophotography the geonet lines are extracted from the dem using a least cost path tracing technique that is guided by a minimum threshold flow accumulation skeleton sangireddy et al 2016 in our case the flow accumulation skeleton is generated using a minimum threshold of 1000 cells which is expected to over extract water flow network and fully define the drainage paths we used a svm classifier based on a radial basis function rbf kernel with a kernel approximation strategy for speeding up the training process rahimi and recht 2008 the parameters of kernel degree g and penalty c are tuned using a two level grid search in the range of 10 5 to 105 and 10 5 to 1 respectively for the benchmark neural network model we construct a model with two hidden layers and a sigmoid activation function as the output layer the parameters of number of hidden layers learning rate momentum and decaying rate are also tuned using grid search the reference data including training validation and testing data are the same between different models for model training parameter tuning and generating the final feature maps 3 2 the u net model the u net model is a special type of fully convolutional networks fcns unlike normal convolutional neural networks cnns the last fully connected layer from fcns are substituted by a series of transposed convolutional layers with larger and larger receptive fields fcns are built only by locally connected layers including convolution pooling and upsampling layers without using any dense connected layer this practice greatly reduces the number of parameters for model tuning and thus reduces redundant computation compared to traditional cnns a typical fcn has two parts a contractive path and an upsampling path where the former is used to extract important information and reduce spatial redundancy and the latter is used to project the extracted information to specific locations in the original image ronneberger et al 2015 the u net model is a state of the art fcn that achieves high accuracy for solving image segmentation problems ronneberger et al 2015 based on the fundamental structure of an fcn it further applies feature concatenations to recover and fully utilize the information extracted at different resolution levels in the contractive path to the corresponding locations in the expanding path details of model layers are described as follows convolutional layer the convolutional layer is the major workforce for extracting important features from images krizhevsky and sutskever 2012 it conducts image filtering by using kernel filters in this process image features with strong signals are extracted pooling layer pooling layer is used to conduct downsampling on activation maps downsampling reduces the sampling rate of a raster by decreasing the raster resolution i e increasing pixel size the max function is often used to filter out redundant information and preserve the strongest feature signals relu layer relu layer is short for rectified linear unit layer which is one of the most commonly used activation functions in cnns agarap 2018 it consists of a linear function for all positive input values and zero for all negative values it truncates unimportant features generated from the convolutional layer and only reserves the important ones transposed convolutional layer this layer projects the extracted dense features from the coarse resolution to its precise location in the original image by using upsampling i e increasing pixel resolution or spatial interpolation the attention module is a technique originally designed for sequence dependency modeling that has recently been adopted for modeling feature dependencies in image analysis oktay et al 2018 vaswani et al 2017 it can progressively suppress feature responses in irrelevant background regions and make the model focus on important features in this research we integrate five attention gates ags into the u net model and thus create an attention u net model for achieving high accuracy results as shown in fig 4 a the standard attention module maps query pixels and their key value pairs to the output the output is a set of weighted values and the attention weight matrix is calculated by a compatibility function of the query with the corresponding key vaswani et al 2017 finally the weighted inputs are multiplied by a scaling hyperparameter α initialized as 1 and added to the original input to produce the final output since the original attention weight matrixes at shallower layers are too large to fit in the memory we used a second attention module fig 4b adapted from oktay et al 2018 in our model the only difference is the second one directly combines the convolutional results from the feature maps and the gating signal to a relu layer to remove negative values and utilizes a bottleneck convolutional layer to reduce the channel dimension for memory saving and a sigmoid function to calculate the final attention weight matrix the architecture of the attention u net model is shown in fig 5 it applies six triple convolutional layers to the contractive path and five in the expanding path five pooling layers are used between each of the triple convolutional layers for downsampling in the expanding path five transposed convolutional layers size 2 2 and stride 2 are used for feature upsampling in each horizontal level of the two paths the network uses attention gates to filter the features propagated through the skip connections based on the gating signal of the contextual information from coarser scales to achieve high accuracy in segmentation results we utilize an adam optimizer for calculating the change direction of loss and adjust the weights in the back propagation process kingma and ba 2014 we use python 2 7 and keras 2 0 with backend of tensorflow 1 0 for the model construction we also utilize python libraries including sklearn 0 18 1 scikit image 0 16 2 gdal 3 0 2 numpy 1 17 3 supported by anaconda 2 0 the model is tested using both gpu and cpu devices it takes 15 h using a state of the art cpu and 2 h for a tesla m80 gpu to finish model training for our 6 07 km2 study area in this paper we run the attention u net and the u net model separately five times and the average statistics are reported for evaluation the attention u net model uses dice s coefficient 1 as the loss function the coefficient is the quotient of similarity and ranges between 0 and 1 dice s coefficient value equals twice true positive tp divided by the sum of twice true positive tp false positive fp and false negative fn as shown in equation 1 we use precision recall and f1 score to evaluate the model performance against testing data because of the difficulty in correctly labeling all streamline pixels relaxed methods are adopted to calculate precisions and recalls mnih and hinton 2010 the relaxed precision is defined as the fraction of number of pixels predicted as stream within a range of ρ pixels from pixels labeled as stream the relaxed recall is the fraction of number of pixels labeled as stream that are within a range of ρ pixels from pixels predicted as stream pixels in our experiments the slack parameter ρ is set to 3 according to previous research mnih and hinton 2010 1 d i c e s c o e f f i c i e n t 2 tp 2 t p f p f n we used a grid search for hyperparameter tuning of the learning rate filter size dropout rate and decaying factor in this process the adam optimizer is used to calculate and adjust the weights during training zhang 2018 fig 6 shows the change of training accuracy from different learning rates against the number of training epochs using the standard u net model we can see that the learning rate of 3 59e 05 achieves a good convergence and accuracy the plot of training and validation losses using the selected learning rate is shown in fig 7 4 results we evaluate our method against multiple benchmark methods in four scenarios for the first two scenarios we split the study area horizontally and use the lower portion as testing data in scenario one and the upper portion as testing data in scenario two for scenarios three and four we split the study area vertically and use the right portion as testing data in scenario three and the left portion as testing data in scenario four three metrics precision 2 recall 3 and f1 score 4 are used to evaluate the performance of the methods and are defined as follows tp true positive fp false positive fn false negative 2 p r e c i s i o n t p t p f p 3 r e c a l l t p t p f n 4 f 1 s c o r e 2 p r e c i s i o n r e c a l l p r e c i s i o n r e c a l l in this research we focus on the f1 score because it is the harmonic mean of precision and recall the highest f1 score means the model has an optimal balance of recall and precision while recall expresses the model s ability to find all streamline pixels in the input data precision expresses the portion of pixels that a model classifies as streamline correctly therefore there is a trade off between these two metrics the f1 score precision and recall of test accuracies for the attention u net u net and benchmark methods among the four scenarios are shown in tables 2 4 respectively table 2 shows that both the attention u net and u net models outperform all benchmark methods for the four scenarios in overall f1 score also the attention u net slightly outperforms the u net model in terms of the average f1 score among the four benchmark methods ann achieves the highest accuracy and nhd has the lowest accuracy the attention u net model outperforms ann by 8 61 9 39 13 68 and 13 31 svm by 12 12 12 98 22 69 and 13 54 nhd by 45 51 and geonet by 23 61 on average for precision svm achieves the best and outperforms the attention u net model by 12 04 7 64 1 99 and 9 51 from scenario 1 to 4 the attention u net model outperforms the u net model by 4 73 3 23 3 18 and 2 84 from scenario 1 to 4 for recall the geonet model has the highest accuracy of 92 66 which is 1 78 higher than the attention u net model apart from that the attention u net model achieves the next highest recalls and outperforms the u net model by 0 64 svm by 32 15 ann by 17 62 and nhd by 54 92 on average in scenario 1 to 4 overall the attention u net model outperforms the u net and all benchmark methods according to the average f1 score although one of the benchmark methods generates better precision values errors reflected by recall values are large and makes it worse in terms of general performance compared to both the attention u net and u net model we visualize two large extent locations fig 8 and two further zoomed in contexts fig 9 in scenario 1 to demonstrate the improved performance compared with the benchmark methods both the attention u net and u net model generate better streamline delineations with better connectivity and smoother shapes following channels compared to ann and svm which generate fragmented channels the nhd vector features are smooth and well connected as are the geonet drainage lines which are generated from a least cost path model guided by flow accumulation sangireddy et al 2016 however geonet lines overestimate channels by false recognition of the dry drainage lines as stream channels as expected from 1 24 000 scale data the nhd is sparse and only contains the several major channels in the study area the u net and attention u net model also perform better in extracting most water related features including water bodies in the two locations where all the automated benchmark methods fail to do so nhd includes the small lakes interpreted from orthophotography when the attention u net model is compared to the u net model the former eliminates many overestimated streamlines in the middle part of location 1 and 2 and better extracts water bodies from fig 9 we can see that the attention u net model is superior to traditional machine learning methods in extracting smooth streamlines and water bodies and avoids a majority of overestimated streamline pixels in nhd hr and the geonet flow accumulation model the u net model performs similarly to the attention u net model but the latter has a better delineation of the streamlines in the north and middle parts less overestimations of location 1 and better delineation of water bodies of location 2 5 discussion and conclusions this research developed an attention u net model for hydrologic streamline extraction using lidar derived feature maps specifically we have solved an image segmentation problem segmentation of streamlines based on the binary classification of stream versus non stream pixels this problem is difficult because hydrologic streamlines are formed by complex processes and occupy only a small portion of diverse land cover types while extracted streamlines need to be well connected furthermore surface water features e g clear turbid rivers swamps ponds and lakes are spatially heterogeneous and thus are difficult to extract using traditional machine learning methods e g ann and svm that cannot effectively handle multi scale context information e g topology land cover distribution topography the u net model is a special type of fully cnns using skip connections to combine local content from the contractive path to global content in the expanding path which ensures adequate connectivity of segmentation results because it enables the model to take both the global and local context information into consideration while extracting streamlines the attention module is added to the u net model to progressively suppress feature responses in irrelevant background regions and make the model focus on important streamline features according to the reference data addition of the attention module further enhances the accuracy for difficult instances such as the boundary of lakes river bends and dried channels a comprehensive evaluation of the model shows that our method outperforms multiple machine learning models and conventional flow accumulation methods by providing smoother and better connected streamline and waterbody features to evaluate the model thoroughly we created four different scenarios by splitting our research area into upper lower and left right portions for generating training validation and testing patches respectively the attention u net model generates f1 scores of 83 02 90 53 91 91 and 80 79 across the four scenarios which outperforms the best benchmark by 8 61 9 39 13 68 and 13 31 streamlines extracted using ann and svm are fragmented with missing parts mainly because these pixel based classification methods fail to consider the global context svm achieves a high precision result but a poor recall this indicates that it underestimates the stream class pixels but provides good confidence of those extracted since the reference data are highly imbalanced 1 100 between stream and non stream we also conducted additional experiments that artificially upsample the stream samples although the imbalanced issue is resolved the model has heavy overestimation of the stream class and the accuracy is not comparable to the imbalanced case compared to the manually verified reference data the elevation derived geonet drainage lines and features furnish higher recall scores than precision scores which indicates that this method overestimates the stream class in general and performs better in terms of completeness than precision these overestimations indicate local climate is drier than was assumed when selecting the flow accumulation threshold for the geonet lines which include more dry tributaries than collected in the reference data the 1 24 000 scale nhd only contains major stream channels and lake features and ignores the smaller tributaries so the accuracy is much lower than the other datasets only the nhd benchmark data includes the water bodies in the study area and the attention u net model performs much better in terms of water body extraction than the other benchmark methods the attention u net model utilizes its special feature concatenation design and a cnn to achieve high accuracy adequate connectivity and efficient streamline detection conventional machine learning models produce less optimal results primarily because they employ a pixel based classification strategy the attention u net model also departs from traditional flow accumulation models that heavily rely on expert inputs which in this case includes over extracted drainage lines and no water body extraction leading to large errors this research utilizes geiger mode lidar which provides high density point clouds and precise measurements for enabling transformative discovery and innovative opportunities in many scientific domains mcmanamon et al 2017 the following set of principles distilled from this research are important for guiding the application of the method to other areas of study or solving similar problems ensure a balanced number of convolutional layers at each horizontal level in both the contractive and expanding paths we find triple convolutional layers achieved adequate results and adding more convolutional layers would not benefit the model but increase computational intensity training patches should be randomly generated and have no overlap with validation and testing data training patches can overlap with themselves effect of data augmentation use data augmentation by randomly rotating mirroring shearing and rescaling training samples to ensure the expressivity of the model a dropout layer and a proper dropping rate hyperparameter are necessary in the final convolutional process for the model regularization use early stopping to enhance the training efficiency and prevent overfitting of the model future work will focus on applying the method to more study sites and scaling the model up to regional and national scopes disclaimer any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government author contributions z x and s w conceptualized the idea s w l v s and e l u initialized conceived and supervised the research l v s and e s collected data z x z j a m s and s w developed the deep learning model z x n j l c z l and b s conducted experiments z w s w l v s z j and e s drafted the manuscript all authors provided critical review of the manuscript and approved the final draft for submission data availability the data of this research is available for downloading from this link https doi org 10 6084 m9 figshare 12584975 v1 code availability the code is available under the open source mit license at https github com cybergis streamline detection git declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this paper and associated materials are based in part upon work supported by the national science foundation nsf under grant numbers 1443080 1743184 and 1833225 any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of nsf our computational work used virtual roger which is a cybergis supercomputer supported by the cybergis center for advanced digital and spatial studies and the school of earth society and environment at the university of illinois at urbana champaign 
25834,surface water is an irreplaceable resource for human survival and environmental sustainability accurate finely detailed cartographic representations of hydrologic streamlines are critically important in various scientific domains such as assessing the quantity and quality of present and future water resources modeling climate changes evaluating agricultural suitability mapping flood inundation and monitoring environmental changes conventional approaches to detecting such streamlines cannot adequately incorporate information from the complex three dimensional 3d environment of streams and land surface features such information is vital to accurately delineate streamlines in recent years high accuracy lidar data has become increasingly available for deriving both 3d information and terrestrial surface reflectance this study develops an attention u net model to take advantage of high accuracy lidar data for finely detailed streamline detection and evaluates model results against a baseline of multiple traditional machine learning methods the evaluation shows that the attention u net model outperforms the best baseline machine learning method by an average f1 score of 11 25 and achieves significantly better smoothness and connectivity between classified streamline channels these findings suggest that our deep learning approach can harness high accuracy lidar data for fine scale hydrologic streamline detection and in turn produce desirable benefits for many scientific domains keywords cybergis deep learning hydrologic streamlines hydrography lidar data analysis 1 introduction interactions of water within earth s systems have been studied extensively yet increased demand for this vital resource has expanded interest in monitoring and management of water resources accurate finely detailed delineation of surface hydrologic features is crucial for various scientific investigations and water resource applications such as agricultural suitability river dynamics flood mapping landslide risk analysis wetland inventory watershed analysis environmental monitoring and climate modeling to name just a few maidment 2017 poppenga and gesch 2013 schultz et al 2017 simley and carswell 2009 terziotti 2018 wright and nielsen 2012 while other terrain conditions have a role the spatial pattern of a surface water drainage network is largely a reflection of the type and arrangement of subsurface bedrock which can assist with classification and management of land resources clubb and bookhagen 2019 muller and oberlander 1976 therefore the key objective of this research is to understand how to advance machine intelligence for automatic extraction of detailed hydrologic features from high resolution elevation data and other open geospatial datasets yielding important data that can be used for this type of scientific work the national hydrography dataset nhd is a digital database of surface water features of the united states that is managed by the u s geological survey usgs and partner organizations sheng and wilson 2007 simley and carswell 2009 it provides a common reference for regulation research and modeling noaa 2016 the nhd high resolution hr is a multi scale dataset compiled from the best available data sources having scales of 1 24 000 or larger finer detail except in alaska where 1 63 360 or larger scales finer details are used however the quality of hydrographic data that has been compiled from topographic maps which include the nhd is not suitable for certain hydrologic regulatory and engineering purposes because of inconsistent drainage density and missing headwater content caruso 2014 chorley and dale 1972 colson and gregory 2008 colson 2006 fritz et al 2013 russell 2008 headwaters are small streams formed at the upstream extent of a watershed and comprise more than 50 percent of the stream network by length in the united states nadeau and rains 2007 to overcome these issues since 2009 nhd hr is being updated with more detailed hydrography derived from finer scale source information up to 1 2400 simley and carswell 2009 stanislawski 2009 economically these enhanced hydrographic data are expected to generate over 600 million dollars per year in potential benefits to water resource and emergency response managers in addition to the 500 million dollars in annual benefits already being generated from the existing program hoegberg 2016 updating the nhd hr applies the best available digital elevation model dem data which should use quality level 2 ql2 lidar data or better in the conterminous united states heidemann 2018 since 2014 the usgs 3d elevation program 3dep has been coordinating the collection of ql2 or better lidar point cloud data for the united states except for alaska where cloud penetrating interferometric synthetic aperture radar ifsar is being acquired to simplify collection in remote areas lukas et al 2015 ql2 lidar provides an aggregate nominal pulse spacing of less than 1 m m for first returns heidemann 2018 which supports derivation of a 1 meter m resolution dem the detail inherent to this high resolution dem data enables modeling of surface water dynamics from the continental scale to catchment and headwater scales although some methods to improve the nhd hr have been studied lopez torrijos 2018 poppenga et al 2013 sheng et al 2007 stanislawski and survila 2018 extracting accurate and fine scale hydrography from high resolution dem data using traditional flow accumulation methods is a costly and laborious process depending on the selected workflow various sophisticated issues must be handled which include conditioning the dem for flow modeling estimating flow accumulation weights and a minimum contributing area for stream formation along with tailoring solutions to diverse environmental conditions coupled with the fact that multiple methods are available solutions can vary and assessing the accuracy of extracted drainage lines is further complicated by temporal environmental variations procedures generally involve well known automated methods to derive drainage lines from dems anderson 2012 jenson and domingue 1988 maidment and morehouse 2002 metz and mitasova 2011 montgomery and foufoula georgiou 1993 o callaghan and mark 1984 passalacqua and belmont 2012 poppenga et al 2013 tarboton and bras 1991 with subsequent manual editing to adjust drainage lines and collect waterbodies from high resolution orthorectified images remotely sensed information at high spatial and temporal resolutions such as repeat lidar can facilitate automated analysis and extraction of hydrographic features saving time and increasing the accuracy and consistency of extracted features sharma and xu 2016 advanced computationally intensive machine learning approaches integrated with cybergis cyber geospatial information science and systems for the resolution of computational and data intensive geospatial analyses wang 2010 wang and goodchild 2019 wang and liu 2016 represent an exciting frontier for extracting accurate and fine scale hydrography from lidar to improve the nhd hr recent rapid advances in deep learning have been widely acknowledged and adopted in many challenging pattern recognition and object detection tasks kampffmeyer and salberg 2016 lecun and bengio 2015 maggiori and tarabalka 2017 reichstein et al 2019 schmidhuber 2015 sun and zhang 2018 xu and guan 2018 zhu et al 2017 compared to the traditional or hand crafted feature engineering deep learning has demonstrated advances in accuracy and efficiency for complex feature learning in various application domains liang and sun 2017 lin and tegmark 2017 lin and nie 2017 xu and mountrakis 2017 while such strategies promise a new way for hydrologic feature extraction from geospatial big data limited effort has taken advantage of deep learning for accurate efficient and fine scale delineation of hydrologic features moreover the full utilization of the most recent technology of geiger mode lidar could significantly improve high quality delineation of natural features clifton et al 2015 stoker and abdullah 2016 this research develops a deep learning model based on the u net structure ronneberger and fischer 2015 and attention mechanism oktay et al 2018 vaswani et al 2017 which consists of a contractive path and an expanding path for segmenting streamlines from input feature maps the contractive path is comprised of six triple convolutional layers plus five pooling layers for accurate extraction of global features and reduction of spatial redundancy while the expanding path is comprised of five transposed convolutional layers plus five triple convolutional layers for projecting the extracted global feature content to original locations in the prediction map the number of layers is chosen to reduce contractive path and upsample expanding path the x y dimension of feature maps from 224 by 224 to 7 by 7 or from 7 by 7 to 224 by 224 with a stride of two the patch size of 224 influences the model accuracy and efficiency small patch sizes cause poor accuracies due to the lack of context information while large patch sizes add extra computational burden without particular benefits to model performance in this research patch sizes of 64 112 224 448 and 512 were tested and 224 was chosen over smaller or larger ones based on evaluation of model accuracy and efficiency meanwhile feature concatenation is used to combine the extracted local and global information at different levels from the contractive path to its corresponding locations in the expanding path to enhance the expressivity of the model during the convolution and transposed convolutional processes computationally we use gpu processing to speed up model training that is based on keras and tensorflow two types of benchmark methods are adopted for model comparisons the first type includes two traditional pixel based classification methods a support vector machine svm and an artificial neural network ann model the other types include the nhd hr data compiled from topographic maps and orthophotography and elevation derived drainage lines generated from geonet tools sangireddy and stark 2016 the comparison shows that our method based on the attention u net model outperforms the best benchmark method by 8 61 9 39 13 68 and 13 31 in four different scenarios the resulting streamline map also indicates that the attention u net model generates smoother and more topologically connected features than the benchmark methods which is significant for hydrologic applications the major contributions of this research are two fold 1 a novel application of the attention u net for accurate and fine scale hydrologic streamline detection and 2 an effective streamline detection method that fully utilizes the geometric and intensity information from high resolution lidar data 2 study area and input dataset development our study area is a watershed in rowan county which is located in west central north carolina fig 1 this area encompasses a set of tributaries that flow into second creek which is the primary flowline feature of 12 digit nhd watershed 030401020504 the study area is 6 3 km2 and has a humid subtropical climate winters are short and mild while summers are usually hot and humid spring and fall are distinct and refreshing periods of transition temperature ranges between 100 f 38 c to 10 f 12 c within a year the watershed lies in the central interior and appalachian ecological division comer et al 2003 in terms of land cover types forest dominates the area and most of the stream channels are underneath closed canopy the lidar dataset used in this research is small footprint discrete return geiger mode lidar that was collected by the state government of north carolina in the fall of 2016 the lidar dataset requires about 21 gb of disk storage and the projection coordinate system is the 2011 state plane of north carolina this area has elevations ranging from 194 to 256 m because a geiger mode lidar sensor was used the point density of all returns reaches 43 returns per square meter a field validated set of intermittent stream heads surveyed between 2013 and 2014 along with on screen editing was used to generate reference data shavers and stanislawski 2018 a 3 m buffer was generated along the reference streamline to simulate the width of stream channels eight co registered 1 m resolution raster data layers were derived from the lidar point cloud data and were used for training validation and testing in the research the layers were selected through extensive comparison of elevation derivatives and optical imagery having national coverage with validated surface hydrography in diverse landscapes the raster layers include 1 a 1 m resolution digital elevation model dem derived from the ground return points 2 geometric curvature determined from the dem 3 a topographic position index tpi derived from the dem using a 3 cell by 3 cell window 4 a tpi derived from the dem using a 21 cell by 21 cell window 5 zenith angle positive openness derived from the dem using a 10 cell radius with 32 directions doneus 2013 6 return intensity determined from the lidar ground points averaged with inverse distance weighting using 10 nearest points 7 point density for return points between zero and 1 foot above ground and 8 point density for return points between 0 and 3 feet above ground geometric curvature is determined using geonet software sangireddy et al 2016 the software applies the non linear diffusion perona malik filter on the dem to remove noise and sharpen the localization of channels passalacqua et al 2010 geometric curvature which sums curvature in the x and y directions is then determined for the filtered dem the tpi value of a cell is the difference between the cell elevation and the local average elevation within a specific radius or within a surrounding window of cells de reu et al 2013 as noted average values for tpi layer 3 and 4 are computed based on 3 3 and 21 21 surrounding cell windows respectively the tpi exaggerates local lows and highs in a dem relative to the nearby topographic features accentuating ridges and valleys zenith angle positive opennes 5 with 10 m radius can enhance drainage and small stream channels doneus 2013 lidar return intensity 6 is usually lower for water surfaces and wet areas than for dry areas because of energy absorption by water hooshyar et al 2015 return point density layers 7 and 8 respectively estimate the density of land surface features such as shrubs and tree limbs up to 1 and 3 feet above ground which is most likely vegetation under the forest canopy shavers and stanislawski 2018 suggest vegetation density structure in the riparian zones may be reflected in these layers the eight raster layers are shown in fig 2 with summary statistics presented in table 1 inputs to our model are individual image patches sampled from the eight different feature maps derived from the lidar data at a resolution of 1 m the feature maps are normalized versions of the eight raster data layers normalizing each of the floating point datasets within the study area to a corresponding unsigned integer feature map to effectively test our method we created four different classification scenarios by splitting the research area into upper lower and left right portions when conducting our experiments we used one of the portions for generating training validation patches and the other portion to generate testing patches for accuracy assessment in order to evaluate model generalizability sample patches for training and validation were generated based on a random process that ensures no overlap between training and validation patches a visualization of the locations of our generated training and validation patches is shown in fig 3 to further enhance our training data we applied image augmentation by randomly rotating each training patch by 30 150 and 210 330 rescaling each sample by 0 5 0 8 and 1 5 2 0 shearing each sample by random ranges from 30 to 30 and mirroring each sample horizontally to create six augmented samples for each training sample finally 200 training 1400 after data augmentation and 30 validation patches were selected 3 methods 3 1 benchmark methods to evaluate the performance of our method it is compared to existing hydrography data elevation derived drainage lines and hydrography predicted from two machine learning methods we set up four baseline benchmarks which include nhd hr elevation derived drainage lines from geonet svm and an ann model the high resolution nhd hr was retrieved from the usgs nhd website https www usgs gov core science systems ngp national hydrography nhd data for the study area were compiled from 1 24 000 scale digital line graph data in 2001 with waterbodies and associated features manually adjusted in 2013 to fit national agriculture imagery program naip 1 m resolution color infrared digital orthophotography the geonet lines are extracted from the dem using a least cost path tracing technique that is guided by a minimum threshold flow accumulation skeleton sangireddy et al 2016 in our case the flow accumulation skeleton is generated using a minimum threshold of 1000 cells which is expected to over extract water flow network and fully define the drainage paths we used a svm classifier based on a radial basis function rbf kernel with a kernel approximation strategy for speeding up the training process rahimi and recht 2008 the parameters of kernel degree g and penalty c are tuned using a two level grid search in the range of 10 5 to 105 and 10 5 to 1 respectively for the benchmark neural network model we construct a model with two hidden layers and a sigmoid activation function as the output layer the parameters of number of hidden layers learning rate momentum and decaying rate are also tuned using grid search the reference data including training validation and testing data are the same between different models for model training parameter tuning and generating the final feature maps 3 2 the u net model the u net model is a special type of fully convolutional networks fcns unlike normal convolutional neural networks cnns the last fully connected layer from fcns are substituted by a series of transposed convolutional layers with larger and larger receptive fields fcns are built only by locally connected layers including convolution pooling and upsampling layers without using any dense connected layer this practice greatly reduces the number of parameters for model tuning and thus reduces redundant computation compared to traditional cnns a typical fcn has two parts a contractive path and an upsampling path where the former is used to extract important information and reduce spatial redundancy and the latter is used to project the extracted information to specific locations in the original image ronneberger et al 2015 the u net model is a state of the art fcn that achieves high accuracy for solving image segmentation problems ronneberger et al 2015 based on the fundamental structure of an fcn it further applies feature concatenations to recover and fully utilize the information extracted at different resolution levels in the contractive path to the corresponding locations in the expanding path details of model layers are described as follows convolutional layer the convolutional layer is the major workforce for extracting important features from images krizhevsky and sutskever 2012 it conducts image filtering by using kernel filters in this process image features with strong signals are extracted pooling layer pooling layer is used to conduct downsampling on activation maps downsampling reduces the sampling rate of a raster by decreasing the raster resolution i e increasing pixel size the max function is often used to filter out redundant information and preserve the strongest feature signals relu layer relu layer is short for rectified linear unit layer which is one of the most commonly used activation functions in cnns agarap 2018 it consists of a linear function for all positive input values and zero for all negative values it truncates unimportant features generated from the convolutional layer and only reserves the important ones transposed convolutional layer this layer projects the extracted dense features from the coarse resolution to its precise location in the original image by using upsampling i e increasing pixel resolution or spatial interpolation the attention module is a technique originally designed for sequence dependency modeling that has recently been adopted for modeling feature dependencies in image analysis oktay et al 2018 vaswani et al 2017 it can progressively suppress feature responses in irrelevant background regions and make the model focus on important features in this research we integrate five attention gates ags into the u net model and thus create an attention u net model for achieving high accuracy results as shown in fig 4 a the standard attention module maps query pixels and their key value pairs to the output the output is a set of weighted values and the attention weight matrix is calculated by a compatibility function of the query with the corresponding key vaswani et al 2017 finally the weighted inputs are multiplied by a scaling hyperparameter α initialized as 1 and added to the original input to produce the final output since the original attention weight matrixes at shallower layers are too large to fit in the memory we used a second attention module fig 4b adapted from oktay et al 2018 in our model the only difference is the second one directly combines the convolutional results from the feature maps and the gating signal to a relu layer to remove negative values and utilizes a bottleneck convolutional layer to reduce the channel dimension for memory saving and a sigmoid function to calculate the final attention weight matrix the architecture of the attention u net model is shown in fig 5 it applies six triple convolutional layers to the contractive path and five in the expanding path five pooling layers are used between each of the triple convolutional layers for downsampling in the expanding path five transposed convolutional layers size 2 2 and stride 2 are used for feature upsampling in each horizontal level of the two paths the network uses attention gates to filter the features propagated through the skip connections based on the gating signal of the contextual information from coarser scales to achieve high accuracy in segmentation results we utilize an adam optimizer for calculating the change direction of loss and adjust the weights in the back propagation process kingma and ba 2014 we use python 2 7 and keras 2 0 with backend of tensorflow 1 0 for the model construction we also utilize python libraries including sklearn 0 18 1 scikit image 0 16 2 gdal 3 0 2 numpy 1 17 3 supported by anaconda 2 0 the model is tested using both gpu and cpu devices it takes 15 h using a state of the art cpu and 2 h for a tesla m80 gpu to finish model training for our 6 07 km2 study area in this paper we run the attention u net and the u net model separately five times and the average statistics are reported for evaluation the attention u net model uses dice s coefficient 1 as the loss function the coefficient is the quotient of similarity and ranges between 0 and 1 dice s coefficient value equals twice true positive tp divided by the sum of twice true positive tp false positive fp and false negative fn as shown in equation 1 we use precision recall and f1 score to evaluate the model performance against testing data because of the difficulty in correctly labeling all streamline pixels relaxed methods are adopted to calculate precisions and recalls mnih and hinton 2010 the relaxed precision is defined as the fraction of number of pixels predicted as stream within a range of ρ pixels from pixels labeled as stream the relaxed recall is the fraction of number of pixels labeled as stream that are within a range of ρ pixels from pixels predicted as stream pixels in our experiments the slack parameter ρ is set to 3 according to previous research mnih and hinton 2010 1 d i c e s c o e f f i c i e n t 2 tp 2 t p f p f n we used a grid search for hyperparameter tuning of the learning rate filter size dropout rate and decaying factor in this process the adam optimizer is used to calculate and adjust the weights during training zhang 2018 fig 6 shows the change of training accuracy from different learning rates against the number of training epochs using the standard u net model we can see that the learning rate of 3 59e 05 achieves a good convergence and accuracy the plot of training and validation losses using the selected learning rate is shown in fig 7 4 results we evaluate our method against multiple benchmark methods in four scenarios for the first two scenarios we split the study area horizontally and use the lower portion as testing data in scenario one and the upper portion as testing data in scenario two for scenarios three and four we split the study area vertically and use the right portion as testing data in scenario three and the left portion as testing data in scenario four three metrics precision 2 recall 3 and f1 score 4 are used to evaluate the performance of the methods and are defined as follows tp true positive fp false positive fn false negative 2 p r e c i s i o n t p t p f p 3 r e c a l l t p t p f n 4 f 1 s c o r e 2 p r e c i s i o n r e c a l l p r e c i s i o n r e c a l l in this research we focus on the f1 score because it is the harmonic mean of precision and recall the highest f1 score means the model has an optimal balance of recall and precision while recall expresses the model s ability to find all streamline pixels in the input data precision expresses the portion of pixels that a model classifies as streamline correctly therefore there is a trade off between these two metrics the f1 score precision and recall of test accuracies for the attention u net u net and benchmark methods among the four scenarios are shown in tables 2 4 respectively table 2 shows that both the attention u net and u net models outperform all benchmark methods for the four scenarios in overall f1 score also the attention u net slightly outperforms the u net model in terms of the average f1 score among the four benchmark methods ann achieves the highest accuracy and nhd has the lowest accuracy the attention u net model outperforms ann by 8 61 9 39 13 68 and 13 31 svm by 12 12 12 98 22 69 and 13 54 nhd by 45 51 and geonet by 23 61 on average for precision svm achieves the best and outperforms the attention u net model by 12 04 7 64 1 99 and 9 51 from scenario 1 to 4 the attention u net model outperforms the u net model by 4 73 3 23 3 18 and 2 84 from scenario 1 to 4 for recall the geonet model has the highest accuracy of 92 66 which is 1 78 higher than the attention u net model apart from that the attention u net model achieves the next highest recalls and outperforms the u net model by 0 64 svm by 32 15 ann by 17 62 and nhd by 54 92 on average in scenario 1 to 4 overall the attention u net model outperforms the u net and all benchmark methods according to the average f1 score although one of the benchmark methods generates better precision values errors reflected by recall values are large and makes it worse in terms of general performance compared to both the attention u net and u net model we visualize two large extent locations fig 8 and two further zoomed in contexts fig 9 in scenario 1 to demonstrate the improved performance compared with the benchmark methods both the attention u net and u net model generate better streamline delineations with better connectivity and smoother shapes following channels compared to ann and svm which generate fragmented channels the nhd vector features are smooth and well connected as are the geonet drainage lines which are generated from a least cost path model guided by flow accumulation sangireddy et al 2016 however geonet lines overestimate channels by false recognition of the dry drainage lines as stream channels as expected from 1 24 000 scale data the nhd is sparse and only contains the several major channels in the study area the u net and attention u net model also perform better in extracting most water related features including water bodies in the two locations where all the automated benchmark methods fail to do so nhd includes the small lakes interpreted from orthophotography when the attention u net model is compared to the u net model the former eliminates many overestimated streamlines in the middle part of location 1 and 2 and better extracts water bodies from fig 9 we can see that the attention u net model is superior to traditional machine learning methods in extracting smooth streamlines and water bodies and avoids a majority of overestimated streamline pixels in nhd hr and the geonet flow accumulation model the u net model performs similarly to the attention u net model but the latter has a better delineation of the streamlines in the north and middle parts less overestimations of location 1 and better delineation of water bodies of location 2 5 discussion and conclusions this research developed an attention u net model for hydrologic streamline extraction using lidar derived feature maps specifically we have solved an image segmentation problem segmentation of streamlines based on the binary classification of stream versus non stream pixels this problem is difficult because hydrologic streamlines are formed by complex processes and occupy only a small portion of diverse land cover types while extracted streamlines need to be well connected furthermore surface water features e g clear turbid rivers swamps ponds and lakes are spatially heterogeneous and thus are difficult to extract using traditional machine learning methods e g ann and svm that cannot effectively handle multi scale context information e g topology land cover distribution topography the u net model is a special type of fully cnns using skip connections to combine local content from the contractive path to global content in the expanding path which ensures adequate connectivity of segmentation results because it enables the model to take both the global and local context information into consideration while extracting streamlines the attention module is added to the u net model to progressively suppress feature responses in irrelevant background regions and make the model focus on important streamline features according to the reference data addition of the attention module further enhances the accuracy for difficult instances such as the boundary of lakes river bends and dried channels a comprehensive evaluation of the model shows that our method outperforms multiple machine learning models and conventional flow accumulation methods by providing smoother and better connected streamline and waterbody features to evaluate the model thoroughly we created four different scenarios by splitting our research area into upper lower and left right portions for generating training validation and testing patches respectively the attention u net model generates f1 scores of 83 02 90 53 91 91 and 80 79 across the four scenarios which outperforms the best benchmark by 8 61 9 39 13 68 and 13 31 streamlines extracted using ann and svm are fragmented with missing parts mainly because these pixel based classification methods fail to consider the global context svm achieves a high precision result but a poor recall this indicates that it underestimates the stream class pixels but provides good confidence of those extracted since the reference data are highly imbalanced 1 100 between stream and non stream we also conducted additional experiments that artificially upsample the stream samples although the imbalanced issue is resolved the model has heavy overestimation of the stream class and the accuracy is not comparable to the imbalanced case compared to the manually verified reference data the elevation derived geonet drainage lines and features furnish higher recall scores than precision scores which indicates that this method overestimates the stream class in general and performs better in terms of completeness than precision these overestimations indicate local climate is drier than was assumed when selecting the flow accumulation threshold for the geonet lines which include more dry tributaries than collected in the reference data the 1 24 000 scale nhd only contains major stream channels and lake features and ignores the smaller tributaries so the accuracy is much lower than the other datasets only the nhd benchmark data includes the water bodies in the study area and the attention u net model performs much better in terms of water body extraction than the other benchmark methods the attention u net model utilizes its special feature concatenation design and a cnn to achieve high accuracy adequate connectivity and efficient streamline detection conventional machine learning models produce less optimal results primarily because they employ a pixel based classification strategy the attention u net model also departs from traditional flow accumulation models that heavily rely on expert inputs which in this case includes over extracted drainage lines and no water body extraction leading to large errors this research utilizes geiger mode lidar which provides high density point clouds and precise measurements for enabling transformative discovery and innovative opportunities in many scientific domains mcmanamon et al 2017 the following set of principles distilled from this research are important for guiding the application of the method to other areas of study or solving similar problems ensure a balanced number of convolutional layers at each horizontal level in both the contractive and expanding paths we find triple convolutional layers achieved adequate results and adding more convolutional layers would not benefit the model but increase computational intensity training patches should be randomly generated and have no overlap with validation and testing data training patches can overlap with themselves effect of data augmentation use data augmentation by randomly rotating mirroring shearing and rescaling training samples to ensure the expressivity of the model a dropout layer and a proper dropping rate hyperparameter are necessary in the final convolutional process for the model regularization use early stopping to enhance the training efficiency and prevent overfitting of the model future work will focus on applying the method to more study sites and scaling the model up to regional and national scopes disclaimer any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government author contributions z x and s w conceptualized the idea s w l v s and e l u initialized conceived and supervised the research l v s and e s collected data z x z j a m s and s w developed the deep learning model z x n j l c z l and b s conducted experiments z w s w l v s z j and e s drafted the manuscript all authors provided critical review of the manuscript and approved the final draft for submission data availability the data of this research is available for downloading from this link https doi org 10 6084 m9 figshare 12584975 v1 code availability the code is available under the open source mit license at https github com cybergis streamline detection git declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this paper and associated materials are based in part upon work supported by the national science foundation nsf under grant numbers 1443080 1743184 and 1833225 any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of nsf our computational work used virtual roger which is a cybergis supercomputer supported by the cybergis center for advanced digital and spatial studies and the school of earth society and environment at the university of illinois at urbana champaign 
