index,text
7375,rainfall networks are the most direct sources of precipitation data and their optimization and evaluation are essential and important information entropy can not only represent the uncertainty of rainfall distribution but can also reflect the correlation and information transmission between rainfall stations using entropy this study performs optimization of rainfall networks that are of similar size located in two big cities in china shanghai in yangtze river basin and xi an in yellow river basin with respect to temporal variability analysis through an easy to implement greedy ranking algorithm based on the criterion called maximum information minimum redundancy mimr stations of the networks in the two areas each area is further divided into two subareas are ranked during sliding inter annual series and under different meteorological conditions it is found that observation series with different starting days affect the ranking alluding to the temporal variability during network evaluation we propose a dynamic network evaluation framework for considering temporal variability which ranks stations under different starting days with a fixed time window 1 year 2 year and 5 year therefore we can identify rainfall stations which are temporarily of importance or redundancy and provide some useful suggestions for decision makers the proposed framework can serve as a supplement for the primary mimr optimization approach in addition during different periods wet season or dry season the optimal network from mimr exhibits differences in entropy values and the optimal network from wet season tended to produce higher entropy values differences in spatial distribution of the optimal networks suggest that optimizing the rainfall network for changing meteorological conditions may be more recommended keywords entropy rainfall network design temporal variability dynamic network evaluation framework maximum information minimum redundancy 1 introduction a hydrometric network is a data collection system which is of fundamental significance for research and development mishra and coulibaly 2009 reviewed several commonly used methods for hydrometric network design and evaluation including 1 statistically based 2 information theory based 3 user survey 4 hybrid 5 physiographic components and 6 sampling strategies chacon hurtado et al 2017 made a review of rainfall and streamflow sensor network design with respect to different criteria proposed a framework for classifying the design methods and suggested a generalized procedure for optimal network design in recent years hydrometric network evaluation and optimization methods particularly based on entropy have received significant attention keum et al 2017 reviewed studies that applied entropy theory in water monitoring network design and evaluation especially for publications after 2009 which were not covered in the review by mishra and coulibaly 2009 several groups of methods using entropy theory have been developed for network design and optimization fahle et al 2015 divided most methods into three main groups 1 derivation of minimum distance between stations or formation of regional information maps e g husain 1989 masoumi and kerachian 2010 su and you 2014 2 provision of optimal sets or ranking of stations or iterative updating until reaching a certain threshold relative to specific objective functions for existing networks e g markus et al 2003 alfonso et al 2010a li et al 2012b mishra and coulibaly 2014 stosic et al 2017 and 3 multi objective optimization for simultaneously fulfilling several objectives of network design e g alfonso et al 2010b samuel et al 2013 alfonso et al 2014 xu et al 2015 leach et al 2015 leach et al 2016 keum and coulibaly 2017a b sometimes hybrid methods are also applied and different groups of methods are compared a brief literature review on hydrometric network design and optimization based on entropy is now given langbein 1954 first proposed that monitoring network efficiency should be based on space time measure of information caselton and zidek 1984 chose optimal stations from maximum information transmission of the whole network husain 1987 used an objective function for network information based on bivariate normal multivariate normal and lognormal distributions but limited it to reducing stations from a dense network later he improved the method generalized it for multivariate gamma distribution and presented an approach based on transinformation distance correlation for expansion of a sparse network husain 1989 more improvements using information transmission and distance correlation have since been made for network design and optimization e g mogheir et al 2003 mogheir et al 2006 masoumi and kerachian 2008 masoumi and kerachian 2010 owlia et al 2011 su and you 2014 many other measures originating in entropy theory have been applied to network evaluation such as information transfer index iti directional information transfer dit coefficient of non transferred and transferred information t1 and t0 ensemble entropy permutation entropy and others see krstanovic and singh 1992a b yang and burn 1994 markus et al 2003 ridolfi et al 2011 alfonso et al 2014 stosic et al 2017 additionally network evaluation and optimization have encompassed various types of hydrometric networks including water quality and groundwater monitoring networks harmancioglu and alpaslan 1992 ozkul et al 2000 mogheir and singh 2002 mogheir et al 2004a mogheir et al 2004b 2006 guo and wang 2010 leach et al 2016 rainfall networks krstanovic and singh 1992a b yoo et al 2008 chen et al 2008 su and you 2014 xu et al 2015 yeh et al 2017 streamflow gauge networks markus et al 2003 mishra and coulibaly 2010 alfonso et al 2012 li et al 2012b samuel et al 2013 mishra and coulibaly 2014 leach et al 2015 stosic et al 2017 and water level gauge networks alfonso et al 2010a b 2014 mondal and singh 2012 fahle et al 2015 among different types of hydrometric networks regional rainfall networks provide a basis for analyzing rainfall characteristics flood forecasting water level monitoring and design of hydraulic structures thus evaluation and optimization of rainfall networks remain an important issue in addition the effect of temporal variability and spatial differences on network design and optimization has received significant attention recently e g wei et al 2014 mishra and coulibaly 2014 fahle et al 2015 climate change human activities and nonstationarity of hydrological processes cannot be overlooked during hydrometric network design and management especially for rainfall events which are highly heterogeneous localized and strongly influenced by geographical topographical and climate factors zhang et al 2014 song et al 2015 wang et al 2016 gu et al 2017 different temporal conditions can have appreciable impacts on hydrometric network design meaning that an optimal network may only be optimal during the time when observations were made though temporal variability affects network optimization and its transferability to changing conditions to the best of the authors knowledge studies on rainfall networks with respect to this temporal variability are limited accordingly evaluation of rainfall networks under different times should be analyzed to complement the design procedure and to further strengthen the reliability and flexibility of network optimization the objectives of this study therefore are to 1 propose a framework for assessing the temporal variability of optimized rainfall networks by maximum information minimum redundancy mimr criterion during changing periods with fixed window lengths 2 examine how large the temporal variability is in terms of entropy values and rankings and quantify its effect on rainfall station ranking by ranking disorder index rdi and 3 investigate impacts of meteorological conditions on network optimization especially for wet season and dry season and compare the resulting optimal networks the framework proposed in this study and related discussion and results will be useful for decision makers for obtaining optimal rainfall networks considering temporal variability specifically for designing and optimizing a rainfall network with a dynamic perspective in addition before conducting network optimization we compare different quantization methods for entropy estimation and briefly discuss the impact of data length which has been proposed and discussed in entropy related hydrological studies mishra et al 2011 mishra and coulibaly 2014 keum and coulibaly 2017b the paper is organized as follows first in section 2 we introduce information entropy concepts and their estimation in section 3 natural and geographical conditions of the study areas as well as current rainfall networks are described in section 4 methodologies including mimr algorithm rdi and a dynamic network evaluation framework considering temporal variability are presented in section 5 we provide results and discussion on our rainfall networks conclusions and outlook are presented in the last section 6 2 entropy concepts shannon 1948 laid the mathematical foundation of entropy as a measure of information or uncertainty entropy quantifies the uncertainty of a random variable via its probability distribution in hydrometeorology hydrology and water resources frequently used entropy measures include marginal entropy joint entropy transinformation mutual information and total correlation 2 1 basic entropy measures for entropy measures used in network design and optimization there are basically three questions we need to address 1 how much information would a station or several stations contain 2 how much information can one station or several stations transmit to other stations 3 how much information is shared by several stations these three questions are answered by marginal entropy joint entropy transinformation and total correlation consider a discrete variable x with n possible results xi i 1 2 3 n where the probability of occurrence of xi is pi 0 p i 1 and i p 1 the marginal entropy is defined as 1 h x h p 1 p 2 p 3 p n k i 1 n p i log p i where k is a constant and in general it can be taken as unity singh 2013 the marginal entropy quantifies the average reduction in uncertainty of x or its probability distribution since variance is a measurement of dispersion or variability of a variable it has been compared with entropy by wei 1987 and ebrahimi et al 1999 while variance measures concentration only around the mean entropy measures diffuseness of the probability irrespective of the location of concentration and may also be related to higher order moments of a distribution leading to a closer characterization of the probability density function mishra et al 2009 there is therefore no unique relation between variance and entropy for two discrete random variables x and y with the joint probability of x i and y j denoted as p x i y j p ij i 1 2 n j 1 2 m the joint entropy between them can be defined as 2 h x y i 1 n j 1 m p ij log p ij transinformation describes transmission or sharing information between two random variables which is defined as 3 t x y h x h y h x y for more than two variables x1 xn multivariate joint entropy with p i 1 i n i 1 1 2 n 1 i 2 1 2 n 2 i n 1 2 n n being an n dimensional probability distribution can be defined as 4 h x 1 x n i 1 1 n 1 i 2 1 n 2 i n 1 n n p i 1 i n log p i 1 i n total correlation describes information redundancy within multiple variables which is an extension of transinformation and is defined as mcgill 1954 watanabe 1960 5 c x 1 x n i 1 n h x i h x 1 x n to illustrate bivariate and multivariate joint entropy transinformation total correlation and relationships among them more directly venn diagrams are provided in fig 1 for clarity note that all the measures introduced above are with a logarithmic base of 2 so the units of entropy are bits also if p i equals zero then we stipulate that 0log0 0 2 2 estimation of entropy measures as the probability distribution is contained in the definition of entropy the key point is to estimate the probability distribution the methods of probability distribution estimation can mainly be classified into two types 1 fitting a probability distribution function pdf to data and its parameter estimation and 2 nonparametric density estimation without assuming a distribution in the first type typical distributions such as normal lognormal and gamma have been mostly applied e g husain 1989 krstanovic and singh 1992b harmancioglu and alpaslan 1992 yoo et al 2008 when it comes to multivariate case the calculations become quite difficult for other distributions further many natural phenomena like streamflow and precipitation are heavy tail distributed li et al 2012a and the distribution assumptions may be debatable due to the unsuitable selection of distributions while nonparametric density estimation avoids this step in the entropy estimation fahle et al 2015 among different nonparametric estimators the most predominant and frequently used is the binning estimator since it is easy to understand and compute e g markus et al 2003 alfonso et al 2010a li et al 2012b the main idea of this method is to discretize approximately a continuous random variable and there are different methods of discretization for example histogram method and mathematical floor function for the histogram method bins are often selected to group observation data into different classes and then use frequency as a substitute for probability in matlab 2016a version the function histcounts which sets a binwidth parameter automatically or manually can be used for the histogram method the mathematical floor function is another method of discretization alfonso et al 2010a which directly transfers continuous signals into discrete pulses and the conversion is performed by floor brackets as 6 x q a 2 x a 2 a where x denotes the analog e g continuous value and x q denotes the quantized discrete value while a denotes the bin width other commonly used nonparametric methods especially for estimating mutual information include kernel density estimation kde e g yang and burn 1994 mishra and coulibaly 2014 and k nearest neighbor knn e g kraskov et al 2004 khan et al 2007 fahle et al 2015 however due to their limited applications and sophisticated calculations in multivariate entropy estimation we finally chose the binning estimator for our entropy calculations generally the bin width selection has always been an important issue for it directly affects the approximate probability distribution and consequently affects the entropy value e g amorocho and espildora 1973 scott 1979 shimazaki and shinomoto 2007 ruddell and kumar 2009 the influence of different bin widths or class intervals on the entropy value has been discussed by singh 1997 and careful selection of class interval and sampling interval are suggested because the entropy value decreases as the class interval or the sampling interval increases both fixed bin width and optimal bin width can be chosen and no standard rules for bin size class interval exist but empirical formulas can be referred to papana and kugiumtzis 2009 scott 1979 suggested an optimal bin width as 7 a 3 49 σ x n 1 3 where σ denotes the standard deviation of an observation series of x e g daily series of one station and n denotes the sampling size scott 1979 sturges 1926 presented the optimal bin width as 8 a r x 1 log 2 n where r x is the range of the observation series of x and n is the sampling size of x bendat and piersol 1966 developed another method for determining the optimal bin width as 9 a r x 1 87 n 1 0 4 where r x is the range of the observation series of x and n is the sampling size of x there have been some comparisons between different bin width selections e g fahle et al 2015 keum and coulibaly 2017a fahle et al 2015 indicated that a fixed bin width was better suited since a temporary rise in nearly constant water level increased the standard deviation and corresponding larger optimal bin width and lower marginal entropy loss of information which is against the initial intention of water level monitoring i e detecting a change in the water level is more important in the case of keum and coulibaly 2017a they compared two fixed bin widths and two optimal bin widths sturges 1926 scott 1979 for entropy values and found no strong concordance among the four quantization cases two fixed bin widths and two optimal bin widths in entropy values though similar relative rankings were found for precipitation stations most streamflow stations yielded very different relative rankings under different quantization cases no matter which specific method is applied the estimation method will influence the entropy value and hence rankings fahle et al 2015 keum and coulibaly 2017a and the influence can be specific for different cases so we will also compare different methods for multivariate entropy estimation it is necessary to estimate the multivariate joint distribution to calculate the joint entropy alfonso et al 2010b and li et al 2012b applied a grouping property see kraskov et al 2005 for the estimation of multivariate joint entropy the basic idea for this method lies in generating a new variable containing the information equal to that of the original variables it can be verified that the information amount keeps invariant under this merging method multiple variables can be converted to a nested variable and then multivariate joint entropy can be computed for example h x y z can be rewritten as h a z h x y z with denoting the merging operator in a similar way multivariate joint entropy with n variables can be transformed as 10 h x 1 x 2 x n h x 1 x 2 x 3 x n h x 1 x 2 x 3 x n h x 1 x 2 x 3 x n 1 x n h x 1 x 2 x 3 x n 1 x n then the total correlation can be computed by eq 5 or also by a grouping property the variable merging method is straightforward and for detailed information the reader can refer to alfonso et al 2010b and li et al 2012b our study will apply this merging method for multivariate entropy calculation 3 material the yangtze river and the yellow river are the two most important rivers in china their watersheds exhibit greatly different geographical and hydrological characteristics we selected two typical big cities shanghai in taihu lake basin the yangtze river and xi an in wei river basin the yellow river as case studies since they have similar rainfall network sizes these two big cities have dense population and are both highly urbanized therefore it is important to study the hydrological and meteorological characteristics of these areas and evaluate rainfall networks therein 3 1 study area shanghai is located on the west coast of the pacific ocean along the eastern asian continent in the front of the yangtze river delta it is east of east china sea south of hangzhou bay west of jiangsu province and zhejiang province north of the mouth of the yangtze river and also the yangtze river and east china sea connect here by the end of 2003 shanghai covered an area of 6340 5 square kilometers accounting for 0 06 of the total area of the country most of the area is covered by flat plains being part of the yangtze river delta with an average altitude of 4 m above the sea level shanghai has north subtropical maritime monsoon climate and is also the convergence zone of the north and south cold warm air masses alternately affected by cold and warm air and marine moist air shanghai has humid climate and four distinct seasons with abundant sunshine and precipitation and its wet season extends from may to september characterized by three relatively low rainfall periods midsummer late autumn and winter and three relatively high rainfall periods spring rainy meiyu and autumn rainy the mean annual rainfall is about 1123 7 mm and it is one of the areas where rainstorms occur frequently with 85 of them occurring in the flood season and mostly concentrated during june september huangpu river is the biggest river that forms the land water system of shanghai which originates from taihu lake with a total length of 113 km and a width of 300 770 m flowing through the urban area and becoming an important water transportation route in shanghai xi an is located in the central part of the yellow river basin it is the political economic and cultural center of shaanxi province the city covers an area of 10 108 square kilometers of which the urban area is about 1066 square kilometers the landform of xi an includes four basic types plain loess tableland hill and mountain with the main geomorphic characteristics of the city being high in the south and low in the north another characteristic is its clear boundary between plain and mountain qinling mountains and wei river plain constitute the main typical topography of this area wei river plain is formed by the alluviation of wei river the largest tributary of the yellow river and its tributaries which is also known as the guanzhong plain hills are mainly distributed in the south of mount li lantian county and lintong county covering an area of about 740 square kilometers accounting for 7 41 of the city s total area mountains include the north slope of the qinling mountains and mount li covering an area of 4875 6 square kilometers accounting for 48 84 of the city s total area xi an is located in the transitional zone between humid climate of the southeast coast and arid climate of the northwest inland and therefore has two types of climate characteristics it has warmtemperate semi humid continental monsoon climate and the wet season extends approximately from may to september the mean annual rainfall is about 537 5 1028 4 mm and the spatial distribution of rainfall shows significant differences with more rainfall in southern qinling mountains than in northern wei river plain interannual variability of rainfall is also significant for wet year exceeding the average by 15 33 and dry year being only 56 7 77 4 of the average 3 2 dataset and network in total 47 rainfall stations in shanghai except chongming district and 53 rainfall stations in xi an and their daily precipitation data of ten years from 2006 to 2015 were chosen for a case study shanghai is further divided into two parts by the huangpu river which flows through the area and splits shanghai into two parts east of huangpu river seh with 23 rainfall stations and west of huangpu river swh with 24 rainfall stations xi an is also divided into two parts according to topography one part is plain with 24 rainfall stations xpl and another part is mountains and hills with 29 rainfall stations xmh here we should note that two stations no 10 11 in swh located in kunshan jiangsu province are included since they are close to other stations in the network system similarly three stations no 4 5 11 in xpl located in xianyang are included rainfall networks in two study areas with dem values are briefly shown in fig 2 detailed geographical coordinate information of stations can be referred to in appendix a for prior knowledge of the two study areas we first computed the network density and referred to hydrological guides for rainfall gauges provided by wmo the guide to hydrological practices 2008 and ministry of water resources 2013 as shown in table 1 according to wmo the recommended minimum densities of precipitation stations for coastal mountains hilly area and interior plains are 900 250 575 and 575 km2 per station respectively according to the guidelines of china the control area of a single station should not be greater than 200 km2 150 km2 especially in the plain river network region thus the rainfall network densities of these two cities shown in table 1 already meet both global and regional demands for recommended minimum network densities since it is not readily definable whether to add or reduce stations we used the existing network for temporal analysis especially for network reduction scenarios however the framework of analysis can be applied and generalized for network augmentation or relocation if data in ungauged area is available by simulation or interpolation meanwhile it should be realized that regulations are often strict not in terms of station density but in the quality of data for providing information about the hydrological system chacon hurtado et al 2017 since one of the main objectives is to present a framework for dynamic rainfall network evaluation we will mainly focus on rankings of importance for existing rainfall network and its corresponding network performance in terms of entropy values 4 methodology in this section an information theory based method applied for rainfall network evaluation and design is introduced named maximum information minimum redundancy mimr greedy ranking algorithm and a ranking disorder index rdi for rainfall stations based on normalized apportionment entropy is supplemented for identifying stations with high temporal variability of the rankings besides the framework of dynamic rainfall network evaluation mainly considering temporal variability analysis is developed for further discussion 4 1 mimr algorithm mimr li et al 2012b greedy ranking criterion provides an easy to implement way to solve the network optimization problem mainly suitable for selecting the best set of stations from a dense network or to obtain ranking of importance for stations in the network the objective function contains three items joint entropy h transinformation t and total correlation c and the optimization problem is formulated as max mimr λ 1 h x s 1 x s 2 x s k i 1 m t x s 1 x s 2 x s k x f i λ 2 c x s 1 x s 2 x s k subject to k m n 11 h x s 1 x s 2 x s k pct h x s 1 x s 2 x s k x f 1 x f 2 x f m where k is the number of selected stations denoted as a set of s m is the number of unselected stations denoted as a set of f n is the total number of stations in the current network and λ1 λ2 are trade off weights between information and redundancy and λ 1 λ 2 1 pct is the percentage of joint entropy representing the ratio between joint entropy for the selected stations and joint entropy for all stations in the existing network the first item in mimr denotes the total information contained by the selected stations in the network eq 4 the second item denotes the information that can be obtained about unselected stations from the selected stations eq 3 and the third item denotes the information redundancy among the selected stations eq 5 to solve the optimization problem and avoid computation burden from global search li et al 2012b generalized a greedy selection algorithm first we identify the central station with the highest marginal entropy and then rankings of stations can be obtained by implementing the mimr criterion step by step as a threshold pct is set e g obtaining 95 of the total joint entropy in the existing network we can get an optimal subset of stations for the final decision after selecting the last station rankings of all stations can be determined and used for further variability analysis for more details about mimr readers can refer to li et al 2012b since the first goal of network design is generally accepted as maximizing information of the network system λ1 is usually larger than λ2 suggested by li et al 2012b as 0 8 we made sensitivity analysis on different information redundancy trade off weights and found that most stations kept stable with λ1 varying from 0 5 to 1 λ2 varying from 0 5 to 0 to sum up the default parameter set for our mimr selection is given by λ1 0 8 λ2 0 2 and pct 95 4 2 ranking disorder index to assess the dispersion of ranks for a specific rainfall station by mimr we adopted a ranking disorder index rdi for identifying rainfall stations whose importance is mostly affected by temporal change the definition of the index originates from apportionment entropy ae maruyama and kawachi 1998 kawachi et al 2001 maruyama et al 2005 as 12 ae i 1 n r i n log 2 r i n where n is the number of all possible ranks for a station so it is equal to the number of stations in a network n is the number of ranks under different times and r i is the number of a certain ith rank as the maximum values of ae is log 2 n rdi is calculated by normalized apportionment entropy nae defined as 13 rdi nae ae log 2 n the rdi is an index for the uncertainty and fluctuation of the rank assigned to a rainfall station fahle et al 2015 which mainly considers the impact of temporal change on rankings of rainfall stations thus a higher rdi reflects a station whose importance is more affected by temporal variability so more careful attention should be paid to such station as its importance can vary a lot during a relatively short period 4 3 dynamic network evaluation framework one of the main objectives of this study is to present a dynamic network evaluation framework from a given length of time series fig 3 shows an overview of the proposed framework and a brief step by step description is given below step 1 obtain rainfall data at all stations in a network step 2 preprocess data for different time windows e g 1 year 2 year 5 year step 3 do mimr ranking with different data series from step 2 step 4 obtain network performances entropy values and rankings for sliding series step 5 compute ranking disorder index rdi for all stations with rankings in step 4 step 6 rank all stations in descending order of rdi step 7 identify ranking variations for individual stations step 8 obtain final suggestions for network design for data preprocessing in step 2 we mainly focus on shifting inter annual time window to assess temporal variability from the inter annual aspect fig 4 shows how sliding inter annual observation series fixed window length of 1 year 2 year and 5 year with a 10 day increment are obtained from a continuous ten year series 2006 2015 to assess the temporal variability from the seasonal aspect we divide the entire data set into wet season 5 months may to september and dry season 7 months october to april to see the seasonal variability effect on average and avoid years with extreme precipitation we use a 50 month series 5 months for one year 10 years in total for wet season and 70 month series 7 months for one year 10 years in total for dry season 5 results and discussion 5 1 entropy estimation and data length impact entropy values are affected by different binning parameters when using the binning estimator we compared fixed and optimal bin widths considering the fixed bin width a in eq 6 was set as 1 mm d and 3 mm d for optimal bin width three methods scott sturge bendat piersol were used to calculate the optimal bin width for each station table 2 shows that entropy values diverged for different bin widths especially for joint entropy and total correlation with fixed and optimal bin widths the differences between two fixed widths a 1 mm d and a 3 mm d suggested that a larger bin width led to smaller entropy values which agreed with the results from singh 1997 for the optimal bin size suggested by scott bendat piersol and fixed bin width as a 1 mm d the maximum marginal entropy and average marginal entropy were a little larger than the values calculated using sturge s method and the fixed bin width as a 3 mm d though the maximum marginal entropy affected the first step in mimr ranking the differences between different methods fell in a narrow range meanwhile both fixed bin widths with a 1 mm d and a 3 mm d had joint entropies lower than 4 bits while the other three methods yielded average joint entropies higher than 5 bits as the saturated value for the joint entropy was equal to log 2 n 11 83 bits n is the number of data points given by ten year daily series which is equal to 3652 the joint entropies estimated by fixed bin widths with a 1 mm d and a 3 mm d were much lower than the saturated joint entropy the total correlation was also subject to different bin widths and even related to the number of variables stations therefore we provided not only the total correlation but also the values divided by the number of stations tc n though entropy values would change under different binning estimations it was interesting to find that the relative differences between areas were maintained for instance all the entropy values of xpl were the smallest among the four subareas no matter which binning estimation was used in addition the entropy values of seh and swh were always similar regardless of different binning sizes going one step further basic statistical properties of precipitation in the four subareas can serve as a reference fig 5 from fig 5 xpl showed the lowest mean precipitation lowest maximum precipitation and lowest standard deviation std dev additionally seh and swh had similar mean precipitation and standard deviation hence it can be assumed that the average rainfall and its variability roughly determined the intrinsic characteristic of a network which would be consistently quantified by entropy values as a consequence compared with other three subareas rainfall network in certain areas such as interior plain xpl with both less rainfall and lower variability would generate less information in total relative differences for entropy values among different areas were maintained with five estimation methods to avoid multifarious analysis with different estimation methods we finally chose scott s method for entropy estimation in the following sections as it yielded the largest marginal entropy and joint entropy in order to see the impact of data length on entropy values we calculated average marginal entropy joint entropy and total correlation with increasing length of data series fig 6 shows that variations of entropy values are similar for the four subareas generally the values became larger with increasing data length and then tended to stabilize it should also be noted that the increasing trends were not monotonous and fluctuations may emerge at certain data lengths e g around 500 days 1200 days and 1500 days this agrees with the previous conclusion that both joint entropy and total correlation were sensitive to time series length keum and coulibaly 2017b as they finally recommended the use of at least 10 years of daily time series data to avoid any significant information loss of the network we also used a ten year series for mimr selection to get an optimal network for reference however shorter lengths of series 1 year 2 year and 5 year were still used for assessing temporal variations of the optimized network with mimr due to limited length of data series available we also noticed that the entropy values had nearly reached 70 80 and 90 of the values estimated by 10 year series using 1 year 2 year and 5 year series respectively more importantly network performance was subject to inter annual variability and it was necessary to capture and analyze the variability with relatively shorter time series 5 2 optimal networks under sliding series using mimr before optimizing rainfall networks under sliding series primary mimr ranking and selection results based on the entire 10 year series are shown in table 3 generally higher percentage of stations were selected for xpl and xmh than those for seh and swh though mimr values for xpl and xmh were relatively lower in addition the lower mimr values were mainly due to lower transinformation in xpl and xmh in order to conduct temporal variability analysis on network optimization it is necessary to identify impacts of sliding time series on optimal rainfall networks using mimr the graphs in fig 7 show the dispersion pattern and temporal evolution trend of the information joint entropy and redundancy total correlation content for optimal networks in four subareas with different time series which are from moving time windows of 1 year 2 years and 5 years for the sake of comparison the ranges of coordinate axes were deliberately set as the same under different time windows from fig 7 left column a longer time window brought less diffusive pattern of joint entropy and total correlation meanwhile the quantity of joint entropy and total correlation also generally increased with longer time windows shown as scatters in three subplots from top to bottom gradually shifting to upper right of the subplot which again confirmed the data length impact discussed in section 5 1 considering the temporal evolution of entropy values the middle and right columns in fig 7 revealed that the fluctuations could be more significant with 1 year series and 2 year series in total the results revealed that the performance of the optimal network using mimr could fluctuate a lot especially under shorter duration however it was found that the variation trend of joint entropy would be basically maintained for three time windows from fig 7 middle column variation curves with different time windows especially for 1 year and 2 year as the total number of series was greater than the number of 5 year series showed similar patterns and periodicity for joint entropy yet the similarity was not much comparable for total correlation under three time windows this demonstrated that changing series indeed affected the information content of the optimal network in such case a finer time window may capture this variability and periodicity more easily and optimization based on long time series may weaken such temporal variability in addition two subareas in the same city i e seh and swh in shanghai xpl and xmh in xi an also showed similar variation patterns for joint entropy so we only chose swh and xpl for further discussion for swh fig 8 a six scenarios with high and low joint entropy were chosen for comparison scenario 1 2 and 3 represented the optimal networks of the 80 th series series started from around 2008 03 01 for three window lengths with relatively high joint entropy scenario 4 5 and 6 represented the optimal networks of the 120 th series series started from around 2009 04 05 for three window lengths with relatively low joint entropy the selected stations and information content were provided in table 4 similarly six scenarios from different series scenarios 1 2 3 are the 55 th series started from around 2007 06 25 and scenarios 4 5 6 are the 110 th series started from around 2008 12 26 were also chosen for xpl fig 8 b table 5 basically the differences between high and low joint entropy gradually fell in a narrower range with increasing window length however the pattern was not the same for total correlation also the optimal network was highly impacted by temporal variability and changes in entropy values would also correspond to different selected stations much fewer stations only 6 and 7 stations were selected for 1 year series than in other scenarios in swh table 4 however the differences between the number of selected stations in different scenarios were not much significant in xpl table 5 regarding specific stations selected in the optimal network there was no strong pattern among all scenarios but more stations tended to appear in both selected sets of scenarios with high and low joint entropy as the fixed window length increased tables 4 and 5 marked as red numbers specifically only one station no 5 appeared both in selected sets of scenario 1 and 4 while six stations no 17 4 3 5 18 14 appeared both in selected sets of scenario 2 and 5 and the number for scenario 3 and 6 was eight stations no 2 20 5 18 24 16 6 4 in xpl since the selected number was much larger selected sets of stations were much more similar in both scenarios of the same length with high and low joint entropy specifically the numbers marked as red were 12 stations for scenario 1 and 4 13 stations for scenario 2 and 5 and 14 stations for scenario 3 and 6 naturally larger selected number of stations would lead to less different selected set the differences in the selected network were much more noticeable in swh shanghai since the number of selected stations would be more affected by the window length of the series generally high or low joint entropy signified different specific rainfall distribution patterns during a given time period therefore the sliding series using mimr may also help find the changing point in rainfall distributions for rainfall network optimization 5 3 ranking variation and dynamic network evaluation apart from variations of entropy values rankings by mimr of all stations also fluctuated with time lapse from bubble plots figs 9 and 10 with the bubble size representing the relative frequencies of different ranges of the assigned rank in descending order i e the top 20 rank meaning the most important it can be concluded that a rainfall station can either be high ranked or low ranked during different times results showed that no clear fixed ranks were found for most stations in shanghai as most bubbles on the same horizontal lines were of similar sizes in fig 9 a and b meaning that most stations could be ranked with any order especially for 1 year and 2 year sliding series in other words it can be concluded that the importance of stations fluctuated a lot and exhibited great variability for 1 year and 2 year sliding series for example bubbles of station no 5 15 16 in seh were almost of similar sizes for different ranks fig 9 a meaning that the rank of the station can be either high or low in different times similar results were found for many other stations in seh e g station no 22 and station no 23 meanwhile we also found that as the time window got longer the differences between relative frequencies of different ranks of some stations became a little more clear for instance station no 14 seh was mostly ranked the last 20 and station no 19 seh was mostly ranked 20 40 for 5 year time window fig 9 c it can be explained that when a longer time window was used for temporal variability analysis the variation in rainfall series would be more moderate with lower percentage of days changed and the optimization results would accordingly be less affected and fluctuating in swh fig 9 d f similar results again warned that rankings of rainfall stations can vary a lot and getting an optimal result only on a fixed long observation period may bring risk at times compared with seh and swh differences between relative frequencies of different assigned ranks for a specific station were a little more noticeable in xpl and xmh and longer time window again brought more diverging bubble sizes for instance station no 6 xpl fig 10 c was always ranked top 20 while station no 7 xpl fig 10 c was mostly ranked last 20 it can be assumed that the temporal variability of rankings might be a little lower for stations in xi an than stations in shanghai considering dynamic network evaluation rdi of all stations under different time windows were calculated for the convenience of discussion only the top five stations with highest rdi were selected for assessment see in table 6 stations with the highest rdi were not the same under different time windows but some stations would appear at the top in rdi order under different window lengths e g station no 20 seh and station no 22 xpl generally a shorter window length would yield a higher rdi for the same station for example station no 20 in seh yielded rdis of 0 96 0 93 and 0 82 respectively for 1 year 2 year and 5 year windows the results by rdi again confirmed the variation patterns under different window lengths shown in the bubble plots figs 9 and 10 moreover ranking variations for top five stations are shown in figs 11 and 12 under different sliding time windows the ranks of these stations showed high temporal variability under sliding series indicated by a broad range of assigned color referring to the specific rank during the observation period on one hand for instance station no 22 seh was in the selected set based on the 10 year series see in table 3 however during 1 year series starting from 2006 to around 2008 fig 11 1 it was frequently ranked last except several blue lines while after around june in 2009 it may become more important indicated by more blue lines on the other hand though station no 20 seh was not selected for the optimal network based on the 10 year series see in table 3 it was found to be relatively important during the sliding series starting from around april in 2009 regardless of the window length fig 11 1 2 3 another case is station no 17 swh which was selected in the optimal network see in table 3 however its rank fluctuated a lot both in 1 year and 2 year windows of the sliding series fig 11 4 5 especially from 2008 03 11 to 2010 05 20 as seen from either dark blue or dark red lines similar results can be found in xpl station no 13 18 etc fig 12 1 2 3 and xmh station no 1 3 4 etc fig 12 4 5 6 generally the variation patterns were not very regular and it was also very difficult to predict when a station would be more important however we gained an insight from the results that the ranking from mimr based on a fixed observation period may not be much reliable if temporal variability is considered as a consequence we should be more careful when using the mimr based ranking maybe other ranking criteria for network design due to the significant temporal variability impact the dynamic network evaluation can be viewed as a supplement for mimr selection based on fixed long time series moreover we tend to suggest avoiding the reduction of stations with great ranking variability in the final network if economic conditions allow 5 4 comparison between optimal networks under wet and dry seasons temporal variability is partly due to forcing factors and changing patterns for rainfall distribution both in space and time therefore we discuss rainfall optimization under different meteorological conditions the optimal network results and relative network performances were found to be different in wet season and dry season see table 7 results for the optimized network with the entire series ten years are also provided for reference in general wet season produced more information than did dry season for the joint entropy was approximately 2 bits higher in wet season than in dry season in four subareas moreover wet season even generated higher joint entropy than the entire series which might be explained by relatively more concentrated rainfall and variability during this period for example the average daily rainfall for shanghai was 4 56 mm during wet season and 2 31 mm during dry season meanwhile the average variances were 160 58 for wet season and 52 12 for dry season though the same numbers of stations were selected in wet and dry seasons for swh a little more total correlation redundant information was also obtained in wet season the range of differences of the total correlation was even larger for xpl about 15 bits in two seasons furthermore more transinformation in wet season suggested that the information transition ability was higher during this period for the four subareas in total the mimr values in different cases signified that the optimal network from wet season may yield higher information efficiency than that from dry season despite different entropy values and rankings the majority of selected stations were similar under three conditions wet season dry season and entire series considering the relatively high overlapping ratio in table 7 we noticed that the optimal network of the wet season was slightly more different from the optimal network of the entire series in the central area of shanghai fig 13 1 for instance stations no 7 8 9 and 19 in swh were included during wet season but were unselected in the optimal network of dry season and entire series fewer stations in the southwestern area of xi an only station no 2 4 8 in xmh fig 13 4 were selected in the optimal network of the wet season than the other two optimal networks generally the rainfall distribution pattern during wet season could be much different from dry season or entire series which may explain the dissimilarities among different optimal networks under different meteorological conditions thus some of the selected or unselected stations would be temporarily of importance or redundancy so we again suggest that great caution should be exercised when dealing with the optimization of a rainfall network since the elimination of some stations based on fixed entire series would cause significant information loss or redundancy under other conditions in general optimizing the rainfall network to make it more adapted to the changing meteorological conditions may be more recommended according to our results 6 conclusions this study focused on rainfall network optimization considering temporal variability results from the study suggest that temporal variability of rainfall network optimization may have a great impact on relative network performance entropy values and station rankings especially under sliding inter annual series for two study areas optimal networks tended to be more different under shorter shifting time windows however the information content of optimal networks under different lengths of sliding windows showed similar and even periodic variation patterns meaning that the temporal variability can be captured by the sliding series application of the proposed framework can help get an insight into network optimization with respect to temporal variability and even identify the changing point for network optimization under such situation the optimized rainfall network may not be the same as the optimal one based on the entire series which especially weakens our confidence in the reduced network based on static boundary conditions additionally the proposed framework of rainfall network evaluation aims to complement the primary mimr optimization approach this is because it does not change the procedure of mimr selection but adds an independent dynamic evaluation process to make rankings for stations under different times with the obtained rankings we can identify stations whose importance is most likely to be impacted by temporal changes with high rdi consequently design of an optimal network should be careful enough to avoid risks of losing information under different temporal cases on one hand for those stations whose ranks fluctuate a lot even under longer window length we suggest greater caution should be exercised on the decision of reducing them just based on an optimal result from the entire series on the other hand for those stations which are always ranked last in the sliding series more certainty can be gained on removing them still we should point out that a longer window length can be adopted e g 10 year in the future if a longer series is available also other optimization approaches may be applied in the dynamic network evaluation framework for comparison and verification in future studies differences between optimal networks from wet season and dry season revealed that meteorological conditions may impact the resulting optimal network and rainfall network optimization based on fixed entire series would cause information loss or redundancy under different conditions as meteorological conditions can partly be seen as one of the inner forcing factors for temporal variability of rainfall network the study on network optimization under different meteorological conditions is critical for better understanding and recognition of dynamic network design since our division and classification was relatively simple and rough more careful and multifarious categories of meteorological conditions can be defined and compared in the future generally how to relate rainfall network optimization to its regional dynamic rainfall distribution characteristics in order to derive a more integrated and practical optimization scheme under changing environment is a future direction that deserves consideration acknowledgments this study was supported by national natural science fund of china no 41571017 51679118 91647203 national key research and development program of china 2017yfc1502704 2016yfc0401501 appendix a see appendix a 
7375,rainfall networks are the most direct sources of precipitation data and their optimization and evaluation are essential and important information entropy can not only represent the uncertainty of rainfall distribution but can also reflect the correlation and information transmission between rainfall stations using entropy this study performs optimization of rainfall networks that are of similar size located in two big cities in china shanghai in yangtze river basin and xi an in yellow river basin with respect to temporal variability analysis through an easy to implement greedy ranking algorithm based on the criterion called maximum information minimum redundancy mimr stations of the networks in the two areas each area is further divided into two subareas are ranked during sliding inter annual series and under different meteorological conditions it is found that observation series with different starting days affect the ranking alluding to the temporal variability during network evaluation we propose a dynamic network evaluation framework for considering temporal variability which ranks stations under different starting days with a fixed time window 1 year 2 year and 5 year therefore we can identify rainfall stations which are temporarily of importance or redundancy and provide some useful suggestions for decision makers the proposed framework can serve as a supplement for the primary mimr optimization approach in addition during different periods wet season or dry season the optimal network from mimr exhibits differences in entropy values and the optimal network from wet season tended to produce higher entropy values differences in spatial distribution of the optimal networks suggest that optimizing the rainfall network for changing meteorological conditions may be more recommended keywords entropy rainfall network design temporal variability dynamic network evaluation framework maximum information minimum redundancy 1 introduction a hydrometric network is a data collection system which is of fundamental significance for research and development mishra and coulibaly 2009 reviewed several commonly used methods for hydrometric network design and evaluation including 1 statistically based 2 information theory based 3 user survey 4 hybrid 5 physiographic components and 6 sampling strategies chacon hurtado et al 2017 made a review of rainfall and streamflow sensor network design with respect to different criteria proposed a framework for classifying the design methods and suggested a generalized procedure for optimal network design in recent years hydrometric network evaluation and optimization methods particularly based on entropy have received significant attention keum et al 2017 reviewed studies that applied entropy theory in water monitoring network design and evaluation especially for publications after 2009 which were not covered in the review by mishra and coulibaly 2009 several groups of methods using entropy theory have been developed for network design and optimization fahle et al 2015 divided most methods into three main groups 1 derivation of minimum distance between stations or formation of regional information maps e g husain 1989 masoumi and kerachian 2010 su and you 2014 2 provision of optimal sets or ranking of stations or iterative updating until reaching a certain threshold relative to specific objective functions for existing networks e g markus et al 2003 alfonso et al 2010a li et al 2012b mishra and coulibaly 2014 stosic et al 2017 and 3 multi objective optimization for simultaneously fulfilling several objectives of network design e g alfonso et al 2010b samuel et al 2013 alfonso et al 2014 xu et al 2015 leach et al 2015 leach et al 2016 keum and coulibaly 2017a b sometimes hybrid methods are also applied and different groups of methods are compared a brief literature review on hydrometric network design and optimization based on entropy is now given langbein 1954 first proposed that monitoring network efficiency should be based on space time measure of information caselton and zidek 1984 chose optimal stations from maximum information transmission of the whole network husain 1987 used an objective function for network information based on bivariate normal multivariate normal and lognormal distributions but limited it to reducing stations from a dense network later he improved the method generalized it for multivariate gamma distribution and presented an approach based on transinformation distance correlation for expansion of a sparse network husain 1989 more improvements using information transmission and distance correlation have since been made for network design and optimization e g mogheir et al 2003 mogheir et al 2006 masoumi and kerachian 2008 masoumi and kerachian 2010 owlia et al 2011 su and you 2014 many other measures originating in entropy theory have been applied to network evaluation such as information transfer index iti directional information transfer dit coefficient of non transferred and transferred information t1 and t0 ensemble entropy permutation entropy and others see krstanovic and singh 1992a b yang and burn 1994 markus et al 2003 ridolfi et al 2011 alfonso et al 2014 stosic et al 2017 additionally network evaluation and optimization have encompassed various types of hydrometric networks including water quality and groundwater monitoring networks harmancioglu and alpaslan 1992 ozkul et al 2000 mogheir and singh 2002 mogheir et al 2004a mogheir et al 2004b 2006 guo and wang 2010 leach et al 2016 rainfall networks krstanovic and singh 1992a b yoo et al 2008 chen et al 2008 su and you 2014 xu et al 2015 yeh et al 2017 streamflow gauge networks markus et al 2003 mishra and coulibaly 2010 alfonso et al 2012 li et al 2012b samuel et al 2013 mishra and coulibaly 2014 leach et al 2015 stosic et al 2017 and water level gauge networks alfonso et al 2010a b 2014 mondal and singh 2012 fahle et al 2015 among different types of hydrometric networks regional rainfall networks provide a basis for analyzing rainfall characteristics flood forecasting water level monitoring and design of hydraulic structures thus evaluation and optimization of rainfall networks remain an important issue in addition the effect of temporal variability and spatial differences on network design and optimization has received significant attention recently e g wei et al 2014 mishra and coulibaly 2014 fahle et al 2015 climate change human activities and nonstationarity of hydrological processes cannot be overlooked during hydrometric network design and management especially for rainfall events which are highly heterogeneous localized and strongly influenced by geographical topographical and climate factors zhang et al 2014 song et al 2015 wang et al 2016 gu et al 2017 different temporal conditions can have appreciable impacts on hydrometric network design meaning that an optimal network may only be optimal during the time when observations were made though temporal variability affects network optimization and its transferability to changing conditions to the best of the authors knowledge studies on rainfall networks with respect to this temporal variability are limited accordingly evaluation of rainfall networks under different times should be analyzed to complement the design procedure and to further strengthen the reliability and flexibility of network optimization the objectives of this study therefore are to 1 propose a framework for assessing the temporal variability of optimized rainfall networks by maximum information minimum redundancy mimr criterion during changing periods with fixed window lengths 2 examine how large the temporal variability is in terms of entropy values and rankings and quantify its effect on rainfall station ranking by ranking disorder index rdi and 3 investigate impacts of meteorological conditions on network optimization especially for wet season and dry season and compare the resulting optimal networks the framework proposed in this study and related discussion and results will be useful for decision makers for obtaining optimal rainfall networks considering temporal variability specifically for designing and optimizing a rainfall network with a dynamic perspective in addition before conducting network optimization we compare different quantization methods for entropy estimation and briefly discuss the impact of data length which has been proposed and discussed in entropy related hydrological studies mishra et al 2011 mishra and coulibaly 2014 keum and coulibaly 2017b the paper is organized as follows first in section 2 we introduce information entropy concepts and their estimation in section 3 natural and geographical conditions of the study areas as well as current rainfall networks are described in section 4 methodologies including mimr algorithm rdi and a dynamic network evaluation framework considering temporal variability are presented in section 5 we provide results and discussion on our rainfall networks conclusions and outlook are presented in the last section 6 2 entropy concepts shannon 1948 laid the mathematical foundation of entropy as a measure of information or uncertainty entropy quantifies the uncertainty of a random variable via its probability distribution in hydrometeorology hydrology and water resources frequently used entropy measures include marginal entropy joint entropy transinformation mutual information and total correlation 2 1 basic entropy measures for entropy measures used in network design and optimization there are basically three questions we need to address 1 how much information would a station or several stations contain 2 how much information can one station or several stations transmit to other stations 3 how much information is shared by several stations these three questions are answered by marginal entropy joint entropy transinformation and total correlation consider a discrete variable x with n possible results xi i 1 2 3 n where the probability of occurrence of xi is pi 0 p i 1 and i p 1 the marginal entropy is defined as 1 h x h p 1 p 2 p 3 p n k i 1 n p i log p i where k is a constant and in general it can be taken as unity singh 2013 the marginal entropy quantifies the average reduction in uncertainty of x or its probability distribution since variance is a measurement of dispersion or variability of a variable it has been compared with entropy by wei 1987 and ebrahimi et al 1999 while variance measures concentration only around the mean entropy measures diffuseness of the probability irrespective of the location of concentration and may also be related to higher order moments of a distribution leading to a closer characterization of the probability density function mishra et al 2009 there is therefore no unique relation between variance and entropy for two discrete random variables x and y with the joint probability of x i and y j denoted as p x i y j p ij i 1 2 n j 1 2 m the joint entropy between them can be defined as 2 h x y i 1 n j 1 m p ij log p ij transinformation describes transmission or sharing information between two random variables which is defined as 3 t x y h x h y h x y for more than two variables x1 xn multivariate joint entropy with p i 1 i n i 1 1 2 n 1 i 2 1 2 n 2 i n 1 2 n n being an n dimensional probability distribution can be defined as 4 h x 1 x n i 1 1 n 1 i 2 1 n 2 i n 1 n n p i 1 i n log p i 1 i n total correlation describes information redundancy within multiple variables which is an extension of transinformation and is defined as mcgill 1954 watanabe 1960 5 c x 1 x n i 1 n h x i h x 1 x n to illustrate bivariate and multivariate joint entropy transinformation total correlation and relationships among them more directly venn diagrams are provided in fig 1 for clarity note that all the measures introduced above are with a logarithmic base of 2 so the units of entropy are bits also if p i equals zero then we stipulate that 0log0 0 2 2 estimation of entropy measures as the probability distribution is contained in the definition of entropy the key point is to estimate the probability distribution the methods of probability distribution estimation can mainly be classified into two types 1 fitting a probability distribution function pdf to data and its parameter estimation and 2 nonparametric density estimation without assuming a distribution in the first type typical distributions such as normal lognormal and gamma have been mostly applied e g husain 1989 krstanovic and singh 1992b harmancioglu and alpaslan 1992 yoo et al 2008 when it comes to multivariate case the calculations become quite difficult for other distributions further many natural phenomena like streamflow and precipitation are heavy tail distributed li et al 2012a and the distribution assumptions may be debatable due to the unsuitable selection of distributions while nonparametric density estimation avoids this step in the entropy estimation fahle et al 2015 among different nonparametric estimators the most predominant and frequently used is the binning estimator since it is easy to understand and compute e g markus et al 2003 alfonso et al 2010a li et al 2012b the main idea of this method is to discretize approximately a continuous random variable and there are different methods of discretization for example histogram method and mathematical floor function for the histogram method bins are often selected to group observation data into different classes and then use frequency as a substitute for probability in matlab 2016a version the function histcounts which sets a binwidth parameter automatically or manually can be used for the histogram method the mathematical floor function is another method of discretization alfonso et al 2010a which directly transfers continuous signals into discrete pulses and the conversion is performed by floor brackets as 6 x q a 2 x a 2 a where x denotes the analog e g continuous value and x q denotes the quantized discrete value while a denotes the bin width other commonly used nonparametric methods especially for estimating mutual information include kernel density estimation kde e g yang and burn 1994 mishra and coulibaly 2014 and k nearest neighbor knn e g kraskov et al 2004 khan et al 2007 fahle et al 2015 however due to their limited applications and sophisticated calculations in multivariate entropy estimation we finally chose the binning estimator for our entropy calculations generally the bin width selection has always been an important issue for it directly affects the approximate probability distribution and consequently affects the entropy value e g amorocho and espildora 1973 scott 1979 shimazaki and shinomoto 2007 ruddell and kumar 2009 the influence of different bin widths or class intervals on the entropy value has been discussed by singh 1997 and careful selection of class interval and sampling interval are suggested because the entropy value decreases as the class interval or the sampling interval increases both fixed bin width and optimal bin width can be chosen and no standard rules for bin size class interval exist but empirical formulas can be referred to papana and kugiumtzis 2009 scott 1979 suggested an optimal bin width as 7 a 3 49 σ x n 1 3 where σ denotes the standard deviation of an observation series of x e g daily series of one station and n denotes the sampling size scott 1979 sturges 1926 presented the optimal bin width as 8 a r x 1 log 2 n where r x is the range of the observation series of x and n is the sampling size of x bendat and piersol 1966 developed another method for determining the optimal bin width as 9 a r x 1 87 n 1 0 4 where r x is the range of the observation series of x and n is the sampling size of x there have been some comparisons between different bin width selections e g fahle et al 2015 keum and coulibaly 2017a fahle et al 2015 indicated that a fixed bin width was better suited since a temporary rise in nearly constant water level increased the standard deviation and corresponding larger optimal bin width and lower marginal entropy loss of information which is against the initial intention of water level monitoring i e detecting a change in the water level is more important in the case of keum and coulibaly 2017a they compared two fixed bin widths and two optimal bin widths sturges 1926 scott 1979 for entropy values and found no strong concordance among the four quantization cases two fixed bin widths and two optimal bin widths in entropy values though similar relative rankings were found for precipitation stations most streamflow stations yielded very different relative rankings under different quantization cases no matter which specific method is applied the estimation method will influence the entropy value and hence rankings fahle et al 2015 keum and coulibaly 2017a and the influence can be specific for different cases so we will also compare different methods for multivariate entropy estimation it is necessary to estimate the multivariate joint distribution to calculate the joint entropy alfonso et al 2010b and li et al 2012b applied a grouping property see kraskov et al 2005 for the estimation of multivariate joint entropy the basic idea for this method lies in generating a new variable containing the information equal to that of the original variables it can be verified that the information amount keeps invariant under this merging method multiple variables can be converted to a nested variable and then multivariate joint entropy can be computed for example h x y z can be rewritten as h a z h x y z with denoting the merging operator in a similar way multivariate joint entropy with n variables can be transformed as 10 h x 1 x 2 x n h x 1 x 2 x 3 x n h x 1 x 2 x 3 x n h x 1 x 2 x 3 x n 1 x n h x 1 x 2 x 3 x n 1 x n then the total correlation can be computed by eq 5 or also by a grouping property the variable merging method is straightforward and for detailed information the reader can refer to alfonso et al 2010b and li et al 2012b our study will apply this merging method for multivariate entropy calculation 3 material the yangtze river and the yellow river are the two most important rivers in china their watersheds exhibit greatly different geographical and hydrological characteristics we selected two typical big cities shanghai in taihu lake basin the yangtze river and xi an in wei river basin the yellow river as case studies since they have similar rainfall network sizes these two big cities have dense population and are both highly urbanized therefore it is important to study the hydrological and meteorological characteristics of these areas and evaluate rainfall networks therein 3 1 study area shanghai is located on the west coast of the pacific ocean along the eastern asian continent in the front of the yangtze river delta it is east of east china sea south of hangzhou bay west of jiangsu province and zhejiang province north of the mouth of the yangtze river and also the yangtze river and east china sea connect here by the end of 2003 shanghai covered an area of 6340 5 square kilometers accounting for 0 06 of the total area of the country most of the area is covered by flat plains being part of the yangtze river delta with an average altitude of 4 m above the sea level shanghai has north subtropical maritime monsoon climate and is also the convergence zone of the north and south cold warm air masses alternately affected by cold and warm air and marine moist air shanghai has humid climate and four distinct seasons with abundant sunshine and precipitation and its wet season extends from may to september characterized by three relatively low rainfall periods midsummer late autumn and winter and three relatively high rainfall periods spring rainy meiyu and autumn rainy the mean annual rainfall is about 1123 7 mm and it is one of the areas where rainstorms occur frequently with 85 of them occurring in the flood season and mostly concentrated during june september huangpu river is the biggest river that forms the land water system of shanghai which originates from taihu lake with a total length of 113 km and a width of 300 770 m flowing through the urban area and becoming an important water transportation route in shanghai xi an is located in the central part of the yellow river basin it is the political economic and cultural center of shaanxi province the city covers an area of 10 108 square kilometers of which the urban area is about 1066 square kilometers the landform of xi an includes four basic types plain loess tableland hill and mountain with the main geomorphic characteristics of the city being high in the south and low in the north another characteristic is its clear boundary between plain and mountain qinling mountains and wei river plain constitute the main typical topography of this area wei river plain is formed by the alluviation of wei river the largest tributary of the yellow river and its tributaries which is also known as the guanzhong plain hills are mainly distributed in the south of mount li lantian county and lintong county covering an area of about 740 square kilometers accounting for 7 41 of the city s total area mountains include the north slope of the qinling mountains and mount li covering an area of 4875 6 square kilometers accounting for 48 84 of the city s total area xi an is located in the transitional zone between humid climate of the southeast coast and arid climate of the northwest inland and therefore has two types of climate characteristics it has warmtemperate semi humid continental monsoon climate and the wet season extends approximately from may to september the mean annual rainfall is about 537 5 1028 4 mm and the spatial distribution of rainfall shows significant differences with more rainfall in southern qinling mountains than in northern wei river plain interannual variability of rainfall is also significant for wet year exceeding the average by 15 33 and dry year being only 56 7 77 4 of the average 3 2 dataset and network in total 47 rainfall stations in shanghai except chongming district and 53 rainfall stations in xi an and their daily precipitation data of ten years from 2006 to 2015 were chosen for a case study shanghai is further divided into two parts by the huangpu river which flows through the area and splits shanghai into two parts east of huangpu river seh with 23 rainfall stations and west of huangpu river swh with 24 rainfall stations xi an is also divided into two parts according to topography one part is plain with 24 rainfall stations xpl and another part is mountains and hills with 29 rainfall stations xmh here we should note that two stations no 10 11 in swh located in kunshan jiangsu province are included since they are close to other stations in the network system similarly three stations no 4 5 11 in xpl located in xianyang are included rainfall networks in two study areas with dem values are briefly shown in fig 2 detailed geographical coordinate information of stations can be referred to in appendix a for prior knowledge of the two study areas we first computed the network density and referred to hydrological guides for rainfall gauges provided by wmo the guide to hydrological practices 2008 and ministry of water resources 2013 as shown in table 1 according to wmo the recommended minimum densities of precipitation stations for coastal mountains hilly area and interior plains are 900 250 575 and 575 km2 per station respectively according to the guidelines of china the control area of a single station should not be greater than 200 km2 150 km2 especially in the plain river network region thus the rainfall network densities of these two cities shown in table 1 already meet both global and regional demands for recommended minimum network densities since it is not readily definable whether to add or reduce stations we used the existing network for temporal analysis especially for network reduction scenarios however the framework of analysis can be applied and generalized for network augmentation or relocation if data in ungauged area is available by simulation or interpolation meanwhile it should be realized that regulations are often strict not in terms of station density but in the quality of data for providing information about the hydrological system chacon hurtado et al 2017 since one of the main objectives is to present a framework for dynamic rainfall network evaluation we will mainly focus on rankings of importance for existing rainfall network and its corresponding network performance in terms of entropy values 4 methodology in this section an information theory based method applied for rainfall network evaluation and design is introduced named maximum information minimum redundancy mimr greedy ranking algorithm and a ranking disorder index rdi for rainfall stations based on normalized apportionment entropy is supplemented for identifying stations with high temporal variability of the rankings besides the framework of dynamic rainfall network evaluation mainly considering temporal variability analysis is developed for further discussion 4 1 mimr algorithm mimr li et al 2012b greedy ranking criterion provides an easy to implement way to solve the network optimization problem mainly suitable for selecting the best set of stations from a dense network or to obtain ranking of importance for stations in the network the objective function contains three items joint entropy h transinformation t and total correlation c and the optimization problem is formulated as max mimr λ 1 h x s 1 x s 2 x s k i 1 m t x s 1 x s 2 x s k x f i λ 2 c x s 1 x s 2 x s k subject to k m n 11 h x s 1 x s 2 x s k pct h x s 1 x s 2 x s k x f 1 x f 2 x f m where k is the number of selected stations denoted as a set of s m is the number of unselected stations denoted as a set of f n is the total number of stations in the current network and λ1 λ2 are trade off weights between information and redundancy and λ 1 λ 2 1 pct is the percentage of joint entropy representing the ratio between joint entropy for the selected stations and joint entropy for all stations in the existing network the first item in mimr denotes the total information contained by the selected stations in the network eq 4 the second item denotes the information that can be obtained about unselected stations from the selected stations eq 3 and the third item denotes the information redundancy among the selected stations eq 5 to solve the optimization problem and avoid computation burden from global search li et al 2012b generalized a greedy selection algorithm first we identify the central station with the highest marginal entropy and then rankings of stations can be obtained by implementing the mimr criterion step by step as a threshold pct is set e g obtaining 95 of the total joint entropy in the existing network we can get an optimal subset of stations for the final decision after selecting the last station rankings of all stations can be determined and used for further variability analysis for more details about mimr readers can refer to li et al 2012b since the first goal of network design is generally accepted as maximizing information of the network system λ1 is usually larger than λ2 suggested by li et al 2012b as 0 8 we made sensitivity analysis on different information redundancy trade off weights and found that most stations kept stable with λ1 varying from 0 5 to 1 λ2 varying from 0 5 to 0 to sum up the default parameter set for our mimr selection is given by λ1 0 8 λ2 0 2 and pct 95 4 2 ranking disorder index to assess the dispersion of ranks for a specific rainfall station by mimr we adopted a ranking disorder index rdi for identifying rainfall stations whose importance is mostly affected by temporal change the definition of the index originates from apportionment entropy ae maruyama and kawachi 1998 kawachi et al 2001 maruyama et al 2005 as 12 ae i 1 n r i n log 2 r i n where n is the number of all possible ranks for a station so it is equal to the number of stations in a network n is the number of ranks under different times and r i is the number of a certain ith rank as the maximum values of ae is log 2 n rdi is calculated by normalized apportionment entropy nae defined as 13 rdi nae ae log 2 n the rdi is an index for the uncertainty and fluctuation of the rank assigned to a rainfall station fahle et al 2015 which mainly considers the impact of temporal change on rankings of rainfall stations thus a higher rdi reflects a station whose importance is more affected by temporal variability so more careful attention should be paid to such station as its importance can vary a lot during a relatively short period 4 3 dynamic network evaluation framework one of the main objectives of this study is to present a dynamic network evaluation framework from a given length of time series fig 3 shows an overview of the proposed framework and a brief step by step description is given below step 1 obtain rainfall data at all stations in a network step 2 preprocess data for different time windows e g 1 year 2 year 5 year step 3 do mimr ranking with different data series from step 2 step 4 obtain network performances entropy values and rankings for sliding series step 5 compute ranking disorder index rdi for all stations with rankings in step 4 step 6 rank all stations in descending order of rdi step 7 identify ranking variations for individual stations step 8 obtain final suggestions for network design for data preprocessing in step 2 we mainly focus on shifting inter annual time window to assess temporal variability from the inter annual aspect fig 4 shows how sliding inter annual observation series fixed window length of 1 year 2 year and 5 year with a 10 day increment are obtained from a continuous ten year series 2006 2015 to assess the temporal variability from the seasonal aspect we divide the entire data set into wet season 5 months may to september and dry season 7 months october to april to see the seasonal variability effect on average and avoid years with extreme precipitation we use a 50 month series 5 months for one year 10 years in total for wet season and 70 month series 7 months for one year 10 years in total for dry season 5 results and discussion 5 1 entropy estimation and data length impact entropy values are affected by different binning parameters when using the binning estimator we compared fixed and optimal bin widths considering the fixed bin width a in eq 6 was set as 1 mm d and 3 mm d for optimal bin width three methods scott sturge bendat piersol were used to calculate the optimal bin width for each station table 2 shows that entropy values diverged for different bin widths especially for joint entropy and total correlation with fixed and optimal bin widths the differences between two fixed widths a 1 mm d and a 3 mm d suggested that a larger bin width led to smaller entropy values which agreed with the results from singh 1997 for the optimal bin size suggested by scott bendat piersol and fixed bin width as a 1 mm d the maximum marginal entropy and average marginal entropy were a little larger than the values calculated using sturge s method and the fixed bin width as a 3 mm d though the maximum marginal entropy affected the first step in mimr ranking the differences between different methods fell in a narrow range meanwhile both fixed bin widths with a 1 mm d and a 3 mm d had joint entropies lower than 4 bits while the other three methods yielded average joint entropies higher than 5 bits as the saturated value for the joint entropy was equal to log 2 n 11 83 bits n is the number of data points given by ten year daily series which is equal to 3652 the joint entropies estimated by fixed bin widths with a 1 mm d and a 3 mm d were much lower than the saturated joint entropy the total correlation was also subject to different bin widths and even related to the number of variables stations therefore we provided not only the total correlation but also the values divided by the number of stations tc n though entropy values would change under different binning estimations it was interesting to find that the relative differences between areas were maintained for instance all the entropy values of xpl were the smallest among the four subareas no matter which binning estimation was used in addition the entropy values of seh and swh were always similar regardless of different binning sizes going one step further basic statistical properties of precipitation in the four subareas can serve as a reference fig 5 from fig 5 xpl showed the lowest mean precipitation lowest maximum precipitation and lowest standard deviation std dev additionally seh and swh had similar mean precipitation and standard deviation hence it can be assumed that the average rainfall and its variability roughly determined the intrinsic characteristic of a network which would be consistently quantified by entropy values as a consequence compared with other three subareas rainfall network in certain areas such as interior plain xpl with both less rainfall and lower variability would generate less information in total relative differences for entropy values among different areas were maintained with five estimation methods to avoid multifarious analysis with different estimation methods we finally chose scott s method for entropy estimation in the following sections as it yielded the largest marginal entropy and joint entropy in order to see the impact of data length on entropy values we calculated average marginal entropy joint entropy and total correlation with increasing length of data series fig 6 shows that variations of entropy values are similar for the four subareas generally the values became larger with increasing data length and then tended to stabilize it should also be noted that the increasing trends were not monotonous and fluctuations may emerge at certain data lengths e g around 500 days 1200 days and 1500 days this agrees with the previous conclusion that both joint entropy and total correlation were sensitive to time series length keum and coulibaly 2017b as they finally recommended the use of at least 10 years of daily time series data to avoid any significant information loss of the network we also used a ten year series for mimr selection to get an optimal network for reference however shorter lengths of series 1 year 2 year and 5 year were still used for assessing temporal variations of the optimized network with mimr due to limited length of data series available we also noticed that the entropy values had nearly reached 70 80 and 90 of the values estimated by 10 year series using 1 year 2 year and 5 year series respectively more importantly network performance was subject to inter annual variability and it was necessary to capture and analyze the variability with relatively shorter time series 5 2 optimal networks under sliding series using mimr before optimizing rainfall networks under sliding series primary mimr ranking and selection results based on the entire 10 year series are shown in table 3 generally higher percentage of stations were selected for xpl and xmh than those for seh and swh though mimr values for xpl and xmh were relatively lower in addition the lower mimr values were mainly due to lower transinformation in xpl and xmh in order to conduct temporal variability analysis on network optimization it is necessary to identify impacts of sliding time series on optimal rainfall networks using mimr the graphs in fig 7 show the dispersion pattern and temporal evolution trend of the information joint entropy and redundancy total correlation content for optimal networks in four subareas with different time series which are from moving time windows of 1 year 2 years and 5 years for the sake of comparison the ranges of coordinate axes were deliberately set as the same under different time windows from fig 7 left column a longer time window brought less diffusive pattern of joint entropy and total correlation meanwhile the quantity of joint entropy and total correlation also generally increased with longer time windows shown as scatters in three subplots from top to bottom gradually shifting to upper right of the subplot which again confirmed the data length impact discussed in section 5 1 considering the temporal evolution of entropy values the middle and right columns in fig 7 revealed that the fluctuations could be more significant with 1 year series and 2 year series in total the results revealed that the performance of the optimal network using mimr could fluctuate a lot especially under shorter duration however it was found that the variation trend of joint entropy would be basically maintained for three time windows from fig 7 middle column variation curves with different time windows especially for 1 year and 2 year as the total number of series was greater than the number of 5 year series showed similar patterns and periodicity for joint entropy yet the similarity was not much comparable for total correlation under three time windows this demonstrated that changing series indeed affected the information content of the optimal network in such case a finer time window may capture this variability and periodicity more easily and optimization based on long time series may weaken such temporal variability in addition two subareas in the same city i e seh and swh in shanghai xpl and xmh in xi an also showed similar variation patterns for joint entropy so we only chose swh and xpl for further discussion for swh fig 8 a six scenarios with high and low joint entropy were chosen for comparison scenario 1 2 and 3 represented the optimal networks of the 80 th series series started from around 2008 03 01 for three window lengths with relatively high joint entropy scenario 4 5 and 6 represented the optimal networks of the 120 th series series started from around 2009 04 05 for three window lengths with relatively low joint entropy the selected stations and information content were provided in table 4 similarly six scenarios from different series scenarios 1 2 3 are the 55 th series started from around 2007 06 25 and scenarios 4 5 6 are the 110 th series started from around 2008 12 26 were also chosen for xpl fig 8 b table 5 basically the differences between high and low joint entropy gradually fell in a narrower range with increasing window length however the pattern was not the same for total correlation also the optimal network was highly impacted by temporal variability and changes in entropy values would also correspond to different selected stations much fewer stations only 6 and 7 stations were selected for 1 year series than in other scenarios in swh table 4 however the differences between the number of selected stations in different scenarios were not much significant in xpl table 5 regarding specific stations selected in the optimal network there was no strong pattern among all scenarios but more stations tended to appear in both selected sets of scenarios with high and low joint entropy as the fixed window length increased tables 4 and 5 marked as red numbers specifically only one station no 5 appeared both in selected sets of scenario 1 and 4 while six stations no 17 4 3 5 18 14 appeared both in selected sets of scenario 2 and 5 and the number for scenario 3 and 6 was eight stations no 2 20 5 18 24 16 6 4 in xpl since the selected number was much larger selected sets of stations were much more similar in both scenarios of the same length with high and low joint entropy specifically the numbers marked as red were 12 stations for scenario 1 and 4 13 stations for scenario 2 and 5 and 14 stations for scenario 3 and 6 naturally larger selected number of stations would lead to less different selected set the differences in the selected network were much more noticeable in swh shanghai since the number of selected stations would be more affected by the window length of the series generally high or low joint entropy signified different specific rainfall distribution patterns during a given time period therefore the sliding series using mimr may also help find the changing point in rainfall distributions for rainfall network optimization 5 3 ranking variation and dynamic network evaluation apart from variations of entropy values rankings by mimr of all stations also fluctuated with time lapse from bubble plots figs 9 and 10 with the bubble size representing the relative frequencies of different ranges of the assigned rank in descending order i e the top 20 rank meaning the most important it can be concluded that a rainfall station can either be high ranked or low ranked during different times results showed that no clear fixed ranks were found for most stations in shanghai as most bubbles on the same horizontal lines were of similar sizes in fig 9 a and b meaning that most stations could be ranked with any order especially for 1 year and 2 year sliding series in other words it can be concluded that the importance of stations fluctuated a lot and exhibited great variability for 1 year and 2 year sliding series for example bubbles of station no 5 15 16 in seh were almost of similar sizes for different ranks fig 9 a meaning that the rank of the station can be either high or low in different times similar results were found for many other stations in seh e g station no 22 and station no 23 meanwhile we also found that as the time window got longer the differences between relative frequencies of different ranks of some stations became a little more clear for instance station no 14 seh was mostly ranked the last 20 and station no 19 seh was mostly ranked 20 40 for 5 year time window fig 9 c it can be explained that when a longer time window was used for temporal variability analysis the variation in rainfall series would be more moderate with lower percentage of days changed and the optimization results would accordingly be less affected and fluctuating in swh fig 9 d f similar results again warned that rankings of rainfall stations can vary a lot and getting an optimal result only on a fixed long observation period may bring risk at times compared with seh and swh differences between relative frequencies of different assigned ranks for a specific station were a little more noticeable in xpl and xmh and longer time window again brought more diverging bubble sizes for instance station no 6 xpl fig 10 c was always ranked top 20 while station no 7 xpl fig 10 c was mostly ranked last 20 it can be assumed that the temporal variability of rankings might be a little lower for stations in xi an than stations in shanghai considering dynamic network evaluation rdi of all stations under different time windows were calculated for the convenience of discussion only the top five stations with highest rdi were selected for assessment see in table 6 stations with the highest rdi were not the same under different time windows but some stations would appear at the top in rdi order under different window lengths e g station no 20 seh and station no 22 xpl generally a shorter window length would yield a higher rdi for the same station for example station no 20 in seh yielded rdis of 0 96 0 93 and 0 82 respectively for 1 year 2 year and 5 year windows the results by rdi again confirmed the variation patterns under different window lengths shown in the bubble plots figs 9 and 10 moreover ranking variations for top five stations are shown in figs 11 and 12 under different sliding time windows the ranks of these stations showed high temporal variability under sliding series indicated by a broad range of assigned color referring to the specific rank during the observation period on one hand for instance station no 22 seh was in the selected set based on the 10 year series see in table 3 however during 1 year series starting from 2006 to around 2008 fig 11 1 it was frequently ranked last except several blue lines while after around june in 2009 it may become more important indicated by more blue lines on the other hand though station no 20 seh was not selected for the optimal network based on the 10 year series see in table 3 it was found to be relatively important during the sliding series starting from around april in 2009 regardless of the window length fig 11 1 2 3 another case is station no 17 swh which was selected in the optimal network see in table 3 however its rank fluctuated a lot both in 1 year and 2 year windows of the sliding series fig 11 4 5 especially from 2008 03 11 to 2010 05 20 as seen from either dark blue or dark red lines similar results can be found in xpl station no 13 18 etc fig 12 1 2 3 and xmh station no 1 3 4 etc fig 12 4 5 6 generally the variation patterns were not very regular and it was also very difficult to predict when a station would be more important however we gained an insight from the results that the ranking from mimr based on a fixed observation period may not be much reliable if temporal variability is considered as a consequence we should be more careful when using the mimr based ranking maybe other ranking criteria for network design due to the significant temporal variability impact the dynamic network evaluation can be viewed as a supplement for mimr selection based on fixed long time series moreover we tend to suggest avoiding the reduction of stations with great ranking variability in the final network if economic conditions allow 5 4 comparison between optimal networks under wet and dry seasons temporal variability is partly due to forcing factors and changing patterns for rainfall distribution both in space and time therefore we discuss rainfall optimization under different meteorological conditions the optimal network results and relative network performances were found to be different in wet season and dry season see table 7 results for the optimized network with the entire series ten years are also provided for reference in general wet season produced more information than did dry season for the joint entropy was approximately 2 bits higher in wet season than in dry season in four subareas moreover wet season even generated higher joint entropy than the entire series which might be explained by relatively more concentrated rainfall and variability during this period for example the average daily rainfall for shanghai was 4 56 mm during wet season and 2 31 mm during dry season meanwhile the average variances were 160 58 for wet season and 52 12 for dry season though the same numbers of stations were selected in wet and dry seasons for swh a little more total correlation redundant information was also obtained in wet season the range of differences of the total correlation was even larger for xpl about 15 bits in two seasons furthermore more transinformation in wet season suggested that the information transition ability was higher during this period for the four subareas in total the mimr values in different cases signified that the optimal network from wet season may yield higher information efficiency than that from dry season despite different entropy values and rankings the majority of selected stations were similar under three conditions wet season dry season and entire series considering the relatively high overlapping ratio in table 7 we noticed that the optimal network of the wet season was slightly more different from the optimal network of the entire series in the central area of shanghai fig 13 1 for instance stations no 7 8 9 and 19 in swh were included during wet season but were unselected in the optimal network of dry season and entire series fewer stations in the southwestern area of xi an only station no 2 4 8 in xmh fig 13 4 were selected in the optimal network of the wet season than the other two optimal networks generally the rainfall distribution pattern during wet season could be much different from dry season or entire series which may explain the dissimilarities among different optimal networks under different meteorological conditions thus some of the selected or unselected stations would be temporarily of importance or redundancy so we again suggest that great caution should be exercised when dealing with the optimization of a rainfall network since the elimination of some stations based on fixed entire series would cause significant information loss or redundancy under other conditions in general optimizing the rainfall network to make it more adapted to the changing meteorological conditions may be more recommended according to our results 6 conclusions this study focused on rainfall network optimization considering temporal variability results from the study suggest that temporal variability of rainfall network optimization may have a great impact on relative network performance entropy values and station rankings especially under sliding inter annual series for two study areas optimal networks tended to be more different under shorter shifting time windows however the information content of optimal networks under different lengths of sliding windows showed similar and even periodic variation patterns meaning that the temporal variability can be captured by the sliding series application of the proposed framework can help get an insight into network optimization with respect to temporal variability and even identify the changing point for network optimization under such situation the optimized rainfall network may not be the same as the optimal one based on the entire series which especially weakens our confidence in the reduced network based on static boundary conditions additionally the proposed framework of rainfall network evaluation aims to complement the primary mimr optimization approach this is because it does not change the procedure of mimr selection but adds an independent dynamic evaluation process to make rankings for stations under different times with the obtained rankings we can identify stations whose importance is most likely to be impacted by temporal changes with high rdi consequently design of an optimal network should be careful enough to avoid risks of losing information under different temporal cases on one hand for those stations whose ranks fluctuate a lot even under longer window length we suggest greater caution should be exercised on the decision of reducing them just based on an optimal result from the entire series on the other hand for those stations which are always ranked last in the sliding series more certainty can be gained on removing them still we should point out that a longer window length can be adopted e g 10 year in the future if a longer series is available also other optimization approaches may be applied in the dynamic network evaluation framework for comparison and verification in future studies differences between optimal networks from wet season and dry season revealed that meteorological conditions may impact the resulting optimal network and rainfall network optimization based on fixed entire series would cause information loss or redundancy under different conditions as meteorological conditions can partly be seen as one of the inner forcing factors for temporal variability of rainfall network the study on network optimization under different meteorological conditions is critical for better understanding and recognition of dynamic network design since our division and classification was relatively simple and rough more careful and multifarious categories of meteorological conditions can be defined and compared in the future generally how to relate rainfall network optimization to its regional dynamic rainfall distribution characteristics in order to derive a more integrated and practical optimization scheme under changing environment is a future direction that deserves consideration acknowledgments this study was supported by national natural science fund of china no 41571017 51679118 91647203 national key research and development program of china 2017yfc1502704 2016yfc0401501 appendix a see appendix a 
7376,under the effects of global change water crisis ranks as the top global risk in the future decade and water conflict in transboundary river basins as well as the geostrategic competition led by it is most concerned this study presents an innovative integrated ppmgwo model of water resources optimization allocation in a transboundary river basin which is integrated through the projection pursuit model ppm and grey wolf optimization gwo method this study uses the songhua river basin and 25 control units as examples adopting the ppmgwo model proposed in this study to allocate the water quantity using water consumption in all control units in the songhua river basin in 2015 as reference to compare with optimization allocation results of firefly algorithm fa and particle swarm optimization pso algorithms as well as the ppmgwo model results indicate that the average difference between corresponding allocation results and reference values are 0 195 bil m3 0 151 bil m3 and 0 085 bil m3 respectively obviously the average difference of the ppmgwo model is the lowest and its optimization allocation result is closer to reality which further confirms the reasonability feasibility and accuracy of the ppmgwo model and then the ppmgwo model is adopted to simulate allocation of available water quantity in songhua river basin in 2018 2020 and 2030 the simulation results show water quantity which could be allocated in all controls demonstrates an overall increasing trend with reasonable and equal exploitation and utilization of water resources in the songhua river basin in future in addition this study has a certain reference value and application meaning to comprehensive management and water resources allocation in other transboundary river basins keywords projection pursuit model grey wolf optimization optimization allocation transboundary river basins songhua river basin 1 introduction water sources scarcity is the most important resource issues faced by mankind in the 21st century eliasson 2015 li et al 2017b lu et al 2018 transboundary river basins are glocal natural resources which has simultaneously local national and international dimensions transboundary river basins are shared by two or more nations and these transboundary resources are linked by a complex web of environmental political economic and security interdependencies kucukmehmetoglu and geymen 2014 the interdependencies extend not only across national borders but also between the different water systems munia et al 2016 de stefano et al 2017 there are around 276 internationally shared river basins in the world such as the nile the mekong river etc approximately 40 of the world s population lives in these rivers basins degefu et al 2016 population growth industrial and agricultural production activity rapid development of urbanization and climate condition mutations have brought a huge impact on limited water resources and gradually deteriorating aquatic environment in transboundary river basins severely affected socio economic development of the transboundary river basins and their regions therefore threatened human well being haddeland et al 2014 yu and wang 2014 dalin et al 2015 larsen et al 2016 grizzetti et al 2017 he et al 2017a chen et al 2018 in essence transboundary water resources conflicts is resulted from the failure of water resources allocation manifested as the conflicts between people and water as well as conflicts among different people on water quota böhmelt et al 2014 swatuk 2015 chen et al 2017 realizing fair and reasonable allocation of water resources in transboundary river basins is the core of solving water conflicts in transboundary river basins and local regions arjoon et al 2016 carrying out reasonable and optimized allocation for water resources in transboundary river basins is the effective measure of water resources comprehensive management which gives consideration to both survival and development of human beings how to carry out efficient allocation for limited water resources to maximize its benefits in accelerating harmonic development of social economy in transboundary river basins has become very important and urgent optimization allocation of water resources is a highly complicated risk decision problem with multi level multi stages multi subjects multi objectives and nonlinear correlations singh 2014 hassan esfahani et al 2015 davijani et al 2016a abdulbaki et al 2017 lu et al 2017 he et al 2017b mathematical programming method is the most widely used approach in water resources optimization allocation including linear programming georgakakos 2012 non linear programming li et al 2017a dynamic programming anvari et al 2014 quadratic programming marques et al 2010 multi objective programming mosleh et al 2017 etc with the development of applied mathematics theories game theory girard et al 2016 fuzzy mathematics theory nikoo et al 2013 etc have achieved successful application in recent decades system science theory and relevant chaos theory sivakumar 2000 entropy method connor and paulin 2017 system dynamics ryu et al 2012 etc have been gradually applied to water resources allocation with more and more complicated objectives and issues considered in water resource allocation its solution algorithms have been gradually improved the modern intelligent bionic evolutionary algorithm has become the most important model solution and parameter optimization method of water resources allocation besides traditional mathematical programming including genetic algorithm nicklow et al 2010 bat algorithm bozorg haddad et al 2014 firefly algorithm miguel et al 2014 artificial bee colony algorithm pérez et al 2017 differential evolution algorithm mansouri et al 2015 particle swarm optimization gaur et al 2013 imperialism competitive algorithm karamouz et al 2014 artificial fish swarm algorithm he and he 2015 shuffled frog leaping algorithm niknam et al 2011 ant colony optimization szemis et al 2013 etc in recent years more and more scholars have carried out deep investigation on model construction and solution issues of water resources allocation in which intelligent optimization algorithm has been widely applied in model solution abed elmdoust and kerachian 2012 jafarzadegan et al 2013 mianabadi et al 2014 vaghefi et al 2015 davijani et al 2016b chen et al 2016 however the effectiveness of conventional optimization algorithm is not ideal from perspectives of convergence calculation speed initial value sensitivity etc due to the complexity and multi objectives of water resources optimization allocation further studies are needed to improve algorithm efficiency obtain global optimal solution to solve problems such as continuity and differentiability of objective function and constraint conditions grey wolf optimization mirjalili et al 2014 is a novel swarm intelligent optimization algorithm which realizes objective optimization by simulating actions such as tracking encircling attacking preys etc in hunting process of grey wolf packs this algorithm has advantages such as fast convergence speed strong global optimization capacity etc this study designs integrated ppmgwo algorithm by analyzing attributes of multi objective projection pursuit method ppm friedman and tukey 2006 and grey wolf optimization gwo algorithm based on previous studies the songhua river basin which is the largest branch of heilongjiang river is taken as a case study in this paper the integrated optimization model is used to conduct spatial optimization allocation to water resources this study is based on water resources allocation under the most strict water resources management system to realize real demand decided by supply for water resources in transboundary river basins the three red lines as the core of the most stringent water resources management policy consists of restricting water resources exploitation and utilization controlling water consumption efficiency and limiting water resources pollution the quantification results could offer reference for the formulation of reasonable water resources allocation for transboundary river basins 2 indicator system of water quantity allocation water resources allocation in transboundary river basins is a multi objective and multi level decision problem involving multiple factors such as society economy environment water resources pollution treatment investment etc in transboundary river basins and control units cities and presents regional difference as well in process of water resources allocation it is necessary to consider the present situation of water resources human activity intensity the level of developing and exploitation and abilities of water resources protection etc of transboundary river basins and control units together with the environmental equity and water usage efficiency the research background of this paper depends mainly on the project of the major science and technology program for water pollution control and treatment based on the previous research work yu et al 2016 the relationship has been built among economic growth water resources consumption and water pollution emissions therefore the 15 typical water allocation indicators are identified and screened to construct indicator system of water resources allocation in transboundary river basins following the principle of equity of water resources and sustainable development as well as the current situation of transboundary river basin table 1 3 ppmgwo water quantity allocation model 3 1 projection pursuit model ppm projection pursuit pp is defined as a technique for exploratory analysis of large multivariate datasets aimed at unsupervised dimensionality reduction and feature extraction herrera et al 2010 pei et al 2016 projection pursuit model is used to process and analyze high dimensional data the basic principle of projection pursuit model ppm is to project high dimensional data on low dimensional subspace though a certain combination reflect structure or characteristics of original high dimensional data by minimizing projection index and analyze data structure in low dimensional space in order to realize the objective of studying and analyzing high dimensional data the ppm could be used in water quantity allocation and its algorithm procedure is shown below step 1 undimensionalization of data assuming the water quantity allocation data is x ij i 1 2 3 n j 1 2 3 m n and m denote total amount of water allocation regions and water allocation indicators respectively x ij denotes the j th water allocation indicator value in the i th water allocation region since the dimension of evaluation indicators varies an undimensionalization process is carried out for evaluation indicators before model construction in order to reduce effects of dimensions 1 positiveindicator x ij x ij x j max negativeindicator x ij x j min x ij in which x j max and x j min denote the maximum value and the minimum value of the j th indicator respectively step 2 construct projection indicator function project high dimensional data in one dimension linear space using linear projection let a be unit projection vector a a 1 a 2 a m thus one dimension eigenvalue z i of x ij could be expressed as 2 z i j 1 m a j x ij j 1 m a j x ij i 1 2 3 n in which z ij denotes the projection component of the j th water allocation indicator in the i th water allocation area and z z 1 z 2 z n denotes projection eigenvalue vector step 3 construct projection objective function to find out the characteristics of the structure of the combined data in multi dimensional indicators the projection value z i is required to extract variation information of x ij as much as possible during comprehensive projection i e the distance between classes s z of z i distributed in one dimension space is as large as possible in addition the local density d z of projection value z i reaches the maximum i e indicators in same projection space concentrate as much as possible and construct projection objective function as 3 ω a s z d z in which s z denotes the standard deviation of projection value z i where s z z i e z 2 n 1 e z denotes the average value of projection eigenvalue z i d z denotes local density of projection value z i where d z r r ij u r r ij r denotes the density window breadth the value of r is usually θ s z where θ could be 0 1 0 01 or 0 001 distance r ij z i z k k 1 n denotes the distance between any two projection eigenvalues u denotes unit step function when r r ij 0 its function value is 1 and when r r ij 0 the function value is 0 step 4 optimize projection indicator function when the value of evaluation indicator is given the projection indicator function only changes with the change of projection direction when the value projection indicator function is the maximum corresponding direction α is the optimum projection direction which reflects data feature the most therefore the optimum projection direction could be estimated by solving the maximum projection indicator function i e 4 max ω a s z d z a j 1 step 5 calculate projection value calculate projection value z i of all indicators based on the optimum projection direction value a j projection value is weighted by the optimum projection direction and standard value of all evaluation indicators and the objectives could be evaluated and analyzed according to the value of projection 3 2 grey wolf optimizer gwo the gwo is a simulation of wolf pack group hunting action cluster leadership encircling preys and locating preys are three main behaviors of grey wolf hunting preys 1 cluster leadership each wolf pack has strict hierarchy the individual with the maximum fitness value in the pack becomes α wolf individuals with the second and the third largest fitness value are called β wolf and δ wolf respectively and the rest individuals are called ω wolf in gwo algorithms hunting behavior is led by α β and δ wolf wolf ω is responsible for following these three dominant wolves in order to explore the optimum solution 2 encircling preys grey wolf first needs to determine the distance d between itself and preys when encircling preys 5 d c x p t x t in which x p t denotes the spatial position of the prey at generation t x t denotes the spatial position of grey wolf individual at generation t constant c denotes oscillation factor where c 2 r 1 r 1 denotes the random number in the range of 0 1 then grey wolf updates its position according to the distance between prey and itself 6 x t 1 x p t a d 7 a 2 λ r 2 λ in which a denotes convergence factor r 2 denotes the random number in the range of 0 1 λ denotes a constant number linearly decreasing from 2 to 0 with the increase of iterations when all grey wolves complete position updates means they has finished a encircling behavior 3 hunting behavior the prey positions keep changing in gwo algorithm the grey wolf does not know the specific location of the prey in order to hunt the prey position information of three wolves with the best fitness value α β and δ is used to locate the position of the prey the mathematical description of α β and δ wolves tracking orientation of preys could be realized by equations 8 d α c 1 x α t x t x 1 x α a 1 d α 9 d β c 2 x β t x t x 2 x β a 2 d β 10 d δ g 3 x δ t x t x 3 x δ a 3 d δ 11 x t 1 x 1 x 2 x 3 3 3 3 implementation procedure of the ppmgwo model the water allocation implementation procedure of the ppmgwo model could be generalized as follows step 1 construct the indicator system of water quantity allocation and normalize indicators using eq 1 step 2 determine objective function since gwo algorithm is to solve the minimum value the reciprocal of eq 4 is used as the objective function i e set eq 12 as fitness function 12 min ω a 1 s z d z a j 1 step 3 initialize parameters set number of grey wolf species as n search space as d dimension the maximum iteration number as m the optimum individual in species α individuals corresponding to fitness value ranked second and third as β and δ rest individuals as ω set termination condition for the algorithm let t 0 randomly initialized location of the i th grey wolf in d dimensional space in solution space is expressed as x i x i 1 x i 2 x i 3 x id step 4 obtain spatial position of the prey based on the above gwo algorithm procedure i e the optimum projection direction a step 5 substitute the optimum projection direction a in eq 2 to solve the optimum projection value z i in all water areas normalization of z i refers to weight allocation of water quantity in all control units and the product of weight and total water allocation quantity denotes the water quantity allocation result of each control unit 4 contrast analysis of algorithms select 6 high dimensional test functions table 2 to compare optimization results of firefly algorithm fa particle swarm optimization pso and gwo algorithm to solve the minimum value of test functions where u denotes unimodal function and m denotes multimodal function in table 2 rosenbrock sphere and sumsquares are unimodal function which could test strength of algorithm exploitation ability and ackley griewank and rastrigin are multimodal functions which could test strength of algorithm exploration ability the parameter settings of algorithms are as follows the maximum iteration number of fa algorithm t 2000 population size n 50 the maximum attraction degree β 0 2 light intensity absorption coefficient γ 1 and step factor α 0 2 the maximum iteration number of pso algorithm t 2000 population size n 50 attenuation coefficient ω 0 99 local learning factor c 1 2 0 and global learning factor c 2 2 0 the maximum iteration number of gwo algorithm t 2000 population size of grey wolf n 50 adopt m language to carry out optimization calculations for repeatedly 30 times for 6 test functions shown in table 2 using matlab 2010a evaluate fa pso and wo algorithms from five perspective the optimum value the worst value the average value the standard deviation and the success rate of calculate in which optimization average value reflects the solution accuracy reached by the algorithm when running to the maximum iteration number standard deviation reflects the convergence stability of the algorithm and the success rate reflects the optimization reliability of the algorithm under given precision condition the optimization calculation terminates when eq 13 is satisfied 13 f f 1 10 10 in which f denotes ideal optimum value of the function and f denotes the optimum function value of each optimization calculation table 3 demonstrates the optimization calculation statistical results of fa pso and gwo algorithm the optimization result is seen as 0 if it is smaller than 1 10 10 according to optimization results of sphere sumsquares rosenbrock griewank ackley and rastrigin functions the optimization result of gwo algorithm is completely superior to that of fa and pso algorithms gwo algorithm has a relatively strong exploration ability and shows relatively high precision and global optimization ability in optimization contrasts of all test functions which could effectively overcome disadvantages of fa and pso algorithms falling into local optimization 5 case study 5 1 overview of study area the songhua river basin srb fig 1 which is one of the seven big watersheds in china is located in the north of northeast china 41 42 n 51 38 n 119 52 e 132 31 e wang et al 2015 it originates form the northern part of the greater khingan daxinganling mountain and flows first southward then northeastward at the sanchahe before joining the heilongjiang river at the outlet of the river basin the basin is 20 km wide from east to west and 1070 km long from north to south the total area of the basin is about 557 000 km2 accounting for about 60 of the total area of the northeastern china the annual precipitation is less than 400 mm in the southwest and more than 750 mm in the east precipitation during the flood season june to september accounts for more than 70 of the total rainfall song et al 2015 it is an important component of heavy industry base which is also important base of agriculture forestry and animal husbandry in china the distribution of water resource in the river basin is characterized as more in east and less in west more in north and less in south more in borders and less in central areas which is not in accordance with productivity distribution the total water consumption in 2015 is 35 49 bil m3 in which agricultural water consumption accounts for 82 46 industrial water consumption accounts for 6 49 living water consumption accounts for 6 41 ecological water consumption is 4 64 obviously agricultural water use is the largest water consumer in the srb carry out the research of water quantity allocation in the srb has important significance in clarifying water use equity in the srb and control regions guaranteeing drinking water safety water supply safety and ecology safety according to comprehensive planning of songhua river basin 2012 2030 the control objective of total amount of water use in 2018 2020 and 2030 is 38 482 bil m3 40 477 bil m3 and 43 024 bil m3 this study allocates water quantity in 25 control units cities governed by the srb using 2015 as reference year the water use indicators in all control units are shown in table 4 5 2 optimization results and analysis according to the principle of the ppmgwo water quantity allocation model first adopt eq 1 to normalize indicator values in control units in the srb then construct projection indicator function and solve the optimum projection direction of pp model using gwo algorithm the maximum iteration number t 2000 population size of grey wolf n 50 and search space setting as 0 1 of gwo algorithm continuously ran the gwo algorithm for 5 times the evolution process is shown in fig 2 calculated and obtained the average value of optimum projection direction of all water use indicators a 0 1893 0 1774 0 1134 0 1689 0 1246 0 0498 0 1603 0 1475 0 1522 0 1331 0 1597 0 1007 0 1611 0 0738 0 0854 the best fitness value is 5 91 10 6 for this 5 times substituted this best projection direction into eq 2 obtained the projection value z i i of water quantity allocation in 25 control units normalized z i i and obtained water quantity allocation weights of 25 control units the water quantity allocation results in 2018 2020 and 2030 could be obtained by multiplying weights with total amount of water use in 2018 2020 and 2030 respectively it could be seen from fig 2 relative global optimum solution of 5 9125 10 6 is obtained for 5 continuous runs of gwo algorithm indicating a relatively good convergence precision and global optimization ability in order to validate the reliability and feasibility of the ppmgwo water quantity allocation model proposed in this study amount of water use in the srb and 25 control units cities governed by the srb in 2015 is used as reference value using fa pso and the ppmgwo methods to determine water quantity allocation weights of each control unit in 2015 according to water use indicators in all control units in 2015 table 4 simulated the allocation of available water resources in the srb in 2015 and compared the allocation results with reference value table 5 it could be seen from table 5 average difference between reference value and allocation results of fa pso and the ppmgwo models are 0 195 bil m3 0 151 bil m3 and 0 085 bil m3 respectively obviously the average difference of the ppmgwo is the smallest indicating that the optimization allocation results of the ppmgwo is closer to reality fig 3 in which top three control units cities with the largest difference between reference value and results obtained from fa allocation method are kiamusze jilin and changchun the difference values are 0 433 bil m3 0 420 bil m3 and 0 301 bil m3 while the differences obtained from the ppmgwo allocation are 0 143 bil m3 0 180 bil m3 and 0 720 bil m3 respectively the top three control units cities with the largest difference between reference value and results obtained from pso allocation method are changchun mudanjiang and daqing the difference values are 0 390 bil m3 0 358 bil m3 and 0 309 bil m3 while the differences obtained from ppmgwo allocation are 0 72 bil m3 0 141 bil m3 and 0 125 bil m3 respectively this confirmed the reliability and feasibility of the ppmgwo model in this study further the ppmgwo model is adopted to allocate available water resources in the srb in 2018 2020 and 2030 it could be seen from fig 4 that the water quantity could be allocated demonstrates an increasing trend in the srb and all governed control units due to the reasonable exploitation of water resources effectively increased water resources utility rate and increase of industrial water reuse rate in the srb in the future the amount of water resources allocation in industrially developed cities such as harbin daqing jilin changchun increase from 2 851 bil m3 2 691 bil m3 2 647 bil m3 and 2 547 bil m3 in 2018 to 3 063 bil m3 2 891 bil m3 2 843 bil m3 and 2 736 bil m3 in 2030 while water resources allocation amount in agricultural cities such as songyuan mudanjiang suihua and kiamusze increase from 2 150 bil m3 1 750 bil m3 1 605 bil m3 and 1 497 bil m3 in 2018 to 2 310 bil m3 1 880 bil m3 1 724 bil m3 and 1 609 billion m3 in 2030 respectively obviously the ppmgwo model considers both environmental fairness of industrially developed cities and sustainable development of agricultural cities this further demonstrates that with the implementation of the most strict water resources management system the total discharge quantity of water pollutants in the songhua river basin is effectively controlled according to the songhua river basin integrated water resources plan 2012 2030 the water conservancy infrastructure is further improving the total water availability including the reclaimed water rainwater etc has showed growth trend during the 13th five year so the allocatable average water resources in the srb during the 13th five year period increase by 14 06 compare with that during the 12th five year period till 2030 the average water resources which could be allocated in the srb increase by 6 29 compared with that during the 13th five year from perspective of optimization results of optimum projection direction table 5 weights of indicators such as total population gdp available water resources exploitation rate of water resources wastewater treatment rate proportion of pollution treatment investment in gdp etc are the highest and their impacts to water quantity allocation are also the maximum in which weights of irrigation utilization coefficient water consumption of ten thousand yuan of industrial added value industrial water reuse rate etc are the lowest and their impacts to the water quantity allocation is also the minimum in addition from the perspective of indicator selection and social economic development potential of all control units the reasons that fa and pso methods have relatively large difference in water quantity allocation results in some control units are first only water resources exploitation ability and water supply ability of hydraulic engineering design in control units are considered and considered less on indicators concerning with social economic potential second the deviation between water quantity and practical water consumption is relatively high because the water consumption in control units are determined using quota method and third the environmental fairness of all control units and sustainable development of water resources are not completely considered the predicted results usually have relatively large deviation to reality resulting in relatively large difference of water quantity allocation results 6 conclusions after comprehensively considering three perspectives of respecting current status environmental equity and sustainable development this study selects 15 indicators of water resources allocation in transboundary river basins and regions which is in agreement with the reality and the indicator system of water resources allocation is established in transboundary river basin and control units an innovative integration optimization model of ppmgwo is purposed to carry out optimization allocation of water resources in transboundary river basins conducting simulation validation for gwo algorithm using 6 typical test functions and comparing optimization results of fa pso and gwo algorithms the optimization results indicate that the optimization effect of gwo algorithm is superior to fa and pso which is characterized by rapid convergence speed high optimization precision strong global optimization ability and good convergence stability as well as convergence reliability discovering the optimum projection direction of the pp model using gwo algorithm not only improves evaluation precision of the pp model but also provides a new way to solve the optimum projection direction of the pp model this study uses songhua river basin and 25 control units cities as examples adopting the ppmgwo model proposed in this study to allocate the water quantity using water consumption in all control units in songhua river basin in 2015 as reference to compare with optimization allocation results of fa and pso algorithms as well as the ppmgwo model results indicate that the average difference between corresponding allocation results and reference values are 0 195 bil m3 0 151 bil m3 and 0 085 bil m3 respectively obviously the average difference of the ppmgwo model is the lowest and its optimization allocation result is closer to reality which further confirms the reasonability feasibility and accuracy of the ppmgwo model the ppmgwo model is also adopted to simulate allocation of available water quantity in songhua river basin in 2018 2020 and 2030 the simulation results indicate water quantity which could be allocated in all controls demonstrates an overall increasing trend with reasonable and equal exploitation and utilization of water resources in songhua river basin in future in the 13th five year period water resources which could be allocated in all control unites in songhua river basin increase by14 06 compared with that in 12th five year period till 2030 water resources which could be allocated in all control unites in songhua river basin increase by 6 29 compared with that in the13th five year period according to the results of water quantity allocation the ppmgwo model could obtain more scientific and reasonable water quantity allocation results indeed water quantity allocation indicator established in this study all belongs to dynamic indicators in practical application water quantity allocation indicators should be increased or reduced according to practical situation in different areas and by apply real time adjustment to the indicator data we could obtain more scientific and reasonable water quantity allocation results which satisfies requirements of all water allocation areas in addition this study has a certain reference value and application meaning to comprehensive management and water resources allocation in other river basins acknowledgements this research was supported by the key laboratory of water cycle and related land surface processes institute of geographic sciences and natural resources research chinese academy of sciences no wl2017003 the program for china postdoctoral science foundation china no 2016m591139 
7376,under the effects of global change water crisis ranks as the top global risk in the future decade and water conflict in transboundary river basins as well as the geostrategic competition led by it is most concerned this study presents an innovative integrated ppmgwo model of water resources optimization allocation in a transboundary river basin which is integrated through the projection pursuit model ppm and grey wolf optimization gwo method this study uses the songhua river basin and 25 control units as examples adopting the ppmgwo model proposed in this study to allocate the water quantity using water consumption in all control units in the songhua river basin in 2015 as reference to compare with optimization allocation results of firefly algorithm fa and particle swarm optimization pso algorithms as well as the ppmgwo model results indicate that the average difference between corresponding allocation results and reference values are 0 195 bil m3 0 151 bil m3 and 0 085 bil m3 respectively obviously the average difference of the ppmgwo model is the lowest and its optimization allocation result is closer to reality which further confirms the reasonability feasibility and accuracy of the ppmgwo model and then the ppmgwo model is adopted to simulate allocation of available water quantity in songhua river basin in 2018 2020 and 2030 the simulation results show water quantity which could be allocated in all controls demonstrates an overall increasing trend with reasonable and equal exploitation and utilization of water resources in the songhua river basin in future in addition this study has a certain reference value and application meaning to comprehensive management and water resources allocation in other transboundary river basins keywords projection pursuit model grey wolf optimization optimization allocation transboundary river basins songhua river basin 1 introduction water sources scarcity is the most important resource issues faced by mankind in the 21st century eliasson 2015 li et al 2017b lu et al 2018 transboundary river basins are glocal natural resources which has simultaneously local national and international dimensions transboundary river basins are shared by two or more nations and these transboundary resources are linked by a complex web of environmental political economic and security interdependencies kucukmehmetoglu and geymen 2014 the interdependencies extend not only across national borders but also between the different water systems munia et al 2016 de stefano et al 2017 there are around 276 internationally shared river basins in the world such as the nile the mekong river etc approximately 40 of the world s population lives in these rivers basins degefu et al 2016 population growth industrial and agricultural production activity rapid development of urbanization and climate condition mutations have brought a huge impact on limited water resources and gradually deteriorating aquatic environment in transboundary river basins severely affected socio economic development of the transboundary river basins and their regions therefore threatened human well being haddeland et al 2014 yu and wang 2014 dalin et al 2015 larsen et al 2016 grizzetti et al 2017 he et al 2017a chen et al 2018 in essence transboundary water resources conflicts is resulted from the failure of water resources allocation manifested as the conflicts between people and water as well as conflicts among different people on water quota böhmelt et al 2014 swatuk 2015 chen et al 2017 realizing fair and reasonable allocation of water resources in transboundary river basins is the core of solving water conflicts in transboundary river basins and local regions arjoon et al 2016 carrying out reasonable and optimized allocation for water resources in transboundary river basins is the effective measure of water resources comprehensive management which gives consideration to both survival and development of human beings how to carry out efficient allocation for limited water resources to maximize its benefits in accelerating harmonic development of social economy in transboundary river basins has become very important and urgent optimization allocation of water resources is a highly complicated risk decision problem with multi level multi stages multi subjects multi objectives and nonlinear correlations singh 2014 hassan esfahani et al 2015 davijani et al 2016a abdulbaki et al 2017 lu et al 2017 he et al 2017b mathematical programming method is the most widely used approach in water resources optimization allocation including linear programming georgakakos 2012 non linear programming li et al 2017a dynamic programming anvari et al 2014 quadratic programming marques et al 2010 multi objective programming mosleh et al 2017 etc with the development of applied mathematics theories game theory girard et al 2016 fuzzy mathematics theory nikoo et al 2013 etc have achieved successful application in recent decades system science theory and relevant chaos theory sivakumar 2000 entropy method connor and paulin 2017 system dynamics ryu et al 2012 etc have been gradually applied to water resources allocation with more and more complicated objectives and issues considered in water resource allocation its solution algorithms have been gradually improved the modern intelligent bionic evolutionary algorithm has become the most important model solution and parameter optimization method of water resources allocation besides traditional mathematical programming including genetic algorithm nicklow et al 2010 bat algorithm bozorg haddad et al 2014 firefly algorithm miguel et al 2014 artificial bee colony algorithm pérez et al 2017 differential evolution algorithm mansouri et al 2015 particle swarm optimization gaur et al 2013 imperialism competitive algorithm karamouz et al 2014 artificial fish swarm algorithm he and he 2015 shuffled frog leaping algorithm niknam et al 2011 ant colony optimization szemis et al 2013 etc in recent years more and more scholars have carried out deep investigation on model construction and solution issues of water resources allocation in which intelligent optimization algorithm has been widely applied in model solution abed elmdoust and kerachian 2012 jafarzadegan et al 2013 mianabadi et al 2014 vaghefi et al 2015 davijani et al 2016b chen et al 2016 however the effectiveness of conventional optimization algorithm is not ideal from perspectives of convergence calculation speed initial value sensitivity etc due to the complexity and multi objectives of water resources optimization allocation further studies are needed to improve algorithm efficiency obtain global optimal solution to solve problems such as continuity and differentiability of objective function and constraint conditions grey wolf optimization mirjalili et al 2014 is a novel swarm intelligent optimization algorithm which realizes objective optimization by simulating actions such as tracking encircling attacking preys etc in hunting process of grey wolf packs this algorithm has advantages such as fast convergence speed strong global optimization capacity etc this study designs integrated ppmgwo algorithm by analyzing attributes of multi objective projection pursuit method ppm friedman and tukey 2006 and grey wolf optimization gwo algorithm based on previous studies the songhua river basin which is the largest branch of heilongjiang river is taken as a case study in this paper the integrated optimization model is used to conduct spatial optimization allocation to water resources this study is based on water resources allocation under the most strict water resources management system to realize real demand decided by supply for water resources in transboundary river basins the three red lines as the core of the most stringent water resources management policy consists of restricting water resources exploitation and utilization controlling water consumption efficiency and limiting water resources pollution the quantification results could offer reference for the formulation of reasonable water resources allocation for transboundary river basins 2 indicator system of water quantity allocation water resources allocation in transboundary river basins is a multi objective and multi level decision problem involving multiple factors such as society economy environment water resources pollution treatment investment etc in transboundary river basins and control units cities and presents regional difference as well in process of water resources allocation it is necessary to consider the present situation of water resources human activity intensity the level of developing and exploitation and abilities of water resources protection etc of transboundary river basins and control units together with the environmental equity and water usage efficiency the research background of this paper depends mainly on the project of the major science and technology program for water pollution control and treatment based on the previous research work yu et al 2016 the relationship has been built among economic growth water resources consumption and water pollution emissions therefore the 15 typical water allocation indicators are identified and screened to construct indicator system of water resources allocation in transboundary river basins following the principle of equity of water resources and sustainable development as well as the current situation of transboundary river basin table 1 3 ppmgwo water quantity allocation model 3 1 projection pursuit model ppm projection pursuit pp is defined as a technique for exploratory analysis of large multivariate datasets aimed at unsupervised dimensionality reduction and feature extraction herrera et al 2010 pei et al 2016 projection pursuit model is used to process and analyze high dimensional data the basic principle of projection pursuit model ppm is to project high dimensional data on low dimensional subspace though a certain combination reflect structure or characteristics of original high dimensional data by minimizing projection index and analyze data structure in low dimensional space in order to realize the objective of studying and analyzing high dimensional data the ppm could be used in water quantity allocation and its algorithm procedure is shown below step 1 undimensionalization of data assuming the water quantity allocation data is x ij i 1 2 3 n j 1 2 3 m n and m denote total amount of water allocation regions and water allocation indicators respectively x ij denotes the j th water allocation indicator value in the i th water allocation region since the dimension of evaluation indicators varies an undimensionalization process is carried out for evaluation indicators before model construction in order to reduce effects of dimensions 1 positiveindicator x ij x ij x j max negativeindicator x ij x j min x ij in which x j max and x j min denote the maximum value and the minimum value of the j th indicator respectively step 2 construct projection indicator function project high dimensional data in one dimension linear space using linear projection let a be unit projection vector a a 1 a 2 a m thus one dimension eigenvalue z i of x ij could be expressed as 2 z i j 1 m a j x ij j 1 m a j x ij i 1 2 3 n in which z ij denotes the projection component of the j th water allocation indicator in the i th water allocation area and z z 1 z 2 z n denotes projection eigenvalue vector step 3 construct projection objective function to find out the characteristics of the structure of the combined data in multi dimensional indicators the projection value z i is required to extract variation information of x ij as much as possible during comprehensive projection i e the distance between classes s z of z i distributed in one dimension space is as large as possible in addition the local density d z of projection value z i reaches the maximum i e indicators in same projection space concentrate as much as possible and construct projection objective function as 3 ω a s z d z in which s z denotes the standard deviation of projection value z i where s z z i e z 2 n 1 e z denotes the average value of projection eigenvalue z i d z denotes local density of projection value z i where d z r r ij u r r ij r denotes the density window breadth the value of r is usually θ s z where θ could be 0 1 0 01 or 0 001 distance r ij z i z k k 1 n denotes the distance between any two projection eigenvalues u denotes unit step function when r r ij 0 its function value is 1 and when r r ij 0 the function value is 0 step 4 optimize projection indicator function when the value of evaluation indicator is given the projection indicator function only changes with the change of projection direction when the value projection indicator function is the maximum corresponding direction α is the optimum projection direction which reflects data feature the most therefore the optimum projection direction could be estimated by solving the maximum projection indicator function i e 4 max ω a s z d z a j 1 step 5 calculate projection value calculate projection value z i of all indicators based on the optimum projection direction value a j projection value is weighted by the optimum projection direction and standard value of all evaluation indicators and the objectives could be evaluated and analyzed according to the value of projection 3 2 grey wolf optimizer gwo the gwo is a simulation of wolf pack group hunting action cluster leadership encircling preys and locating preys are three main behaviors of grey wolf hunting preys 1 cluster leadership each wolf pack has strict hierarchy the individual with the maximum fitness value in the pack becomes α wolf individuals with the second and the third largest fitness value are called β wolf and δ wolf respectively and the rest individuals are called ω wolf in gwo algorithms hunting behavior is led by α β and δ wolf wolf ω is responsible for following these three dominant wolves in order to explore the optimum solution 2 encircling preys grey wolf first needs to determine the distance d between itself and preys when encircling preys 5 d c x p t x t in which x p t denotes the spatial position of the prey at generation t x t denotes the spatial position of grey wolf individual at generation t constant c denotes oscillation factor where c 2 r 1 r 1 denotes the random number in the range of 0 1 then grey wolf updates its position according to the distance between prey and itself 6 x t 1 x p t a d 7 a 2 λ r 2 λ in which a denotes convergence factor r 2 denotes the random number in the range of 0 1 λ denotes a constant number linearly decreasing from 2 to 0 with the increase of iterations when all grey wolves complete position updates means they has finished a encircling behavior 3 hunting behavior the prey positions keep changing in gwo algorithm the grey wolf does not know the specific location of the prey in order to hunt the prey position information of three wolves with the best fitness value α β and δ is used to locate the position of the prey the mathematical description of α β and δ wolves tracking orientation of preys could be realized by equations 8 d α c 1 x α t x t x 1 x α a 1 d α 9 d β c 2 x β t x t x 2 x β a 2 d β 10 d δ g 3 x δ t x t x 3 x δ a 3 d δ 11 x t 1 x 1 x 2 x 3 3 3 3 implementation procedure of the ppmgwo model the water allocation implementation procedure of the ppmgwo model could be generalized as follows step 1 construct the indicator system of water quantity allocation and normalize indicators using eq 1 step 2 determine objective function since gwo algorithm is to solve the minimum value the reciprocal of eq 4 is used as the objective function i e set eq 12 as fitness function 12 min ω a 1 s z d z a j 1 step 3 initialize parameters set number of grey wolf species as n search space as d dimension the maximum iteration number as m the optimum individual in species α individuals corresponding to fitness value ranked second and third as β and δ rest individuals as ω set termination condition for the algorithm let t 0 randomly initialized location of the i th grey wolf in d dimensional space in solution space is expressed as x i x i 1 x i 2 x i 3 x id step 4 obtain spatial position of the prey based on the above gwo algorithm procedure i e the optimum projection direction a step 5 substitute the optimum projection direction a in eq 2 to solve the optimum projection value z i in all water areas normalization of z i refers to weight allocation of water quantity in all control units and the product of weight and total water allocation quantity denotes the water quantity allocation result of each control unit 4 contrast analysis of algorithms select 6 high dimensional test functions table 2 to compare optimization results of firefly algorithm fa particle swarm optimization pso and gwo algorithm to solve the minimum value of test functions where u denotes unimodal function and m denotes multimodal function in table 2 rosenbrock sphere and sumsquares are unimodal function which could test strength of algorithm exploitation ability and ackley griewank and rastrigin are multimodal functions which could test strength of algorithm exploration ability the parameter settings of algorithms are as follows the maximum iteration number of fa algorithm t 2000 population size n 50 the maximum attraction degree β 0 2 light intensity absorption coefficient γ 1 and step factor α 0 2 the maximum iteration number of pso algorithm t 2000 population size n 50 attenuation coefficient ω 0 99 local learning factor c 1 2 0 and global learning factor c 2 2 0 the maximum iteration number of gwo algorithm t 2000 population size of grey wolf n 50 adopt m language to carry out optimization calculations for repeatedly 30 times for 6 test functions shown in table 2 using matlab 2010a evaluate fa pso and wo algorithms from five perspective the optimum value the worst value the average value the standard deviation and the success rate of calculate in which optimization average value reflects the solution accuracy reached by the algorithm when running to the maximum iteration number standard deviation reflects the convergence stability of the algorithm and the success rate reflects the optimization reliability of the algorithm under given precision condition the optimization calculation terminates when eq 13 is satisfied 13 f f 1 10 10 in which f denotes ideal optimum value of the function and f denotes the optimum function value of each optimization calculation table 3 demonstrates the optimization calculation statistical results of fa pso and gwo algorithm the optimization result is seen as 0 if it is smaller than 1 10 10 according to optimization results of sphere sumsquares rosenbrock griewank ackley and rastrigin functions the optimization result of gwo algorithm is completely superior to that of fa and pso algorithms gwo algorithm has a relatively strong exploration ability and shows relatively high precision and global optimization ability in optimization contrasts of all test functions which could effectively overcome disadvantages of fa and pso algorithms falling into local optimization 5 case study 5 1 overview of study area the songhua river basin srb fig 1 which is one of the seven big watersheds in china is located in the north of northeast china 41 42 n 51 38 n 119 52 e 132 31 e wang et al 2015 it originates form the northern part of the greater khingan daxinganling mountain and flows first southward then northeastward at the sanchahe before joining the heilongjiang river at the outlet of the river basin the basin is 20 km wide from east to west and 1070 km long from north to south the total area of the basin is about 557 000 km2 accounting for about 60 of the total area of the northeastern china the annual precipitation is less than 400 mm in the southwest and more than 750 mm in the east precipitation during the flood season june to september accounts for more than 70 of the total rainfall song et al 2015 it is an important component of heavy industry base which is also important base of agriculture forestry and animal husbandry in china the distribution of water resource in the river basin is characterized as more in east and less in west more in north and less in south more in borders and less in central areas which is not in accordance with productivity distribution the total water consumption in 2015 is 35 49 bil m3 in which agricultural water consumption accounts for 82 46 industrial water consumption accounts for 6 49 living water consumption accounts for 6 41 ecological water consumption is 4 64 obviously agricultural water use is the largest water consumer in the srb carry out the research of water quantity allocation in the srb has important significance in clarifying water use equity in the srb and control regions guaranteeing drinking water safety water supply safety and ecology safety according to comprehensive planning of songhua river basin 2012 2030 the control objective of total amount of water use in 2018 2020 and 2030 is 38 482 bil m3 40 477 bil m3 and 43 024 bil m3 this study allocates water quantity in 25 control units cities governed by the srb using 2015 as reference year the water use indicators in all control units are shown in table 4 5 2 optimization results and analysis according to the principle of the ppmgwo water quantity allocation model first adopt eq 1 to normalize indicator values in control units in the srb then construct projection indicator function and solve the optimum projection direction of pp model using gwo algorithm the maximum iteration number t 2000 population size of grey wolf n 50 and search space setting as 0 1 of gwo algorithm continuously ran the gwo algorithm for 5 times the evolution process is shown in fig 2 calculated and obtained the average value of optimum projection direction of all water use indicators a 0 1893 0 1774 0 1134 0 1689 0 1246 0 0498 0 1603 0 1475 0 1522 0 1331 0 1597 0 1007 0 1611 0 0738 0 0854 the best fitness value is 5 91 10 6 for this 5 times substituted this best projection direction into eq 2 obtained the projection value z i i of water quantity allocation in 25 control units normalized z i i and obtained water quantity allocation weights of 25 control units the water quantity allocation results in 2018 2020 and 2030 could be obtained by multiplying weights with total amount of water use in 2018 2020 and 2030 respectively it could be seen from fig 2 relative global optimum solution of 5 9125 10 6 is obtained for 5 continuous runs of gwo algorithm indicating a relatively good convergence precision and global optimization ability in order to validate the reliability and feasibility of the ppmgwo water quantity allocation model proposed in this study amount of water use in the srb and 25 control units cities governed by the srb in 2015 is used as reference value using fa pso and the ppmgwo methods to determine water quantity allocation weights of each control unit in 2015 according to water use indicators in all control units in 2015 table 4 simulated the allocation of available water resources in the srb in 2015 and compared the allocation results with reference value table 5 it could be seen from table 5 average difference between reference value and allocation results of fa pso and the ppmgwo models are 0 195 bil m3 0 151 bil m3 and 0 085 bil m3 respectively obviously the average difference of the ppmgwo is the smallest indicating that the optimization allocation results of the ppmgwo is closer to reality fig 3 in which top three control units cities with the largest difference between reference value and results obtained from fa allocation method are kiamusze jilin and changchun the difference values are 0 433 bil m3 0 420 bil m3 and 0 301 bil m3 while the differences obtained from the ppmgwo allocation are 0 143 bil m3 0 180 bil m3 and 0 720 bil m3 respectively the top three control units cities with the largest difference between reference value and results obtained from pso allocation method are changchun mudanjiang and daqing the difference values are 0 390 bil m3 0 358 bil m3 and 0 309 bil m3 while the differences obtained from ppmgwo allocation are 0 72 bil m3 0 141 bil m3 and 0 125 bil m3 respectively this confirmed the reliability and feasibility of the ppmgwo model in this study further the ppmgwo model is adopted to allocate available water resources in the srb in 2018 2020 and 2030 it could be seen from fig 4 that the water quantity could be allocated demonstrates an increasing trend in the srb and all governed control units due to the reasonable exploitation of water resources effectively increased water resources utility rate and increase of industrial water reuse rate in the srb in the future the amount of water resources allocation in industrially developed cities such as harbin daqing jilin changchun increase from 2 851 bil m3 2 691 bil m3 2 647 bil m3 and 2 547 bil m3 in 2018 to 3 063 bil m3 2 891 bil m3 2 843 bil m3 and 2 736 bil m3 in 2030 while water resources allocation amount in agricultural cities such as songyuan mudanjiang suihua and kiamusze increase from 2 150 bil m3 1 750 bil m3 1 605 bil m3 and 1 497 bil m3 in 2018 to 2 310 bil m3 1 880 bil m3 1 724 bil m3 and 1 609 billion m3 in 2030 respectively obviously the ppmgwo model considers both environmental fairness of industrially developed cities and sustainable development of agricultural cities this further demonstrates that with the implementation of the most strict water resources management system the total discharge quantity of water pollutants in the songhua river basin is effectively controlled according to the songhua river basin integrated water resources plan 2012 2030 the water conservancy infrastructure is further improving the total water availability including the reclaimed water rainwater etc has showed growth trend during the 13th five year so the allocatable average water resources in the srb during the 13th five year period increase by 14 06 compare with that during the 12th five year period till 2030 the average water resources which could be allocated in the srb increase by 6 29 compared with that during the 13th five year from perspective of optimization results of optimum projection direction table 5 weights of indicators such as total population gdp available water resources exploitation rate of water resources wastewater treatment rate proportion of pollution treatment investment in gdp etc are the highest and their impacts to water quantity allocation are also the maximum in which weights of irrigation utilization coefficient water consumption of ten thousand yuan of industrial added value industrial water reuse rate etc are the lowest and their impacts to the water quantity allocation is also the minimum in addition from the perspective of indicator selection and social economic development potential of all control units the reasons that fa and pso methods have relatively large difference in water quantity allocation results in some control units are first only water resources exploitation ability and water supply ability of hydraulic engineering design in control units are considered and considered less on indicators concerning with social economic potential second the deviation between water quantity and practical water consumption is relatively high because the water consumption in control units are determined using quota method and third the environmental fairness of all control units and sustainable development of water resources are not completely considered the predicted results usually have relatively large deviation to reality resulting in relatively large difference of water quantity allocation results 6 conclusions after comprehensively considering three perspectives of respecting current status environmental equity and sustainable development this study selects 15 indicators of water resources allocation in transboundary river basins and regions which is in agreement with the reality and the indicator system of water resources allocation is established in transboundary river basin and control units an innovative integration optimization model of ppmgwo is purposed to carry out optimization allocation of water resources in transboundary river basins conducting simulation validation for gwo algorithm using 6 typical test functions and comparing optimization results of fa pso and gwo algorithms the optimization results indicate that the optimization effect of gwo algorithm is superior to fa and pso which is characterized by rapid convergence speed high optimization precision strong global optimization ability and good convergence stability as well as convergence reliability discovering the optimum projection direction of the pp model using gwo algorithm not only improves evaluation precision of the pp model but also provides a new way to solve the optimum projection direction of the pp model this study uses songhua river basin and 25 control units cities as examples adopting the ppmgwo model proposed in this study to allocate the water quantity using water consumption in all control units in songhua river basin in 2015 as reference to compare with optimization allocation results of fa and pso algorithms as well as the ppmgwo model results indicate that the average difference between corresponding allocation results and reference values are 0 195 bil m3 0 151 bil m3 and 0 085 bil m3 respectively obviously the average difference of the ppmgwo model is the lowest and its optimization allocation result is closer to reality which further confirms the reasonability feasibility and accuracy of the ppmgwo model the ppmgwo model is also adopted to simulate allocation of available water quantity in songhua river basin in 2018 2020 and 2030 the simulation results indicate water quantity which could be allocated in all controls demonstrates an overall increasing trend with reasonable and equal exploitation and utilization of water resources in songhua river basin in future in the 13th five year period water resources which could be allocated in all control unites in songhua river basin increase by14 06 compared with that in 12th five year period till 2030 water resources which could be allocated in all control unites in songhua river basin increase by 6 29 compared with that in the13th five year period according to the results of water quantity allocation the ppmgwo model could obtain more scientific and reasonable water quantity allocation results indeed water quantity allocation indicator established in this study all belongs to dynamic indicators in practical application water quantity allocation indicators should be increased or reduced according to practical situation in different areas and by apply real time adjustment to the indicator data we could obtain more scientific and reasonable water quantity allocation results which satisfies requirements of all water allocation areas in addition this study has a certain reference value and application meaning to comprehensive management and water resources allocation in other river basins acknowledgements this research was supported by the key laboratory of water cycle and related land surface processes institute of geographic sciences and natural resources research chinese academy of sciences no wl2017003 the program for china postdoctoral science foundation china no 2016m591139 
7377,the copula functions have been widely applied as an advance technique to create joint probability distribution of drought duration and severity the approach of data collection as well as the amount of data and dispersion of data series can last a significant impact on creating such joint probability distribution using copulas usually such traditional analyses have shed an unconnected drought runs udr approach towards droughts in other word droughts with different durations would be independent of each other emphasis on such data collection method causes the omission of actual potentials of short term extreme droughts located within a long term udr meanwhile traditional method is often faced with significant gap in drought data series however a long term udr can be approached as a combination of short term connected drought runs cdr therefore this study aims to evaluate systematically two udr and cdr procedures in joint probability of drought duration and severity investigations for this purpose rainfall data 1971 2013 from 24 rain gauges in lake urmia basin iran were applied also seven common univariate marginal distributions and seven types of bivariate copulas were examined compared to traditional approach the results demonstrated a significant comparative advantage of the new approach such comparative advantages led to determine the correct copula function more accurate estimation of copula parameter more realistic estimation of joint conditional probabilities of drought duration and severity and significant reduction in uncertainty for modeling keywords copula drought spi conditional probability 1 introduction given the spatial extent and persistence of droughts it may be argued that droughts are among the important phenomena occurring in the nature generally drought is caused by lack of water supply in a specific geographical area for a prolonged period rossi 2000 rossi and cancelliere 2013 this natural phenomenon is an inevitable part of climatic variation and it is repeated in different climatic zones of the world wilhite 2000 the occurrence of a drought may be approached from different aspects including drought duration severity intensity and inter arrival time as such those characteristics are very important for planning and management of available water resources dracup et al 1980 in the meantime two major characteristics of drought duration and severity have been the focus of researchers attention because a they are considered as primary parameters in estimating of other drought characteristic such as intensity and b they are regarded as two basic characteristics for real time and long term drought management kim et al 2006 shiau et al 2007 shiau and modarres 2009 hao and singh 2013 several researchers have analyzed those drought characteristics in terms of univariate variable around the world i e fernández and salas 1999 chung and salas 2000 shiau and shen 2001 cancelliere and salas 2004 salas et al 2005 serinaldi et al 2009 montaseri and amirataee 2017 conducted a comprehensive study to analyze the generalized or long term behavior of abovementioned four drought characteristics as univariate variables drought duration severity intensity and inter arrival time using extensive use of monte carlo simulation method across different climatic conditions around the world however the characteristics of any given drought drought duration and severity in particular indicate a significant dependence to each other accordingly the joint probability analyses of such drought characteristics bivariate or trivariate dependent variables can provide very valuable complementary criteria for developing upstream policies governing decision making in the realm of drought crisis management song and singh 2010 however the bivariate multivariate probability analyses require much more complex techniques and computing methods than univariate probability analyses of drought characteristics salvadori et al 2007 this may be one of the main reasons why researchers prefer univariate drought characteristics analyses besides it may justify the limited number of studies that have investigated the drought characteristics via bivariate multivariate joint probability analyses traditionally most of studies on joint probability analyses of drought characteristics including bivariate probability analysis of drought duration and severity have been based on classical bivariate distributions such as bivariate normal distribution goel et al 1998 yue 1999 since the drought duration and severity are dependent and do not generally follow the same marginal probability distribution zhang and singh 2006 it may be argued that the classical joint probability distributions do not have the capacity required for satisfactory and accurate modeling of dependent drought characteristics such as drought duration and severity salvadori et al 2007 given the past few years numerous researchers have attempted to resort to competent capabilities of different marginal distributions so that they can make use of copula functions as a novel advanced technique in analyzing the joint probability of two or more hydrological dependent variables e g de michele and salvadori 2003 de michele et al 2005 grimaldi and serinaldi 2006 shiau et al 2006 zhang and singh 2006 kao and govindaraju 2007 2008 shiau and modarres 2009 lee and salas 2011 mirabbasi et al 2013 the following section discusses these items thereof shiau 2006 made use of standardized precipitation index spi and copula functions to model the joint probability distribution of drought duration and severity in taiwan the exponential and gamma probability distributions were considered as the appropriate nominated distribution of drought duration and severity respectively in this study seven copula functions of various families were examined in detail finally galambos was selected as the most suitable copula in bivariate analysis of drought duration and severity in the study area shiau and modarres 2009 applied a probability approach and copula functions to examine the relationship among drought duration severity and frequency in the period 1954 2003 across two rain gauge stations located within two different dry and wet climatic conditions in iran having assumed the exponential and gamma probability distributions as the marginal distributions for drought duration and severity it was attempted to make use of clayton copula function to model the joint probability distribution of drought severity duration frequency the results indicated that if fluctuations in rainfall were high enough the drought would be more severe in the wet area ma et al 2013 applied trivariate joint functions to examine such drought characteristics as drought duration severity and peak of drought in weihe river basin in china accordingly they examined six different copula and concluded that the normal and t student copula functions best fitted with the three dependent variables in the study area having assumed the marginal distributions of exponential and gamma in the peninsular malaysia zin et al 2013 employed spi and copula functions to analyze the meteorological drought severity and duration in this study they resorted to rainfall data collected from 50 rain gauge stations during the period 1975 2004 then five copula functions of clayton frank joe gumbel hougaard and galambos were examined to determine the most appropriate copula to establish a joint probability distribution subsequently the joe copula was selected as the most appropriate copula thereof mirabbasi et al 2013 employed various bivariate copulas to analyze the characteristics of meteorological drought in iran two characteristics of drought duration and severity were assessed via spi and then the exponential and gamma distributions were fitted to drought duration and severity values respectively the results indicated that according to upper tail dependence coefficient and goodness of fit tests the galambos copula was the best copula to model the joint probability distribution of drought characteristics besides the selected copula revealed such bivariate probability characteristics as return periods conditional and joint probabilities abdul rauf and zeephongsekul 2014 employed monthly rainfall data of selected rain gauge stations to investigate the joint probability distribution of drought duration and severity in the state of victoria australia during the period 1950 2010 besides they applied extreme value and archimedean copulas to determine the bivariate probability distribution of drought duration and severity on the basis of spi the results indicated that weibull and lognormal distributions were the best marginal distributions for fitting the drought duration and severity respectively also each of the clayton and gumbel hougaard copulas were determined as the best copula for 50 of selected stations xu et al 2015 attempted to evaluate the spatial and temporal changes in droughts on the basis of trivariate copula in the southwest china in the period 1961 2012 as such they examined different types of copula functions such as elliptical symmetrical and asymmetrical archimedean copulas then three characteristics of drought severity duration and areal extent were calculated to analyze the drought frequency given the various types of copulas they recognized the joe and gumbel copulas as the best copulas to the three dependent variables then the drought return period was estimated the results indicated that the drought of 2009 2010 had a return period of about 94 years and consequently it was one of the worst droughts in southwest china during the entire period of study a review of the above studies resulted in the following important point generally the drought duration and severity data series are collected from unconnected drought runs udr or independent droughts namely the periods before and after a drought are non drought periods in this way a 1 month drought should be located in an independent period e g between the two non drought months accordingly it is not possible to simultaneously consider a 2 month drought as well as two 1 month droughts if much emphasis is placed on such a data collection procedure it might be possible to overlook the real potentials of short term extreme droughts that take place within a continuous long term drought besides the traditional data collection procedure often led to a significant gap or discontinuation across drought duration and severity e g lack of data for a range of drought duration and severity variables and a malformed probability density function of duration and severity data accordingly it was expected that the basic parameters of joint probability distribution of drought duration and severity could heavily affect such items as parameters of copula functions the joint probabilities of drought return period etc as such it was likely that these parameters could turn into unrealistic elements actually a continuous long term drought might be subdivided into a number of short term connected drought runs cdr or non independent droughts with different duration for example a continuous three month drought also represents three 1 month drought and two 2 month drought as well therefore in this study a new approach namely connected drought runs cdr procedure was examined to investigate the joint probability distribution of drought duration and severity for this purpose both traditional and new proposed methods namely udr and cdr procedures respectively were employed to collect drought data i e duration and severity values in study area to do this end historical monthly rainfall data 1971 2013 were examined across 24 rain gauge stations in lake urmia basin iran accordingly the drought monitoring and drought characteristics data based on udr and cdr procedures were gauged using the spi then the copula function technique was used to investigate the bivariate probability of drought duration and severity on the basis of the aforementioned two approaches 2 material and methods 2 1 study area and data lake urmia is one of the largest salty lakes in the world located in the northwestern of iran the area of lake urmia basin is about 52 000 km2 and it is bounded by the sabalan mountains which is located in the vicinity of aras river and the zagros mountains to the north as well as the zaab river to the south the elevation of lake urmia basin varies between 1236 m and 3733 m above sea level us geological survey s earth explorer https earthexplorer usgs gov the climatic condition of lake urmia basin is classified into cold semi arid köppen geiger classification bsk and dsa peel et al 2007 topographical and geographical map of the study area is shown in fig 1 currently lake urmia is faced with a severe crisis in terms of reduced water volume given the reduction of water resources surface water and groundwater and occurrence of frequent droughts in lake urmia basin in recent decades it seems that this natural habitat has faced a severe ecosystemic and environmental risk aghakouchak et al 2015 therefore the concerned data on the study area included significant changes across extreme drought events besides this can include very valuable and relevant data to compare and evaluate both traditional and new proposed methods in the joint probability analysis of drought duration and severity on the other hand a careful and real analysis of dependent characteristics of drought duration and severity can be considered a very important tool in managing drought risk in lake urmia basin in this study it was attempted to make use of monthly rainfall data records collected from 24 rainfall stations in lake urmia basin for the period 1971 2013 besides the quality of the data was strictly controlled before released by the authors given the selected stations it was indicated that the average annual rainfall varied between 542 6 mm in hashemabad station and 239 7 mm in azer shahr station furthermore it was observed that their annual rainfall coefficients of variation were located between 22 and 41 the lag one serial correlation of annual rainfall was positive across the majority of stations and the values of skewness varied between 0 28 and 1 48 the locations of the selected rainfall stations are shown in fig 1 in addition the main statistical properties of annual rainfall data records such as mean coefficient of variation cv skewness skew lag one serial correlation ρ are presented in table 1 2 2 standardized precipitation index spi the standardized precipitation index spi was developed by mckee et al in 1993 to determine and monitor droughts the spi is simple spatially invariant and probabilistic in nature and can be applied to analyze different types of drought phenomena such as meteorological agricultural and hydrological data mishra and singh 2010 montaseri and amirataee 2017 carried out a comprehensive stochastic assessment of seven common meteorological drought indices using the rainfall data from different part of the world embedded with various climatic conditions they concluded that the spi had an accurate and realistic meteorological drought monitoring capabilities and thus it was essentially superior to other precipitation based meteorological drought indices the spi could determine wet and dry situations for a specific time scale for each location using the rainfall data firstly the appropriate probability distribution is fitted to the long term rainfall data and then the cumulative distribution function is turned into the normal distribution via equal probabilities finally the transformed data to normal distribution are used to calculate spi values montaseri and amirataee 2017 2 3 drought identification and data collection of drought duration and severity yevjevich 1967 proposed the use of theory of runs to define drought characteristics fig 2 a run is defined as a portion of time series of variable xt in which all values are either below i e a negative run or above i e a positive run a selected truncation level of x0 accordingly it is called either a negative or positive run regarding this two main components of a drought event as shown in fig 2 are identified drought duration di it is the time period between the start and end of a consecutive drought event and drought severity si it indicates a cumulative deficiency of a consecutive drought parameter below the critical level traditionally the copula based analysis of joint probability distribution of drought duration and severity has commonly considered the individual drought for different duration as an unconnected drought run udr i e the periods before and after the drought are non drought periods fig 2 based on this definition in fig 2 as a hypothetical example it might be argued that only four 1 month droughts of d1 d3 d5 d8 would be obtained for a 50 month record length their corresponding drought severity values positive values are 0 5 1 25 1 and 1 5 respectively however a long term unconnected drought run udr represented several short term connected drought runs cdr that occurred as successive short term runs traditionally there has been an overemphasis on the assumption of independency of droughts with different durations in analyzing joint probability of drought duration and severity accordingly the actual potentials of short term extreme droughts located within a continuous long term drought may be ignored for example fig 2 indicates that the traditional worst severity values for 1 month and 2 month droughts are 1 5 and 3 respectively conversely four in d4 d6 d7 and d9 and one in d9 severities are determined as the much worse severity values for 1 month and 2 month droughts within the continuous long term droughts respectively fig 2 on the other hand the udr data series with different duration have often faced with significant gaps i e lack of data for a range of drought duration given traditional method fig 2 it is indicated that only 1 2 3 and 6 month droughts could be obtained and consequently 4 and 5 month droughts could not be observed actually it would affect the results of an analysis of joint probability distribution of drought duration and severity however a 6 month unconnected drought run udr could be approached in terms of six 1 month periods five 2 month periods four 3 month periods three 4 month periods two 5 month periods and a 6 month period connected drought runs cdr using moving windows technique in new proposed methods table 2 depicts the data series of drought duration and severity of above example fig 2 on the basis of both traditional and new approaches 2 4 copula based joint distribution function the copula based multivariate distribution structure was developed by sklar 1959 joe 1997 nelsen 2006 and salvadori et al 2007 as such the copula function may couple the marginal distribution into bivariate or multivariate distributions accordingly the copulas are widely used in the joint probability analysis in such areas as finance hydrology meteorology and risk management grimaldi et al 2016 salvadori et al 2007 according to sklar s theorem nelsen 2006 if fxy is a two dimensional joint distribution function of dependent random variables of x and y with marginal distribution functions fx and fy then there exist a copula c such that 1 p x x y y f xy x y c f x x f y y conversely for any distribution functions fx and fy and any copula c the function fxy defined above is a two dimensional function with marginal fx and fy furthermore if fx and fy are continuous c is unique if the probability density functions pdf fx x and fy y of x and y exist then the joint pdf of the two random variables can be expressed as 2 f x y x y c f x x f y y f x x f y y where c is the density function of c nelsen 2006 copulas have been constituted from different families and the following seven types were used in this study archimedean clayton frank gumbel and joe extreme value gumbel elliptical normal and others plackett the application of copula theory in the bivariate multivariate joint probability distribution investigations require to examine correlation structure of dependent variables via two types of correlation tests such as cross pearson s r and rank correlation coefficients kendall s tau and spearman s rho compared with pearson s correlation coefficient it seemed that the rank correlation coefficients were less susceptible to outlier values and more sensitive to the nonlinear correlation of variables zhang and singh 2012 given this it was attempted to utilize kendall s tau and spearman s rho rank correlation coefficients tests in order to detect the correlation structure of drought variables 2 5 marginal distributions of drought duration and severity the identification and determination of the most appropriate marginal distribution for each of the dependent variables may play a key role in the joint probability analysis of drought characteristics using copula technique given the analysis of joint probability of drought duration and severity it was usually indicated that the exponential and gamma probability functions were appropriately fitted on drought duration and severity respectively hao and singh 2013 mirabbasi et al 2013 contrarily some direct tests conducted by different researchers have shown that the aforementioned distributions have not always fitted as the superior distribution for drought duration and severity values wong et al 2008 yusof et al 2013 thus in this study seven distributions of exponential gamma gumbel normal lognormal logistic and weibull were directly examined to determine the most appropriate probability for drought duration and severity values the goodness of fit test was applied to define the best probability distribution using two common methods of akaike information criterion aic and bayesian information criterion bic also the parameters of the probability distributions seven types were calculated using the maximum likelihood estimation mle bardossy and pegram 2009 zhang and singh 2007 the most commonly used methods for estimating the copula parameters are as follow method of moments genest and rivest 1993 exact maximum likelihood method eml inference function for margins ifm joe 1997 and maximum pseudo likelihood estimation method mpl genest et al 1995 compared with other estimation methods it is indicated that the ifm method is the preferred full parametric method for multidimensional parameter estimation because its approach is close to maximum likelihood and it is easier to implement joe and xu 1996 joe 1997 therefore the ifm method was employed for estimation of the copula parameters in this study in this method firstly the parameters of the marginal probability distribution were estimated using maximum likelihood estimation based on the log likelihood function of drought duration and severity then the copula dependence parameter θ was determined through the maximization of the log likelihood function of copula shiau 2006 selecting a suitable copula is quite important for modeling dependence correctly among marginal distributions therefore log likelihood akaike information criterion aic and bayesian information criterion bic tests along with rmse were used to evaluate the performance of fitted copula models the model with the highest lowest log likelihood aic bic and rmse statistic values represented the most suitable copula model 2 6 the joint probability distribution of drought duration and severity the bivariate probability distribution of drought duration and severity is considered as a very efficient tool in predicting forecasting and managing of drought events shiau 2006 the latter is due to this fact that the risk analysis of drought duration or severity may not solely yield sufficient information about the real conditions of drought event as a two dependent climatic phenomenon actually such case makes it very difficult to develop the operation and management of water resources systems in different drought conditions zhang and singh 2006 for example a 1 month and a 3 month drought with a same severity value of 3 in a water resources system is embedded with very diverse rate of vulnerabilities and damage therefore if the bivariate risk of drought duration and severity exceeds above a specified threshold then it may be concluded that there occurs a critical situation for an ecosystem these bivariate multivariate probability distributions could easily be calculated by copula given this the following equation was used to calculate the joint exceedance probability of drought duration d and severity s over their respective thresholds d and s shiau 2006 3 p d d s s 1 f d d f s s c f d d f s s where fd d and fs s represent the exceedance probability of duration and severity from their fitted marginal distributions respectively and c represent the copula function all three aforementioned probabilities are shown schematically in fig 3 also bivariate drought conditional probabilities could be calculated using the copula functions given the analysis of these drought related variables one might resort to the following equations to calculate the conditional probability of drought severity duration when the drought duration severity exceeded a specified threshold d s 4 p s s d d f s s c f d d f s s 1 f d d 5 p d d s s f d d c f d d f s s 1 f s s 3 results and discussions 3 1 the probabilities of drought occurrence the frequency distribution of monthly values of spi across the selected stations from 1971 to 2013 in terms of 25 50 and 75 exceedance probabilities are depicted in fig 4 as seen in fig 4 dry and wet periods were frequent and without a clear pattern repeated in the region during recorded length 43 years also fig 5 depicts the detailed results of frequency analysis of two correlated drought variables at rahimabad station according to fig 5a the empirical cumulative distribution of drought duration p d i d was formed as a down tailless s shaped as such any given increase in duration of short term drought would increase the non exceedance probability of drought duration with a very sharp slope for example the non exceedance exceedance probability of 1 month 3 month and 6 month droughts at rahimabad station were determined as 0 51 0 83 and 0 95 0 49 0 17 and 0 05 respectively thus the empirical probability distribution of drought duration should be asymmetrical figs 6 and 8 also the empirical probability density function of wet and dry periods at rahimabad station is shown in fig 5b given this the empirical probability density function of wet and dry periods was almost symmetrical and close to the standard normal probability density function montaseri and amirataee 2017 it was found that the probability of normal state was equal to 0 77 and total probabilities of dry and wet states extreme severe and moderate were 0 07 and 0 17 respectively in addition the variation of average severity for 1 month to 7 month droughts against the exceedance probability of drought duration i e 1 month to 7 month droughts demonstrated a logarithmic relationship with a correlation coefficient of 0 99 fig 5c this represented that as the drought duration increased the drought intensity and probability of drought duration decreased 3 2 characteristics of drought duration and severity according to the definition provided for spi any given value less than 1 in any given period is classified as a dry drought period mckee et al 1993 mishra and singh 2010 however some researchers believe that consecutive semi dry periods 1 spi 0 with massive cumulative deficit i e high rate of drought severity have a greater adverse effect on any given ecosystem than short periods embedded with spi less than 1 shiau 2006 therefore the authors made use of two drought thresholds of zero and 1 i e spi 1 and spi 0 in order to collect data series of drought duration and severity across the selected stations statistical parameters of spi data series for three different procedures i e spi 1 udr spi 0 udr and spi 0 cdr at rahimabad station depict in table 3 according to table 3 there were much more drought events in return for zero drought threshold especially in case of cdr this element along with such varying values of statistical parameters as mean standard deviation and skewness of three spi data series as well as the argument raised by shiau in 2006 might play a key role in determining the best marginal distribution of drought duration and severity an interesting point about cdr method was that compared to udr method the data skewness declined considerably fig 6 shows the effect of drought threshold and two udr and cdr methods in drought data collections using scatter plot of drought duration and severity data together with their marginal histograms it should be mentioned that the results of rahimabad station were similarly repeated in the rest of the stations as such fig 7 depicts the scatter plot of drought duration and severity using the two applied methods udr and cdr across four other stations in the study area it is essential to assess the correlation between two dependent variables in analyzing the joint probability distribution of drought duration and severity as such table 4 depicts the correlation coefficient between drought duration and severity values based on udr and cdr methods using kendall s tau and spearman s rho rank dependence tests given all three methods the results showed that the correlation coefficient between drought duration and severity was significant and that the new proposed method yielded a higher statistic in this regard compared to other two methods regarding the results and given the argument raised by shiau in 2006 it was decided to rely on new proposed method cdr spi 0 to analyze joint probability distribution of drought duration and severity across the selected stations 3 3 marginal distribution of drought variables the identification and determination of the most appropriate marginal distribution for each dependent variable perform a key role in relation to development of bivariate multivariate joint probability distribution using copula to do this end seven probability distributions of exponential gamma gumbel normal lognormal logistic and weibull were fitted to drought duration and severity data the parameters of probability distributions were calculated using maximum likelihood estimation mle the two goodness of fit test namely aic and bic was next applied to realize the best marginal probability distribution of drought duration and severity values the results of goodness of fit tests of drought duration and severity values at rahimabad station as well as the percentages of best fitted marginal probability distribution across all the selected stations are presented in table 5 moreover for graphical test specification the theoretical and empirical pdfs and cdfs plot of drought duration and severity values based on udr and cdr approaches at rahimabad station are shown in fig 8 based on table 5 the lognormal and gamma distributions were defined as the most appropriate probability distributions for drought duration and severity values derived from cdr method in the selected stations respectively on the other hand for both dependent variables of udr method the lognormal were selected as the best probability distribution in the study area the latter fact pointed to the merits of conducting a direct test to select the most appropriate marginal distribution rather than resorting to the suggested distributions of exponential and gamma in fitting with drought duration and severity 3 4 joint dependence structure using copula selecting the appropriate copula in this section the results derived from the application of both traditional and new proposed methods abbreviated as udr lnln and cdr lng in joint probability analysis of drought duration and severity were comprehensively compared to do this end bivariate probability distribution of drought duration and severity was modeled in accordance with the seven common copula functions of clayton gumbel frank joe galambos plackett and normal for each station in the study area the parameters of copula were estimated using the common and reliable method of ifm shiau 2006 shiau and modarres 2009 li et al 2012 finally the most appropriate copula functions with minimum maximum statistic values of aic bic and rmse log likelihood were determined zhang and singh 2006 li et al 2012 those criteria could significantly and quantitatively summarize the deviation of the estimated and observed values shiau 2006 karmakar and simonovic 2009 chen et al 2013 yusof et al 2013 the results of joint probability modeling of two drought dependent variables with seven copula functions at rahimabad station as well as other stations in the study area were summarized and presented in table 6 according to table 6 the following conclusions could be deduced in order of importance 1 since the two copula functions of clayton and plackett were not considered as proper copula in the first and second priority levels across none of the selected stations with widish range of statistical properties i e the average rainfall of 240 543 mm cv of 0 20 0 42 and elevation of 1281 2150 m therefore it was concluded that those two copula functions were not appropriate for modeling the joint probability of drought duration and severity in the study area as such these copula functions would be used in other regions if proper level of reflection and caution was considered however these same copulas have been used directly and without evaluating other proposed copula functions for modeling the joint probability of drought duration and severity shiau and modarres 2009 2 given new proposed method cdr lng it was indicated that the normal copula as designated as the first priority across 21 selected stations 88 of stations or almost all locations in the study area therefore it seems that the normal copula was not embedded with spatial independency within new proposed method regarding traditional method it was indicated that the appropriate copula function as the first priority of galambos normal frank and joe was dispersed across 13 6 3 and 2 stations respectively on the other hand galambos copula occupied the first 13 stations and second 8 stations priority status in the 21 selected stations as such it could be argued that it represented the best performance in analyzing the joint probability of drought duration and severity across the study area in terms of traditional method 3 having compared the results of the evaluation statistics of log likelihood aic bic and rmse it was indicated that the performance of all copulas in modeling the joint probability of drought duration and severity was significantly improved in terms of new proposed method on the other hand the results indicated relatively slight differences in statistics of goodness of fit tests in terms of first priority and second priority copula functions accordingly it was decided to include the copula function for the first and second priorities and subsequently it was concluded that the copula functions of normal galambos frank and gumbel 92 58 38 and 12 represented an appropriate performance in modeling the joint probability of drought duration and severity across the selected stations respectively fig 9 represents the frequency distribution of parameter values of copula functions across the selected stations using the box plot diagrams maximum 25 50 75 and minimum values of data as seen in fig 9 the appropriate copula parameter for each method in the study area i e normal and galambos was estimated higher in relation to the new proposed and traditional methods respectively as such it corroborated the hypothesis that it is improving performance of copula function was significantly associated with increasing parameter of appropriate copula regarding the analysis of joint probability of drought duration and severity it seems very important to fit the tail of copula function with the extreme observation values conversely the traditional method was embedded with very sparse and scattered extreme data within observed data series e g in rahimabad station a 20 month drought was depicted alone and with a long distance from its immediate smaller 10 month drought in such a case the theoretical copula and its parameter were highly inclined to justify the central values of observed data series and a few observed higher extreme values did not significantly affect the selection of appropriate copula function and its parameter estimations accordingly there was a wide deviation between tail fit of theoretical distribution and observational data in terms of traditional method fig 11 in a similar vein some researchers have argued that goodness of fit tests conducted in tail extreme values may pave the way for selecting the best copula function in the domain of traditional method poulin et al 2007 lee et al 2013 mirabbasi et al 2013 reddy and singh 2014 however the accuracy and appropriacy of such a method is still faced with serious doubts when having scattered dispersed and sparse extreme data in this regard linear regression of maximum values of drought duration against copula parameter was considered across the selected stations in terms of both the applied methods fig 10 shows those correlation coefficient values calculated for seven different copula functions it indicates that the correlation coefficient between the maximum values of drought duration and copula parameter across the selected stations in return for new proposed method is very high ρ 0 8 conversely the latter such statistic is very low and close to zero in return for traditional method it meant that the copula parameter in traditional method was completely independent of the maximum values of drought duration besides such a copula function would not have the capability to correctly predict the extreme events or tail events of drought duration and severity in addition it was attempted to make use of monte carlo procedure to generate 2000 pair of random data for drought duration and severity in return for each of the copula to graphically compare the performance of copula in terms of the two applied methods then the generated data of drought duration and severity were transformed back into their original domains using marginal distribution functions fig 11 shows the scatter plot of generated random pair data gray dots and observed data red dots of drought duration and severity variables to compare the performance of the two applied methods according to fig 11 the above three conclusions were displayed graphically in relation to rahimabad station the conclusions might is summarized as follow failure of clayton and plackett copulas in the joint probability analysis of drought duration and severity substantial and significant promotion in performance of copula functions on the basis of new proposed method compared to traditional method and guaranteeing the tail fit of copula functions with extreme observed data it was emphatically stated that the scatter plot showed that the performance of best copula functions first and second priorities in reproducing the observed data especially upper tailed values was excellent in terms of new proposed method while traditional method completely was failed thereof 3 5 design characteristics of drought duration and severity joint and conditional probabilities of drought related events including drought duration and severity are considered the most important and the main criteria for making decision in relation to short term and long term management of droughts shiau 2006 thus joint and conditional probabilities of future dependent events of drought duration and severity were investigated in accordance with the following subsections 3 5 1 joint probability according to this subsection the authors employed the best copula included in traditional and new proposed methods udr lnln and cdr lng to calculate the value of joint cumulative probability of drought duration and severity p d d s s coupla f d d f s s in the selected stations fig 12 depicts the contour plot of joint cumulative probability of drought duration and severity in return for the best copula normal copula at rahimabad station it indicates that the cumulative joint probability of a drought embedded with duration and severity less than a specified threshold p d d s s was smaller in new proposed method compared to traditional method this might be due to ignoring short term drought events with greater severity in long term continuous droughts in traditional method for example the joint non exccedance probability of a drought with duration 3 months and severity 4 p d 3 s 4 in new proposed and traditional methods was calculated as 0 58 and 0 77 respectively eq 1 on the other hand the joint exccedance probability of p d 3 s 4 in new proposed and traditional methods was determined as 0 14 and 0 07 respectively eq 3 the three dimensional diagram of joint exceedance probability of drought duration and severity p d d s s in return for normal copula in new proposed and traditional methods is depicted in fig 13 3 5 2 joint conditional probability according to the two applied methods it was attempted to calculate the conditional probabilities of drought duration and severity i e p s s d d and p d d s s in the selected stations using the eqs 4 and 5 fig 14 fig 14 indicated that given the new proposed method any given increase in the drought duration would decrease the drought intensity and that the latter decrease would be more severe in comparison to traditional method actually the conditional probability of drought for the status of p d 6 s 8 based on new proposed method was almost close to zero while this probability in the context of traditional method was very significant and equal to 0 61 since the drought threshold level was placed at zero value it seems that the chance event of p d 6 s 8 0 61 is associated with serious doubts as a result the new proposed method seems a much more reasonable than traditional method also it was attempted to depict the joint and conditional probabilities of drought duration and severity in return for different cumulative probabilities i e f x p x x as well as the values of drought duration and severity corresponding to their univariate marginal distribution functions in table 7 and their supplementary diagrams in fig 15 at rahimabad station the forenamed table and figure indicated that regarding the different joint probabilities i e p d d s s 0 05 to 0 98 the values of drought duration in new proposed method were higher than the same values in traditional method besides it was indicated that the difference between them in return for any given increase in joint probability increased exponentially on the other hand the increasing trend of values of drought duration and severity lasted in terms of a linear relation till the joint probability of 85 after that spectrum i e from the probability of 85 98 the increasing trend continued in form of a mild and severe exponential trend in return for traditional and new propose methods respectively this latter result indicated that new proposed method was extremely efficient in reproducing the long term observed extreme droughts e g 20 months in return for the joint probability of drought p d d s s 97 98 or the exceedance probability of drought embedded with 3 conversely traditional method put it that long term extreme droughts in return for joint probability of 97 to 98 were located between 6 7 and 8 7 months given new proposed method it was indicated that the occurrence of long term probable droughts e g 10 to 18 month droughts embedded with probability of smaller than 5 was highly expected in a similar vein the recorded data indicated the occurrence of a 20 month drought during the concerned 43 year study period on the other hand fig 15 indicated that there was an almost linear trend between drought intensity and increased level of conditional probability p d d s s in terms of traditional method as such it indicated that as the duration of drought increased the intensity of drought increased as well however the new proposed method put is that after the upward trend of drought intensity reached its maximum value in conditional probability equal to p d 3 90 s 2 63 0 29 it continued its way within a gentle downward slope in other words the new proposed method put it that the maximum drought intensity belonged to a 4 month drought and after that as the drought duration increased the drought intensity decreased thereof this large difference in performance between the two applied methods was due to low weights of individual observation of extreme drought events i e drought with duration equal to 20 months in rahimabad station to determine the copula function in traditional method fig 11 4 conclusions in this study the researchers attempted to propose a new method cdr to collect drought dependent data and qualify the real climatic conditions so that a more detailed analysis of joint probability distribution of drought duration and severity might be introduced in cdr data collection method unconnected drought runs udr are a combination of short term connected drought runs cdr given this it was tried to make use of short term extreme droughts occurred within a long term drought in order to provide real conditions in the joint probability drought investigations accordingly rainfall data records at 24 selected stations inside the lake urmia basin iran were employed to compare the performance of traditional and new proposed methods in analyzing the joint probability of drought duration and severity on the basis of copula besides the spi was employed as the most sufficient meteorological drought index to collect drought characteristics across the study area having reviewed the results the following findings were highlighted the application of traditional method was indicated that its results cannot be trusted to predict long term bivariate or dependent drought events i e drought duration and severity in particular where the drought data series were embedded with individual or sparse extreme values the inefficiency of traditional method was strongly felt it was found that two copula functions of clayton and plackett represented an unsatisfactory performance in the study area with almost wide range of statistical parameters of rainfall mean standard deviation and skewness as such the application of these copula functions would be more cautious for model joint probability of drought duration and severity in other parts of the world the normal copula was determined as the first dominant priority 88 across the study area based on cdr method however in traditional method the appropriate copula function was allocated by four different copulas thus the hypothesis of independency of appropriate copula normal in cdr method was confirmed in terms of non general case given the cdr method it was indicated that the performance of appropriate copula has significantly improved in terms of modeling the joint probability of drought duration and severity regarding the new proposed method the upper tail of theoretical copula distribution function was exactly fitted with the observed extreme values accordingly the correlation coefficient between maximum values of drought duration and the copula parameter was significantly greater throughout the selected stations this implied the high capability of the selected copula on the basis of new proposed method in appropriate predicting the extreme or tail events of drought duration and severity having examined the joint and conditional probabilities it was indicated that the performance of new proposed model was more superior in modeling the drought return periods conclusively the results represented the high comparative advantage of new proposed method cdr in data collection for a realistic and accurate investigation of joint probability of drought duration and severity besides this advantage was reflected in determining the correct copula function more accurate estimation of copula parameter more realistic estimation of joint or conditional probabilities of drought duration and severity and significant reduction in uncertainty for modeling the joint probability of drought duration and severity 
7377,the copula functions have been widely applied as an advance technique to create joint probability distribution of drought duration and severity the approach of data collection as well as the amount of data and dispersion of data series can last a significant impact on creating such joint probability distribution using copulas usually such traditional analyses have shed an unconnected drought runs udr approach towards droughts in other word droughts with different durations would be independent of each other emphasis on such data collection method causes the omission of actual potentials of short term extreme droughts located within a long term udr meanwhile traditional method is often faced with significant gap in drought data series however a long term udr can be approached as a combination of short term connected drought runs cdr therefore this study aims to evaluate systematically two udr and cdr procedures in joint probability of drought duration and severity investigations for this purpose rainfall data 1971 2013 from 24 rain gauges in lake urmia basin iran were applied also seven common univariate marginal distributions and seven types of bivariate copulas were examined compared to traditional approach the results demonstrated a significant comparative advantage of the new approach such comparative advantages led to determine the correct copula function more accurate estimation of copula parameter more realistic estimation of joint conditional probabilities of drought duration and severity and significant reduction in uncertainty for modeling keywords copula drought spi conditional probability 1 introduction given the spatial extent and persistence of droughts it may be argued that droughts are among the important phenomena occurring in the nature generally drought is caused by lack of water supply in a specific geographical area for a prolonged period rossi 2000 rossi and cancelliere 2013 this natural phenomenon is an inevitable part of climatic variation and it is repeated in different climatic zones of the world wilhite 2000 the occurrence of a drought may be approached from different aspects including drought duration severity intensity and inter arrival time as such those characteristics are very important for planning and management of available water resources dracup et al 1980 in the meantime two major characteristics of drought duration and severity have been the focus of researchers attention because a they are considered as primary parameters in estimating of other drought characteristic such as intensity and b they are regarded as two basic characteristics for real time and long term drought management kim et al 2006 shiau et al 2007 shiau and modarres 2009 hao and singh 2013 several researchers have analyzed those drought characteristics in terms of univariate variable around the world i e fernández and salas 1999 chung and salas 2000 shiau and shen 2001 cancelliere and salas 2004 salas et al 2005 serinaldi et al 2009 montaseri and amirataee 2017 conducted a comprehensive study to analyze the generalized or long term behavior of abovementioned four drought characteristics as univariate variables drought duration severity intensity and inter arrival time using extensive use of monte carlo simulation method across different climatic conditions around the world however the characteristics of any given drought drought duration and severity in particular indicate a significant dependence to each other accordingly the joint probability analyses of such drought characteristics bivariate or trivariate dependent variables can provide very valuable complementary criteria for developing upstream policies governing decision making in the realm of drought crisis management song and singh 2010 however the bivariate multivariate probability analyses require much more complex techniques and computing methods than univariate probability analyses of drought characteristics salvadori et al 2007 this may be one of the main reasons why researchers prefer univariate drought characteristics analyses besides it may justify the limited number of studies that have investigated the drought characteristics via bivariate multivariate joint probability analyses traditionally most of studies on joint probability analyses of drought characteristics including bivariate probability analysis of drought duration and severity have been based on classical bivariate distributions such as bivariate normal distribution goel et al 1998 yue 1999 since the drought duration and severity are dependent and do not generally follow the same marginal probability distribution zhang and singh 2006 it may be argued that the classical joint probability distributions do not have the capacity required for satisfactory and accurate modeling of dependent drought characteristics such as drought duration and severity salvadori et al 2007 given the past few years numerous researchers have attempted to resort to competent capabilities of different marginal distributions so that they can make use of copula functions as a novel advanced technique in analyzing the joint probability of two or more hydrological dependent variables e g de michele and salvadori 2003 de michele et al 2005 grimaldi and serinaldi 2006 shiau et al 2006 zhang and singh 2006 kao and govindaraju 2007 2008 shiau and modarres 2009 lee and salas 2011 mirabbasi et al 2013 the following section discusses these items thereof shiau 2006 made use of standardized precipitation index spi and copula functions to model the joint probability distribution of drought duration and severity in taiwan the exponential and gamma probability distributions were considered as the appropriate nominated distribution of drought duration and severity respectively in this study seven copula functions of various families were examined in detail finally galambos was selected as the most suitable copula in bivariate analysis of drought duration and severity in the study area shiau and modarres 2009 applied a probability approach and copula functions to examine the relationship among drought duration severity and frequency in the period 1954 2003 across two rain gauge stations located within two different dry and wet climatic conditions in iran having assumed the exponential and gamma probability distributions as the marginal distributions for drought duration and severity it was attempted to make use of clayton copula function to model the joint probability distribution of drought severity duration frequency the results indicated that if fluctuations in rainfall were high enough the drought would be more severe in the wet area ma et al 2013 applied trivariate joint functions to examine such drought characteristics as drought duration severity and peak of drought in weihe river basin in china accordingly they examined six different copula and concluded that the normal and t student copula functions best fitted with the three dependent variables in the study area having assumed the marginal distributions of exponential and gamma in the peninsular malaysia zin et al 2013 employed spi and copula functions to analyze the meteorological drought severity and duration in this study they resorted to rainfall data collected from 50 rain gauge stations during the period 1975 2004 then five copula functions of clayton frank joe gumbel hougaard and galambos were examined to determine the most appropriate copula to establish a joint probability distribution subsequently the joe copula was selected as the most appropriate copula thereof mirabbasi et al 2013 employed various bivariate copulas to analyze the characteristics of meteorological drought in iran two characteristics of drought duration and severity were assessed via spi and then the exponential and gamma distributions were fitted to drought duration and severity values respectively the results indicated that according to upper tail dependence coefficient and goodness of fit tests the galambos copula was the best copula to model the joint probability distribution of drought characteristics besides the selected copula revealed such bivariate probability characteristics as return periods conditional and joint probabilities abdul rauf and zeephongsekul 2014 employed monthly rainfall data of selected rain gauge stations to investigate the joint probability distribution of drought duration and severity in the state of victoria australia during the period 1950 2010 besides they applied extreme value and archimedean copulas to determine the bivariate probability distribution of drought duration and severity on the basis of spi the results indicated that weibull and lognormal distributions were the best marginal distributions for fitting the drought duration and severity respectively also each of the clayton and gumbel hougaard copulas were determined as the best copula for 50 of selected stations xu et al 2015 attempted to evaluate the spatial and temporal changes in droughts on the basis of trivariate copula in the southwest china in the period 1961 2012 as such they examined different types of copula functions such as elliptical symmetrical and asymmetrical archimedean copulas then three characteristics of drought severity duration and areal extent were calculated to analyze the drought frequency given the various types of copulas they recognized the joe and gumbel copulas as the best copulas to the three dependent variables then the drought return period was estimated the results indicated that the drought of 2009 2010 had a return period of about 94 years and consequently it was one of the worst droughts in southwest china during the entire period of study a review of the above studies resulted in the following important point generally the drought duration and severity data series are collected from unconnected drought runs udr or independent droughts namely the periods before and after a drought are non drought periods in this way a 1 month drought should be located in an independent period e g between the two non drought months accordingly it is not possible to simultaneously consider a 2 month drought as well as two 1 month droughts if much emphasis is placed on such a data collection procedure it might be possible to overlook the real potentials of short term extreme droughts that take place within a continuous long term drought besides the traditional data collection procedure often led to a significant gap or discontinuation across drought duration and severity e g lack of data for a range of drought duration and severity variables and a malformed probability density function of duration and severity data accordingly it was expected that the basic parameters of joint probability distribution of drought duration and severity could heavily affect such items as parameters of copula functions the joint probabilities of drought return period etc as such it was likely that these parameters could turn into unrealistic elements actually a continuous long term drought might be subdivided into a number of short term connected drought runs cdr or non independent droughts with different duration for example a continuous three month drought also represents three 1 month drought and two 2 month drought as well therefore in this study a new approach namely connected drought runs cdr procedure was examined to investigate the joint probability distribution of drought duration and severity for this purpose both traditional and new proposed methods namely udr and cdr procedures respectively were employed to collect drought data i e duration and severity values in study area to do this end historical monthly rainfall data 1971 2013 were examined across 24 rain gauge stations in lake urmia basin iran accordingly the drought monitoring and drought characteristics data based on udr and cdr procedures were gauged using the spi then the copula function technique was used to investigate the bivariate probability of drought duration and severity on the basis of the aforementioned two approaches 2 material and methods 2 1 study area and data lake urmia is one of the largest salty lakes in the world located in the northwestern of iran the area of lake urmia basin is about 52 000 km2 and it is bounded by the sabalan mountains which is located in the vicinity of aras river and the zagros mountains to the north as well as the zaab river to the south the elevation of lake urmia basin varies between 1236 m and 3733 m above sea level us geological survey s earth explorer https earthexplorer usgs gov the climatic condition of lake urmia basin is classified into cold semi arid köppen geiger classification bsk and dsa peel et al 2007 topographical and geographical map of the study area is shown in fig 1 currently lake urmia is faced with a severe crisis in terms of reduced water volume given the reduction of water resources surface water and groundwater and occurrence of frequent droughts in lake urmia basin in recent decades it seems that this natural habitat has faced a severe ecosystemic and environmental risk aghakouchak et al 2015 therefore the concerned data on the study area included significant changes across extreme drought events besides this can include very valuable and relevant data to compare and evaluate both traditional and new proposed methods in the joint probability analysis of drought duration and severity on the other hand a careful and real analysis of dependent characteristics of drought duration and severity can be considered a very important tool in managing drought risk in lake urmia basin in this study it was attempted to make use of monthly rainfall data records collected from 24 rainfall stations in lake urmia basin for the period 1971 2013 besides the quality of the data was strictly controlled before released by the authors given the selected stations it was indicated that the average annual rainfall varied between 542 6 mm in hashemabad station and 239 7 mm in azer shahr station furthermore it was observed that their annual rainfall coefficients of variation were located between 22 and 41 the lag one serial correlation of annual rainfall was positive across the majority of stations and the values of skewness varied between 0 28 and 1 48 the locations of the selected rainfall stations are shown in fig 1 in addition the main statistical properties of annual rainfall data records such as mean coefficient of variation cv skewness skew lag one serial correlation ρ are presented in table 1 2 2 standardized precipitation index spi the standardized precipitation index spi was developed by mckee et al in 1993 to determine and monitor droughts the spi is simple spatially invariant and probabilistic in nature and can be applied to analyze different types of drought phenomena such as meteorological agricultural and hydrological data mishra and singh 2010 montaseri and amirataee 2017 carried out a comprehensive stochastic assessment of seven common meteorological drought indices using the rainfall data from different part of the world embedded with various climatic conditions they concluded that the spi had an accurate and realistic meteorological drought monitoring capabilities and thus it was essentially superior to other precipitation based meteorological drought indices the spi could determine wet and dry situations for a specific time scale for each location using the rainfall data firstly the appropriate probability distribution is fitted to the long term rainfall data and then the cumulative distribution function is turned into the normal distribution via equal probabilities finally the transformed data to normal distribution are used to calculate spi values montaseri and amirataee 2017 2 3 drought identification and data collection of drought duration and severity yevjevich 1967 proposed the use of theory of runs to define drought characteristics fig 2 a run is defined as a portion of time series of variable xt in which all values are either below i e a negative run or above i e a positive run a selected truncation level of x0 accordingly it is called either a negative or positive run regarding this two main components of a drought event as shown in fig 2 are identified drought duration di it is the time period between the start and end of a consecutive drought event and drought severity si it indicates a cumulative deficiency of a consecutive drought parameter below the critical level traditionally the copula based analysis of joint probability distribution of drought duration and severity has commonly considered the individual drought for different duration as an unconnected drought run udr i e the periods before and after the drought are non drought periods fig 2 based on this definition in fig 2 as a hypothetical example it might be argued that only four 1 month droughts of d1 d3 d5 d8 would be obtained for a 50 month record length their corresponding drought severity values positive values are 0 5 1 25 1 and 1 5 respectively however a long term unconnected drought run udr represented several short term connected drought runs cdr that occurred as successive short term runs traditionally there has been an overemphasis on the assumption of independency of droughts with different durations in analyzing joint probability of drought duration and severity accordingly the actual potentials of short term extreme droughts located within a continuous long term drought may be ignored for example fig 2 indicates that the traditional worst severity values for 1 month and 2 month droughts are 1 5 and 3 respectively conversely four in d4 d6 d7 and d9 and one in d9 severities are determined as the much worse severity values for 1 month and 2 month droughts within the continuous long term droughts respectively fig 2 on the other hand the udr data series with different duration have often faced with significant gaps i e lack of data for a range of drought duration given traditional method fig 2 it is indicated that only 1 2 3 and 6 month droughts could be obtained and consequently 4 and 5 month droughts could not be observed actually it would affect the results of an analysis of joint probability distribution of drought duration and severity however a 6 month unconnected drought run udr could be approached in terms of six 1 month periods five 2 month periods four 3 month periods three 4 month periods two 5 month periods and a 6 month period connected drought runs cdr using moving windows technique in new proposed methods table 2 depicts the data series of drought duration and severity of above example fig 2 on the basis of both traditional and new approaches 2 4 copula based joint distribution function the copula based multivariate distribution structure was developed by sklar 1959 joe 1997 nelsen 2006 and salvadori et al 2007 as such the copula function may couple the marginal distribution into bivariate or multivariate distributions accordingly the copulas are widely used in the joint probability analysis in such areas as finance hydrology meteorology and risk management grimaldi et al 2016 salvadori et al 2007 according to sklar s theorem nelsen 2006 if fxy is a two dimensional joint distribution function of dependent random variables of x and y with marginal distribution functions fx and fy then there exist a copula c such that 1 p x x y y f xy x y c f x x f y y conversely for any distribution functions fx and fy and any copula c the function fxy defined above is a two dimensional function with marginal fx and fy furthermore if fx and fy are continuous c is unique if the probability density functions pdf fx x and fy y of x and y exist then the joint pdf of the two random variables can be expressed as 2 f x y x y c f x x f y y f x x f y y where c is the density function of c nelsen 2006 copulas have been constituted from different families and the following seven types were used in this study archimedean clayton frank gumbel and joe extreme value gumbel elliptical normal and others plackett the application of copula theory in the bivariate multivariate joint probability distribution investigations require to examine correlation structure of dependent variables via two types of correlation tests such as cross pearson s r and rank correlation coefficients kendall s tau and spearman s rho compared with pearson s correlation coefficient it seemed that the rank correlation coefficients were less susceptible to outlier values and more sensitive to the nonlinear correlation of variables zhang and singh 2012 given this it was attempted to utilize kendall s tau and spearman s rho rank correlation coefficients tests in order to detect the correlation structure of drought variables 2 5 marginal distributions of drought duration and severity the identification and determination of the most appropriate marginal distribution for each of the dependent variables may play a key role in the joint probability analysis of drought characteristics using copula technique given the analysis of joint probability of drought duration and severity it was usually indicated that the exponential and gamma probability functions were appropriately fitted on drought duration and severity respectively hao and singh 2013 mirabbasi et al 2013 contrarily some direct tests conducted by different researchers have shown that the aforementioned distributions have not always fitted as the superior distribution for drought duration and severity values wong et al 2008 yusof et al 2013 thus in this study seven distributions of exponential gamma gumbel normal lognormal logistic and weibull were directly examined to determine the most appropriate probability for drought duration and severity values the goodness of fit test was applied to define the best probability distribution using two common methods of akaike information criterion aic and bayesian information criterion bic also the parameters of the probability distributions seven types were calculated using the maximum likelihood estimation mle bardossy and pegram 2009 zhang and singh 2007 the most commonly used methods for estimating the copula parameters are as follow method of moments genest and rivest 1993 exact maximum likelihood method eml inference function for margins ifm joe 1997 and maximum pseudo likelihood estimation method mpl genest et al 1995 compared with other estimation methods it is indicated that the ifm method is the preferred full parametric method for multidimensional parameter estimation because its approach is close to maximum likelihood and it is easier to implement joe and xu 1996 joe 1997 therefore the ifm method was employed for estimation of the copula parameters in this study in this method firstly the parameters of the marginal probability distribution were estimated using maximum likelihood estimation based on the log likelihood function of drought duration and severity then the copula dependence parameter θ was determined through the maximization of the log likelihood function of copula shiau 2006 selecting a suitable copula is quite important for modeling dependence correctly among marginal distributions therefore log likelihood akaike information criterion aic and bayesian information criterion bic tests along with rmse were used to evaluate the performance of fitted copula models the model with the highest lowest log likelihood aic bic and rmse statistic values represented the most suitable copula model 2 6 the joint probability distribution of drought duration and severity the bivariate probability distribution of drought duration and severity is considered as a very efficient tool in predicting forecasting and managing of drought events shiau 2006 the latter is due to this fact that the risk analysis of drought duration or severity may not solely yield sufficient information about the real conditions of drought event as a two dependent climatic phenomenon actually such case makes it very difficult to develop the operation and management of water resources systems in different drought conditions zhang and singh 2006 for example a 1 month and a 3 month drought with a same severity value of 3 in a water resources system is embedded with very diverse rate of vulnerabilities and damage therefore if the bivariate risk of drought duration and severity exceeds above a specified threshold then it may be concluded that there occurs a critical situation for an ecosystem these bivariate multivariate probability distributions could easily be calculated by copula given this the following equation was used to calculate the joint exceedance probability of drought duration d and severity s over their respective thresholds d and s shiau 2006 3 p d d s s 1 f d d f s s c f d d f s s where fd d and fs s represent the exceedance probability of duration and severity from their fitted marginal distributions respectively and c represent the copula function all three aforementioned probabilities are shown schematically in fig 3 also bivariate drought conditional probabilities could be calculated using the copula functions given the analysis of these drought related variables one might resort to the following equations to calculate the conditional probability of drought severity duration when the drought duration severity exceeded a specified threshold d s 4 p s s d d f s s c f d d f s s 1 f d d 5 p d d s s f d d c f d d f s s 1 f s s 3 results and discussions 3 1 the probabilities of drought occurrence the frequency distribution of monthly values of spi across the selected stations from 1971 to 2013 in terms of 25 50 and 75 exceedance probabilities are depicted in fig 4 as seen in fig 4 dry and wet periods were frequent and without a clear pattern repeated in the region during recorded length 43 years also fig 5 depicts the detailed results of frequency analysis of two correlated drought variables at rahimabad station according to fig 5a the empirical cumulative distribution of drought duration p d i d was formed as a down tailless s shaped as such any given increase in duration of short term drought would increase the non exceedance probability of drought duration with a very sharp slope for example the non exceedance exceedance probability of 1 month 3 month and 6 month droughts at rahimabad station were determined as 0 51 0 83 and 0 95 0 49 0 17 and 0 05 respectively thus the empirical probability distribution of drought duration should be asymmetrical figs 6 and 8 also the empirical probability density function of wet and dry periods at rahimabad station is shown in fig 5b given this the empirical probability density function of wet and dry periods was almost symmetrical and close to the standard normal probability density function montaseri and amirataee 2017 it was found that the probability of normal state was equal to 0 77 and total probabilities of dry and wet states extreme severe and moderate were 0 07 and 0 17 respectively in addition the variation of average severity for 1 month to 7 month droughts against the exceedance probability of drought duration i e 1 month to 7 month droughts demonstrated a logarithmic relationship with a correlation coefficient of 0 99 fig 5c this represented that as the drought duration increased the drought intensity and probability of drought duration decreased 3 2 characteristics of drought duration and severity according to the definition provided for spi any given value less than 1 in any given period is classified as a dry drought period mckee et al 1993 mishra and singh 2010 however some researchers believe that consecutive semi dry periods 1 spi 0 with massive cumulative deficit i e high rate of drought severity have a greater adverse effect on any given ecosystem than short periods embedded with spi less than 1 shiau 2006 therefore the authors made use of two drought thresholds of zero and 1 i e spi 1 and spi 0 in order to collect data series of drought duration and severity across the selected stations statistical parameters of spi data series for three different procedures i e spi 1 udr spi 0 udr and spi 0 cdr at rahimabad station depict in table 3 according to table 3 there were much more drought events in return for zero drought threshold especially in case of cdr this element along with such varying values of statistical parameters as mean standard deviation and skewness of three spi data series as well as the argument raised by shiau in 2006 might play a key role in determining the best marginal distribution of drought duration and severity an interesting point about cdr method was that compared to udr method the data skewness declined considerably fig 6 shows the effect of drought threshold and two udr and cdr methods in drought data collections using scatter plot of drought duration and severity data together with their marginal histograms it should be mentioned that the results of rahimabad station were similarly repeated in the rest of the stations as such fig 7 depicts the scatter plot of drought duration and severity using the two applied methods udr and cdr across four other stations in the study area it is essential to assess the correlation between two dependent variables in analyzing the joint probability distribution of drought duration and severity as such table 4 depicts the correlation coefficient between drought duration and severity values based on udr and cdr methods using kendall s tau and spearman s rho rank dependence tests given all three methods the results showed that the correlation coefficient between drought duration and severity was significant and that the new proposed method yielded a higher statistic in this regard compared to other two methods regarding the results and given the argument raised by shiau in 2006 it was decided to rely on new proposed method cdr spi 0 to analyze joint probability distribution of drought duration and severity across the selected stations 3 3 marginal distribution of drought variables the identification and determination of the most appropriate marginal distribution for each dependent variable perform a key role in relation to development of bivariate multivariate joint probability distribution using copula to do this end seven probability distributions of exponential gamma gumbel normal lognormal logistic and weibull were fitted to drought duration and severity data the parameters of probability distributions were calculated using maximum likelihood estimation mle the two goodness of fit test namely aic and bic was next applied to realize the best marginal probability distribution of drought duration and severity values the results of goodness of fit tests of drought duration and severity values at rahimabad station as well as the percentages of best fitted marginal probability distribution across all the selected stations are presented in table 5 moreover for graphical test specification the theoretical and empirical pdfs and cdfs plot of drought duration and severity values based on udr and cdr approaches at rahimabad station are shown in fig 8 based on table 5 the lognormal and gamma distributions were defined as the most appropriate probability distributions for drought duration and severity values derived from cdr method in the selected stations respectively on the other hand for both dependent variables of udr method the lognormal were selected as the best probability distribution in the study area the latter fact pointed to the merits of conducting a direct test to select the most appropriate marginal distribution rather than resorting to the suggested distributions of exponential and gamma in fitting with drought duration and severity 3 4 joint dependence structure using copula selecting the appropriate copula in this section the results derived from the application of both traditional and new proposed methods abbreviated as udr lnln and cdr lng in joint probability analysis of drought duration and severity were comprehensively compared to do this end bivariate probability distribution of drought duration and severity was modeled in accordance with the seven common copula functions of clayton gumbel frank joe galambos plackett and normal for each station in the study area the parameters of copula were estimated using the common and reliable method of ifm shiau 2006 shiau and modarres 2009 li et al 2012 finally the most appropriate copula functions with minimum maximum statistic values of aic bic and rmse log likelihood were determined zhang and singh 2006 li et al 2012 those criteria could significantly and quantitatively summarize the deviation of the estimated and observed values shiau 2006 karmakar and simonovic 2009 chen et al 2013 yusof et al 2013 the results of joint probability modeling of two drought dependent variables with seven copula functions at rahimabad station as well as other stations in the study area were summarized and presented in table 6 according to table 6 the following conclusions could be deduced in order of importance 1 since the two copula functions of clayton and plackett were not considered as proper copula in the first and second priority levels across none of the selected stations with widish range of statistical properties i e the average rainfall of 240 543 mm cv of 0 20 0 42 and elevation of 1281 2150 m therefore it was concluded that those two copula functions were not appropriate for modeling the joint probability of drought duration and severity in the study area as such these copula functions would be used in other regions if proper level of reflection and caution was considered however these same copulas have been used directly and without evaluating other proposed copula functions for modeling the joint probability of drought duration and severity shiau and modarres 2009 2 given new proposed method cdr lng it was indicated that the normal copula as designated as the first priority across 21 selected stations 88 of stations or almost all locations in the study area therefore it seems that the normal copula was not embedded with spatial independency within new proposed method regarding traditional method it was indicated that the appropriate copula function as the first priority of galambos normal frank and joe was dispersed across 13 6 3 and 2 stations respectively on the other hand galambos copula occupied the first 13 stations and second 8 stations priority status in the 21 selected stations as such it could be argued that it represented the best performance in analyzing the joint probability of drought duration and severity across the study area in terms of traditional method 3 having compared the results of the evaluation statistics of log likelihood aic bic and rmse it was indicated that the performance of all copulas in modeling the joint probability of drought duration and severity was significantly improved in terms of new proposed method on the other hand the results indicated relatively slight differences in statistics of goodness of fit tests in terms of first priority and second priority copula functions accordingly it was decided to include the copula function for the first and second priorities and subsequently it was concluded that the copula functions of normal galambos frank and gumbel 92 58 38 and 12 represented an appropriate performance in modeling the joint probability of drought duration and severity across the selected stations respectively fig 9 represents the frequency distribution of parameter values of copula functions across the selected stations using the box plot diagrams maximum 25 50 75 and minimum values of data as seen in fig 9 the appropriate copula parameter for each method in the study area i e normal and galambos was estimated higher in relation to the new proposed and traditional methods respectively as such it corroborated the hypothesis that it is improving performance of copula function was significantly associated with increasing parameter of appropriate copula regarding the analysis of joint probability of drought duration and severity it seems very important to fit the tail of copula function with the extreme observation values conversely the traditional method was embedded with very sparse and scattered extreme data within observed data series e g in rahimabad station a 20 month drought was depicted alone and with a long distance from its immediate smaller 10 month drought in such a case the theoretical copula and its parameter were highly inclined to justify the central values of observed data series and a few observed higher extreme values did not significantly affect the selection of appropriate copula function and its parameter estimations accordingly there was a wide deviation between tail fit of theoretical distribution and observational data in terms of traditional method fig 11 in a similar vein some researchers have argued that goodness of fit tests conducted in tail extreme values may pave the way for selecting the best copula function in the domain of traditional method poulin et al 2007 lee et al 2013 mirabbasi et al 2013 reddy and singh 2014 however the accuracy and appropriacy of such a method is still faced with serious doubts when having scattered dispersed and sparse extreme data in this regard linear regression of maximum values of drought duration against copula parameter was considered across the selected stations in terms of both the applied methods fig 10 shows those correlation coefficient values calculated for seven different copula functions it indicates that the correlation coefficient between the maximum values of drought duration and copula parameter across the selected stations in return for new proposed method is very high ρ 0 8 conversely the latter such statistic is very low and close to zero in return for traditional method it meant that the copula parameter in traditional method was completely independent of the maximum values of drought duration besides such a copula function would not have the capability to correctly predict the extreme events or tail events of drought duration and severity in addition it was attempted to make use of monte carlo procedure to generate 2000 pair of random data for drought duration and severity in return for each of the copula to graphically compare the performance of copula in terms of the two applied methods then the generated data of drought duration and severity were transformed back into their original domains using marginal distribution functions fig 11 shows the scatter plot of generated random pair data gray dots and observed data red dots of drought duration and severity variables to compare the performance of the two applied methods according to fig 11 the above three conclusions were displayed graphically in relation to rahimabad station the conclusions might is summarized as follow failure of clayton and plackett copulas in the joint probability analysis of drought duration and severity substantial and significant promotion in performance of copula functions on the basis of new proposed method compared to traditional method and guaranteeing the tail fit of copula functions with extreme observed data it was emphatically stated that the scatter plot showed that the performance of best copula functions first and second priorities in reproducing the observed data especially upper tailed values was excellent in terms of new proposed method while traditional method completely was failed thereof 3 5 design characteristics of drought duration and severity joint and conditional probabilities of drought related events including drought duration and severity are considered the most important and the main criteria for making decision in relation to short term and long term management of droughts shiau 2006 thus joint and conditional probabilities of future dependent events of drought duration and severity were investigated in accordance with the following subsections 3 5 1 joint probability according to this subsection the authors employed the best copula included in traditional and new proposed methods udr lnln and cdr lng to calculate the value of joint cumulative probability of drought duration and severity p d d s s coupla f d d f s s in the selected stations fig 12 depicts the contour plot of joint cumulative probability of drought duration and severity in return for the best copula normal copula at rahimabad station it indicates that the cumulative joint probability of a drought embedded with duration and severity less than a specified threshold p d d s s was smaller in new proposed method compared to traditional method this might be due to ignoring short term drought events with greater severity in long term continuous droughts in traditional method for example the joint non exccedance probability of a drought with duration 3 months and severity 4 p d 3 s 4 in new proposed and traditional methods was calculated as 0 58 and 0 77 respectively eq 1 on the other hand the joint exccedance probability of p d 3 s 4 in new proposed and traditional methods was determined as 0 14 and 0 07 respectively eq 3 the three dimensional diagram of joint exceedance probability of drought duration and severity p d d s s in return for normal copula in new proposed and traditional methods is depicted in fig 13 3 5 2 joint conditional probability according to the two applied methods it was attempted to calculate the conditional probabilities of drought duration and severity i e p s s d d and p d d s s in the selected stations using the eqs 4 and 5 fig 14 fig 14 indicated that given the new proposed method any given increase in the drought duration would decrease the drought intensity and that the latter decrease would be more severe in comparison to traditional method actually the conditional probability of drought for the status of p d 6 s 8 based on new proposed method was almost close to zero while this probability in the context of traditional method was very significant and equal to 0 61 since the drought threshold level was placed at zero value it seems that the chance event of p d 6 s 8 0 61 is associated with serious doubts as a result the new proposed method seems a much more reasonable than traditional method also it was attempted to depict the joint and conditional probabilities of drought duration and severity in return for different cumulative probabilities i e f x p x x as well as the values of drought duration and severity corresponding to their univariate marginal distribution functions in table 7 and their supplementary diagrams in fig 15 at rahimabad station the forenamed table and figure indicated that regarding the different joint probabilities i e p d d s s 0 05 to 0 98 the values of drought duration in new proposed method were higher than the same values in traditional method besides it was indicated that the difference between them in return for any given increase in joint probability increased exponentially on the other hand the increasing trend of values of drought duration and severity lasted in terms of a linear relation till the joint probability of 85 after that spectrum i e from the probability of 85 98 the increasing trend continued in form of a mild and severe exponential trend in return for traditional and new propose methods respectively this latter result indicated that new proposed method was extremely efficient in reproducing the long term observed extreme droughts e g 20 months in return for the joint probability of drought p d d s s 97 98 or the exceedance probability of drought embedded with 3 conversely traditional method put it that long term extreme droughts in return for joint probability of 97 to 98 were located between 6 7 and 8 7 months given new proposed method it was indicated that the occurrence of long term probable droughts e g 10 to 18 month droughts embedded with probability of smaller than 5 was highly expected in a similar vein the recorded data indicated the occurrence of a 20 month drought during the concerned 43 year study period on the other hand fig 15 indicated that there was an almost linear trend between drought intensity and increased level of conditional probability p d d s s in terms of traditional method as such it indicated that as the duration of drought increased the intensity of drought increased as well however the new proposed method put is that after the upward trend of drought intensity reached its maximum value in conditional probability equal to p d 3 90 s 2 63 0 29 it continued its way within a gentle downward slope in other words the new proposed method put it that the maximum drought intensity belonged to a 4 month drought and after that as the drought duration increased the drought intensity decreased thereof this large difference in performance between the two applied methods was due to low weights of individual observation of extreme drought events i e drought with duration equal to 20 months in rahimabad station to determine the copula function in traditional method fig 11 4 conclusions in this study the researchers attempted to propose a new method cdr to collect drought dependent data and qualify the real climatic conditions so that a more detailed analysis of joint probability distribution of drought duration and severity might be introduced in cdr data collection method unconnected drought runs udr are a combination of short term connected drought runs cdr given this it was tried to make use of short term extreme droughts occurred within a long term drought in order to provide real conditions in the joint probability drought investigations accordingly rainfall data records at 24 selected stations inside the lake urmia basin iran were employed to compare the performance of traditional and new proposed methods in analyzing the joint probability of drought duration and severity on the basis of copula besides the spi was employed as the most sufficient meteorological drought index to collect drought characteristics across the study area having reviewed the results the following findings were highlighted the application of traditional method was indicated that its results cannot be trusted to predict long term bivariate or dependent drought events i e drought duration and severity in particular where the drought data series were embedded with individual or sparse extreme values the inefficiency of traditional method was strongly felt it was found that two copula functions of clayton and plackett represented an unsatisfactory performance in the study area with almost wide range of statistical parameters of rainfall mean standard deviation and skewness as such the application of these copula functions would be more cautious for model joint probability of drought duration and severity in other parts of the world the normal copula was determined as the first dominant priority 88 across the study area based on cdr method however in traditional method the appropriate copula function was allocated by four different copulas thus the hypothesis of independency of appropriate copula normal in cdr method was confirmed in terms of non general case given the cdr method it was indicated that the performance of appropriate copula has significantly improved in terms of modeling the joint probability of drought duration and severity regarding the new proposed method the upper tail of theoretical copula distribution function was exactly fitted with the observed extreme values accordingly the correlation coefficient between maximum values of drought duration and the copula parameter was significantly greater throughout the selected stations this implied the high capability of the selected copula on the basis of new proposed method in appropriate predicting the extreme or tail events of drought duration and severity having examined the joint and conditional probabilities it was indicated that the performance of new proposed model was more superior in modeling the drought return periods conclusively the results represented the high comparative advantage of new proposed method cdr in data collection for a realistic and accurate investigation of joint probability of drought duration and severity besides this advantage was reflected in determining the correct copula function more accurate estimation of copula parameter more realistic estimation of joint or conditional probabilities of drought duration and severity and significant reduction in uncertainty for modeling the joint probability of drought duration and severity 
7378,in sparsely fractured rock the ubiquitous heterogeneity of the matrix which has been observed in different laboratory and in situ experiments has been shown to have a significant influence on retardation mechanisms that are of importance for the safety of deep geological repositories for nuclear waste here we propose a conceptualisation of a typical heterogeneous granitic rock matrix based on micro discrete fracture networks micro dfn different sets of fractures are used to represent grain boundary pores as well as micro fractures that transect different mineral grains the micro dfn model offers a great flexibility in the way inter and intra granular space is represented as the different parameters that characterise each fracture set can be fine tuned to represent samples of different characteristics here the parameters of the model have been calibrated against experimental observations from granitic rock samples taken at forsmark sweden and different variant cases have been used to illustrate how the model can be tied to rock samples with different attributes numerical through diffusion simulations have been carried out to infer the bulk properties of the model as well as to compare the computed mass flux with the experimental data from an analogous laboratory experiment the general good agreement between the model results and the experimental observations shows that the model presented here is a reliable tool for the understanding of retardation mechanisms occurring at the mm scale in the matrix keywords crystalline rock matrix micro dfn through diffusion experiment inter granular space 1 introduction in the context of safety analyses of nuclear waste repositories in crystalline rocks the rock matrix is considered as an important natural buffer as it can retain harmful radionuclides potentially released due to a repository failure or attenuate potential changes in hydrogeochemical conditions due to e g the infiltration of oxygen rich glacial melt water however the retardation capacity of the rock matrix is intimately related to its mineralogical and physical heterogeneity e g cvetkovic 2010 voutilainen et al 2013 trinchero et al 2017a iraola et al 2017 laboratory studies conducted in the framework of different site characterisation campaigns have pointed out the ubiquitous heterogeneity of the unaltered granitic rock matrix penttinen et al 2006 selnert et al 2008 ikonen et al 2014 the influence of matrix heterogeneity on solute diffusion has also been observed in different in situ experiments for instance in the diffusion experiment ltde sd carried out in the äspö hard rock laboratory sweden the penetration profiles observed for non sorbing and sorbing radionuclides showed anomalous sometimes referred to as non fickian shapes which were attributed to the heterogeneous nature of the rock matrix in terms of both the microporous network and mineral surfaces available for sorption nilsson et al 2010 the increasing availability of micro characterisation techniques based on e g micro computed tomography e g blunt et al 2013 fusseis et al 2014 voutilainen et al 2012 has provided valuable data sets that describe the physical and mineralogical heterogeneity of given rock samples down to a resolution of about a micron these datasets can be directly used in lagrangian based diffusion calculations voutilainen et al 2013 or even included in more computationally expensive multi component reactive transport models iraola et al 2017 however their use is still limited due to the relatively high cost of sample analyses and the small sample sizes for this reason there is still the need for abstraction in the conceptualisation of the rock matrix in order to provide generic models which are in turn of interest for e g the parameterisation of geochemical reactions trinchero et al 2017a b with this objective in mind we investigate here if a micro discrete fracture network micro dfn model can be used to represent the diffusion available pore space of the intact rock matrix of a water saturated crystalline rock in doing this the small scale porosity of the rock matrix is represented by different sets of planar fractures with different intensities lengths and apertures the conceptual understanding of the small scale porosity is used to tie each fracture set to well known features of the rock matrix which are of importance for its capacity for matrix diffusion of solutes where possible experimental laboratory data from the forsmark site in sweden are used when quantifying the most important input data of the model experimental parameters used are porosity porosity distribution effective diffusivity effective tortuosity constrictivity pore aperture pore length and grain size where there are unknowns different modelling cases are used to shed light on their importance this work has not investigated sorption or immobilisation of solutes on mineral surfaces or electrostatic interactions between minerals and charged solutes however the resulting model can be used e g to study the interplay of physical and mineralogical heterogeneity on solute transport by means of a dfn consistent parameterisation e g trinchero et al 2017c 2 conceptual model of the rock matrix a common feature of crystalline rocks is that the main building blocks are mineral grains typically on the mm scale or smaller up to the cm scale for igneous rock having phaneritic texture such as granite one may typically say that if the volume dominating grain size is about 1 mm the rock is fine grained if it is medium grained the grain size is between 1 and 5 mm and if it is coarse grained 5 and 50 mm sen 2013 between the volume dominating grains which are often framework silicates such as feldspars and quartz there are smaller grains consisting of for example sheet silicates and alteration products moreover the whole grains or only part of them may have been subjected to deuteric or hydrothermal alteration alteration products as well as sheet silicates often feature a relatively larger intra granular porosity mineral grains are thereby surrounded and or intersected by small scale pore space that is depending on the rock type dominated by micro fractures grain boundary pores sheet silicate pores and or solution pores mori et al 2003 this is illustrated by fig 1 showing veined gneiss from the oilkiluoto site in finland ikonen et al 2014 where the sample is 37 by 24 mm the left hand side shows a close up photo where different mineral grains are visible the right hand side shows an autoradiograph of the same sample which had previously been impregnated by the 14c pmma polymethylmethacrylate technique e g hellmuth et al 1993 oila et al 2005 the blackening of the autoradiograph indicates connected porosity which is estimated to be 0 4 on average the porosity is interpreted as being divided into three types in ikonen et al 2014 the relatively large and planar structures correspond to micro fractures which may transect mineral grains around the mineral grains inter granular grain boundary pores are shown the grain boundary porosity can be seen as a more or less continuous porous network separating the mineral grains this network can be envisioned as being constructed of more or less planar grain boundary pores where the length scale of the individual segment is similar to the average grain size in the rock sample of fig 1 the aperture of the grain boundary pores is seemingly smaller than those of micro fractures blackened spots in the autoradiograph correspond to altered mineral clusters or sheet silicates e g biotite of a relatively high intra granular porosity spot wise blackening of the autoradiographs could also from a general perspective correspond to inter granular porosity such as ruptured fluid inclusions if taking a closer look at a phaneritic crystalline rock sample by for example scanning electron microscope or x ray diffraction tullborg et al 2008 it becomes apparent that the size of the volume dominating grains on the mm to cm scale only represents the upper part of the grain size distribution mineral grains fragments of mineral grains or altered parts of mineral grains can be found down to the μm range and probably lower depending on where one draws the limit the large range of grain sizes together with the multitude of pore types including intra granular porosity makes the porous network tremendously complicated with pore lengths from the μm scale and upwards and pore apertures from the nm scale and upwards pore apertures of granitic rock can be determined with for example hg porosimetry or x ray computed microtomography klobes et al 1997 voutilainen et al 2012 klobes et al 1997 for example found pore apertures in the range 10 8 10 5 m in porphyritic granodiorite samples where pores in the lower middle as well as upper aperture range contributed significantly to the total porosity 3 experimental observations from forsmark rock matrix in this article the proposed micro dfn model aims to represent unaltered rock matrix from the forsmark site from hereon the conceptual model of the rock matrix is discussed in the light of this rock type with aid from laboratory work where possible the unaltered rock matrix at the forsmark site has been extensively investigated in site investigations carried out by the swedish nuclear fuel and waste management company skb an overview of the site characterisation and the site geology is provided in strom et al 2008 tullborg et al 2008 andersson et al 2013 this article focuses on a representative section of borehole kfm02a where a short drill core section from about 550 m depth has been carefully investigated in the laboratory regarding its porosity and diffusivity the rock type is categorised as metamorphosed fine to medium grained granitoid by sandstrom and stephens 2009 which indicates that the volume governing grain sizes are about one or a few millimetres the mineral composition is given in table 3 3 of sandstrom and stephens 2009 investigations include porosity characterisation by 14c pmma impregnation and water gravimetry as well as measurements of the effective diffusivity and rock capacity factor in through diffusion experiments using tritiated water as a tracer penttinen et al 2006 selnert et al 2008 from the borehole interval 552 23 552 33 m a 10 cm long sample fig 2 was taken penttinen et al 2006 characterise the rock as fine to medium grained granite changing to granodiorite and tonalite the matrix was metamorphic by using the 14c pmma technique a 40 mm long subsample was impregnated and a cut was investigated by digital autoradiography after polishing in autoradiography the radiation from the 14c pmma that impregnates the connected pore space blackens an autoradiographic film or its equivalent digital sensor the blackening intensity as averaged over a pixel can be converted to a local porosity in a recent version of digital autoradiography the detection of 14c radiation is made with a resolution of 600 dpi making the base of a square pixel 42 μm sardini et al 2015 a photo of the analysed cut is shown in fig 3 left together with the blackened autoradiographic film middle the sample diameter is 51 mm fig 3 right shows post processed results from digital autoradiography the 14c pmma technique gives a very large number of pixels with individual porosities which can be treated statistically the porosity histogram of the studied sample is given in fig 4 on the order of 90 of the pixels show no or very low detectable intensity and correspond to non porous mineral grains 6 of the pixels have a porosity of about 0 2 at the tail of the histogram the porosity reaches 2 5 both microfractures and grain boundary pores can locally be approximated as planes even though the network is tortuous and constrictive there are other pore types such as solution pores and other inter granular porosity which are typically non planar such pores should have a limited contribution to the effective diffusivity of the sample but may significantly contribute to its storage capacity in fig 3 middle one can see black dots indicating non planar pores porous patches according to penttinen et al 2006 grain boundary pores dominate this particular sample the overall pmma porosity of the studied sample was 0 05 which is less than the water immersion porosity of 0 2 obtained on the same sample penttinen et al 2006 claim that the rock sample had been fully pmma impregnated the discrepancy may be due to sample preparation and methodological issues or sampling issues as one method samples a volume while the other samples a surface from the nearby borehole section 554 59 554 95 m a 36 cm long intact piece of the drill core was sawn into 12 rock samples the water gravimetry porosity of these samples was measured as well as their diffusive properties in through diffusion experiments tables a1 2 and a 2 in selnert et al 2008 the twelve samples had the following lengths 5 mm three samples 10 mm three samples 30 mm three samples and 50 mm three samples results from samples of the same length have been pooled in table 1 showing their arithmetic mean and standard deviation in normal space for one 5 mm sample no reliable effective diffusivity and effective tortuosity could be obtained the remaining two 5 mm samples have significantly higher effective diffusivities and porosities as well as lower effective tortuosities compared to the longer samples the parameters listed in table 1 relate to the following equations e g neretnieks 1980 1 d e 2 c x 2 α c t 2 d e f f d w ϕ δ τ 2 d w 3 τ d τ δ ϕ d w d e d w d p 4 α ϕ k d ρ where c is solute concentration mol m w 3 m w 3 indicates the volume of water d e m2 s the effective diffusivity α the rock capacity factor f f the formation factor d w m2 s the diffusivity in unconstrained solution d p m2 s pore diffusivity δ constrictivity τ tortuosity τ d effective tortuosity ϕ connected porosity k d m3 kg the sorption partitioning coefficient and ρ kg m3 the rock bulk dry density the sawing of the drill core likely induced additional micro fractures li 2001 tullborg and larson 2006 which may especially manifest in the 5 mm samples if a damaged zone forms at the sawing cuts the volumetric fraction of this zone would be larger for the 5 mm sample in addition the shorter sample may have sustained the mechanical forces from the sawing worse than the longer samples causing more severe micro fracturing also non connected pores may have become part of the connected porosity both at the sample edges and due to the additional micro fracturing for all these reasons the 5 mm samples are not considered as representative for this study disregarding the 5 mm samples table 1 gives the average water immersion porosity effective diffusivity and effective tortuosity for the remaining samples these data are used in calibrating the micro dfn model of the rock matrix 4 model formulation 4 1 generation of the porosity micro structure the domain considered in this study is a typical cylindrical rock sample of length l 30 mm and diameter ø 50 mm we describe in this sub section the assumptions and the numerical approach used to generate the porosity micro structure of this sample for the so called base case model alternative formulations are provided in section 5 and tested in section 6 a sketch showing the dfn based approach used to represent the porosity micro structure of the base case bc model is shown in fig 5 the small scale inter and intra granular pore space is represented using two sets of fractures set 2 and 3 one with length interval 4 5 mm and the other in the 1 2 mm interval this roughly equals the range of volume dominating grain sizes in fine to medium grained crystalline rock the largest set set 1 represents micro fractures that may intersect mineral grains its lower limit is 5 mm i e approximately equal to the size of the largest grains whereas the upper limit is restricted to 20 mm i e slightly smaller that the dimension of the sample in the axial direction the micro dfn is generated using the finite volume groundwater flow code darcytools svensson et al 2010a svensson and ferry 2014 in darcytools fracture orientation follows a fisher distribution but here a random orientation is used and spatial centers are statistically independent and follow a poisson process the generation of fractures is governed by the following equation 5 n i a l dl l ref a l l ref a where n is the number of fractures per unit volume i m 3 is the intensity a is the power law exponent and l ref m is the reference length which is here set to 1 m the generation of the micro dfn follows this stepwise approach 1 the two larger fracture sets sets 1 and 2 are represented onto the underlying numerical grid of cell size 0 25 mm using the formulation provided by svensson 2001a b all the cells that are not in contact with the generated fractures are removed partly to save computational power moreover isolated fractures and their contacting cells are removed 2 from step 1 a grid with holes is generated these holes represent mineral grains of negligible intra granular porosity it can be seen from fig 5 that the length and intensity of the fractures in fracture set 2 will determine the size of the holes 3 the smallest fracture set set 3 is generated with fracture centres located within the grid in a final step all three fracture sets are used to generate properties in the grid see section 4 3 an overview of the generation of the micro dfn is provided in appendix a for further information the reader is referred to svensson et al 2010b the resulting micro structure of porosity of the bc model is shown in fig 6 it is worthwhile noting that the different fracture sets used in the bc model as well as in the other alternative models see section 5 are conceptually treated as planar fracture regions such a fracture region includes the main pore which is not envisioned to be strictly planar the main pore may also be in contact with mineral grains having small scale intra granular porosity and be connected to small scale inter granular porosity around mineral grains much smaller than 1 mm this creates a water pathway that has a quite complex local geometry to account for the fact that only part of the fracture region contains the water pathway the fracture region is assigned a local intrinsic effective tortuosity that is above 1 this is further discussed in the next sub section 4 2 parameters of the micro dfn in the bc model we have arbitrarily set the fracture aperture of all the fracture sets to 0 5 μm and fine tuned their fracture intensity in order to get a bulk porosity in agreement with the values listed in table 1 i e ϕ b 0 2 different cases with different aperture are presented in section 5 in the bc model the intrinsic tortuosity τ d of the three fracture sets has been set equal to a constant value of 5 i e little more than half of the observed effective tortuosity cf table 1 this local value of tortuosity accounts for the fact that all pores are assumed to be planar with constant aperture whereas the actual individual pores are tortuous and foremost constricted the appropriateness of setting the intrinsic tortuosity to about half of the observed effective tortuosity is discussed in the result section in practice this means that the pore diffusivity used in all fracture sets is 4 10 10 m2 s based on a d w of 2 10 9 m2 s the bulk pore diffusivity i e the pore diffusivity measured over the scale of the whole sample is expected to be lower due to intrinsic tortuosity of the underlying micro dfn a summary of the parameters used to generate the micro dfn is provided in table 2 4 3 solute transport in the dfn based micro continuum model the dfn discussed in sections 4 1 and 4 2 is represented in an equivalent continuum porous medium ecpm model using the approach described by svensson 2001a b intersecting volumes between fractures and grid cell control volumes are computed and contributions from all intersecting fractures are added to the related parameters for each grid cell thus defining dfn derived pore diffusivity and porosity using this micro continuum model and darcytools two different sets of transport calculations have been carried out 1 steady state diffusion equation de the steady state diffusion equation is solved with dirichlet boundary condition applied to the two circular faces of the sample c 1 mol m w 3 at the inlet and c 0 mol m w 3 at the outlet this set of calculations is used to compute the bulk properties of the sample i e bulk diffusion coefficient and bulk tortuosity 2 transient de the transient diffusion equation eq 1 is solved with the same boundary conditions as for the steady state case cumulative mass discharge is computed at the outlet face 5 setup of alternative cases in order to test the influence of parameters such as fracture aperture and intensity of the different sets of fractures as well as to show how the model can be tied to rock samples of other characteristics three additional variant cases have been defined 5 1 variant case 1 vc1 in vc1 the aperture of the third set of fractures set 3 was increased to 5 10 4 m and the intensity of this fracture set was consequently decreased by one order of magnitude i 50 m 3 and the porosity set to 0 01 to get the same value of bulk porosity as the bc model see section 6 for further details vc1 aims at capturing the influence of porous patches fig 3 middle 5 2 variant case 2 vc2 in vc2 the intensity of set 3 was decreased i 120 m 3 and a fourth set of fractures was included set 4 with the same parameters as set 3 table 2 but with a smaller aperture 2 10 7 m and a higher intensity i 950 m 3 these parameters have been calibrated to get the same bulk porosity as the bc model this variant model is intended to give a better qualitative match with porosity distributions from pmma measurements see section 6 for further details 5 3 variant case 3 vc3 vc3 is a more simplified case based on the same parameters as the bc model for set 1 and 2 but with set 3 being neglected as discussed in detail in section 6 the bulk porosity of this model is significantly lower than for the rest of simulation cases this model is used to investigate the influence of fracture set 3 on solute diffusion of non sorbing species on the cm scale a summary of the number of fractures and the grid size of the base case and the three other alternative cases is presented in table 3 6 modelling results 6 1 evaluation of local and bulk properties we refer here to local properties as the properties defined over the volume of a single computational cell whereas the bulk properties refer to the overall bulk behaviour of the model all the local and bulk properties presented in this section are obtained from an inspection of the numerical grid except for effective diffusivity that is inferred from the results of a steady state calculation point 1 in section 4 3 more specifically the effective diffusion coefficient is computed as the mass flux through the sample divided by the concentration gradient the spatial distribution of steady state concentration for the bc model is shown in fig 7 the local and bulk properties of each model are listed in tables 4 and 5 the local properties which are computed as the average arithmetic mean over the active grid cells of the continuum model display very modest changes among the different models however despite all the models except vc3 that is based on an set up of much too low bulk porosity having the same value of bulk porosity ϕ b 0 2 in agreement with the experimental observations of table 1 a visual analysis of the rock sample fig 8 points out their significant global differences for instance compared to the bc model when set 3 is modified by using a larger fracture aperture and a lower intensity vc1 most of the void space related to grain boundary pores is localised in sparsely distributed spots which are less connected to each other this most probably explains the slightly lower bulk pore diffusivity of this model on the contrary when a fourth set of fractures with very low aperture and high intensity is included vc2 the inter connection between small scale inter granular pores is increased with a related slight increase of pore diffusivity when the third set of fractures is neglected vc3 a considerable amount of pore space is lost which points out that in the bc model only around 15 of the pore space is constituted by micro fractures and relatively large grain boundary pores as already discussed in section 4 2 the bulk values of diffusivity are significantly lower than the local values due to the intrinsic complexity and tortuosity of the network of connections which results in an additive term to local tortuosity particle tracking simulations carried out using the bc model details of the results are not shown here for the sake of brevity have pointed out that the average geometric tortuosity i e the actual path length followed by a particle trajectory divided by the sample length is around 1 6 this value multiplied by the intrinsic local tortuosity 5 see table 2 gives a value of bulk tortuosity of 3 6 which is in agreement with the measurements listed in table 1 the histograms of porosity distribution are shown in fig 9 for the bc model and for vc2 these histograms were computed using a cell size of 42 μm both histograms capture the qualitative behaviour observed from the pmma measurements fig 4 with most of the pixels active grid cells having a porosity of 0 2 and with a relatively long tailing to higher values however both histograms of fig 9 and in particular that of bc have a significantly longer tailing and a larger number of porous pixels which in the case of the models correspond to the active grid cells see fig 8 this difference is due to the fact that the models have been calibrated to match the bulk porosity measured from the water immersion technique it has already been discussed in section 3 that there is a discrepancy between these measurements and the estimates obtained from 14c pmma technique the latter being characterised by significantly lower values 6 2 breakthrough curves the results of the transient de calculations point 2 in section 4 3 are shown in fig 10 which shows the cumulative mass discharge for the three considered models the results of the more simplified vc3 model are not shown here as they are not relevant for the discussion when the system reaches a steady state the relationship between mass discharge and time becomes linear and this time can be approximated by the following characteristic time for diffusion t c l c 2 d p b where l c is the sample length the slope of the linear part of the curves is instead proportional to d e b in other words the bulk pore diffusivity controls the time taken by the system to attain steady state whereas the bulk effective diffusivity controls the slope of the curve this explains why steady state is attained at earlier times for vc2 whose mass discharge curve also shows a higher slope on the contrary the curve of the vc1 model which is characterised by a significantly lower bulk pore and effective diffusivity value is displaced to later times and has a much lower slope 6 3 comparison with an experimental through diffusion experiment we have already discussed in section 6 1 how the characterisation data of the 12 rock samples introduced in section 3 have been used to calibrate the models here we focus on one of these samples the one taken in the interval 554 72 554 75 m and we compare the breakthrough curves observed during the through diffusion experiment with that computed using the previously calibrated bc model it is worthwhile stressing that the results presented here are related to the calibration exercise discussed in section 6 1 as the same experimental data are used earlier we only considered the steady state flux now the transient problem is in focus in the through diffusion experiment a face of the sample was exposed to a volume of groundwater tagged with the target conservative radionuclide inlet reservoir c inl e and the rising concentration was monitored in an outlet reservoir in contact with the opposite face c out e given the relatively large volume of both the inlet and outlet reservoirs the set up of the experiment is consistent with the boundary conditions of the transient calculations see section 4 3 here the normalized concentration observed in the outlet reservoir during the experiment c n c out e v res c inl e v sample with v res and v sample being the volume of the outlet reservoir and rock sample m3 respectively is compared with the normalised cumulative mass discharge computed with the bc model i e cumulative mass discharge divided by the volume of the rock sample notice that in the model a unitary concentration is set at the inlet the resulting c n vs time plots are shown in fig 11 some discrepancy between the model and the experimental results is observed at early times similar deviations between experimental data and a model used for parameters estimations were already observed in earlier studies valkiainen et al 1991 johansson et al 1999 selnert et al 2008 here this discrepancy is attributable to small scale heterogeneity of the porosity field not completely resolved by the scale of resolution of the model yet at late times the numerical curve agrees well with the experimental results which indicates that the bc model captures the overall bulk behaviour of the considered granitic rock it is worthwhile stressing that a similar fitting exercise could be carried out using a different rock sample in that case the fitting procedure would require fine tuning the model to capture the bulk behavior of the considered sample which is given by its bulk effective diffusion coefficient and its bulk pore diffusion coefficient 7 discussion and conclusions we have presented here a conceptualisation of a typical unaltered granitic matrix based on a micro dfn model our study leads to the following main conclusions 1 the use of different sets of fractures offers a great flexibility in the way inter granular space i e micro fractures that may transect mineral grains grain boundary pores and other types of pores is represented in fact the different parameters that represent each set of fractures e g length interval intensity aperture etc can be fine tuned to tie the model to rock samples of different characteristics 2 the second largest fracture set is shown to control the average size of grain minerals by changing the parameters of this fracture set one can represent rock samples with different volume dominating grain size i e from fine to medium to coarse grained 3 the parameters of the smallest sets of fractures are shown to control characteristics of the rock sample such as the degree of inter connection between small scale inter and intra granular space 4 transport calculations carried out using different variant cases of the micro dfn model gave insight into the influence of the different fracture sets on the resulting estimated bulk parameters for instance when the degree of inter connection between small scale pores is lower the resulting bulk pore diffusivity is also lower 5 the base case bc model which was intentionally parameterised to mimic a specific rock type i e the cored borehole kfm02a taken at forsmark sweden reproduces well the results of a through diffusion experiment carried out in a rock sample taken from the same cored borehole the matrix model presented here is seen as a useful tool for the evaluation of reactive transport processes occurring at the grain scale e g trinchero et al 2017b as well as for the related upscaling procedures required to extend the signature of matrix heterogeneity to the scale of interest of the afore mentioned safety assessment studies as all conceptualisations the model is a simplified representation of the available pore space of a granitic rock matrix this is in fact constituted by micro fractures grain boundary pores and microporous minerals patches that are often characterised by very irregular and different geometries e g sardini et al 2007 all these geometries are simplified here using different sets of planar stochastically generated micro fractures despite this simplification the model has been shown to be able to reproduce the bulk behaviour of a non sorbing tracer for sorbing tracers parameters such as fracture intensity and aperture are expected to have a strong control on the density of sorption sites and thus in principle they could also be fine tuned to reproduce the observed bulk retention capacity of the rock matrix acknowledgements us ml and pt thank the swedish nuclear fuel and waste management company skb for the financial support this work has benefited from insightful comments by paul sardini and two anonymous reviewers appendix a overview of the dfn generation geometrically a random fracture is a rectangular parallelepiped two of whose three edge lengths are equal we refer to the two equal edge lengths as the length l of the fracture and to the third edge length as the thickness b the centre of the fracture is the geometrical central point of the parallelepiped and the axis is the straight line through the centre which is parallel to the thickness edges all fractures in the fracture set have the same thickness b the lengths l are restricted to the interval l min l max the centre locations are restricted to a region of space called the generation domain the lengths and centre locations of the fractures are calculated in a random process where the number of fractures comes out as a by product the process also has the parameters l ref p and i ref l ref is specified in m p is dimensionless and i ref is specified in m 3 the process has the following stochastic properties consider an infinitesimal length interval l l dl belonging to the interval l min l max also consider an infinitesimal volume element belonging to the generation domain and let dv be the volume of the element let dn denote the number of generated fractures whose lengths belong to l l dl and whose centres belong to the volume element the expected value of dn will then be given by the following power law a 1 e dn i ref l l ref p 1 dl l ref dv the numbers of fractures generated for disjunct pairs of infinitesimal length intervals and volume elements are stochastically independent two pairs are considered disjunct if the length intervals of the pairs are disjunct or the volume elements of the pairs are disjunct from these properties it is possible to conclude that the total number n of fractures generated for the entire length interval l min l max and the entire generation domain will have a poisson distribution with expected value a 2 e n i ref p l max l ref p l min l ref p v if p 0 i ref ln l max l min v if p 0 where v is the volume of the generation domain 
7378,in sparsely fractured rock the ubiquitous heterogeneity of the matrix which has been observed in different laboratory and in situ experiments has been shown to have a significant influence on retardation mechanisms that are of importance for the safety of deep geological repositories for nuclear waste here we propose a conceptualisation of a typical heterogeneous granitic rock matrix based on micro discrete fracture networks micro dfn different sets of fractures are used to represent grain boundary pores as well as micro fractures that transect different mineral grains the micro dfn model offers a great flexibility in the way inter and intra granular space is represented as the different parameters that characterise each fracture set can be fine tuned to represent samples of different characteristics here the parameters of the model have been calibrated against experimental observations from granitic rock samples taken at forsmark sweden and different variant cases have been used to illustrate how the model can be tied to rock samples with different attributes numerical through diffusion simulations have been carried out to infer the bulk properties of the model as well as to compare the computed mass flux with the experimental data from an analogous laboratory experiment the general good agreement between the model results and the experimental observations shows that the model presented here is a reliable tool for the understanding of retardation mechanisms occurring at the mm scale in the matrix keywords crystalline rock matrix micro dfn through diffusion experiment inter granular space 1 introduction in the context of safety analyses of nuclear waste repositories in crystalline rocks the rock matrix is considered as an important natural buffer as it can retain harmful radionuclides potentially released due to a repository failure or attenuate potential changes in hydrogeochemical conditions due to e g the infiltration of oxygen rich glacial melt water however the retardation capacity of the rock matrix is intimately related to its mineralogical and physical heterogeneity e g cvetkovic 2010 voutilainen et al 2013 trinchero et al 2017a iraola et al 2017 laboratory studies conducted in the framework of different site characterisation campaigns have pointed out the ubiquitous heterogeneity of the unaltered granitic rock matrix penttinen et al 2006 selnert et al 2008 ikonen et al 2014 the influence of matrix heterogeneity on solute diffusion has also been observed in different in situ experiments for instance in the diffusion experiment ltde sd carried out in the äspö hard rock laboratory sweden the penetration profiles observed for non sorbing and sorbing radionuclides showed anomalous sometimes referred to as non fickian shapes which were attributed to the heterogeneous nature of the rock matrix in terms of both the microporous network and mineral surfaces available for sorption nilsson et al 2010 the increasing availability of micro characterisation techniques based on e g micro computed tomography e g blunt et al 2013 fusseis et al 2014 voutilainen et al 2012 has provided valuable data sets that describe the physical and mineralogical heterogeneity of given rock samples down to a resolution of about a micron these datasets can be directly used in lagrangian based diffusion calculations voutilainen et al 2013 or even included in more computationally expensive multi component reactive transport models iraola et al 2017 however their use is still limited due to the relatively high cost of sample analyses and the small sample sizes for this reason there is still the need for abstraction in the conceptualisation of the rock matrix in order to provide generic models which are in turn of interest for e g the parameterisation of geochemical reactions trinchero et al 2017a b with this objective in mind we investigate here if a micro discrete fracture network micro dfn model can be used to represent the diffusion available pore space of the intact rock matrix of a water saturated crystalline rock in doing this the small scale porosity of the rock matrix is represented by different sets of planar fractures with different intensities lengths and apertures the conceptual understanding of the small scale porosity is used to tie each fracture set to well known features of the rock matrix which are of importance for its capacity for matrix diffusion of solutes where possible experimental laboratory data from the forsmark site in sweden are used when quantifying the most important input data of the model experimental parameters used are porosity porosity distribution effective diffusivity effective tortuosity constrictivity pore aperture pore length and grain size where there are unknowns different modelling cases are used to shed light on their importance this work has not investigated sorption or immobilisation of solutes on mineral surfaces or electrostatic interactions between minerals and charged solutes however the resulting model can be used e g to study the interplay of physical and mineralogical heterogeneity on solute transport by means of a dfn consistent parameterisation e g trinchero et al 2017c 2 conceptual model of the rock matrix a common feature of crystalline rocks is that the main building blocks are mineral grains typically on the mm scale or smaller up to the cm scale for igneous rock having phaneritic texture such as granite one may typically say that if the volume dominating grain size is about 1 mm the rock is fine grained if it is medium grained the grain size is between 1 and 5 mm and if it is coarse grained 5 and 50 mm sen 2013 between the volume dominating grains which are often framework silicates such as feldspars and quartz there are smaller grains consisting of for example sheet silicates and alteration products moreover the whole grains or only part of them may have been subjected to deuteric or hydrothermal alteration alteration products as well as sheet silicates often feature a relatively larger intra granular porosity mineral grains are thereby surrounded and or intersected by small scale pore space that is depending on the rock type dominated by micro fractures grain boundary pores sheet silicate pores and or solution pores mori et al 2003 this is illustrated by fig 1 showing veined gneiss from the oilkiluoto site in finland ikonen et al 2014 where the sample is 37 by 24 mm the left hand side shows a close up photo where different mineral grains are visible the right hand side shows an autoradiograph of the same sample which had previously been impregnated by the 14c pmma polymethylmethacrylate technique e g hellmuth et al 1993 oila et al 2005 the blackening of the autoradiograph indicates connected porosity which is estimated to be 0 4 on average the porosity is interpreted as being divided into three types in ikonen et al 2014 the relatively large and planar structures correspond to micro fractures which may transect mineral grains around the mineral grains inter granular grain boundary pores are shown the grain boundary porosity can be seen as a more or less continuous porous network separating the mineral grains this network can be envisioned as being constructed of more or less planar grain boundary pores where the length scale of the individual segment is similar to the average grain size in the rock sample of fig 1 the aperture of the grain boundary pores is seemingly smaller than those of micro fractures blackened spots in the autoradiograph correspond to altered mineral clusters or sheet silicates e g biotite of a relatively high intra granular porosity spot wise blackening of the autoradiographs could also from a general perspective correspond to inter granular porosity such as ruptured fluid inclusions if taking a closer look at a phaneritic crystalline rock sample by for example scanning electron microscope or x ray diffraction tullborg et al 2008 it becomes apparent that the size of the volume dominating grains on the mm to cm scale only represents the upper part of the grain size distribution mineral grains fragments of mineral grains or altered parts of mineral grains can be found down to the μm range and probably lower depending on where one draws the limit the large range of grain sizes together with the multitude of pore types including intra granular porosity makes the porous network tremendously complicated with pore lengths from the μm scale and upwards and pore apertures from the nm scale and upwards pore apertures of granitic rock can be determined with for example hg porosimetry or x ray computed microtomography klobes et al 1997 voutilainen et al 2012 klobes et al 1997 for example found pore apertures in the range 10 8 10 5 m in porphyritic granodiorite samples where pores in the lower middle as well as upper aperture range contributed significantly to the total porosity 3 experimental observations from forsmark rock matrix in this article the proposed micro dfn model aims to represent unaltered rock matrix from the forsmark site from hereon the conceptual model of the rock matrix is discussed in the light of this rock type with aid from laboratory work where possible the unaltered rock matrix at the forsmark site has been extensively investigated in site investigations carried out by the swedish nuclear fuel and waste management company skb an overview of the site characterisation and the site geology is provided in strom et al 2008 tullborg et al 2008 andersson et al 2013 this article focuses on a representative section of borehole kfm02a where a short drill core section from about 550 m depth has been carefully investigated in the laboratory regarding its porosity and diffusivity the rock type is categorised as metamorphosed fine to medium grained granitoid by sandstrom and stephens 2009 which indicates that the volume governing grain sizes are about one or a few millimetres the mineral composition is given in table 3 3 of sandstrom and stephens 2009 investigations include porosity characterisation by 14c pmma impregnation and water gravimetry as well as measurements of the effective diffusivity and rock capacity factor in through diffusion experiments using tritiated water as a tracer penttinen et al 2006 selnert et al 2008 from the borehole interval 552 23 552 33 m a 10 cm long sample fig 2 was taken penttinen et al 2006 characterise the rock as fine to medium grained granite changing to granodiorite and tonalite the matrix was metamorphic by using the 14c pmma technique a 40 mm long subsample was impregnated and a cut was investigated by digital autoradiography after polishing in autoradiography the radiation from the 14c pmma that impregnates the connected pore space blackens an autoradiographic film or its equivalent digital sensor the blackening intensity as averaged over a pixel can be converted to a local porosity in a recent version of digital autoradiography the detection of 14c radiation is made with a resolution of 600 dpi making the base of a square pixel 42 μm sardini et al 2015 a photo of the analysed cut is shown in fig 3 left together with the blackened autoradiographic film middle the sample diameter is 51 mm fig 3 right shows post processed results from digital autoradiography the 14c pmma technique gives a very large number of pixels with individual porosities which can be treated statistically the porosity histogram of the studied sample is given in fig 4 on the order of 90 of the pixels show no or very low detectable intensity and correspond to non porous mineral grains 6 of the pixels have a porosity of about 0 2 at the tail of the histogram the porosity reaches 2 5 both microfractures and grain boundary pores can locally be approximated as planes even though the network is tortuous and constrictive there are other pore types such as solution pores and other inter granular porosity which are typically non planar such pores should have a limited contribution to the effective diffusivity of the sample but may significantly contribute to its storage capacity in fig 3 middle one can see black dots indicating non planar pores porous patches according to penttinen et al 2006 grain boundary pores dominate this particular sample the overall pmma porosity of the studied sample was 0 05 which is less than the water immersion porosity of 0 2 obtained on the same sample penttinen et al 2006 claim that the rock sample had been fully pmma impregnated the discrepancy may be due to sample preparation and methodological issues or sampling issues as one method samples a volume while the other samples a surface from the nearby borehole section 554 59 554 95 m a 36 cm long intact piece of the drill core was sawn into 12 rock samples the water gravimetry porosity of these samples was measured as well as their diffusive properties in through diffusion experiments tables a1 2 and a 2 in selnert et al 2008 the twelve samples had the following lengths 5 mm three samples 10 mm three samples 30 mm three samples and 50 mm three samples results from samples of the same length have been pooled in table 1 showing their arithmetic mean and standard deviation in normal space for one 5 mm sample no reliable effective diffusivity and effective tortuosity could be obtained the remaining two 5 mm samples have significantly higher effective diffusivities and porosities as well as lower effective tortuosities compared to the longer samples the parameters listed in table 1 relate to the following equations e g neretnieks 1980 1 d e 2 c x 2 α c t 2 d e f f d w ϕ δ τ 2 d w 3 τ d τ δ ϕ d w d e d w d p 4 α ϕ k d ρ where c is solute concentration mol m w 3 m w 3 indicates the volume of water d e m2 s the effective diffusivity α the rock capacity factor f f the formation factor d w m2 s the diffusivity in unconstrained solution d p m2 s pore diffusivity δ constrictivity τ tortuosity τ d effective tortuosity ϕ connected porosity k d m3 kg the sorption partitioning coefficient and ρ kg m3 the rock bulk dry density the sawing of the drill core likely induced additional micro fractures li 2001 tullborg and larson 2006 which may especially manifest in the 5 mm samples if a damaged zone forms at the sawing cuts the volumetric fraction of this zone would be larger for the 5 mm sample in addition the shorter sample may have sustained the mechanical forces from the sawing worse than the longer samples causing more severe micro fracturing also non connected pores may have become part of the connected porosity both at the sample edges and due to the additional micro fracturing for all these reasons the 5 mm samples are not considered as representative for this study disregarding the 5 mm samples table 1 gives the average water immersion porosity effective diffusivity and effective tortuosity for the remaining samples these data are used in calibrating the micro dfn model of the rock matrix 4 model formulation 4 1 generation of the porosity micro structure the domain considered in this study is a typical cylindrical rock sample of length l 30 mm and diameter ø 50 mm we describe in this sub section the assumptions and the numerical approach used to generate the porosity micro structure of this sample for the so called base case model alternative formulations are provided in section 5 and tested in section 6 a sketch showing the dfn based approach used to represent the porosity micro structure of the base case bc model is shown in fig 5 the small scale inter and intra granular pore space is represented using two sets of fractures set 2 and 3 one with length interval 4 5 mm and the other in the 1 2 mm interval this roughly equals the range of volume dominating grain sizes in fine to medium grained crystalline rock the largest set set 1 represents micro fractures that may intersect mineral grains its lower limit is 5 mm i e approximately equal to the size of the largest grains whereas the upper limit is restricted to 20 mm i e slightly smaller that the dimension of the sample in the axial direction the micro dfn is generated using the finite volume groundwater flow code darcytools svensson et al 2010a svensson and ferry 2014 in darcytools fracture orientation follows a fisher distribution but here a random orientation is used and spatial centers are statistically independent and follow a poisson process the generation of fractures is governed by the following equation 5 n i a l dl l ref a l l ref a where n is the number of fractures per unit volume i m 3 is the intensity a is the power law exponent and l ref m is the reference length which is here set to 1 m the generation of the micro dfn follows this stepwise approach 1 the two larger fracture sets sets 1 and 2 are represented onto the underlying numerical grid of cell size 0 25 mm using the formulation provided by svensson 2001a b all the cells that are not in contact with the generated fractures are removed partly to save computational power moreover isolated fractures and their contacting cells are removed 2 from step 1 a grid with holes is generated these holes represent mineral grains of negligible intra granular porosity it can be seen from fig 5 that the length and intensity of the fractures in fracture set 2 will determine the size of the holes 3 the smallest fracture set set 3 is generated with fracture centres located within the grid in a final step all three fracture sets are used to generate properties in the grid see section 4 3 an overview of the generation of the micro dfn is provided in appendix a for further information the reader is referred to svensson et al 2010b the resulting micro structure of porosity of the bc model is shown in fig 6 it is worthwhile noting that the different fracture sets used in the bc model as well as in the other alternative models see section 5 are conceptually treated as planar fracture regions such a fracture region includes the main pore which is not envisioned to be strictly planar the main pore may also be in contact with mineral grains having small scale intra granular porosity and be connected to small scale inter granular porosity around mineral grains much smaller than 1 mm this creates a water pathway that has a quite complex local geometry to account for the fact that only part of the fracture region contains the water pathway the fracture region is assigned a local intrinsic effective tortuosity that is above 1 this is further discussed in the next sub section 4 2 parameters of the micro dfn in the bc model we have arbitrarily set the fracture aperture of all the fracture sets to 0 5 μm and fine tuned their fracture intensity in order to get a bulk porosity in agreement with the values listed in table 1 i e ϕ b 0 2 different cases with different aperture are presented in section 5 in the bc model the intrinsic tortuosity τ d of the three fracture sets has been set equal to a constant value of 5 i e little more than half of the observed effective tortuosity cf table 1 this local value of tortuosity accounts for the fact that all pores are assumed to be planar with constant aperture whereas the actual individual pores are tortuous and foremost constricted the appropriateness of setting the intrinsic tortuosity to about half of the observed effective tortuosity is discussed in the result section in practice this means that the pore diffusivity used in all fracture sets is 4 10 10 m2 s based on a d w of 2 10 9 m2 s the bulk pore diffusivity i e the pore diffusivity measured over the scale of the whole sample is expected to be lower due to intrinsic tortuosity of the underlying micro dfn a summary of the parameters used to generate the micro dfn is provided in table 2 4 3 solute transport in the dfn based micro continuum model the dfn discussed in sections 4 1 and 4 2 is represented in an equivalent continuum porous medium ecpm model using the approach described by svensson 2001a b intersecting volumes between fractures and grid cell control volumes are computed and contributions from all intersecting fractures are added to the related parameters for each grid cell thus defining dfn derived pore diffusivity and porosity using this micro continuum model and darcytools two different sets of transport calculations have been carried out 1 steady state diffusion equation de the steady state diffusion equation is solved with dirichlet boundary condition applied to the two circular faces of the sample c 1 mol m w 3 at the inlet and c 0 mol m w 3 at the outlet this set of calculations is used to compute the bulk properties of the sample i e bulk diffusion coefficient and bulk tortuosity 2 transient de the transient diffusion equation eq 1 is solved with the same boundary conditions as for the steady state case cumulative mass discharge is computed at the outlet face 5 setup of alternative cases in order to test the influence of parameters such as fracture aperture and intensity of the different sets of fractures as well as to show how the model can be tied to rock samples of other characteristics three additional variant cases have been defined 5 1 variant case 1 vc1 in vc1 the aperture of the third set of fractures set 3 was increased to 5 10 4 m and the intensity of this fracture set was consequently decreased by one order of magnitude i 50 m 3 and the porosity set to 0 01 to get the same value of bulk porosity as the bc model see section 6 for further details vc1 aims at capturing the influence of porous patches fig 3 middle 5 2 variant case 2 vc2 in vc2 the intensity of set 3 was decreased i 120 m 3 and a fourth set of fractures was included set 4 with the same parameters as set 3 table 2 but with a smaller aperture 2 10 7 m and a higher intensity i 950 m 3 these parameters have been calibrated to get the same bulk porosity as the bc model this variant model is intended to give a better qualitative match with porosity distributions from pmma measurements see section 6 for further details 5 3 variant case 3 vc3 vc3 is a more simplified case based on the same parameters as the bc model for set 1 and 2 but with set 3 being neglected as discussed in detail in section 6 the bulk porosity of this model is significantly lower than for the rest of simulation cases this model is used to investigate the influence of fracture set 3 on solute diffusion of non sorbing species on the cm scale a summary of the number of fractures and the grid size of the base case and the three other alternative cases is presented in table 3 6 modelling results 6 1 evaluation of local and bulk properties we refer here to local properties as the properties defined over the volume of a single computational cell whereas the bulk properties refer to the overall bulk behaviour of the model all the local and bulk properties presented in this section are obtained from an inspection of the numerical grid except for effective diffusivity that is inferred from the results of a steady state calculation point 1 in section 4 3 more specifically the effective diffusion coefficient is computed as the mass flux through the sample divided by the concentration gradient the spatial distribution of steady state concentration for the bc model is shown in fig 7 the local and bulk properties of each model are listed in tables 4 and 5 the local properties which are computed as the average arithmetic mean over the active grid cells of the continuum model display very modest changes among the different models however despite all the models except vc3 that is based on an set up of much too low bulk porosity having the same value of bulk porosity ϕ b 0 2 in agreement with the experimental observations of table 1 a visual analysis of the rock sample fig 8 points out their significant global differences for instance compared to the bc model when set 3 is modified by using a larger fracture aperture and a lower intensity vc1 most of the void space related to grain boundary pores is localised in sparsely distributed spots which are less connected to each other this most probably explains the slightly lower bulk pore diffusivity of this model on the contrary when a fourth set of fractures with very low aperture and high intensity is included vc2 the inter connection between small scale inter granular pores is increased with a related slight increase of pore diffusivity when the third set of fractures is neglected vc3 a considerable amount of pore space is lost which points out that in the bc model only around 15 of the pore space is constituted by micro fractures and relatively large grain boundary pores as already discussed in section 4 2 the bulk values of diffusivity are significantly lower than the local values due to the intrinsic complexity and tortuosity of the network of connections which results in an additive term to local tortuosity particle tracking simulations carried out using the bc model details of the results are not shown here for the sake of brevity have pointed out that the average geometric tortuosity i e the actual path length followed by a particle trajectory divided by the sample length is around 1 6 this value multiplied by the intrinsic local tortuosity 5 see table 2 gives a value of bulk tortuosity of 3 6 which is in agreement with the measurements listed in table 1 the histograms of porosity distribution are shown in fig 9 for the bc model and for vc2 these histograms were computed using a cell size of 42 μm both histograms capture the qualitative behaviour observed from the pmma measurements fig 4 with most of the pixels active grid cells having a porosity of 0 2 and with a relatively long tailing to higher values however both histograms of fig 9 and in particular that of bc have a significantly longer tailing and a larger number of porous pixels which in the case of the models correspond to the active grid cells see fig 8 this difference is due to the fact that the models have been calibrated to match the bulk porosity measured from the water immersion technique it has already been discussed in section 3 that there is a discrepancy between these measurements and the estimates obtained from 14c pmma technique the latter being characterised by significantly lower values 6 2 breakthrough curves the results of the transient de calculations point 2 in section 4 3 are shown in fig 10 which shows the cumulative mass discharge for the three considered models the results of the more simplified vc3 model are not shown here as they are not relevant for the discussion when the system reaches a steady state the relationship between mass discharge and time becomes linear and this time can be approximated by the following characteristic time for diffusion t c l c 2 d p b where l c is the sample length the slope of the linear part of the curves is instead proportional to d e b in other words the bulk pore diffusivity controls the time taken by the system to attain steady state whereas the bulk effective diffusivity controls the slope of the curve this explains why steady state is attained at earlier times for vc2 whose mass discharge curve also shows a higher slope on the contrary the curve of the vc1 model which is characterised by a significantly lower bulk pore and effective diffusivity value is displaced to later times and has a much lower slope 6 3 comparison with an experimental through diffusion experiment we have already discussed in section 6 1 how the characterisation data of the 12 rock samples introduced in section 3 have been used to calibrate the models here we focus on one of these samples the one taken in the interval 554 72 554 75 m and we compare the breakthrough curves observed during the through diffusion experiment with that computed using the previously calibrated bc model it is worthwhile stressing that the results presented here are related to the calibration exercise discussed in section 6 1 as the same experimental data are used earlier we only considered the steady state flux now the transient problem is in focus in the through diffusion experiment a face of the sample was exposed to a volume of groundwater tagged with the target conservative radionuclide inlet reservoir c inl e and the rising concentration was monitored in an outlet reservoir in contact with the opposite face c out e given the relatively large volume of both the inlet and outlet reservoirs the set up of the experiment is consistent with the boundary conditions of the transient calculations see section 4 3 here the normalized concentration observed in the outlet reservoir during the experiment c n c out e v res c inl e v sample with v res and v sample being the volume of the outlet reservoir and rock sample m3 respectively is compared with the normalised cumulative mass discharge computed with the bc model i e cumulative mass discharge divided by the volume of the rock sample notice that in the model a unitary concentration is set at the inlet the resulting c n vs time plots are shown in fig 11 some discrepancy between the model and the experimental results is observed at early times similar deviations between experimental data and a model used for parameters estimations were already observed in earlier studies valkiainen et al 1991 johansson et al 1999 selnert et al 2008 here this discrepancy is attributable to small scale heterogeneity of the porosity field not completely resolved by the scale of resolution of the model yet at late times the numerical curve agrees well with the experimental results which indicates that the bc model captures the overall bulk behaviour of the considered granitic rock it is worthwhile stressing that a similar fitting exercise could be carried out using a different rock sample in that case the fitting procedure would require fine tuning the model to capture the bulk behavior of the considered sample which is given by its bulk effective diffusion coefficient and its bulk pore diffusion coefficient 7 discussion and conclusions we have presented here a conceptualisation of a typical unaltered granitic matrix based on a micro dfn model our study leads to the following main conclusions 1 the use of different sets of fractures offers a great flexibility in the way inter granular space i e micro fractures that may transect mineral grains grain boundary pores and other types of pores is represented in fact the different parameters that represent each set of fractures e g length interval intensity aperture etc can be fine tuned to tie the model to rock samples of different characteristics 2 the second largest fracture set is shown to control the average size of grain minerals by changing the parameters of this fracture set one can represent rock samples with different volume dominating grain size i e from fine to medium to coarse grained 3 the parameters of the smallest sets of fractures are shown to control characteristics of the rock sample such as the degree of inter connection between small scale inter and intra granular space 4 transport calculations carried out using different variant cases of the micro dfn model gave insight into the influence of the different fracture sets on the resulting estimated bulk parameters for instance when the degree of inter connection between small scale pores is lower the resulting bulk pore diffusivity is also lower 5 the base case bc model which was intentionally parameterised to mimic a specific rock type i e the cored borehole kfm02a taken at forsmark sweden reproduces well the results of a through diffusion experiment carried out in a rock sample taken from the same cored borehole the matrix model presented here is seen as a useful tool for the evaluation of reactive transport processes occurring at the grain scale e g trinchero et al 2017b as well as for the related upscaling procedures required to extend the signature of matrix heterogeneity to the scale of interest of the afore mentioned safety assessment studies as all conceptualisations the model is a simplified representation of the available pore space of a granitic rock matrix this is in fact constituted by micro fractures grain boundary pores and microporous minerals patches that are often characterised by very irregular and different geometries e g sardini et al 2007 all these geometries are simplified here using different sets of planar stochastically generated micro fractures despite this simplification the model has been shown to be able to reproduce the bulk behaviour of a non sorbing tracer for sorbing tracers parameters such as fracture intensity and aperture are expected to have a strong control on the density of sorption sites and thus in principle they could also be fine tuned to reproduce the observed bulk retention capacity of the rock matrix acknowledgements us ml and pt thank the swedish nuclear fuel and waste management company skb for the financial support this work has benefited from insightful comments by paul sardini and two anonymous reviewers appendix a overview of the dfn generation geometrically a random fracture is a rectangular parallelepiped two of whose three edge lengths are equal we refer to the two equal edge lengths as the length l of the fracture and to the third edge length as the thickness b the centre of the fracture is the geometrical central point of the parallelepiped and the axis is the straight line through the centre which is parallel to the thickness edges all fractures in the fracture set have the same thickness b the lengths l are restricted to the interval l min l max the centre locations are restricted to a region of space called the generation domain the lengths and centre locations of the fractures are calculated in a random process where the number of fractures comes out as a by product the process also has the parameters l ref p and i ref l ref is specified in m p is dimensionless and i ref is specified in m 3 the process has the following stochastic properties consider an infinitesimal length interval l l dl belonging to the interval l min l max also consider an infinitesimal volume element belonging to the generation domain and let dv be the volume of the element let dn denote the number of generated fractures whose lengths belong to l l dl and whose centres belong to the volume element the expected value of dn will then be given by the following power law a 1 e dn i ref l l ref p 1 dl l ref dv the numbers of fractures generated for disjunct pairs of infinitesimal length intervals and volume elements are stochastically independent two pairs are considered disjunct if the length intervals of the pairs are disjunct or the volume elements of the pairs are disjunct from these properties it is possible to conclude that the total number n of fractures generated for the entire length interval l min l max and the entire generation domain will have a poisson distribution with expected value a 2 e n i ref p l max l ref p l min l ref p v if p 0 i ref ln l max l min v if p 0 where v is the volume of the generation domain 
7379,wetlands are important ecosystems that provide many ecological benefits and their quality and presence are protected by federal regulations these regulations require wetland delineations which can be costly and time consuming to perform computer models can assist in this process but lack the accuracy necessary for environmental planning scale wetland identification in this study the potential for improvement of wetland identification models through modification of digital elevation model dem derivatives derived from high resolution and increasingly available light detection and ranging lidar data at a scale necessary for small scale wetland delineations is evaluated a novel approach of flow convergence modelling is presented where topographic wetness index twi curvature and cartographic depth to water index dtw are modified to better distinguish wetland from upland areas combined with ancillary soil data and used in a random forest classification this approach is applied to four study sites in virginia implemented as an arcgis model the model resulted in significant improvement in average wetland accuracy compared to the commonly used national wetland inventory 84 9 vs 32 1 at the expense of a moderately lower average non wetland accuracy 85 6 vs 98 0 and average overall accuracy 85 6 vs 92 0 from this we concluded that modifying twi curvature and dtw provides more robust wetland and non wetland signatures to the models by improving accuracy rates compared to classifications using the original indices the resulting arcgis model is a general tool able to modify these local lidar dem derivatives based on site characteristics to identify wetlands at a high resolution keywords wetlands lidar topographic indices random forest 1 introduction wetlands are important ecosystems that not only provide habitat for many plant and animal species but also improve water quality recharge groundwater and ease flood and drought severity guo et al 2017 despite the ecological value of wetlands their quality and presence are threatened by agricultural or development repurposing pollutant runoff and climate change klemas 2011 current estimates are that approximately 50 of wetlands have been lost globally since 1900 davidson 2014 and approximately 53 of wetlands of the conterminous u s have been lost since the early 1600s dahl et al 1991 the historic loss of wetlands and sustained threat to remaining wetlands have motivated increased efforts by scientists and government to protect and maintain these ecosystems u s federal regulations play an important role in the abatement of further wetland loss one of the most important policies in support of this effort is section 404 of the clean water act which protects the nation s waters including wetlands according to page and wilcher 1990 this law states that environmental planning entities must identify and assess environmental impact due to land development and water resources projects this requires environmental planning entities such as state departments of transportation dots to provide wetland delineations that are ultimately jurisdictionally confirmed by the u s army corps of engineers usace the usace wetlands delineation manual states that wetlands can be identified by environmental characteristics shared among the many wetland types the usace guidelines for wetland delineations use these common features and are based on the presence of hydrologic conditions that inundate the area vegetation adapted for life in saturated soil conditions and hydric soils environmental laboratory 1987 manual surveying by trained analysts will always be the most accurate method to delineate wetlands however carrying out detailed field surveys can be time consuming and costly according to estimates provided by representatives from the virginia dot vdot environmental division the costs of these delineations range from 60 to 140 per acre 0 4 ha personal communication november 28 2017 these estimates are based on recent vdot projects and can vary widely across projects to offset these costs the wetland permitting process could potentially be streamlined by supplementing and guiding the manual delineations with accurate digital wetland inventories however developing and updating wetland inventories can be expensive and technically challenging due to the complexity of wetland features kloiber et al 2015 furthermore the existing national scale wetland inventory in the u s the national wetland inventory nwi is not ideal for assisting in the permitting process despite being one of the most commonly used sources of wetland data in the u s nwi maps were never intended to map federally regulated wetlands cowardin and golet 1995 environmental laboratory 1987 and research has shown that relying solely on the nwi may fail to protect a considerable fraction of wetlands morrissey and sweeney 2006 thus a wetland inventory with the reliability necessary to assist in the wetland permitting process is an unmet need remote sensing has long been recognized as a powerful tool for identifying wetlands environmental laboratory 1987 and may offer an accurate and cost effective way to fulfill this need guo et al 2017 lang et al 2013 lang and mccarty 2014 past studies have incorporated remote sensing data such as multispectral imagery radar and light detection and ranging lidar for wetland identification a review of wetland remote sensing studies of the past 50 years shows that most researchers incorporate multispectral imagery in wetland classifications guo et al 2017 however the incorporation of multispectral imagery can weaken the potential for use during the wetland permitting process by introducing issues of resolution or accessibility for example the commonly used landsat multispectral imagery is freely available on a national scale but the 30 m resolution of this data can be too coarse to detect wetlands at a scale relevant to environmental planning entities which can require a spatial accuracy of at least 1 5 m vdot environmental division personal communication november 28 2017 while studies have shown higher resolution multispectral data can result in accurate wetland classifications e g kloiber et al 2015 these data can be inaccessible due to cost alternatively lidar is a remote sensing data that has been rapidly endorsed by the wetland science and management community for its growing availability and technological benefit to wetland mapping kloiber et al 2015 lang and mccarty 2014 lidar sensors provide detailed information on the earth s landscape and bare surface by collecting x y and z data that can be interpolated to create digital elevation models dems lang and mccarty 2014 lidar dem availability has increased rapidly over the past 20 years and although current coverage in the conterminous u s is at about one third there is an ongoing effort by multiple federal agencies to hasten the collection of lidar throughout the entire u s snyder and lang 2012 while conventional dems and their derivatives have been shown to be useful for wetland delineation e g hogg and todd 2007 lidar dems allow for more detailed mapping of topographic metrics that describe flow convergence lang and mccarty 2014 previous research has shown that dem derivatives have the potential to model spatial patterns of saturated areas and that lidar dem derivatives improve the ability of these metrics to do so e g hogg and todd 2007 lang et al 2013 millard and richardson 2013 among the dem derivatives found to be useful for this purpose are curvature topographic wetness index twi and the cartographic depth to water index dtw e g ågren et al 2014 lang et al 2013 murphy et al 2009 2011 sangireddy et al 2016 curvature is defined as the second derivative of the elevation surface and can describe the degree of convergence and acceleration of flow moore et al 1991 the twi developed by beven and kirkby 1979 relates the tendency of a site to receive water to the tendency of a site to evacuate water and is defined as 1 twi ln α tan β where α is the specific catchment area or contributing area per unit contour length and tan β is the local slope the dtw is a soil moisture index developed by murphy et al 2007 that is based on the assumption that soils very close in elevation to the nearest surface water are more likely to be saturated the dtw model is calculated in grid form as 2 dtw m dz i dx i a x c where dz dx is the downward slope of a pixel i is a pixel along a calculated least cost i e slope path to the assigned source i e surface water pixel a is 1 when the flow path is parallel to pixel boundaries or 2 when the flow crosses diagonally and x c is the pixel length murphy et al 2007 although many studies have shown the benefit of using topographic indices to identify wetted areas and the added benefit of deriving these indices at higher resolutions there are also unique challenges inherent to using lidar dems researchers have noted that lidar dems used for modelling landform characteristics must be resampled to coarser resolutions and smoothed to overcome issues of increased noise from excessive topographic detail macmillan et al 2003 with this topographic noise arising from dems on the order of 1 m pixel size richardson et al 2009 moreover variations in dem resolution result in significantly different spatial and statistical distributions of contributing areas and downslope flow path lengths woodrow et al 2016 and at high resolutions micro topographic features can lead to highly variable slope values and provide unrealistic estimates of hydraulic gradients grabs et al 2009 lanni et al 2011 previous studies have acknowledged the negative effect that these micro topographic features have on the ability of curvature e g sangireddy et al 2016 and twi e g sørensen and seibert 2007 to identify hydrologic features of interest for example ågren et al 2014 found that high resolution dems 2 m caused local twi variations that are too strong to separate wetlands from uplands whereas deriving the index from coarser dems 24 m reduced these variations but resulted in poorly delineated flow channels and local depressions in contrast the researchers also concluded that dtw derivations were not sensitive to scale but suggested that the dtw could be further optimized ågren et al 2014 lidar dem data and other remote sensing data are commonly used to map wetlands through supervised classification algorithms random forest rf classification is a relatively new supervised classification method that is widely used for its ability to handle both continuous and categorical high dimensional data and its generation of descriptive variable importance measures millard and richardson 2015 rodriguez galiano et al 2012 rf has been shown to produce higher accuracies than other classification techniques such as maximum likelihood when incorporating multisource data duro et al 2012 miao et al 2012 rodriguez galiano et al 2012 furthermore studies have shown that lidar dem metrics are suitable input variables for the rf approach e g deng et al 2017 kloiber et al 2015 zhu and pierskalla 2016 and that using this classifier has strong potential to improve mapping and imagery classification of wetlands e g millard and richardson 2013 many studies have relied primarily on ecological factors and spectral indices provided by multispectral imagery to classify wetlands and fewer studies have evaluated the predictive power of lidar dem data alone for this purpose the primary objective of this study was to further advance the application of lidar dem derivatives to wetland mapping by evaluating the potential of modified twi curvature and dtw grids to address limitations noted by researchers and identify small i e environmental planning scale wetlands across varying ecoregions rf classifications of original and modified indices where the twi and curvature were modified via smoothing and the dtw was modified via adjustments to the input slope grid along with ancillary national scale soil data were assessed against field mapped testing data and compared to nwi maps to identify the best performing models accuracy assessments of these classifications provided a measure of the benefits of modifying these input data this approach was applied to four study sites across varying ecoregions of virginia and implemented in arcgis and has the potential for further refinement and utility by environmental planning entities 2 study areas the four sites in this study were selected due to availability of vdot wetland delineations and lidar dems and to have applications of this approach across varying ecoregions of virginia as seen in fig 1 the study sites span five of the seven level iii epa ecoregions of virginia the piedmont 45 the mid atlantic coastal plain 63 the northern piedmont 64 the southeastern plains 65 and the ridge and valley 67 according to the epa 2013 the piedmont ecoregion is considered the non mountainous region of the appalachian highlands comprising of transitional areas between the mountainous appalachians to the northwest and the relatively flat coastal plain to the southeast the soils in this region tend to be finer textured than in ecoregions 63 and 65 the mid atlantic coastal plain is characterized by low nearly flat plains with many swamps marshes and estuaries this region has a mix of coarse and finer textured soils and poorly drained soils are common here the northern piedmont consists of low rounded hills irregular plains and open valleys it is a transitional region between the low mountains in ecoregion 66 and the flat coastal area of ecoregions 63 and 65 the southeastern plains are irregular and are comprised of a mosaic of cropland pasture woodland and forest the subsurface is predominantly sands silts and clays the ridge and valley ecoregion is relatively low lying and characterized by alternating forested ridges and agricultural valleys additional information describing the conditions of each study site can be found in table 1 3 input data freely available lidar elevation data land cover data national scale hydrography data national scale soil data and vdot wetland delineations were used as inputs to the wetland identification model 3 1 lidar elevation data lidar derived elevation data used in this study were provided by the virginia information technologies agency vita in raster format http vgin maps arcgis com vita lidar data were freely available and included hydro flattened bare earth dems the lidar dems used in this study were collected and processed between 2010 and 2015 and have horizontal resolutions ranging from 0 76 m to 1 5 m dem tiles of different resolutions were merged and resampled to the coarsest resolution for each site using the bilinear resampling method in arcgis following the approach previously done by ågren et al 2014 site 2 was unique in that lidar data were unavailable for approximately 230 km2 23 of the processing extent and 0 8 km2 12 of the vdot delineation area to fill the missing areas 3 m data from the national elevation dataset were used https viewer nationalmap gov and resampled to 1 5 m to match the more abundant lidar data while resampling to finer resolutions is not ideal maintaining consistency in the application of the highest resolution lidar data across all study sites was prioritized over the error introduced in the relatively small portion of the processing extent and even smaller portion of the delineation area 3 2 land cover data land cover data were used for post classification filtering land cover data used in this study were provided by vita in raster format http vgin maps arcgis com vita land cover data were derived from the virginia base mapping program 4 band orthophotography collected between 2011 and 2014 these data provided 12 land cover classifications with 85 95 accuracy and have a horizontal resolution of 1 m worldview solutions inc 2016 3 3 national scale data national scale soil and hydrography data were incorporated in the classification as ancillary data soil data used in this study were obtained from the soil survey geographic database ssurgo and distributed by the natural resources conservation service s web soil survey in polygon vector format https websoilsurvey sc egov usda gov the ssurgo hydric rating depth to water table hydrologic soil group surface texture and soil drainage class were used as indicators of saturated conditions according to the soil survey staff 2017 the hydric rating attribute indicates the percentage of a map unit that meets the criteria for hydric soils hydric soils are characteristic of wetlands defined as soil that is formed under conditions of saturation flooding or ponding long enough during the growing season to develop anaerobic conditions in the upper horizon federal register 1994 the surface texture attribute describes the representative texture class according to percentage of sand silt and clay in the fraction of the soil that is less than 2 mm in diameter the soil drainage class attribute identifies the natural drainage conditions of the soil and refers to the frequency of wet periods without considering alterations of the water regime by human activities unless they have significantly changed the morphology of the soil the hydrologic soil group assignment is based on estimates of the rate of water infiltration when the soils are not protected by vegetation are thoroughly wet and receive precipitation from long duration storms the depth to water table attribute indicates the representative depth to the saturated zone in the soil hydrography data used in this study were provided by the national hydrography dataset nhd in polygon vector format https viewer nationalmap gov nhd huc 12 watersheds intersected by the limits of vdot delineations were combined to be used as the processing extent for each study site in order to encompass the hydrologically connected area nhd streams and waterbodies within processing extents were also used 3 4 vdot wetland delineations wetland delineations for each site were provided by vdot and were used to create training and testing datasets the vdot delineations in site 2 site 3 and site 4 were jurisdictionally confirmed by the usace and all study sites were produced through field surveys conducted by professional wetland scientists for these reasons the vdot delineations were considered to be ground truth for the purpose of training and testing the wetland identification model vdot delineations were provided in polygon vector format and included both wetlands and streambeds both were included in subsequent processing because both are considered waters of the state and therefore must be delineated during the wetland permitting process although the delineations were categorized by wetland type by vdot analysts all areas were merged into a single wetland category before application in this study additionally limits of delineations were used to identify true non wetland areas 4 methods the workflow followed to implement the wetland identification approach consisted of three main parts preprocessing supervised classification and post processing fig 2 the workflow was implemented in arcgis 10 4 and the modelbuilder tool was used to automate processes that did not require user intervention outputs of the workflow were model predictions and confusion matrices used to assess the accuracy of predictions components of the workflow are described in more detail in the following sections 4 1 preprocessing the preprocessing phase consisted of a combination of automated and semi automated processes which required user intervention preprocessing steps not explicitly shown in fig 2 included projection of input data to the appropriate north or south virginia state plane coordinate system clipping data to the huc 12 processing extent rasterizing input data originally in polygon vector format by using the site lidar dem as the pixel size constraint and filling sinks within the lidar dem rasterizing the polygon vector layers mapped at coarser scales assumes that the information provided at the original scale ranging from 1 12 000 to 1 24 000 is true for each pixel of the output grid ranging from 0 76 to 1 52 m the lidar dem was filled using the depression filling algorithm of planchon and darboux 2002 that is implemented in arcgis intermediate outputs created by the preprocessing phase were calibrated input variables training data and testing data 4 1 1 input variable creation input variables included the modified twi modified curvature modified dtw and selected soil thematic maps input variables were modified based on site characteristics and information provided by vdot delineations in order to produce distinct wetland and non wetland signatures and user intervention was necessary to execute some of the processes table 2 summarizes the modification parameters for topographic indices as well as information relevant to the calculation of those parameters italicized and selected soil thematic maps for each study site the methods used to create input variables are described in the following sections 4 1 1 1 twi modifications the modified twi grid is based on the twi as defined in eq 1 the twi was created in arcgis as a map algebra expression the inputs required for this calculation were a flow accumulation grid to represent the α term and a slope grid to represent the tan β term both derived from the filled lidar dem the d8 method jenson and domingue 1988 was used to generate flow direction and flow accumulation grids a slope grid was generated with the arcgis slope tool calculated as the steepest downhill descent from each pixel in units of m m burrough and mcdonell 1998 a constant equal to 1 was added to flow accumulation grids so that every pixel received flow from itself as well as upslope pixels to avoid undefined twi values and a constant equal to 0 0001 m m was added to slope grids to avoid dividing by zero an example of the resulting twi grid overlaid with vdot wetland areas for a portion of site 1 is shown in fig 4 panel a1 this twi grid models the presence of wetter areas high twi values in locations of high flow accumulation and flat slopes and drier areas low twi values in locations of low flow accumulation and steep slopes larger clusters of relatively high twi values align with the vdot delineated wetlands however there is also a scattering of high twi values outside of these wetland boundaries corroborating the challenges of high resolution twis previously described in the literature e g ågren et al 2014 sørensen and seibert 2007 although some researchers recommend deriving twis from coarser dems e g ågren et al 2014 doing so would sacrifice the rich detail provided by lidar dems that may be needed to precisely model shape and size of environmental planning scale wetlands although these scatterings of relatively high twi values may be modelling true micro topographic features their location outside of the field mapped wetlands suggests these flow channels are not large enough to result in saturated conditions rather than lose hydrologic detail by resampling the lidar data anomalous local variations were smoothed by applying a low pass filter over a moving nxn window to create the modified twi this low pass filter searches over a user defined window in which every pixel is replaced with the statistical value from the surrounding nxn pixels as done by ali et al 2014 buchanan et al 2014 and lanni et al 2011 the window size for the smoothing operation is significant in that it is usually set with consideration of the average size of the feature of interest sangireddy et al 2016 in this study we estimated that areas of interest must be at least 5 m in width based on the size of vdot delineated wetlands therefore window sizes were set to smooth over a total area of approximately 25 m2 5 m 5 m with this window size varying slightly across study sites depending on pixel length of the lidar data additionally a median filter was chosen to perform smoothing rather than a mean filter visual assessment of both statistic types showed that the median filter better retained clustered high twi values aligned with vdot wetland edges while removing scattered high twi values outside of these boundaries twi smoothing was implemented in the arcgis model using the focal statistics tool window sizes used to calculate the modified twi grid for each site are shown in table 2 and an example of applying this modification for a portion of site 1 is shown in fig 4 panel a2 compared to the unmodified twi panel a1 this scene shows the cluster of relatively high twi values within vdot delineated wetlands was maintained but the discrete small flow channels outside of the true wetland boundaries have been smoothed via replacement of these pixels with relatively lower twi values 4 1 1 2 curvature modifications curvature grids as defined by moore et al 1991 were created from the filled lidar dem using the arcgis curvature tool curvature has been shown to be a key component in the process of identifying likely channelized pixels indicating flow convergence ågren et al 2014 hogg and todd 2007 kloiber et al 2015 millard and richardson 2013 sangireddy et al 2016 it was anticipated that the high resolution of the lidar derived curvature grids would assist in separating small differences in concavity between nearly flat roadways and shallow local depressions however visual assessment of the lidar derived curvature grids showed a similar issue of topographic noise as seen in the twi in that micro topographic channels were also mapped an example of the output curvature grid for a portion of site 1 is shown in fig 4 panel b1 this image shows negative and zero curvature values within vdot wetland extents which correspond to concave and flat areas respectively similar to modified twi creation the curvature was modified by applying a statistical smoothing process to curvature grids similar to the approach of sangireddy et al 2016 when choosing the window size for this calculation the assumption of the average size of features of interest was kept consistent with that of the twi i e at least 5 m in width in this case a mean filter was chosen to smooth the curvature data rather than a median filter due to a visual improvement in vdot wetland edge retention resulting from the mean smoothing the modified curvature grid was created by applying the arcgis focal statistics tool window sizes used to calculate the modified curvature grid for each site are shown in table 2 and an example of this modification for a portion of site 1 is shown in fig 4 panel b2 in this image one can see that the modified curvature grid has a smoother appearance but maintains significant areas of concavity 4 1 1 3 dtw modifications the modified dtw grid is based on the dtw as defined in eq 2 this iterative function finds the cumulative slope value along the least downward slope i e cost path to the nearest surface water i e source pixel with which it is most likely to be hydrologically connected murphy et al 2009 to calculate the dtw two input grids are required a grid of slope values and a grid of areas of open water murphy et al 2009 in this study slope grids were derived from the filled lidar dem using the arcgis slope function as done in the original formulation of the dtw model e g murphy et al 2007 2009 2011 and the source grids were created from rasterized nhd waterbodies and streams while the publicly available nhd was chosen in this study to maintain consistency between the four sites there are alternatives for researchers without publicly available open water data the source grid can also be generated directly from elevation data by deriving streams based on a flow accumulation threshold murphy et al 2009 or by using channel extraction software such as geonet sangireddy et al 2016 the effects and limitations of using the relatively coarsely mapped nhd as the source grid for the dtw are discussed in section 5 2 of this paper the arcgis cost distance tool was used to evaluate eq 2 within the model using the slope and nhd source grids as inputs it was also necessary to add a small constant 0 0001 m m to the slope grid to differentiate from source grid pixels which are assigned a value of zero for the calculation an example of the resulting dtw grid for a portion of site 1 is shown in panel c1 of fig 4 as expected low wetness high dtw values occurred in areas further and higher along the terrain from surface water and high wetness low dtw values occurred in areas of low slopes that are closer to surface water while wetted areas calculated by the dtw correspond to vdot delineated wetlands the transition from wet to drier areas is gradual we found this to result in lower non wetland accuracy or an overestimation of wetlands when using only the original dtw formulation to identify wetland areas therefore a modified dtw was created to accelerate the gradual transition from wetlands to uplands in an effort to better distinguish wet from dry locations the method outlined above was used to calculate the modified dtw except that the input slope grid was replaced with an adjusted slope grid defined as 3 y γ x β where x is the slope with a small constant added as described earlier and γ and β are calculated slope adjustment parameters this adjustment to the slope values was intended to create two distinct ranges of low cost areas where wetlands are likely to exist and high cost areas where wetlands are unlikely to exist based on the observed distribution of wetland slope values in each site the γ parameter allows users to control the cutoff between the low and high cost slope values which correspond to a designated representative wetland slope value the β parameter allows users to control the rate of increase in cost as the slopes increase throughout the site in this study β was set to a value of 2 for all sites while γ was individually calculated we hypothesized that setting the wetland slope value equal to the 95th percentile of all underlying vdot wetland slope values would result in a γ parameter that further flattens the terrain i e reduces the cost where most wetlands exist disregarding assumed outliers while steepening the terrain i e increasing the cost elsewhere representative slope values were calculated by extracting slope values within vdot wetland boundaries and calculating the 95th percentile of each array with the numpy python library fig 3 shows an example of this adjusted slope calculation and describes the effect of this adjustment for site 1 where the 95th percentile was 0 088 m m which corresponded to a γ value of 11 42 with the adjustments to the slope grid applied eq 2 becomes 4 modified dtw m γ dz i dx i 2 a x c where γ and β 2 are introduced slope adjustment parameters and relevant site characteristics used to calculate the parameters are shown for each site in table 2 an example of the effect of modifying the dtw in site 1 using this calculation is shown in panel c2 of fig 4 in this figure the modified dtw shows relatively wetter areas within vdot wetland boundaries and an accelerated increase to drier values moving away from vdot wetlands compared to the original dtw c1 4 1 1 4 soil thematic maps the final input variables created in the preprocessing phase were soil thematic maps soil thematic maps were created from the extensive ssurgo database using the soil data viewer arcmap extension nrcs 2015 although the soil data viewer creates soil thematic maps automatically combinations of soil layers were manually chosen for each site based on correspondence of the soil data to the current physical landscape this correspondence was assessed by visual comparison to vdot delineations and vita land cover data soil layers that appeared too coarse i e generally did not vary enough within the vdot delineated area to describe features of interest were not selected 4 1 2 training and testing data an automated process was used to randomly designate 10 of vdot delineation area to train the classifier and reserve the remaining 90 to test model results it has been noted that statistical classifiers and machine learning algorithms may be sensitive to imbalanced training data or cases where rare classes are being classified such as most cases of wetland identification and the sensitivity of rf specifically to training class proportions was investigated by millard and richardson 2015 the researchers found that when training samples were disproportionately higher or lower than the true distribution of that feature the final classification over or under predicted that class respectively they concluded that using a sampling strategy that ensures representative class proportions and minimal spatial autocorrelation minimized proportion error in their results millard and richardson 2015 in this study we took into account the findings of millard and richardson 2015 when designing the methodology to randomly separate vdot delineations into training and testing data this process consisted of four steps random point creation point buffering value extraction and training data separation fig 5 a stratified random sampling method was used in the first step to distribute a designated number of training sample locations proportionately between wetland and non wetland areas panel a these randomly generated points were then buffered to create circle polygons with an area of approximately 100 m2 each panel b in the value extraction step panel c training data composed of approximately 10 of the delineated area and with representative class proportions were produced by rasterizing the buffered polygons with pixel values extracted from vdot delineations this corrected cases where buffered polygons encompassed both wetland and non wetland classes the testing data were created by separating the training data from the vdot delineations leaving approximately 90 of the delineated area to be used for accuracy assessment panel d statistics describing the training and testing datasets for each site are found in table 3 4 2 supervised classification in the first phase of the supervised classification portion of the workflow the input variables created during preprocessing were combined into a multidimensional composite image where each dimension stores an independent input variable wetland and non wetland signatures were extracted from this composite image and used to perform the supervised classification rf classification was chosen as the classification algorithm for its noted advantages in similar studies as previously described e g duro et al 2012 miao et al 2012 millard and richardson 2013 rodriguez galiano et al 2012 according to breiman 2001 rf is an ensemble classifier that produces many classification and regression like trees where each tree is generated from different bootstrapped samples of training data and input variables are randomly selected for generating trees this algorithm also produces variable importance information which measures the mean decrease in accuracy when a variable is not used in generating a tree the rf classification was executed in arcgis with the train random trees and classify raster tools esri 2016 the train random trees tool utilizes the opencv implementation of the rf algorithm bradski 2000 using train random trees the training data were used to extract class signatures from the dimensions i e input variables of the composite image creating an esri classifier definition file with variable importance measures the classifier definition file was subsequently used to classify the remainder of the composite image the result of these operations is a grid where each pixel has been classified as wetland or non wetland as the focus of this study was to analyze the response of classification models to input data the rf parameters were not varied or calibrated to study sites for this reason the default values of maximum number of trees maximum tree depth and maximum numbers of samples per class were held constant at the recommended default values of 50 30 and 1000 respectively future work should perform a sensitivity analysis to test the effect of adjusting these parameters 4 3 post processing the first phase of post processing was post classification filtering the objective of the post classification filtering was to account for areas that may be susceptible to water accumulation due to its local topography but cannot be wetland areas due to impervious land cover the post classification filtering algorithm first used a logical statement to determine if a classified wetland pixel overlaps vita land cover designated as impervious if this was false the pixel classification was unchanged if this was true a second logical statement was used to account for cases where wetlands may exist under bridges by determining if classified wetland pixels are within 30 m of nhd streams the 30 m buffer distance was an estimated value based on visual inspection and more precise measurements would increase effectiveness of post classification filtering if this second statement was false the pixel was reclassified as non wetland otherwise it was unchanged this process produced the model predictions the second phase of post processing was accuracy assessment the model predictions and nwi map for the study area were assessed for accuracy in terms of agreement with the test dataset accuracy assessments were evaluated with confusion matrices which summarized pixels as areas of wetland agreement non wetland agreement false negative predictions cases where true wetland areas were predicted to be non wetland or false positive predictions cases where true non wetland areas were predicted to be wetland confusion matrices were used to calculate wetland accuracy non wetland accuracy and overall accuracy using eqs 5 7 5 wetland accuracy wetland agreement k m 2 test actual wetland k m 2 6 nonwetland accuracy nonwetland agreement k m 2 test actual nonwetland k m 2 7 overall accuracy wetland agreement k m 2 nonwetland agreement k m 2 test area k m 2 the use of these metrics to assess wetland classifications is common in literature e g ågren et al 2014 millard and richardson 2013 5 results and discussion 5 1 highest performing models to determine the highest performing models classifications varying only topographic inputs were first performed and assessed and the input data that resulted in highest overall accuracy were combined with relevant soil layers if any in the coming sections the following results are discussed 1 scenes for each site comparing highest performing models and their level of agreement with vdot delineations compared to nwi maps 2 variable importance of highest performing input data and 3 the accuracy assessment of highest performing models compared to the nwi the input data used to produce the best performing models and the importance of these inputs according to the esri classifier definition file are listed in table 4 although accuracy assessments for each site only extend to testing dataset limits scenes depicting predictions and vdot delineations prior to the separation process are shown for clarity 5 1 1 site 1 results wetland predictions and nwi data for site 1 are shown in fig 6 both of the nwi scenes a1 and b1 exemplify the tendency of the nwi to underestimate the size of vdot delineated wetlands by mapping wetlands primarily along streams while the narrow nwi wetlands precisely map the wetland areas that are in agreement with vdot delineations the nwi fails to match the contours or the size of larger wetland zones these larger wetland zones were more fully mapped by wetland predictions produced by the model a2 and b2 however the model also produced relatively higher overestimation of wetlands overestimation of wetlands is especially prevalent in location 1 underlying input variables indicated that overestimation here was due to a depression that was filled to become a large zero slope area this flat zone resulted in a corresponding generalized area of high wetness values in the modified twi and modified dtw in addition the surface texture input indicated that silty clay loam which have relatively slow infiltration rates 0 5 cm h soil survey staff 2017 was also present in this overestimated area likely contributing to the wetland predictions here it is possible that the results in this site could be improved by using an alternative to the pit filling i e arcgis fill algorithm to avoid creation of generalized flat areas more severe adjustments to the slope grid for the modified dtw or higher resolution soil data panel b2 shows more precise model wetland predictions represented by conformity of predicted wetlands to the curvature of vdot delineated wetlands this panel encompasses the scene in fig 4 c2 where the modification to the dtw was shown to more precisely map wetland areas for that reason we attribute the relatively precise mapping of wetlands in b2 in part to the modifications used for the dtw location 2 shows one small wetland that was undetected by the model this may indicate a wetland formed due to conditions more strongly driven by vegetation rather than topography or proximity to surface water 5 1 2 site 2 results two scenes of the model predictions and nwi maps for site 2 are shown in fig 7 in panels a1 and a2 the nwi dataset and model predictions both show similar overestimation of wetland area although the model resulted in higher overestimation the false positive predictions in this area were due to flow convergence indicated by the topographic inputs and the presence of hydric soils indicated by the ssurgo data also many false positive predictions in this site were in locations overlapping road features e g location 1 this may indicate a need for alternative modifications to topographic inputs especially curvature to better differentiate channelized areas due to road features from channelized areas that are wetlands panel b1 shows another example of nwi wetland delineations following along streams but failing to capture the extents of larger wetland zones for this same area the model predicted wetlands further from the streambeds due to the gradual slopes surrounding them and better encompassed vdot delineated wetlands locations 2 and 3 5 1 3 site 3 results examples of model predictions and nwi data for site 3 are shown in fig 8 as seen in table 4 site 3 was unique in that no soil layers were included in the best performing model visual assessment of relevant soil layers in this area showed that the ssurgo data did not vary in a way that effectively differentiated between features of interest site 3 was also unique for its wetlands which were typically narrow and located along small flow channels rather than in larger wetland zones the nwi data shown either do not conform to the bends along the length of wetlands a1 or failed to map a number of wetlands in these channelized areas b1 the model predicted a larger portion of the vdot delineated wetlands in both scenes however the wetland predictions often extended too far on either side of the narrow wetlands a2 location 1 shows another example of a local depression filled to become a generalized flat area resulting in an overestimation due to the modified twi and modified dtw additionally both scenes a2 and b2 show that the model detected road edges and road medians as wetland areas this is a shortcoming of the model that was observed in other sites such as site 2 and indicates a need for further modification to topographic indices 5 1 4 site 4 results fig 9 shows three scenes from the nwi maps and model predictions for site 4 which was the largest site studied site 4 was also unique for having the largest distribution of vdot delineated wetlands covering more than 40 of the surveyed area as well as the mildest average slope see table 1 nwi maps underestimated a large portion of vdot delineated wetlands and the portions of these wetlands that were mapped were delineated with less precision than typically seen by the nwi e g location 2 the model predictions also resulted in a large number of false negative predictions and imprecise wetland delineations the well defined contours of model predictions e g locations 1 3 and 4 exemplify the heavy reliance of the model on soil thematic layers in these scenes the primary drivers for wetland prediction were the presence of hydric soils and shallow depth to water table which both outlined the same contours as these wetland predictions the relatively lower reliance on topographic indices in this site is likely due to the unchanging topography of the area which is characteristic of the mid atlantic coastal plain as there was often little to no flow convergence indicated by the topographic indices where vdot delineated wetlands were mapped it is possible that alternative filtering techniques or more severe adjustments to the slope grid could increase the effectiveness of topographic indices to detect wetted areas however the correspondence of the model to the soil layers and the relatively high occurrence of false negative predictions imply that vegetation data would also be valuable in this region 5 1 5 variable importance an important output from the rf classification was the esri classifier definition file which provided the variable importance of each input used in classifications see table 4 variable importance measures were used to gauge the ability of input variables to provide unique significant information to the classifier table 4 shows that in site 1 site 3 and site 4 the modified dtw was the most important topographic index and in site 2 the original dtw was the most important topographic index in contrast the modified twi was the overall least important input variable in every study site the low ranking of the modified twi relative to the modified and original dtw suggests that some information was duplicated by these inputs but that the modified dtw provided more robust wetland and non wetland signatures this corresponds to the findings of previous studies e g ågren et al 2014 murphy et al 2009 which stated that wet twi values were restricted to discrete lines of flow accumulation within wetted areas whereas the dtw model effectively encompassed wetted areas as a whole and was therefore more robust for this same reason it was unexpected that for site 3 the modified dtw ranked higher than the modified twi as the vdot delineated wetlands here were primarily restricted to narrow lines of flow accumulation soil data were among the most important variables in all sites that included them in site 1 and site 2 this is likely due to the abundance of road features and the ability of the soil information to better distinguish these from wetlands relative to the topographic indices which were observed to detect water accumulation along roads the higher importance of soil layers in site 4 is likely due to the flat terrain and is in line with the wetland predictions seen in fig 9 which were dictated primarily by areas of hydric soil and shallow depth to water table the low importance of the topographic indices in site 4 also reinforces the claim that topographic indices that are static and assume the local slope is an adequate proxy for subsurface flow patterns such as the twi and dtw are less suitable in flat areas due to undefined flow directions that are likely to change over time grabs et al 2009 the lower importance of modified curvature relative to dtw inputs in all sites may indicate that our application of the curvature was limited by the arcgis fill operation and filtering technique i e mean smoothing which generalized potentially significant terrain features since curvature has been shown to strongly determine flow convergence in flat topography sangireddy et al 2016 5 1 6 accuracy assessment model accuracy was assessed using the testing data and compared to the accuracy achieved by the nwi maps table 5 shows the confusion matrices produced for the best performing models and the nwi maps across all study sites in each confusion matrix test data are represented along columns and nwi and model predictions are represented along rows categorized pixels expressed as total km2 in table 5 were used to calculate wetland accuracy non wetland accuracy and overall accuracy using eqs 5 7 it is important to note that the accuracy assessment only extended to the limits of the testing data which as previously described are randomly selected subsets of the original vdot delineations and the effect of varying testing and training data separation on model accuracy was not assessed fig 10 summarizes the accuracy achieved by the best performing model predictions and nwi maps in the context of the wetland permitting process it is important to have high values for all accuracy metrics to uphold the objective of protecting existing wetlands wetland accuracy is of high importance and in order to provide realistic estimates of potentially impacted wetland areas in transportation and environmental planning high non wetland accuracy is also necessary however it is important to be aware of the potential for overall accuracy which measures the portion of the entire area that is correctly classified regardless of class to be misleading due to the uneven distribution of landscape classes for example the consistently conservative wetland mapping by the nwi is reflected by the high average non wetland accuracy 98 0 due to the uneven distribution of wetland and non wetland classes in all but one of the study sites the conservative nature of the nwi predictions also translated into high average overall accuracy 92 0 despite an average wetland accuracy of 32 1 in contrast the model predictions resulted in significantly higher average wetland accuracy 84 9 but at the expense of moderately lower average non wetland and overall accuracy 85 6 and 85 6 respectively as previously discussed site 4 was the lowest performing site the low wetland accuracy here may be due to a lack of vegetative signatures to distinguish wetland from upland area especially in this excessively flat area where terrain indices were found to be less important 5 2 response of model to input data modification iteration results in terms of wetland non wetland and overall accuracy highlight the benefit and cost of applying the modifications described here as well as including the coarser mapped 1 12 000 1 24 000 ssurgo data results of the analysis of model responses to classification iterations are shown in table 6 where the highest performing iteration per accuracy metric not including iteration 5 which built off of top performing topographic inputs is indicated with a superscript and modified topographic inputs are indicated with an asterisk shown in table 6 non wetland accuracy and overall classification accuracy from iteration 1 where the original versions of all indices were used improved in every site as a result of modifying all topographic indices iteration 2 in addition for three of the four sites modifying all topographic indices resulted in the highest overall accuracy these results suggest there is a benefit to applying the modifications presented here rather than using the indices as they are traditionally calculated where this benefit is a reduction in false positive predictions and increase in overall accuracy furthermore in every site that relevant soil layers were applicable the inclusion of these soil layers with top performing topographic indices i e iteration 5 further improved the rf classification from this we conclude that in these sites the soil data provided important information to the classifier despite its relatively coarse scale both site 2 and site 4 saw relatively high increases in wetland accuracy resulting from iteration 5 which suggests the topographic indices were not effective in encompassing flow convergence or subsurface moisture conditions in order to detect wetlands iterations 3 and 4 were performed to determine the effect of individual modifications on the classification note that for this evaluation modified twi and modified curvature were generalized into a single category of modifications because of their similar adjustment parameters and methods the purpose of modifying topographic indices was largely to reduce false positive predictions in that twi and curvature grids were modified to reduce unrealistic flow convergence due to excess topographic detail and the dtw was modified to accelerate the transition from wetland to upland areas results in table 6 show that the effect of modifying only the twi and curvature grids iteration 4 vs iteration 1 was an increase in non wetland accuracy in every study site as well as an increase in wetland accuracy in all but site 1 the decrease in wetland accuracy in this site may indicate unintentional smoothing of some features of interest i e too large of a window size and it is possible that a mean filter or smaller window would have performed better in sites 2 3 and 4 results of iteration 4 suggest the statistic type and window size were effective despite the improvements to classifications with these modifications the modified twi and curvature grids can be further advanced the current approach should be expanded to test the effects of varying window sizes of smoothing filters and statistic type as well as the twi formulation the effect of modifying only the dtw iteration 3 vs iteration 1 appeared to be an increase in wetland accuracy in sites 2 3 and 4 and an unexpected decrease in non wetland accuracy in every site this suggests that while the modified dtw was effective in increasing non wetland accuracy when combined with modified twi and modified curvature the dtw modification alone may not be sufficient for reducing false positive predictions the limited improvements provided by the dtw modification could be due to the designation of the representative wetland slope value which may not apply an effective cut off between low and high cost areas additionally improvements to the original dtw calculation before applying modifications may enhance the results of iteration 3 the dtw calculation can be improved first by calculating the slope grid from a dem preprocessed with more sophisticated correction methods and second by deriving the source grid by extracting surface water features directly from the lidar data in this study dtw source grids were generated from rasterized nhd data which are relatively coarsely mapped 1 12 000 1 24 000 compared to the lidar data and therefore do not capture precise curvature and locations of streams and open water 6 conclusions this study evaluated the potential for modification of lidar dem derivatives combined with ancillary national scale soil data to improve a rf classification of wetland areas at a scale relevant for the wetland permitting process over four study sites in virginia the approach was implemented as a model in arcgis and performed a rf classification of input variables that were modified to provide distinct wetland and non wetland signatures model predictions were assessed against field mapped testing data provided by the virginia dot and compared to nwi maps accuracy assessments showed that compared to nwi maps the highest performing models produced significantly higher average wetland accuracy 84 9 and 32 1 respectively while resulting in moderately lower average non wetland accuracy 85 6 and 98 0 respectively and overall accuracy 85 6 and 92 0 respectively through multiple iterations of input variable combinations we concluded that there is potential to improve classifications through modification of topographic indices in every site the highest performing models included modified topographic indices and the addition of available soil layers further improved these classifications assessment of the variable importance of the highest performing models showed that dtw inputs were of higher importance compared to the modified twi in all study sites this finding supports conclusions of previous studies e g ågren et al 2014 murphy et al 2009 which found that the dtw model provided more robust flow convergence information compared to the twi the low variable importance of the twi relative to the dtw also suggests that there is duplicate information provided between these two indices in addition the heavy reliance of the model in site 4 on soil data reinforces previous findings that topographic indices like the twi and dtw are less effective in flat areas due to undefined flow directions that are likely to change over time whereas these indices typically model static conditions and assume local slope describes subsurface flow patterns grabs et al 2009 murphy et al 2009 through classification iterations we found that non wetland and overall classification accuracy increased in all sites when all topographic indices were modified compared to the accuracy achieved by using the original versions of these indices while modifications to the dtw alone did not reduce false positive predictions modifications to only the twi and curvature did have this effect however we believe the dtw modification approach could be further improved on in addition iteration accuracies varied by small margins in many cases and it is important to note that that rf parameters and training and testing data separation were not varied or calibrated to sites in this study completing this additional calibration step may produce different outcomes of iteration comparisons results from this study offer a starting point to the enhancement of the model to include the capability of modifying lidar dem derivatives based on site characteristics to map small scale wetlands in support of environmental planning and conservation efforts the results while successful have also highlighted shortcomings that should be addressed to further enhance the approach and model implementation we found that the topographic indices were limited by the use of the arcgis fill function which removed sinks in the lidar dem by creating larger areas of flat terrain and the use of a simple low pass filtering to reduce topographic noise observed in the twi and curvature studies have shown that sinks and excess variations in high resolution elevation data can be addressed through more sophisticated methods e g besl et al 1989 haralick et al 1983 lindsay 2016 mainguy et al 1995 sangireddy et al 2016 and exploring these methods could improve the accuracy of the topographic indices especially in low relief areas the twi modification can be further advanced on by assessing model responses to alternate twi formulations such as the d infinity method for deriving flow accumulation tarboton 1997 and the soil topographic index formulation which has been shown to improve modelling of soil moisture patterns through inclusion of relevant soil properties e g buchanan et al 2014 lanni et al 2011 alternative curvature modifications should also be explored as this index has been shown to effectively model flow convergence in low relief and engineered landscapes by applying automated filtering techniques sangireddy et al 2016 improvements to the dtw modification should include deriving source data directly from lidar dems through calibrated flow initiation thresholds as shown by ågren et al 2014 and deriving flow accumulation using the d infinity method murphy et al 2009 2011 or incorporating the use of automated channel extracting software such as geonet sangireddy et al 2016 furthermore variable importance indicated that the dtw and twi may provide duplicate information in many cases and efforts should be made to effectively combine these indices through a mathematical relationship to reduce feature space for the classifier future work should also address the excessive computation times needed to process the high resolution lidar data implementing this approach using parallel computing could allow for reductions in runtime needed to calculate γ and β parameters through an iterative calibration to study sites in the dtw modification process alternative implementations of the rf algorithm should be tested as well as the arcgis implementation is limited in output data provided to users lastly the approach presented here should be applied to additional study areas to begin to identify modification parameters that can be effectively generalized by site characteristics while the prototype model has produced more accurate wetland predictions for the study sites compared to nwi these improvements would strengthen the potential for this approach to be a useful tool for wetland identification in support of environmental decision making in areas where wetland maps are currently unavailable acknowledgements the authors wish to thank michael fitch j cooper wamsley daniel redgate and caleb parks from the virginia department of transportation vdot for providing valuable feedback throughout the course of this project and for their support in collecting important data funding for this project was provided by the virginia transportation research council vtrc 
7379,wetlands are important ecosystems that provide many ecological benefits and their quality and presence are protected by federal regulations these regulations require wetland delineations which can be costly and time consuming to perform computer models can assist in this process but lack the accuracy necessary for environmental planning scale wetland identification in this study the potential for improvement of wetland identification models through modification of digital elevation model dem derivatives derived from high resolution and increasingly available light detection and ranging lidar data at a scale necessary for small scale wetland delineations is evaluated a novel approach of flow convergence modelling is presented where topographic wetness index twi curvature and cartographic depth to water index dtw are modified to better distinguish wetland from upland areas combined with ancillary soil data and used in a random forest classification this approach is applied to four study sites in virginia implemented as an arcgis model the model resulted in significant improvement in average wetland accuracy compared to the commonly used national wetland inventory 84 9 vs 32 1 at the expense of a moderately lower average non wetland accuracy 85 6 vs 98 0 and average overall accuracy 85 6 vs 92 0 from this we concluded that modifying twi curvature and dtw provides more robust wetland and non wetland signatures to the models by improving accuracy rates compared to classifications using the original indices the resulting arcgis model is a general tool able to modify these local lidar dem derivatives based on site characteristics to identify wetlands at a high resolution keywords wetlands lidar topographic indices random forest 1 introduction wetlands are important ecosystems that not only provide habitat for many plant and animal species but also improve water quality recharge groundwater and ease flood and drought severity guo et al 2017 despite the ecological value of wetlands their quality and presence are threatened by agricultural or development repurposing pollutant runoff and climate change klemas 2011 current estimates are that approximately 50 of wetlands have been lost globally since 1900 davidson 2014 and approximately 53 of wetlands of the conterminous u s have been lost since the early 1600s dahl et al 1991 the historic loss of wetlands and sustained threat to remaining wetlands have motivated increased efforts by scientists and government to protect and maintain these ecosystems u s federal regulations play an important role in the abatement of further wetland loss one of the most important policies in support of this effort is section 404 of the clean water act which protects the nation s waters including wetlands according to page and wilcher 1990 this law states that environmental planning entities must identify and assess environmental impact due to land development and water resources projects this requires environmental planning entities such as state departments of transportation dots to provide wetland delineations that are ultimately jurisdictionally confirmed by the u s army corps of engineers usace the usace wetlands delineation manual states that wetlands can be identified by environmental characteristics shared among the many wetland types the usace guidelines for wetland delineations use these common features and are based on the presence of hydrologic conditions that inundate the area vegetation adapted for life in saturated soil conditions and hydric soils environmental laboratory 1987 manual surveying by trained analysts will always be the most accurate method to delineate wetlands however carrying out detailed field surveys can be time consuming and costly according to estimates provided by representatives from the virginia dot vdot environmental division the costs of these delineations range from 60 to 140 per acre 0 4 ha personal communication november 28 2017 these estimates are based on recent vdot projects and can vary widely across projects to offset these costs the wetland permitting process could potentially be streamlined by supplementing and guiding the manual delineations with accurate digital wetland inventories however developing and updating wetland inventories can be expensive and technically challenging due to the complexity of wetland features kloiber et al 2015 furthermore the existing national scale wetland inventory in the u s the national wetland inventory nwi is not ideal for assisting in the permitting process despite being one of the most commonly used sources of wetland data in the u s nwi maps were never intended to map federally regulated wetlands cowardin and golet 1995 environmental laboratory 1987 and research has shown that relying solely on the nwi may fail to protect a considerable fraction of wetlands morrissey and sweeney 2006 thus a wetland inventory with the reliability necessary to assist in the wetland permitting process is an unmet need remote sensing has long been recognized as a powerful tool for identifying wetlands environmental laboratory 1987 and may offer an accurate and cost effective way to fulfill this need guo et al 2017 lang et al 2013 lang and mccarty 2014 past studies have incorporated remote sensing data such as multispectral imagery radar and light detection and ranging lidar for wetland identification a review of wetland remote sensing studies of the past 50 years shows that most researchers incorporate multispectral imagery in wetland classifications guo et al 2017 however the incorporation of multispectral imagery can weaken the potential for use during the wetland permitting process by introducing issues of resolution or accessibility for example the commonly used landsat multispectral imagery is freely available on a national scale but the 30 m resolution of this data can be too coarse to detect wetlands at a scale relevant to environmental planning entities which can require a spatial accuracy of at least 1 5 m vdot environmental division personal communication november 28 2017 while studies have shown higher resolution multispectral data can result in accurate wetland classifications e g kloiber et al 2015 these data can be inaccessible due to cost alternatively lidar is a remote sensing data that has been rapidly endorsed by the wetland science and management community for its growing availability and technological benefit to wetland mapping kloiber et al 2015 lang and mccarty 2014 lidar sensors provide detailed information on the earth s landscape and bare surface by collecting x y and z data that can be interpolated to create digital elevation models dems lang and mccarty 2014 lidar dem availability has increased rapidly over the past 20 years and although current coverage in the conterminous u s is at about one third there is an ongoing effort by multiple federal agencies to hasten the collection of lidar throughout the entire u s snyder and lang 2012 while conventional dems and their derivatives have been shown to be useful for wetland delineation e g hogg and todd 2007 lidar dems allow for more detailed mapping of topographic metrics that describe flow convergence lang and mccarty 2014 previous research has shown that dem derivatives have the potential to model spatial patterns of saturated areas and that lidar dem derivatives improve the ability of these metrics to do so e g hogg and todd 2007 lang et al 2013 millard and richardson 2013 among the dem derivatives found to be useful for this purpose are curvature topographic wetness index twi and the cartographic depth to water index dtw e g ågren et al 2014 lang et al 2013 murphy et al 2009 2011 sangireddy et al 2016 curvature is defined as the second derivative of the elevation surface and can describe the degree of convergence and acceleration of flow moore et al 1991 the twi developed by beven and kirkby 1979 relates the tendency of a site to receive water to the tendency of a site to evacuate water and is defined as 1 twi ln α tan β where α is the specific catchment area or contributing area per unit contour length and tan β is the local slope the dtw is a soil moisture index developed by murphy et al 2007 that is based on the assumption that soils very close in elevation to the nearest surface water are more likely to be saturated the dtw model is calculated in grid form as 2 dtw m dz i dx i a x c where dz dx is the downward slope of a pixel i is a pixel along a calculated least cost i e slope path to the assigned source i e surface water pixel a is 1 when the flow path is parallel to pixel boundaries or 2 when the flow crosses diagonally and x c is the pixel length murphy et al 2007 although many studies have shown the benefit of using topographic indices to identify wetted areas and the added benefit of deriving these indices at higher resolutions there are also unique challenges inherent to using lidar dems researchers have noted that lidar dems used for modelling landform characteristics must be resampled to coarser resolutions and smoothed to overcome issues of increased noise from excessive topographic detail macmillan et al 2003 with this topographic noise arising from dems on the order of 1 m pixel size richardson et al 2009 moreover variations in dem resolution result in significantly different spatial and statistical distributions of contributing areas and downslope flow path lengths woodrow et al 2016 and at high resolutions micro topographic features can lead to highly variable slope values and provide unrealistic estimates of hydraulic gradients grabs et al 2009 lanni et al 2011 previous studies have acknowledged the negative effect that these micro topographic features have on the ability of curvature e g sangireddy et al 2016 and twi e g sørensen and seibert 2007 to identify hydrologic features of interest for example ågren et al 2014 found that high resolution dems 2 m caused local twi variations that are too strong to separate wetlands from uplands whereas deriving the index from coarser dems 24 m reduced these variations but resulted in poorly delineated flow channels and local depressions in contrast the researchers also concluded that dtw derivations were not sensitive to scale but suggested that the dtw could be further optimized ågren et al 2014 lidar dem data and other remote sensing data are commonly used to map wetlands through supervised classification algorithms random forest rf classification is a relatively new supervised classification method that is widely used for its ability to handle both continuous and categorical high dimensional data and its generation of descriptive variable importance measures millard and richardson 2015 rodriguez galiano et al 2012 rf has been shown to produce higher accuracies than other classification techniques such as maximum likelihood when incorporating multisource data duro et al 2012 miao et al 2012 rodriguez galiano et al 2012 furthermore studies have shown that lidar dem metrics are suitable input variables for the rf approach e g deng et al 2017 kloiber et al 2015 zhu and pierskalla 2016 and that using this classifier has strong potential to improve mapping and imagery classification of wetlands e g millard and richardson 2013 many studies have relied primarily on ecological factors and spectral indices provided by multispectral imagery to classify wetlands and fewer studies have evaluated the predictive power of lidar dem data alone for this purpose the primary objective of this study was to further advance the application of lidar dem derivatives to wetland mapping by evaluating the potential of modified twi curvature and dtw grids to address limitations noted by researchers and identify small i e environmental planning scale wetlands across varying ecoregions rf classifications of original and modified indices where the twi and curvature were modified via smoothing and the dtw was modified via adjustments to the input slope grid along with ancillary national scale soil data were assessed against field mapped testing data and compared to nwi maps to identify the best performing models accuracy assessments of these classifications provided a measure of the benefits of modifying these input data this approach was applied to four study sites across varying ecoregions of virginia and implemented in arcgis and has the potential for further refinement and utility by environmental planning entities 2 study areas the four sites in this study were selected due to availability of vdot wetland delineations and lidar dems and to have applications of this approach across varying ecoregions of virginia as seen in fig 1 the study sites span five of the seven level iii epa ecoregions of virginia the piedmont 45 the mid atlantic coastal plain 63 the northern piedmont 64 the southeastern plains 65 and the ridge and valley 67 according to the epa 2013 the piedmont ecoregion is considered the non mountainous region of the appalachian highlands comprising of transitional areas between the mountainous appalachians to the northwest and the relatively flat coastal plain to the southeast the soils in this region tend to be finer textured than in ecoregions 63 and 65 the mid atlantic coastal plain is characterized by low nearly flat plains with many swamps marshes and estuaries this region has a mix of coarse and finer textured soils and poorly drained soils are common here the northern piedmont consists of low rounded hills irregular plains and open valleys it is a transitional region between the low mountains in ecoregion 66 and the flat coastal area of ecoregions 63 and 65 the southeastern plains are irregular and are comprised of a mosaic of cropland pasture woodland and forest the subsurface is predominantly sands silts and clays the ridge and valley ecoregion is relatively low lying and characterized by alternating forested ridges and agricultural valleys additional information describing the conditions of each study site can be found in table 1 3 input data freely available lidar elevation data land cover data national scale hydrography data national scale soil data and vdot wetland delineations were used as inputs to the wetland identification model 3 1 lidar elevation data lidar derived elevation data used in this study were provided by the virginia information technologies agency vita in raster format http vgin maps arcgis com vita lidar data were freely available and included hydro flattened bare earth dems the lidar dems used in this study were collected and processed between 2010 and 2015 and have horizontal resolutions ranging from 0 76 m to 1 5 m dem tiles of different resolutions were merged and resampled to the coarsest resolution for each site using the bilinear resampling method in arcgis following the approach previously done by ågren et al 2014 site 2 was unique in that lidar data were unavailable for approximately 230 km2 23 of the processing extent and 0 8 km2 12 of the vdot delineation area to fill the missing areas 3 m data from the national elevation dataset were used https viewer nationalmap gov and resampled to 1 5 m to match the more abundant lidar data while resampling to finer resolutions is not ideal maintaining consistency in the application of the highest resolution lidar data across all study sites was prioritized over the error introduced in the relatively small portion of the processing extent and even smaller portion of the delineation area 3 2 land cover data land cover data were used for post classification filtering land cover data used in this study were provided by vita in raster format http vgin maps arcgis com vita land cover data were derived from the virginia base mapping program 4 band orthophotography collected between 2011 and 2014 these data provided 12 land cover classifications with 85 95 accuracy and have a horizontal resolution of 1 m worldview solutions inc 2016 3 3 national scale data national scale soil and hydrography data were incorporated in the classification as ancillary data soil data used in this study were obtained from the soil survey geographic database ssurgo and distributed by the natural resources conservation service s web soil survey in polygon vector format https websoilsurvey sc egov usda gov the ssurgo hydric rating depth to water table hydrologic soil group surface texture and soil drainage class were used as indicators of saturated conditions according to the soil survey staff 2017 the hydric rating attribute indicates the percentage of a map unit that meets the criteria for hydric soils hydric soils are characteristic of wetlands defined as soil that is formed under conditions of saturation flooding or ponding long enough during the growing season to develop anaerobic conditions in the upper horizon federal register 1994 the surface texture attribute describes the representative texture class according to percentage of sand silt and clay in the fraction of the soil that is less than 2 mm in diameter the soil drainage class attribute identifies the natural drainage conditions of the soil and refers to the frequency of wet periods without considering alterations of the water regime by human activities unless they have significantly changed the morphology of the soil the hydrologic soil group assignment is based on estimates of the rate of water infiltration when the soils are not protected by vegetation are thoroughly wet and receive precipitation from long duration storms the depth to water table attribute indicates the representative depth to the saturated zone in the soil hydrography data used in this study were provided by the national hydrography dataset nhd in polygon vector format https viewer nationalmap gov nhd huc 12 watersheds intersected by the limits of vdot delineations were combined to be used as the processing extent for each study site in order to encompass the hydrologically connected area nhd streams and waterbodies within processing extents were also used 3 4 vdot wetland delineations wetland delineations for each site were provided by vdot and were used to create training and testing datasets the vdot delineations in site 2 site 3 and site 4 were jurisdictionally confirmed by the usace and all study sites were produced through field surveys conducted by professional wetland scientists for these reasons the vdot delineations were considered to be ground truth for the purpose of training and testing the wetland identification model vdot delineations were provided in polygon vector format and included both wetlands and streambeds both were included in subsequent processing because both are considered waters of the state and therefore must be delineated during the wetland permitting process although the delineations were categorized by wetland type by vdot analysts all areas were merged into a single wetland category before application in this study additionally limits of delineations were used to identify true non wetland areas 4 methods the workflow followed to implement the wetland identification approach consisted of three main parts preprocessing supervised classification and post processing fig 2 the workflow was implemented in arcgis 10 4 and the modelbuilder tool was used to automate processes that did not require user intervention outputs of the workflow were model predictions and confusion matrices used to assess the accuracy of predictions components of the workflow are described in more detail in the following sections 4 1 preprocessing the preprocessing phase consisted of a combination of automated and semi automated processes which required user intervention preprocessing steps not explicitly shown in fig 2 included projection of input data to the appropriate north or south virginia state plane coordinate system clipping data to the huc 12 processing extent rasterizing input data originally in polygon vector format by using the site lidar dem as the pixel size constraint and filling sinks within the lidar dem rasterizing the polygon vector layers mapped at coarser scales assumes that the information provided at the original scale ranging from 1 12 000 to 1 24 000 is true for each pixel of the output grid ranging from 0 76 to 1 52 m the lidar dem was filled using the depression filling algorithm of planchon and darboux 2002 that is implemented in arcgis intermediate outputs created by the preprocessing phase were calibrated input variables training data and testing data 4 1 1 input variable creation input variables included the modified twi modified curvature modified dtw and selected soil thematic maps input variables were modified based on site characteristics and information provided by vdot delineations in order to produce distinct wetland and non wetland signatures and user intervention was necessary to execute some of the processes table 2 summarizes the modification parameters for topographic indices as well as information relevant to the calculation of those parameters italicized and selected soil thematic maps for each study site the methods used to create input variables are described in the following sections 4 1 1 1 twi modifications the modified twi grid is based on the twi as defined in eq 1 the twi was created in arcgis as a map algebra expression the inputs required for this calculation were a flow accumulation grid to represent the α term and a slope grid to represent the tan β term both derived from the filled lidar dem the d8 method jenson and domingue 1988 was used to generate flow direction and flow accumulation grids a slope grid was generated with the arcgis slope tool calculated as the steepest downhill descent from each pixel in units of m m burrough and mcdonell 1998 a constant equal to 1 was added to flow accumulation grids so that every pixel received flow from itself as well as upslope pixels to avoid undefined twi values and a constant equal to 0 0001 m m was added to slope grids to avoid dividing by zero an example of the resulting twi grid overlaid with vdot wetland areas for a portion of site 1 is shown in fig 4 panel a1 this twi grid models the presence of wetter areas high twi values in locations of high flow accumulation and flat slopes and drier areas low twi values in locations of low flow accumulation and steep slopes larger clusters of relatively high twi values align with the vdot delineated wetlands however there is also a scattering of high twi values outside of these wetland boundaries corroborating the challenges of high resolution twis previously described in the literature e g ågren et al 2014 sørensen and seibert 2007 although some researchers recommend deriving twis from coarser dems e g ågren et al 2014 doing so would sacrifice the rich detail provided by lidar dems that may be needed to precisely model shape and size of environmental planning scale wetlands although these scatterings of relatively high twi values may be modelling true micro topographic features their location outside of the field mapped wetlands suggests these flow channels are not large enough to result in saturated conditions rather than lose hydrologic detail by resampling the lidar data anomalous local variations were smoothed by applying a low pass filter over a moving nxn window to create the modified twi this low pass filter searches over a user defined window in which every pixel is replaced with the statistical value from the surrounding nxn pixels as done by ali et al 2014 buchanan et al 2014 and lanni et al 2011 the window size for the smoothing operation is significant in that it is usually set with consideration of the average size of the feature of interest sangireddy et al 2016 in this study we estimated that areas of interest must be at least 5 m in width based on the size of vdot delineated wetlands therefore window sizes were set to smooth over a total area of approximately 25 m2 5 m 5 m with this window size varying slightly across study sites depending on pixel length of the lidar data additionally a median filter was chosen to perform smoothing rather than a mean filter visual assessment of both statistic types showed that the median filter better retained clustered high twi values aligned with vdot wetland edges while removing scattered high twi values outside of these boundaries twi smoothing was implemented in the arcgis model using the focal statistics tool window sizes used to calculate the modified twi grid for each site are shown in table 2 and an example of applying this modification for a portion of site 1 is shown in fig 4 panel a2 compared to the unmodified twi panel a1 this scene shows the cluster of relatively high twi values within vdot delineated wetlands was maintained but the discrete small flow channels outside of the true wetland boundaries have been smoothed via replacement of these pixels with relatively lower twi values 4 1 1 2 curvature modifications curvature grids as defined by moore et al 1991 were created from the filled lidar dem using the arcgis curvature tool curvature has been shown to be a key component in the process of identifying likely channelized pixels indicating flow convergence ågren et al 2014 hogg and todd 2007 kloiber et al 2015 millard and richardson 2013 sangireddy et al 2016 it was anticipated that the high resolution of the lidar derived curvature grids would assist in separating small differences in concavity between nearly flat roadways and shallow local depressions however visual assessment of the lidar derived curvature grids showed a similar issue of topographic noise as seen in the twi in that micro topographic channels were also mapped an example of the output curvature grid for a portion of site 1 is shown in fig 4 panel b1 this image shows negative and zero curvature values within vdot wetland extents which correspond to concave and flat areas respectively similar to modified twi creation the curvature was modified by applying a statistical smoothing process to curvature grids similar to the approach of sangireddy et al 2016 when choosing the window size for this calculation the assumption of the average size of features of interest was kept consistent with that of the twi i e at least 5 m in width in this case a mean filter was chosen to smooth the curvature data rather than a median filter due to a visual improvement in vdot wetland edge retention resulting from the mean smoothing the modified curvature grid was created by applying the arcgis focal statistics tool window sizes used to calculate the modified curvature grid for each site are shown in table 2 and an example of this modification for a portion of site 1 is shown in fig 4 panel b2 in this image one can see that the modified curvature grid has a smoother appearance but maintains significant areas of concavity 4 1 1 3 dtw modifications the modified dtw grid is based on the dtw as defined in eq 2 this iterative function finds the cumulative slope value along the least downward slope i e cost path to the nearest surface water i e source pixel with which it is most likely to be hydrologically connected murphy et al 2009 to calculate the dtw two input grids are required a grid of slope values and a grid of areas of open water murphy et al 2009 in this study slope grids were derived from the filled lidar dem using the arcgis slope function as done in the original formulation of the dtw model e g murphy et al 2007 2009 2011 and the source grids were created from rasterized nhd waterbodies and streams while the publicly available nhd was chosen in this study to maintain consistency between the four sites there are alternatives for researchers without publicly available open water data the source grid can also be generated directly from elevation data by deriving streams based on a flow accumulation threshold murphy et al 2009 or by using channel extraction software such as geonet sangireddy et al 2016 the effects and limitations of using the relatively coarsely mapped nhd as the source grid for the dtw are discussed in section 5 2 of this paper the arcgis cost distance tool was used to evaluate eq 2 within the model using the slope and nhd source grids as inputs it was also necessary to add a small constant 0 0001 m m to the slope grid to differentiate from source grid pixels which are assigned a value of zero for the calculation an example of the resulting dtw grid for a portion of site 1 is shown in panel c1 of fig 4 as expected low wetness high dtw values occurred in areas further and higher along the terrain from surface water and high wetness low dtw values occurred in areas of low slopes that are closer to surface water while wetted areas calculated by the dtw correspond to vdot delineated wetlands the transition from wet to drier areas is gradual we found this to result in lower non wetland accuracy or an overestimation of wetlands when using only the original dtw formulation to identify wetland areas therefore a modified dtw was created to accelerate the gradual transition from wetlands to uplands in an effort to better distinguish wet from dry locations the method outlined above was used to calculate the modified dtw except that the input slope grid was replaced with an adjusted slope grid defined as 3 y γ x β where x is the slope with a small constant added as described earlier and γ and β are calculated slope adjustment parameters this adjustment to the slope values was intended to create two distinct ranges of low cost areas where wetlands are likely to exist and high cost areas where wetlands are unlikely to exist based on the observed distribution of wetland slope values in each site the γ parameter allows users to control the cutoff between the low and high cost slope values which correspond to a designated representative wetland slope value the β parameter allows users to control the rate of increase in cost as the slopes increase throughout the site in this study β was set to a value of 2 for all sites while γ was individually calculated we hypothesized that setting the wetland slope value equal to the 95th percentile of all underlying vdot wetland slope values would result in a γ parameter that further flattens the terrain i e reduces the cost where most wetlands exist disregarding assumed outliers while steepening the terrain i e increasing the cost elsewhere representative slope values were calculated by extracting slope values within vdot wetland boundaries and calculating the 95th percentile of each array with the numpy python library fig 3 shows an example of this adjusted slope calculation and describes the effect of this adjustment for site 1 where the 95th percentile was 0 088 m m which corresponded to a γ value of 11 42 with the adjustments to the slope grid applied eq 2 becomes 4 modified dtw m γ dz i dx i 2 a x c where γ and β 2 are introduced slope adjustment parameters and relevant site characteristics used to calculate the parameters are shown for each site in table 2 an example of the effect of modifying the dtw in site 1 using this calculation is shown in panel c2 of fig 4 in this figure the modified dtw shows relatively wetter areas within vdot wetland boundaries and an accelerated increase to drier values moving away from vdot wetlands compared to the original dtw c1 4 1 1 4 soil thematic maps the final input variables created in the preprocessing phase were soil thematic maps soil thematic maps were created from the extensive ssurgo database using the soil data viewer arcmap extension nrcs 2015 although the soil data viewer creates soil thematic maps automatically combinations of soil layers were manually chosen for each site based on correspondence of the soil data to the current physical landscape this correspondence was assessed by visual comparison to vdot delineations and vita land cover data soil layers that appeared too coarse i e generally did not vary enough within the vdot delineated area to describe features of interest were not selected 4 1 2 training and testing data an automated process was used to randomly designate 10 of vdot delineation area to train the classifier and reserve the remaining 90 to test model results it has been noted that statistical classifiers and machine learning algorithms may be sensitive to imbalanced training data or cases where rare classes are being classified such as most cases of wetland identification and the sensitivity of rf specifically to training class proportions was investigated by millard and richardson 2015 the researchers found that when training samples were disproportionately higher or lower than the true distribution of that feature the final classification over or under predicted that class respectively they concluded that using a sampling strategy that ensures representative class proportions and minimal spatial autocorrelation minimized proportion error in their results millard and richardson 2015 in this study we took into account the findings of millard and richardson 2015 when designing the methodology to randomly separate vdot delineations into training and testing data this process consisted of four steps random point creation point buffering value extraction and training data separation fig 5 a stratified random sampling method was used in the first step to distribute a designated number of training sample locations proportionately between wetland and non wetland areas panel a these randomly generated points were then buffered to create circle polygons with an area of approximately 100 m2 each panel b in the value extraction step panel c training data composed of approximately 10 of the delineated area and with representative class proportions were produced by rasterizing the buffered polygons with pixel values extracted from vdot delineations this corrected cases where buffered polygons encompassed both wetland and non wetland classes the testing data were created by separating the training data from the vdot delineations leaving approximately 90 of the delineated area to be used for accuracy assessment panel d statistics describing the training and testing datasets for each site are found in table 3 4 2 supervised classification in the first phase of the supervised classification portion of the workflow the input variables created during preprocessing were combined into a multidimensional composite image where each dimension stores an independent input variable wetland and non wetland signatures were extracted from this composite image and used to perform the supervised classification rf classification was chosen as the classification algorithm for its noted advantages in similar studies as previously described e g duro et al 2012 miao et al 2012 millard and richardson 2013 rodriguez galiano et al 2012 according to breiman 2001 rf is an ensemble classifier that produces many classification and regression like trees where each tree is generated from different bootstrapped samples of training data and input variables are randomly selected for generating trees this algorithm also produces variable importance information which measures the mean decrease in accuracy when a variable is not used in generating a tree the rf classification was executed in arcgis with the train random trees and classify raster tools esri 2016 the train random trees tool utilizes the opencv implementation of the rf algorithm bradski 2000 using train random trees the training data were used to extract class signatures from the dimensions i e input variables of the composite image creating an esri classifier definition file with variable importance measures the classifier definition file was subsequently used to classify the remainder of the composite image the result of these operations is a grid where each pixel has been classified as wetland or non wetland as the focus of this study was to analyze the response of classification models to input data the rf parameters were not varied or calibrated to study sites for this reason the default values of maximum number of trees maximum tree depth and maximum numbers of samples per class were held constant at the recommended default values of 50 30 and 1000 respectively future work should perform a sensitivity analysis to test the effect of adjusting these parameters 4 3 post processing the first phase of post processing was post classification filtering the objective of the post classification filtering was to account for areas that may be susceptible to water accumulation due to its local topography but cannot be wetland areas due to impervious land cover the post classification filtering algorithm first used a logical statement to determine if a classified wetland pixel overlaps vita land cover designated as impervious if this was false the pixel classification was unchanged if this was true a second logical statement was used to account for cases where wetlands may exist under bridges by determining if classified wetland pixels are within 30 m of nhd streams the 30 m buffer distance was an estimated value based on visual inspection and more precise measurements would increase effectiveness of post classification filtering if this second statement was false the pixel was reclassified as non wetland otherwise it was unchanged this process produced the model predictions the second phase of post processing was accuracy assessment the model predictions and nwi map for the study area were assessed for accuracy in terms of agreement with the test dataset accuracy assessments were evaluated with confusion matrices which summarized pixels as areas of wetland agreement non wetland agreement false negative predictions cases where true wetland areas were predicted to be non wetland or false positive predictions cases where true non wetland areas were predicted to be wetland confusion matrices were used to calculate wetland accuracy non wetland accuracy and overall accuracy using eqs 5 7 5 wetland accuracy wetland agreement k m 2 test actual wetland k m 2 6 nonwetland accuracy nonwetland agreement k m 2 test actual nonwetland k m 2 7 overall accuracy wetland agreement k m 2 nonwetland agreement k m 2 test area k m 2 the use of these metrics to assess wetland classifications is common in literature e g ågren et al 2014 millard and richardson 2013 5 results and discussion 5 1 highest performing models to determine the highest performing models classifications varying only topographic inputs were first performed and assessed and the input data that resulted in highest overall accuracy were combined with relevant soil layers if any in the coming sections the following results are discussed 1 scenes for each site comparing highest performing models and their level of agreement with vdot delineations compared to nwi maps 2 variable importance of highest performing input data and 3 the accuracy assessment of highest performing models compared to the nwi the input data used to produce the best performing models and the importance of these inputs according to the esri classifier definition file are listed in table 4 although accuracy assessments for each site only extend to testing dataset limits scenes depicting predictions and vdot delineations prior to the separation process are shown for clarity 5 1 1 site 1 results wetland predictions and nwi data for site 1 are shown in fig 6 both of the nwi scenes a1 and b1 exemplify the tendency of the nwi to underestimate the size of vdot delineated wetlands by mapping wetlands primarily along streams while the narrow nwi wetlands precisely map the wetland areas that are in agreement with vdot delineations the nwi fails to match the contours or the size of larger wetland zones these larger wetland zones were more fully mapped by wetland predictions produced by the model a2 and b2 however the model also produced relatively higher overestimation of wetlands overestimation of wetlands is especially prevalent in location 1 underlying input variables indicated that overestimation here was due to a depression that was filled to become a large zero slope area this flat zone resulted in a corresponding generalized area of high wetness values in the modified twi and modified dtw in addition the surface texture input indicated that silty clay loam which have relatively slow infiltration rates 0 5 cm h soil survey staff 2017 was also present in this overestimated area likely contributing to the wetland predictions here it is possible that the results in this site could be improved by using an alternative to the pit filling i e arcgis fill algorithm to avoid creation of generalized flat areas more severe adjustments to the slope grid for the modified dtw or higher resolution soil data panel b2 shows more precise model wetland predictions represented by conformity of predicted wetlands to the curvature of vdot delineated wetlands this panel encompasses the scene in fig 4 c2 where the modification to the dtw was shown to more precisely map wetland areas for that reason we attribute the relatively precise mapping of wetlands in b2 in part to the modifications used for the dtw location 2 shows one small wetland that was undetected by the model this may indicate a wetland formed due to conditions more strongly driven by vegetation rather than topography or proximity to surface water 5 1 2 site 2 results two scenes of the model predictions and nwi maps for site 2 are shown in fig 7 in panels a1 and a2 the nwi dataset and model predictions both show similar overestimation of wetland area although the model resulted in higher overestimation the false positive predictions in this area were due to flow convergence indicated by the topographic inputs and the presence of hydric soils indicated by the ssurgo data also many false positive predictions in this site were in locations overlapping road features e g location 1 this may indicate a need for alternative modifications to topographic inputs especially curvature to better differentiate channelized areas due to road features from channelized areas that are wetlands panel b1 shows another example of nwi wetland delineations following along streams but failing to capture the extents of larger wetland zones for this same area the model predicted wetlands further from the streambeds due to the gradual slopes surrounding them and better encompassed vdot delineated wetlands locations 2 and 3 5 1 3 site 3 results examples of model predictions and nwi data for site 3 are shown in fig 8 as seen in table 4 site 3 was unique in that no soil layers were included in the best performing model visual assessment of relevant soil layers in this area showed that the ssurgo data did not vary in a way that effectively differentiated between features of interest site 3 was also unique for its wetlands which were typically narrow and located along small flow channels rather than in larger wetland zones the nwi data shown either do not conform to the bends along the length of wetlands a1 or failed to map a number of wetlands in these channelized areas b1 the model predicted a larger portion of the vdot delineated wetlands in both scenes however the wetland predictions often extended too far on either side of the narrow wetlands a2 location 1 shows another example of a local depression filled to become a generalized flat area resulting in an overestimation due to the modified twi and modified dtw additionally both scenes a2 and b2 show that the model detected road edges and road medians as wetland areas this is a shortcoming of the model that was observed in other sites such as site 2 and indicates a need for further modification to topographic indices 5 1 4 site 4 results fig 9 shows three scenes from the nwi maps and model predictions for site 4 which was the largest site studied site 4 was also unique for having the largest distribution of vdot delineated wetlands covering more than 40 of the surveyed area as well as the mildest average slope see table 1 nwi maps underestimated a large portion of vdot delineated wetlands and the portions of these wetlands that were mapped were delineated with less precision than typically seen by the nwi e g location 2 the model predictions also resulted in a large number of false negative predictions and imprecise wetland delineations the well defined contours of model predictions e g locations 1 3 and 4 exemplify the heavy reliance of the model on soil thematic layers in these scenes the primary drivers for wetland prediction were the presence of hydric soils and shallow depth to water table which both outlined the same contours as these wetland predictions the relatively lower reliance on topographic indices in this site is likely due to the unchanging topography of the area which is characteristic of the mid atlantic coastal plain as there was often little to no flow convergence indicated by the topographic indices where vdot delineated wetlands were mapped it is possible that alternative filtering techniques or more severe adjustments to the slope grid could increase the effectiveness of topographic indices to detect wetted areas however the correspondence of the model to the soil layers and the relatively high occurrence of false negative predictions imply that vegetation data would also be valuable in this region 5 1 5 variable importance an important output from the rf classification was the esri classifier definition file which provided the variable importance of each input used in classifications see table 4 variable importance measures were used to gauge the ability of input variables to provide unique significant information to the classifier table 4 shows that in site 1 site 3 and site 4 the modified dtw was the most important topographic index and in site 2 the original dtw was the most important topographic index in contrast the modified twi was the overall least important input variable in every study site the low ranking of the modified twi relative to the modified and original dtw suggests that some information was duplicated by these inputs but that the modified dtw provided more robust wetland and non wetland signatures this corresponds to the findings of previous studies e g ågren et al 2014 murphy et al 2009 which stated that wet twi values were restricted to discrete lines of flow accumulation within wetted areas whereas the dtw model effectively encompassed wetted areas as a whole and was therefore more robust for this same reason it was unexpected that for site 3 the modified dtw ranked higher than the modified twi as the vdot delineated wetlands here were primarily restricted to narrow lines of flow accumulation soil data were among the most important variables in all sites that included them in site 1 and site 2 this is likely due to the abundance of road features and the ability of the soil information to better distinguish these from wetlands relative to the topographic indices which were observed to detect water accumulation along roads the higher importance of soil layers in site 4 is likely due to the flat terrain and is in line with the wetland predictions seen in fig 9 which were dictated primarily by areas of hydric soil and shallow depth to water table the low importance of the topographic indices in site 4 also reinforces the claim that topographic indices that are static and assume the local slope is an adequate proxy for subsurface flow patterns such as the twi and dtw are less suitable in flat areas due to undefined flow directions that are likely to change over time grabs et al 2009 the lower importance of modified curvature relative to dtw inputs in all sites may indicate that our application of the curvature was limited by the arcgis fill operation and filtering technique i e mean smoothing which generalized potentially significant terrain features since curvature has been shown to strongly determine flow convergence in flat topography sangireddy et al 2016 5 1 6 accuracy assessment model accuracy was assessed using the testing data and compared to the accuracy achieved by the nwi maps table 5 shows the confusion matrices produced for the best performing models and the nwi maps across all study sites in each confusion matrix test data are represented along columns and nwi and model predictions are represented along rows categorized pixels expressed as total km2 in table 5 were used to calculate wetland accuracy non wetland accuracy and overall accuracy using eqs 5 7 it is important to note that the accuracy assessment only extended to the limits of the testing data which as previously described are randomly selected subsets of the original vdot delineations and the effect of varying testing and training data separation on model accuracy was not assessed fig 10 summarizes the accuracy achieved by the best performing model predictions and nwi maps in the context of the wetland permitting process it is important to have high values for all accuracy metrics to uphold the objective of protecting existing wetlands wetland accuracy is of high importance and in order to provide realistic estimates of potentially impacted wetland areas in transportation and environmental planning high non wetland accuracy is also necessary however it is important to be aware of the potential for overall accuracy which measures the portion of the entire area that is correctly classified regardless of class to be misleading due to the uneven distribution of landscape classes for example the consistently conservative wetland mapping by the nwi is reflected by the high average non wetland accuracy 98 0 due to the uneven distribution of wetland and non wetland classes in all but one of the study sites the conservative nature of the nwi predictions also translated into high average overall accuracy 92 0 despite an average wetland accuracy of 32 1 in contrast the model predictions resulted in significantly higher average wetland accuracy 84 9 but at the expense of moderately lower average non wetland and overall accuracy 85 6 and 85 6 respectively as previously discussed site 4 was the lowest performing site the low wetland accuracy here may be due to a lack of vegetative signatures to distinguish wetland from upland area especially in this excessively flat area where terrain indices were found to be less important 5 2 response of model to input data modification iteration results in terms of wetland non wetland and overall accuracy highlight the benefit and cost of applying the modifications described here as well as including the coarser mapped 1 12 000 1 24 000 ssurgo data results of the analysis of model responses to classification iterations are shown in table 6 where the highest performing iteration per accuracy metric not including iteration 5 which built off of top performing topographic inputs is indicated with a superscript and modified topographic inputs are indicated with an asterisk shown in table 6 non wetland accuracy and overall classification accuracy from iteration 1 where the original versions of all indices were used improved in every site as a result of modifying all topographic indices iteration 2 in addition for three of the four sites modifying all topographic indices resulted in the highest overall accuracy these results suggest there is a benefit to applying the modifications presented here rather than using the indices as they are traditionally calculated where this benefit is a reduction in false positive predictions and increase in overall accuracy furthermore in every site that relevant soil layers were applicable the inclusion of these soil layers with top performing topographic indices i e iteration 5 further improved the rf classification from this we conclude that in these sites the soil data provided important information to the classifier despite its relatively coarse scale both site 2 and site 4 saw relatively high increases in wetland accuracy resulting from iteration 5 which suggests the topographic indices were not effective in encompassing flow convergence or subsurface moisture conditions in order to detect wetlands iterations 3 and 4 were performed to determine the effect of individual modifications on the classification note that for this evaluation modified twi and modified curvature were generalized into a single category of modifications because of their similar adjustment parameters and methods the purpose of modifying topographic indices was largely to reduce false positive predictions in that twi and curvature grids were modified to reduce unrealistic flow convergence due to excess topographic detail and the dtw was modified to accelerate the transition from wetland to upland areas results in table 6 show that the effect of modifying only the twi and curvature grids iteration 4 vs iteration 1 was an increase in non wetland accuracy in every study site as well as an increase in wetland accuracy in all but site 1 the decrease in wetland accuracy in this site may indicate unintentional smoothing of some features of interest i e too large of a window size and it is possible that a mean filter or smaller window would have performed better in sites 2 3 and 4 results of iteration 4 suggest the statistic type and window size were effective despite the improvements to classifications with these modifications the modified twi and curvature grids can be further advanced the current approach should be expanded to test the effects of varying window sizes of smoothing filters and statistic type as well as the twi formulation the effect of modifying only the dtw iteration 3 vs iteration 1 appeared to be an increase in wetland accuracy in sites 2 3 and 4 and an unexpected decrease in non wetland accuracy in every site this suggests that while the modified dtw was effective in increasing non wetland accuracy when combined with modified twi and modified curvature the dtw modification alone may not be sufficient for reducing false positive predictions the limited improvements provided by the dtw modification could be due to the designation of the representative wetland slope value which may not apply an effective cut off between low and high cost areas additionally improvements to the original dtw calculation before applying modifications may enhance the results of iteration 3 the dtw calculation can be improved first by calculating the slope grid from a dem preprocessed with more sophisticated correction methods and second by deriving the source grid by extracting surface water features directly from the lidar data in this study dtw source grids were generated from rasterized nhd data which are relatively coarsely mapped 1 12 000 1 24 000 compared to the lidar data and therefore do not capture precise curvature and locations of streams and open water 6 conclusions this study evaluated the potential for modification of lidar dem derivatives combined with ancillary national scale soil data to improve a rf classification of wetland areas at a scale relevant for the wetland permitting process over four study sites in virginia the approach was implemented as a model in arcgis and performed a rf classification of input variables that were modified to provide distinct wetland and non wetland signatures model predictions were assessed against field mapped testing data provided by the virginia dot and compared to nwi maps accuracy assessments showed that compared to nwi maps the highest performing models produced significantly higher average wetland accuracy 84 9 and 32 1 respectively while resulting in moderately lower average non wetland accuracy 85 6 and 98 0 respectively and overall accuracy 85 6 and 92 0 respectively through multiple iterations of input variable combinations we concluded that there is potential to improve classifications through modification of topographic indices in every site the highest performing models included modified topographic indices and the addition of available soil layers further improved these classifications assessment of the variable importance of the highest performing models showed that dtw inputs were of higher importance compared to the modified twi in all study sites this finding supports conclusions of previous studies e g ågren et al 2014 murphy et al 2009 which found that the dtw model provided more robust flow convergence information compared to the twi the low variable importance of the twi relative to the dtw also suggests that there is duplicate information provided between these two indices in addition the heavy reliance of the model in site 4 on soil data reinforces previous findings that topographic indices like the twi and dtw are less effective in flat areas due to undefined flow directions that are likely to change over time whereas these indices typically model static conditions and assume local slope describes subsurface flow patterns grabs et al 2009 murphy et al 2009 through classification iterations we found that non wetland and overall classification accuracy increased in all sites when all topographic indices were modified compared to the accuracy achieved by using the original versions of these indices while modifications to the dtw alone did not reduce false positive predictions modifications to only the twi and curvature did have this effect however we believe the dtw modification approach could be further improved on in addition iteration accuracies varied by small margins in many cases and it is important to note that that rf parameters and training and testing data separation were not varied or calibrated to sites in this study completing this additional calibration step may produce different outcomes of iteration comparisons results from this study offer a starting point to the enhancement of the model to include the capability of modifying lidar dem derivatives based on site characteristics to map small scale wetlands in support of environmental planning and conservation efforts the results while successful have also highlighted shortcomings that should be addressed to further enhance the approach and model implementation we found that the topographic indices were limited by the use of the arcgis fill function which removed sinks in the lidar dem by creating larger areas of flat terrain and the use of a simple low pass filtering to reduce topographic noise observed in the twi and curvature studies have shown that sinks and excess variations in high resolution elevation data can be addressed through more sophisticated methods e g besl et al 1989 haralick et al 1983 lindsay 2016 mainguy et al 1995 sangireddy et al 2016 and exploring these methods could improve the accuracy of the topographic indices especially in low relief areas the twi modification can be further advanced on by assessing model responses to alternate twi formulations such as the d infinity method for deriving flow accumulation tarboton 1997 and the soil topographic index formulation which has been shown to improve modelling of soil moisture patterns through inclusion of relevant soil properties e g buchanan et al 2014 lanni et al 2011 alternative curvature modifications should also be explored as this index has been shown to effectively model flow convergence in low relief and engineered landscapes by applying automated filtering techniques sangireddy et al 2016 improvements to the dtw modification should include deriving source data directly from lidar dems through calibrated flow initiation thresholds as shown by ågren et al 2014 and deriving flow accumulation using the d infinity method murphy et al 2009 2011 or incorporating the use of automated channel extracting software such as geonet sangireddy et al 2016 furthermore variable importance indicated that the dtw and twi may provide duplicate information in many cases and efforts should be made to effectively combine these indices through a mathematical relationship to reduce feature space for the classifier future work should also address the excessive computation times needed to process the high resolution lidar data implementing this approach using parallel computing could allow for reductions in runtime needed to calculate γ and β parameters through an iterative calibration to study sites in the dtw modification process alternative implementations of the rf algorithm should be tested as well as the arcgis implementation is limited in output data provided to users lastly the approach presented here should be applied to additional study areas to begin to identify modification parameters that can be effectively generalized by site characteristics while the prototype model has produced more accurate wetland predictions for the study sites compared to nwi these improvements would strengthen the potential for this approach to be a useful tool for wetland identification in support of environmental decision making in areas where wetland maps are currently unavailable acknowledgements the authors wish to thank michael fitch j cooper wamsley daniel redgate and caleb parks from the virginia department of transportation vdot for providing valuable feedback throughout the course of this project and for their support in collecting important data funding for this project was provided by the virginia transportation research council vtrc 
