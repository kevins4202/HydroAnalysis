index,text
8565,increasing river floods and infrastructure development in many parts of the world have created an urgent need for accurate high resolution flood hazard mapping for more efficient flood risk management mapping accuracy hinges on the quality of the underlying digital terrain model dtm and other spatial datasets this article presents a processing strategy to ensure consistent adaption of countrywide spatial datasets to the requirements of hydraulic modelling the suggested methods are automatized to a large extent and include i automatic fitting of river axis positions to the dtm ii detection of culverts and obstacles in the river channel iii smooth elimination of obstacles by interpolation along the river axes iv geometric detection of water land borders and the top edge of embankments for v integration of the submerged river bed geometry into the dtm the processing chain is applied to a river network 33880 km and a dtm from airborne laser scanning als with 1 m spatial resolution covering the entire territory of austria 84000 km 2 thus countrywide consistency of data and methods is achieved along with high local relevance semi automatic validation and extensive manual checks demonstrate that processing significantly improves the dtm with respect to topographic and hydraulic consistency however some open issues of automatic processing remain e g in case of long underground river reaches keywords dem topographic data hydraulic modelling flood hazard mapping high resolution 1 introduction 1 1 background and motivation the last years and decades have seen numerous extreme river flooding events in europe which have caused damage to persons and infrastructure changes of the frequency and magnitude of floods have been observed on a continental scale blöschl et al 2015 and globally douben 2006 najibi and devineni 2018 ceola et al 2014 among other factors climate change has been identified as an important driver bertola et al 2021 blöschl et al 2019 alfieri et al 2017 these changes require authorities to extend and adapt flood protection measures in their respective administrative domains citizens and other stakeholders such as insurance companies also have strong interest in accurate flood risk assessments accordingly there is a need for detailed and up to date flood hazard maps based on hydraulic simulations while in the past computational requirements have limited the use of high resolution data to small areas this is no longer the case horváth et al 2016 buttinger kreuzhuber et al 2021 using region or country wide data at a resolution of meters or even below has become feasible hydraulic simulations require spatial data as inputs including digital terrain models dtms for specifying the geometry of the flow domain and vector data of the river network for specifying boundary conditions e g inflow hydrographs these datasets in their original forms tend to be inconsistent due to different acquisition techniques and temporal decorrelation additionally dtms are often created for other purposes and thus do not initially fulfill all requirements of hydraulic modelling however dtm quality and resolution has massive impacts on further processing results chaplot 2005 lin et al 2010 sørensen and seibert 2007 xu et al 2016 the aim of this paper is to develop a procedure that prepares and augments extensive spatial datasets to make them suitable inputs for flood modelling given potentially very large datasets the most crucial question is the possible degree of automatization while still reliably adapting to different landscape forms and river sizes 1 2 state of the art and contributions due to the importance of flood risk mapping there is extensive literature on the acquisition and preparation of dedicated spatial data recently more and more studies use spatial data with high resolutions of around a meter or less for flood modelling these are often based on airborne laser scanning als data muhadi et al 2020 mandlburger et al 2011 woodrow et al 2016 but also on other techniques such as photogrammetry from unmanned aerial vehicles hashemi beni et al 2018 because of the rather costly data acquisition per area and the computational requirements many of these studies cover small areas e g individual river reaches or catchments up to several 100 km 2 thus extensive manual editing as part of the workflow is not of major concern since the effort remains low when it comes to larger areas such as whole provinces or countries there are hybrid approaches locally refining an extensive low resolution height model in settled areas where a higher level of detail is needed leitão and de sousa 2018 several studies compare and evaluate the implications of using dtms or more generally digital elevation models dems with different spatial resolutions mohanty et al 2020 muthusamy et al 2021 saksena and merwade 2015 generally investigations on a regional or continental scale usually rely on dems with comparably coarse spatial resolution hutchinson and dowling 1991 alfieri et al 2014 fleischmann et al 2019 jarihani et al 2015 li et al 2020 e g from srtm yan et al 2019 lehner et al 2008 geoscience australia 2015 or from tandem x mason et al 2016 the position of the drainage networks and river valley lines is thereby computed with flow routing algorithms based on the respective elevation model tribe 1992 rieger 1993 soille et al 2003 hou et al 2011 poggio and soille 2012 the procedure is very sensitive to pits in the dem so these have to be removed beforehand soille et al 2003 tianqi et al 2003 grimaldi et al 2007 poggio and soille 2012 some existing software packages such as anudem hutchinson 2011 cover the respective processing chains in case of using higher resolution dtms additional issues have to be taken into account most notably surface roughness lindsay et al 2019 and infrastructure or other features obstructing possible underpasses thereby creating erroneous pitches barber and shortridge 2005 the latter is often solved with depression breaching which avoids negative influence of an obstacle on flow routing without geometrically removing it from the dtm rieger 1998 soille et al 2003 lindsay and dhun 2015 the specific task of identifying and eliminating bridges obstructing the river course was treated in sithole and vosselman 2006 apart from that also the detection and description of river embankments from dems is covered in the literature e g krüger and meinel 2008 köthe and bock 2009 casas et al 2012 sofia et al 2014 steinfeld et al 2013 provide a comparison with image based detection approaches the focus of these mostly local studies is on the detection of levees and other anthropogenic features clearly elevated over the floodplains the main contribution of this study is the combination of a high level of detail with large spatial coverage our aim is to design execute and evaluate methods to process heterogeneous 1 m grid width terrain elevation data and heterogeneous stream line data to provide a compound homogeneous dataset ready for hydraulic simulation for an entire country austria the methodology intends to include large streams as well as small less regulated rivers given by a pre existing vector river network which is preserved in terms of topology throughout all geometric corrections and adaptions 2 data and context the developed workflow was applied to datasets covering the whole territory of austria 84 000 km 2 with a river network of 33880 km as part of the project hora 3 0 the goal of this project was to create country wide high resolution flood hazard maps for different return periods based on instationary 2d hydraulic simulations buttinger kreuzhuber et al 2021 waser et al 2011 visdom 2021 accordingly the project area corresponds to the whole territory of austria along with the immediate border regions c f fig 1 except for a couple of rivers along the northern border it mainly contains catchment areas of two rivers the rhein vorarlberg and the danube rest of austria with its tributaries mur and drau south of the main alpine ridge elevations vary between slightly over 100 m in the pannonian basin close to the eastern border and nearly 3800 m in the central eastern alps for the complete area a dtm and a digital surface model dsm with a spatial resolution of 1 m was available the underlying point clouds for these models were acquired in various airborne laser scanning als campaigns between the years 2003 and 2013 by the austrian federal states therefore inconsistencies are to be expected due to both temporal decorrelation and different terrain modelling workflows the most important vector dataset is the river network line shp file as used by the austrian administrative authorities similarly to the raster models the nationwide dataset has been put together from separate datasets acquired by the federal states due to different acquisition techniques level of detail and accuracy of the river axes vary between federal states but often also depend on river size the total length of all river axes amounts to approximately 40 000 km however only those with a total catchment area of at least 10 km 2 are considered in this work altogether 2735 river lines other important country wide vector datasets used in the project are land use and buildings from the digital cadastral map as well as lakes and power plants additionally local datasets such as measured river cross sections or other bathymetric data served as inputs to be integrated into the dtm c f 3 5 2 1 requirements besides the need for consistent and accurate spatial data there are also some specific formal requirements these come from the model and the framework for hydraulic modelling in order to ensure compatibility and computationally efficient processing the hydraulic model used in flood hazard mapping is based on the shallow water equations swes c f buttinger kreuzhuber et al 2019 one popular numerical discretization of the swes is the finite volume method where the variables water depth and discharge are represented by their averages on the computational grid efficient and robust numerical schemes together with massive parallelization e g on gpus enable the processing of large areas at high spatial resolution horváth et al 2016 however the accuracy of the hydraulic model hinges on the accuracy of the input data including the hydrological flood discharges and the dtm as the simulated inundated areas tend to be very sensitive to the dtm a precise and reliable dtm is of paramount importance in this study the dtm is provided in 2 5d so it can be expressed as a function f z f x y assigning exactly one height value z to every position coordinates x y thus we assume that the domain of the function f is simply connected if this is not the case gap filling has to be applied first it is furthermore required to be consistent throughout the simulation domain consistency includes the removal of artifacts from measurement and interpolation errors in fact not only errors in the dtm acquisition can lead to problematic results of the simulations but also actually existing objects such as bridges which can potentially act as incorrect barriers since the 2 5d representation is not able to map 3d underpasses correctly upstream of the barrier the water levels are overestimated while inundated areas are underestimated downstream these kind of objects have to be removed from the dtm ensuring that the water flow along the river is not erroneously restricted however there are exceptions for culverts under settled areas under other water bodies and retention dams since in these cases cutting the dtm would not lead to meaningful results a separate dataset holding the respective river segments was created instead additionally many common dtm acquisition techniques are not capable of providing bathymetric data for example topographic airborne laser scanning als of water bodies either leads to void areas or to data representing the water surface this is because inundated parts of the river bed can not be measured due to the absorption of infrared light in the water in order to fully represent the river bed topography bathymetric data from other sources were integrated which resulted in a hydrologically enforced dtm dtm h another crucial input for the hydraulic simulation are hydraulic boundary conditions bcs at the origin of a river and wherever a river enters or leaves the simulation domain an inflow or outflow bc has to be specified the locations and cross sectional widths of the in and outflows were derived from the river center lines and the river banks alignment of these vector datasets with the river course in the dtm ensures a correct application of the bcs the inflow outflow hydrographs themselves were estimated by hydraulic modelling 3 methods a common property of nearly all spatial datasets is their heterogeneity c f section 2 processing thus has to ensure compatibility between existing datasets before deriving new ones from them the raster dtm has a central role in the workflow as it covers the entire domain in order to make it processable the dtm is organized in tiles of 10 km by 10 km the most important input vector dataset is the river network representing the axis of each river as one line feature since most further working steps are based on a combination of dtm and river network discrepancies between these two datasets have to be corrected this is realized by adapting river axes to the geometric properties of the dtm section 3 1 subsequently a proper dtm representation of the river courses is ensured by finding and eliminating obstacles sections 3 2 and 3 3 assuming an unobstructed river bed representation river embankments were detected based on the geometric cross section properties section 3 4 these enable the application of boundary conditions and the integration of river bed geometry into the dtm to get a dtm h section 3 5 fig 2 provides an overview of the methodology which is explicitly described in the following subsections processing and analysis were carried out primarily using the software packages opals pfeifer et al 2014 python matlab and qgis 3 1 positional correction of the river network the original dtm and river network datasets show clear geometric discrepancies meaning that the river axes as indicated by the shape file only partly overlap with the river bed according to the dtm harmonization of these two datasets has highest priority since all following working steps 3 2 3 5 rely on it the reasons for these discrepancies are mainly temporal decorrelation and too coarse digitization of river lines the latter especially for minor rivers the assumption that the river network is the dominating source of discrepancies is confirmed by the fact that deviations often amount to several meters this is significantly larger than the accuracy of als data which normally does not exceed a few decimeters kraus 2007 pfeifer and briese 2014 accordingly it was decided to correct the river axes while keeping the dtm unchanged another key argument for always correcting river axes is that changing a 2d line geometry depends on far less assumptions than changing the whole river bed river banks and their flood plain in the dtm since approximate river axis positions were already available a sequential cross section based approach was developed for the spatial correction the node structure of the river network was densified to ensure a maximum node to node distance is not exceeded for each node a cross section normal to the original river axis was derived fig 3 4 the height profile along these cross sections was sampled from the dtm and smoothed this function is called h d where d is the signed distance orthogonal to the river axis the actual correction starts from the first point of each river axis i e the source in case of natural rivers and proceeds in downstream direction for each cross section six weight functions are formulated in order to account for several geometric criteria the river axis position is then corrected to where the maximum of the product over all weight functions appears in the following the individual weight functions are described in detail they are based on geometric and hydraulic reasoning as well as on test analyses over limited areas cross section form the center of the river bed is expected to be a local height minimum of h d the weight function therefore is essentially a ratio of the second derivative h d and the first derivative h d 1 w 1 d h d β 1 h d where β 1 has the purpose of avoiding division by zero and shifting the balance between the influence of first and second derivative this function is maximized in case of a clearly positive h d and with h d close to zero which are both given at local height minima height dependency at least in a local neighbourhood lower points are more likely to be located in the river bed and therefore receive a higher weight the related weight function was designed with an exponential expression which enables adjusting the influence of this criterion by changing the base β 2 with β 2 1 2 w 2 d β 2 mean h d h d all points above the mean height thus get a weight between 0 and 1 while the points below have clearly higher weights initial approximation in most cases the original river axis is not more than several meters off the real river bed according to the dtm making it a viable coarse approximation therefore a gaussian function centered in the middle of the cross section is added as another weight function 3 w 3 d g d σ 3 0 4 g d σ λ 1 σ 2 π e d λ 2 2 π 2 compared to the other criteria w 3 is less discriminative it mainly plays a decisive role if other weight functions are similarly high for large parts of the cross section e g wide flat river beds river axis prediction another approximation for the river axis position is found by estimating a cubic function through the prior four already corrected positions the intersection d i of that extrapolated river course with the current cross section is used as the center point of another gaussian function c f eq 4 5 w 4 d g d σ 4 d pred the value of σ 4 was chosen slightly smaller than σ 3 in order to act as a counterbalance to w 3 eq 3 for bad original river axes water flow properties water tends to flow along direct paths without ascents thus further weight functions are introduced to prefer a short horizontal path from the previous cross section w 5 and a cheap path in the sense of having no significant ascent on the way w 6 both weight functions are again formulated in a similar way as w 2 eq 2 the exponent of w 5 depends on the horizontal 2d distances s d from the previous river axis point a difference from the mean value is chosen in order not to make the criterion too discriminative all distances above the mean value result in weights between 1 and β 5 6 w 5 d β 5 mean s d s d for w 6 all positive height differences along the horizontal path are accumulated δ h cum d due to dtm accuracy and resolution δ h cum d tends to be slightly above 0 in practice a threshold δ h max is introduced to assign weight 0 to all points exceeding a tolerable cumulative ascent the threshold is determined depending on dtm properties as well as on the distance between cross sections it makes w 6 a very discriminative criterion since it usually excludes all points outside the river bed even if they are lower in elevation 7 w 6 d β 6 δ h cum d if δ h cum d δ h max 0 else moreover an option was added to skip a cross section in case it contains no single point that can be reached without major height gains on the path δ h cum d δ h max d that way detours around erroneous obstacles in the river bed c f section 3 2 are avoided finally all weight functions are combined by multiplying them 8 w final k 1 6 w k the multiplication ensures that exclusion w k 0 of a certain cross section part due to one single criterion cannot be overruled by others if the resulting function has a maximum larger than 0 the corresponding position d max in the cross section is introduced as a new point of the corrected river axis and the same procedure is repeated for the next downstream cross section 3 2 detection of obstacles bridges and culverts after ensuring correspondence between the river network and the dtm it is still not guaranteed to have monotonously falling height profiles along the river lines significant local height increments are caused by different kinds of obstacles in the dtm representation of the river these obstacles can be existing objects such as bridges or other buildings many of them also originate from interpolation effects since the water surfaces are often insufficiently covered by als point clouds especially forested river banks tend to favour discontinuous representations of the water surface these obstacles are problem for simulations using 2 5 d dtms which do not allow the modelling of full 3d information e g bridge underpasses consequently all objects above the river bed act like dams and therefore have to be detected and then removed or treated explicitly the detection is realized here using the height profile along the corrected river axes as sampled from the dtm after smoothing the ascending and descending hill of a potential obstacle is detected as a clear local maximum of the first derivative followed by a clear local minimum within a certain distance c f fig 3 2 c clear is intended to imply that the absolute values of the extrema exceed a certain threshold an additional criterion is that the height after the obstacle has to be lower than before however elimination should not start at the steepest slope but at the point where the obstacle effectively starts changing the height profile so after having detected an obstacle based on the first derivative a subsequent fine detection is run in the surrounding section of the height profile using the same combination of first and second derivative as described in eq 1 c f fig 3 2 d the value for β 1 is chosen to be very small giving the first derivative a strong weight this means local minima in the height profile are identified these local minima are the points of the river axis where the height profile stops fulfilling the expectation of monotonicity additionally a small margin is added before and after the obstacle to ensure a proper channel representation when removing obstacles c f 3 3 finally a decision has to be made how to proceed with the detected obstacle although there are some objective criteria the final decision was made manually by an operator using the dtm and optionally other datasets such as orthophoto dsm and the cadastral map the three possible outcomes are cut out if the obstacle represents a bridge over the river or an artifact in the dtm it is in most cases cut out and replaced by an interpolation of the river channel using the approach described in section 3 3 culverts there are however some cases where cutting out would strongly deteriorate the simulations for example if a river passes underground through settled areas it is clearly not meaningful to cut a channel into the dtm and assume the river to run on the surface another example is a river that runs through a small culvert under a retention dam or under another river in all of these examples no cutting out is done instead the respective segments of the river axes are saved to a separate vector dataset and are modeled as culverts in the hydraulic simulations no further treatment if the increment of the height profile represents an actual obstacle e g rock in the river bed no cutting is necessary because the water may pass on the left and the right these cases are rather rare as the axis correction 3 1 tends to avoid these objects 3 3 removal of obstacles in the river channel the obstacle detection 3 2 results in a dataset indicating segments of the river axes where the dtm is no proper representation of the river channel due to some kinds of obstacles in the following step some of these are removed by i interpolating between the valid sections of the river channel ii replacing the obstructed parts with the interpolated channel and iii ensuring a smooth transition between replaced and unchanged dtm the interpolation for one segment is realized based on n densely spaced cross sections c i with i 1 n normal to the river axis each cross section has a height profile h i orig d as sampled from the dtm d is the normal distance from the river axis the first and last cross section c 1 and c n are before and after the obstacle so their respective height profiles are assumed not to be affected by the obstacle for the interpolation the cross sections are transformed to a profile coordinate system which is equivalent to unwrapping the river axis to a straight line mandlburger 2000 mandlburger et al 2011 accordingly all cross sections are now considered parallel the interpolated cross sections h i inter d are obtained by linear interpolation between h 1 orig d and h n orig d obviously h i orig d h i inter d for i 1 n so a smooth transition in the direction of the river axis is given by design in lateral direction this is realized on the cross section level therefore the extent of the river embankment left and right of the river axis given by d l and d r is determined based on the height profile for each cross section c f 3 4 1 for a detailed description of the approach between the river banks only the interpolated heights h i inter d are used outside the river banks plus a margin of a few meters δ the original height profile stays unchanged in the margins on both sides a cosine shaped weight function is applied to smooth the transition c f fig 5 left 9 h i final d h i orig d if d d l δ w l d h i inter d 1 w l d h i orig d if d l δ d d l h i inter d if d l d d r w r d h i inter d 1 w r d h i orig d if d r d d r δ h i orig d if d d r δ where w l d 1 2 cos π d d l δ δ 0 5 and w r d 1 2 cos π d d r δ 0 5 the adapted height profiles h i final d are brought back into their original positions by inverting the transformation to a profile coordinate system an equidistant sampling of the single height profiles leads to a point cloud which is then interpolated to get a local raster model with eliminated obstacles the procedure is concluded by integrating these new raster models for all obstacles into the original dtm example results are visualized in fig 5 3 4 detection of river embankments up to this point rivers were represented by their central axes however for boundary conditions or integration of bathymetric measurements it is important to represent rivers with their full spatial extent the two datasets derived here are the top edge of embankment and the water land boundary fig 6 illustrates these two datasets each one is saved as one vector line pair per river 3 4 1 top edge of embankment the upper edge of embankment is defined as the terrain edge between the river channel and the surrounding topography typically it shows a strong negative curvature due to the flattening above the river banks this idealized form can however not always be expected especially in alpine v shaped valleys no clear delineation of river embankments is possible in this case the geometric criteria are replaced by a height threshold due to the very heterogeneous form and size of rivers the automatic detection is again realized based on cross sections through densified river axis nodes similar to the obstacle detection section 3 2 two step approach is adopted where the embankments are coarsely detected in the first step before refining the search to identify their upper edge at this point it is assumed that the position of the river axis corresponds to the center of the cross section accordingly the first derivation h symm d of the height profile is calculated starting from the center which leads to the left and right river embankment both having clearly positive values furthermore they are expected to be in the transition from a positive curvature in the river bed to a negative curvature towards the top edge consequently the function for coarse embankment detection searches for a high first and a small second derivative by maximizing the function 10 f coarse d h symm d γ h d where γ 0 is a scalar to shift the emphasis between the two derivatives and avoid division by zero it is of course not meaningful to search the whole cross section for maxima of this function especially since eq 10 does not explicitly contain terrain height the height above the river axis is instead introduced as a hard criterion for limiting the search area to certain parts of the cross section this is done by introducing an adaptive height threshold on each side of the river axis separately 1 all parts on one side of the cross section with heights in a certain range above the river axis h s d h axis ε h s d h axis δ h are selected the tolerance ε should be in the range of the height variations expected on the water surface due to interpolation effects and the height range δ h is slightly higher than the maximum expected embankment height e g 15 m 2 the river flood plain height h f on the current side of the cross section is approximately given by h f median h s d in case of v shaped valleys without a flatter foreland h f will be around δ h 2 3 on both sides of the river axis d 0 the lateral search area is limited by the first point d d lim coming close to flood plain height h d lim h axis h f h axis γ with γ depending on the dominating bank and flood plain topography e g γ 0 9 having found a steep part of the river embankment on both sides a lateral margin of several meters outwards is used to limit the search area for fine detection of the top edge of embankment the detection itself is executed by finding minima of a function with the same design as eq 1 however in this case a higher value for β 1 is chosen compared to the application in 3 1 since the top edge of embankment is not necessarily expected to have a first derivative near zero if the distance between cross sections is small enough the top edge of embankment line is simply completed by connecting the detected points of the cross sections finally this line is smoothed e g in gis 3 4 2 water land border compared to the top edge of embankment the delineation of the water land border is not as clear since there is often no obvious geometric manifestation e g flat gravel banks radiometrically the border is theoretically very clear but detection from orthophotos would be limited to large rivers without vegetation on the banks which is a small portion of all rivers in austria therefore it was decided to define the water land border as the lower edge of embankment while applying slightly more restrictive criteria to the embankment detection that way even minor ascents are detected as banks so their lower edge is very likely the water land border with this assumption the workflow for the water land border is very similar to the previous one for the top edge of embankment 3 4 1 first a coarse detection is run to find the river banks using eq 10 the second step is to refine the search this time inwards i e between banks and river axis the lower edge of embankment is detected by finding the strongest curvature in the search area by maximizing h d 3 5 integration of the river bed into the dtm the dtm discussed above represents water by the water surface if at all in order to make it suitable for hydraulic modelling river bed information from other data sources has to be integrated a common river bed representation are measured height profiles normal to the river axis in this case the concept of transforming cross sections to a profile coordinate system c f 3 3 was used to densify the measurements along the river axis via interpolation alternatively mesh terrain data such as sms or the mesh containing the trapezoidal profiles were available these were converted to regular raster data with linear interpolation for river reaches without prior information about river bed geometry a trapezoidal shaped cross sectional profile of the river was assumed the width of the trapezoid was based on the distance between the water land borders its depth was calculated with a fixed point iteration from the hydrological discharges using the manning formula buttinger kreuzhuber et al 2021 depending on the reliability and the extent of the river bed representation the water land border or the top edge of embankment serve as boundaries between the different datasets meaning that the profile data replace the dtm between the river banks whereas the original dtm remains unchanged for the flood plain 4 results and discussion the methods described in section 3 were applied to datasets covering all of austria as the basis of flood hazard mapping c f section 2 the automatic procedures were supplemented by manual validation and adaption at some points of the workflow to ensure highest reliability of the results table 1 summarizes the most important outputs as well as the respective degree of automatization overall the vector datasets required increased manual interaction compared to the raster datasets this has to do with i their content partly being subject to interpretation and ii their importance as intermediate products the latter is also a prerequisite for the high automatization of raster processing the river network has a key role in the workflow all further products essentially depend on the correctness of the river network i e the river lines being at least inside the water land borders according to the dtm this is largely ensured by the automatic correction procedure however the procedure fails in cases of poor approximations or if the river channel is not clearly observable in the dtm due to long culverts poor interpolation of gaps over water bodies etc extensive manual validation and correction ensured that the assumption of a correct river network holds and thus all further working steps could be automatized more reliably manual checking for both obstacle and embankment detection was then limited to distinct automatically detected positions fig 7 shows the results for two example scenes the river axes are correctly positioned inside the river channels however they don t always take the most direct route but tend to show slight zigzag patterns this has to do with the strong emphasis of the correction procedure on avoiding ascents in downstream direction as a consequence small detours are accepted in order to avoid undulating parts of the river channels in the dtm the top edge of embankment and the water land border confine the river banks very well nevertheless the latter does not always exactly limit the water body the orthophoto in fig 7b indicates that there are also gravel banks included as long as these are flat enough to not clearly stand out against the water surface geometrically fig 7a also shows two eliminated bridges in the dtm furthermore the two most frequent use cases of culverts are illustrated these are a cases where cutting would obviously deteriorate the hydraulic simulations and b overbuilt rivers through cities apart from these local quality assessments also more extensive validation of the spatial data was carried out although there are some limitations concerning the availability of ground truth the final flood hazard maps were validated with good overall results on a country scale against local flood hazard maps buttinger kreuzhuber et al 2021 these comparisons also provide an implicit quality assessment of the spatial datasets nevertheless the assessment is rather indirect since i the hydraulic simulations also use several other inputs and ii the local hazard maps also considered detailed information e g mobile flood control facilities not available at the national scale separate validation of spatial datasets is possible with some assumptions for instance the exhaustively checked and corrected river network can be considered ground truth in terms of being situated in the river channel even though the dtm does not contain enough information to enable a unique river thalweg definition the final river axes can be expected to be located in between the river embankments in this respect quality checking of the automatic correction procedure can be realized by intersecting the river axes with the final water land borders after manual corrections to this end the water land boundary line pairs were closed to river polygons so that each polygon represents the water body of one river the original river axes r orig and the river axes after the automatic correction r corr were converted to point shp files with each point representing one vertex of the line shp files the correctness of a river axis was then defined as the percentage of points within the respective river polygon the correctness value was calculated both for the complete river network and the unobstructed parts alone the latter excludes all river segments in the immediate vicinity of bridges and culverts by design the manually corrected river network has a correctness of 100 since it is the basis of the water land border detection c f section 3 4 2 results for the other two stages of the river network correction can be found in table 2 even though the automatic river network correction improves position correctness substantially there remain unsatisfactory results for about 2 3 of all unobstructed river segments the most common reasons are poor approximations the correction procedure implicitly limits the search space for the correct river bed position via cross section widths e g 100 m to both sides of the original river axis additionally small deviations from the original axis are slightly preferred over larger ones eq 3 section 3 1 this leads to a potential failure of the correction in case of exceptionally poor approximations unanticipated complexity in some cases meandering rivers are strongly simplified in the original dataset even though the axes are densified before correction the procedure is sometimes not able to recover the full complexity which is to some degree related to the next point dtm river channel representation as mentioned in section 2 als data acquisition usually results in measurement gaps for a large part of water surfaces closing these gaps leads to artifacts in the river channel especially if the banks are vegetated due to these effects the true river channel is probably not the cheapest path instead a corrected river line running through smooth terrain a few meters off the actual river channel may lead to clearly less ascending slopes and is therefore preferred by the automatic correction error propagation long underground sections e g culverts hamper river channel detection from the dtm this can also have an impact further downstream beyond the directly affected reach if the river channel is bordered by ridges the corrected axis is not able to immediately return to the channel without uphill slopes once it is outside c f eq 7 in section 3 1 the detected obstacles in the river course can also be verified wherever they correspond to actual bridges therefore a road and path layer from open street map was intersected with all river axes resulting in approximately 52000 points for all of austria these crossings between rivers and traffic routes were then assessed as to whether potential obstructions of the river channel e g bridges could be detected and eliminated of the 52000 intersections about 22000 were detected as obstacles or culverts the remaining almost 30000 points are largely related to bridges that had already been eliminated from the original dtm or tunnels in order to find meaningful quality measures these points are manually assessed for four federal states vorarlberg and salzburg have predominantly alpine character whereas burgenland and vienna are rather flat the latter with mostly urban characteristics in total for 122 intersections 1 7 the dtm still contains bridges or perceptible remains of bridges after elimination more details can be found in fig 8 the undetected bridges are mainly located in the upper river reaches the detail view in the center of fig 8 represents one typical example the channel of the small river is not adequately resolved in the dtm consequently the crossing road only creates a plateau in the height profile but there is no ascent in downstream direction and the bridge is not detected in all the four states there was no remaining bridge over large rivers there are however remarkable differences concerning the ratio between bridges already eliminated in the original dtm blue and the bridges detected and eliminated in our workflow green this clearly shows the heterogeneity of the dtm due to different processing chains in the individual states validation of the other vector datasets is limited to thorough visual examination e g fig 7 since there is no extensive comparable ground truth at a similar spatial resolution these datasets were created for a very specific application and thus do not always have a clear correspondence with the dtm for example river banks in steep alpine valleys often do not have a geometric pattern as expected for the top edge of embankment however if there is a clear top edge of embankment detection is usually more reliable than for the water land borders because the latter very much depend on a smooth and artifact free representation of the river channel especially the water surface concerning the raster products spot checks were conducted to assess output quality the obstacle elimination proved to work exceptionally well in terms of smoothly interpolating the obstructed river channel without leaving artifacts c f figs 5 and 7a integration of river bed measurements mainly depends on the quality and compatibility e g height system of the respective bathymetric data overall the results demonstrate that processing of high resolution spatial data for hydraulic modelling is no longer restricted to small study areas the developed methods are able to provide countrywide datasets based on a 1 m dtm with local relevance at these spatial extents a high degree of automatization is essential however similarly to prior studies c f section 1 2 we conclude that fully satisfactory results still require some level of checking and corrections by a human operator 5 conclusions we present a methodology to adapt extensive high resolution spatial datasets to the requirements of hydraulic modelling for flood hazard mapping the aim of these methods is to ensure suitable and consistent representation of a river network and the surrounding area with a high degree of automatization this includes the elimination of discrepancies between datasets e g river network vs dtm an improved river channel description in the dtm obstacle removal bathymetry integration as well as the creation of new datasets such as culverts and embankment lines the workflow was designed in a way to make its parts as independent as independent of each other as possible in order to facilitate the isolated execution of single steps and enhance planning flexibility furthermore a focus is on high adaptability with respect to different river sizes and dtm properties which is achieved by purely geometric cross section based processing all methods were applied to data covering the whole territory of austria with 33880 river km on a total area of about 84000 km 2 the automatic correction increased the percentage of river axes matching the dtm river channel from about 85 to 97 after automatic detection and elimination more than 98 of all bridges do no longer affect the river bed representation nevertheless a certain degree of manual interaction is necessary for achieving the full reliability due to inconsistent input data and artifacts in the dtm manual checks at an early stage of the processing chain e g checking the complete corrected river network substantially reduces the required manual efforts in later working steps derivation of the original dtm with particular emphasis on a sound mapping of river channels could also likely improve the results of automatic procedures another promising possibility is processing directly based on point clouds this would however require significantly more computational resources due to the rapid progress of surveying techniques especially als and computational possibilities we also expect an increasing demand for locally relevant flood risk modelling on regional and country scales accordingly scalable and automatable procedures to adapt spatial data will be needed our proposed methodology has been shown to work on a large challenging dataset for a very specific application while the approach was not extensively tested for other datasets outside austria we still expect the method to adapt well to comparable data since it provided consistent quality over all federal states despite heterogeneous inputs most correction functions contain tuning parameters which can be calibrated in order to flexibly adapt to specific topographic circumstances the applicability mainly depends on whether the underlying basic assumptions concerning geometry and input data hold c f section 3 the methodology could also be used for a dtm with different spatial resolution as long as it is high enough to resolve the river channels of interest credit authorship contribution statement michael h wimmer conceptualization methodology software validation formal analysis investigation data curation writing original draft visualization markus hollaus conceptualization writing review editing supervision günter blöschl writing review editing project administration funding acquisition andreas buttinger kreuzhuber validation writing review editing jürgen komma writing review editing jürgen waser validation project administration norbert pfeifer conceptualization resources writing review editing supervision project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement financial support from the austrian federal ministry of agriculture regions and tourism and the fwf vienna doctoral programme on water resource systems dk w1219 n28 is acknowledged vrvis is funded by bmk bmdw styria sfg tyrol and vienna business agency in the scope of comet competence centers for excellent technologies 879730 which is managed by ffg the authors acknowledge tu wien bibliothek for financial support through its open access funding programme 
8565,increasing river floods and infrastructure development in many parts of the world have created an urgent need for accurate high resolution flood hazard mapping for more efficient flood risk management mapping accuracy hinges on the quality of the underlying digital terrain model dtm and other spatial datasets this article presents a processing strategy to ensure consistent adaption of countrywide spatial datasets to the requirements of hydraulic modelling the suggested methods are automatized to a large extent and include i automatic fitting of river axis positions to the dtm ii detection of culverts and obstacles in the river channel iii smooth elimination of obstacles by interpolation along the river axes iv geometric detection of water land borders and the top edge of embankments for v integration of the submerged river bed geometry into the dtm the processing chain is applied to a river network 33880 km and a dtm from airborne laser scanning als with 1 m spatial resolution covering the entire territory of austria 84000 km 2 thus countrywide consistency of data and methods is achieved along with high local relevance semi automatic validation and extensive manual checks demonstrate that processing significantly improves the dtm with respect to topographic and hydraulic consistency however some open issues of automatic processing remain e g in case of long underground river reaches keywords dem topographic data hydraulic modelling flood hazard mapping high resolution 1 introduction 1 1 background and motivation the last years and decades have seen numerous extreme river flooding events in europe which have caused damage to persons and infrastructure changes of the frequency and magnitude of floods have been observed on a continental scale blöschl et al 2015 and globally douben 2006 najibi and devineni 2018 ceola et al 2014 among other factors climate change has been identified as an important driver bertola et al 2021 blöschl et al 2019 alfieri et al 2017 these changes require authorities to extend and adapt flood protection measures in their respective administrative domains citizens and other stakeholders such as insurance companies also have strong interest in accurate flood risk assessments accordingly there is a need for detailed and up to date flood hazard maps based on hydraulic simulations while in the past computational requirements have limited the use of high resolution data to small areas this is no longer the case horváth et al 2016 buttinger kreuzhuber et al 2021 using region or country wide data at a resolution of meters or even below has become feasible hydraulic simulations require spatial data as inputs including digital terrain models dtms for specifying the geometry of the flow domain and vector data of the river network for specifying boundary conditions e g inflow hydrographs these datasets in their original forms tend to be inconsistent due to different acquisition techniques and temporal decorrelation additionally dtms are often created for other purposes and thus do not initially fulfill all requirements of hydraulic modelling however dtm quality and resolution has massive impacts on further processing results chaplot 2005 lin et al 2010 sørensen and seibert 2007 xu et al 2016 the aim of this paper is to develop a procedure that prepares and augments extensive spatial datasets to make them suitable inputs for flood modelling given potentially very large datasets the most crucial question is the possible degree of automatization while still reliably adapting to different landscape forms and river sizes 1 2 state of the art and contributions due to the importance of flood risk mapping there is extensive literature on the acquisition and preparation of dedicated spatial data recently more and more studies use spatial data with high resolutions of around a meter or less for flood modelling these are often based on airborne laser scanning als data muhadi et al 2020 mandlburger et al 2011 woodrow et al 2016 but also on other techniques such as photogrammetry from unmanned aerial vehicles hashemi beni et al 2018 because of the rather costly data acquisition per area and the computational requirements many of these studies cover small areas e g individual river reaches or catchments up to several 100 km 2 thus extensive manual editing as part of the workflow is not of major concern since the effort remains low when it comes to larger areas such as whole provinces or countries there are hybrid approaches locally refining an extensive low resolution height model in settled areas where a higher level of detail is needed leitão and de sousa 2018 several studies compare and evaluate the implications of using dtms or more generally digital elevation models dems with different spatial resolutions mohanty et al 2020 muthusamy et al 2021 saksena and merwade 2015 generally investigations on a regional or continental scale usually rely on dems with comparably coarse spatial resolution hutchinson and dowling 1991 alfieri et al 2014 fleischmann et al 2019 jarihani et al 2015 li et al 2020 e g from srtm yan et al 2019 lehner et al 2008 geoscience australia 2015 or from tandem x mason et al 2016 the position of the drainage networks and river valley lines is thereby computed with flow routing algorithms based on the respective elevation model tribe 1992 rieger 1993 soille et al 2003 hou et al 2011 poggio and soille 2012 the procedure is very sensitive to pits in the dem so these have to be removed beforehand soille et al 2003 tianqi et al 2003 grimaldi et al 2007 poggio and soille 2012 some existing software packages such as anudem hutchinson 2011 cover the respective processing chains in case of using higher resolution dtms additional issues have to be taken into account most notably surface roughness lindsay et al 2019 and infrastructure or other features obstructing possible underpasses thereby creating erroneous pitches barber and shortridge 2005 the latter is often solved with depression breaching which avoids negative influence of an obstacle on flow routing without geometrically removing it from the dtm rieger 1998 soille et al 2003 lindsay and dhun 2015 the specific task of identifying and eliminating bridges obstructing the river course was treated in sithole and vosselman 2006 apart from that also the detection and description of river embankments from dems is covered in the literature e g krüger and meinel 2008 köthe and bock 2009 casas et al 2012 sofia et al 2014 steinfeld et al 2013 provide a comparison with image based detection approaches the focus of these mostly local studies is on the detection of levees and other anthropogenic features clearly elevated over the floodplains the main contribution of this study is the combination of a high level of detail with large spatial coverage our aim is to design execute and evaluate methods to process heterogeneous 1 m grid width terrain elevation data and heterogeneous stream line data to provide a compound homogeneous dataset ready for hydraulic simulation for an entire country austria the methodology intends to include large streams as well as small less regulated rivers given by a pre existing vector river network which is preserved in terms of topology throughout all geometric corrections and adaptions 2 data and context the developed workflow was applied to datasets covering the whole territory of austria 84 000 km 2 with a river network of 33880 km as part of the project hora 3 0 the goal of this project was to create country wide high resolution flood hazard maps for different return periods based on instationary 2d hydraulic simulations buttinger kreuzhuber et al 2021 waser et al 2011 visdom 2021 accordingly the project area corresponds to the whole territory of austria along with the immediate border regions c f fig 1 except for a couple of rivers along the northern border it mainly contains catchment areas of two rivers the rhein vorarlberg and the danube rest of austria with its tributaries mur and drau south of the main alpine ridge elevations vary between slightly over 100 m in the pannonian basin close to the eastern border and nearly 3800 m in the central eastern alps for the complete area a dtm and a digital surface model dsm with a spatial resolution of 1 m was available the underlying point clouds for these models were acquired in various airborne laser scanning als campaigns between the years 2003 and 2013 by the austrian federal states therefore inconsistencies are to be expected due to both temporal decorrelation and different terrain modelling workflows the most important vector dataset is the river network line shp file as used by the austrian administrative authorities similarly to the raster models the nationwide dataset has been put together from separate datasets acquired by the federal states due to different acquisition techniques level of detail and accuracy of the river axes vary between federal states but often also depend on river size the total length of all river axes amounts to approximately 40 000 km however only those with a total catchment area of at least 10 km 2 are considered in this work altogether 2735 river lines other important country wide vector datasets used in the project are land use and buildings from the digital cadastral map as well as lakes and power plants additionally local datasets such as measured river cross sections or other bathymetric data served as inputs to be integrated into the dtm c f 3 5 2 1 requirements besides the need for consistent and accurate spatial data there are also some specific formal requirements these come from the model and the framework for hydraulic modelling in order to ensure compatibility and computationally efficient processing the hydraulic model used in flood hazard mapping is based on the shallow water equations swes c f buttinger kreuzhuber et al 2019 one popular numerical discretization of the swes is the finite volume method where the variables water depth and discharge are represented by their averages on the computational grid efficient and robust numerical schemes together with massive parallelization e g on gpus enable the processing of large areas at high spatial resolution horváth et al 2016 however the accuracy of the hydraulic model hinges on the accuracy of the input data including the hydrological flood discharges and the dtm as the simulated inundated areas tend to be very sensitive to the dtm a precise and reliable dtm is of paramount importance in this study the dtm is provided in 2 5d so it can be expressed as a function f z f x y assigning exactly one height value z to every position coordinates x y thus we assume that the domain of the function f is simply connected if this is not the case gap filling has to be applied first it is furthermore required to be consistent throughout the simulation domain consistency includes the removal of artifacts from measurement and interpolation errors in fact not only errors in the dtm acquisition can lead to problematic results of the simulations but also actually existing objects such as bridges which can potentially act as incorrect barriers since the 2 5d representation is not able to map 3d underpasses correctly upstream of the barrier the water levels are overestimated while inundated areas are underestimated downstream these kind of objects have to be removed from the dtm ensuring that the water flow along the river is not erroneously restricted however there are exceptions for culverts under settled areas under other water bodies and retention dams since in these cases cutting the dtm would not lead to meaningful results a separate dataset holding the respective river segments was created instead additionally many common dtm acquisition techniques are not capable of providing bathymetric data for example topographic airborne laser scanning als of water bodies either leads to void areas or to data representing the water surface this is because inundated parts of the river bed can not be measured due to the absorption of infrared light in the water in order to fully represent the river bed topography bathymetric data from other sources were integrated which resulted in a hydrologically enforced dtm dtm h another crucial input for the hydraulic simulation are hydraulic boundary conditions bcs at the origin of a river and wherever a river enters or leaves the simulation domain an inflow or outflow bc has to be specified the locations and cross sectional widths of the in and outflows were derived from the river center lines and the river banks alignment of these vector datasets with the river course in the dtm ensures a correct application of the bcs the inflow outflow hydrographs themselves were estimated by hydraulic modelling 3 methods a common property of nearly all spatial datasets is their heterogeneity c f section 2 processing thus has to ensure compatibility between existing datasets before deriving new ones from them the raster dtm has a central role in the workflow as it covers the entire domain in order to make it processable the dtm is organized in tiles of 10 km by 10 km the most important input vector dataset is the river network representing the axis of each river as one line feature since most further working steps are based on a combination of dtm and river network discrepancies between these two datasets have to be corrected this is realized by adapting river axes to the geometric properties of the dtm section 3 1 subsequently a proper dtm representation of the river courses is ensured by finding and eliminating obstacles sections 3 2 and 3 3 assuming an unobstructed river bed representation river embankments were detected based on the geometric cross section properties section 3 4 these enable the application of boundary conditions and the integration of river bed geometry into the dtm to get a dtm h section 3 5 fig 2 provides an overview of the methodology which is explicitly described in the following subsections processing and analysis were carried out primarily using the software packages opals pfeifer et al 2014 python matlab and qgis 3 1 positional correction of the river network the original dtm and river network datasets show clear geometric discrepancies meaning that the river axes as indicated by the shape file only partly overlap with the river bed according to the dtm harmonization of these two datasets has highest priority since all following working steps 3 2 3 5 rely on it the reasons for these discrepancies are mainly temporal decorrelation and too coarse digitization of river lines the latter especially for minor rivers the assumption that the river network is the dominating source of discrepancies is confirmed by the fact that deviations often amount to several meters this is significantly larger than the accuracy of als data which normally does not exceed a few decimeters kraus 2007 pfeifer and briese 2014 accordingly it was decided to correct the river axes while keeping the dtm unchanged another key argument for always correcting river axes is that changing a 2d line geometry depends on far less assumptions than changing the whole river bed river banks and their flood plain in the dtm since approximate river axis positions were already available a sequential cross section based approach was developed for the spatial correction the node structure of the river network was densified to ensure a maximum node to node distance is not exceeded for each node a cross section normal to the original river axis was derived fig 3 4 the height profile along these cross sections was sampled from the dtm and smoothed this function is called h d where d is the signed distance orthogonal to the river axis the actual correction starts from the first point of each river axis i e the source in case of natural rivers and proceeds in downstream direction for each cross section six weight functions are formulated in order to account for several geometric criteria the river axis position is then corrected to where the maximum of the product over all weight functions appears in the following the individual weight functions are described in detail they are based on geometric and hydraulic reasoning as well as on test analyses over limited areas cross section form the center of the river bed is expected to be a local height minimum of h d the weight function therefore is essentially a ratio of the second derivative h d and the first derivative h d 1 w 1 d h d β 1 h d where β 1 has the purpose of avoiding division by zero and shifting the balance between the influence of first and second derivative this function is maximized in case of a clearly positive h d and with h d close to zero which are both given at local height minima height dependency at least in a local neighbourhood lower points are more likely to be located in the river bed and therefore receive a higher weight the related weight function was designed with an exponential expression which enables adjusting the influence of this criterion by changing the base β 2 with β 2 1 2 w 2 d β 2 mean h d h d all points above the mean height thus get a weight between 0 and 1 while the points below have clearly higher weights initial approximation in most cases the original river axis is not more than several meters off the real river bed according to the dtm making it a viable coarse approximation therefore a gaussian function centered in the middle of the cross section is added as another weight function 3 w 3 d g d σ 3 0 4 g d σ λ 1 σ 2 π e d λ 2 2 π 2 compared to the other criteria w 3 is less discriminative it mainly plays a decisive role if other weight functions are similarly high for large parts of the cross section e g wide flat river beds river axis prediction another approximation for the river axis position is found by estimating a cubic function through the prior four already corrected positions the intersection d i of that extrapolated river course with the current cross section is used as the center point of another gaussian function c f eq 4 5 w 4 d g d σ 4 d pred the value of σ 4 was chosen slightly smaller than σ 3 in order to act as a counterbalance to w 3 eq 3 for bad original river axes water flow properties water tends to flow along direct paths without ascents thus further weight functions are introduced to prefer a short horizontal path from the previous cross section w 5 and a cheap path in the sense of having no significant ascent on the way w 6 both weight functions are again formulated in a similar way as w 2 eq 2 the exponent of w 5 depends on the horizontal 2d distances s d from the previous river axis point a difference from the mean value is chosen in order not to make the criterion too discriminative all distances above the mean value result in weights between 1 and β 5 6 w 5 d β 5 mean s d s d for w 6 all positive height differences along the horizontal path are accumulated δ h cum d due to dtm accuracy and resolution δ h cum d tends to be slightly above 0 in practice a threshold δ h max is introduced to assign weight 0 to all points exceeding a tolerable cumulative ascent the threshold is determined depending on dtm properties as well as on the distance between cross sections it makes w 6 a very discriminative criterion since it usually excludes all points outside the river bed even if they are lower in elevation 7 w 6 d β 6 δ h cum d if δ h cum d δ h max 0 else moreover an option was added to skip a cross section in case it contains no single point that can be reached without major height gains on the path δ h cum d δ h max d that way detours around erroneous obstacles in the river bed c f section 3 2 are avoided finally all weight functions are combined by multiplying them 8 w final k 1 6 w k the multiplication ensures that exclusion w k 0 of a certain cross section part due to one single criterion cannot be overruled by others if the resulting function has a maximum larger than 0 the corresponding position d max in the cross section is introduced as a new point of the corrected river axis and the same procedure is repeated for the next downstream cross section 3 2 detection of obstacles bridges and culverts after ensuring correspondence between the river network and the dtm it is still not guaranteed to have monotonously falling height profiles along the river lines significant local height increments are caused by different kinds of obstacles in the dtm representation of the river these obstacles can be existing objects such as bridges or other buildings many of them also originate from interpolation effects since the water surfaces are often insufficiently covered by als point clouds especially forested river banks tend to favour discontinuous representations of the water surface these obstacles are problem for simulations using 2 5 d dtms which do not allow the modelling of full 3d information e g bridge underpasses consequently all objects above the river bed act like dams and therefore have to be detected and then removed or treated explicitly the detection is realized here using the height profile along the corrected river axes as sampled from the dtm after smoothing the ascending and descending hill of a potential obstacle is detected as a clear local maximum of the first derivative followed by a clear local minimum within a certain distance c f fig 3 2 c clear is intended to imply that the absolute values of the extrema exceed a certain threshold an additional criterion is that the height after the obstacle has to be lower than before however elimination should not start at the steepest slope but at the point where the obstacle effectively starts changing the height profile so after having detected an obstacle based on the first derivative a subsequent fine detection is run in the surrounding section of the height profile using the same combination of first and second derivative as described in eq 1 c f fig 3 2 d the value for β 1 is chosen to be very small giving the first derivative a strong weight this means local minima in the height profile are identified these local minima are the points of the river axis where the height profile stops fulfilling the expectation of monotonicity additionally a small margin is added before and after the obstacle to ensure a proper channel representation when removing obstacles c f 3 3 finally a decision has to be made how to proceed with the detected obstacle although there are some objective criteria the final decision was made manually by an operator using the dtm and optionally other datasets such as orthophoto dsm and the cadastral map the three possible outcomes are cut out if the obstacle represents a bridge over the river or an artifact in the dtm it is in most cases cut out and replaced by an interpolation of the river channel using the approach described in section 3 3 culverts there are however some cases where cutting out would strongly deteriorate the simulations for example if a river passes underground through settled areas it is clearly not meaningful to cut a channel into the dtm and assume the river to run on the surface another example is a river that runs through a small culvert under a retention dam or under another river in all of these examples no cutting out is done instead the respective segments of the river axes are saved to a separate vector dataset and are modeled as culverts in the hydraulic simulations no further treatment if the increment of the height profile represents an actual obstacle e g rock in the river bed no cutting is necessary because the water may pass on the left and the right these cases are rather rare as the axis correction 3 1 tends to avoid these objects 3 3 removal of obstacles in the river channel the obstacle detection 3 2 results in a dataset indicating segments of the river axes where the dtm is no proper representation of the river channel due to some kinds of obstacles in the following step some of these are removed by i interpolating between the valid sections of the river channel ii replacing the obstructed parts with the interpolated channel and iii ensuring a smooth transition between replaced and unchanged dtm the interpolation for one segment is realized based on n densely spaced cross sections c i with i 1 n normal to the river axis each cross section has a height profile h i orig d as sampled from the dtm d is the normal distance from the river axis the first and last cross section c 1 and c n are before and after the obstacle so their respective height profiles are assumed not to be affected by the obstacle for the interpolation the cross sections are transformed to a profile coordinate system which is equivalent to unwrapping the river axis to a straight line mandlburger 2000 mandlburger et al 2011 accordingly all cross sections are now considered parallel the interpolated cross sections h i inter d are obtained by linear interpolation between h 1 orig d and h n orig d obviously h i orig d h i inter d for i 1 n so a smooth transition in the direction of the river axis is given by design in lateral direction this is realized on the cross section level therefore the extent of the river embankment left and right of the river axis given by d l and d r is determined based on the height profile for each cross section c f 3 4 1 for a detailed description of the approach between the river banks only the interpolated heights h i inter d are used outside the river banks plus a margin of a few meters δ the original height profile stays unchanged in the margins on both sides a cosine shaped weight function is applied to smooth the transition c f fig 5 left 9 h i final d h i orig d if d d l δ w l d h i inter d 1 w l d h i orig d if d l δ d d l h i inter d if d l d d r w r d h i inter d 1 w r d h i orig d if d r d d r δ h i orig d if d d r δ where w l d 1 2 cos π d d l δ δ 0 5 and w r d 1 2 cos π d d r δ 0 5 the adapted height profiles h i final d are brought back into their original positions by inverting the transformation to a profile coordinate system an equidistant sampling of the single height profiles leads to a point cloud which is then interpolated to get a local raster model with eliminated obstacles the procedure is concluded by integrating these new raster models for all obstacles into the original dtm example results are visualized in fig 5 3 4 detection of river embankments up to this point rivers were represented by their central axes however for boundary conditions or integration of bathymetric measurements it is important to represent rivers with their full spatial extent the two datasets derived here are the top edge of embankment and the water land boundary fig 6 illustrates these two datasets each one is saved as one vector line pair per river 3 4 1 top edge of embankment the upper edge of embankment is defined as the terrain edge between the river channel and the surrounding topography typically it shows a strong negative curvature due to the flattening above the river banks this idealized form can however not always be expected especially in alpine v shaped valleys no clear delineation of river embankments is possible in this case the geometric criteria are replaced by a height threshold due to the very heterogeneous form and size of rivers the automatic detection is again realized based on cross sections through densified river axis nodes similar to the obstacle detection section 3 2 two step approach is adopted where the embankments are coarsely detected in the first step before refining the search to identify their upper edge at this point it is assumed that the position of the river axis corresponds to the center of the cross section accordingly the first derivation h symm d of the height profile is calculated starting from the center which leads to the left and right river embankment both having clearly positive values furthermore they are expected to be in the transition from a positive curvature in the river bed to a negative curvature towards the top edge consequently the function for coarse embankment detection searches for a high first and a small second derivative by maximizing the function 10 f coarse d h symm d γ h d where γ 0 is a scalar to shift the emphasis between the two derivatives and avoid division by zero it is of course not meaningful to search the whole cross section for maxima of this function especially since eq 10 does not explicitly contain terrain height the height above the river axis is instead introduced as a hard criterion for limiting the search area to certain parts of the cross section this is done by introducing an adaptive height threshold on each side of the river axis separately 1 all parts on one side of the cross section with heights in a certain range above the river axis h s d h axis ε h s d h axis δ h are selected the tolerance ε should be in the range of the height variations expected on the water surface due to interpolation effects and the height range δ h is slightly higher than the maximum expected embankment height e g 15 m 2 the river flood plain height h f on the current side of the cross section is approximately given by h f median h s d in case of v shaped valleys without a flatter foreland h f will be around δ h 2 3 on both sides of the river axis d 0 the lateral search area is limited by the first point d d lim coming close to flood plain height h d lim h axis h f h axis γ with γ depending on the dominating bank and flood plain topography e g γ 0 9 having found a steep part of the river embankment on both sides a lateral margin of several meters outwards is used to limit the search area for fine detection of the top edge of embankment the detection itself is executed by finding minima of a function with the same design as eq 1 however in this case a higher value for β 1 is chosen compared to the application in 3 1 since the top edge of embankment is not necessarily expected to have a first derivative near zero if the distance between cross sections is small enough the top edge of embankment line is simply completed by connecting the detected points of the cross sections finally this line is smoothed e g in gis 3 4 2 water land border compared to the top edge of embankment the delineation of the water land border is not as clear since there is often no obvious geometric manifestation e g flat gravel banks radiometrically the border is theoretically very clear but detection from orthophotos would be limited to large rivers without vegetation on the banks which is a small portion of all rivers in austria therefore it was decided to define the water land border as the lower edge of embankment while applying slightly more restrictive criteria to the embankment detection that way even minor ascents are detected as banks so their lower edge is very likely the water land border with this assumption the workflow for the water land border is very similar to the previous one for the top edge of embankment 3 4 1 first a coarse detection is run to find the river banks using eq 10 the second step is to refine the search this time inwards i e between banks and river axis the lower edge of embankment is detected by finding the strongest curvature in the search area by maximizing h d 3 5 integration of the river bed into the dtm the dtm discussed above represents water by the water surface if at all in order to make it suitable for hydraulic modelling river bed information from other data sources has to be integrated a common river bed representation are measured height profiles normal to the river axis in this case the concept of transforming cross sections to a profile coordinate system c f 3 3 was used to densify the measurements along the river axis via interpolation alternatively mesh terrain data such as sms or the mesh containing the trapezoidal profiles were available these were converted to regular raster data with linear interpolation for river reaches without prior information about river bed geometry a trapezoidal shaped cross sectional profile of the river was assumed the width of the trapezoid was based on the distance between the water land borders its depth was calculated with a fixed point iteration from the hydrological discharges using the manning formula buttinger kreuzhuber et al 2021 depending on the reliability and the extent of the river bed representation the water land border or the top edge of embankment serve as boundaries between the different datasets meaning that the profile data replace the dtm between the river banks whereas the original dtm remains unchanged for the flood plain 4 results and discussion the methods described in section 3 were applied to datasets covering all of austria as the basis of flood hazard mapping c f section 2 the automatic procedures were supplemented by manual validation and adaption at some points of the workflow to ensure highest reliability of the results table 1 summarizes the most important outputs as well as the respective degree of automatization overall the vector datasets required increased manual interaction compared to the raster datasets this has to do with i their content partly being subject to interpretation and ii their importance as intermediate products the latter is also a prerequisite for the high automatization of raster processing the river network has a key role in the workflow all further products essentially depend on the correctness of the river network i e the river lines being at least inside the water land borders according to the dtm this is largely ensured by the automatic correction procedure however the procedure fails in cases of poor approximations or if the river channel is not clearly observable in the dtm due to long culverts poor interpolation of gaps over water bodies etc extensive manual validation and correction ensured that the assumption of a correct river network holds and thus all further working steps could be automatized more reliably manual checking for both obstacle and embankment detection was then limited to distinct automatically detected positions fig 7 shows the results for two example scenes the river axes are correctly positioned inside the river channels however they don t always take the most direct route but tend to show slight zigzag patterns this has to do with the strong emphasis of the correction procedure on avoiding ascents in downstream direction as a consequence small detours are accepted in order to avoid undulating parts of the river channels in the dtm the top edge of embankment and the water land border confine the river banks very well nevertheless the latter does not always exactly limit the water body the orthophoto in fig 7b indicates that there are also gravel banks included as long as these are flat enough to not clearly stand out against the water surface geometrically fig 7a also shows two eliminated bridges in the dtm furthermore the two most frequent use cases of culverts are illustrated these are a cases where cutting would obviously deteriorate the hydraulic simulations and b overbuilt rivers through cities apart from these local quality assessments also more extensive validation of the spatial data was carried out although there are some limitations concerning the availability of ground truth the final flood hazard maps were validated with good overall results on a country scale against local flood hazard maps buttinger kreuzhuber et al 2021 these comparisons also provide an implicit quality assessment of the spatial datasets nevertheless the assessment is rather indirect since i the hydraulic simulations also use several other inputs and ii the local hazard maps also considered detailed information e g mobile flood control facilities not available at the national scale separate validation of spatial datasets is possible with some assumptions for instance the exhaustively checked and corrected river network can be considered ground truth in terms of being situated in the river channel even though the dtm does not contain enough information to enable a unique river thalweg definition the final river axes can be expected to be located in between the river embankments in this respect quality checking of the automatic correction procedure can be realized by intersecting the river axes with the final water land borders after manual corrections to this end the water land boundary line pairs were closed to river polygons so that each polygon represents the water body of one river the original river axes r orig and the river axes after the automatic correction r corr were converted to point shp files with each point representing one vertex of the line shp files the correctness of a river axis was then defined as the percentage of points within the respective river polygon the correctness value was calculated both for the complete river network and the unobstructed parts alone the latter excludes all river segments in the immediate vicinity of bridges and culverts by design the manually corrected river network has a correctness of 100 since it is the basis of the water land border detection c f section 3 4 2 results for the other two stages of the river network correction can be found in table 2 even though the automatic river network correction improves position correctness substantially there remain unsatisfactory results for about 2 3 of all unobstructed river segments the most common reasons are poor approximations the correction procedure implicitly limits the search space for the correct river bed position via cross section widths e g 100 m to both sides of the original river axis additionally small deviations from the original axis are slightly preferred over larger ones eq 3 section 3 1 this leads to a potential failure of the correction in case of exceptionally poor approximations unanticipated complexity in some cases meandering rivers are strongly simplified in the original dataset even though the axes are densified before correction the procedure is sometimes not able to recover the full complexity which is to some degree related to the next point dtm river channel representation as mentioned in section 2 als data acquisition usually results in measurement gaps for a large part of water surfaces closing these gaps leads to artifacts in the river channel especially if the banks are vegetated due to these effects the true river channel is probably not the cheapest path instead a corrected river line running through smooth terrain a few meters off the actual river channel may lead to clearly less ascending slopes and is therefore preferred by the automatic correction error propagation long underground sections e g culverts hamper river channel detection from the dtm this can also have an impact further downstream beyond the directly affected reach if the river channel is bordered by ridges the corrected axis is not able to immediately return to the channel without uphill slopes once it is outside c f eq 7 in section 3 1 the detected obstacles in the river course can also be verified wherever they correspond to actual bridges therefore a road and path layer from open street map was intersected with all river axes resulting in approximately 52000 points for all of austria these crossings between rivers and traffic routes were then assessed as to whether potential obstructions of the river channel e g bridges could be detected and eliminated of the 52000 intersections about 22000 were detected as obstacles or culverts the remaining almost 30000 points are largely related to bridges that had already been eliminated from the original dtm or tunnels in order to find meaningful quality measures these points are manually assessed for four federal states vorarlberg and salzburg have predominantly alpine character whereas burgenland and vienna are rather flat the latter with mostly urban characteristics in total for 122 intersections 1 7 the dtm still contains bridges or perceptible remains of bridges after elimination more details can be found in fig 8 the undetected bridges are mainly located in the upper river reaches the detail view in the center of fig 8 represents one typical example the channel of the small river is not adequately resolved in the dtm consequently the crossing road only creates a plateau in the height profile but there is no ascent in downstream direction and the bridge is not detected in all the four states there was no remaining bridge over large rivers there are however remarkable differences concerning the ratio between bridges already eliminated in the original dtm blue and the bridges detected and eliminated in our workflow green this clearly shows the heterogeneity of the dtm due to different processing chains in the individual states validation of the other vector datasets is limited to thorough visual examination e g fig 7 since there is no extensive comparable ground truth at a similar spatial resolution these datasets were created for a very specific application and thus do not always have a clear correspondence with the dtm for example river banks in steep alpine valleys often do not have a geometric pattern as expected for the top edge of embankment however if there is a clear top edge of embankment detection is usually more reliable than for the water land borders because the latter very much depend on a smooth and artifact free representation of the river channel especially the water surface concerning the raster products spot checks were conducted to assess output quality the obstacle elimination proved to work exceptionally well in terms of smoothly interpolating the obstructed river channel without leaving artifacts c f figs 5 and 7a integration of river bed measurements mainly depends on the quality and compatibility e g height system of the respective bathymetric data overall the results demonstrate that processing of high resolution spatial data for hydraulic modelling is no longer restricted to small study areas the developed methods are able to provide countrywide datasets based on a 1 m dtm with local relevance at these spatial extents a high degree of automatization is essential however similarly to prior studies c f section 1 2 we conclude that fully satisfactory results still require some level of checking and corrections by a human operator 5 conclusions we present a methodology to adapt extensive high resolution spatial datasets to the requirements of hydraulic modelling for flood hazard mapping the aim of these methods is to ensure suitable and consistent representation of a river network and the surrounding area with a high degree of automatization this includes the elimination of discrepancies between datasets e g river network vs dtm an improved river channel description in the dtm obstacle removal bathymetry integration as well as the creation of new datasets such as culverts and embankment lines the workflow was designed in a way to make its parts as independent as independent of each other as possible in order to facilitate the isolated execution of single steps and enhance planning flexibility furthermore a focus is on high adaptability with respect to different river sizes and dtm properties which is achieved by purely geometric cross section based processing all methods were applied to data covering the whole territory of austria with 33880 river km on a total area of about 84000 km 2 the automatic correction increased the percentage of river axes matching the dtm river channel from about 85 to 97 after automatic detection and elimination more than 98 of all bridges do no longer affect the river bed representation nevertheless a certain degree of manual interaction is necessary for achieving the full reliability due to inconsistent input data and artifacts in the dtm manual checks at an early stage of the processing chain e g checking the complete corrected river network substantially reduces the required manual efforts in later working steps derivation of the original dtm with particular emphasis on a sound mapping of river channels could also likely improve the results of automatic procedures another promising possibility is processing directly based on point clouds this would however require significantly more computational resources due to the rapid progress of surveying techniques especially als and computational possibilities we also expect an increasing demand for locally relevant flood risk modelling on regional and country scales accordingly scalable and automatable procedures to adapt spatial data will be needed our proposed methodology has been shown to work on a large challenging dataset for a very specific application while the approach was not extensively tested for other datasets outside austria we still expect the method to adapt well to comparable data since it provided consistent quality over all federal states despite heterogeneous inputs most correction functions contain tuning parameters which can be calibrated in order to flexibly adapt to specific topographic circumstances the applicability mainly depends on whether the underlying basic assumptions concerning geometry and input data hold c f section 3 the methodology could also be used for a dtm with different spatial resolution as long as it is high enough to resolve the river channels of interest credit authorship contribution statement michael h wimmer conceptualization methodology software validation formal analysis investigation data curation writing original draft visualization markus hollaus conceptualization writing review editing supervision günter blöschl writing review editing project administration funding acquisition andreas buttinger kreuzhuber validation writing review editing jürgen komma writing review editing jürgen waser validation project administration norbert pfeifer conceptualization resources writing review editing supervision project administration funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement financial support from the austrian federal ministry of agriculture regions and tourism and the fwf vienna doctoral programme on water resource systems dk w1219 n28 is acknowledged vrvis is funded by bmk bmdw styria sfg tyrol and vienna business agency in the scope of comet competence centers for excellent technologies 879730 which is managed by ffg the authors acknowledge tu wien bibliothek for financial support through its open access funding programme 
8566,the regionalisation of flood frequencies is a precondition for the estimation of flood statistics for ungauged basins it is often based on either the concept of hydrological similarity of catchments or spatial proximity similarity is usually defined by comparing catchment attributes or distances here we apply flood types in regionalisation directly to consider the type specific aspects of similarity the different flood types are classified according to their meteorological causes and hydrographs their probability distributions are modelled by type specific distribution functions which are combined into one statistical annual mixture model afterwards for regionalisation we specified the parameters of each type specific probability distribution separately with hierarchical clustering and regressions from catchment attributes by selection of most relevant features depending on the flood type the specifics of flood generating processes and meteorological causes were considered the results demonstrate how this consideration of deterministic aspects can improve the transferability of distribution parameters to ungauged catchments the type specific regionalisation approach offers a higher degree of freedom for regionalisation as it describes the relationships between catchment characteristics meteorological causes of floods and response of watersheds keywords regionalisation flood types mixture model flood frequency analysis 1 introduction regionalisation is one of the main important research topics for predictions in ungauged basins it includes many complex challenges both of climate and anthropogenic nature sivapalan et al 2003 since the call by sivapalan et al 2003 and the following decade dedicated to the prediction in ungauged basins pub many new insights on this topic were obtained yet there are still open questions in regionalisation arsenault and françois 2016 regionalisation is defined as the transfer of parameters or variables from hydrologically similar catchments to a catchment of interest blöschl and sivapalan 1995 therefore regionalisation is of particular interest for regions where the hydrological gauging network is sparse and the extension of it is not possible the main challenge in regionalisation lies in the identification of catchments similar to the site of interest in order to specify the options and limitations of transferring hydrological information in space this similarity forms the basis for the transfer of information from gauged to ungauged basins where false assumptions can lead to serious errors in the outcome of regionalisation basically one can distinguish between two concepts of similarity spatial proximity and hydrological similarity merz and blöschl 2005 the first concept assumes that catchments close to each other have similar runoff regimes such that distance based approaches like k nearest neighbours are used to define similar catchments recent studies extend classical concepts of spatial proximity by using machine learning to handle large data sets oppel and schumann 2020 the second concept hydrological similarity assumes that similar catchment attributes lead to similar runoff regimes hydrologically similar catchments are then identified by statistically defining homogeneous groups e g by clustering principal component analysis or multiple regressions with the flow as response variable of the catchment attributes nathan and mcmahon 1990 a summary of regionalisation studies for different catchments was performed e g by odry and arnaud 2017 or salinas et al 2013 the above mentioned methods to define similarity have their benefits and drawbacks the choice of the methods mostly depends on the given circumstances like data availability the climate in the region of interest or catchment characteristics however not only the method for the definition of hydrological similarity can be varied but also the aim of regionalisation is diverse the two main applications are the specification of model parameters in rainfall runoff modelling e g bárdossy 2007 merz and blöschl 2005 viviroli et al 2009 or of flood statistical indices e g perez et al 2019 the regionalisation of flood statistics follows two different paths one approach tries to regionalise flood quantiles of given return periods the other one aims to specify the parameters of a statistical model similar to the regionalisation of deterministic models the regionalisation of flood quantiles or the derivation of regional flood frequency curves is most often based on assumed proportionality to the catchment area following the concept of scale invariance de michele and rosso 2002 like in the index flood approach dalrymple 1960 if instead the parameters of the statistical model are regionalised directly a higher flexibility in the model is obtained yet the uncertainty is increased since normally several parameters have to be estimated tyralis et al 2019 however the estimated parameter specific homogeneous regions could also provide insights on the different flood generating mechanisms in general it turned out that the consideration of flood generating characteristics could be beneficial for the regionalisation of flood statistics e g de michele and rosso 2002 parajka et al 2012 for example the seasonality index developed by burn 1997 was shown to improve regionalisation in many cases and is an often used characteristic for regionalisation merz et al 1999 ouarda et al 2006 by considering the seasonality indices the flood generating processes of those floods that led to the largest flood peaks can be considered in regionalisation implicitly still they do not cover the full spectrum of processes for all flood events but only of the one with largest peak per year one index to describe seasonality seems to be insufficient if multiple mechanisms of flood generation have to be considered throughout the year the potential of these mechanisms to generate large flood peaks is non uniformly distributed fischer et al 2019 there is a gap in regionalisation when it comes to explicitly taking into account deterministic flood generating processes in the definition of hydrologically similar clusters for regionalising the parameters of the statistical distribution we aim to close this gap by combining regionalisation with type specific flood statistics flood types offer a possibility to consider the variety of flood generating processes they are normally based on the categorisation of flood events by their hydrometeorological drivers and dominating hydrological processes or they are hydrograph based tarasova 2019 to apply the first approach the main drivers of floods have to be identified even if we use long time series the information about these processes is limited flood typologies which are based on analyses of hydrographs are an alternative approach they are easily applicable to long discharge series and in particular suitable for the application of peak over threshold statistics pot since they require only few additional information to the discharge fischer et al 2019 in this paper we consider a hybrid flood typology for regionalisation using both the hydrograph shape as well as precipitation and snowmelt data with the aim to better capture the differences among meteorological drivers and the initial and boundary conditions of flood processes more precisely we use a statistical mixture distribution that explicitly considers the different flood types fischer 2018 for regionalisation we derive relationships between the parameters of type specific distribution functions and catchment characteristics by the typing of floods we can search for the type specific most relevant catchment characteristics this has implications for the definition of homogeneous regions which were chosen according to hierarchical cluster methods or multiple regression in a type specific way the considered characteristics differ in their relevance between flood types for example the rainfall intensity plays a more crucial role for heavy rainfall floods compared to snowmelt floods therefore clusters were chosen for each flood type separately such that different drivers and impacting factors could be considered separately the results are compared to those obtained from regionalisation of annual maximum series in a second step the differences between the goodness of regionalisation for a single flood type were discussed as the relevance of flood types varies with quantile ranges fischer and schumann 2019 it gives an opportunity to reduce the complexity of regionalisation with this approach we aim to improve regionalisation by identification of connections between flood generation and catchment attributes the proposed method offers a deeper insight into the relevance of catchment attributes for specific flood types and how these can be incorporated in regionalisation the following research questions are addressed how can regionalisation of flood statistics take into account the flood type explicitly which catchment attributes should be considered for different flood types how does the ability of transferring distribution parameters to ungauged catchments depend on the available number of gauges and the catchment attributes 2 data the catchments considered in this paper are mainly located in bavaria in southern germany parts of the catchments are also located in austria and czech republic the catchments belong to the three main basins of the danube river main river a tributary of the rhine river and saale river a tributary of the elbe river the physiography ranges from alpine catchments with mean elevation of 2000 m a s l in the south to uplands in the north and east and lowlands with mean elevation of 300 m a s l in the centre of the study region catchment sizes vary from small headwater catchments with minimum catchment size of 42 km2 up to the large macroscale catchments of the danube with more than 10 000 km2 an overview of the catchments and topography is given in fig 1 a the analyses in this paper are based on the daily mean and monthly maximum discharges daily precipitation data and spatial catchment characteristics in an initial step discharge data were checked for data errors or anthropogenic impacts like reservoirs or water transfers in or from neighbouring basins using catchment information as well as mann kendall trend test and wilcoxon test both tests are among the most common tests in hydrology for trends and change points in mean and have the advantage that they are non parametric and robust the mann kendall test is able to detect continuous changes in the discharge data e g caused by increasing urbanisation in a catchment leading to a faster response of the catchment and therefore higher flood peaks the wilcoxon tests detects abrupt changes in mean e g caused by the building of a dam and therefore the reduction of peaks and increase of low flows a minimum observation period of 30 years was required to perform the flood frequency analysis the application of these criteria resulted in 168 catchments with observation periods from 30 to 119 years to be suitable for our purposes for computation of catchment characteristics a digital elevation model with a resolution of 1x1km was used courtesy of the european environment agency precipitation and temperature data were derived from the e obs 0 1 degree grid data set cornes et al 2018 snowmelt data were computed with the hbv model bergström 1995 from precipitation temperature and elevation zones of 100 m we applied a lumped version of the model except for the estimation of snowmelt which was done for different elevation zones and summed up afterwards with the degree day method for estimating snowmelt and a single linear storage for routing the first year of daily discharges was used as warm up period each discharge series was split into a calibration period first 60 and a validation period last 40 where the bobyqa algorithm powell 2009 was applied to optimise the parameters in a total of 15 runs with 1000 repetitions each per catchment the hbv model performance was measured with the nash sutcliffe efficiency applied to all daily discharges in the validation period which was 0 738 on average selected catchment attributes of the study area are given in fig 2 for the regionalisation a spectrum of different catchment attributes could be used the area of the catchment the elevation land use soil storage capacity statistical parameters of rainfall and general discharge characteristics e g from a hydrological atlas these attributes were selected to include aspects like the topography which has an impact on the hydrograph shape and its flashiness the nature class which served as indicator for the soil and is closely connected to land uses like agriculture and forestry e g sandy soils are expected to decrease the flashiness of floods compared to rocky soils pfister et al 2017 while a high degree of urbanisation leads to a fast catchment response or quantiles of the sum of flood inducing rain which was used as an indicator for pluvial exposition of regions and the spatial extremity of the flood inducing rain events the nature class is an eco geographical construct which is provided by the federal state of bavaria https www lfu bayern de natur naturraeume index htm a natural class is defined by similar geological morphological hydrological climatic and land use characteristics but also the distribution of species of flora and fauna five different nature classes are defined for the considered study region fig 1b the nature class assigned to a catchment was defined by the highest proportion on the catchment area however since one nature class western uplands only consisted of three small catchments which is not feasible for a separate regionalisation this was merged with the neighbouring nature class the south western uplands catchments which are mostly located outside of the defined nature classes are assigned with the adjacent nature class soil types were available from bük 200 krug et al 2010 and river network density was derived from the available river network with 100 m 100 m resolution derived from hydrorivers by dividing the river length by the catchment size the river network density can provide information on the probability of flood superposition and routing in complex river networks the flood peak may increase due to superposition of flood peaks while in less complex networks the hydrograph may be reduced in peak downstream seo and schmidt 2013 more information on how to derive the type specific catchment attributes is given in the following section 3 methodology 3 1 flood event classification to make an application of a flood event classification possible flood events had to be separated from the continuous series of daily discharges first for this purpose the automated separation method proposed by fischer et al 2021 was applied which allowed for a fast semi automated computation for all catchments the variance based threshold for identification of flood events t h var va r dvar θ var v a r dvar was used with a 3 day window dvar and θ 0 25 since the overall aim of this study was the estimation of flood statistics the flood peak value of each event was estimated by using the monthly maximum discharges a quasi instantaneous value based on high resolution measurements this way the use of the actual peak value was ensured instead of using the daily average the latter could lead to serious underestimation of the flood peak due to daily smoothening especially for small catchments where the runoff dynamic is high event separation resulted in approximately 2 4 flood events per year in each catchment second flood event classification was applied where the hybrid classification by fischer et al 2019 was chosen this classification has the advantage that it only requires flood events in daily resolution precipitation and temperature data the latter two for the estimation of snowmelt a summary of the flood event classification is given in fig 3 a first the flood events were differentiated into snow impacted and rainfall induced floods by applying a threshold to the amount of flood generating water sum of snowmelt and rainfall associated with the flood event that originated from snowmelt second the snow impacted floods were clustered further into rain on snow floods high amounts of rainfall together with snowmelt referred to as s1 and snowmelt induced floods dominated by water originating from snowmelt with small contributions from rain referred to as s2 for clustering the kmeans algorithm was applied to the amount of rainfall amount of snowmelt and the runoff coefficient direct flood volume without baseflow divided by the sum of rainfall and snowmelt of the corresponding flood event the rainfall induced flood events were classified according to the linear relationship between flood peak and flood volume both characteristics were estimated from baseflow separated flood events straight line method fischer et al 2021 and represent the direct runoff only three rainfall induced flood types were classified by optimising the coefficient of determination and hence the linear regression model for the relation between direct peak and volume fig 3b flood type r1 resulted in floods associated with high peaks and small volumes and hence a flashy shape of the hydrograph these flood events occurred together with short and intense rainfall flood type r2 corresponds to flood events with a medium peak volume relationship mostly occurring due to 5 10 days of rainfall of moderate intensity flood type r3 includes flood events that are characterised by large volumes but seldom have large peaks these events occur due to long duration rainfall of several weeks the distribution of the flood types and their frequency is not uniform for the considered study region and depends on the catchment location and the magnitude of the flood peak fig 3c the peak over threshold pot sample was obtained with a threshold of thrice the mean discharge to only include large floods we considered pot samples since the classical flood frequency analyses applies given thresholds for partial duration series more details are given in the following section 3 2 type specific flood statistics to consider each flood type separately and to identify the impact of each flood type on the joint flood statistics in flood frequency analysis the use of type specific mixture distributions is required such a model was proposed e g by fischer 2018 for a general number of m flood types assume that each flood type corresponds to a sample of flood peaks x 1 j x n m j j 1 m in a first step each flood type was modelled separately for this purpose a pot approach was applied with a type specific threshold u j the distribution of the exceedances of this threshold x i j u j was modelled for each flood type j j 1 m by the generalized pareto distribution gpd defined as g j x θ j κ j β j u j 1 1 κ j x u j β j 1 κ j for a shape parameter κ j 0 and scale parameter β 0 with support x u j the choice of the threshold u j is crucial and has a large impact on the resulting flood quantiles here we chose u j type specifically as equal to 3 times the type weighted mean discharge the type weighted mean was derived from the monthly means of discharges weighted according to the relative frequency of the flood type in the respective months this choice was based on german directives dwa 2012 and makes the results comparable to those obtained from the annual maximum series fischer 2018 the pot approach can be generalized to an annual distribution g j for each flood type simply by application of the total probability theorem to obtain g j x k 0 p j l k g j x θ j u j k where p j l k is the probability that the annual number l of flood peaks of type j above the threshold u j is equal to k and can be described by the poisson distribution with parameter λ j cunnane 1973 stedinger et al 1993 p j l k λ j k k e λ j to obtain the annual joint distribution of all flood types a mixture model was applied h x j 1 t g j x θ j u j 1 f j u j ϑ j f j u j ϑ j the model is denoted as type based mixture model of partial duration series tmps in this model the pot distribution g j from above is multiplied with the distribution of non exceedance of the threshold f j f j was modelled by the generalized extreme value gev distribution based on all flood peaks in the sample of the respective flood type f j x ϑ j ξ j μ j σ j exp 1 ξ j x μ j σ j 1 ξ j for 1 ξ j x μ j σ j 0 where ξ j r is the shape parameter σ j 0 is the scale parameter and μ j r is the location parameter 3 3 regionalisation regionalised flood frequency analyses consist of two fundamental steps the selection of similarity measures and the grouping of catchments into hydrologically similar clusters in the first step one can differentiate between two basic concepts the runoff similarity and the catchment similarity we use a combination of both where runoff similarity is considered implicitly by distinguishing between different flood types while catchment similarity is introduced by defining clusters of catchments according to their characteristics to obtain the type specific clusters in the proposed novel regionalisation introduced in this section the available geo data had to be further processed to specify catchment attributes with regard to the different characteristics of the flood types especially rainfall and discharge attributes were estimated type specifically one of these characteristics is the flood inducing rain for each flood type the medians of the duration of the related rain event d in days were estimated for which 80 of the total flood inducing rain fell for these time spans and each flood event type the median and the 80 quantile of flood inducing amount of precipitation were estimated however for ungauged catchments this sum cannot be derived directly since it results from flood event analyses in these cases the characteristics were estimated by using the type specific weighted mean of the d day sums of the 10 nearest neighbour catchments weights were chosen according to the similarity of catchment sizes with the target catchment where the catchment with most similar area was considered with the largest weight more details are provided in the appendix a with this approach we considered the scale variability of response time similarly the 80 quantile of the maximum 1 day rainfall during all flood inducing rain events was considered type specifically for each catchment as an indicator for rainfall intensity to take into account the variability of the runoff conditions and the storage capacity of a catchment the averages of the monthly minimum discharges daily means were considered it can be assumed that the discharge is directly related to the storage of the catchment for example in the water balance equation karlsen et al 2019 to take into account the seasonally different occurrence of each flood type here a weighted mean of these monthly minimum discharges was estimated for each flood type where the weights were determined by the relative frequency of occurrence of this flood type in a certain month this attribute is normalized by the catchment area to avoid the scale impact for ungauged basins both characteristics rainfall intensity and storage capacity were estimated similarly to the d day rainfall sums from observed neighbouring catchments with regard to snowmelt and orographic rain differences between hypsometric curves were considered as another attribute for regionalisation it can be assumed that their shapes have an impact on the temporal sequence of snowmelt according to the proportions of elevation zones and plateaus with similar elevation the elevation itself was not considered in these hypsometric analyses but included in terms of the mean elevation of each catchment and the elevation range table 1 to compare the shapes of the hypsometric curves all curves were separated into 50 equidistant sections using clustering of longitudinal data cld and by application of the extended kmeans clustering these curves were then clustered fig 4 the number of clusters was varied between two and six for 20 repetitions where five clusters were selected as best fit according to the calinski harabasz criterion calinski and harabasz 1974 using the kml package in r genolini et al 2015 cld in combination with kmeans makes clustering of several samples with ordered indices possible by a modification of the classical kmeans with euclidean distance where classical clustering methods fail to classify these ordered samples according to their shape and not their values this pre processing led to the catchments attributes available for regionalisation listed in table 1 for a regionalisation of the type specific distributions and the joint tmps model several parameters had to be considered for the tmps model the six parameters κ j and ξ j shape β j and σ j scale and u j and μ j location had to be estimated for the annual flood type specific distributions g j additionally the poisson parameter λ j had to be estimated the regionalisation was performed iteratively this means that the clusters that were used for the regionalised estimation of the parameters κ ξ β and σ described in the following were optimized with each iteration step to check the hypothesis that the explaining potential of different catchment characteristics depends on the flood types we used all of them table 1 to estimate the clusters resulting in minimum error for the regionalisation for each flood type all possible combinations of characteristics from table 1 were tested and the one with minimum absolute normalized bias see section 3 4 for definition was kept for regionalisation this procedure was done twofold first only one parameter at a time was considered while the remaining parameters were treated as known i e the at site fitted values were used second clusters for all parameters were optimized jointly to reach a global optimisation for the second step the results from the first step served as initial values and benchmark for ungauged basins appropriate characteristics can be selected by performing a similar analysis to the gauged catchments in the study area before applying the regionalization procedure the catchment attributes were checked for correlations among themselves if highly correlated attributes are included in the regionalisation the information gained is limited for our study region the evaluation showed that river network density soil type and land use were highly correlated with the remaining attributes and therefore were excluded in regionalisation appendix b please note that regionalised values of type specific attributes median 80 quantile d day precipitation intensity and average monthly minimum discharge as described in appendix a are used in the regionalisation to take into account the uncertainty in estimation of these 3 3 1 regionalisation of the shape parameters κ j the shape parameters of both gev and gpd are known to be afflicted with highest uncertainty in estimations as the shape is strongly affected by the very extreme events which not only depend on the location of the catchment but also on the observation period e g ragulina and reitan 2017 therefore shape parameters are most difficult to regionalise in contrast to scale and location parameters the shape parameter is not directly linked with the catchment size or the mean flood peaks in many regionalisation approaches e g the index flood method dalrymple 1960 the shape parameter is assumed to be equal for all gauges de michele and rosso 2002 but this assumption could not be applied here with standard deviation of 0 5 up to 0 8 depending on the flood type the shape parameters had higher variation than can be explained by uncertainty in the estimation meaning that it was beyond the confidence bands despite of these problems shape parameters are characterising the high quantiles and the upper tail which are most relevant in the flood design practice to investigate which catchment attributes have the highest relevance for regionalising the shape parameters we applied a clustering approach the resulting clusters of gauged catchments were used to estimate the shape parameter for ungauged catchments however the number of clusters was not known beforehand we applied a hierarchical clustering a distance based approach that uses a dendritic structure and which does not require a pre defined number of clusters in each step the number of clusters is reduced by merging those with smallest distance using an agglomerative approach we applied ward s minimum variance criterion for linkage and euclidean distance as similarity measure to define the number of clusters required after each clustering the total sums of squared distances from the cluster centres tss were calculated the total sum of squares for a number of k clusters and n observations of p variables is defined as tss k 1 k i s k j 1 p x ij x kj 2 where s k is the set of all observations in cluster k x ij is the ith observation of the jth variable catchment attributes from table 1 and x kj is the jth variable of the cluster centre for the kth cluster the smaller the tss values the better defined are the clusters of course these values are declining with increasing number of clusters at the same time a high number of clusters is not beneficial for regionalisation since the assignment of an ungauged catchment to a cluster becomes more difficult the final number of clusters therefore had to be balanced between low tss and a small number of clusters to define this final number of clusters for each shape parameter and each flood type the tss was plotted against the number of clusters as soon as the tss decreased significantly slower which was tested by fitting two regressions and comparing their slope with a t test the final number of clusters was reached e g salvador and chan 2004 for the resulting clusters the respective type specific shape parameters κ j and ξ j were then estimated as the median of the shape parameters of the gauged catchments in the clusters hierarchical clustering can only be applied to values in the interval 0 1 therefore the input values were normalized by application of the min max transformation for ungauged catchments the assigned clusters can be identified by using the estimated centroids for this identification the respective catchment attributes used for clustering have to be considered and the minimum distance of these attributes to the centroids of the clusters has to be calculated to identify the assigned cluster 3 3 2 regionalisation of the scale parameters the scale parameters of gpd and gev distributions β j and σ j depend on the variability of the samples with regard to scale dependencies of both parameters two opposite aspects have to be considered in large basins the storages in soil and the river networks balance the variabilities of rainfall intensities within rain events but heavy rain close to the outlet of such basins may result in extraordinary high peaks nicótina et al 2008 therefore large catchments can be assumed to show scale dependence of the scale parameter for small catchments a unique relation between catchment size and scale parameters seems to be unlikely since the response of the catchments is more direct meaning that the rainfall is transported faster into the river and not smoothed over area therefore the catchments were differentiated by their area into four categories two without and two with consideration of scale dependence the lower mesoscale 300 km2 the upper mesoscale 300 1000 km2 the lower macroscale 1000 10000 km2 and the upper macroscale greater than10000 km2 becker 1995 this differentiation made a different handling of the catchment sizes possible and at the same time preserved a similar and sufficient number of catchments for each class fig 6 while for catchments of the lower and upper macroscale mostly a linear dependence of the scale parameter on the catchment size was present this was not the case for catchments of the lower and upper mesoscale smaller catchments were affected by a much higher variability in the scale parameter and no nature class specific clusters were visible fig 5 this can be explained by the very different topography of the considered catchments fast reaction times and a direct dependence on variable rainfall intensities the different catchment characteristics affect the flood generation and therefore the variability of floods for example in steep catchments in the alps flash floods are more probable than in the flatlands this led to the following differentiation in the regionalisation of the scale parameters β j and σ j for the catchments of the lower and upper mesoscale a clustering approach analogously to the shape parameter section 3 3 1 was applied to define the clusters since a linear relation between catchment size and scale parameter cannot be assumed instead the scale parameter is more related to the catchment attributes and thus a clustering approach similar to the shape parameter is required again the considered catchment attributes were selected type specifically for the lower and upper macroscale a linear relation between the scale parameters and the catchment size was assumed ϕ i j k α i j k γ i j k a e where k 1 2 indicates the parameter ϕ 1 β ϕ 2 σ j 1 m indicates the flood type and i 1 2 indicates the catchment class for example γ 1 2 1 is the slope of the linear regression between catchment size and parameter β 2 for flood type r2 and catchments of the lower macroscale ae denotes the catchment size yet results in fig 7 delivered a dependence structure within the linear relationship there is not one group but two groups indicating a different slope parameter in the linear regression for these two groups again these groups were not related to natural classes the definition of the two groups was done by fitting two linear regressions to the catchment size and parameter values under consideration of every possible combination of two groups then those regressions were chosen that delivered the overall greatest coefficients of determination this way the data were split such that dependence was highest within the two groups for ungauged catchments the parameters were estimated by using the catchment size and the linear regression model of the neighbouring gauges 10 nearest neighbours of the same catchment area class 3 3 3 regionalisation of the location parameters the threshold parameter of the gpd distribution u j and the location parameter of the gev distribution μ j determine the location of the distributions they almost completely depend on the catchment size since they mirror the mean annual flood discharge of the catchment in classical regionalisation theory dalrymple 1960 de michele and rosso 2002 the location parameter therefore mostly is estimated by a non linear relationship to the catchment size in regressions here the threshold parameter u j of the gpd was defined by mean discharges for the gauged catchments directly and hence its dependence on the catchment size is obvious therefore regionalisation of these parameters was done analogously to the scale parameters of the lower and upper macroscale section 3 3 2 a linear regression through origin was performed for two groups for each of the four catchment classes lower and upper mesoscale lower and upper macroscale differentiated by the parameter and the flood type goodness of fit coefficients of these regressions are given in section 4 3 3 4 regionalisation of the poisson parameter the poisson parameter λ j describes the number of flood events per year for each flood type j 1 m it is used for estimations of type specific annual distributions since there is always a high coincidence between the numbers of floods per year with neighbouring gauges this parameter can be estimated for ungauged basins simply by using the type specific mean of the annual events of the same type at neighbouring gauges weighted by the difference in catchment size similar to the regionalisation of the d day precipitation sum for neighbouring catchments with similar catchment size the largest weight was assigned to estimations of these means 3 3 5 regionalisation of the distribution of the annual maximum discharges to allow a comparison of the proposed model to classical models in flood frequency analysis the annual maximum series ams of discharges is considered too for the ams the gev is among the most frequent applied distributions salinas et al 2014 and led in this case to the overall best performance in terms of anderson darling test and akaike information criterion aic compared to other common distributions such as log pearsoniii gumbel and normal distributions therefore it was decided to use this distribution for the regionalisation of the ams the three parameters of the gev have been introduced before and to make a comparison between both models fair they are regionalised in a similar manner as the gev parameters of the tmps model the shape parameter ξ is estimated by the median value of all shape parameters of gauged catchments in the same cluster as the ungauged catchment clusters are once again defined according to the catchment attributes and optimised iteratively type specific attributes like the d day rainfall were not considered for clustering instead these attributes were considered as non type specific meaning that one overall attribute was calculated for all floods in case of the median d day rainfall this would mean that duration d as well as the median rainfall over d days were estimated for all flood events jointly the scale parameter σ was regionalised analogously to the scale parameters σ j above according to the catchment size class regional estimates of the location parameter μ were obtained by regression on the catchment size this way the regionalisation of the ams is similar to the tmps but without any consideration of flood types it has to be pointed out that this comparison does not provide information on the ability of the newly proposed method to outperform classical regionalisation procedures but instead it demonstrates if the method is suitable for transferring the parameters of the tmps model to the ungauged locations compared to the suitability of this method for regionalisation of ams 3 4 error estimation to evaluate the goodness of the performed regionalisation the delete d jack knife method was applied for each of the considered 168 catchments at one time d catchments were chosen these catchments were left out in the regionalisation process and handled as ungauged then with the estimated regionalized distribution parameters the annual distribution of each flood type and the tmps model were estimated for the regionalisation the regionalised type specific catchment attributes such as d day precipitation sum intensity or average monthly minimum discharge were used clusters were estimated based on two different approaches first all available catchments were used to build the clusters while for the estimation of the shape and scale parameter of each cluster only the gauged catchments in a cluster were used this approach considered that in theory it should be possible to define clusters for all ungauged catchments in a study region since the cluster definition relies on catchment attributes only which could be derived even for ungauged catchments if not noted otherwise results are based on this approach second only the gauged catchments were used for cluster estimation to estimate the impact of the gauge density on the clustering afterwards quantiles for selected return periods t 10 100 500 years were estimated the choice of the return periods was made such that flood quantiles with medium return period 10 year return period as well as standard design floods 100 years and extreme floods of the right tail 500 years were compared though flood quantiles with 500 year return periods are afflicted with uncertainty it is important to analyse these as well since only in this range of high quantiles the tail behaviour of the distribution can be understood it has to be noted though that for bounded distribution functions as was present e g for some of the snow impacted flood types the difference between 100 and 500 year return period was small the estimated regionalised quantiles were compared with the estimated at site quantiles which are the ones obtained from fitting the flood type specific distributions and the tmps model to the observed flood peaks as benchmark d was chosen as 10 with 100 repetitions of randomly drawing d catchments from the sample and we show detailed results for this case however to take into account lower gauge density of basins we also increased d to d 20 40 60 80 such that up to half of the catchments were treated as ungauged in this case repetitions were limited to 50 due to the computational time that is required to estimate the quantiles of the tmps model when treating such large amounts of catchments as ungauged it may occur that one of the catchment area classes is not represented by any of the gauged catchments in this case the run was removed and estimation was repeated until 50 repetitions were obtained since errors in regionalisation of certain flood types might be assigned to certain meteorological causes that were not represented well in regionalisation they may help to identify possible additional attributes that are required for regionalisation for this purpose the nbias estimated for the quantile with 100 year return period with delete 10 jack knife method and 100 repetitions is differentiated by catchment attributes for one attribute at a time meaning that the error is averaged for all catchments with the given attribute as goodness of fit criterion we applied the normalized bias and the normalized rmse the normalized bias and rmse which includes bias as well as variance for return period t are defined as 1 nbia s t 1 d i 1 d q i r e g t q i o b s t q i o b s t 2 nrms e t 1 d i 1 d q i r e g t q i o b s t q i o b s t 2 where d is the number of left out catchments in the jack knife method q i r e g t is the regionalized quantile with return period t for catchment i and q i o b s t is the at site quantile with return period t for catchment i the at site quantile refers to the quantile obtained when fitting the model to the observed data the above definition of the error measurement or variations of it with the use of the total value is standard for many regionalisation approaches salinas et al 2013 however if there is no general bias in the regionalisation positive and negative errors could equalise each other such that the overall error would be close to zero therefore to make an evaluation fair and transparent bias values were estimated with and without absolute values of the errors this way a systematic bias and the overall deviation from the target quantile can be estimated 4 results and discussion in this section we present the results of the regionalisation and discuss them we only present the results for the global optimisation and omit a detailed description of the local optimisation of each parameter this way only the final dependencies obtained with the regionalisation model are presented first the obtained dependencies between the distribution parameters and the catchment characteristics are given second the goodness of the regionalisation approach is evaluated together with the impact of gauge density moreover the type wise transferability of the parameters of the tmps is compared to the case of the annual maximum series third the errors of regionalisation are examined more detailed in terms of dependence of catchment characteristics such as elevation hypsometric curves or catchment size 4 1 relevance of catchment characteristics and differences between regionalisation clusters the resulting number of clusters and the catchment attributes used to define these clusters for each parameter and flood type are given in table 2 the parameter d for the d day rainfall sums was estimated flood type specifically for flood types r1 and s1 this was two days for floods type r2 four days and six days for flood type r3 with the longest flood inducing precipitation period the type specific characteristics d day sum of precipitation intensity and average monthly minimum discharge were regionalised for the ungauged catchments as described in section 3 errors for catchments which were assumed to be unobserved delete d jack knife method were small see appendix a 1 due to this type specific character of the rainfall attributes of the catchments the assignment of a cluster to a catchment could vary between the flood types although the same attributes were used for clustering for the definition of the clusters differences between the parameters and flood types occurred table 2 the relevance of catchment attributes obtained from the regionalisation using attributes leading to the smallest error depends on the different flood generating processes that are related to each flood type e g heavy rainfall for flood type r1 or snowmelt for flood type s2 for the shape parameter of flood type r1 the rainfall intensity 80 quantile of maximum 1 day rainfall and the extreme rainfall sums 80 quantile of d day rainfall were most relevant for defining the clusters floods of this type are associated with heavy rainfall which is characterised by high intensities and large rainfall sums in short time periods for flood type r2 instead which is associated with medium rainfall floods the intensity proved to be the only relevant rainfall attribute which is surprising since one would have expected that the amount of rainfall would be relevant too flood type r3 instead is associated with long duration rainfall of several days with low intensity therefore it is meaningful that only the median rainfall sum was a relevant cluster attribute for flood type s1 the rain on snow floods median rainfall sum and intensity were relevant while for s2 the snowmelt induced floods none of the rainfall attributes was relevant however for both snow impacted flood types as well as flood type r3 the hypsometric curve and therefore the elevation relations in the catchment proved to be relevant the hypsometric curves can be used to describe the snowmelt in the catchment for example if there exist any plateaus where the snow melts in large amounts at the same time for flood type s2 additionally the elevation range of the catchment was used for clustering interestingly for all catchments the nature class and the storage capacity weighted mean minimum discharge were relevant the spatial distribution of the clusters for the four parameters is given in fig 5 clearly spatial coherences occurred though these are not limited to certain nature classes and often are mixed indicating that nature class is not the main attribute used for clustering yet many flood types had one cluster located closely to alps indicating that for this region the flood generating mechanisms may be different than for the alpine foreland though for the shape parameters identical catchment characteristics were used as input data a different number of clusters resulted from the hierarchical clustering for some flood types where the shape parameter of the gev distribution had fewer clusters than that of the gpd distribution table 2 the regionalisation of the scale parameter was twofold for mesoscale catchments a clustering approach similar to the shape parameter was performed for the scale parameters of catchments of the lower and upper mesoscale differences occurred too for these parameters the main input characteristics were the catchment area elevation range and the nature class these factors proved to be important for all flood types which confirms the results from above where a dependence of the scale parameter on the catchment area was shown but this was not the only relevant attribute for clustering again intensity was relevant for flood types r1 and s1 similar to the results for the shape parameters while the median rainfall sum was relevant for the flood types associated with longer rainfall r2 and r3 the resulting clusters showed rather spatially coherent patterns which is due to the use of nature classes and hypsometric curves respectively elevation fig 5 for ungauged basins using the catchments characteristics and the location of the catchment as well as regionalised attributes from neighbouring catchments an assignment to a cluster hence is easily possible one could also argue that some flood types may be similar in their catchment reaction and may be merged in a first attempt it was tested to merge flood types posteriorly for defining clusters of the shape and scale parameters for this the correlation between the clusters was estimated and a correlation threshold for merging flood types was defined such that the regionalisation error did not increase by more than 5 as a result flood types r1 and r2 as well as s1 and s2 turned out to be able to be considered jointly for shape respectively scale parameter similar processes may lead to similar clusters in this case however this was only a first investigation it has to be noticed that the considered attribute nature class is a rather specific one of the study area and since it is provided by state authorities it may not be available or valid elsewhere according to their definition nature classes include probably valuable additional information compared to the sole use of soil and land use data only still there is high correlation between the nature class and the catchment attributes as demonstrated in the appendix a indeed if a cart classification is applied to predict the nature class an accuracy of in mean 73 was obtained using the catchment attributes named in table 1 and appendix b using 60 of the data as training and 40 as test samples and 1000 repetitions therefore even for regions where nature classes are not applicable a similar amount of information can be obtained by e g including soil and land use classes here the nature classes were used to include all possible information e g also on flora and fauna again it has to be emphasised that the selection of catchment attributes for regionalisation depends highly on the considered study area and the flood generating process therein for larger catchments the scale parameter was regionalised differently for catchments of the lower and upper macroscale a twofold linear regression through origin between catchment area and scale parameter was performed the regression for these catchments resulted in corresponding coefficients of determination of more than 0 7 for most catchments fig 6a indicating a strong dependence between the catchment size and scale parameters for macroscale catchments and a high explanatory power of the catchment size for this parameter errors in regionalisation arising from this specific parameter therefore can assumed to be small the same holds true for the location parameters uj and μ j fig 6b who had smallest observed coefficient of determination equal to 0 82 fig 6a though many of the alpine catchments were included in the group for the second regression a clear distinction between alpine and the remaining nature classes did not appear and each group was a mixture of catchments belonging to different nature classes a regression for the alpine catchments in one group and the remaining nature classes in another worsened the regionalisation results by 6 the same holds true when all catchment area classes were considered jointly where additional errors of up to 10 appeared the chosen classification therefore proved to be beneficial for regionalisation resulting in only small errors for the location parameter 4 2 regionalisation error and compensating effect of the tmps based on the clusters and the linear regressions described in the previous section the final regionalisation error was estimated with the delete d jack knife technique again the regionalised characteristics d day sum of precipitation intensity and average monthly minimum discharge were considered for ungauged catchments to make investigation of the error propagation in the different regionalisation steps possible the resulting normalized bias with and without consideration of the absolute value and rmse for the quantiles with return period t 10 100 and 500 years are given in table 3 quantiles were estimated for each flood type as well as for the tmps model as a comparison in fig 7 the mean relative errors of regionalisation i e the difference between at site and regionalized quantile divided by the at site quantile derived with delete 10 jack knife and 100 repetitions are given for each catchment at site quantile in this case refers to the quantile obtained from the fit of the type specific distributions and the tmps model to the observed flood events in a catchment analogous results for the flood types are given in the appendix in fig d 1 the resulting bias for quantiles of return periods of 10 100 and 500 years table 3 differed between the flood types and the joint tmps model in general flood types delivered higher errors than the tmps model though the error is non uniformly distributed between the flood types moreover certain spatial coherences became visible fig d 1 for flood type r3 large errors for distinct catchments occurred mostly in the north and east these are catchments close to the uplands where the fewest overall precipitation sums in bavaria occur the heavy rainfall floods of type r1 showed a pattern of high errors in the northern parts of the danube basin but not limited to a certain nature class leading to a positive bias in the estimation and large variance especially for return period 10 years table 3 r1 floods are related to heavy rainfall events especially the largest flood peaks of this flood type often occur due to local extreme rainfall flash floods which occurs frequently in this region with more than 25 l m2 in one hour dwd 2016 however these mostly local events only have spatially very limited extent and are therefore often recorded for single catchments only which makes them most difficult to incorporate in regionalisation however what is most relevant for the evaluation of the regionalisation is the tmps model itself the type specific errors were partially propagated to the tmps model though the errors for the single flood types can give information on where the highest errors occurred and which potential drivers were not captured well by the model not every flood type played a crucial role for the final tmps model as was shown by fischer and schumann 2020 and also visible in fig 3a and fig c 1 in the appendix a different flood types dominate different quantile ranges of the resulting tmps model meaning that they lead to the highest quantiles the frequency and dominance of the flood types differ between the catchments and large floods are dominated by different flood types than smaller floods fig 3b for quantiles of return periods of 200 years and higher the role of snowmelt induced events was negligible and only few catchments in the north east of the region were dominated by these flood types for the central catchments draining to the danube river the flood type r3 dominated these large quantiles which is due to the large size and slow reaction of these catchments the southern parts instead were dominated by flood types r1 and r2 for almost all quantiles regionalisation errors of flood types that do not dominate the tmps model have less impact on the overall error this mainly applies to snow impacted flood types which often only have an impact on smaller quantiles with return periods of 10 50 years northern parts however this range is represented by many events from different flood types large return periods instead are mostly dominated by one or two flood types most often rainfall induced and therefore more emphasis has to be given to an appropriate representation of these types for 18 gauges mostly in the southern parts snowmelt impacted floods or floods of type r3 were not included in the statistics since less than four events occurred for this type and a calculation of the statistic would not be meaningful these were left out in the mixture model hence the given errors are not automatically forwarded to the tmps model this is important to notice since it reduces the required number of regionalised parameters for the tmps model significantly as different type specific distribution functions determine certain reaches of the tmps distribution we concentrated mostly on the tmps errors in regionalisation and only used the errors of flood type distributions to identify drivers that were not captured well by the regionalisation procedure the nbias without absolute value where the sign of the error and hence a potential bias was considered were in the range of 0 12 and did show a slight positive bias for the tmps model the large nrmses imply that single catchments may have contributed much to this value with very high individual errors the tmps model in total therefore was regionalised well for all quantiles which results from the compensatory effect of deviations of distributions of single flood types when considering the absolute value of the nbias mean deviations from 34 to 44 occurred increasing with increasing return period this is meaningful since of course larger return periods are afflicted with much more uncertainty in estimation for the estimated at site as well as the regionalised quantiles only few events were observed in this range and information is reduced even more since mostly extreme events occurred at several gauges at the same time except for flash floods associated with heavy rainfall which have spatially limited extent see above meaning that for extreme events several events deliver the same information for regionalisation this makes regionalisation especially of the shape and scale parameters difficult still the results are in reasonable ranges and in the lower range compared to existing regionalisation studies salinas et al 2013 the larger number of parameters does not necessarily have to be drawback in regionalisation moreover the type specific consideration can also help in the analysis of the error origins as will be shown in the following subsection the results for increased values of d and therefore under the assumption of decreasing gauge density in the delete d jack knife method are given in fig 8 where up to d 80 catchments were treated as ungauged in panel a the clusters for shape and scale parameters have been estimated using all available catchments under the assumption that even for ungauged catchments the required attributes are available either from a data basis or by regionalisation in panel b only those catchments are used for estimating clusters which are assumed to be gauged all remaining regionalisation steps regionalisation of attributes regression parameter estimation were performed using gauged catchments only and parameters were then transferred to ungauged catchments with an increasing number of catchments that were treated as ungauged the normalised bias increased only slowly fig 8a by 10 moreover the nbias only increased slightly for larger return periods however the variance became much larger as can be seen by the normalized rmse for both increasing number of ungauged catchments as well as return periods therefore the proposed methodology is applicable also for regions with lower density of gauges though results suffer from a higher variability if the clusters are estimated on the basis of gauged catchments only fig 8b the nbias increased by 10 20 an rmse increased by 20 30 compared to the case where all catchments are used for clustering therefore a smaller number of gauged catchments decreases the performance of the regionalisation especially of the cluster estimation still a number of about 100 gauged catchments in the study region delivered regionalisation results with overall errors of about 25 for all return periods a limitation of the methodology consists in the split of catchments into different catchment classes which increases the number of required gauges in the study region to obtain a meaningful representation of each class this was for example the case when 80 or more catchments were treated as ungauged and several runs of the regionalisation had to be repeated due to catchment classes without members the results of the regionalised tmps model were compared to regionalisation of classical flood frequency analyses methods the ams an application of analogous regionalisation techniques for the parameters of the gev ensured a fair comparison between ams and tmps the ams can be seen as special case of the tmps without flood type differentiation results of the regionalisation of the ams are given in table 4 using the same performance measures as before the estimated at site quantiles in this case refer to the quantiles obtained when fitting a gev to the observed ams a delete 10 jack knife with 100 repetitions was applied the simpler ams regionalisation approach with less parameters resulted in larger errors than the tmps regionalisation but with smaller variability the increased number of parameters therefore helps to better capture the variable flood generating processes of the different flood types and makes the proposed regionalisation in combination with the tmps model more suitable to transfer the model parameters to ungauged basins compared to the ams in the ams all these flood types are considered jointly and therefore all processes have to be captured by one parameter set this may reduce the flexibility of the approach especially if the annual maximum series consists of many different flood types the higher variability of the resulting errors for the tmps regionalisation approach indicates that a higher uncertainty is introduced by the additional parameters too 4 3 the impact of catchment attributes on type specific errors since the catchment attributes played an important role especially for defining the clusters of the shape and scale parameters it is crucial to ask whether the regionalisation results differ between the attributes results of this analysis are given in table 5 again we discuss here the error of the tmps model only of course there exist interdependencies between the characteristics which has to be considered for example similar performance can be found for catchments of high elevation and the nature class alps since most of the catchments with high elevation are part of this class close the alps as discussed below the differences between flood types and attributes given in table 5 can be summarised as follows regionalisation of the saale basin resulted in higher errors than danube and main basins this is probably due to the small number of catchments available for this basin only 5 this small number is not sufficient to represent the saale basin adequately in the clustering regionalisation of catchments of lower and upper mesoscale resulted in higher errors than lower and upper macroscale in the macroscale the scale parameters clearly showed as linear relation to the catchment area for the mesoscale these parameters had to be estimated with clustering approaches which probably introduced a higher uncertainty the relationships between parameter and catchment attributes were less pronounced for the smaller catchments surprisingly the alps delivered the smallest regionalisation error while alpine foreland and south western uplands resulted in higher errors which is caused mainly by errors in the regionalisation of flood type r1 in the alps there were more homogeneous clusters for flood type r1 probably because a large number of heavy rainfall events led to a larger information density and homogeneity for this flood type regionalisation of catchments with hypsometric clusters with steep upper parts and flatter lower parts fig 4 class d and b resulted in smaller errors than for those with more uniform shape a and c the higher dynamics of those catchments with steep topography and hence probably faster response are more easily to capture than low dynamics and slow catchments responses the higher dynamics are characterised by higher scale parameters of the distributions which are easier to identify since they are pronounced and define variation of the distribution function for smaller scale parameters several parameter values can lead to similar distribution shapes regarding the flood types especially flood types r1 and r2 showed significant tendencies of where large errors in regionalisation appeared flood type r1 had high errors for small catchments in the alpine forelands with medium elevation and hypsometric curves of convex shape especially the alpine forelands are known to be affected much by heavy rainfall events dwd 2016 again such events are most difficult to incorporate in regionalisation due to their local nature therefore the shape parameters of r1 floods vary much between the catchments depending on whether there occurred a heavy rainfall event or not this could also have an effect on the regionalisation of the input attributes of rainfall intensity and d sum rainfall makes the regionalisation of this type difficult and led to larger errors compared to the remaining flood types for flood type r2 also some extreme events occurred which affected the regionalisation results i e these events lead to higher errors especially for the transfer of the shape parameter since for extreme events less information is provided in the data however the effect and thus the regionalisation error was smaller than for flood type r1 large errors for snowmelt induced floods occurred mostly for alpine catchments with high elevation and convex hypsometric curves for these catchments the varying periods of snowmelt depending greatly on the temperature in the respective elevation zone made a regionalisation difficult since complex snowmelt processes are considered in a rather simple way by the degree day method in this approach 5 conclusion we proposed a novel regionalisation approach for the type specific mixture model of partial duration series tmps the regionalisation is based on catchment attributes that describe topography catchment shape in terms of hypsometric curves or the quantiles of precipitation and flow and is performed type specifically the resulting errors measured with the normalized bias for quantiles of different return periods are in the range of 15 respectively 35 absolute valued and do not show a remarkable bias the number of required gauged catchments depends on the diversity of the considered study area for the topographically and hydrologically diverse region of bavaria considered here a number of 100 gauged catchments proved to be sufficient to obtain comparably small errors this number mostly results from the requirement of an adequate representation of the different catchment classes and clusters for regionalisation where we found a number of at least 10 catchments per class and 5 catchments per cluster sufficient the catchments should be diverse according to catchment sizes and elevation but do not necessarily have to be spatially uniformly distributed moreover we recommend to consider more than 10 gauged catchments per river basin for these numbers stable regionalisation errors and meaningful variance in the regionalisation was obtained the use of the type specific model proved to have two advantages on the one hand as long as the dominant flood types for the mixedt model were estimated well the overall error of the regionalisation was kept small hence not all flood types had to be estimated with high accuracy since not all are relevant for the joint annual statistics for large return periods mostly one or two flood types dominate the sample and the distribution and only these have to be estimated in regions with mixed flood types the proposed approach will be beneficial when transferring distribution parameters to ungauged basins as was shown in this study area when comparing the regionalisation of the tmps to the ams on the other hand the flood type specific distributions and the resulting errors in regionalisation can also be used to better understand the origin of errors since flood types are related to certain meteorological causes errors in distinct flood types can be linked directly to these causes therefore additional data that could improve the regionalisation of this type can be identified easily for example if errors for heavy rainfall floods are large an additional characteristic for regionalisation could be the improved consideration of the temporal distribution of rainfall during events we were able to identify considerable differences in the goodness of regionalisation regionalisation errors were largest for small steep catchments in the alpine foreland and were assigned to errors in heavy rainfall associated flood types a consideration of higher temporal and spatial resolutions of rainfall data which were not available in our study could improve the regionalisation of this flood type since a more detailed description of the spatial and temporal distribution would be possible the regionalisation could be combined with clusters of temporal and spatial rainfall distributions oppel and fischer 2020 where the frequency of certain rainfall patterns can be included as clustering input further improvement especially for the north eastern catchments could be obtained by a better description of the snowmelt here future studies should include a more sophisticated model to describe the snowmelt and also snow cover and include these as attributes into the regionalisation yet to include these data for long time series suitable models have to be developed which is beyond the scope of this paper drawbacks of the proposed methods are the requirement of long data series suitable for flood statistics moreover the clusters used for estimating the distribution parameters are not spatially coherent such that for ungauged basins additional characteristics like the catchment area and the hypsometric curves are required to assign a catchment to a cluster additionally the regressions performed to estimate location and scale parameters require a high density of gauges to have sufficient representation of each catchment size class a merging of classes for basins with lower gauge density is possible though it was shown that this increases the regionalisation error by 5 10 however it was demonstrated that the proposed approach is applicable also for regions with lower gauge density with increasing errors of up to 10 if up to 50 of the gauges where removed moreover the structure of the considered basins has limited complexity and seldom nested catchments are considered for more complex networks or basins with several nested sub catchments and confluences additional to the proposed method also the interaction between catchments flood type changes and flood superposition have to be investigated and included in regionalisation to better understand the spatial scale effects in summary the results emphasised that the proposed method makes a regionalisation of the tmps model possible with comparably small errors but can be improved by more detailed generation specific input data moreover it should be tested whether flood types could be combined for regionalisation this would reduce the complexity of regionalisation a first attempt showed potential for this procedure in future more detailed studies are required that should reveal similarities between the clusters and flood types and link these to the flood inducing processes of course this depends much on the study area and the flood inducing processes therein the proposed approach is more complex in comparison to conventional methods but considers that the sample of observed flood events is not homogenous as the drivers and mechanisms of flood generation differ between flood types by adding more hydrological information future regionalisation studies could concentrate on certain attributes considering the differences among flood generating mechanisms in the catchment and the most dominant flood types moreover the type specific impacts of changes in drivers e g convective rain due to climate change or anthropogenic impacts e g reservoir management could be better captured in regionalisation if type specific non stationary models are considered in future studies in summary the novel approach of consideration of flood types in the regionalisation of the statistical distribution offers a deeper understanding of the relevant flood generating processes and how these are connected to catchment attributes credit authorship contribution statement svenja fischer methodology writing original draft andreas h schumann writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was funded by the german research foundation deutsche forschungsgemeinschaft dfg grant number for 2416 space time dynamics of extreme floods spate we are grateful to bayerisches landesamt für umwelt www lfu bayern de for providing the discharge data climate data can be obtained from the e obs data set https surfobs climate copernicus eu dataaccess access eobs php and the dem from the european environment agency https land copernicus eu imagery in situ eu dem eu dem v1 1 view river network can be obtained from hydrorivers https www hydrosheds org page hydrorivers corine land use data were obtained from european union copernicus land monitoring service 2018 european environment agency eea and bük200 data are available at https www bgr bund de de themen boden informationsgrundlagen bodenkundliche karten datenbanken buek200 buek200 node html the manuscript provides all the information needed to replicate the results r codes for flood typology flood event separation and the statistical model are available in the r package floodr github com philippbuehler the r code for the hbv model can be obtained from the authors upon request we would like to thank two anonymous reviewers and the associate editor franceso marra for their helpful comments that helped to improve the manuscript appendix a estimation of the type specific median d day precipitation sum the estimation of the d day precipitation sums for ungauged catchments cannot be done for the catchment directly the resulting sums are flood type specific and since no information on the flood inducing rainfall is available for ungauged catchments information from surrounding catchments has to be used here the k nearest neighbour approach was applied for each ungauged catchment the k 10 nearest catchments according to the distance of catchment centroids were considered for each flood type the median or 80 quantiles depending on the desired attribute d day precipitation sums were used and the weighted mean was applied the weights of the mean were chosen according to the similarity criterion for catchment sizes more precisely pre c d y i 1 10 ω i p r e c d x i i 1 10 ω i where ω i 1 a e x i a e y y is the ungauged catchment x i i 1 10 are the nearest neighbour catchments ae is the catchment size and pre c d is the median or 80 quantile precipitation for d days exemplary the normalized absolute bias for the median d day precipitation sums is given in table a 1 estimated with the delete 10 jack knife method regional results are given in fig a 1 the precipitation sums for the rainfall induced flood types were all estimated well with errors around 12 20 for snowmelt induced flood types these errors increased to 25 35 probably because the snowmelt plays a more important role for these flood types and rainfall is not defined as precisely as for the rainfall induced floods yet since the precipitation sums were not used directly for regionalisation but only for defining the clusters for some of the parameters the effect on the results was negligible and differences between regionalisation errors with and without regionalised precipitation sums was 1 in mean with a standard deviation of 5 however what has to be taken into account is the gauge density within the study area in our case this density is quite high and may be favourable for the estimation of the regionalized input parameter in other basins this might not be the case and therefore it should be investigated in how far the results of the estimation are affected by the gauge density for this purpose the number of available gauges in the study area was reduced artificially more precisely random samples of different numbers of gauges were drawn from the sample of all gauges for these gauges the approach described above for estimating the regionalized input data based on the nearest neighbour approach was applied for each sample size the procedure was repeated 100 times results in terms of normalized rmse where the difference of observed and estimated variable is normalized by the observed variable to ensure scale independence for the median d day precipitation intensity and average monthly minimum discharge are given in fig a 2 for d day precipitation and intensity the rmse decreases from a number of 120 gauges in the study area still the rmse is not very high for small numbers of gauges in the basin either thus it can be argued that the proposed regionalisation of these three characteristics can be applied also for basins with low gauge density interestingly the mean minimum discharge is not at all affected by the number of available gauges and only the variance reduces with increasing number which is not surprising since the samples of gauges vary less with increasing number this independence of the mean minimum discharge from the number of available gauges can be explained by the applied normalization with the catchment size which makes the mean minimum discharge less variable between the catchments precipitation instead remains variable and a higher density of gauges improves the estimation b selection of the catchment attributes the number of available catchment attributes that can be used for regionalisation is high there exist topographic and geological variables that are directly available from gis or meteorological attributes though many different attributes are available for regionalisation they often do not all contain valuable new information for example the catchment elevation is correlated with the soil and land use the higher the catchment the less agriculture one will have and e g the less sandy soils if the nature class is considered too the information one will obtain from soil class and land use decreases even more therefore we performed a pre selection of attributes that were used for regionalisation however this is a catchment specific selection for different regions e g arid or semi arid different attributes could be more suitable for regionalisation here we used the attributes given in table 2 and compared them with the proportion of land use in the catchment derived from corine european union copernicus land monitoring service 2018 european environment agency eea the soil type with largest proportion on the catchment obtained from bük200 and the river network density obtained by spatial analysis of the river network from hydrorivers for each combination of attributes the correlation was estimated either with the spearman correlation two continuous variables the interclass correlation case 3 shrout and fleiss 1979 one continuous and one nominal variable or cramer s v two nominal variables based on the correlation between the attributes fig b 1 in the appendix it can be seen that there exists a high inter dependency between many attributes with correlation larger than 0 3 which served as threshold for the selection of independent attributes for example river network density is highly correlated with the elevation the higher the catchment is located the larger is the density of the river network the soil is correlated with the nature class provided that this information is included in the definition river network density area and the land use classes are slightly correlated with almost all attributes therefore we decided to exclude attributes which were correlated with many other attributes since not much additional information can be expected from them at the same time the correlation matrix also revealed important structures for the remaining attributes some are also highly correlated and should not be considered jointly for clustering e g the median d day precipitation and the 80 quantile of the d day precipitation such attributes where not considered for clustering at the same time but only one of them at each time since different catchment classes lower and upper meso and macroscale were considered when catchment area was used for regionalisation it was avoided to have correlation of attributes with the catchment area c dominant flood types for selected return periods d regionalisation error for all flood types 
8566,the regionalisation of flood frequencies is a precondition for the estimation of flood statistics for ungauged basins it is often based on either the concept of hydrological similarity of catchments or spatial proximity similarity is usually defined by comparing catchment attributes or distances here we apply flood types in regionalisation directly to consider the type specific aspects of similarity the different flood types are classified according to their meteorological causes and hydrographs their probability distributions are modelled by type specific distribution functions which are combined into one statistical annual mixture model afterwards for regionalisation we specified the parameters of each type specific probability distribution separately with hierarchical clustering and regressions from catchment attributes by selection of most relevant features depending on the flood type the specifics of flood generating processes and meteorological causes were considered the results demonstrate how this consideration of deterministic aspects can improve the transferability of distribution parameters to ungauged catchments the type specific regionalisation approach offers a higher degree of freedom for regionalisation as it describes the relationships between catchment characteristics meteorological causes of floods and response of watersheds keywords regionalisation flood types mixture model flood frequency analysis 1 introduction regionalisation is one of the main important research topics for predictions in ungauged basins it includes many complex challenges both of climate and anthropogenic nature sivapalan et al 2003 since the call by sivapalan et al 2003 and the following decade dedicated to the prediction in ungauged basins pub many new insights on this topic were obtained yet there are still open questions in regionalisation arsenault and françois 2016 regionalisation is defined as the transfer of parameters or variables from hydrologically similar catchments to a catchment of interest blöschl and sivapalan 1995 therefore regionalisation is of particular interest for regions where the hydrological gauging network is sparse and the extension of it is not possible the main challenge in regionalisation lies in the identification of catchments similar to the site of interest in order to specify the options and limitations of transferring hydrological information in space this similarity forms the basis for the transfer of information from gauged to ungauged basins where false assumptions can lead to serious errors in the outcome of regionalisation basically one can distinguish between two concepts of similarity spatial proximity and hydrological similarity merz and blöschl 2005 the first concept assumes that catchments close to each other have similar runoff regimes such that distance based approaches like k nearest neighbours are used to define similar catchments recent studies extend classical concepts of spatial proximity by using machine learning to handle large data sets oppel and schumann 2020 the second concept hydrological similarity assumes that similar catchment attributes lead to similar runoff regimes hydrologically similar catchments are then identified by statistically defining homogeneous groups e g by clustering principal component analysis or multiple regressions with the flow as response variable of the catchment attributes nathan and mcmahon 1990 a summary of regionalisation studies for different catchments was performed e g by odry and arnaud 2017 or salinas et al 2013 the above mentioned methods to define similarity have their benefits and drawbacks the choice of the methods mostly depends on the given circumstances like data availability the climate in the region of interest or catchment characteristics however not only the method for the definition of hydrological similarity can be varied but also the aim of regionalisation is diverse the two main applications are the specification of model parameters in rainfall runoff modelling e g bárdossy 2007 merz and blöschl 2005 viviroli et al 2009 or of flood statistical indices e g perez et al 2019 the regionalisation of flood statistics follows two different paths one approach tries to regionalise flood quantiles of given return periods the other one aims to specify the parameters of a statistical model similar to the regionalisation of deterministic models the regionalisation of flood quantiles or the derivation of regional flood frequency curves is most often based on assumed proportionality to the catchment area following the concept of scale invariance de michele and rosso 2002 like in the index flood approach dalrymple 1960 if instead the parameters of the statistical model are regionalised directly a higher flexibility in the model is obtained yet the uncertainty is increased since normally several parameters have to be estimated tyralis et al 2019 however the estimated parameter specific homogeneous regions could also provide insights on the different flood generating mechanisms in general it turned out that the consideration of flood generating characteristics could be beneficial for the regionalisation of flood statistics e g de michele and rosso 2002 parajka et al 2012 for example the seasonality index developed by burn 1997 was shown to improve regionalisation in many cases and is an often used characteristic for regionalisation merz et al 1999 ouarda et al 2006 by considering the seasonality indices the flood generating processes of those floods that led to the largest flood peaks can be considered in regionalisation implicitly still they do not cover the full spectrum of processes for all flood events but only of the one with largest peak per year one index to describe seasonality seems to be insufficient if multiple mechanisms of flood generation have to be considered throughout the year the potential of these mechanisms to generate large flood peaks is non uniformly distributed fischer et al 2019 there is a gap in regionalisation when it comes to explicitly taking into account deterministic flood generating processes in the definition of hydrologically similar clusters for regionalising the parameters of the statistical distribution we aim to close this gap by combining regionalisation with type specific flood statistics flood types offer a possibility to consider the variety of flood generating processes they are normally based on the categorisation of flood events by their hydrometeorological drivers and dominating hydrological processes or they are hydrograph based tarasova 2019 to apply the first approach the main drivers of floods have to be identified even if we use long time series the information about these processes is limited flood typologies which are based on analyses of hydrographs are an alternative approach they are easily applicable to long discharge series and in particular suitable for the application of peak over threshold statistics pot since they require only few additional information to the discharge fischer et al 2019 in this paper we consider a hybrid flood typology for regionalisation using both the hydrograph shape as well as precipitation and snowmelt data with the aim to better capture the differences among meteorological drivers and the initial and boundary conditions of flood processes more precisely we use a statistical mixture distribution that explicitly considers the different flood types fischer 2018 for regionalisation we derive relationships between the parameters of type specific distribution functions and catchment characteristics by the typing of floods we can search for the type specific most relevant catchment characteristics this has implications for the definition of homogeneous regions which were chosen according to hierarchical cluster methods or multiple regression in a type specific way the considered characteristics differ in their relevance between flood types for example the rainfall intensity plays a more crucial role for heavy rainfall floods compared to snowmelt floods therefore clusters were chosen for each flood type separately such that different drivers and impacting factors could be considered separately the results are compared to those obtained from regionalisation of annual maximum series in a second step the differences between the goodness of regionalisation for a single flood type were discussed as the relevance of flood types varies with quantile ranges fischer and schumann 2019 it gives an opportunity to reduce the complexity of regionalisation with this approach we aim to improve regionalisation by identification of connections between flood generation and catchment attributes the proposed method offers a deeper insight into the relevance of catchment attributes for specific flood types and how these can be incorporated in regionalisation the following research questions are addressed how can regionalisation of flood statistics take into account the flood type explicitly which catchment attributes should be considered for different flood types how does the ability of transferring distribution parameters to ungauged catchments depend on the available number of gauges and the catchment attributes 2 data the catchments considered in this paper are mainly located in bavaria in southern germany parts of the catchments are also located in austria and czech republic the catchments belong to the three main basins of the danube river main river a tributary of the rhine river and saale river a tributary of the elbe river the physiography ranges from alpine catchments with mean elevation of 2000 m a s l in the south to uplands in the north and east and lowlands with mean elevation of 300 m a s l in the centre of the study region catchment sizes vary from small headwater catchments with minimum catchment size of 42 km2 up to the large macroscale catchments of the danube with more than 10 000 km2 an overview of the catchments and topography is given in fig 1 a the analyses in this paper are based on the daily mean and monthly maximum discharges daily precipitation data and spatial catchment characteristics in an initial step discharge data were checked for data errors or anthropogenic impacts like reservoirs or water transfers in or from neighbouring basins using catchment information as well as mann kendall trend test and wilcoxon test both tests are among the most common tests in hydrology for trends and change points in mean and have the advantage that they are non parametric and robust the mann kendall test is able to detect continuous changes in the discharge data e g caused by increasing urbanisation in a catchment leading to a faster response of the catchment and therefore higher flood peaks the wilcoxon tests detects abrupt changes in mean e g caused by the building of a dam and therefore the reduction of peaks and increase of low flows a minimum observation period of 30 years was required to perform the flood frequency analysis the application of these criteria resulted in 168 catchments with observation periods from 30 to 119 years to be suitable for our purposes for computation of catchment characteristics a digital elevation model with a resolution of 1x1km was used courtesy of the european environment agency precipitation and temperature data were derived from the e obs 0 1 degree grid data set cornes et al 2018 snowmelt data were computed with the hbv model bergström 1995 from precipitation temperature and elevation zones of 100 m we applied a lumped version of the model except for the estimation of snowmelt which was done for different elevation zones and summed up afterwards with the degree day method for estimating snowmelt and a single linear storage for routing the first year of daily discharges was used as warm up period each discharge series was split into a calibration period first 60 and a validation period last 40 where the bobyqa algorithm powell 2009 was applied to optimise the parameters in a total of 15 runs with 1000 repetitions each per catchment the hbv model performance was measured with the nash sutcliffe efficiency applied to all daily discharges in the validation period which was 0 738 on average selected catchment attributes of the study area are given in fig 2 for the regionalisation a spectrum of different catchment attributes could be used the area of the catchment the elevation land use soil storage capacity statistical parameters of rainfall and general discharge characteristics e g from a hydrological atlas these attributes were selected to include aspects like the topography which has an impact on the hydrograph shape and its flashiness the nature class which served as indicator for the soil and is closely connected to land uses like agriculture and forestry e g sandy soils are expected to decrease the flashiness of floods compared to rocky soils pfister et al 2017 while a high degree of urbanisation leads to a fast catchment response or quantiles of the sum of flood inducing rain which was used as an indicator for pluvial exposition of regions and the spatial extremity of the flood inducing rain events the nature class is an eco geographical construct which is provided by the federal state of bavaria https www lfu bayern de natur naturraeume index htm a natural class is defined by similar geological morphological hydrological climatic and land use characteristics but also the distribution of species of flora and fauna five different nature classes are defined for the considered study region fig 1b the nature class assigned to a catchment was defined by the highest proportion on the catchment area however since one nature class western uplands only consisted of three small catchments which is not feasible for a separate regionalisation this was merged with the neighbouring nature class the south western uplands catchments which are mostly located outside of the defined nature classes are assigned with the adjacent nature class soil types were available from bük 200 krug et al 2010 and river network density was derived from the available river network with 100 m 100 m resolution derived from hydrorivers by dividing the river length by the catchment size the river network density can provide information on the probability of flood superposition and routing in complex river networks the flood peak may increase due to superposition of flood peaks while in less complex networks the hydrograph may be reduced in peak downstream seo and schmidt 2013 more information on how to derive the type specific catchment attributes is given in the following section 3 methodology 3 1 flood event classification to make an application of a flood event classification possible flood events had to be separated from the continuous series of daily discharges first for this purpose the automated separation method proposed by fischer et al 2021 was applied which allowed for a fast semi automated computation for all catchments the variance based threshold for identification of flood events t h var va r dvar θ var v a r dvar was used with a 3 day window dvar and θ 0 25 since the overall aim of this study was the estimation of flood statistics the flood peak value of each event was estimated by using the monthly maximum discharges a quasi instantaneous value based on high resolution measurements this way the use of the actual peak value was ensured instead of using the daily average the latter could lead to serious underestimation of the flood peak due to daily smoothening especially for small catchments where the runoff dynamic is high event separation resulted in approximately 2 4 flood events per year in each catchment second flood event classification was applied where the hybrid classification by fischer et al 2019 was chosen this classification has the advantage that it only requires flood events in daily resolution precipitation and temperature data the latter two for the estimation of snowmelt a summary of the flood event classification is given in fig 3 a first the flood events were differentiated into snow impacted and rainfall induced floods by applying a threshold to the amount of flood generating water sum of snowmelt and rainfall associated with the flood event that originated from snowmelt second the snow impacted floods were clustered further into rain on snow floods high amounts of rainfall together with snowmelt referred to as s1 and snowmelt induced floods dominated by water originating from snowmelt with small contributions from rain referred to as s2 for clustering the kmeans algorithm was applied to the amount of rainfall amount of snowmelt and the runoff coefficient direct flood volume without baseflow divided by the sum of rainfall and snowmelt of the corresponding flood event the rainfall induced flood events were classified according to the linear relationship between flood peak and flood volume both characteristics were estimated from baseflow separated flood events straight line method fischer et al 2021 and represent the direct runoff only three rainfall induced flood types were classified by optimising the coefficient of determination and hence the linear regression model for the relation between direct peak and volume fig 3b flood type r1 resulted in floods associated with high peaks and small volumes and hence a flashy shape of the hydrograph these flood events occurred together with short and intense rainfall flood type r2 corresponds to flood events with a medium peak volume relationship mostly occurring due to 5 10 days of rainfall of moderate intensity flood type r3 includes flood events that are characterised by large volumes but seldom have large peaks these events occur due to long duration rainfall of several weeks the distribution of the flood types and their frequency is not uniform for the considered study region and depends on the catchment location and the magnitude of the flood peak fig 3c the peak over threshold pot sample was obtained with a threshold of thrice the mean discharge to only include large floods we considered pot samples since the classical flood frequency analyses applies given thresholds for partial duration series more details are given in the following section 3 2 type specific flood statistics to consider each flood type separately and to identify the impact of each flood type on the joint flood statistics in flood frequency analysis the use of type specific mixture distributions is required such a model was proposed e g by fischer 2018 for a general number of m flood types assume that each flood type corresponds to a sample of flood peaks x 1 j x n m j j 1 m in a first step each flood type was modelled separately for this purpose a pot approach was applied with a type specific threshold u j the distribution of the exceedances of this threshold x i j u j was modelled for each flood type j j 1 m by the generalized pareto distribution gpd defined as g j x θ j κ j β j u j 1 1 κ j x u j β j 1 κ j for a shape parameter κ j 0 and scale parameter β 0 with support x u j the choice of the threshold u j is crucial and has a large impact on the resulting flood quantiles here we chose u j type specifically as equal to 3 times the type weighted mean discharge the type weighted mean was derived from the monthly means of discharges weighted according to the relative frequency of the flood type in the respective months this choice was based on german directives dwa 2012 and makes the results comparable to those obtained from the annual maximum series fischer 2018 the pot approach can be generalized to an annual distribution g j for each flood type simply by application of the total probability theorem to obtain g j x k 0 p j l k g j x θ j u j k where p j l k is the probability that the annual number l of flood peaks of type j above the threshold u j is equal to k and can be described by the poisson distribution with parameter λ j cunnane 1973 stedinger et al 1993 p j l k λ j k k e λ j to obtain the annual joint distribution of all flood types a mixture model was applied h x j 1 t g j x θ j u j 1 f j u j ϑ j f j u j ϑ j the model is denoted as type based mixture model of partial duration series tmps in this model the pot distribution g j from above is multiplied with the distribution of non exceedance of the threshold f j f j was modelled by the generalized extreme value gev distribution based on all flood peaks in the sample of the respective flood type f j x ϑ j ξ j μ j σ j exp 1 ξ j x μ j σ j 1 ξ j for 1 ξ j x μ j σ j 0 where ξ j r is the shape parameter σ j 0 is the scale parameter and μ j r is the location parameter 3 3 regionalisation regionalised flood frequency analyses consist of two fundamental steps the selection of similarity measures and the grouping of catchments into hydrologically similar clusters in the first step one can differentiate between two basic concepts the runoff similarity and the catchment similarity we use a combination of both where runoff similarity is considered implicitly by distinguishing between different flood types while catchment similarity is introduced by defining clusters of catchments according to their characteristics to obtain the type specific clusters in the proposed novel regionalisation introduced in this section the available geo data had to be further processed to specify catchment attributes with regard to the different characteristics of the flood types especially rainfall and discharge attributes were estimated type specifically one of these characteristics is the flood inducing rain for each flood type the medians of the duration of the related rain event d in days were estimated for which 80 of the total flood inducing rain fell for these time spans and each flood event type the median and the 80 quantile of flood inducing amount of precipitation were estimated however for ungauged catchments this sum cannot be derived directly since it results from flood event analyses in these cases the characteristics were estimated by using the type specific weighted mean of the d day sums of the 10 nearest neighbour catchments weights were chosen according to the similarity of catchment sizes with the target catchment where the catchment with most similar area was considered with the largest weight more details are provided in the appendix a with this approach we considered the scale variability of response time similarly the 80 quantile of the maximum 1 day rainfall during all flood inducing rain events was considered type specifically for each catchment as an indicator for rainfall intensity to take into account the variability of the runoff conditions and the storage capacity of a catchment the averages of the monthly minimum discharges daily means were considered it can be assumed that the discharge is directly related to the storage of the catchment for example in the water balance equation karlsen et al 2019 to take into account the seasonally different occurrence of each flood type here a weighted mean of these monthly minimum discharges was estimated for each flood type where the weights were determined by the relative frequency of occurrence of this flood type in a certain month this attribute is normalized by the catchment area to avoid the scale impact for ungauged basins both characteristics rainfall intensity and storage capacity were estimated similarly to the d day rainfall sums from observed neighbouring catchments with regard to snowmelt and orographic rain differences between hypsometric curves were considered as another attribute for regionalisation it can be assumed that their shapes have an impact on the temporal sequence of snowmelt according to the proportions of elevation zones and plateaus with similar elevation the elevation itself was not considered in these hypsometric analyses but included in terms of the mean elevation of each catchment and the elevation range table 1 to compare the shapes of the hypsometric curves all curves were separated into 50 equidistant sections using clustering of longitudinal data cld and by application of the extended kmeans clustering these curves were then clustered fig 4 the number of clusters was varied between two and six for 20 repetitions where five clusters were selected as best fit according to the calinski harabasz criterion calinski and harabasz 1974 using the kml package in r genolini et al 2015 cld in combination with kmeans makes clustering of several samples with ordered indices possible by a modification of the classical kmeans with euclidean distance where classical clustering methods fail to classify these ordered samples according to their shape and not their values this pre processing led to the catchments attributes available for regionalisation listed in table 1 for a regionalisation of the type specific distributions and the joint tmps model several parameters had to be considered for the tmps model the six parameters κ j and ξ j shape β j and σ j scale and u j and μ j location had to be estimated for the annual flood type specific distributions g j additionally the poisson parameter λ j had to be estimated the regionalisation was performed iteratively this means that the clusters that were used for the regionalised estimation of the parameters κ ξ β and σ described in the following were optimized with each iteration step to check the hypothesis that the explaining potential of different catchment characteristics depends on the flood types we used all of them table 1 to estimate the clusters resulting in minimum error for the regionalisation for each flood type all possible combinations of characteristics from table 1 were tested and the one with minimum absolute normalized bias see section 3 4 for definition was kept for regionalisation this procedure was done twofold first only one parameter at a time was considered while the remaining parameters were treated as known i e the at site fitted values were used second clusters for all parameters were optimized jointly to reach a global optimisation for the second step the results from the first step served as initial values and benchmark for ungauged basins appropriate characteristics can be selected by performing a similar analysis to the gauged catchments in the study area before applying the regionalization procedure the catchment attributes were checked for correlations among themselves if highly correlated attributes are included in the regionalisation the information gained is limited for our study region the evaluation showed that river network density soil type and land use were highly correlated with the remaining attributes and therefore were excluded in regionalisation appendix b please note that regionalised values of type specific attributes median 80 quantile d day precipitation intensity and average monthly minimum discharge as described in appendix a are used in the regionalisation to take into account the uncertainty in estimation of these 3 3 1 regionalisation of the shape parameters κ j the shape parameters of both gev and gpd are known to be afflicted with highest uncertainty in estimations as the shape is strongly affected by the very extreme events which not only depend on the location of the catchment but also on the observation period e g ragulina and reitan 2017 therefore shape parameters are most difficult to regionalise in contrast to scale and location parameters the shape parameter is not directly linked with the catchment size or the mean flood peaks in many regionalisation approaches e g the index flood method dalrymple 1960 the shape parameter is assumed to be equal for all gauges de michele and rosso 2002 but this assumption could not be applied here with standard deviation of 0 5 up to 0 8 depending on the flood type the shape parameters had higher variation than can be explained by uncertainty in the estimation meaning that it was beyond the confidence bands despite of these problems shape parameters are characterising the high quantiles and the upper tail which are most relevant in the flood design practice to investigate which catchment attributes have the highest relevance for regionalising the shape parameters we applied a clustering approach the resulting clusters of gauged catchments were used to estimate the shape parameter for ungauged catchments however the number of clusters was not known beforehand we applied a hierarchical clustering a distance based approach that uses a dendritic structure and which does not require a pre defined number of clusters in each step the number of clusters is reduced by merging those with smallest distance using an agglomerative approach we applied ward s minimum variance criterion for linkage and euclidean distance as similarity measure to define the number of clusters required after each clustering the total sums of squared distances from the cluster centres tss were calculated the total sum of squares for a number of k clusters and n observations of p variables is defined as tss k 1 k i s k j 1 p x ij x kj 2 where s k is the set of all observations in cluster k x ij is the ith observation of the jth variable catchment attributes from table 1 and x kj is the jth variable of the cluster centre for the kth cluster the smaller the tss values the better defined are the clusters of course these values are declining with increasing number of clusters at the same time a high number of clusters is not beneficial for regionalisation since the assignment of an ungauged catchment to a cluster becomes more difficult the final number of clusters therefore had to be balanced between low tss and a small number of clusters to define this final number of clusters for each shape parameter and each flood type the tss was plotted against the number of clusters as soon as the tss decreased significantly slower which was tested by fitting two regressions and comparing their slope with a t test the final number of clusters was reached e g salvador and chan 2004 for the resulting clusters the respective type specific shape parameters κ j and ξ j were then estimated as the median of the shape parameters of the gauged catchments in the clusters hierarchical clustering can only be applied to values in the interval 0 1 therefore the input values were normalized by application of the min max transformation for ungauged catchments the assigned clusters can be identified by using the estimated centroids for this identification the respective catchment attributes used for clustering have to be considered and the minimum distance of these attributes to the centroids of the clusters has to be calculated to identify the assigned cluster 3 3 2 regionalisation of the scale parameters the scale parameters of gpd and gev distributions β j and σ j depend on the variability of the samples with regard to scale dependencies of both parameters two opposite aspects have to be considered in large basins the storages in soil and the river networks balance the variabilities of rainfall intensities within rain events but heavy rain close to the outlet of such basins may result in extraordinary high peaks nicótina et al 2008 therefore large catchments can be assumed to show scale dependence of the scale parameter for small catchments a unique relation between catchment size and scale parameters seems to be unlikely since the response of the catchments is more direct meaning that the rainfall is transported faster into the river and not smoothed over area therefore the catchments were differentiated by their area into four categories two without and two with consideration of scale dependence the lower mesoscale 300 km2 the upper mesoscale 300 1000 km2 the lower macroscale 1000 10000 km2 and the upper macroscale greater than10000 km2 becker 1995 this differentiation made a different handling of the catchment sizes possible and at the same time preserved a similar and sufficient number of catchments for each class fig 6 while for catchments of the lower and upper macroscale mostly a linear dependence of the scale parameter on the catchment size was present this was not the case for catchments of the lower and upper mesoscale smaller catchments were affected by a much higher variability in the scale parameter and no nature class specific clusters were visible fig 5 this can be explained by the very different topography of the considered catchments fast reaction times and a direct dependence on variable rainfall intensities the different catchment characteristics affect the flood generation and therefore the variability of floods for example in steep catchments in the alps flash floods are more probable than in the flatlands this led to the following differentiation in the regionalisation of the scale parameters β j and σ j for the catchments of the lower and upper mesoscale a clustering approach analogously to the shape parameter section 3 3 1 was applied to define the clusters since a linear relation between catchment size and scale parameter cannot be assumed instead the scale parameter is more related to the catchment attributes and thus a clustering approach similar to the shape parameter is required again the considered catchment attributes were selected type specifically for the lower and upper macroscale a linear relation between the scale parameters and the catchment size was assumed ϕ i j k α i j k γ i j k a e where k 1 2 indicates the parameter ϕ 1 β ϕ 2 σ j 1 m indicates the flood type and i 1 2 indicates the catchment class for example γ 1 2 1 is the slope of the linear regression between catchment size and parameter β 2 for flood type r2 and catchments of the lower macroscale ae denotes the catchment size yet results in fig 7 delivered a dependence structure within the linear relationship there is not one group but two groups indicating a different slope parameter in the linear regression for these two groups again these groups were not related to natural classes the definition of the two groups was done by fitting two linear regressions to the catchment size and parameter values under consideration of every possible combination of two groups then those regressions were chosen that delivered the overall greatest coefficients of determination this way the data were split such that dependence was highest within the two groups for ungauged catchments the parameters were estimated by using the catchment size and the linear regression model of the neighbouring gauges 10 nearest neighbours of the same catchment area class 3 3 3 regionalisation of the location parameters the threshold parameter of the gpd distribution u j and the location parameter of the gev distribution μ j determine the location of the distributions they almost completely depend on the catchment size since they mirror the mean annual flood discharge of the catchment in classical regionalisation theory dalrymple 1960 de michele and rosso 2002 the location parameter therefore mostly is estimated by a non linear relationship to the catchment size in regressions here the threshold parameter u j of the gpd was defined by mean discharges for the gauged catchments directly and hence its dependence on the catchment size is obvious therefore regionalisation of these parameters was done analogously to the scale parameters of the lower and upper macroscale section 3 3 2 a linear regression through origin was performed for two groups for each of the four catchment classes lower and upper mesoscale lower and upper macroscale differentiated by the parameter and the flood type goodness of fit coefficients of these regressions are given in section 4 3 3 4 regionalisation of the poisson parameter the poisson parameter λ j describes the number of flood events per year for each flood type j 1 m it is used for estimations of type specific annual distributions since there is always a high coincidence between the numbers of floods per year with neighbouring gauges this parameter can be estimated for ungauged basins simply by using the type specific mean of the annual events of the same type at neighbouring gauges weighted by the difference in catchment size similar to the regionalisation of the d day precipitation sum for neighbouring catchments with similar catchment size the largest weight was assigned to estimations of these means 3 3 5 regionalisation of the distribution of the annual maximum discharges to allow a comparison of the proposed model to classical models in flood frequency analysis the annual maximum series ams of discharges is considered too for the ams the gev is among the most frequent applied distributions salinas et al 2014 and led in this case to the overall best performance in terms of anderson darling test and akaike information criterion aic compared to other common distributions such as log pearsoniii gumbel and normal distributions therefore it was decided to use this distribution for the regionalisation of the ams the three parameters of the gev have been introduced before and to make a comparison between both models fair they are regionalised in a similar manner as the gev parameters of the tmps model the shape parameter ξ is estimated by the median value of all shape parameters of gauged catchments in the same cluster as the ungauged catchment clusters are once again defined according to the catchment attributes and optimised iteratively type specific attributes like the d day rainfall were not considered for clustering instead these attributes were considered as non type specific meaning that one overall attribute was calculated for all floods in case of the median d day rainfall this would mean that duration d as well as the median rainfall over d days were estimated for all flood events jointly the scale parameter σ was regionalised analogously to the scale parameters σ j above according to the catchment size class regional estimates of the location parameter μ were obtained by regression on the catchment size this way the regionalisation of the ams is similar to the tmps but without any consideration of flood types it has to be pointed out that this comparison does not provide information on the ability of the newly proposed method to outperform classical regionalisation procedures but instead it demonstrates if the method is suitable for transferring the parameters of the tmps model to the ungauged locations compared to the suitability of this method for regionalisation of ams 3 4 error estimation to evaluate the goodness of the performed regionalisation the delete d jack knife method was applied for each of the considered 168 catchments at one time d catchments were chosen these catchments were left out in the regionalisation process and handled as ungauged then with the estimated regionalized distribution parameters the annual distribution of each flood type and the tmps model were estimated for the regionalisation the regionalised type specific catchment attributes such as d day precipitation sum intensity or average monthly minimum discharge were used clusters were estimated based on two different approaches first all available catchments were used to build the clusters while for the estimation of the shape and scale parameter of each cluster only the gauged catchments in a cluster were used this approach considered that in theory it should be possible to define clusters for all ungauged catchments in a study region since the cluster definition relies on catchment attributes only which could be derived even for ungauged catchments if not noted otherwise results are based on this approach second only the gauged catchments were used for cluster estimation to estimate the impact of the gauge density on the clustering afterwards quantiles for selected return periods t 10 100 500 years were estimated the choice of the return periods was made such that flood quantiles with medium return period 10 year return period as well as standard design floods 100 years and extreme floods of the right tail 500 years were compared though flood quantiles with 500 year return periods are afflicted with uncertainty it is important to analyse these as well since only in this range of high quantiles the tail behaviour of the distribution can be understood it has to be noted though that for bounded distribution functions as was present e g for some of the snow impacted flood types the difference between 100 and 500 year return period was small the estimated regionalised quantiles were compared with the estimated at site quantiles which are the ones obtained from fitting the flood type specific distributions and the tmps model to the observed flood peaks as benchmark d was chosen as 10 with 100 repetitions of randomly drawing d catchments from the sample and we show detailed results for this case however to take into account lower gauge density of basins we also increased d to d 20 40 60 80 such that up to half of the catchments were treated as ungauged in this case repetitions were limited to 50 due to the computational time that is required to estimate the quantiles of the tmps model when treating such large amounts of catchments as ungauged it may occur that one of the catchment area classes is not represented by any of the gauged catchments in this case the run was removed and estimation was repeated until 50 repetitions were obtained since errors in regionalisation of certain flood types might be assigned to certain meteorological causes that were not represented well in regionalisation they may help to identify possible additional attributes that are required for regionalisation for this purpose the nbias estimated for the quantile with 100 year return period with delete 10 jack knife method and 100 repetitions is differentiated by catchment attributes for one attribute at a time meaning that the error is averaged for all catchments with the given attribute as goodness of fit criterion we applied the normalized bias and the normalized rmse the normalized bias and rmse which includes bias as well as variance for return period t are defined as 1 nbia s t 1 d i 1 d q i r e g t q i o b s t q i o b s t 2 nrms e t 1 d i 1 d q i r e g t q i o b s t q i o b s t 2 where d is the number of left out catchments in the jack knife method q i r e g t is the regionalized quantile with return period t for catchment i and q i o b s t is the at site quantile with return period t for catchment i the at site quantile refers to the quantile obtained when fitting the model to the observed data the above definition of the error measurement or variations of it with the use of the total value is standard for many regionalisation approaches salinas et al 2013 however if there is no general bias in the regionalisation positive and negative errors could equalise each other such that the overall error would be close to zero therefore to make an evaluation fair and transparent bias values were estimated with and without absolute values of the errors this way a systematic bias and the overall deviation from the target quantile can be estimated 4 results and discussion in this section we present the results of the regionalisation and discuss them we only present the results for the global optimisation and omit a detailed description of the local optimisation of each parameter this way only the final dependencies obtained with the regionalisation model are presented first the obtained dependencies between the distribution parameters and the catchment characteristics are given second the goodness of the regionalisation approach is evaluated together with the impact of gauge density moreover the type wise transferability of the parameters of the tmps is compared to the case of the annual maximum series third the errors of regionalisation are examined more detailed in terms of dependence of catchment characteristics such as elevation hypsometric curves or catchment size 4 1 relevance of catchment characteristics and differences between regionalisation clusters the resulting number of clusters and the catchment attributes used to define these clusters for each parameter and flood type are given in table 2 the parameter d for the d day rainfall sums was estimated flood type specifically for flood types r1 and s1 this was two days for floods type r2 four days and six days for flood type r3 with the longest flood inducing precipitation period the type specific characteristics d day sum of precipitation intensity and average monthly minimum discharge were regionalised for the ungauged catchments as described in section 3 errors for catchments which were assumed to be unobserved delete d jack knife method were small see appendix a 1 due to this type specific character of the rainfall attributes of the catchments the assignment of a cluster to a catchment could vary between the flood types although the same attributes were used for clustering for the definition of the clusters differences between the parameters and flood types occurred table 2 the relevance of catchment attributes obtained from the regionalisation using attributes leading to the smallest error depends on the different flood generating processes that are related to each flood type e g heavy rainfall for flood type r1 or snowmelt for flood type s2 for the shape parameter of flood type r1 the rainfall intensity 80 quantile of maximum 1 day rainfall and the extreme rainfall sums 80 quantile of d day rainfall were most relevant for defining the clusters floods of this type are associated with heavy rainfall which is characterised by high intensities and large rainfall sums in short time periods for flood type r2 instead which is associated with medium rainfall floods the intensity proved to be the only relevant rainfall attribute which is surprising since one would have expected that the amount of rainfall would be relevant too flood type r3 instead is associated with long duration rainfall of several days with low intensity therefore it is meaningful that only the median rainfall sum was a relevant cluster attribute for flood type s1 the rain on snow floods median rainfall sum and intensity were relevant while for s2 the snowmelt induced floods none of the rainfall attributes was relevant however for both snow impacted flood types as well as flood type r3 the hypsometric curve and therefore the elevation relations in the catchment proved to be relevant the hypsometric curves can be used to describe the snowmelt in the catchment for example if there exist any plateaus where the snow melts in large amounts at the same time for flood type s2 additionally the elevation range of the catchment was used for clustering interestingly for all catchments the nature class and the storage capacity weighted mean minimum discharge were relevant the spatial distribution of the clusters for the four parameters is given in fig 5 clearly spatial coherences occurred though these are not limited to certain nature classes and often are mixed indicating that nature class is not the main attribute used for clustering yet many flood types had one cluster located closely to alps indicating that for this region the flood generating mechanisms may be different than for the alpine foreland though for the shape parameters identical catchment characteristics were used as input data a different number of clusters resulted from the hierarchical clustering for some flood types where the shape parameter of the gev distribution had fewer clusters than that of the gpd distribution table 2 the regionalisation of the scale parameter was twofold for mesoscale catchments a clustering approach similar to the shape parameter was performed for the scale parameters of catchments of the lower and upper mesoscale differences occurred too for these parameters the main input characteristics were the catchment area elevation range and the nature class these factors proved to be important for all flood types which confirms the results from above where a dependence of the scale parameter on the catchment area was shown but this was not the only relevant attribute for clustering again intensity was relevant for flood types r1 and s1 similar to the results for the shape parameters while the median rainfall sum was relevant for the flood types associated with longer rainfall r2 and r3 the resulting clusters showed rather spatially coherent patterns which is due to the use of nature classes and hypsometric curves respectively elevation fig 5 for ungauged basins using the catchments characteristics and the location of the catchment as well as regionalised attributes from neighbouring catchments an assignment to a cluster hence is easily possible one could also argue that some flood types may be similar in their catchment reaction and may be merged in a first attempt it was tested to merge flood types posteriorly for defining clusters of the shape and scale parameters for this the correlation between the clusters was estimated and a correlation threshold for merging flood types was defined such that the regionalisation error did not increase by more than 5 as a result flood types r1 and r2 as well as s1 and s2 turned out to be able to be considered jointly for shape respectively scale parameter similar processes may lead to similar clusters in this case however this was only a first investigation it has to be noticed that the considered attribute nature class is a rather specific one of the study area and since it is provided by state authorities it may not be available or valid elsewhere according to their definition nature classes include probably valuable additional information compared to the sole use of soil and land use data only still there is high correlation between the nature class and the catchment attributes as demonstrated in the appendix a indeed if a cart classification is applied to predict the nature class an accuracy of in mean 73 was obtained using the catchment attributes named in table 1 and appendix b using 60 of the data as training and 40 as test samples and 1000 repetitions therefore even for regions where nature classes are not applicable a similar amount of information can be obtained by e g including soil and land use classes here the nature classes were used to include all possible information e g also on flora and fauna again it has to be emphasised that the selection of catchment attributes for regionalisation depends highly on the considered study area and the flood generating process therein for larger catchments the scale parameter was regionalised differently for catchments of the lower and upper macroscale a twofold linear regression through origin between catchment area and scale parameter was performed the regression for these catchments resulted in corresponding coefficients of determination of more than 0 7 for most catchments fig 6a indicating a strong dependence between the catchment size and scale parameters for macroscale catchments and a high explanatory power of the catchment size for this parameter errors in regionalisation arising from this specific parameter therefore can assumed to be small the same holds true for the location parameters uj and μ j fig 6b who had smallest observed coefficient of determination equal to 0 82 fig 6a though many of the alpine catchments were included in the group for the second regression a clear distinction between alpine and the remaining nature classes did not appear and each group was a mixture of catchments belonging to different nature classes a regression for the alpine catchments in one group and the remaining nature classes in another worsened the regionalisation results by 6 the same holds true when all catchment area classes were considered jointly where additional errors of up to 10 appeared the chosen classification therefore proved to be beneficial for regionalisation resulting in only small errors for the location parameter 4 2 regionalisation error and compensating effect of the tmps based on the clusters and the linear regressions described in the previous section the final regionalisation error was estimated with the delete d jack knife technique again the regionalised characteristics d day sum of precipitation intensity and average monthly minimum discharge were considered for ungauged catchments to make investigation of the error propagation in the different regionalisation steps possible the resulting normalized bias with and without consideration of the absolute value and rmse for the quantiles with return period t 10 100 and 500 years are given in table 3 quantiles were estimated for each flood type as well as for the tmps model as a comparison in fig 7 the mean relative errors of regionalisation i e the difference between at site and regionalized quantile divided by the at site quantile derived with delete 10 jack knife and 100 repetitions are given for each catchment at site quantile in this case refers to the quantile obtained from the fit of the type specific distributions and the tmps model to the observed flood events in a catchment analogous results for the flood types are given in the appendix in fig d 1 the resulting bias for quantiles of return periods of 10 100 and 500 years table 3 differed between the flood types and the joint tmps model in general flood types delivered higher errors than the tmps model though the error is non uniformly distributed between the flood types moreover certain spatial coherences became visible fig d 1 for flood type r3 large errors for distinct catchments occurred mostly in the north and east these are catchments close to the uplands where the fewest overall precipitation sums in bavaria occur the heavy rainfall floods of type r1 showed a pattern of high errors in the northern parts of the danube basin but not limited to a certain nature class leading to a positive bias in the estimation and large variance especially for return period 10 years table 3 r1 floods are related to heavy rainfall events especially the largest flood peaks of this flood type often occur due to local extreme rainfall flash floods which occurs frequently in this region with more than 25 l m2 in one hour dwd 2016 however these mostly local events only have spatially very limited extent and are therefore often recorded for single catchments only which makes them most difficult to incorporate in regionalisation however what is most relevant for the evaluation of the regionalisation is the tmps model itself the type specific errors were partially propagated to the tmps model though the errors for the single flood types can give information on where the highest errors occurred and which potential drivers were not captured well by the model not every flood type played a crucial role for the final tmps model as was shown by fischer and schumann 2020 and also visible in fig 3a and fig c 1 in the appendix a different flood types dominate different quantile ranges of the resulting tmps model meaning that they lead to the highest quantiles the frequency and dominance of the flood types differ between the catchments and large floods are dominated by different flood types than smaller floods fig 3b for quantiles of return periods of 200 years and higher the role of snowmelt induced events was negligible and only few catchments in the north east of the region were dominated by these flood types for the central catchments draining to the danube river the flood type r3 dominated these large quantiles which is due to the large size and slow reaction of these catchments the southern parts instead were dominated by flood types r1 and r2 for almost all quantiles regionalisation errors of flood types that do not dominate the tmps model have less impact on the overall error this mainly applies to snow impacted flood types which often only have an impact on smaller quantiles with return periods of 10 50 years northern parts however this range is represented by many events from different flood types large return periods instead are mostly dominated by one or two flood types most often rainfall induced and therefore more emphasis has to be given to an appropriate representation of these types for 18 gauges mostly in the southern parts snowmelt impacted floods or floods of type r3 were not included in the statistics since less than four events occurred for this type and a calculation of the statistic would not be meaningful these were left out in the mixture model hence the given errors are not automatically forwarded to the tmps model this is important to notice since it reduces the required number of regionalised parameters for the tmps model significantly as different type specific distribution functions determine certain reaches of the tmps distribution we concentrated mostly on the tmps errors in regionalisation and only used the errors of flood type distributions to identify drivers that were not captured well by the regionalisation procedure the nbias without absolute value where the sign of the error and hence a potential bias was considered were in the range of 0 12 and did show a slight positive bias for the tmps model the large nrmses imply that single catchments may have contributed much to this value with very high individual errors the tmps model in total therefore was regionalised well for all quantiles which results from the compensatory effect of deviations of distributions of single flood types when considering the absolute value of the nbias mean deviations from 34 to 44 occurred increasing with increasing return period this is meaningful since of course larger return periods are afflicted with much more uncertainty in estimation for the estimated at site as well as the regionalised quantiles only few events were observed in this range and information is reduced even more since mostly extreme events occurred at several gauges at the same time except for flash floods associated with heavy rainfall which have spatially limited extent see above meaning that for extreme events several events deliver the same information for regionalisation this makes regionalisation especially of the shape and scale parameters difficult still the results are in reasonable ranges and in the lower range compared to existing regionalisation studies salinas et al 2013 the larger number of parameters does not necessarily have to be drawback in regionalisation moreover the type specific consideration can also help in the analysis of the error origins as will be shown in the following subsection the results for increased values of d and therefore under the assumption of decreasing gauge density in the delete d jack knife method are given in fig 8 where up to d 80 catchments were treated as ungauged in panel a the clusters for shape and scale parameters have been estimated using all available catchments under the assumption that even for ungauged catchments the required attributes are available either from a data basis or by regionalisation in panel b only those catchments are used for estimating clusters which are assumed to be gauged all remaining regionalisation steps regionalisation of attributes regression parameter estimation were performed using gauged catchments only and parameters were then transferred to ungauged catchments with an increasing number of catchments that were treated as ungauged the normalised bias increased only slowly fig 8a by 10 moreover the nbias only increased slightly for larger return periods however the variance became much larger as can be seen by the normalized rmse for both increasing number of ungauged catchments as well as return periods therefore the proposed methodology is applicable also for regions with lower density of gauges though results suffer from a higher variability if the clusters are estimated on the basis of gauged catchments only fig 8b the nbias increased by 10 20 an rmse increased by 20 30 compared to the case where all catchments are used for clustering therefore a smaller number of gauged catchments decreases the performance of the regionalisation especially of the cluster estimation still a number of about 100 gauged catchments in the study region delivered regionalisation results with overall errors of about 25 for all return periods a limitation of the methodology consists in the split of catchments into different catchment classes which increases the number of required gauges in the study region to obtain a meaningful representation of each class this was for example the case when 80 or more catchments were treated as ungauged and several runs of the regionalisation had to be repeated due to catchment classes without members the results of the regionalised tmps model were compared to regionalisation of classical flood frequency analyses methods the ams an application of analogous regionalisation techniques for the parameters of the gev ensured a fair comparison between ams and tmps the ams can be seen as special case of the tmps without flood type differentiation results of the regionalisation of the ams are given in table 4 using the same performance measures as before the estimated at site quantiles in this case refer to the quantiles obtained when fitting a gev to the observed ams a delete 10 jack knife with 100 repetitions was applied the simpler ams regionalisation approach with less parameters resulted in larger errors than the tmps regionalisation but with smaller variability the increased number of parameters therefore helps to better capture the variable flood generating processes of the different flood types and makes the proposed regionalisation in combination with the tmps model more suitable to transfer the model parameters to ungauged basins compared to the ams in the ams all these flood types are considered jointly and therefore all processes have to be captured by one parameter set this may reduce the flexibility of the approach especially if the annual maximum series consists of many different flood types the higher variability of the resulting errors for the tmps regionalisation approach indicates that a higher uncertainty is introduced by the additional parameters too 4 3 the impact of catchment attributes on type specific errors since the catchment attributes played an important role especially for defining the clusters of the shape and scale parameters it is crucial to ask whether the regionalisation results differ between the attributes results of this analysis are given in table 5 again we discuss here the error of the tmps model only of course there exist interdependencies between the characteristics which has to be considered for example similar performance can be found for catchments of high elevation and the nature class alps since most of the catchments with high elevation are part of this class close the alps as discussed below the differences between flood types and attributes given in table 5 can be summarised as follows regionalisation of the saale basin resulted in higher errors than danube and main basins this is probably due to the small number of catchments available for this basin only 5 this small number is not sufficient to represent the saale basin adequately in the clustering regionalisation of catchments of lower and upper mesoscale resulted in higher errors than lower and upper macroscale in the macroscale the scale parameters clearly showed as linear relation to the catchment area for the mesoscale these parameters had to be estimated with clustering approaches which probably introduced a higher uncertainty the relationships between parameter and catchment attributes were less pronounced for the smaller catchments surprisingly the alps delivered the smallest regionalisation error while alpine foreland and south western uplands resulted in higher errors which is caused mainly by errors in the regionalisation of flood type r1 in the alps there were more homogeneous clusters for flood type r1 probably because a large number of heavy rainfall events led to a larger information density and homogeneity for this flood type regionalisation of catchments with hypsometric clusters with steep upper parts and flatter lower parts fig 4 class d and b resulted in smaller errors than for those with more uniform shape a and c the higher dynamics of those catchments with steep topography and hence probably faster response are more easily to capture than low dynamics and slow catchments responses the higher dynamics are characterised by higher scale parameters of the distributions which are easier to identify since they are pronounced and define variation of the distribution function for smaller scale parameters several parameter values can lead to similar distribution shapes regarding the flood types especially flood types r1 and r2 showed significant tendencies of where large errors in regionalisation appeared flood type r1 had high errors for small catchments in the alpine forelands with medium elevation and hypsometric curves of convex shape especially the alpine forelands are known to be affected much by heavy rainfall events dwd 2016 again such events are most difficult to incorporate in regionalisation due to their local nature therefore the shape parameters of r1 floods vary much between the catchments depending on whether there occurred a heavy rainfall event or not this could also have an effect on the regionalisation of the input attributes of rainfall intensity and d sum rainfall makes the regionalisation of this type difficult and led to larger errors compared to the remaining flood types for flood type r2 also some extreme events occurred which affected the regionalisation results i e these events lead to higher errors especially for the transfer of the shape parameter since for extreme events less information is provided in the data however the effect and thus the regionalisation error was smaller than for flood type r1 large errors for snowmelt induced floods occurred mostly for alpine catchments with high elevation and convex hypsometric curves for these catchments the varying periods of snowmelt depending greatly on the temperature in the respective elevation zone made a regionalisation difficult since complex snowmelt processes are considered in a rather simple way by the degree day method in this approach 5 conclusion we proposed a novel regionalisation approach for the type specific mixture model of partial duration series tmps the regionalisation is based on catchment attributes that describe topography catchment shape in terms of hypsometric curves or the quantiles of precipitation and flow and is performed type specifically the resulting errors measured with the normalized bias for quantiles of different return periods are in the range of 15 respectively 35 absolute valued and do not show a remarkable bias the number of required gauged catchments depends on the diversity of the considered study area for the topographically and hydrologically diverse region of bavaria considered here a number of 100 gauged catchments proved to be sufficient to obtain comparably small errors this number mostly results from the requirement of an adequate representation of the different catchment classes and clusters for regionalisation where we found a number of at least 10 catchments per class and 5 catchments per cluster sufficient the catchments should be diverse according to catchment sizes and elevation but do not necessarily have to be spatially uniformly distributed moreover we recommend to consider more than 10 gauged catchments per river basin for these numbers stable regionalisation errors and meaningful variance in the regionalisation was obtained the use of the type specific model proved to have two advantages on the one hand as long as the dominant flood types for the mixedt model were estimated well the overall error of the regionalisation was kept small hence not all flood types had to be estimated with high accuracy since not all are relevant for the joint annual statistics for large return periods mostly one or two flood types dominate the sample and the distribution and only these have to be estimated in regions with mixed flood types the proposed approach will be beneficial when transferring distribution parameters to ungauged basins as was shown in this study area when comparing the regionalisation of the tmps to the ams on the other hand the flood type specific distributions and the resulting errors in regionalisation can also be used to better understand the origin of errors since flood types are related to certain meteorological causes errors in distinct flood types can be linked directly to these causes therefore additional data that could improve the regionalisation of this type can be identified easily for example if errors for heavy rainfall floods are large an additional characteristic for regionalisation could be the improved consideration of the temporal distribution of rainfall during events we were able to identify considerable differences in the goodness of regionalisation regionalisation errors were largest for small steep catchments in the alpine foreland and were assigned to errors in heavy rainfall associated flood types a consideration of higher temporal and spatial resolutions of rainfall data which were not available in our study could improve the regionalisation of this flood type since a more detailed description of the spatial and temporal distribution would be possible the regionalisation could be combined with clusters of temporal and spatial rainfall distributions oppel and fischer 2020 where the frequency of certain rainfall patterns can be included as clustering input further improvement especially for the north eastern catchments could be obtained by a better description of the snowmelt here future studies should include a more sophisticated model to describe the snowmelt and also snow cover and include these as attributes into the regionalisation yet to include these data for long time series suitable models have to be developed which is beyond the scope of this paper drawbacks of the proposed methods are the requirement of long data series suitable for flood statistics moreover the clusters used for estimating the distribution parameters are not spatially coherent such that for ungauged basins additional characteristics like the catchment area and the hypsometric curves are required to assign a catchment to a cluster additionally the regressions performed to estimate location and scale parameters require a high density of gauges to have sufficient representation of each catchment size class a merging of classes for basins with lower gauge density is possible though it was shown that this increases the regionalisation error by 5 10 however it was demonstrated that the proposed approach is applicable also for regions with lower gauge density with increasing errors of up to 10 if up to 50 of the gauges where removed moreover the structure of the considered basins has limited complexity and seldom nested catchments are considered for more complex networks or basins with several nested sub catchments and confluences additional to the proposed method also the interaction between catchments flood type changes and flood superposition have to be investigated and included in regionalisation to better understand the spatial scale effects in summary the results emphasised that the proposed method makes a regionalisation of the tmps model possible with comparably small errors but can be improved by more detailed generation specific input data moreover it should be tested whether flood types could be combined for regionalisation this would reduce the complexity of regionalisation a first attempt showed potential for this procedure in future more detailed studies are required that should reveal similarities between the clusters and flood types and link these to the flood inducing processes of course this depends much on the study area and the flood inducing processes therein the proposed approach is more complex in comparison to conventional methods but considers that the sample of observed flood events is not homogenous as the drivers and mechanisms of flood generation differ between flood types by adding more hydrological information future regionalisation studies could concentrate on certain attributes considering the differences among flood generating mechanisms in the catchment and the most dominant flood types moreover the type specific impacts of changes in drivers e g convective rain due to climate change or anthropogenic impacts e g reservoir management could be better captured in regionalisation if type specific non stationary models are considered in future studies in summary the novel approach of consideration of flood types in the regionalisation of the statistical distribution offers a deeper understanding of the relevant flood generating processes and how these are connected to catchment attributes credit authorship contribution statement svenja fischer methodology writing original draft andreas h schumann writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was funded by the german research foundation deutsche forschungsgemeinschaft dfg grant number for 2416 space time dynamics of extreme floods spate we are grateful to bayerisches landesamt für umwelt www lfu bayern de for providing the discharge data climate data can be obtained from the e obs data set https surfobs climate copernicus eu dataaccess access eobs php and the dem from the european environment agency https land copernicus eu imagery in situ eu dem eu dem v1 1 view river network can be obtained from hydrorivers https www hydrosheds org page hydrorivers corine land use data were obtained from european union copernicus land monitoring service 2018 european environment agency eea and bük200 data are available at https www bgr bund de de themen boden informationsgrundlagen bodenkundliche karten datenbanken buek200 buek200 node html the manuscript provides all the information needed to replicate the results r codes for flood typology flood event separation and the statistical model are available in the r package floodr github com philippbuehler the r code for the hbv model can be obtained from the authors upon request we would like to thank two anonymous reviewers and the associate editor franceso marra for their helpful comments that helped to improve the manuscript appendix a estimation of the type specific median d day precipitation sum the estimation of the d day precipitation sums for ungauged catchments cannot be done for the catchment directly the resulting sums are flood type specific and since no information on the flood inducing rainfall is available for ungauged catchments information from surrounding catchments has to be used here the k nearest neighbour approach was applied for each ungauged catchment the k 10 nearest catchments according to the distance of catchment centroids were considered for each flood type the median or 80 quantiles depending on the desired attribute d day precipitation sums were used and the weighted mean was applied the weights of the mean were chosen according to the similarity criterion for catchment sizes more precisely pre c d y i 1 10 ω i p r e c d x i i 1 10 ω i where ω i 1 a e x i a e y y is the ungauged catchment x i i 1 10 are the nearest neighbour catchments ae is the catchment size and pre c d is the median or 80 quantile precipitation for d days exemplary the normalized absolute bias for the median d day precipitation sums is given in table a 1 estimated with the delete 10 jack knife method regional results are given in fig a 1 the precipitation sums for the rainfall induced flood types were all estimated well with errors around 12 20 for snowmelt induced flood types these errors increased to 25 35 probably because the snowmelt plays a more important role for these flood types and rainfall is not defined as precisely as for the rainfall induced floods yet since the precipitation sums were not used directly for regionalisation but only for defining the clusters for some of the parameters the effect on the results was negligible and differences between regionalisation errors with and without regionalised precipitation sums was 1 in mean with a standard deviation of 5 however what has to be taken into account is the gauge density within the study area in our case this density is quite high and may be favourable for the estimation of the regionalized input parameter in other basins this might not be the case and therefore it should be investigated in how far the results of the estimation are affected by the gauge density for this purpose the number of available gauges in the study area was reduced artificially more precisely random samples of different numbers of gauges were drawn from the sample of all gauges for these gauges the approach described above for estimating the regionalized input data based on the nearest neighbour approach was applied for each sample size the procedure was repeated 100 times results in terms of normalized rmse where the difference of observed and estimated variable is normalized by the observed variable to ensure scale independence for the median d day precipitation intensity and average monthly minimum discharge are given in fig a 2 for d day precipitation and intensity the rmse decreases from a number of 120 gauges in the study area still the rmse is not very high for small numbers of gauges in the basin either thus it can be argued that the proposed regionalisation of these three characteristics can be applied also for basins with low gauge density interestingly the mean minimum discharge is not at all affected by the number of available gauges and only the variance reduces with increasing number which is not surprising since the samples of gauges vary less with increasing number this independence of the mean minimum discharge from the number of available gauges can be explained by the applied normalization with the catchment size which makes the mean minimum discharge less variable between the catchments precipitation instead remains variable and a higher density of gauges improves the estimation b selection of the catchment attributes the number of available catchment attributes that can be used for regionalisation is high there exist topographic and geological variables that are directly available from gis or meteorological attributes though many different attributes are available for regionalisation they often do not all contain valuable new information for example the catchment elevation is correlated with the soil and land use the higher the catchment the less agriculture one will have and e g the less sandy soils if the nature class is considered too the information one will obtain from soil class and land use decreases even more therefore we performed a pre selection of attributes that were used for regionalisation however this is a catchment specific selection for different regions e g arid or semi arid different attributes could be more suitable for regionalisation here we used the attributes given in table 2 and compared them with the proportion of land use in the catchment derived from corine european union copernicus land monitoring service 2018 european environment agency eea the soil type with largest proportion on the catchment obtained from bük200 and the river network density obtained by spatial analysis of the river network from hydrorivers for each combination of attributes the correlation was estimated either with the spearman correlation two continuous variables the interclass correlation case 3 shrout and fleiss 1979 one continuous and one nominal variable or cramer s v two nominal variables based on the correlation between the attributes fig b 1 in the appendix it can be seen that there exists a high inter dependency between many attributes with correlation larger than 0 3 which served as threshold for the selection of independent attributes for example river network density is highly correlated with the elevation the higher the catchment is located the larger is the density of the river network the soil is correlated with the nature class provided that this information is included in the definition river network density area and the land use classes are slightly correlated with almost all attributes therefore we decided to exclude attributes which were correlated with many other attributes since not much additional information can be expected from them at the same time the correlation matrix also revealed important structures for the remaining attributes some are also highly correlated and should not be considered jointly for clustering e g the median d day precipitation and the 80 quantile of the d day precipitation such attributes where not considered for clustering at the same time but only one of them at each time since different catchment classes lower and upper meso and macroscale were considered when catchment area was used for regionalisation it was avoided to have correlation of attributes with the catchment area c dominant flood types for selected return periods d regionalisation error for all flood types 
8567,grace gravity recovery and climate experiment and grace fo follow on satellites have provided unique insights into the evolution of terrestrial water storage tws in space and time despite such advancements various grace solutions produced by different data centers display uneven spatial attributes with varying associated uncertainties via spatial diagnostics tools and a modified triple collocation mtc approach this research evaluates the tws terrestrial water storage trend estimations on the grid scale from 11 gridded grace products of rl05 and rl06 releases between 2002 and 2017 distinct from classic tca triple collocation analysis the mtc employs a gwr geographically weighted regression scaling scheme with distinctive spatial coefficients the spatial diagnostics analyses identified different autocorrelation patterns clustering tendencies of hot positive and cold negative spots agglomeration at varying spatial width and unique frequency distributions the results indicated that within a 10 degree spatial radius the shs spherical harmonics of rl05 and rl06 are highly autocorrelated compared to the mascons mass concentration blocks solutions the spatial clustering results revealed that many solutions agreed on the overall directions and distribution of the hot and cold spots the clustering among mascon products however reflected more localized mass anomalies at the scale of drainage basins the trend magnitude as well as their associated uncertainties appeared to be driven by the occurrence of spatial clusters within the basin area the mtc results showed that the uncertainty patterns follow the same spatial extent within each cluster the mtc analysis underscored the added benefits of cluster analysis and the gwr scaling over the classic ols approach keywords grace grace fo triple collocation gwr ols climate spatial statistics autocorrelation cluster analysis 1 introduction grace gravity recovery and climate experiment and its successor grace fo follow on mission have significantly expanded the frontiers of satellite remote sensing applications in hydrology for nearly two decades grace has enabled the hydrological community to explore how terrestrial water storage tws evolves in space and time syed et al 2009 longuevergne et al 2013 rodell et al 2018 tapley et al 2019 analyze the hydrological water budget hassan and jin 2016 felfelani et al 2017 lv et al 2017 scanlon et al 2018 monitor water scarcity status hasan et al 2019a investigate groundwater exploitation flood and drought spell severity rodell et al 2006 strassberg et al 2007 strassberg et al 2009 voss et al 2013 long et al 2014 reager et al 2014 chen et al 2015 richey et al 2015 reager et al 2015 frappart and ramillien 2018 hasan et al 2021 study lake storage dynamics longuevergne et al 2013 moore and williams 2014 geoid height modeling glacial isostatic adjustments and glacial mass fluctuations paulson et al 2007 geruo et al 2013 sutterley et al 2014 these applications are made possible because of an expanding array of new data products as well as ongoing enhancements to existing processing analysis schemes produced by grace developers and science data system sds centers the most prominent of these centers include the center for space research csr at the university of texas at austin ut austin usa the jet propulsion laboratory jpl california usa and the deutsches geoforschungszentrum gfz potsdam germany to produce equivalent water heights ewh as relative changes in the water column in cm or terrestrial water storage anomalies twsa the sds centers have employed predominantly two processing schemes or solutions namely spherical harmonics shs are produced by csr jpl and gfz and mass concentration blocks mascons are produced by csr and jpl the shs offered standard ewh estimates for the first decade of grace observations shs were made available through different parameterizations and filtering strategies i e different degrees and order sh coefficients spatial filter north south destriping smoothing and scaling factors landerer and swenson 2012 long et al 2015 recently the mascons represented an alternative approach to derive comprehensive twsa estimations the mascons have the advantages of retaining an improved gravity signal using location information reducing residual noise and minimizing the spatial leakage error in most cases the mascons solutions could be used directly without applying any scaling factors or any post processing schemes watkins et al 2015 each of these solutions has undergone several iterations and refinements resulting in several distinctive releases rl rl04 rl05 and rl06 each updated release has provided an improved grace product over the prior version s as a result there are dozens of gridded twsa products from the three sds centers at the basin scale however different grace products display different temporal statistical properties such as trends variance or distributions these dissimilarities arise due to algorithmic structures and calibration assumptions that each sds center uses in converting grace signals gravity fields into ewhs consequently different gridded products exhibit distinctive spatial characteristics such as autocorrelation clustering tendencies and frequency distributions this research discusses the noted spatial characteristics in detail the gridded grace product was first introduced by landerer and swenson 2012 researchers however are frequently elected to not prioritize or assign greater weight to one grace twsa solution over another several studies have integrated various grace twsa time series from different solutions to obtain essentially an ensembled twsa estimate at a basin scale e g long et al 2017 and jing et al 2019 still others have relied on a single grace twsa product i e csr m center for space research mascon see for instance sultan et al 2019 and hasan and tarhule 2020 among others to derive the twsa at a given region or for specific hydrologic application in a similar context tourian et al 2018 utilized the grace jpl solution to assess the total drainable water over amazon river basin in addition ahmed and wiese 2019 have reported the rates of the changes in africa s water resources via investigating the short term trends from the tws estimates from csr jpl and gfz datasets with other ancillary observations yet examining the spatial properties among different gridded grace products along with the distribution of the associated uncertainties is not fully explored by the available literature existing studies have compared individual grace time series components at the basin scale i e cyclical trend scanlon et al 2018 and the seasonal fluctuations scanlon et al 2019 with independent lsms land surface models outputs this research therefore is proposing a set of spatial diagnostic tools to determine the spatial characteristics among different gridded grace twsa products specifically we tested the spatial autocorrelation the clustering tendency and the spatial distribution using the long term trend from 11 grace gridded solutions besides we applied a new modification to the classic triple collocation analysis tca approach to assess the relative uncertainties at the grid scale among various gridded grace products as noted earlier different grace twsa time series components were compared to other independent twsa estimates i e lsms based outputs also the associated uncertainties within the grace twsa series were assessed temporally at the basin scale following the standard approach as introduced by tiwari et al 2009 and scanlon et al 2016 specifically removing the deterministic components include the deterministic trend interannual variability and seasonal components from the twsa time series and then express the uncertainty in terms of the standard deviations of the residual component while in widespread use this approach overestimates the magnitude of the uncertainty because the residual nearly always contains some deterministic signals hasan et al 2019b alternatively another approach to assess the uncertainties within grace twsa estimates through comparing them to other independent or ancillary remote sensing data lsm observations and or hydrologic model outputs and the difference between the two estimates is used as a proxy for twsa uncertainty wahr et al 2006 this approach has been criticized on the grounds that the two data types are mismatched in nature further the approach unjustifiably sets up one data type as the standard truth or reference ferreira et al 2016 from a statistical perspective one approach that overcomes the limitations described above is the tca scipal et al 2010 vogelzang and stoffelen 2012 tca assesses the relative uncertainty of three or more products that estimate the same geophysical variable independent measurements using either the standard deviations or root mean square errors rmses stoffelen 1998 vogelzang and stoffelen 2012 tca has been used to provide observation error covariance information required by data assimilation and least squares merging approaches yilmaz and crow 2014 thus it is a stand alone rescaling methodology to remove systematic differences between the signal variance component of observations in data assimilation studies scipal et al 2010 alemohammad et al 2015 pan et al 2015 given that the same geophysical variable may be observed simulated using different methods or instruments tca estimates the relative uncertainties among the different methods by making certain assumptions about for example error orthogonality and cross correlation stoffelen 1998 vogelzang and stoffelen 2012 an attractive characteristic of the tca is that it scales each of the comparison variables to a target measurement without treating any as perfectly observed truth vogelzang and stoffelen 2012 dong and crow 2017 scaling is achieved using an ordinary least square ols model stoffelen 1998 see section 2 3 for additional details others have applied the tca by treating the residual between the truth and further measurements as a proxy to their relative uncertainty e g scipal et al 2010 several studies have successfully applied the tca error model to quantify the uncertainty of mutually yet equivalent measurements alemohammad et al 2015 pan et al 2015 the tca has been broadly used in error quantification in satellite based surface soil moisture products scipal et al 2010 yilmaz and crow 2014 dong and crow 2017 ocean salinity validation studies hoareau et al 2018 sea surface temperature and precipitation estimates among other hydrological constituents see for instance alemohammad et al 2015 tsamalis and saunders 2018 saha et al 2020 a major caveat to the tca approach is the assumption that the error is normally distributed and a single scaling coefficient is applied to the entire domain this paper thus applies several spatial diagnostic tools and a modified tca mtc approach to compare spatial characteristics and quantify the relative error uncertainties among 11 grace twsa solutions from three common sds centers of two widely used releases rl05 and rl06 specifically we used the pixel wise nonparametric mann kendall mk trend test to estimate the secular tws trends see section 2 1 for each grace solution at the grid level then three spatial diagnostics analyses were applied to determine the spatial autocorrelation clustering tendencies and spatial distribution among these estimates further we employed a gwr geographically weighted regression approach to modify the classic ols scaling scheme in the tca to estimate the relative uncertainties within these trend products at the grid scale explicitly the traditional tca employed an ols scaling approach that assumed a sole correlation coefficient for the entire spatial domain the proposed gwr modeling on the other hand applies a gaussian distance decay function to assign different weights with distinctive correlation coefficients at different spatial locations throughout the study domain in response to clustering tendencies and the autocorrelation levels within a specific spatial radius section 2 3 provides further explications of the approach and method finally we summarized the mtc uncertainty results spatially as well as over 80 transboundary river basins in 6 different climate zones see fig s1 for the basins locations this study contributes to the emerging literature to understand the evolution of twsa in space via different spatial characteristics on various gridded grace twsa products globally it also assesses the effect of the spatial domain in analyzing the associated uncertainties within various gridded grace products by analyzing the spatial patterns and statistical distribution of the trend magnitude it contributes to improving the understanding of tws variability within the intra basin scale finally the study highlights the effect of spatial clustering in the distribution of the tws variability as well as their associated uncertainty the analysis is based on the gridded outputs as well as the basin scale average over 80 major drainage basins 2 material and methods this study utilized 11 gridded twsa estimates from two widely employed grace releases the rl05 and the recent rl06 release using both shs and mascons solutions the data cover the period between april 2002 june 2017 and were processed by three grace sds centers csr jpl and gfz table 1 lists the types and spatial resolutions of each dataset the three centers distribute data independently and in different versions or releases for instance between 2002 and 2017 the csr revised updated and distributed five versions of their dataset rl01 rl02 rl04 and rl05 of level 2 data the csr skipped rl03 altogether in order to harmonize their releases with other sds centers in 2018 the csr distributed rl06 of their data the gfz also updated its estimates over five releases or iterations their most recent release rl06 was produced in 2018 like the csr jpl also skipped rl03 and their most recent update is labeled as rl06 as noted previously grace data are available via two processing schemes the traditional sh functions and the mascon solutions the unconstrained harmonics have typically suffered from poor east west gradients resulting in the so called n s stripes these strips are conventionally removed via empirical smoothing or destriping algorithms the disadvantage of that approach however is that it risks removing real geophysical signals along with the stripes the mascons on the other hand provide convenient mass change solutions with less leakage errors and did not require any empirical filters furthermore most mascons solutions are not involving any scaling factors to produce accurate mass estimates this study employed the shs and mascons solutions from rl05 and rl06 however the solutions were originally driven from the same level 1 instrument products level 3 gridded data are uniquely processed by the three sds following distinct analytical schemes background models and forcing parameters in this research we acquired the data in their gridded format as originally posted by the respective sds centers no further processing was done other than applying the provided scaling factors to the shs of rl05 and the jpl mascons of rl05 and rl06 the analytical steps employed in this research are as follows first we calculated the pixel wise long term tws trends using the nonparametric mk trend test mann 1945 hamed and ramachandra rao 1998 pohlert 2018 specifically we made use of the spatialeco package for spatial analysis and modeling utilities evans and ram 2018 in the mk test algorithm accessed via carn r project second to affix the pixel wise trend grids to a consistent spatial scale mk outputs were resampled to the same 1 degree grid resolution using a nearest neighborhood interpolation tool this step is important to later execute the spatial comparison analysis of on same spatial extent third we applied three diagnostic tests namely spatial autocorrelation clustering analysis and frequency distribution using a cumulative density function cdf to analyze the spatial patterns of trends at the pixel levels finally we assessed the relative uncertainty of each grace solution using the mtc approach as described in detail in section 2 3 fig 1 illustrates the schematic flow of the research methodology further description of the above analytic methods appears in sections 2 1 to 2 3 2 1 trend analysis the trend was estimated for the grace twsa gridded series at each grid point using a nonparametric mk trend test the mk statistic s for a series x 1 x 2 x n is calculated as 1 s k 1 n 1 i k 1 n s g n x i x k where 2 sgn x 1 x 2 1 x 1 x 2 0 0 x 1 x 2 0 1 x 1 x 2 0 where x j and x k are all pairs of sample points the mk method tests the presence and significance of a trend but not its magnitude therefore to determine the magnitude of the trend we applied sen s slope estimator as follows 3 b x j x k j k for 1 k j n where b is the slope n the number of data points and j k are indices then a median q was calculated for all slopes b to express the magnitude of the trend 2 2 spatial autocorrelation clustering and frequency distribution analysis the output pixelated median value q for each grace solution was tested for spatial autocorrelation among neighborhood pixels according to moran s i formula legendre and legendre 1998 chen 2013 moran s i is similar to pearson s correlation coefficient except that moran s i tests the correlation between neighborhood pixels within a specified spatial radius the index measures the clustering tendency or randomness within a field of neighboring pixels occurring within the selected range of a 10 degree euclidean radius distance this cutoff point 10 degree was selected because beyond that range the autocorrelation coefficients decay to statistically negligible levels within this radius d the autocorrelation coefficients can be expressed using a spatial autocorrelogram moran s i is computed from pairs of neighboring observations as follows 4 i d 1 w h 1 m i 1 m w hi y h y y i y 1 m i 1 m y h y 2 where y h and y i are the values of the observed variable at locations h and i h i w is the sum of the spatial contiguity weights of w hi for the given distance class the value of w hi is 1 when h and i are at distance d otherwise w hi 0 for a given distance class the weights w are written in an m m matrix then a distance 1 or distance 2 among adjacent sites is used for weight instead of 1 s the weight matrix is defined according to a queen spatial contiguity matrix legendre and legendre 1998 based on whether spatial units share a boundary basically eq 4 numerator is a covariance comparing the values found at all pairs of points while the denominator is the maximum likelihood estimator of the variance i e the division by m moran s i usually takes values in the interval 1 to 1 the negative positive values correspond to areas of declining increasing twsa trends at the specified confidence level thus the test also allows identifying the clustering tendency using a 2 d map of the significant clusters in terms of high high hot spots and low low cold spots finally we used a frequency distribution analysis pdfs probability distribution functions to compare and evaluate the density distribution of trends among the different solutions and releases the comparison was done based on the distribution parameters such as variance skewness and kurtosis the goodness of fit was evaluated using the kolmogorov smirnov empirical distribution function ecdf analysis drew et al 2000 2 3 modified triple collocation error analysis as explained earlier the tca approach requires a minimum of three different series of measurements that are collocated correspondingly in space and time classic tca scales the variables under analysis to one of the observations or variables considered as the truth stoffelen 1998 scipal et al 2010 vogelzang and stoffelen 2012 because the observations are collocated in space and congruent in time tca commonly assumes a degree of autocorrelation exists among them further it is assumed that the distribution of the error term is normal following rescaling in classic formulation the scaling process makes use of an ols scheme in which the same correlation coefficient is applied to the entire study domain effectively therefore the study area is treated as a single undifferentiated unit the procedure is as follows given three sets of measurements x y and z each can be scaled to a true measurement t as 5 x α x β x t ε x y α y β y t ε y z α z β z t ε z where α is the intercept β is the slope and ε is the error term in this classic tca eq 5 is applied in an unweighted form and the relationship is assumed to be the same throughout the study domain besides this approach leads to an underestimation of the errors several refinements to the classic approach have been proposed including the extended triple collocation etc mccoll et al 2014 mccoll et al 2016 saha et al 2020 in which the error term is derived by using the correlation coefficient among each measurement variable to the unknown target the approach used in this paper is the mtc which differs from classic tca that in addition to scaling the study variables it also applies a gaussian distance decay function through a gwr approach distinct from ols scaling the gwr formulation requires a degree of independence among the tested datasets it also assigns spatial weights to each variable based on the occurrence of clusters within the study domain second mtc treats each spatial domain distinctively by producing different spatial coefficients instead of a lumped coefficient used by the general ols scaling schemes the procedure is as follows given a series of spatially distributed measurements x y z herein each measurement represents the median q of the sen s slope trend outputs from eq 3 each derived at the same location p i where i 1 2 in an m m space matrix the following regression equations can be developed among the variables for each location as 6 x p i α x p i β x p i t p i ε x p i y p i α y p i β y p i t p i ε y p i z p i α z p i β z p i t p i ε z p i because the weights in eq 6 are derived for each spatial location p i the weight or spatial coefficient varies across the study domain in the gwr model a radius r of 10 degree euclidian distance centered on each p i is used to derive the set of weights ω ij with values between 0 and 1 the weights were calibrated according to a gaussian distance decay function as follows 7 ω ij 1 d ij c 2 2 if d ij r 0 if d ij r where d ij is the euclidean distance between the location of observation i and j the c is a nonnegative decay parameter describing the relationship between weight and distance the c has an adaptive bandwidth size defined as the nearest neighbor distance among pixels the spatial weights at each location p i are proportionate to the strength of the autocorrelation coefficients within different clusters and between each cluster and spatial distance see charlton and fotheringham 2009 nakaya 2012 nakaya 2015 nakaya 2016 then to scale each variable to a common truth we divided eq 6 by the slope according to fang et al 2012 so that x p i x p i β x p i α x p i β x p i and ε x p i ε x p i β x p i etc then 8 x p i t p i ε x p i y p i t p i ε y p i z p i t p i ε z p i herein we scaled all trend measurements relative to the csr m06 solution then to eliminate this scaling effect we first used a pair wise subtraction as 9 x p i y p i ε x p i ε y p i ε x y p i x p i z p i ε x p i ε z p i ε x z p i y p i z p i ε y p i ε z p i ε y z p i eq 9 eliminates the scaling effect then to eliminate the conjugate error estimates we used a pairwise multiplication the individual unconjugated variance for each measurement which represents the residual errors can be estimated as 10 σ x p i 2 x p i y p i x p i z p i σ y p i 2 x p i y p i y p i z p i σ z p i 2 x p i z p i y p i z p i for eqs 9 and 10 there is no specific combination required among measurements instead any combination of three independent measurements can be used in each run herein eq 10 is used to derive the grid wise variance for each grace solution following standard practice the uncertainty errors at the pixel levels were subsequently expressed as the standard deviation of the error variance or the uncertainty as stated above the tca mtc assesses the relative error among three independent datasets under certain assumptions of the error orthogonality i e the product errors are independent among the tested datasets the error orthogonality can be tested if three or more datasets are available for cross comparison in the case of grace products originally grace solutions are derived from the same level 1 instrument however level 3 gridded products were produced independently by three main grace sds centers each data center is following a unique analytical scheme as well as utilizing different background models to produce their gravity field solutions technically speaking each data center csr gfz and jpl produces different flavors of data products that updated frequently with new releases furthermore at the basin scale see fig s2 in the supplementary info the total tws signals from 11 products displayed a sort of general similarities in the overall pattern however the signal amplitude as well as the secular trends are distinct these would result in significant gigatons year differences in the total storage changes among various products thus this research explicitly quantifies the associated uncertainties to the processed data level 3 gridded products any potential uncertainties linked to the instrumental errors or the retrieval process itself are ignored 3 results and discussion 3 1 grace trends fig 2 shows the gridded based results of the mk trend analysis of the twsa for rl05 and rl06 produced by the three processing sds centers the results show that significant decreasing tws trends occur over the greenland ice mass alaska north canadian archipelago antarctica four main ice covered regions west and the southwest usa the brazilian nordeste central argentina the patagonian ice fields the northern saharan region north central the middle east including portions of the tigris and euphrates river basins and western iran central euro asia the himalayan mountains north china and western australia these declining patterns are consistent with those reported in numerous studies and have been attributed to different dynamics including frequent droughts e g rodell et al 2018 groundwater exploitation chen et al 2015 and severe groundwater depletion due to agricultural activity voss et al 2013 conversely pronounced areas of increasing twsa trends appear in the northeastern usa much of the amazon sahel region in western africa great lake area in east africa okavango river delta and north zambezi river basin in africa central india and eastern australia several studies outlined similar figures i e ramillien et al 2014 panda and wahr 2016 werth et al 2017 shamsudduha et al 2017 tourian et al 2018 and rodell et al 2018 while the mk analysis provides a single aggregated trend value during the reference period the twsa behavior over time can be complicated and attributable to various factors in different parts of the world for example in california usa severe and prolonged droughts that occurred between 2006 and 2010 and 2011 2019 contributed to unprecedented demands for groundwater liu et al 2019 the twsa time series ending in this time frame and for the affected basins therefore would most likely be negative similar trends have been reported in texas usa long et al 2013 as regions recover from such historic droughts they may show pronounced upward trends e g the african sahel region elsewhere increasing trends were attributed to a pronounced increase in rainfall amount e g amazonian region tourian et al 2018 it is worth noting that several of the observed tws trends are consistent with climate and surface model predictions rodell et al 2018 for some locations however the reasons behind observed trends are poorly understood or unknown this includes for example the increased tws trends over the okavango delta for additional explanations of the emerging trends in grace solutions see ahmed et al 2014 anyah et al 2018 and rodell et al 2018 while the twsa estimates produced by the various sds centers show strong agreement on the overall trend at a global scale they displayed distinct magnitude as well as other notable differences at the basin level fig 3 for instance shows the basin scale summary of the trend magnitude from 11 gridded trend outputs for 12 major transboundary river basin systems see fig s2 in the supplementary info for the total twsa time series signals across these basins herein fig 3 shows clear examples of the discrepancies among different grace solutions in terms of the trend magnitude and the direction the sign of the trend within the same basin for instance the nile river basin illustrates the magnitude of differences among various solutions to illustrate comparable figures across basin scales we expressed these changes in the storage using gt yr units based on shs of rl05 the tws trend estimates provided by csr jpl and gfz are respectively at 3 15 gt yr 1 28 gt yr and 4 27 gt yr in contrast the mascon solutions of csr m05 csr m06 jpl m05 jpl m06 non cri and jpl m06 cri estimate the trends at 4 18 gt yr 7 87 gt yr 15 02 gt yr 16 27 gt yr and 16 34 gt yr respectively in some basins the solutions do not agree even on the sign direction of the trend this is the case in the colorado basin in the central and western usa contrarily in the mekong okavango and indus basins different solutions are in strong agreement with only a small range of variation in the trend magnitude finally in other basins like the nile mackenzie and the mississippi the solutions agree on the sign of the trend but the differences in the trend magnitudes are substantial additional results about the tws trends magnitude and characteristics are provided in the supplementary information the tables in the xls sheet s1 s2 and s3 the above differences have important implications for water resources planning policy and management as well as the conclusions of research findings the explanation for some of these differences lies in the calibration assumptions that each sds center made regarding the conversion of grace gravity fields into ewh estimates additional research is required to better understand the effects of pixel heterogeneity on the basin scale of the tws trend analysis see tables s1 and s2 for an additional summary about the trend magnitude and other statistical parameters from 11 grace solutions over 80 transboundary river basin systems 3 2 spatial autocorrelation clustering and frequency distribution analysis fig 4 shows the spatial autocorrelation and the level of clustering tendencies among each unique grace solution within a 10 degree spatial radius euclidean distance overall six sh models of rl05 and rl06 displayed strong autocorrelation coefficients with little variation specifically the neighborhood pixels of all shs have autocorrelation coefficients that ranged between 90 percent within a 3 degree radius to 60 percent within a 5 to 7 degree radius on the other hand some of the mascon solutions e g jpl m05 and m06 cri appear significantly different from the sh solutions the csr mascons of rl05 and rl06 as well as the jpl rl06 non cri solutions however covary with sh solutions with the major difference being a lower coefficient value at a 1 degree distance i e 60 percent versus 90 percent among these solutions the spatial autocorrelation among mascons solutions reflects more localized mass anomalies within each cluster compared to the sh solutions the analysis of morans i delineated clusters of positive and negative twsa trends globally fig 5 plots the clusters statistically significant at α 0 05 which we have denoted as hot positive and cold negative spots fig s3 illustrates the difference between the significant cluster domains versus the conventional p value approach three observations can be made with respect to these plots first the significant level of agreement among all the solutions relative to the location frequency and spatial extent of hot and cold spots for nearly all solutions the hot spots are more numerous and spatially extensive than cold spots especially worth noting are the massive hot spots positive anomaly across north america eurasia africa and australia the second worth noting observations are the locations and extent where the solutions disagree on the direction positive or negative hot cold spots examples include north america and the large cold spot negative anomaly area in the middle east extending towards northern turkey pakistan afghanistan india and china third while the solutions may agree in terms of the locations and spatial extent of hot and cold spots significant differences still exist concerning the magnitude for example the sahelian region in east africa displays an average trend magnitude of 0 60 cm to 0 70 cm among different sh solutions compared to 1 cm between different mascons in the northeastern coast of the usa the sh of rl05 showed an increasing trend of 0 82 cm 0 76 cm and 0 84 cm for csr jpl and gfz respectively at the same region the sh of rl06 shows an increase of 0 65 cm from csr and jpl solution while the gfz depicts an increase of 0 56 cm the mascon solutions however exhibit significant discrepancies in the magnitude of 1 13 cm 1 17 cm 1 84 cm 1 31 cm and 1 67 cm as the csr m05 csr m06 jpl m05 jpl m06 non cri and jpl m06 cri respectively based on the rl05 fig 5a shows predominantly positive hotspot trends across africa central interior asia europe the southern half and the eastern usa as well as amazonian and central south america clusters of negative cold spots trends are to be found in the middle east the pacific northwest of the usa and the brazilian nordeste among others the improved spatial resolution of rl06 except for jpl m06 non cri relative to rl05 shows clearly the heterogeneity of tws clusters fig 5b now the general patterns are still maintained but have become more discretized in rl06 clusters of negative trends have begun to appear in australia and africa which no longer show up as entirely red positive hotspot similarly the trend patterns in south america and asia have become much more heterogeneous and granular the occurrence of these clusters at both extremes i e the positive hot and negative cold values underscores the additional insights about the twsa trends across various spatial scales additional research is required to better understand the influence of the spatial variability of tws and their clustering tendencies within the spatial basin domain herein we argue that much of the basin wide tws variability can be attributed to the occurrence of a cluster within the basin or spatial domain fig 6 exhibits the degree of agreement and discordance among solutions in terms of the frequency distribution cdf the sh of rl05 and rl06 display a small range of data values compared to the mascons from rl05 and rl06 the csr and jpl sh solutions of rl05 have similar density distributions fig 6a compared to the mascons solutions the mascons unveiled dissimilar pdf distribution for each solution with distinctive trend levels fig 6b fig 6c shows the ecdf of all 11 solutions superimposed on the same axis the ecdfs agree strongly in the middle range but a degree of divergence is notable toward the outliers or tails of the distributions the observation is consistent with the spatial analysis which shows predominantly strong concordance with relatively few areas of mismatch 3 3 mtc uncertainty analysis fig 7 shows the pixel wise mtc uncertainty outputs for the 11 grace solutions overall the pixelated mtc follows the same spatial clusters within each solution in general rl06 sh shows significantly smaller uncertainty values compared to the sh solution of the rl05 however herein we are reporting strong multicollinearity among sh of rl06 across the eastern antarctica region the mtc model therefore cannot determine the uncertainty levels across this part thus the region was marked with no value in the sh of rl06 outputs moreover the results indicated that the mascon solutions exhibit smaller uncertainty relative to most sh solutions furthermore the magnitude of uncertainties varied significantly from one region to the next for example the arid zone i e across northern african sahara showed generally low uncertainty with little to no significant trend clustering across the northwestern sahara additionally comparing the magnitude and range of uncertainty among the two releases rl05 and rl06 revealed varying degrees of uncertainty across the same region spatial domain for instance over the greenland ice mass region different jpl solutions showed varying degrees of uncertainty notably the range of uncertainty in jpl sh05 shows 1 19 cm yr compared to 2 27 cm yr for jpl sh06 cri the jpl m05 displays 1 74 cm yr jpl m06 non cri 3 01 cm yr while jpl m06 cri 1 72 cm yr in other areas e g the central and southern america the uncertainty in the tws trend is different between rl05 and rl06 i e jpl sh05 0 37 cm yr jpl sh06 0 15 cm yr jpl m05 0 23 cm yr jpl m06 non cri 0 40 cm yr jpl m06 cri 0 20 cm yr for additional details of such discrepancies see table s3 for the output summary of mtc over 80 transboundary river basins for all solutions finally fig 8 used the box and whiskler plots to summarize the characteristics of uncertainties for each solution in 80 major drainage basins in 6 main climate zones fig s4 summarizes the trend magnitude across the studied basins as well their relative variances overall across the studied basins the results show that no single grace solution consistently has the lowest uncertainty in all climate regions for example within the tropical and subtropical climates the sh solutions of rl06 from all three data centers produced similar uncertainty ranges the mascon solutions also generated comparable uncertainty values except for the jpl m06 non cri the range of uncertainties appears to be driven largely by the existence of clustering tendencies within individual basins across the different regions low uncertainty in the semiarid and arid zones as well as in the subarctic and highland regions may be attributed to significant randomness in the trend values with little clustering tendency this research accordingly identified various spatial characteristics among different grace solutions the spatial autocorrelation analysis revealed distinct patterns among various grace products within a 10 degree spatial radius the shs solutions were highly autocorrelated the mascons conversely reflected more localized mass anomalies with low autocorrelation coefficients as many solutions agreed on the overall directions and distribution of the hot and cold spots the analysis revealed unique patterns in terms of the magnitude and their spatial extent the deployed gwr approach modified the classical ols scaling scheme to improve the uncertainty estimation across different spatial domains treating the twsa over a basin or spatial domain as a lumped unit conceals important spatial attributes to determine the variability of the trend magnitude as well as the associated uncertainties instead the magnitude of the trend estimates is strongly linked to specific spatial clusters the pixelated uncertainty estimates followed the same cluster pattern noteworthy all of the presented results revealed that the twsa trends within a basin tend to agglomerate to specific spatial clusters at different locations herein we argue that with respect to other time series components at the grid scale i e seasonal and cyclical trends interannual variability their attributes will be driven by specific spatial clusters within the basin or study domain the reported results in this research are unique and highlight the importance of the spatial domain to analyze the grace twsa variability at various special scales in conclusion the validity of the current approach is strongly linked to the assumption of the error orthogonality among the tested datasets as mentioned earlier when three or more datasets are available the orthogonality hypothesis can be tested however when the tested datasets display strongly correlated errors the orthogonality cannot be established the errors that are common to all the datasets cannot be identified and or the corresponding errors can be misattributed to the corresponding dataset 4 summary for nearly two decades grace and grace fo satellites have provided new and unique insights that have considerably advanced research and decision making in nearly all areas related to hydro climatology and water resources despite such advancements assessing the spatial characteristics among different solutions and their associated uncertainties on space is not yet fully studied this paper contributes toward the resolution of the effect of the space domain on the twsa variability using a set of spatial diagnostic tools and the mtc approach we investigated 11 grace solutions from two versions releases rl05 and rl06 produced by three sds centers explicitly using three spatial analytical tools include spatial autocorrelation clustering analysis and the cdf analysis as well as the mtc approach the analysis revealed distinct autocorrelation patterns among different grace solutions significantly the twsa trend estimates were agglomerated as distinctive hot and cold spots clusters of pronounced positive and negative trends for the uncertainty assessment traditionally the tca approach relied on an ols scheme that assumed a sole autocorrelation coefficient throughout the analysis domain the mtc herein adopts a gaussian distance decay function via a gwr scaling scheme to assign varying weights to the autocorrelation coefficient in the study area for the agreement between rl05 and rl06 overall all solutions produce the same general and consistent trend directions regarding the trajectory of global water resources variability at the scale of the major drainage basins however the magnitude of the trends estimated by different grace solutions differs markedly by region and time at the basin scale shs of rl05 revealed that the twsa trend estimates provided by csr jpl and gfz displayed smaller trend values compared to the mascon solutions from jpl and csr sds centers using the mtc analysis the study showed that uncertainty estimates were attributed to significant spatial clusters among different solutions comparing different solutions revealed as expected the sh of rl06 shows lower uncertainty levels compared to the sh solution of the rl05 and the mascon solutions generally show lower uncertainty levels compared to most sh solutions at the scale of major drainage basins within different climate zones the estimated grace uncertainty appears to be driven by the occurrence of spatial clusters within the basin area summarizing the basin wide mtc results over different climate zones showed that in the tropical climate zone the mtc analysis showed that the sh solutions of rl05 have higher uncertainties compared to sh of rl06 in the subtropical regions the sh solution of rl06 and the mascons except jpl solutions had lower uncertainty within tws estimates the semiarid arid subarctic and highlands climate zones have relatively few significant clusters and correspondingly lower uncertainty ranges the mtc analysis clearly underscored the added benefits of using cluster analysis and the gwr modeling over the previous ols approach additional studies are recommended to test the implemented approach to some real world examples and to confirm the robustness of the clusters as well as the reasons for the trends and spatial patterns identified credit authorship contribution statement emad hasan conceptualization formal analysis investigation methodology validation visualization writing original draft writing review editing aondover tarhule funding acquisition resources supervision writing original draft writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors thank the provost s office at the state university of new york suny at binghamton for providing research funds and research facilities for this study thanks also go to the provost office at illinois state university normal il the usa for supporting this research the authors thank the journal of hydrology editor in chief dr emmanouil anagnostou associated editor dr jesus mateo lazaro and the reviewers dr clement guilloteau and two anonymous reviewers for their constructive and valuable comments in our manuscript we also wish to thank the three grace data centers the center for space research csr at the university of texas austin the jet propulsion laboratory jpl nasa california usa and the deutsches geoforschungszentrum gfz potsdam germany the grace tellus and the physical oceanography distributed active archive center po daac for the data access at https podaac jpl nasa gov grace appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j hydroa 2021 100108 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 supplementary data 2 
8567,grace gravity recovery and climate experiment and grace fo follow on satellites have provided unique insights into the evolution of terrestrial water storage tws in space and time despite such advancements various grace solutions produced by different data centers display uneven spatial attributes with varying associated uncertainties via spatial diagnostics tools and a modified triple collocation mtc approach this research evaluates the tws terrestrial water storage trend estimations on the grid scale from 11 gridded grace products of rl05 and rl06 releases between 2002 and 2017 distinct from classic tca triple collocation analysis the mtc employs a gwr geographically weighted regression scaling scheme with distinctive spatial coefficients the spatial diagnostics analyses identified different autocorrelation patterns clustering tendencies of hot positive and cold negative spots agglomeration at varying spatial width and unique frequency distributions the results indicated that within a 10 degree spatial radius the shs spherical harmonics of rl05 and rl06 are highly autocorrelated compared to the mascons mass concentration blocks solutions the spatial clustering results revealed that many solutions agreed on the overall directions and distribution of the hot and cold spots the clustering among mascon products however reflected more localized mass anomalies at the scale of drainage basins the trend magnitude as well as their associated uncertainties appeared to be driven by the occurrence of spatial clusters within the basin area the mtc results showed that the uncertainty patterns follow the same spatial extent within each cluster the mtc analysis underscored the added benefits of cluster analysis and the gwr scaling over the classic ols approach keywords grace grace fo triple collocation gwr ols climate spatial statistics autocorrelation cluster analysis 1 introduction grace gravity recovery and climate experiment and its successor grace fo follow on mission have significantly expanded the frontiers of satellite remote sensing applications in hydrology for nearly two decades grace has enabled the hydrological community to explore how terrestrial water storage tws evolves in space and time syed et al 2009 longuevergne et al 2013 rodell et al 2018 tapley et al 2019 analyze the hydrological water budget hassan and jin 2016 felfelani et al 2017 lv et al 2017 scanlon et al 2018 monitor water scarcity status hasan et al 2019a investigate groundwater exploitation flood and drought spell severity rodell et al 2006 strassberg et al 2007 strassberg et al 2009 voss et al 2013 long et al 2014 reager et al 2014 chen et al 2015 richey et al 2015 reager et al 2015 frappart and ramillien 2018 hasan et al 2021 study lake storage dynamics longuevergne et al 2013 moore and williams 2014 geoid height modeling glacial isostatic adjustments and glacial mass fluctuations paulson et al 2007 geruo et al 2013 sutterley et al 2014 these applications are made possible because of an expanding array of new data products as well as ongoing enhancements to existing processing analysis schemes produced by grace developers and science data system sds centers the most prominent of these centers include the center for space research csr at the university of texas at austin ut austin usa the jet propulsion laboratory jpl california usa and the deutsches geoforschungszentrum gfz potsdam germany to produce equivalent water heights ewh as relative changes in the water column in cm or terrestrial water storage anomalies twsa the sds centers have employed predominantly two processing schemes or solutions namely spherical harmonics shs are produced by csr jpl and gfz and mass concentration blocks mascons are produced by csr and jpl the shs offered standard ewh estimates for the first decade of grace observations shs were made available through different parameterizations and filtering strategies i e different degrees and order sh coefficients spatial filter north south destriping smoothing and scaling factors landerer and swenson 2012 long et al 2015 recently the mascons represented an alternative approach to derive comprehensive twsa estimations the mascons have the advantages of retaining an improved gravity signal using location information reducing residual noise and minimizing the spatial leakage error in most cases the mascons solutions could be used directly without applying any scaling factors or any post processing schemes watkins et al 2015 each of these solutions has undergone several iterations and refinements resulting in several distinctive releases rl rl04 rl05 and rl06 each updated release has provided an improved grace product over the prior version s as a result there are dozens of gridded twsa products from the three sds centers at the basin scale however different grace products display different temporal statistical properties such as trends variance or distributions these dissimilarities arise due to algorithmic structures and calibration assumptions that each sds center uses in converting grace signals gravity fields into ewhs consequently different gridded products exhibit distinctive spatial characteristics such as autocorrelation clustering tendencies and frequency distributions this research discusses the noted spatial characteristics in detail the gridded grace product was first introduced by landerer and swenson 2012 researchers however are frequently elected to not prioritize or assign greater weight to one grace twsa solution over another several studies have integrated various grace twsa time series from different solutions to obtain essentially an ensembled twsa estimate at a basin scale e g long et al 2017 and jing et al 2019 still others have relied on a single grace twsa product i e csr m center for space research mascon see for instance sultan et al 2019 and hasan and tarhule 2020 among others to derive the twsa at a given region or for specific hydrologic application in a similar context tourian et al 2018 utilized the grace jpl solution to assess the total drainable water over amazon river basin in addition ahmed and wiese 2019 have reported the rates of the changes in africa s water resources via investigating the short term trends from the tws estimates from csr jpl and gfz datasets with other ancillary observations yet examining the spatial properties among different gridded grace products along with the distribution of the associated uncertainties is not fully explored by the available literature existing studies have compared individual grace time series components at the basin scale i e cyclical trend scanlon et al 2018 and the seasonal fluctuations scanlon et al 2019 with independent lsms land surface models outputs this research therefore is proposing a set of spatial diagnostic tools to determine the spatial characteristics among different gridded grace twsa products specifically we tested the spatial autocorrelation the clustering tendency and the spatial distribution using the long term trend from 11 grace gridded solutions besides we applied a new modification to the classic triple collocation analysis tca approach to assess the relative uncertainties at the grid scale among various gridded grace products as noted earlier different grace twsa time series components were compared to other independent twsa estimates i e lsms based outputs also the associated uncertainties within the grace twsa series were assessed temporally at the basin scale following the standard approach as introduced by tiwari et al 2009 and scanlon et al 2016 specifically removing the deterministic components include the deterministic trend interannual variability and seasonal components from the twsa time series and then express the uncertainty in terms of the standard deviations of the residual component while in widespread use this approach overestimates the magnitude of the uncertainty because the residual nearly always contains some deterministic signals hasan et al 2019b alternatively another approach to assess the uncertainties within grace twsa estimates through comparing them to other independent or ancillary remote sensing data lsm observations and or hydrologic model outputs and the difference between the two estimates is used as a proxy for twsa uncertainty wahr et al 2006 this approach has been criticized on the grounds that the two data types are mismatched in nature further the approach unjustifiably sets up one data type as the standard truth or reference ferreira et al 2016 from a statistical perspective one approach that overcomes the limitations described above is the tca scipal et al 2010 vogelzang and stoffelen 2012 tca assesses the relative uncertainty of three or more products that estimate the same geophysical variable independent measurements using either the standard deviations or root mean square errors rmses stoffelen 1998 vogelzang and stoffelen 2012 tca has been used to provide observation error covariance information required by data assimilation and least squares merging approaches yilmaz and crow 2014 thus it is a stand alone rescaling methodology to remove systematic differences between the signal variance component of observations in data assimilation studies scipal et al 2010 alemohammad et al 2015 pan et al 2015 given that the same geophysical variable may be observed simulated using different methods or instruments tca estimates the relative uncertainties among the different methods by making certain assumptions about for example error orthogonality and cross correlation stoffelen 1998 vogelzang and stoffelen 2012 an attractive characteristic of the tca is that it scales each of the comparison variables to a target measurement without treating any as perfectly observed truth vogelzang and stoffelen 2012 dong and crow 2017 scaling is achieved using an ordinary least square ols model stoffelen 1998 see section 2 3 for additional details others have applied the tca by treating the residual between the truth and further measurements as a proxy to their relative uncertainty e g scipal et al 2010 several studies have successfully applied the tca error model to quantify the uncertainty of mutually yet equivalent measurements alemohammad et al 2015 pan et al 2015 the tca has been broadly used in error quantification in satellite based surface soil moisture products scipal et al 2010 yilmaz and crow 2014 dong and crow 2017 ocean salinity validation studies hoareau et al 2018 sea surface temperature and precipitation estimates among other hydrological constituents see for instance alemohammad et al 2015 tsamalis and saunders 2018 saha et al 2020 a major caveat to the tca approach is the assumption that the error is normally distributed and a single scaling coefficient is applied to the entire domain this paper thus applies several spatial diagnostic tools and a modified tca mtc approach to compare spatial characteristics and quantify the relative error uncertainties among 11 grace twsa solutions from three common sds centers of two widely used releases rl05 and rl06 specifically we used the pixel wise nonparametric mann kendall mk trend test to estimate the secular tws trends see section 2 1 for each grace solution at the grid level then three spatial diagnostics analyses were applied to determine the spatial autocorrelation clustering tendencies and spatial distribution among these estimates further we employed a gwr geographically weighted regression approach to modify the classic ols scaling scheme in the tca to estimate the relative uncertainties within these trend products at the grid scale explicitly the traditional tca employed an ols scaling approach that assumed a sole correlation coefficient for the entire spatial domain the proposed gwr modeling on the other hand applies a gaussian distance decay function to assign different weights with distinctive correlation coefficients at different spatial locations throughout the study domain in response to clustering tendencies and the autocorrelation levels within a specific spatial radius section 2 3 provides further explications of the approach and method finally we summarized the mtc uncertainty results spatially as well as over 80 transboundary river basins in 6 different climate zones see fig s1 for the basins locations this study contributes to the emerging literature to understand the evolution of twsa in space via different spatial characteristics on various gridded grace twsa products globally it also assesses the effect of the spatial domain in analyzing the associated uncertainties within various gridded grace products by analyzing the spatial patterns and statistical distribution of the trend magnitude it contributes to improving the understanding of tws variability within the intra basin scale finally the study highlights the effect of spatial clustering in the distribution of the tws variability as well as their associated uncertainty the analysis is based on the gridded outputs as well as the basin scale average over 80 major drainage basins 2 material and methods this study utilized 11 gridded twsa estimates from two widely employed grace releases the rl05 and the recent rl06 release using both shs and mascons solutions the data cover the period between april 2002 june 2017 and were processed by three grace sds centers csr jpl and gfz table 1 lists the types and spatial resolutions of each dataset the three centers distribute data independently and in different versions or releases for instance between 2002 and 2017 the csr revised updated and distributed five versions of their dataset rl01 rl02 rl04 and rl05 of level 2 data the csr skipped rl03 altogether in order to harmonize their releases with other sds centers in 2018 the csr distributed rl06 of their data the gfz also updated its estimates over five releases or iterations their most recent release rl06 was produced in 2018 like the csr jpl also skipped rl03 and their most recent update is labeled as rl06 as noted previously grace data are available via two processing schemes the traditional sh functions and the mascon solutions the unconstrained harmonics have typically suffered from poor east west gradients resulting in the so called n s stripes these strips are conventionally removed via empirical smoothing or destriping algorithms the disadvantage of that approach however is that it risks removing real geophysical signals along with the stripes the mascons on the other hand provide convenient mass change solutions with less leakage errors and did not require any empirical filters furthermore most mascons solutions are not involving any scaling factors to produce accurate mass estimates this study employed the shs and mascons solutions from rl05 and rl06 however the solutions were originally driven from the same level 1 instrument products level 3 gridded data are uniquely processed by the three sds following distinct analytical schemes background models and forcing parameters in this research we acquired the data in their gridded format as originally posted by the respective sds centers no further processing was done other than applying the provided scaling factors to the shs of rl05 and the jpl mascons of rl05 and rl06 the analytical steps employed in this research are as follows first we calculated the pixel wise long term tws trends using the nonparametric mk trend test mann 1945 hamed and ramachandra rao 1998 pohlert 2018 specifically we made use of the spatialeco package for spatial analysis and modeling utilities evans and ram 2018 in the mk test algorithm accessed via carn r project second to affix the pixel wise trend grids to a consistent spatial scale mk outputs were resampled to the same 1 degree grid resolution using a nearest neighborhood interpolation tool this step is important to later execute the spatial comparison analysis of on same spatial extent third we applied three diagnostic tests namely spatial autocorrelation clustering analysis and frequency distribution using a cumulative density function cdf to analyze the spatial patterns of trends at the pixel levels finally we assessed the relative uncertainty of each grace solution using the mtc approach as described in detail in section 2 3 fig 1 illustrates the schematic flow of the research methodology further description of the above analytic methods appears in sections 2 1 to 2 3 2 1 trend analysis the trend was estimated for the grace twsa gridded series at each grid point using a nonparametric mk trend test the mk statistic s for a series x 1 x 2 x n is calculated as 1 s k 1 n 1 i k 1 n s g n x i x k where 2 sgn x 1 x 2 1 x 1 x 2 0 0 x 1 x 2 0 1 x 1 x 2 0 where x j and x k are all pairs of sample points the mk method tests the presence and significance of a trend but not its magnitude therefore to determine the magnitude of the trend we applied sen s slope estimator as follows 3 b x j x k j k for 1 k j n where b is the slope n the number of data points and j k are indices then a median q was calculated for all slopes b to express the magnitude of the trend 2 2 spatial autocorrelation clustering and frequency distribution analysis the output pixelated median value q for each grace solution was tested for spatial autocorrelation among neighborhood pixels according to moran s i formula legendre and legendre 1998 chen 2013 moran s i is similar to pearson s correlation coefficient except that moran s i tests the correlation between neighborhood pixels within a specified spatial radius the index measures the clustering tendency or randomness within a field of neighboring pixels occurring within the selected range of a 10 degree euclidean radius distance this cutoff point 10 degree was selected because beyond that range the autocorrelation coefficients decay to statistically negligible levels within this radius d the autocorrelation coefficients can be expressed using a spatial autocorrelogram moran s i is computed from pairs of neighboring observations as follows 4 i d 1 w h 1 m i 1 m w hi y h y y i y 1 m i 1 m y h y 2 where y h and y i are the values of the observed variable at locations h and i h i w is the sum of the spatial contiguity weights of w hi for the given distance class the value of w hi is 1 when h and i are at distance d otherwise w hi 0 for a given distance class the weights w are written in an m m matrix then a distance 1 or distance 2 among adjacent sites is used for weight instead of 1 s the weight matrix is defined according to a queen spatial contiguity matrix legendre and legendre 1998 based on whether spatial units share a boundary basically eq 4 numerator is a covariance comparing the values found at all pairs of points while the denominator is the maximum likelihood estimator of the variance i e the division by m moran s i usually takes values in the interval 1 to 1 the negative positive values correspond to areas of declining increasing twsa trends at the specified confidence level thus the test also allows identifying the clustering tendency using a 2 d map of the significant clusters in terms of high high hot spots and low low cold spots finally we used a frequency distribution analysis pdfs probability distribution functions to compare and evaluate the density distribution of trends among the different solutions and releases the comparison was done based on the distribution parameters such as variance skewness and kurtosis the goodness of fit was evaluated using the kolmogorov smirnov empirical distribution function ecdf analysis drew et al 2000 2 3 modified triple collocation error analysis as explained earlier the tca approach requires a minimum of three different series of measurements that are collocated correspondingly in space and time classic tca scales the variables under analysis to one of the observations or variables considered as the truth stoffelen 1998 scipal et al 2010 vogelzang and stoffelen 2012 because the observations are collocated in space and congruent in time tca commonly assumes a degree of autocorrelation exists among them further it is assumed that the distribution of the error term is normal following rescaling in classic formulation the scaling process makes use of an ols scheme in which the same correlation coefficient is applied to the entire study domain effectively therefore the study area is treated as a single undifferentiated unit the procedure is as follows given three sets of measurements x y and z each can be scaled to a true measurement t as 5 x α x β x t ε x y α y β y t ε y z α z β z t ε z where α is the intercept β is the slope and ε is the error term in this classic tca eq 5 is applied in an unweighted form and the relationship is assumed to be the same throughout the study domain besides this approach leads to an underestimation of the errors several refinements to the classic approach have been proposed including the extended triple collocation etc mccoll et al 2014 mccoll et al 2016 saha et al 2020 in which the error term is derived by using the correlation coefficient among each measurement variable to the unknown target the approach used in this paper is the mtc which differs from classic tca that in addition to scaling the study variables it also applies a gaussian distance decay function through a gwr approach distinct from ols scaling the gwr formulation requires a degree of independence among the tested datasets it also assigns spatial weights to each variable based on the occurrence of clusters within the study domain second mtc treats each spatial domain distinctively by producing different spatial coefficients instead of a lumped coefficient used by the general ols scaling schemes the procedure is as follows given a series of spatially distributed measurements x y z herein each measurement represents the median q of the sen s slope trend outputs from eq 3 each derived at the same location p i where i 1 2 in an m m space matrix the following regression equations can be developed among the variables for each location as 6 x p i α x p i β x p i t p i ε x p i y p i α y p i β y p i t p i ε y p i z p i α z p i β z p i t p i ε z p i because the weights in eq 6 are derived for each spatial location p i the weight or spatial coefficient varies across the study domain in the gwr model a radius r of 10 degree euclidian distance centered on each p i is used to derive the set of weights ω ij with values between 0 and 1 the weights were calibrated according to a gaussian distance decay function as follows 7 ω ij 1 d ij c 2 2 if d ij r 0 if d ij r where d ij is the euclidean distance between the location of observation i and j the c is a nonnegative decay parameter describing the relationship between weight and distance the c has an adaptive bandwidth size defined as the nearest neighbor distance among pixels the spatial weights at each location p i are proportionate to the strength of the autocorrelation coefficients within different clusters and between each cluster and spatial distance see charlton and fotheringham 2009 nakaya 2012 nakaya 2015 nakaya 2016 then to scale each variable to a common truth we divided eq 6 by the slope according to fang et al 2012 so that x p i x p i β x p i α x p i β x p i and ε x p i ε x p i β x p i etc then 8 x p i t p i ε x p i y p i t p i ε y p i z p i t p i ε z p i herein we scaled all trend measurements relative to the csr m06 solution then to eliminate this scaling effect we first used a pair wise subtraction as 9 x p i y p i ε x p i ε y p i ε x y p i x p i z p i ε x p i ε z p i ε x z p i y p i z p i ε y p i ε z p i ε y z p i eq 9 eliminates the scaling effect then to eliminate the conjugate error estimates we used a pairwise multiplication the individual unconjugated variance for each measurement which represents the residual errors can be estimated as 10 σ x p i 2 x p i y p i x p i z p i σ y p i 2 x p i y p i y p i z p i σ z p i 2 x p i z p i y p i z p i for eqs 9 and 10 there is no specific combination required among measurements instead any combination of three independent measurements can be used in each run herein eq 10 is used to derive the grid wise variance for each grace solution following standard practice the uncertainty errors at the pixel levels were subsequently expressed as the standard deviation of the error variance or the uncertainty as stated above the tca mtc assesses the relative error among three independent datasets under certain assumptions of the error orthogonality i e the product errors are independent among the tested datasets the error orthogonality can be tested if three or more datasets are available for cross comparison in the case of grace products originally grace solutions are derived from the same level 1 instrument however level 3 gridded products were produced independently by three main grace sds centers each data center is following a unique analytical scheme as well as utilizing different background models to produce their gravity field solutions technically speaking each data center csr gfz and jpl produces different flavors of data products that updated frequently with new releases furthermore at the basin scale see fig s2 in the supplementary info the total tws signals from 11 products displayed a sort of general similarities in the overall pattern however the signal amplitude as well as the secular trends are distinct these would result in significant gigatons year differences in the total storage changes among various products thus this research explicitly quantifies the associated uncertainties to the processed data level 3 gridded products any potential uncertainties linked to the instrumental errors or the retrieval process itself are ignored 3 results and discussion 3 1 grace trends fig 2 shows the gridded based results of the mk trend analysis of the twsa for rl05 and rl06 produced by the three processing sds centers the results show that significant decreasing tws trends occur over the greenland ice mass alaska north canadian archipelago antarctica four main ice covered regions west and the southwest usa the brazilian nordeste central argentina the patagonian ice fields the northern saharan region north central the middle east including portions of the tigris and euphrates river basins and western iran central euro asia the himalayan mountains north china and western australia these declining patterns are consistent with those reported in numerous studies and have been attributed to different dynamics including frequent droughts e g rodell et al 2018 groundwater exploitation chen et al 2015 and severe groundwater depletion due to agricultural activity voss et al 2013 conversely pronounced areas of increasing twsa trends appear in the northeastern usa much of the amazon sahel region in western africa great lake area in east africa okavango river delta and north zambezi river basin in africa central india and eastern australia several studies outlined similar figures i e ramillien et al 2014 panda and wahr 2016 werth et al 2017 shamsudduha et al 2017 tourian et al 2018 and rodell et al 2018 while the mk analysis provides a single aggregated trend value during the reference period the twsa behavior over time can be complicated and attributable to various factors in different parts of the world for example in california usa severe and prolonged droughts that occurred between 2006 and 2010 and 2011 2019 contributed to unprecedented demands for groundwater liu et al 2019 the twsa time series ending in this time frame and for the affected basins therefore would most likely be negative similar trends have been reported in texas usa long et al 2013 as regions recover from such historic droughts they may show pronounced upward trends e g the african sahel region elsewhere increasing trends were attributed to a pronounced increase in rainfall amount e g amazonian region tourian et al 2018 it is worth noting that several of the observed tws trends are consistent with climate and surface model predictions rodell et al 2018 for some locations however the reasons behind observed trends are poorly understood or unknown this includes for example the increased tws trends over the okavango delta for additional explanations of the emerging trends in grace solutions see ahmed et al 2014 anyah et al 2018 and rodell et al 2018 while the twsa estimates produced by the various sds centers show strong agreement on the overall trend at a global scale they displayed distinct magnitude as well as other notable differences at the basin level fig 3 for instance shows the basin scale summary of the trend magnitude from 11 gridded trend outputs for 12 major transboundary river basin systems see fig s2 in the supplementary info for the total twsa time series signals across these basins herein fig 3 shows clear examples of the discrepancies among different grace solutions in terms of the trend magnitude and the direction the sign of the trend within the same basin for instance the nile river basin illustrates the magnitude of differences among various solutions to illustrate comparable figures across basin scales we expressed these changes in the storage using gt yr units based on shs of rl05 the tws trend estimates provided by csr jpl and gfz are respectively at 3 15 gt yr 1 28 gt yr and 4 27 gt yr in contrast the mascon solutions of csr m05 csr m06 jpl m05 jpl m06 non cri and jpl m06 cri estimate the trends at 4 18 gt yr 7 87 gt yr 15 02 gt yr 16 27 gt yr and 16 34 gt yr respectively in some basins the solutions do not agree even on the sign direction of the trend this is the case in the colorado basin in the central and western usa contrarily in the mekong okavango and indus basins different solutions are in strong agreement with only a small range of variation in the trend magnitude finally in other basins like the nile mackenzie and the mississippi the solutions agree on the sign of the trend but the differences in the trend magnitudes are substantial additional results about the tws trends magnitude and characteristics are provided in the supplementary information the tables in the xls sheet s1 s2 and s3 the above differences have important implications for water resources planning policy and management as well as the conclusions of research findings the explanation for some of these differences lies in the calibration assumptions that each sds center made regarding the conversion of grace gravity fields into ewh estimates additional research is required to better understand the effects of pixel heterogeneity on the basin scale of the tws trend analysis see tables s1 and s2 for an additional summary about the trend magnitude and other statistical parameters from 11 grace solutions over 80 transboundary river basin systems 3 2 spatial autocorrelation clustering and frequency distribution analysis fig 4 shows the spatial autocorrelation and the level of clustering tendencies among each unique grace solution within a 10 degree spatial radius euclidean distance overall six sh models of rl05 and rl06 displayed strong autocorrelation coefficients with little variation specifically the neighborhood pixels of all shs have autocorrelation coefficients that ranged between 90 percent within a 3 degree radius to 60 percent within a 5 to 7 degree radius on the other hand some of the mascon solutions e g jpl m05 and m06 cri appear significantly different from the sh solutions the csr mascons of rl05 and rl06 as well as the jpl rl06 non cri solutions however covary with sh solutions with the major difference being a lower coefficient value at a 1 degree distance i e 60 percent versus 90 percent among these solutions the spatial autocorrelation among mascons solutions reflects more localized mass anomalies within each cluster compared to the sh solutions the analysis of morans i delineated clusters of positive and negative twsa trends globally fig 5 plots the clusters statistically significant at α 0 05 which we have denoted as hot positive and cold negative spots fig s3 illustrates the difference between the significant cluster domains versus the conventional p value approach three observations can be made with respect to these plots first the significant level of agreement among all the solutions relative to the location frequency and spatial extent of hot and cold spots for nearly all solutions the hot spots are more numerous and spatially extensive than cold spots especially worth noting are the massive hot spots positive anomaly across north america eurasia africa and australia the second worth noting observations are the locations and extent where the solutions disagree on the direction positive or negative hot cold spots examples include north america and the large cold spot negative anomaly area in the middle east extending towards northern turkey pakistan afghanistan india and china third while the solutions may agree in terms of the locations and spatial extent of hot and cold spots significant differences still exist concerning the magnitude for example the sahelian region in east africa displays an average trend magnitude of 0 60 cm to 0 70 cm among different sh solutions compared to 1 cm between different mascons in the northeastern coast of the usa the sh of rl05 showed an increasing trend of 0 82 cm 0 76 cm and 0 84 cm for csr jpl and gfz respectively at the same region the sh of rl06 shows an increase of 0 65 cm from csr and jpl solution while the gfz depicts an increase of 0 56 cm the mascon solutions however exhibit significant discrepancies in the magnitude of 1 13 cm 1 17 cm 1 84 cm 1 31 cm and 1 67 cm as the csr m05 csr m06 jpl m05 jpl m06 non cri and jpl m06 cri respectively based on the rl05 fig 5a shows predominantly positive hotspot trends across africa central interior asia europe the southern half and the eastern usa as well as amazonian and central south america clusters of negative cold spots trends are to be found in the middle east the pacific northwest of the usa and the brazilian nordeste among others the improved spatial resolution of rl06 except for jpl m06 non cri relative to rl05 shows clearly the heterogeneity of tws clusters fig 5b now the general patterns are still maintained but have become more discretized in rl06 clusters of negative trends have begun to appear in australia and africa which no longer show up as entirely red positive hotspot similarly the trend patterns in south america and asia have become much more heterogeneous and granular the occurrence of these clusters at both extremes i e the positive hot and negative cold values underscores the additional insights about the twsa trends across various spatial scales additional research is required to better understand the influence of the spatial variability of tws and their clustering tendencies within the spatial basin domain herein we argue that much of the basin wide tws variability can be attributed to the occurrence of a cluster within the basin or spatial domain fig 6 exhibits the degree of agreement and discordance among solutions in terms of the frequency distribution cdf the sh of rl05 and rl06 display a small range of data values compared to the mascons from rl05 and rl06 the csr and jpl sh solutions of rl05 have similar density distributions fig 6a compared to the mascons solutions the mascons unveiled dissimilar pdf distribution for each solution with distinctive trend levels fig 6b fig 6c shows the ecdf of all 11 solutions superimposed on the same axis the ecdfs agree strongly in the middle range but a degree of divergence is notable toward the outliers or tails of the distributions the observation is consistent with the spatial analysis which shows predominantly strong concordance with relatively few areas of mismatch 3 3 mtc uncertainty analysis fig 7 shows the pixel wise mtc uncertainty outputs for the 11 grace solutions overall the pixelated mtc follows the same spatial clusters within each solution in general rl06 sh shows significantly smaller uncertainty values compared to the sh solution of the rl05 however herein we are reporting strong multicollinearity among sh of rl06 across the eastern antarctica region the mtc model therefore cannot determine the uncertainty levels across this part thus the region was marked with no value in the sh of rl06 outputs moreover the results indicated that the mascon solutions exhibit smaller uncertainty relative to most sh solutions furthermore the magnitude of uncertainties varied significantly from one region to the next for example the arid zone i e across northern african sahara showed generally low uncertainty with little to no significant trend clustering across the northwestern sahara additionally comparing the magnitude and range of uncertainty among the two releases rl05 and rl06 revealed varying degrees of uncertainty across the same region spatial domain for instance over the greenland ice mass region different jpl solutions showed varying degrees of uncertainty notably the range of uncertainty in jpl sh05 shows 1 19 cm yr compared to 2 27 cm yr for jpl sh06 cri the jpl m05 displays 1 74 cm yr jpl m06 non cri 3 01 cm yr while jpl m06 cri 1 72 cm yr in other areas e g the central and southern america the uncertainty in the tws trend is different between rl05 and rl06 i e jpl sh05 0 37 cm yr jpl sh06 0 15 cm yr jpl m05 0 23 cm yr jpl m06 non cri 0 40 cm yr jpl m06 cri 0 20 cm yr for additional details of such discrepancies see table s3 for the output summary of mtc over 80 transboundary river basins for all solutions finally fig 8 used the box and whiskler plots to summarize the characteristics of uncertainties for each solution in 80 major drainage basins in 6 main climate zones fig s4 summarizes the trend magnitude across the studied basins as well their relative variances overall across the studied basins the results show that no single grace solution consistently has the lowest uncertainty in all climate regions for example within the tropical and subtropical climates the sh solutions of rl06 from all three data centers produced similar uncertainty ranges the mascon solutions also generated comparable uncertainty values except for the jpl m06 non cri the range of uncertainties appears to be driven largely by the existence of clustering tendencies within individual basins across the different regions low uncertainty in the semiarid and arid zones as well as in the subarctic and highland regions may be attributed to significant randomness in the trend values with little clustering tendency this research accordingly identified various spatial characteristics among different grace solutions the spatial autocorrelation analysis revealed distinct patterns among various grace products within a 10 degree spatial radius the shs solutions were highly autocorrelated the mascons conversely reflected more localized mass anomalies with low autocorrelation coefficients as many solutions agreed on the overall directions and distribution of the hot and cold spots the analysis revealed unique patterns in terms of the magnitude and their spatial extent the deployed gwr approach modified the classical ols scaling scheme to improve the uncertainty estimation across different spatial domains treating the twsa over a basin or spatial domain as a lumped unit conceals important spatial attributes to determine the variability of the trend magnitude as well as the associated uncertainties instead the magnitude of the trend estimates is strongly linked to specific spatial clusters the pixelated uncertainty estimates followed the same cluster pattern noteworthy all of the presented results revealed that the twsa trends within a basin tend to agglomerate to specific spatial clusters at different locations herein we argue that with respect to other time series components at the grid scale i e seasonal and cyclical trends interannual variability their attributes will be driven by specific spatial clusters within the basin or study domain the reported results in this research are unique and highlight the importance of the spatial domain to analyze the grace twsa variability at various special scales in conclusion the validity of the current approach is strongly linked to the assumption of the error orthogonality among the tested datasets as mentioned earlier when three or more datasets are available the orthogonality hypothesis can be tested however when the tested datasets display strongly correlated errors the orthogonality cannot be established the errors that are common to all the datasets cannot be identified and or the corresponding errors can be misattributed to the corresponding dataset 4 summary for nearly two decades grace and grace fo satellites have provided new and unique insights that have considerably advanced research and decision making in nearly all areas related to hydro climatology and water resources despite such advancements assessing the spatial characteristics among different solutions and their associated uncertainties on space is not yet fully studied this paper contributes toward the resolution of the effect of the space domain on the twsa variability using a set of spatial diagnostic tools and the mtc approach we investigated 11 grace solutions from two versions releases rl05 and rl06 produced by three sds centers explicitly using three spatial analytical tools include spatial autocorrelation clustering analysis and the cdf analysis as well as the mtc approach the analysis revealed distinct autocorrelation patterns among different grace solutions significantly the twsa trend estimates were agglomerated as distinctive hot and cold spots clusters of pronounced positive and negative trends for the uncertainty assessment traditionally the tca approach relied on an ols scheme that assumed a sole autocorrelation coefficient throughout the analysis domain the mtc herein adopts a gaussian distance decay function via a gwr scaling scheme to assign varying weights to the autocorrelation coefficient in the study area for the agreement between rl05 and rl06 overall all solutions produce the same general and consistent trend directions regarding the trajectory of global water resources variability at the scale of the major drainage basins however the magnitude of the trends estimated by different grace solutions differs markedly by region and time at the basin scale shs of rl05 revealed that the twsa trend estimates provided by csr jpl and gfz displayed smaller trend values compared to the mascon solutions from jpl and csr sds centers using the mtc analysis the study showed that uncertainty estimates were attributed to significant spatial clusters among different solutions comparing different solutions revealed as expected the sh of rl06 shows lower uncertainty levels compared to the sh solution of the rl05 and the mascon solutions generally show lower uncertainty levels compared to most sh solutions at the scale of major drainage basins within different climate zones the estimated grace uncertainty appears to be driven by the occurrence of spatial clusters within the basin area summarizing the basin wide mtc results over different climate zones showed that in the tropical climate zone the mtc analysis showed that the sh solutions of rl05 have higher uncertainties compared to sh of rl06 in the subtropical regions the sh solution of rl06 and the mascons except jpl solutions had lower uncertainty within tws estimates the semiarid arid subarctic and highlands climate zones have relatively few significant clusters and correspondingly lower uncertainty ranges the mtc analysis clearly underscored the added benefits of using cluster analysis and the gwr modeling over the previous ols approach additional studies are recommended to test the implemented approach to some real world examples and to confirm the robustness of the clusters as well as the reasons for the trends and spatial patterns identified credit authorship contribution statement emad hasan conceptualization formal analysis investigation methodology validation visualization writing original draft writing review editing aondover tarhule funding acquisition resources supervision writing original draft writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors thank the provost s office at the state university of new york suny at binghamton for providing research funds and research facilities for this study thanks also go to the provost office at illinois state university normal il the usa for supporting this research the authors thank the journal of hydrology editor in chief dr emmanouil anagnostou associated editor dr jesus mateo lazaro and the reviewers dr clement guilloteau and two anonymous reviewers for their constructive and valuable comments in our manuscript we also wish to thank the three grace data centers the center for space research csr at the university of texas austin the jet propulsion laboratory jpl nasa california usa and the deutsches geoforschungszentrum gfz potsdam germany the grace tellus and the physical oceanography distributed active archive center po daac for the data access at https podaac jpl nasa gov grace appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j hydroa 2021 100108 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 supplementary data 2 
8568,in this paper we propose a set of simple benchmarks for the evaluation of data based models for real time streamflow forecasting such as those developed with sophisticated artificial intelligence ai algorithms the benchmarks are also data based and provide context to judge incremental improvements in the performance metrics from the more complicated approaches the benchmarks include temporal and spatial persistence persistence corrected for baseflow and streamflow as well as river distance weighted runoff obtained from space time distributed rainfall in the development of the benchmarks we use basic hydrologic insights such as flow aggregation by the river network scale dependence in basin response streamflow partitioning into quick flow and baseflow water travel time and rainfall averaging by the basin width function the study uses 140 streamflow gauges in iowa that cover a range of basin scales between 7 and 37 000 km2 the data cover 17 years this work demonstrates that the proposed benchmarks can provide good performance according to several commonly used metrics for example streamflow forecasting at half of the test locations across years achieves a kling gupta efficiency kge score of 0 6 or higher at one day ahead lead time and 20 of cases reach the kge of 0 8 or higher the proposed benchmarks are easy to implement and should prove useful for developers of data based as well as physics based hydrologic models and real time data assimilation techniques keywords real time streamflow forecasting artificial intelligence hydrologic insights benchmarking persistence iowa 1 introduction as applications of data driven artificial intelligence ai methods such as neural networks adaptive neural based fuzzy inference systems genetic programming support vector machine multi layered perceptron and deep learning algorithms find their way into hydrology we need to clearly define the scope of the uncharted territory where these methods have the potential to improve on existing approaches in this paper we focus on real time streamflow forecasting the goal is to establish benchmarks for the ai methods to beat across a range of spatial scales and forecast lead times the proposed benchmarks are data based simple to explain and easy to implement real time forecasting methods so that ai researchers can calculate them for their region and data period of interest for real time streamflow forecasting the benchmarks cover a range of spatial and temporal scales and relevant forecast lead time horizons ranging from hours to several days clearly the same benchmarks can be used for other non ai data based forecasting models providing additional context regarding model performance our motivation can be illustrated with a simple example assume a performance metric with the dynamic range from 0 to 1 with 1 being the best if the benchmark that is achievable with a simple method for a given scale region and lead time can result in the metric value of 0 8 this implies that the margin for improvement for data intensive methods is only 0 2 the opposite example is obvious data intensive methods at locations where the benchmark metric has a value of only 0 2 offer much more room for improvement simply reporting the metric devoid of the context provided by the benchmark can be misleading about the contributions offered by the data intensive methods note here that benchmarks are methods not metrics we illustrate the benchmarking framework using data from the well instrumented state of iowa as we imply in the title the benchmarks in addition to being data based take advantage of hydrologic insights something the ai has long promised we discuss these insights as we explore the methods but first we briefly review the existing approaches to real time streamflow forecasting focusing on the current practice in the united states a puzzling contrast exists between the operational methods and the approaches reported in the research literature a plethora of rainfall runoff models are discussed in the literature only a handful of methods are used in operational streamflow forecasting for instance most river forecast centers in the u s national weather service nws still use lumped models e g the sacramento soil moisture accounting model sacsma to estimate runoff at monitoring sites mcintire et al 1998 stellman et al 2021 reading the literature one gets the impression that the problem is easy and that many models upon calibration can handle the task and provide skillful forecasts this is in stark contrast to operational methods that still heavily rely on time proven lumped and empirical models developed decades ago and more importantly on the experience and regional knowledge of the human forecasters it is also shockingly obvious how little has been documented in the literature on the operational performance in real time streamflow forecasting the studies by welles et al 2007 welles and sorooshian 2009 and zalenski et al 2017 are rare exceptions a gap exists in communicating real time forecast skills relative to benchmarking forecasts and their dependence on various characteristics of the monitored basins there is also a gap in benchmark datasets for streamflow forecasting that can serve as a summary of a problem area providing a simple interface between disciplines without requiring extensive background knowledge to engage ai researchers in hydrological studies this situation provides the backdrop for the entry of the ai methods in particular the recent ones such as deep learning the literature reports some amazing results for different variants of the new kid on the block for instance xiang and demir 2020 xiang et al 2020 fang et al 2021 kratzert et al 2018 cheng et al 2020 and zuo et al 2020 employed deep learning techniques such as long short term memory lstm lee and ahn 2021 used stacking ensemble model chu et al 2021 used deep belief networks and lima et al 2016 employed online sequential extreme learning to demonstrate marked improvement over process based methods for most instances several studies e g nourani et al 2014 wang et al 2009 firat 2008 sit et al 2020 humphrey et al 2008 ardabili et al 2020 mosavi et al 2018 zounemat kermani et al 2021 provide a comprehensive overview of recent articles published in the field on the applications of the ai methods this paper abstains from casting judgment on any particular contribution focusing instead on benchmarking the benchmarks should be simple to calculate based on the widely available data both static and dynamic simplicity within the context of model complexity and functional utility is the key requirement of the framework we propose static data can include digital elevation models dem and the derivative topology of the drainage network as well as the land use and soil types which are widely available dynamic data comprise mainly of streamflow and rainfall and could easily also include weather data such as air temperature humidity wind and solar radiation the data should be complemented by hydrologic insights as mechanisms to extract knowledge from the data for example palash et al 2018 used a temporal persistence framework integrating rainfall at larger basins while leedal et al 2013 shahzad and plate 2014 smith et al 2014 and young 2002 demonstrated the utility of data based mechanistic models using transfer functions these studies make up the few notable contributions that use data based mechanistic models in real time streamflow forecasting in this paper we do not apply any ai algorithms leaving this to others in the hope that they will see the value of the benchmarks we propose we simply want to initiate a conversation between the ai experts and the hydrologic research community in the next section we describe the data we use in this study which is merely an illustration of the need for well defined benchmarks after discussing the data we describe four simple methods of data based streamflow forecasting that could serve as benchmarks along with those we examine the relevant hydrologic insights at the core of the methods performance we conclude with the discussion 2 data our benchmarking framework demonstration is based in the state of iowa in the upper midwest of the united states fig 1 iowa has a surface area of about 150 000 km2 and is bordered by two major rivers the mississippi river on the east and the missouri river on the west most interior river basins are contained within the state s borders and drain to those two large rivers the total water domain expands 160 000 km2 and includes parts of minnesota there are some 150 stream gauges operated by the united states geological survey usgs in addition the iowa flood center ifc krajewski et al 2017 at the university of iowa has added nearly 300 gauges bridge mounted ultrasonic sensors that measure river stage kruger et al 2016 the sensor network data is accessible from the iowa flood information system demir and krajewski 2013 the ifc in partnership with other agencies has developed synthetic rating curves for the bridge sensor locations quintero et al 2020a b we used streamflow observations from usgs monitored sites in iowa available in near real time the usgs uses 15 minute resolution river stage data to convert to streamflow using well maintained rating curves for our study we used data from 2002 to 2018 as the usgs started to make 15 minutes resolution data publicly available in 2002 we focus on nine major river basins in iowa fig 1 and table 1 rainfall data come from the national network of weather radars we used the nationwide product known as stage iv the product offers the longest available radar rainfall record at the national center for environmental prediction ncep from the regional hourly multi sensor radar and gauges precipitation analyses produced by the 13 river forecast centers ncep 2020 the high resolution 4 km by 1 hour of stage iv rainfall makes it flexible to calculate areal quantities for basins of various sizes and shapes it is attractive for mapping into high resolution units of the rainfall runoff models e g krajewski et al 2017 noaa 2020 while we acknowledge the existence of a higher resolution radar based rainfall data set known as multi radar multi sensor zhang et al 2016 a real time rain gauge corrected hourly product provided on a 1 km2 grid we consider that the stage iv product with its long record is more appropriate for our purposes 3 methods we propose four simple benchmarks as references for the evaluation of model based streamflow forecasts the models could be statistical conceptual physics based or ai based all these model categories have been the subject of numerous discussions in the literature and adding to this discussion is outside of the scope of this paper however we would like to point out that the overarching goal of real time streamflow forecasting is to provide accurate estimates of peak flow value and peak flow timing at all relevant locations in the river network irrespective of whether this is feasible with a given model in fig s1 supplementary information we present the overall methodology adopted for this study 3 1 proposed benchmarks the first and perhaps simplest benchmark for an assessment of forecasting models is persistence tomorrow will be the same as today ghimire and krajewski 2020 and krajewski et al 2020 have comprehensively discussed this approach across temporal and spatial scales while temporal persistence applies to the location of an observer spatial persistence krajewski et al 2020 is of broader relevance to our goal of proposing general benchmarks we illustrate the general problem with fig 2 at point a streamflow observations integrate the basin runoff response upstream and can be used to make a spatial persistence forecast at point b however there is often a substantial area that is not monitored thus its contribution to streamflow at b is unknown this brings us to the introduction of a second benchmark spatial persistence corrected for baseflow because streamflow often includes a significant contribution from baseflow the spatial persistence based forecast can be corrected thus reducing bias in the predictions the baseflow is known to be proportional to the drainage area e g ahiablame et al 2013 furey and gupta 2000 gebert et al 2007 singh et al 2019 zhu and day 2009 the coefficients of this relationship between the baseflow and drainage area can be estimated statically based on past observations or dynamically considering the more recent past we elaborate on this distinction in the results section the third benchmark is based on another correction that can be applied to the persistence method we refer to it as streamflow correction the concept behind it is that observed streamflow integrates all rainfall and runoff processes in the upstream part of the basin therefore the larger the monitored portion of the basin of interest is the more likely it is that a similar streamflow regime exists in the unmonitored part of the basin also the larger the basin the smaller the role played by the recent or future storms at least relatively we explain the details and compare the two corrective schemes in the results section the fourth and final correction uses observed rainfall we assume here that the available rainfall is distributed in space and time such data come from weather radar networks a commonplace instrument in many parts of the world while it seems that including rainfall data should lead to a vast improvement in the benchmarks it turns out that incorporating rainfall information is not straightforward because the basin response to rainfall is characterized by a delay therefore one needs a transfer function to estimate the resultant streamflow identification and estimation of such a function can be accomplished via calibration we consider any calibrated model as ineligible for the benchmark construction simplicity is the name of the game here in the next section we analyze a couple of ways to include radar rainfall data a relatively simple way of accounting for the rainfall effect is by using overland runoff translation the key question is with what velocity 3 2 forecast evaluation metrics evaluation of streamflow forecasts is the key to getting a sense of the performance of a real time streamflow forecasting system we used several standard continuous verification measures to evaluate forecasts against the usgs streamflow observations we calculate and report all metrics for a range of forecast lead times in the metric definitions below we skip this dependence on the lead time the metrics we consider here are kling gupta efficiency kge mean absolute error mae normalized mae nmae percent bias pbias and percentage peak difference pd the kge metric comprises three components pearson s correlation r variance ratio α and mean ratio β see gupta et al 2009 the ideal value of kge is equal to 1 1 k g e 1 r 1 2 α 1 2 β 1 2 where α σ f σ o and β μ f μ o σf is the standard deviation of forecasts σo is the standard deviation of observations μf is the mean of forecasts and μo is the mean of observations knoben et al 2019 pool et al 2018 and camici et al 2020 provide a detailed interpretation of the kge for hydrologic model evaluation mae is the measure of errors between the forecasts and observations and computed as 2 m a e 1 n i 1 n q f q o where qf and q o are forecasted and observed streamflow respectively and n is the length of the time series we normalize mae by the upstream drainage area to compute nmae pbias measures the average tendency of the forecasts to be larger or smaller than their observed counterparts and is given by 3 p b i a s 100 i 1 n q f q o i 1 n q o the optimal value range of pbias is 0 a positive value indicates overestimation while a negative value indicates underestimation finally we compute relative peak difference pd as 4 p d peak f peak o peak o 100 where peakf and peako are the values of peaks of forecasts and observations respectively 4 results 4 1 benchmark 1 simple persistence using persistence as a reference has been a commonplace way to assess the skill of weather forecasts for the difficult to forecast variables such as rainfall persistence is hard to beat in the context of streamflow forecasting ghimire and krajewski 2020 and krajewski et al 2020 found that forecast skill shows a strong dependence on basin scales and forecast lead times for instance basins 1000 km2 show a one day ahead forecast skill of median kge 0 9 for spatial prediction the dominant factor leading to good performance is not surprisingly the monitoring of a large portion of the basin of interest for example if 80 of the upstream area is monitored gauged one can expect a kge on the order of 0 8 a highly respectable result this is basin scale invariant therefore what matters is the fraction of the basin that is gauged these results are less astounding if one considers that the dynamical response is an aggregation and attenuation of rather slow processes of water movement on the land surface and in the channel network mantilla et al 2006 in time it takes a strong influx of rainfall water i e a significant fraction of the flow already in the channel to create a noticeable change in the streamflow regime most of the time there is no rain or the rain is insignificant and thus the forecasting score is high in space stream and river drainage networks aggregate the flow it takes a major tributary of a similar stream order to significantly change or increase the discharge ayalew and krajewski 2017 ghimire et al 2018 in fig 3 we show an example of the performance of the spatial persistence forecasting spf our proposed benchmark 1 in constructing the figure we used 17 years of data from 140 usgs stream gauges we performed spatial persistence prediction in the downstream directions only see krajewski et al 2020 therefore the evaluation is possible only for gauging stations that have at least one upstream station there are 69 such stations but the number of all possible flow connected pairs of forecast vs evaluation gauge varies from basin to basin and depends on the river network topology see table 1 the total number of such combinations is 110 in our study in fig 3 left panel each of the 1870 dots represents a single evaluation gauge year station year for the current time δ t 0 the prediction in the figure is for the current time i e downstream streamflow is the same as the observed streamflow upstream the performance is expressed as the kge calculated at an annual scale i e based on one year of 15 minute data the panel on the right shows the fraction of stations on the y axis that experience kge value at least as high as that indicated on the x axis a perfect streamflow forecasting system would be represented as a horizontal line at y 1 therefore performance curves above the line shown in the panel i e in the shaded area would indicate the better overall performance of a given benchmark in the presentation of subsequent results we use the format of fig 3 spatial persistence can be combined with an adjustment for the water travel delay as all distances are known within the river network and between stream gauges and points of interest what remains is a specification of water velocity as analyzed in detail by krajewski et al 2020 selecting constant velocity in the upper percentile e g 70 or higher of the observed velocity leads to better performance we do not repeat the analysis here but we will come back to the issue of velocity choice as the time delay of basin response is a strong feature of hydrologic forecasting 4 2 benchmark 2 persistence and baseflow correction the spatial persistence method leads to consistently biased underestimated forecasts refer to krajewski et al 2020 for details two elements of the kge i e the multiplicative bias the ratio of the means and the variance ratio are almost always less than one the correlation is affected by the streamflow fluctuations in the unmonitored portion of the basin fig 2 and the routing of flow in the drainage network the runoff contributions from the unmonitored portion of the basin defined by gauge b see fig 2 are due to two components the baseflow and the quick flow the latter is the response to rainfall which for the moment we assume we do not know however the baseflow which is a slow varying component can easily and accurately be estimated in many regions simply adding baseflow to the streamflow observed at gauge a should lead to improved performance metrics by reducing the bias to check this straightforward hypothesis we performed baseflow separation and examined forecasting of the baseflow only the separation of runoff components from streamflow observation has been researched for a long time e g arnold et al 1995 sloto and crouse 1996 many different approaches have been proposed in the literature here we use an automated technique developed by the usgs known as the hydrograph separation program hysep sloto and crouse 1996 the hysep uses three schemes for baseflow separation fixed interval sliding interval and local minimum here we adopt the sliding interval method for the baseflow separation for further details of the separation technique refer to sloto and crouse 1996 indeed after separating the quick flow the median kge of forecasting the baseflow alone has increased significantly from 0 9 to 0 98 mean kge increases from 0 85 to 0 97 across 140 usgs stream gauges as indicated in earlier studies e g furey and gupta 2000 ahiablame et al 2013 singh et al 2019 gebert et al 2007 zhu and day 2009 baseflow shows a strong dependence on drainage areas using over 30 years of data for iowa we obtained the results shown in fig 4 which confirm that baseflow can be well predicted by using drainage area alone note that the scatter is small particularly for the smaller areas the distribution of unmonitored areas between pairs of usgs gauges in iowa shows a median of about 6 000 km2 while the maximum and minimum are about 28 000 km2 and 800 km2 respectively the figure also shows that while the relationship between drainage area and baseflow is strongly linear it changes from year to year when we account for the baseflow contribution from an unmonitored portion of the basins the consequent correction applied to the persistence forecasting in space krajewski et al 2020 remains static and is given by 5 q i i t 0 δ t q j j t 0 b f f o r δ t 0 δ t max where bf is the baseflow for the unmonitored area a i a j between locations i and j in the river network for example i and j correspond to gauge b and gauge a respectively in fig 2 and qi and qj are the streamflow forecast at i at lead time δt and streamflow observation at j at time t 0 respectively let us now examine the forecasting results of including the baseflow correction in the spatial persistence 5 if bf is estimated from the static relationship between the median annual discharge and the drainage area linear model fit to the red points in fig 2 the correction actually worsens the performance not shown of the persistence forecasts the correction affects the bias of the mean and variance ratio significantly despite its longer predictive time scale than that of the quick flow the static correction does not seem to address the dynamic year to year fluctuations in baseflow hence reducing the kge further across ar this is easy to understand if we consider the interannual variability of baseflow see fig 4 while on average the linear relationship between drainage area and baseflow is very strong there is substantial year to year variability between wet and dry years therefore the potential for introducing significant errors to the static baseflow correction is high on the other hand for any given year baseflow is also strongly proportional to the drainage area in fact the median r2 of all annual fits is 0 95 for the dynamic correction of the baseflow for the unmonitored portion of the basin one could use the area correction factor as shown in 6 6 af a i a j 7 q i i t 0 δ t q j j t 0 b j j t 0 a f f o r δ t 0 δ t max where af is the area correction factor qj is the quick flow ordinate at location j bj is the baseflow ordinate at location j and ai and aj are the upstream drainage areas at location i and j respectively consider fig 5 where we demonstrate the performance in terms of the kge across forecast lead times using the framework in 7 note that each panel demonstrates the kge across basins and years pooled together once dynamic baseflow correction is applied to the spf equation 7 significant improvement is achieved in the kge especially at smaller ar fig 5 left panels this is because the baseflow contributions from the unmonitored portion of the basin are dynamically accounted for in the integration with the spf forecasts note however that there is a larger variability in the kge at smaller values of ar which can primarily be attributed to the rainfall variability in the downstream unmonitored portion of the basin across years 2002 2018 the improvement with the baseflow correction is more apparent when presented in the form of empirical cumulative distribution function cdf fig 5 right panels the kge between 0 and 0 4 demonstrates the most improvement of baseflow correction over the spf as an example consider one day ahead forecasts about 30 of all combinations across years show the kge 0 4 using spf while about 70 of combinations show the kge 0 4 using the baseflow correction in 7 also notice that virtually all combinations result in the positive values of the kge across lead times which indicates performance better than the mean of the observations knoben et al 2019 the notable improvement in the kge arises primarily from the improvement in the bias component of the kge through baseflow correction in fig s2 we present the results in terms of percent peak difference pd panel a and percent bias pbias panel b the results show a similar pattern as the kge but with an even more pronounced reduction of the bias in spf only forecasts also one could explore other metrics such as the timing of hydrographs and the timing of the peak as krajewski et al 2020 indicated we expect similar behavior with the baseflow correction to the spf the dynamic baseflow correction in 7 can be further improved by accounting for travel time between the point of streamflow observations j and the point of interest i for us here an evaluation point the governing equation is 8 q i i t 0 δ t q j j t 0 d v b j j t 0 d v a f f o r δ t 0 δ t max where d is the distance between locations i and j and v is the constant flow velocity the improvement in terms of the kge is not significant and as in the case of spatial persistence higher velocities lead to better results we propose an explanation in the discussion section 4 3 benchmark 3 persistence and streamflow correction the baseflow correction does not capture rainfall caused fluctuations in the downstream unmonitored portions of the basin recognizing this we present a correction to the total streamflow component of the spf the catchment area ratio method has been in practice for many years for streamflow prediction at ungauged sites in the flow connected river network e g mohamoud 2008 nruthya and srinivas 2015 shu and ouarda 2012 swain and patra 2017 we apply the correction factor 6 to the streamflow forecasts using 9 q i i t 0 δ t q j j t 0 a f for δ t 0 δ t m a x where qi is the corrected streamflow forecast at location i and qj is the spatial persistence forecast at location j in fig 6 we show the performance of the spf with streamflow correction the pattern of results across forecast lead times is similar to that presented in fig 5 here we observe a significant improvement over spf forecasts at smaller ar fig 6 left panels as a forecaster one seeks to provide accurate forecasts particularly at riverine communities that are ungauged or sparsely gauged the results with streamflow correction are indeed promising at such locations let us again consider a one day ahead forecast and compare empirical cdfs the spf with streamflow correction shows about 50 improvement in the kge 25 vs 75 of the combinations show the kge 0 4 as fig s3 illustrates pd and pbias show a pattern similar to the kge note however that both shifts further to the right in comparison to the dynamic baseflow correction see figs s2 and s3 a clear advantage of streamflow correction is the significant reduction in the pbias as one would expect the quick flow component of the forecast hydrograph is addressed with 9 but not with 7 somewhat symmetric distribution of both metrics around 0 indicates that the variability across basins and years should arise from heterogeneity in the watershed and rainfall characteristics which streamflow correction assumes to remain similar in space one could compare the improvement in streamflow predictability between baseflow correction 7 and streamflow correction 9 because streamflow correction already includes the baseflow contribution it is reasonable to expect that the streamflow correction is systematically better in fig 7 we show the entire distribution of the kge between the two schemes across ar and forecast lead times results show that streamflow correction demonstrates slightly better performance over baseflow correction most notably for 0 3 ar 0 5 this indicates that accounting for streamflow for the unmonitored portion of the basins contributes to enhanced predictability but not across the entire range of ar note that the assumption of rainfall homogeneity between the forecast location j and evaluation gauges i in the river network does not hold across years to better understand the performance differences and similarities between the two corrective schemes consider the data shown in fig 9 it shows the time series of the kge and its components for two corrections at one station i e cedar rapids iowa with the forecasts made using streamflow data from waterloo iowa while the correlation coefficients track each other with a very slight advantage to the baseflow correction the mean and variance ratios fluctuate and are not always in sync moreover both schemes cross the unity line ideal result in different years this causes the differences in the kge favoring different corrections in different years to correct for the basin response delay one can use the following equation that includes constant flow velocity value v 10 q i i t 0 δ t q j j t 0 d v a f f o r δ t 0 δ t max the improvement of the kge with the addition of flow velocity is not significant which we explain further in the discussion section analyzing the results in figs 5 7 one may observe that there is very little sensitivity of the performance with forecast lead time collectively the performance for five days δ t 5 day ahead is as good as that for the current time δ t 0 this is counter intuitive scrutinizing fig 7 we find that for small values of ar the median kge is higher for the 5 day ahead forecast than it is for the current time careful analysis of the hydrographs shown in fig 8 confirms that this is indeed possible the explanation is that for small ar there is a considerable river distance and hence travel time from the monitored gauge to the evaluation gauge in fig 8 panel b the travel time is approximately three days another puzzle arises from the comparison between figs 5 and 6 the results seem to indicate that baseflow correction and streamflow correction perform essentially equally if we examine fig 9 we see that indeed all components of the kge track each other very closely but the mean ratio α and the variance ratio β cross the value of 1 perfect agreement for different years and this gives differences in year to year comparisons despite the overall similar performance 4 4 benchmark 4 persistence and rainfall because the source of the streamflow fluctuations in the unmonitored areas is rainfall it seems reasonable to expect that by including some simple rainfall descriptors we can further increase the streamflow predictability in keeping with the spirit of simplicity we explore and discuss some simple approaches initially we examined the use of mean areal precipitation map calculated for the ungauged portion of the basins from radar rainfall products the calculations are simple radar rainfall is given on a grid e g cartesian or latitude longitude and the basin boundaries are easily extracted from the dem data and can be pre calculated to form a binary mask the map can be calculated at the temporal resolution of the rainfall product here hourly two questions remain first because not all rainfall ends up as runoff what is the proper runoff coefficient to use the second question is about the time delay because the ungauged basins vary in size a different delay exists between the rainfall and the streamflow response depending on the basin drainage area regarding the question of the runoff coefficient rc we suggest a simple procedure either an initial subtraction such as the soil conservation service scs curve number cn method or a constant factor the latter is easier to implement as it does not require consideration of the event definition e g number of consecutive hours of rain with no break because it is not clear what the best value of rc should be we performed a simple sensitivity analysis our brief sensitivity analysis demonstrates that rc 0 1 is a reasonable number in comparison with the performance of the spf forecasts see figs s4 and s5 here the effective rainfall runoff i e map times rc is added to the spf forecasts note that we do not account for the lag between the rainfall and streamflow flow routing yet which we discuss in the subsequent section fig 10 illustrates the performance of spf coupled with map for δt 0 the addition of map information alone to the spf shows very little improvement over spf forecasts the indication of the rather poor performance of the map based scheme suggests the importance of using a dynamic rc to obtain an accurate estimation of runoff for instance crow et al 2017 and jadidoleslam et al 2019 demonstrated the dynamics of runoff coefficient as a function of antecedent soil moisture and rainfall across a large number of events note however that the objective of this framework is not to complicate the scheme but to simplify it additionally one should notice that the addition of map in its current form does not account for the spatial heterogeneity of rainfall and the routing of the generated flow next let us consider the spatial distribution of rainfall distributed rainfall provides distributed runoff which results in streamflow after being routed and aggregated by the river network a simple yet physically meaningful representation of rainfall is through the river network properties of a basin the width function e g ayalew and krajewski 2017 ghimire et al 2021a perez et al 2018 rodriguez iturbe et al 1998a represents the distribution of the distance to the outlet for each link in a river network the width function wf can be interpreted as the distribution of travel time in the river network if one assumes a constant flow velocity here we weigh the rainfall in the unmonitored area of the basin using the corresponding wf in fig s6 we demonstrate an example of the wf of the cedar river basin at conesville the rainfall is averaged for six zones 1 to 6 that are spaced approximately every 50 km from the basin outlet note that such rainfall representation not only accounts for the spatial variability but also for the flow transport process if one assumes a constant flow velocity to keep the procedure simple the runoff from rainfall at domains 1 to 6 reaches the outlet at different times alternatively streamflow at an arbitrary time represents the contribution from rainfall at different times in the past as krajewski et al 2020 illustrated the selection of the best constant velocity is not straightforward the flow velocity in the streams and rivers in iowa ranges between 0 2 m s to over 2 0 m s ghimire et al 2018 krajewski et al 2020 indicate that a velocity greater than the median on the order of 0 7 m s results in a similar performance below we explore the sensitivity of our benchmark to the selection of the velocity value but first let us discuss the distributed rainfall representation let r1 r2 r3 rn represent the area averaged rainfalls at domains 1 2 3 n respectively see fig s7 note that these domains are based on the width function of the respective basin the unmonitored part let d0 and d1 represent the distance from the basin outlet to the edge of the domain i e a bin of the width function 1 d1 and d2 represent the distance from the basin outlet to the edge of the domain d2 and d3 represent the distance from the basin outlet to the edge of the domain 3 and so on the addition of wf weighted rainfall to the spf results in 11 q i i t 0 δ t q j j t 0 d v c r c r 1 t 0 d 0 d 1 2 v r 2 t 0 d 1 d 2 2 v r 3 t 0 d 2 d 3 2 v r n t 0 d n 1 d n 2 v f o r δ t 0 δ t max where n is the number of equal travel distance zones of the wf of the basin and c is a unit conversion factor that accounts for the unmonitored basin area and time resolution of rainfall note that an equally simple way of representing distributed rainfall is by calculating the distance and thus the travel time along the river network for each pixel in the rainfall grid therefore implicitly representing the width function e g smith et al 2005 in this study we implemented the procedure based on eq 11 in fig 11 we demonstrate the performance of the scheme in eq 11 there is a clear indication that the depiction of spatially distributed rainfall in the spf framework enhances the performance but not as much as baseflow correction or streamflow correction a notable improvement is seen for the smaller monitored area fraction ar 0 4 i e in the range of 0 4 kge 0 4 note that the scheme in 11 still does not account for the baseflow contribution for the unmonitored area therefore one could account for baseflow correction to the scheme in 11 as shown below 12 q i i t 0 δ t c r c r 1 t 0 d 0 d 1 2 v r 2 t 0 d 1 d 2 2 v r 3 t 0 d 2 d 3 2 v r n t 0 d n 1 d n 2 v q j j t 0 d v b j j t 0 d v a f f o r δ t 0 δ t max the result in fig 12 illustrates that the scheme by eq 12 is a significant improvement over the spf its performance follows a similar pattern to that of baseflow and streamflow corrections perhaps even better also see fig s7 the value of distributed rainfall information is evident where the forecasts are made for larger unmonitored areas we foresee that the potential of the scheme given by eq 12 could go even further if we could take advantage of the availability of distributed rainfall by weighting local pixel rainfall with the flow distance to the basin outlet 5 discussion and conclusions the results from our four benchmarks for streamflow forecasting provide some interesting insights first while simple spatial persistence offers good performance particularly for those basins where about 60 or more is monitored by streamflow gauges krajewski et al 2020 the very simple corrections we discussed above can substantially improve the forecast benchmarks 2 and 3 i e the baseflow correction and the streamflow correction respectively are simple to implement based on readily available data offering a markedly improved forecasting skill at this point let us return to the velocity issue because each of the benchmarks can benefit from accounting for the travel time delay let us examine the sensitivity of all schemes to the constant velocity specification in fig 13 we show the sensitivity of the kge to flow velocity across schemes described in 4 b 4 d the velocity quantiles are the median v70 and v90 based on the unconditional distribution of the observed velocities at the usgs sites in iowa ghimire et al 2018 these correspond to 0 45 0 65 and 1 m s respectively here we present the sensitivity results in terms of space time scale or reference time tr which is the ratio of forecast lead time tl to the basin travel time tt ghimire et al 2021b vivoni et al 2006 the presentation in terms of tr allows us to interpret the forecasting skill through space and time interaction and normalizes to some degree the different scales of basin response two patterns emerge from this result first the kge for all schemes shows a decline with the increasing tr and the best performance for tr 1 the value of tr 1 represents the situation in which all the stormwater does not reach the basin outlet or in this case the evaluation point also the pattern remains similar across flow velocities second there is a minor difference across flow velocities for all schemes to elucidate this further we show the forecasted hydrographs for the dynamic baseflow correction the streamflow correction and the distributed rainfall correction in fig s8 for ar 0 68 and fig s9 for ar 0 21 along with the hydrographs we show the observed data and the performance metrics of the kge as well as the correlation coefficient the hydrographs illustrate that the addition of velocity does not result in a significant impact on the kge across schemes it is interesting to note that the best performance is for a high velocity value this is understandable because the actual velocity is a non linear function of discharge and the hydrograph rises in response to rainfall are associated with higher than average velocities additional information in the form of space time distributed rainfall is more challenging to implement there is substantial sensitivity to the specification of rc the cn method cronshey 1986 provides some guidance in this study we used a simple and conservative constant value of rc 0 1 however our use of this value should not be seen as a strong recommendation the other aspect of using rainfall radar data is also challenging while using insights provided by the topology of the river network provides proper weighting of rainfall field features proper accounting for the delay requires specification of river runoff travel velocity as in reality water velocity varies over an order of magnitude and is a nonlinear function of discharge and river channel characteristics representing it well with a single constant over the domain and time value is difficult we used the value above the median of the observed distribution with this in mind we recommend benchmarks 1 2 and 3 without hesitation as they are simple to implement and easy to interpret for benchmark 4 we recommend using it with the associated sensitivity analysis concerning parameters for the runoff coefficient and the water velocity such analysis can prove to be helpful to ai modeling beyond providing a performance benchmark the proposed benchmarks have some obvious limitations they are relevant for the hydrologic regimes where streamflow is dominated by rainfall where interactions of surface and groundwater are not significant where river network connectivity represents water movement and where water flow regulation does not play an important role note that our benchmarking framework cast here as a reference tool for ai models also has utility for other hydrologic models both data based and physics based or a combination of both when data assimilation techniques are incorporated the benchmarks provide a low limit for what can be achieved with data only if a hydrologic model operated in open loop mode has any skill at all it should in principle improve the forecasting when combined with data to demonstrate this we provide an example of the hillslope link model hlm operated state wide at the ifc in real time e g krajewski et al 2017 quintero et al 2020a b the operational hlm is the physics based fully distributed model used for flood forecasting in iowa for details of the hlm refer to krajewski et al 2017 quintero et al 2020a b and ghimire et al 2021a note that we use the stage iv rainfall for the years 2002 2018 and treat it as a proxy for the perfect quantitative precipitation forecasts qpf in fig 14 we show a comparison of forecasting skills between the hydrologic insights based forecasts and the hlm based forecasts with simple data assimilation the most notable improvement over spf is by benchmarks 2 3 and 4 the improvement is primarily in the range of 0 kge 0 8 with some dependence on forecast lead times relative to the operational ifc model benchmarks 2 3 and 4 indicate that they could be regarded as the bounds for evaluation note that the most notable improvement by the ifc model is for the region kge 0 4 with a clear dependence on lead time as one might expect the hydrologic model adds value particularly at longer lead times credit authorship contribution statement witold f krajewski conceptualization methodology resources supervision validation visualization writing original draft writing review editing ganesh r ghimire conceptualization methodology data curation formal analysis validation visualization writing original draft writing review editing ibrahim demir methodology writing original draft writing review editing ricardo mantilla methodology writing original draft writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the iowa flood center at the university of iowa funded this study the first author also acknowledges partial support from the rose joseph summers endowment the authors acknowledge fruitful discussions with our colleagues felipe quintero zhongrun xiang radoslaw goska bongchul seo navid jadidoleslam and nicolas velasquez appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j hydroa 2021 100110 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
8568,in this paper we propose a set of simple benchmarks for the evaluation of data based models for real time streamflow forecasting such as those developed with sophisticated artificial intelligence ai algorithms the benchmarks are also data based and provide context to judge incremental improvements in the performance metrics from the more complicated approaches the benchmarks include temporal and spatial persistence persistence corrected for baseflow and streamflow as well as river distance weighted runoff obtained from space time distributed rainfall in the development of the benchmarks we use basic hydrologic insights such as flow aggregation by the river network scale dependence in basin response streamflow partitioning into quick flow and baseflow water travel time and rainfall averaging by the basin width function the study uses 140 streamflow gauges in iowa that cover a range of basin scales between 7 and 37 000 km2 the data cover 17 years this work demonstrates that the proposed benchmarks can provide good performance according to several commonly used metrics for example streamflow forecasting at half of the test locations across years achieves a kling gupta efficiency kge score of 0 6 or higher at one day ahead lead time and 20 of cases reach the kge of 0 8 or higher the proposed benchmarks are easy to implement and should prove useful for developers of data based as well as physics based hydrologic models and real time data assimilation techniques keywords real time streamflow forecasting artificial intelligence hydrologic insights benchmarking persistence iowa 1 introduction as applications of data driven artificial intelligence ai methods such as neural networks adaptive neural based fuzzy inference systems genetic programming support vector machine multi layered perceptron and deep learning algorithms find their way into hydrology we need to clearly define the scope of the uncharted territory where these methods have the potential to improve on existing approaches in this paper we focus on real time streamflow forecasting the goal is to establish benchmarks for the ai methods to beat across a range of spatial scales and forecast lead times the proposed benchmarks are data based simple to explain and easy to implement real time forecasting methods so that ai researchers can calculate them for their region and data period of interest for real time streamflow forecasting the benchmarks cover a range of spatial and temporal scales and relevant forecast lead time horizons ranging from hours to several days clearly the same benchmarks can be used for other non ai data based forecasting models providing additional context regarding model performance our motivation can be illustrated with a simple example assume a performance metric with the dynamic range from 0 to 1 with 1 being the best if the benchmark that is achievable with a simple method for a given scale region and lead time can result in the metric value of 0 8 this implies that the margin for improvement for data intensive methods is only 0 2 the opposite example is obvious data intensive methods at locations where the benchmark metric has a value of only 0 2 offer much more room for improvement simply reporting the metric devoid of the context provided by the benchmark can be misleading about the contributions offered by the data intensive methods note here that benchmarks are methods not metrics we illustrate the benchmarking framework using data from the well instrumented state of iowa as we imply in the title the benchmarks in addition to being data based take advantage of hydrologic insights something the ai has long promised we discuss these insights as we explore the methods but first we briefly review the existing approaches to real time streamflow forecasting focusing on the current practice in the united states a puzzling contrast exists between the operational methods and the approaches reported in the research literature a plethora of rainfall runoff models are discussed in the literature only a handful of methods are used in operational streamflow forecasting for instance most river forecast centers in the u s national weather service nws still use lumped models e g the sacramento soil moisture accounting model sacsma to estimate runoff at monitoring sites mcintire et al 1998 stellman et al 2021 reading the literature one gets the impression that the problem is easy and that many models upon calibration can handle the task and provide skillful forecasts this is in stark contrast to operational methods that still heavily rely on time proven lumped and empirical models developed decades ago and more importantly on the experience and regional knowledge of the human forecasters it is also shockingly obvious how little has been documented in the literature on the operational performance in real time streamflow forecasting the studies by welles et al 2007 welles and sorooshian 2009 and zalenski et al 2017 are rare exceptions a gap exists in communicating real time forecast skills relative to benchmarking forecasts and their dependence on various characteristics of the monitored basins there is also a gap in benchmark datasets for streamflow forecasting that can serve as a summary of a problem area providing a simple interface between disciplines without requiring extensive background knowledge to engage ai researchers in hydrological studies this situation provides the backdrop for the entry of the ai methods in particular the recent ones such as deep learning the literature reports some amazing results for different variants of the new kid on the block for instance xiang and demir 2020 xiang et al 2020 fang et al 2021 kratzert et al 2018 cheng et al 2020 and zuo et al 2020 employed deep learning techniques such as long short term memory lstm lee and ahn 2021 used stacking ensemble model chu et al 2021 used deep belief networks and lima et al 2016 employed online sequential extreme learning to demonstrate marked improvement over process based methods for most instances several studies e g nourani et al 2014 wang et al 2009 firat 2008 sit et al 2020 humphrey et al 2008 ardabili et al 2020 mosavi et al 2018 zounemat kermani et al 2021 provide a comprehensive overview of recent articles published in the field on the applications of the ai methods this paper abstains from casting judgment on any particular contribution focusing instead on benchmarking the benchmarks should be simple to calculate based on the widely available data both static and dynamic simplicity within the context of model complexity and functional utility is the key requirement of the framework we propose static data can include digital elevation models dem and the derivative topology of the drainage network as well as the land use and soil types which are widely available dynamic data comprise mainly of streamflow and rainfall and could easily also include weather data such as air temperature humidity wind and solar radiation the data should be complemented by hydrologic insights as mechanisms to extract knowledge from the data for example palash et al 2018 used a temporal persistence framework integrating rainfall at larger basins while leedal et al 2013 shahzad and plate 2014 smith et al 2014 and young 2002 demonstrated the utility of data based mechanistic models using transfer functions these studies make up the few notable contributions that use data based mechanistic models in real time streamflow forecasting in this paper we do not apply any ai algorithms leaving this to others in the hope that they will see the value of the benchmarks we propose we simply want to initiate a conversation between the ai experts and the hydrologic research community in the next section we describe the data we use in this study which is merely an illustration of the need for well defined benchmarks after discussing the data we describe four simple methods of data based streamflow forecasting that could serve as benchmarks along with those we examine the relevant hydrologic insights at the core of the methods performance we conclude with the discussion 2 data our benchmarking framework demonstration is based in the state of iowa in the upper midwest of the united states fig 1 iowa has a surface area of about 150 000 km2 and is bordered by two major rivers the mississippi river on the east and the missouri river on the west most interior river basins are contained within the state s borders and drain to those two large rivers the total water domain expands 160 000 km2 and includes parts of minnesota there are some 150 stream gauges operated by the united states geological survey usgs in addition the iowa flood center ifc krajewski et al 2017 at the university of iowa has added nearly 300 gauges bridge mounted ultrasonic sensors that measure river stage kruger et al 2016 the sensor network data is accessible from the iowa flood information system demir and krajewski 2013 the ifc in partnership with other agencies has developed synthetic rating curves for the bridge sensor locations quintero et al 2020a b we used streamflow observations from usgs monitored sites in iowa available in near real time the usgs uses 15 minute resolution river stage data to convert to streamflow using well maintained rating curves for our study we used data from 2002 to 2018 as the usgs started to make 15 minutes resolution data publicly available in 2002 we focus on nine major river basins in iowa fig 1 and table 1 rainfall data come from the national network of weather radars we used the nationwide product known as stage iv the product offers the longest available radar rainfall record at the national center for environmental prediction ncep from the regional hourly multi sensor radar and gauges precipitation analyses produced by the 13 river forecast centers ncep 2020 the high resolution 4 km by 1 hour of stage iv rainfall makes it flexible to calculate areal quantities for basins of various sizes and shapes it is attractive for mapping into high resolution units of the rainfall runoff models e g krajewski et al 2017 noaa 2020 while we acknowledge the existence of a higher resolution radar based rainfall data set known as multi radar multi sensor zhang et al 2016 a real time rain gauge corrected hourly product provided on a 1 km2 grid we consider that the stage iv product with its long record is more appropriate for our purposes 3 methods we propose four simple benchmarks as references for the evaluation of model based streamflow forecasts the models could be statistical conceptual physics based or ai based all these model categories have been the subject of numerous discussions in the literature and adding to this discussion is outside of the scope of this paper however we would like to point out that the overarching goal of real time streamflow forecasting is to provide accurate estimates of peak flow value and peak flow timing at all relevant locations in the river network irrespective of whether this is feasible with a given model in fig s1 supplementary information we present the overall methodology adopted for this study 3 1 proposed benchmarks the first and perhaps simplest benchmark for an assessment of forecasting models is persistence tomorrow will be the same as today ghimire and krajewski 2020 and krajewski et al 2020 have comprehensively discussed this approach across temporal and spatial scales while temporal persistence applies to the location of an observer spatial persistence krajewski et al 2020 is of broader relevance to our goal of proposing general benchmarks we illustrate the general problem with fig 2 at point a streamflow observations integrate the basin runoff response upstream and can be used to make a spatial persistence forecast at point b however there is often a substantial area that is not monitored thus its contribution to streamflow at b is unknown this brings us to the introduction of a second benchmark spatial persistence corrected for baseflow because streamflow often includes a significant contribution from baseflow the spatial persistence based forecast can be corrected thus reducing bias in the predictions the baseflow is known to be proportional to the drainage area e g ahiablame et al 2013 furey and gupta 2000 gebert et al 2007 singh et al 2019 zhu and day 2009 the coefficients of this relationship between the baseflow and drainage area can be estimated statically based on past observations or dynamically considering the more recent past we elaborate on this distinction in the results section the third benchmark is based on another correction that can be applied to the persistence method we refer to it as streamflow correction the concept behind it is that observed streamflow integrates all rainfall and runoff processes in the upstream part of the basin therefore the larger the monitored portion of the basin of interest is the more likely it is that a similar streamflow regime exists in the unmonitored part of the basin also the larger the basin the smaller the role played by the recent or future storms at least relatively we explain the details and compare the two corrective schemes in the results section the fourth and final correction uses observed rainfall we assume here that the available rainfall is distributed in space and time such data come from weather radar networks a commonplace instrument in many parts of the world while it seems that including rainfall data should lead to a vast improvement in the benchmarks it turns out that incorporating rainfall information is not straightforward because the basin response to rainfall is characterized by a delay therefore one needs a transfer function to estimate the resultant streamflow identification and estimation of such a function can be accomplished via calibration we consider any calibrated model as ineligible for the benchmark construction simplicity is the name of the game here in the next section we analyze a couple of ways to include radar rainfall data a relatively simple way of accounting for the rainfall effect is by using overland runoff translation the key question is with what velocity 3 2 forecast evaluation metrics evaluation of streamflow forecasts is the key to getting a sense of the performance of a real time streamflow forecasting system we used several standard continuous verification measures to evaluate forecasts against the usgs streamflow observations we calculate and report all metrics for a range of forecast lead times in the metric definitions below we skip this dependence on the lead time the metrics we consider here are kling gupta efficiency kge mean absolute error mae normalized mae nmae percent bias pbias and percentage peak difference pd the kge metric comprises three components pearson s correlation r variance ratio α and mean ratio β see gupta et al 2009 the ideal value of kge is equal to 1 1 k g e 1 r 1 2 α 1 2 β 1 2 where α σ f σ o and β μ f μ o σf is the standard deviation of forecasts σo is the standard deviation of observations μf is the mean of forecasts and μo is the mean of observations knoben et al 2019 pool et al 2018 and camici et al 2020 provide a detailed interpretation of the kge for hydrologic model evaluation mae is the measure of errors between the forecasts and observations and computed as 2 m a e 1 n i 1 n q f q o where qf and q o are forecasted and observed streamflow respectively and n is the length of the time series we normalize mae by the upstream drainage area to compute nmae pbias measures the average tendency of the forecasts to be larger or smaller than their observed counterparts and is given by 3 p b i a s 100 i 1 n q f q o i 1 n q o the optimal value range of pbias is 0 a positive value indicates overestimation while a negative value indicates underestimation finally we compute relative peak difference pd as 4 p d peak f peak o peak o 100 where peakf and peako are the values of peaks of forecasts and observations respectively 4 results 4 1 benchmark 1 simple persistence using persistence as a reference has been a commonplace way to assess the skill of weather forecasts for the difficult to forecast variables such as rainfall persistence is hard to beat in the context of streamflow forecasting ghimire and krajewski 2020 and krajewski et al 2020 found that forecast skill shows a strong dependence on basin scales and forecast lead times for instance basins 1000 km2 show a one day ahead forecast skill of median kge 0 9 for spatial prediction the dominant factor leading to good performance is not surprisingly the monitoring of a large portion of the basin of interest for example if 80 of the upstream area is monitored gauged one can expect a kge on the order of 0 8 a highly respectable result this is basin scale invariant therefore what matters is the fraction of the basin that is gauged these results are less astounding if one considers that the dynamical response is an aggregation and attenuation of rather slow processes of water movement on the land surface and in the channel network mantilla et al 2006 in time it takes a strong influx of rainfall water i e a significant fraction of the flow already in the channel to create a noticeable change in the streamflow regime most of the time there is no rain or the rain is insignificant and thus the forecasting score is high in space stream and river drainage networks aggregate the flow it takes a major tributary of a similar stream order to significantly change or increase the discharge ayalew and krajewski 2017 ghimire et al 2018 in fig 3 we show an example of the performance of the spatial persistence forecasting spf our proposed benchmark 1 in constructing the figure we used 17 years of data from 140 usgs stream gauges we performed spatial persistence prediction in the downstream directions only see krajewski et al 2020 therefore the evaluation is possible only for gauging stations that have at least one upstream station there are 69 such stations but the number of all possible flow connected pairs of forecast vs evaluation gauge varies from basin to basin and depends on the river network topology see table 1 the total number of such combinations is 110 in our study in fig 3 left panel each of the 1870 dots represents a single evaluation gauge year station year for the current time δ t 0 the prediction in the figure is for the current time i e downstream streamflow is the same as the observed streamflow upstream the performance is expressed as the kge calculated at an annual scale i e based on one year of 15 minute data the panel on the right shows the fraction of stations on the y axis that experience kge value at least as high as that indicated on the x axis a perfect streamflow forecasting system would be represented as a horizontal line at y 1 therefore performance curves above the line shown in the panel i e in the shaded area would indicate the better overall performance of a given benchmark in the presentation of subsequent results we use the format of fig 3 spatial persistence can be combined with an adjustment for the water travel delay as all distances are known within the river network and between stream gauges and points of interest what remains is a specification of water velocity as analyzed in detail by krajewski et al 2020 selecting constant velocity in the upper percentile e g 70 or higher of the observed velocity leads to better performance we do not repeat the analysis here but we will come back to the issue of velocity choice as the time delay of basin response is a strong feature of hydrologic forecasting 4 2 benchmark 2 persistence and baseflow correction the spatial persistence method leads to consistently biased underestimated forecasts refer to krajewski et al 2020 for details two elements of the kge i e the multiplicative bias the ratio of the means and the variance ratio are almost always less than one the correlation is affected by the streamflow fluctuations in the unmonitored portion of the basin fig 2 and the routing of flow in the drainage network the runoff contributions from the unmonitored portion of the basin defined by gauge b see fig 2 are due to two components the baseflow and the quick flow the latter is the response to rainfall which for the moment we assume we do not know however the baseflow which is a slow varying component can easily and accurately be estimated in many regions simply adding baseflow to the streamflow observed at gauge a should lead to improved performance metrics by reducing the bias to check this straightforward hypothesis we performed baseflow separation and examined forecasting of the baseflow only the separation of runoff components from streamflow observation has been researched for a long time e g arnold et al 1995 sloto and crouse 1996 many different approaches have been proposed in the literature here we use an automated technique developed by the usgs known as the hydrograph separation program hysep sloto and crouse 1996 the hysep uses three schemes for baseflow separation fixed interval sliding interval and local minimum here we adopt the sliding interval method for the baseflow separation for further details of the separation technique refer to sloto and crouse 1996 indeed after separating the quick flow the median kge of forecasting the baseflow alone has increased significantly from 0 9 to 0 98 mean kge increases from 0 85 to 0 97 across 140 usgs stream gauges as indicated in earlier studies e g furey and gupta 2000 ahiablame et al 2013 singh et al 2019 gebert et al 2007 zhu and day 2009 baseflow shows a strong dependence on drainage areas using over 30 years of data for iowa we obtained the results shown in fig 4 which confirm that baseflow can be well predicted by using drainage area alone note that the scatter is small particularly for the smaller areas the distribution of unmonitored areas between pairs of usgs gauges in iowa shows a median of about 6 000 km2 while the maximum and minimum are about 28 000 km2 and 800 km2 respectively the figure also shows that while the relationship between drainage area and baseflow is strongly linear it changes from year to year when we account for the baseflow contribution from an unmonitored portion of the basins the consequent correction applied to the persistence forecasting in space krajewski et al 2020 remains static and is given by 5 q i i t 0 δ t q j j t 0 b f f o r δ t 0 δ t max where bf is the baseflow for the unmonitored area a i a j between locations i and j in the river network for example i and j correspond to gauge b and gauge a respectively in fig 2 and qi and qj are the streamflow forecast at i at lead time δt and streamflow observation at j at time t 0 respectively let us now examine the forecasting results of including the baseflow correction in the spatial persistence 5 if bf is estimated from the static relationship between the median annual discharge and the drainage area linear model fit to the red points in fig 2 the correction actually worsens the performance not shown of the persistence forecasts the correction affects the bias of the mean and variance ratio significantly despite its longer predictive time scale than that of the quick flow the static correction does not seem to address the dynamic year to year fluctuations in baseflow hence reducing the kge further across ar this is easy to understand if we consider the interannual variability of baseflow see fig 4 while on average the linear relationship between drainage area and baseflow is very strong there is substantial year to year variability between wet and dry years therefore the potential for introducing significant errors to the static baseflow correction is high on the other hand for any given year baseflow is also strongly proportional to the drainage area in fact the median r2 of all annual fits is 0 95 for the dynamic correction of the baseflow for the unmonitored portion of the basin one could use the area correction factor as shown in 6 6 af a i a j 7 q i i t 0 δ t q j j t 0 b j j t 0 a f f o r δ t 0 δ t max where af is the area correction factor qj is the quick flow ordinate at location j bj is the baseflow ordinate at location j and ai and aj are the upstream drainage areas at location i and j respectively consider fig 5 where we demonstrate the performance in terms of the kge across forecast lead times using the framework in 7 note that each panel demonstrates the kge across basins and years pooled together once dynamic baseflow correction is applied to the spf equation 7 significant improvement is achieved in the kge especially at smaller ar fig 5 left panels this is because the baseflow contributions from the unmonitored portion of the basin are dynamically accounted for in the integration with the spf forecasts note however that there is a larger variability in the kge at smaller values of ar which can primarily be attributed to the rainfall variability in the downstream unmonitored portion of the basin across years 2002 2018 the improvement with the baseflow correction is more apparent when presented in the form of empirical cumulative distribution function cdf fig 5 right panels the kge between 0 and 0 4 demonstrates the most improvement of baseflow correction over the spf as an example consider one day ahead forecasts about 30 of all combinations across years show the kge 0 4 using spf while about 70 of combinations show the kge 0 4 using the baseflow correction in 7 also notice that virtually all combinations result in the positive values of the kge across lead times which indicates performance better than the mean of the observations knoben et al 2019 the notable improvement in the kge arises primarily from the improvement in the bias component of the kge through baseflow correction in fig s2 we present the results in terms of percent peak difference pd panel a and percent bias pbias panel b the results show a similar pattern as the kge but with an even more pronounced reduction of the bias in spf only forecasts also one could explore other metrics such as the timing of hydrographs and the timing of the peak as krajewski et al 2020 indicated we expect similar behavior with the baseflow correction to the spf the dynamic baseflow correction in 7 can be further improved by accounting for travel time between the point of streamflow observations j and the point of interest i for us here an evaluation point the governing equation is 8 q i i t 0 δ t q j j t 0 d v b j j t 0 d v a f f o r δ t 0 δ t max where d is the distance between locations i and j and v is the constant flow velocity the improvement in terms of the kge is not significant and as in the case of spatial persistence higher velocities lead to better results we propose an explanation in the discussion section 4 3 benchmark 3 persistence and streamflow correction the baseflow correction does not capture rainfall caused fluctuations in the downstream unmonitored portions of the basin recognizing this we present a correction to the total streamflow component of the spf the catchment area ratio method has been in practice for many years for streamflow prediction at ungauged sites in the flow connected river network e g mohamoud 2008 nruthya and srinivas 2015 shu and ouarda 2012 swain and patra 2017 we apply the correction factor 6 to the streamflow forecasts using 9 q i i t 0 δ t q j j t 0 a f for δ t 0 δ t m a x where qi is the corrected streamflow forecast at location i and qj is the spatial persistence forecast at location j in fig 6 we show the performance of the spf with streamflow correction the pattern of results across forecast lead times is similar to that presented in fig 5 here we observe a significant improvement over spf forecasts at smaller ar fig 6 left panels as a forecaster one seeks to provide accurate forecasts particularly at riverine communities that are ungauged or sparsely gauged the results with streamflow correction are indeed promising at such locations let us again consider a one day ahead forecast and compare empirical cdfs the spf with streamflow correction shows about 50 improvement in the kge 25 vs 75 of the combinations show the kge 0 4 as fig s3 illustrates pd and pbias show a pattern similar to the kge note however that both shifts further to the right in comparison to the dynamic baseflow correction see figs s2 and s3 a clear advantage of streamflow correction is the significant reduction in the pbias as one would expect the quick flow component of the forecast hydrograph is addressed with 9 but not with 7 somewhat symmetric distribution of both metrics around 0 indicates that the variability across basins and years should arise from heterogeneity in the watershed and rainfall characteristics which streamflow correction assumes to remain similar in space one could compare the improvement in streamflow predictability between baseflow correction 7 and streamflow correction 9 because streamflow correction already includes the baseflow contribution it is reasonable to expect that the streamflow correction is systematically better in fig 7 we show the entire distribution of the kge between the two schemes across ar and forecast lead times results show that streamflow correction demonstrates slightly better performance over baseflow correction most notably for 0 3 ar 0 5 this indicates that accounting for streamflow for the unmonitored portion of the basins contributes to enhanced predictability but not across the entire range of ar note that the assumption of rainfall homogeneity between the forecast location j and evaluation gauges i in the river network does not hold across years to better understand the performance differences and similarities between the two corrective schemes consider the data shown in fig 9 it shows the time series of the kge and its components for two corrections at one station i e cedar rapids iowa with the forecasts made using streamflow data from waterloo iowa while the correlation coefficients track each other with a very slight advantage to the baseflow correction the mean and variance ratios fluctuate and are not always in sync moreover both schemes cross the unity line ideal result in different years this causes the differences in the kge favoring different corrections in different years to correct for the basin response delay one can use the following equation that includes constant flow velocity value v 10 q i i t 0 δ t q j j t 0 d v a f f o r δ t 0 δ t max the improvement of the kge with the addition of flow velocity is not significant which we explain further in the discussion section analyzing the results in figs 5 7 one may observe that there is very little sensitivity of the performance with forecast lead time collectively the performance for five days δ t 5 day ahead is as good as that for the current time δ t 0 this is counter intuitive scrutinizing fig 7 we find that for small values of ar the median kge is higher for the 5 day ahead forecast than it is for the current time careful analysis of the hydrographs shown in fig 8 confirms that this is indeed possible the explanation is that for small ar there is a considerable river distance and hence travel time from the monitored gauge to the evaluation gauge in fig 8 panel b the travel time is approximately three days another puzzle arises from the comparison between figs 5 and 6 the results seem to indicate that baseflow correction and streamflow correction perform essentially equally if we examine fig 9 we see that indeed all components of the kge track each other very closely but the mean ratio α and the variance ratio β cross the value of 1 perfect agreement for different years and this gives differences in year to year comparisons despite the overall similar performance 4 4 benchmark 4 persistence and rainfall because the source of the streamflow fluctuations in the unmonitored areas is rainfall it seems reasonable to expect that by including some simple rainfall descriptors we can further increase the streamflow predictability in keeping with the spirit of simplicity we explore and discuss some simple approaches initially we examined the use of mean areal precipitation map calculated for the ungauged portion of the basins from radar rainfall products the calculations are simple radar rainfall is given on a grid e g cartesian or latitude longitude and the basin boundaries are easily extracted from the dem data and can be pre calculated to form a binary mask the map can be calculated at the temporal resolution of the rainfall product here hourly two questions remain first because not all rainfall ends up as runoff what is the proper runoff coefficient to use the second question is about the time delay because the ungauged basins vary in size a different delay exists between the rainfall and the streamflow response depending on the basin drainage area regarding the question of the runoff coefficient rc we suggest a simple procedure either an initial subtraction such as the soil conservation service scs curve number cn method or a constant factor the latter is easier to implement as it does not require consideration of the event definition e g number of consecutive hours of rain with no break because it is not clear what the best value of rc should be we performed a simple sensitivity analysis our brief sensitivity analysis demonstrates that rc 0 1 is a reasonable number in comparison with the performance of the spf forecasts see figs s4 and s5 here the effective rainfall runoff i e map times rc is added to the spf forecasts note that we do not account for the lag between the rainfall and streamflow flow routing yet which we discuss in the subsequent section fig 10 illustrates the performance of spf coupled with map for δt 0 the addition of map information alone to the spf shows very little improvement over spf forecasts the indication of the rather poor performance of the map based scheme suggests the importance of using a dynamic rc to obtain an accurate estimation of runoff for instance crow et al 2017 and jadidoleslam et al 2019 demonstrated the dynamics of runoff coefficient as a function of antecedent soil moisture and rainfall across a large number of events note however that the objective of this framework is not to complicate the scheme but to simplify it additionally one should notice that the addition of map in its current form does not account for the spatial heterogeneity of rainfall and the routing of the generated flow next let us consider the spatial distribution of rainfall distributed rainfall provides distributed runoff which results in streamflow after being routed and aggregated by the river network a simple yet physically meaningful representation of rainfall is through the river network properties of a basin the width function e g ayalew and krajewski 2017 ghimire et al 2021a perez et al 2018 rodriguez iturbe et al 1998a represents the distribution of the distance to the outlet for each link in a river network the width function wf can be interpreted as the distribution of travel time in the river network if one assumes a constant flow velocity here we weigh the rainfall in the unmonitored area of the basin using the corresponding wf in fig s6 we demonstrate an example of the wf of the cedar river basin at conesville the rainfall is averaged for six zones 1 to 6 that are spaced approximately every 50 km from the basin outlet note that such rainfall representation not only accounts for the spatial variability but also for the flow transport process if one assumes a constant flow velocity to keep the procedure simple the runoff from rainfall at domains 1 to 6 reaches the outlet at different times alternatively streamflow at an arbitrary time represents the contribution from rainfall at different times in the past as krajewski et al 2020 illustrated the selection of the best constant velocity is not straightforward the flow velocity in the streams and rivers in iowa ranges between 0 2 m s to over 2 0 m s ghimire et al 2018 krajewski et al 2020 indicate that a velocity greater than the median on the order of 0 7 m s results in a similar performance below we explore the sensitivity of our benchmark to the selection of the velocity value but first let us discuss the distributed rainfall representation let r1 r2 r3 rn represent the area averaged rainfalls at domains 1 2 3 n respectively see fig s7 note that these domains are based on the width function of the respective basin the unmonitored part let d0 and d1 represent the distance from the basin outlet to the edge of the domain i e a bin of the width function 1 d1 and d2 represent the distance from the basin outlet to the edge of the domain d2 and d3 represent the distance from the basin outlet to the edge of the domain 3 and so on the addition of wf weighted rainfall to the spf results in 11 q i i t 0 δ t q j j t 0 d v c r c r 1 t 0 d 0 d 1 2 v r 2 t 0 d 1 d 2 2 v r 3 t 0 d 2 d 3 2 v r n t 0 d n 1 d n 2 v f o r δ t 0 δ t max where n is the number of equal travel distance zones of the wf of the basin and c is a unit conversion factor that accounts for the unmonitored basin area and time resolution of rainfall note that an equally simple way of representing distributed rainfall is by calculating the distance and thus the travel time along the river network for each pixel in the rainfall grid therefore implicitly representing the width function e g smith et al 2005 in this study we implemented the procedure based on eq 11 in fig 11 we demonstrate the performance of the scheme in eq 11 there is a clear indication that the depiction of spatially distributed rainfall in the spf framework enhances the performance but not as much as baseflow correction or streamflow correction a notable improvement is seen for the smaller monitored area fraction ar 0 4 i e in the range of 0 4 kge 0 4 note that the scheme in 11 still does not account for the baseflow contribution for the unmonitored area therefore one could account for baseflow correction to the scheme in 11 as shown below 12 q i i t 0 δ t c r c r 1 t 0 d 0 d 1 2 v r 2 t 0 d 1 d 2 2 v r 3 t 0 d 2 d 3 2 v r n t 0 d n 1 d n 2 v q j j t 0 d v b j j t 0 d v a f f o r δ t 0 δ t max the result in fig 12 illustrates that the scheme by eq 12 is a significant improvement over the spf its performance follows a similar pattern to that of baseflow and streamflow corrections perhaps even better also see fig s7 the value of distributed rainfall information is evident where the forecasts are made for larger unmonitored areas we foresee that the potential of the scheme given by eq 12 could go even further if we could take advantage of the availability of distributed rainfall by weighting local pixel rainfall with the flow distance to the basin outlet 5 discussion and conclusions the results from our four benchmarks for streamflow forecasting provide some interesting insights first while simple spatial persistence offers good performance particularly for those basins where about 60 or more is monitored by streamflow gauges krajewski et al 2020 the very simple corrections we discussed above can substantially improve the forecast benchmarks 2 and 3 i e the baseflow correction and the streamflow correction respectively are simple to implement based on readily available data offering a markedly improved forecasting skill at this point let us return to the velocity issue because each of the benchmarks can benefit from accounting for the travel time delay let us examine the sensitivity of all schemes to the constant velocity specification in fig 13 we show the sensitivity of the kge to flow velocity across schemes described in 4 b 4 d the velocity quantiles are the median v70 and v90 based on the unconditional distribution of the observed velocities at the usgs sites in iowa ghimire et al 2018 these correspond to 0 45 0 65 and 1 m s respectively here we present the sensitivity results in terms of space time scale or reference time tr which is the ratio of forecast lead time tl to the basin travel time tt ghimire et al 2021b vivoni et al 2006 the presentation in terms of tr allows us to interpret the forecasting skill through space and time interaction and normalizes to some degree the different scales of basin response two patterns emerge from this result first the kge for all schemes shows a decline with the increasing tr and the best performance for tr 1 the value of tr 1 represents the situation in which all the stormwater does not reach the basin outlet or in this case the evaluation point also the pattern remains similar across flow velocities second there is a minor difference across flow velocities for all schemes to elucidate this further we show the forecasted hydrographs for the dynamic baseflow correction the streamflow correction and the distributed rainfall correction in fig s8 for ar 0 68 and fig s9 for ar 0 21 along with the hydrographs we show the observed data and the performance metrics of the kge as well as the correlation coefficient the hydrographs illustrate that the addition of velocity does not result in a significant impact on the kge across schemes it is interesting to note that the best performance is for a high velocity value this is understandable because the actual velocity is a non linear function of discharge and the hydrograph rises in response to rainfall are associated with higher than average velocities additional information in the form of space time distributed rainfall is more challenging to implement there is substantial sensitivity to the specification of rc the cn method cronshey 1986 provides some guidance in this study we used a simple and conservative constant value of rc 0 1 however our use of this value should not be seen as a strong recommendation the other aspect of using rainfall radar data is also challenging while using insights provided by the topology of the river network provides proper weighting of rainfall field features proper accounting for the delay requires specification of river runoff travel velocity as in reality water velocity varies over an order of magnitude and is a nonlinear function of discharge and river channel characteristics representing it well with a single constant over the domain and time value is difficult we used the value above the median of the observed distribution with this in mind we recommend benchmarks 1 2 and 3 without hesitation as they are simple to implement and easy to interpret for benchmark 4 we recommend using it with the associated sensitivity analysis concerning parameters for the runoff coefficient and the water velocity such analysis can prove to be helpful to ai modeling beyond providing a performance benchmark the proposed benchmarks have some obvious limitations they are relevant for the hydrologic regimes where streamflow is dominated by rainfall where interactions of surface and groundwater are not significant where river network connectivity represents water movement and where water flow regulation does not play an important role note that our benchmarking framework cast here as a reference tool for ai models also has utility for other hydrologic models both data based and physics based or a combination of both when data assimilation techniques are incorporated the benchmarks provide a low limit for what can be achieved with data only if a hydrologic model operated in open loop mode has any skill at all it should in principle improve the forecasting when combined with data to demonstrate this we provide an example of the hillslope link model hlm operated state wide at the ifc in real time e g krajewski et al 2017 quintero et al 2020a b the operational hlm is the physics based fully distributed model used for flood forecasting in iowa for details of the hlm refer to krajewski et al 2017 quintero et al 2020a b and ghimire et al 2021a note that we use the stage iv rainfall for the years 2002 2018 and treat it as a proxy for the perfect quantitative precipitation forecasts qpf in fig 14 we show a comparison of forecasting skills between the hydrologic insights based forecasts and the hlm based forecasts with simple data assimilation the most notable improvement over spf is by benchmarks 2 3 and 4 the improvement is primarily in the range of 0 kge 0 8 with some dependence on forecast lead times relative to the operational ifc model benchmarks 2 3 and 4 indicate that they could be regarded as the bounds for evaluation note that the most notable improvement by the ifc model is for the region kge 0 4 with a clear dependence on lead time as one might expect the hydrologic model adds value particularly at longer lead times credit authorship contribution statement witold f krajewski conceptualization methodology resources supervision validation visualization writing original draft writing review editing ganesh r ghimire conceptualization methodology data curation formal analysis validation visualization writing original draft writing review editing ibrahim demir methodology writing original draft writing review editing ricardo mantilla methodology writing original draft writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the iowa flood center at the university of iowa funded this study the first author also acknowledges partial support from the rose joseph summers endowment the authors acknowledge fruitful discussions with our colleagues felipe quintero zhongrun xiang radoslaw goska bongchul seo navid jadidoleslam and nicolas velasquez appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j hydroa 2021 100110 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
8569,this study presents an inverse modeling strategy for organic contaminant source localization the approach infers the hydraulic conductivity field the dispersivity and the source zone location beginning with initial observed data of contaminant concentration and hydraulic head the method follows an iterative strategy of adding new observations and revising the source location estimate non linear optimization using the gauss levenberg marquardt algorithm pest is tested at a real contaminated site then a limited number of drilling locations are added with their positions guided by the data worth analysis capabilities of pyemu the first phase of pest with pyemu guidance followed by addition of monitoring wells provided an initial source location and identified four additional drilling locations the second phase confirmed the source location but the estimated hydraulic conductivity field and the darcy flux were too far from the measured values the mismatch led to a revised conceptual site model that included two distinct zones each with a plume emanating from a separate source a third inverse modelling phase was conducted with the revised site conceptual model finally the source location was compared to results from a geoprobe mihpt campaign and historical records confirming both source locations by merging measurement and modeling in a coupled iterative framework two contaminant sources were located through only two drilling campaigns while also reforming the conceptual model of the site keywords contaminant source localization iterative strategy glma data worth contaminated site management field estimation abbreviations glma gauss levenberg marquardt algorithm dw data worth svd singular value decomposition rmse root mean square error n rmse normalized root mean square error 1 introduction the location of contaminated groundwater plumes remains one of the most difficult challenges in contaminant hydrogeology this task is challenging predominantly due to geologic heterogeneity and uncertainty regarding pollution sources atmadja and bagtzoglou 2001 well defined site characteristics geology and hydrogeology can facilitate source location as shown by the success of high resolution site characterization hrsc methods barber et al 2014 mccall et al 2014 mccall et al 2017 meyer et al 2008 rosenberg et al 2021 these strategies and techniques use scale appropriate measurement and sample density to define contaminant distributions and the physical context of the area with better certainty supporting faster and more effective site cleanup this work has been advanced considerably by the capabilities of geoprobe mihpt to sample the subsurface more rapidly and with higher resolution than standard drilling techniques however although is the direct push technique can be less expensive than drilling in many situations it still represents a large cost for site remediation clearly there are great advantages to use methods that can reduce the number of samples needed by strategically placing them in locations that carry important information regarding the plume location over the past 30 years groundwater flow and pollutant transport models have been coupled with inversion methods for source identification gorelick et al 1983 wagner 1992 michalak and kitanidis 2004a bashi azghadi et al 2016 many methods have been tested on synthetic cases generally relying on concentration data these methods compiled in table 1 and described in more detail in essouayed et al 2020 can be classified as nonlinear optimization geostatistical or backward simulation bagtzoglou and atmadja 2005 unfortunately few of the proposed methods have been tested on real cases and because of their dependence on knowledge of the hydraulic parameters or assumptions of homogeneity they may not be applicable on real sites a recent exception to this is the study by hwang et al 2020 who applied a backward transport approach neuman et al 2012 to a real site where the hydraulic conductivity field is supposed to be know precisely however as generally the k field has a large uncertainty this approach may indicate a wrong source position at most of the field sites another field test has been successful but it required both concentrations and time lapse electrical resistivity tomography of a tracer injection tso et al 2020 as a result it is still an outstanding challenge to locate contaminant sources in realistic heterogeneous conditions with unkown k field while also minimising sample collection cost in practice an approach with a high computational burden may not be used even if it could reduce monitoring costs therefore for applications to contaminant transport on real sites which may require many model runs to account for significant uncertainty there can be real advantages to employ a parsimonious approach such as the gauss levenberg marquardt algorithm method glma essouayed et al 2020 presented the use of glma to identify jointly the source location and the hydraulic and transport properties however this study only considered synthetic cases as a proof of concept of the approach for sites with heterogeneous unknown values of hydraulic conductivity k transverse dispersivity α t and source location for a continuous pollutant release the major objective of this study is to locate a contaminant source at a real contaminated site using a new method essouayed et al 2020 to our knowledge it is the first example of application of an iterative searching strategy at a real site due to the site specificities the potential source position is mainly varying along a direction perpendicular to flow the site has a chlorinated ethenes plume but the location of the source is uncertain very few observations were available at the beginning of the study the method was tested on a synthetic case study in a previous publication essouayed et al 2020 the present study aims to extend that work by testing if the method can be applied at a real site determining for this site how many drilling campaigns are necessary to reach a precise source location analysing the results to see if the estimated parameters especially the hydraulic conductivity field are consistent with site knowledge and verifying at the end of the study if the estimated source location was consistent with other information this last point is key to the study and may seem surprising in most field studies historical data and direct push are used early in the investigation but our objective was to test the ability of the localization method therefore we withheld this information from the investigators and used it for verification data worth dw analysis was used to optimize the location of new measurements to support source location data worth analysis cycle to optimize the data collection has been studied widely in hydrogeology freeze et al 1992 james and freeze 1993 james and gorelick 1994 fu and gómez hernández 2009 gates and kisiel 1974 maddock 1973 dausman et al 2010 hill et al 2013 in this study pyemu white et al 2016 is used for measurement optimization dw and data collection are conducted in an iterative framework new observations are identified then the conceptual and numerical models are updated and the dw is repeated as necessary final testing of the source location is based on comparison with measurements collected after the analyses namely geoprobe mihpt profiles and detailed historical positions of potential sources 2 material and methods 2 1 site context 2 1 1 general information the site is located on an industrial park active since 1970 s several investigations were carried out on the site but the area for this investigation has not been studied before the site is polluted by chlorinated ethenes due to historical tanks potentially present inside the building tce used for machines degreasing the source location approach is focused on the zone presented in fig 1 with a potential chlorinated ethenes source located below the ff building a conceptual model is presented in fig s1 of the supporting information before the present study four monitoring wells of 18 20 m depth were present within the zone of investigation p29 p32 p23 and p30 in fig 1 2 1 2 geology and hydrogeology the site is located on a quaternary alluvial formation overlapped on the upper cenomanian formation and in the glauconius clay of the lower cenomanian formation the main geological facies are filling materials from 0 to 3 5 m deep alluvial formation composed of sand silex and silty sand from 3 5 to 7 m deep orange brown coarse sand from 7 to 18 m deep upper cenomanian formation compact grey clay from 18 to 22 m deep upper cenomanian formation and sand from 22 to 30 m deep delineated on the bottom by the glauconius clay lower cenomanian formation the cenomanian formation is located in the orange brown coarse sand from 7 m below ground level bgl this layer is 23 m thick and is delineated on the bottom by the glauconius clay lower cenomanian formation however based on the presence of a compact grey clay located at 18 m bgl in piezometer logs available over the whole site the cenomaian formation is in fact divided into two aquifers an upper aquifer with a thickness of 11 m 7 18 m and a shallow aquifer 22 30 m that means that the clay layer limits the movement of the pollutant plume to the shallow aquifer no chlorinated ethenes concentrations were found in the few piezometers lying in the lower aquifer no information are available about a potential sorption of solvents on top of the clay layer the general groundwater flow direction is from the north east to the south west 2 2 site flow and transport model to cover the area of interest while being able to assign well defined boundary conditions far from the plume the domain used for the model extends 980 m in x and 880 m in y fig 3 with 5 m thickness water head measurements in existing wells did not show significant vertical gradients so flow was simulated using a two dimensional plan view model groundwater samples obtained at different depth in the aquifer show small vertical gradient see fig s2 suggesting that transport could also be simulated with a 2d approach to limit computational effort while minimizing impacts on the accuracy of the source location a variable grid size was used specifically the area of interest potential contaminated area and monitoring wells in fig 1 corresponding to building ff has a finer grid with a mesh size of 5 2 m the rest of the model has a larger mesh of 20 10 m flow was modeled with modflow2005 harbaugh 2005 monitoring was conducted from july 2017 to august 2018 on p29 and p23 to estimate piezometric temporal variations the results showed minimum temporal variations in hydraulic head allowing for the flow system to be represented as steady state it was found that after a simulation period of 1400 days there were no further changes in contaminant concentration within the domain allowing the plume to be modeled as steady state the natural limits represented by a nearby watercourse are located more than 1 km from the site downstream and upstream the boundary conditions were defined based on the water levels measured in the monitoring wells located at the downstream and upstream limits of the site 44 10 m in the north east and 42 20 m in the south west fig 3 the other boundaries are considered to be coincident with flowlines represented as no flow boundaries 2 3 source location strategy the global strategy is based on an iterative approach to minimise uncertainties at each source location phase this strategy is well suited to real world application as it can be initiated with observations in only a small number of wells in the plume and then expanded with one or two additional sampling campaigns the fig 2 shows a schematic representation of the strategy one cycle corresponds to one run of the source location algorithm glma using pest software and addition of new observations to provide new hydraulic heads h and concentrations c for each cycle the method provides estimated parameters k field α l and the source position including uncertainties on these parameters the position of the pollution source along the y axis is defined as ys the y axis aligned with the building s major direction makes an angle of approximately 30 with the head contours and is thus the building is oriented nearly perpendicular to the flow direction which makes the y axis a good candidate to segregate the source position uncertainties linked to ys are analysed through a dw analysis to identify new drilling locations that are most likely to provide reliable information regarding the source location in this paper one iteration consists of new drillings concentrations measurements glma run to estimate the parameters and data worth to find the points for the next iteration 2 4 glma approach glma implemented through pest doherty 2015 adjusts selected parameters to reduce the gap between observed and simulated data in the present study the adjustable parameters are hydraulic conductivity k field obtained through kriging with pilot points the longitudinal coordinate of the source ys and the transverse dispersivity α t as the present work is on a steady plume the concentrations may vary much more along the y axis than the x one this is why it was assumed as a first step that the position variation occurred along the y axis the number of unknowns k field and transport parameters is larger than the number of observations therefore to obtain a stable solution for this under determined system tikhonov regularisation and singular value decomposition tonkin and doherty 2005 are applied here the global approach fig 2 composed of two steps i an initial optimization to estimate the pilot point parameters for the k field by simulating only the hydraulic head data h ii a second optimization to estimate all of the parameters based on fitting both the hydraulic head h and concentration data c the values of the pilot points estimated in step 1 are not updated in this step the values of k and α l were log10 transformed to stabilize the variation among parameters and the observations were square root transformed to increase the weight on low concentration values in the global objective function further details about the parameter settings are given in essouayed et al 2020 2 5 data worth analysis cycle the prediction of interest is ys y coordinate of the source existing data are represented by the contaminant concentration in each cell of the domain once the simulation is completed determining the sensitivity of each parameter to the simulated data through glma allows consideration of all of the potential sampling locations in the domain that reduce the uncertainty on the prediction of interest the source position vilhelmsen and ferré 2018 define the dw as follows 1 dw σ dec 2 σ base 2 where σ dec 2 and is the variance of the prediction of interest after adding data and σ base 2 the variance value of the prediction of interest on the existing data at the beginning of the analysis dw varies between 0 and 1 for the optimization method assessment of adding new observations is realised with pyemu white et al 2016 with the jacobian matrix pyemu computes the variance σ dec 2 associated with each point each cell of the model then 1 d w is calculated to identify zones within which new observations are most likely to reduce the uncertainty of ys several authors detail the approach for a large number of additional wells e g xue et al 2014 however in practice it is not feasible to conduct a dw analysis for each new well it is much more economically reasonable to expect that multiple wells will be sited and contracted together for this study the number of additional observations after each iteration has been set to three a first point p1 is chosen in the zone where 1 d w is the highest essouayed et al 2020 developed a geostatistical approach to selected 2nd and 3rd points directional variograms are calculated based on the concentration field with the simulated plume from glma with the variogram parameters estimated these parameters are used to construct an ellipse p x p y p r x r y centered on the first selected additional observation the dimensions of the ellipse rx and ry correspond to 2 3 of the corresponding range of the two directional variograms doherty 2010 it is assumed that additional measurements within this ellipse will be too highly correlated with p1 as such the second point p2 is identified as the zone where 1 d w is highest outside of this ellipse around p1 similarly p3 is identified as the zone where 1 d w is the highest outside of ellipses around p1 and p2 2 6 site parametrization to represent the site heterogeneity 95 pilot points were regularly distributed on a relatively loose grid outside the area of interest and with a higher density inside kriging with a spherical type variogram and a range of 100 m and a zero nugget was chosen to interpolate the hydraulic conductivity among the pilot points the parameterization scheme for gmla for source location is implemented in the same way as in essouayed et al 2020 including the hydraulic conductivities represented by the pilot points k the ys coordinate of the source and the dispersivity α t see table 2 as the ratio α t α l is kept constant 0 1 varying α l leads to a proportional variation of α t which has the major influence on the plume concentrations the xs coordinate is considered fixed given the limited information available in the building and the low impact of this parameter on the location of the source tce and cis dce are the two main chlorinated ethenes present in the aquifer as at the site no vc was found it is supposed that cis dce degradation does not occur the tce cis dce sum can thus be considered as a tracer in the intial stage a few model runs were done to estimate a potential concentration in the source which reached a fixed concentration of 75 µmol l 1 this is a point source i e its width equal the cell width or 2 m here the concentration value will be calibrated in the last iteration when enough data will be available due to the low concentrations the model only includes dissolved phase transport sorption was neglected as the model is run until steady state the simulation was done with mt3dms using the tvd scheme for advection 3 results 3 1 iteration 1 3 1 1 initial state because of the small number of existing wells only 4 a first drilling campaign was conducted before applying the presented approach adding piezometers f1 f2 and f3 limited existing information in this area meant that it was possible that there could be a source beneath the building initial wells were added to test this possibility the piezometric map using these points is given in fig 3 showing a global flow direction from north east to south west the measured contaminant concentrations around the ff building are shown on table 3 and fig 4 as the molar sum of tce cis dce due to limited access to the building water sampling at p30 was not possible the highest concentration 17 µmoll 1 was measured at p29 a monitoring well located a priori in the upstream part of the plume f3 f2 and p32 appear to be located in the middle of the plume with similar concentrations 2 3 µmoll 1 at the downstream site boundary p23 presents the lowest measured concentration 1 46 µmoll 1 3 1 2 glma an initial model was constructed to loosely mimic the measured hydraulic head by a first glma optimization this first step allows the model to have initial value for the kpp used for the second step of optimization then the glma optimization based on the measured concentration and hydraulic head was conducted leading to an estimated source position at ys 642 14 m and a dispersivity of 0 5 0 13 m the estimated steady state plume based on the best fit parameter values is presented in fig 5 the root mean square error rmse between simulated and observed data for hydraulic head h and concentration c are 0 06 m and 1 2 μmoll 1 respectively for iteration 1 the coefficients of variation normalized rmse were 5 and 7 9 for h and c respectively fig s3 this indicated acceptable calibration based on the preselected limits of 5 for h and 10 for c the number of pest iteration steps for the flow fitting is generally between 5 and 10 to reach stabilization but this step is quite fast however for the transport runs the number of pest iterations to estimate the k field and dispersivity was limited to 30 due to computation time for the present model this leads to approximately 2 days of calculations on a 20 core machine 3 1 3 data worth analysis cycle the data worth dw analysis was conducted within the plume area a total of 8720 points each cell of the model in this area were added to create the ys uncertainty map similar to the strategy shown in essouayed et al 2020 two directional variograms were determined from the simulated concentrations following iteration 1 the ellipses centered on already sampled points have dimensions equal to the variogram ranges i e of 130 m in the x direction and 55 m in the y direction inaccessible areas e g beneath buildings were eliminated from consideration piezometer f7 was chosen because it is located in an accessible zone where the uncertainty is the highest yellow and red zones in fig 6 a when identifying the second sampling location points within an exclusion ellipse around f7 were disqualified the highest 1 dw areas in consideration are located northeast of this ellipse strictly following the dw protocol would have required that the point with the highest 1 dw should have been chosen however in keeping with the practical nature of this study an existing piezometer p30 which could not be sampled during the iteration 1 and that was located in a comparably high 1 dw zone was selected to minimize cost an ellipse around p30 covers the areas of highest 1 dw the location with the highest value outside of the first two ellipses is f6 because an existing unsampled well was used it was decided to add a fourth sampling location the accessible areas presenting the highest 1 dw outside of the three ellipses around the first three sampling points led to the choice of f5 as the fourth sampling location 3 2 iteration 2 3 2 1 initial state iteration 2 sampling was conducted on september 2018 one month after drilling in 10 piezometers f1 f2 f3 f5 f6 f7 p30 p29 p32 and p23 the results of the chlorinated ethenes concentrations measurements are presented in table 3 fig 7 summarizes the concentrations measured at the beginning of iteration 2 the maximum combined molar concentration 23 80 µmoll 1 was observed in f7 it appears that f6 1 01 µmoll 1 is not affected by the plume it can be seen that f6 is surrounded by locations f7 f5 and f2 that show evidence of contamination but are farther downgradient than f6 3 2 2 glma following iteration 1 the inferred values were ys 642 14 m and αl 4 97 1 27 m optimization based on all of the observations 15 for h and 10 for c results in a source allocation of ys 634 3 66 m and a dispersivity αt of 0 51 0 11 m fig 8 b the rmse of h and c are 0 08 m and 2 5 μmoll 1 nrmse h 5 and nrmse c 11 respectively fig s3 this represents considerable reduction in the uncertainty of both parameters 3 2 3 data worth analysis cycle the reduction in uncertainty following each iteration can also be seen in the dw maps comparing the area covered by high 1 dw values after iteration 1 fig 6 and iteration 2 fig 6b shows both a relocation of the areas likely to provide additional information as well as a reduction in the high 1 dw area 3 3 iteration 3 3 3 1 initial state iteration 3 hydrogeologic investigations are commonly data limited as a result it is common to collect data that lead to hydrologic surprises bredehoeft 2005 as the example in iteration 2 the sequential nature of the proposed approach encourages an investigator to reanalyse data as they are collected and to use the updated analysis to plan additional data collection indeed k field estimated by the gmla model following iteration 2 indicates areas of very low k at the edge of the ff building fig 8 however this low k feature is likely an anomaly that does not represent the real context of the geology and soil found at the site coarse sand and no impermeable foundation present close to the ff building at such depth in addition the darcy fluxes measured integrated over the representative depth using the direct velocity tool essouayed et al 2019 in the field do not agree with the velocities obtained by the model blue points in fig s4 this additional information made it possible to evaluate the origins of the uncertainties encountered in iteration 2 for source location based on the poor agreement between the inferred k field and knowledge of the site geology on the disagreement between the model derived darcy fluxes obtained and field measurements it must be considered that the conceptual model of the site was inadequate specifically it was decided to test an alternative model that proposes the existence of a second source zone in this case the results suggested that an acceptable model could not be formed based on a single contaminant source however as with many real world investigations data had already been collected therefore the first step was to assess the possibility of there being more than one source based on the data gathered in iteration 2 during the field investigations measurements of pollutant concentrations and darcy fluxes in the monitoring wells close to the ff building were conducted using the dvt essouayed et al 2019 this made it possible to evaluate the contaminant mass fluxes passing through each piezometer fig 9 a giving additional information on the plume dynamics f3 and f6 wells show quite small fluxes which suggest that a plume is not crossing this region moreover well f5 contains the highest tce concentration but no dce fig 9b suggesting that it is not possible that the water sampled at this point came from the north f6 and f7 such a tce concentration with the observed low fluxes zone along f3 and f6 would however be consistent with the presence of a second tce plume in the south two separate plumes are shown as color filled elliptical areas on fig 9a the results shown on fig 9b suggest that the northern plume represents degradation of tce to cis dce close to the source as cis dce is present over the whole zone whereas the southern plume is entirely composed of tce 3 3 2 glma iteration 3 in order to integrate this new conceptual model two sources were manually included in the numerical model one source was placed at y s 1 620 m with a concentration of 50 μmoll 1 and a second one y s 2 522 m with an initial source concentration of 40 μmoll 1 transverse dispersivity was considered to be known with a value of 0 51 m estimated during the previous gmla analysis the pilot points were calibrated approximately before optimization began then the gmla optimization was done and the results show i a first source y s 1 at a distance equal to 624 8 m and a concentration of 72 μmoll 1 and ii a second source y s 2 at a distance equal to 533 6 m and a concentration of 17 μmol l 1 fig 10 b the rmse values are equal to 0 08 m nrmse 4 6 and 1 9 μmol l 1 nrmse 8 for h and c respectively the source in the north impacts piezometers f7 p29 and f3 and a source in the south impacts f2 f5 and f1 as part of this analysis a new k field was inferred this new k field shows much better agreement with the site geological characteristics with smoother spatial variations of logk and the absence of long and straight lines fig 10a the possible rapid degradation of tce to cis dce in the north plume has been validated through reactive transport modelling not presented here owing to the small amount of data on the possible reducing area and the degradation rates we considered that it was not possible to include the reactive model inside the source localization approach it would have carried too many uncertainty the reactive transport model was just used to verify that the k field and source position were compatible with the observed cis dce tce ratio no vinyl chloride or ethene found at observation points in addition the simulated darcy fluxes are much closer to the measured values than those inferred following the iteration 2 analysis based on a conceptual model of a single source red points in fig s4 3 4 validation independently from the source location approach two strategies were adopted to estimate the source positions first a classical historical review of the storage and main use of chlorinated ethenes in the factory was conducted this allowed to delineate areas in the factory that were most likely to have been contaminated fig 11 the vertical rectangle oriented south to north does not provide more information about a potential location of a source on the other hand due to the flow orientation any leakage in the rectangle oriented in the west to east direction would lead to a plume reaching f7 which thus highly increases the potential presence of a plume around this area therefore the ys1 source found through the presented approach is likely to come from this area the second validation step was done through geoprobe mihpt drilling along a transect triangles on fig 11 the geoprobe mihpt was carried out before the development of the source localization method which explains why only the part of the ys2 was investigated results of this campaign were consulted only at the end of the study to avoid the knowledge of potential sources during the source localization process chlorinated ethenes were only encountered at two locations red triangles on fig 11 and at quite low concentrations however these locations are in good agreement with the inferred position of the y s 2 source found through the iterative approach unfortunately the geoprobe work was completed before the hypothesis of a second source at ys1 so no results are available yet to test the location of the presence of a source in the north 4 discussion and conclusion the strategy of a contaminant source location was tested on a real contaminated site to evaluate the reliability of the method developed and presented in essouayed et al 2020 due to the limited information available in the chosen zone and in order to perform the method developed in synthetic cases only one potential source was considered in the ff building at the beginning molar sums of tce and cis dce and hydraulic head were used as observed values for the first two iterations of the iterative data collection calibration data worth analysis cycle to our knowledge this is the first realization of this data collection strategy including unkown k field at a real field site it is certainly the first attempt to combine glma and dw the approach was successful in two ways first there was continual reduction of the uncertainty of the location of a source and the transverse dispersivity through the first two iterations of the process second the approach led to clear proposals for additional data collection that are objective and defensible third because the process requires a hydrologist to reevaluate data as it is collected and before new data collection is proposed the procedure forced to reconsider the conceptual model after iteration 2 this led to the consideration of complementary data mass flux measurements and ultimately to the proposal of a second contaminant source the final round of data collection was guided by this revised conceptual model leading to a much improved analysis consistent with all collected data following iteration 3 the final round of analyses led to two separate sources which impact f7 and p29 in the north and f2 and f5 in the south with different chemical signatures the two sources were identified at y s 1 623 8 m y s 2 532 6 m fig 10b the results of historical records related to potential contaminant sources and to a geoprobe mihpt investigation support the inferred source locations the use of two sources arose from the poor result of one source at stage 2 owing to the small amount of data before iteration 2 we think that the 2nd source option could have been tested before this iteration in general with the proposed method and the small amount of available we cannot suggest any theoretical approach to estimate the number of source zones in practice the proposed approach to data collection is designed to reduce the amount of data necessary to optimize specific targets of a field investigation as shown here the method does not replace expert judgement rather it allows an expert to make full use of models to optimize data collection the approach is limited by the initial knowledge about the site as embodied in the conceptual model and existing data but this is the case for any approach to hydrologic field investigations the real advantage of this modular and sequential approach is that it forces a hydrologic investigation to couple data collection and analyses continually improving data collection efficiency and effectiveness as understanding of the site improves this and similar approaches should be used more widely in the industry eventually completing historical approaches that rely on extensive data collection at the beginning of a project followed by modeling with little or no use of the model to guide data collection this work presents a first test of the method application at other sites will be required before it can be determined to be broadly validated there are interesting subtle points in our results specifically the method is aimed specifically at source location therefore as with all model applications care must be taken when interpreting other hydrologically interesting results for example we show that inverse modeling could match the concentrations if the hydraulic field was allowed to vary however for the presented case we demonstrate that the inferred hydraulic conductivity field was incorrect such an error may not be as clear in other cases this result is perhaps to be expected the limited available data could not uniquely constrain the variable hydraulic conductivity field the x y source location and its concentration and the dispersivity dimensional reduction requires both recognition of the uncertainty of some of the inferred parameters e g k x y and a willingness to narrow the scope of an investigation to match the available data in this study we decided to search for the y coordinate only not both the x and y this was a practical consideration the procedure could have been applied to search for the x location but this would have required drilling inside the factory the high cost of this data collection meant that this objective was outside the scope of this paper like many site investigations the presented study is based on a 2d model here we could assume reasonably that vertical variations of concentration were negligible we believe that the model can be extended to consider site conditions that suggest that flow and transport occur in 3d but practically this would be limited to sites that can be simplified to two or three layers and it would require very detailed information on the plume and geology the authors would be happy to collaborate with hydrogeologists who would like to extend the method to these more complex conditions credit authorship contribution statement e essouayed conceptualization methodology t ferré writing review editing g cohen writing review editing n guiserix funding acquisition o atteia conceptualization supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was developed during elyess essouayed phd and supported by innovasol bordeaux inp ensegid and ea 4592 georessources et environnement data of the developed method is available through essouayed et al 2020 with a previous use in synthetic cases appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j hydroa 2021 100111 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
8569,this study presents an inverse modeling strategy for organic contaminant source localization the approach infers the hydraulic conductivity field the dispersivity and the source zone location beginning with initial observed data of contaminant concentration and hydraulic head the method follows an iterative strategy of adding new observations and revising the source location estimate non linear optimization using the gauss levenberg marquardt algorithm pest is tested at a real contaminated site then a limited number of drilling locations are added with their positions guided by the data worth analysis capabilities of pyemu the first phase of pest with pyemu guidance followed by addition of monitoring wells provided an initial source location and identified four additional drilling locations the second phase confirmed the source location but the estimated hydraulic conductivity field and the darcy flux were too far from the measured values the mismatch led to a revised conceptual site model that included two distinct zones each with a plume emanating from a separate source a third inverse modelling phase was conducted with the revised site conceptual model finally the source location was compared to results from a geoprobe mihpt campaign and historical records confirming both source locations by merging measurement and modeling in a coupled iterative framework two contaminant sources were located through only two drilling campaigns while also reforming the conceptual model of the site keywords contaminant source localization iterative strategy glma data worth contaminated site management field estimation abbreviations glma gauss levenberg marquardt algorithm dw data worth svd singular value decomposition rmse root mean square error n rmse normalized root mean square error 1 introduction the location of contaminated groundwater plumes remains one of the most difficult challenges in contaminant hydrogeology this task is challenging predominantly due to geologic heterogeneity and uncertainty regarding pollution sources atmadja and bagtzoglou 2001 well defined site characteristics geology and hydrogeology can facilitate source location as shown by the success of high resolution site characterization hrsc methods barber et al 2014 mccall et al 2014 mccall et al 2017 meyer et al 2008 rosenberg et al 2021 these strategies and techniques use scale appropriate measurement and sample density to define contaminant distributions and the physical context of the area with better certainty supporting faster and more effective site cleanup this work has been advanced considerably by the capabilities of geoprobe mihpt to sample the subsurface more rapidly and with higher resolution than standard drilling techniques however although is the direct push technique can be less expensive than drilling in many situations it still represents a large cost for site remediation clearly there are great advantages to use methods that can reduce the number of samples needed by strategically placing them in locations that carry important information regarding the plume location over the past 30 years groundwater flow and pollutant transport models have been coupled with inversion methods for source identification gorelick et al 1983 wagner 1992 michalak and kitanidis 2004a bashi azghadi et al 2016 many methods have been tested on synthetic cases generally relying on concentration data these methods compiled in table 1 and described in more detail in essouayed et al 2020 can be classified as nonlinear optimization geostatistical or backward simulation bagtzoglou and atmadja 2005 unfortunately few of the proposed methods have been tested on real cases and because of their dependence on knowledge of the hydraulic parameters or assumptions of homogeneity they may not be applicable on real sites a recent exception to this is the study by hwang et al 2020 who applied a backward transport approach neuman et al 2012 to a real site where the hydraulic conductivity field is supposed to be know precisely however as generally the k field has a large uncertainty this approach may indicate a wrong source position at most of the field sites another field test has been successful but it required both concentrations and time lapse electrical resistivity tomography of a tracer injection tso et al 2020 as a result it is still an outstanding challenge to locate contaminant sources in realistic heterogeneous conditions with unkown k field while also minimising sample collection cost in practice an approach with a high computational burden may not be used even if it could reduce monitoring costs therefore for applications to contaminant transport on real sites which may require many model runs to account for significant uncertainty there can be real advantages to employ a parsimonious approach such as the gauss levenberg marquardt algorithm method glma essouayed et al 2020 presented the use of glma to identify jointly the source location and the hydraulic and transport properties however this study only considered synthetic cases as a proof of concept of the approach for sites with heterogeneous unknown values of hydraulic conductivity k transverse dispersivity α t and source location for a continuous pollutant release the major objective of this study is to locate a contaminant source at a real contaminated site using a new method essouayed et al 2020 to our knowledge it is the first example of application of an iterative searching strategy at a real site due to the site specificities the potential source position is mainly varying along a direction perpendicular to flow the site has a chlorinated ethenes plume but the location of the source is uncertain very few observations were available at the beginning of the study the method was tested on a synthetic case study in a previous publication essouayed et al 2020 the present study aims to extend that work by testing if the method can be applied at a real site determining for this site how many drilling campaigns are necessary to reach a precise source location analysing the results to see if the estimated parameters especially the hydraulic conductivity field are consistent with site knowledge and verifying at the end of the study if the estimated source location was consistent with other information this last point is key to the study and may seem surprising in most field studies historical data and direct push are used early in the investigation but our objective was to test the ability of the localization method therefore we withheld this information from the investigators and used it for verification data worth dw analysis was used to optimize the location of new measurements to support source location data worth analysis cycle to optimize the data collection has been studied widely in hydrogeology freeze et al 1992 james and freeze 1993 james and gorelick 1994 fu and gómez hernández 2009 gates and kisiel 1974 maddock 1973 dausman et al 2010 hill et al 2013 in this study pyemu white et al 2016 is used for measurement optimization dw and data collection are conducted in an iterative framework new observations are identified then the conceptual and numerical models are updated and the dw is repeated as necessary final testing of the source location is based on comparison with measurements collected after the analyses namely geoprobe mihpt profiles and detailed historical positions of potential sources 2 material and methods 2 1 site context 2 1 1 general information the site is located on an industrial park active since 1970 s several investigations were carried out on the site but the area for this investigation has not been studied before the site is polluted by chlorinated ethenes due to historical tanks potentially present inside the building tce used for machines degreasing the source location approach is focused on the zone presented in fig 1 with a potential chlorinated ethenes source located below the ff building a conceptual model is presented in fig s1 of the supporting information before the present study four monitoring wells of 18 20 m depth were present within the zone of investigation p29 p32 p23 and p30 in fig 1 2 1 2 geology and hydrogeology the site is located on a quaternary alluvial formation overlapped on the upper cenomanian formation and in the glauconius clay of the lower cenomanian formation the main geological facies are filling materials from 0 to 3 5 m deep alluvial formation composed of sand silex and silty sand from 3 5 to 7 m deep orange brown coarse sand from 7 to 18 m deep upper cenomanian formation compact grey clay from 18 to 22 m deep upper cenomanian formation and sand from 22 to 30 m deep delineated on the bottom by the glauconius clay lower cenomanian formation the cenomanian formation is located in the orange brown coarse sand from 7 m below ground level bgl this layer is 23 m thick and is delineated on the bottom by the glauconius clay lower cenomanian formation however based on the presence of a compact grey clay located at 18 m bgl in piezometer logs available over the whole site the cenomaian formation is in fact divided into two aquifers an upper aquifer with a thickness of 11 m 7 18 m and a shallow aquifer 22 30 m that means that the clay layer limits the movement of the pollutant plume to the shallow aquifer no chlorinated ethenes concentrations were found in the few piezometers lying in the lower aquifer no information are available about a potential sorption of solvents on top of the clay layer the general groundwater flow direction is from the north east to the south west 2 2 site flow and transport model to cover the area of interest while being able to assign well defined boundary conditions far from the plume the domain used for the model extends 980 m in x and 880 m in y fig 3 with 5 m thickness water head measurements in existing wells did not show significant vertical gradients so flow was simulated using a two dimensional plan view model groundwater samples obtained at different depth in the aquifer show small vertical gradient see fig s2 suggesting that transport could also be simulated with a 2d approach to limit computational effort while minimizing impacts on the accuracy of the source location a variable grid size was used specifically the area of interest potential contaminated area and monitoring wells in fig 1 corresponding to building ff has a finer grid with a mesh size of 5 2 m the rest of the model has a larger mesh of 20 10 m flow was modeled with modflow2005 harbaugh 2005 monitoring was conducted from july 2017 to august 2018 on p29 and p23 to estimate piezometric temporal variations the results showed minimum temporal variations in hydraulic head allowing for the flow system to be represented as steady state it was found that after a simulation period of 1400 days there were no further changes in contaminant concentration within the domain allowing the plume to be modeled as steady state the natural limits represented by a nearby watercourse are located more than 1 km from the site downstream and upstream the boundary conditions were defined based on the water levels measured in the monitoring wells located at the downstream and upstream limits of the site 44 10 m in the north east and 42 20 m in the south west fig 3 the other boundaries are considered to be coincident with flowlines represented as no flow boundaries 2 3 source location strategy the global strategy is based on an iterative approach to minimise uncertainties at each source location phase this strategy is well suited to real world application as it can be initiated with observations in only a small number of wells in the plume and then expanded with one or two additional sampling campaigns the fig 2 shows a schematic representation of the strategy one cycle corresponds to one run of the source location algorithm glma using pest software and addition of new observations to provide new hydraulic heads h and concentrations c for each cycle the method provides estimated parameters k field α l and the source position including uncertainties on these parameters the position of the pollution source along the y axis is defined as ys the y axis aligned with the building s major direction makes an angle of approximately 30 with the head contours and is thus the building is oriented nearly perpendicular to the flow direction which makes the y axis a good candidate to segregate the source position uncertainties linked to ys are analysed through a dw analysis to identify new drilling locations that are most likely to provide reliable information regarding the source location in this paper one iteration consists of new drillings concentrations measurements glma run to estimate the parameters and data worth to find the points for the next iteration 2 4 glma approach glma implemented through pest doherty 2015 adjusts selected parameters to reduce the gap between observed and simulated data in the present study the adjustable parameters are hydraulic conductivity k field obtained through kriging with pilot points the longitudinal coordinate of the source ys and the transverse dispersivity α t as the present work is on a steady plume the concentrations may vary much more along the y axis than the x one this is why it was assumed as a first step that the position variation occurred along the y axis the number of unknowns k field and transport parameters is larger than the number of observations therefore to obtain a stable solution for this under determined system tikhonov regularisation and singular value decomposition tonkin and doherty 2005 are applied here the global approach fig 2 composed of two steps i an initial optimization to estimate the pilot point parameters for the k field by simulating only the hydraulic head data h ii a second optimization to estimate all of the parameters based on fitting both the hydraulic head h and concentration data c the values of the pilot points estimated in step 1 are not updated in this step the values of k and α l were log10 transformed to stabilize the variation among parameters and the observations were square root transformed to increase the weight on low concentration values in the global objective function further details about the parameter settings are given in essouayed et al 2020 2 5 data worth analysis cycle the prediction of interest is ys y coordinate of the source existing data are represented by the contaminant concentration in each cell of the domain once the simulation is completed determining the sensitivity of each parameter to the simulated data through glma allows consideration of all of the potential sampling locations in the domain that reduce the uncertainty on the prediction of interest the source position vilhelmsen and ferré 2018 define the dw as follows 1 dw σ dec 2 σ base 2 where σ dec 2 and is the variance of the prediction of interest after adding data and σ base 2 the variance value of the prediction of interest on the existing data at the beginning of the analysis dw varies between 0 and 1 for the optimization method assessment of adding new observations is realised with pyemu white et al 2016 with the jacobian matrix pyemu computes the variance σ dec 2 associated with each point each cell of the model then 1 d w is calculated to identify zones within which new observations are most likely to reduce the uncertainty of ys several authors detail the approach for a large number of additional wells e g xue et al 2014 however in practice it is not feasible to conduct a dw analysis for each new well it is much more economically reasonable to expect that multiple wells will be sited and contracted together for this study the number of additional observations after each iteration has been set to three a first point p1 is chosen in the zone where 1 d w is the highest essouayed et al 2020 developed a geostatistical approach to selected 2nd and 3rd points directional variograms are calculated based on the concentration field with the simulated plume from glma with the variogram parameters estimated these parameters are used to construct an ellipse p x p y p r x r y centered on the first selected additional observation the dimensions of the ellipse rx and ry correspond to 2 3 of the corresponding range of the two directional variograms doherty 2010 it is assumed that additional measurements within this ellipse will be too highly correlated with p1 as such the second point p2 is identified as the zone where 1 d w is highest outside of this ellipse around p1 similarly p3 is identified as the zone where 1 d w is the highest outside of ellipses around p1 and p2 2 6 site parametrization to represent the site heterogeneity 95 pilot points were regularly distributed on a relatively loose grid outside the area of interest and with a higher density inside kriging with a spherical type variogram and a range of 100 m and a zero nugget was chosen to interpolate the hydraulic conductivity among the pilot points the parameterization scheme for gmla for source location is implemented in the same way as in essouayed et al 2020 including the hydraulic conductivities represented by the pilot points k the ys coordinate of the source and the dispersivity α t see table 2 as the ratio α t α l is kept constant 0 1 varying α l leads to a proportional variation of α t which has the major influence on the plume concentrations the xs coordinate is considered fixed given the limited information available in the building and the low impact of this parameter on the location of the source tce and cis dce are the two main chlorinated ethenes present in the aquifer as at the site no vc was found it is supposed that cis dce degradation does not occur the tce cis dce sum can thus be considered as a tracer in the intial stage a few model runs were done to estimate a potential concentration in the source which reached a fixed concentration of 75 µmol l 1 this is a point source i e its width equal the cell width or 2 m here the concentration value will be calibrated in the last iteration when enough data will be available due to the low concentrations the model only includes dissolved phase transport sorption was neglected as the model is run until steady state the simulation was done with mt3dms using the tvd scheme for advection 3 results 3 1 iteration 1 3 1 1 initial state because of the small number of existing wells only 4 a first drilling campaign was conducted before applying the presented approach adding piezometers f1 f2 and f3 limited existing information in this area meant that it was possible that there could be a source beneath the building initial wells were added to test this possibility the piezometric map using these points is given in fig 3 showing a global flow direction from north east to south west the measured contaminant concentrations around the ff building are shown on table 3 and fig 4 as the molar sum of tce cis dce due to limited access to the building water sampling at p30 was not possible the highest concentration 17 µmoll 1 was measured at p29 a monitoring well located a priori in the upstream part of the plume f3 f2 and p32 appear to be located in the middle of the plume with similar concentrations 2 3 µmoll 1 at the downstream site boundary p23 presents the lowest measured concentration 1 46 µmoll 1 3 1 2 glma an initial model was constructed to loosely mimic the measured hydraulic head by a first glma optimization this first step allows the model to have initial value for the kpp used for the second step of optimization then the glma optimization based on the measured concentration and hydraulic head was conducted leading to an estimated source position at ys 642 14 m and a dispersivity of 0 5 0 13 m the estimated steady state plume based on the best fit parameter values is presented in fig 5 the root mean square error rmse between simulated and observed data for hydraulic head h and concentration c are 0 06 m and 1 2 μmoll 1 respectively for iteration 1 the coefficients of variation normalized rmse were 5 and 7 9 for h and c respectively fig s3 this indicated acceptable calibration based on the preselected limits of 5 for h and 10 for c the number of pest iteration steps for the flow fitting is generally between 5 and 10 to reach stabilization but this step is quite fast however for the transport runs the number of pest iterations to estimate the k field and dispersivity was limited to 30 due to computation time for the present model this leads to approximately 2 days of calculations on a 20 core machine 3 1 3 data worth analysis cycle the data worth dw analysis was conducted within the plume area a total of 8720 points each cell of the model in this area were added to create the ys uncertainty map similar to the strategy shown in essouayed et al 2020 two directional variograms were determined from the simulated concentrations following iteration 1 the ellipses centered on already sampled points have dimensions equal to the variogram ranges i e of 130 m in the x direction and 55 m in the y direction inaccessible areas e g beneath buildings were eliminated from consideration piezometer f7 was chosen because it is located in an accessible zone where the uncertainty is the highest yellow and red zones in fig 6 a when identifying the second sampling location points within an exclusion ellipse around f7 were disqualified the highest 1 dw areas in consideration are located northeast of this ellipse strictly following the dw protocol would have required that the point with the highest 1 dw should have been chosen however in keeping with the practical nature of this study an existing piezometer p30 which could not be sampled during the iteration 1 and that was located in a comparably high 1 dw zone was selected to minimize cost an ellipse around p30 covers the areas of highest 1 dw the location with the highest value outside of the first two ellipses is f6 because an existing unsampled well was used it was decided to add a fourth sampling location the accessible areas presenting the highest 1 dw outside of the three ellipses around the first three sampling points led to the choice of f5 as the fourth sampling location 3 2 iteration 2 3 2 1 initial state iteration 2 sampling was conducted on september 2018 one month after drilling in 10 piezometers f1 f2 f3 f5 f6 f7 p30 p29 p32 and p23 the results of the chlorinated ethenes concentrations measurements are presented in table 3 fig 7 summarizes the concentrations measured at the beginning of iteration 2 the maximum combined molar concentration 23 80 µmoll 1 was observed in f7 it appears that f6 1 01 µmoll 1 is not affected by the plume it can be seen that f6 is surrounded by locations f7 f5 and f2 that show evidence of contamination but are farther downgradient than f6 3 2 2 glma following iteration 1 the inferred values were ys 642 14 m and αl 4 97 1 27 m optimization based on all of the observations 15 for h and 10 for c results in a source allocation of ys 634 3 66 m and a dispersivity αt of 0 51 0 11 m fig 8 b the rmse of h and c are 0 08 m and 2 5 μmoll 1 nrmse h 5 and nrmse c 11 respectively fig s3 this represents considerable reduction in the uncertainty of both parameters 3 2 3 data worth analysis cycle the reduction in uncertainty following each iteration can also be seen in the dw maps comparing the area covered by high 1 dw values after iteration 1 fig 6 and iteration 2 fig 6b shows both a relocation of the areas likely to provide additional information as well as a reduction in the high 1 dw area 3 3 iteration 3 3 3 1 initial state iteration 3 hydrogeologic investigations are commonly data limited as a result it is common to collect data that lead to hydrologic surprises bredehoeft 2005 as the example in iteration 2 the sequential nature of the proposed approach encourages an investigator to reanalyse data as they are collected and to use the updated analysis to plan additional data collection indeed k field estimated by the gmla model following iteration 2 indicates areas of very low k at the edge of the ff building fig 8 however this low k feature is likely an anomaly that does not represent the real context of the geology and soil found at the site coarse sand and no impermeable foundation present close to the ff building at such depth in addition the darcy fluxes measured integrated over the representative depth using the direct velocity tool essouayed et al 2019 in the field do not agree with the velocities obtained by the model blue points in fig s4 this additional information made it possible to evaluate the origins of the uncertainties encountered in iteration 2 for source location based on the poor agreement between the inferred k field and knowledge of the site geology on the disagreement between the model derived darcy fluxes obtained and field measurements it must be considered that the conceptual model of the site was inadequate specifically it was decided to test an alternative model that proposes the existence of a second source zone in this case the results suggested that an acceptable model could not be formed based on a single contaminant source however as with many real world investigations data had already been collected therefore the first step was to assess the possibility of there being more than one source based on the data gathered in iteration 2 during the field investigations measurements of pollutant concentrations and darcy fluxes in the monitoring wells close to the ff building were conducted using the dvt essouayed et al 2019 this made it possible to evaluate the contaminant mass fluxes passing through each piezometer fig 9 a giving additional information on the plume dynamics f3 and f6 wells show quite small fluxes which suggest that a plume is not crossing this region moreover well f5 contains the highest tce concentration but no dce fig 9b suggesting that it is not possible that the water sampled at this point came from the north f6 and f7 such a tce concentration with the observed low fluxes zone along f3 and f6 would however be consistent with the presence of a second tce plume in the south two separate plumes are shown as color filled elliptical areas on fig 9a the results shown on fig 9b suggest that the northern plume represents degradation of tce to cis dce close to the source as cis dce is present over the whole zone whereas the southern plume is entirely composed of tce 3 3 2 glma iteration 3 in order to integrate this new conceptual model two sources were manually included in the numerical model one source was placed at y s 1 620 m with a concentration of 50 μmoll 1 and a second one y s 2 522 m with an initial source concentration of 40 μmoll 1 transverse dispersivity was considered to be known with a value of 0 51 m estimated during the previous gmla analysis the pilot points were calibrated approximately before optimization began then the gmla optimization was done and the results show i a first source y s 1 at a distance equal to 624 8 m and a concentration of 72 μmoll 1 and ii a second source y s 2 at a distance equal to 533 6 m and a concentration of 17 μmol l 1 fig 10 b the rmse values are equal to 0 08 m nrmse 4 6 and 1 9 μmol l 1 nrmse 8 for h and c respectively the source in the north impacts piezometers f7 p29 and f3 and a source in the south impacts f2 f5 and f1 as part of this analysis a new k field was inferred this new k field shows much better agreement with the site geological characteristics with smoother spatial variations of logk and the absence of long and straight lines fig 10a the possible rapid degradation of tce to cis dce in the north plume has been validated through reactive transport modelling not presented here owing to the small amount of data on the possible reducing area and the degradation rates we considered that it was not possible to include the reactive model inside the source localization approach it would have carried too many uncertainty the reactive transport model was just used to verify that the k field and source position were compatible with the observed cis dce tce ratio no vinyl chloride or ethene found at observation points in addition the simulated darcy fluxes are much closer to the measured values than those inferred following the iteration 2 analysis based on a conceptual model of a single source red points in fig s4 3 4 validation independently from the source location approach two strategies were adopted to estimate the source positions first a classical historical review of the storage and main use of chlorinated ethenes in the factory was conducted this allowed to delineate areas in the factory that were most likely to have been contaminated fig 11 the vertical rectangle oriented south to north does not provide more information about a potential location of a source on the other hand due to the flow orientation any leakage in the rectangle oriented in the west to east direction would lead to a plume reaching f7 which thus highly increases the potential presence of a plume around this area therefore the ys1 source found through the presented approach is likely to come from this area the second validation step was done through geoprobe mihpt drilling along a transect triangles on fig 11 the geoprobe mihpt was carried out before the development of the source localization method which explains why only the part of the ys2 was investigated results of this campaign were consulted only at the end of the study to avoid the knowledge of potential sources during the source localization process chlorinated ethenes were only encountered at two locations red triangles on fig 11 and at quite low concentrations however these locations are in good agreement with the inferred position of the y s 2 source found through the iterative approach unfortunately the geoprobe work was completed before the hypothesis of a second source at ys1 so no results are available yet to test the location of the presence of a source in the north 4 discussion and conclusion the strategy of a contaminant source location was tested on a real contaminated site to evaluate the reliability of the method developed and presented in essouayed et al 2020 due to the limited information available in the chosen zone and in order to perform the method developed in synthetic cases only one potential source was considered in the ff building at the beginning molar sums of tce and cis dce and hydraulic head were used as observed values for the first two iterations of the iterative data collection calibration data worth analysis cycle to our knowledge this is the first realization of this data collection strategy including unkown k field at a real field site it is certainly the first attempt to combine glma and dw the approach was successful in two ways first there was continual reduction of the uncertainty of the location of a source and the transverse dispersivity through the first two iterations of the process second the approach led to clear proposals for additional data collection that are objective and defensible third because the process requires a hydrologist to reevaluate data as it is collected and before new data collection is proposed the procedure forced to reconsider the conceptual model after iteration 2 this led to the consideration of complementary data mass flux measurements and ultimately to the proposal of a second contaminant source the final round of data collection was guided by this revised conceptual model leading to a much improved analysis consistent with all collected data following iteration 3 the final round of analyses led to two separate sources which impact f7 and p29 in the north and f2 and f5 in the south with different chemical signatures the two sources were identified at y s 1 623 8 m y s 2 532 6 m fig 10b the results of historical records related to potential contaminant sources and to a geoprobe mihpt investigation support the inferred source locations the use of two sources arose from the poor result of one source at stage 2 owing to the small amount of data before iteration 2 we think that the 2nd source option could have been tested before this iteration in general with the proposed method and the small amount of available we cannot suggest any theoretical approach to estimate the number of source zones in practice the proposed approach to data collection is designed to reduce the amount of data necessary to optimize specific targets of a field investigation as shown here the method does not replace expert judgement rather it allows an expert to make full use of models to optimize data collection the approach is limited by the initial knowledge about the site as embodied in the conceptual model and existing data but this is the case for any approach to hydrologic field investigations the real advantage of this modular and sequential approach is that it forces a hydrologic investigation to couple data collection and analyses continually improving data collection efficiency and effectiveness as understanding of the site improves this and similar approaches should be used more widely in the industry eventually completing historical approaches that rely on extensive data collection at the beginning of a project followed by modeling with little or no use of the model to guide data collection this work presents a first test of the method application at other sites will be required before it can be determined to be broadly validated there are interesting subtle points in our results specifically the method is aimed specifically at source location therefore as with all model applications care must be taken when interpreting other hydrologically interesting results for example we show that inverse modeling could match the concentrations if the hydraulic field was allowed to vary however for the presented case we demonstrate that the inferred hydraulic conductivity field was incorrect such an error may not be as clear in other cases this result is perhaps to be expected the limited available data could not uniquely constrain the variable hydraulic conductivity field the x y source location and its concentration and the dispersivity dimensional reduction requires both recognition of the uncertainty of some of the inferred parameters e g k x y and a willingness to narrow the scope of an investigation to match the available data in this study we decided to search for the y coordinate only not both the x and y this was a practical consideration the procedure could have been applied to search for the x location but this would have required drilling inside the factory the high cost of this data collection meant that this objective was outside the scope of this paper like many site investigations the presented study is based on a 2d model here we could assume reasonably that vertical variations of concentration were negligible we believe that the model can be extended to consider site conditions that suggest that flow and transport occur in 3d but practically this would be limited to sites that can be simplified to two or three layers and it would require very detailed information on the plume and geology the authors would be happy to collaborate with hydrogeologists who would like to extend the method to these more complex conditions credit authorship contribution statement e essouayed conceptualization methodology t ferré writing review editing g cohen writing review editing n guiserix funding acquisition o atteia conceptualization supervision writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was developed during elyess essouayed phd and supported by innovasol bordeaux inp ensegid and ea 4592 georessources et environnement data of the developed method is available through essouayed et al 2020 with a previous use in synthetic cases appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j hydroa 2021 100111 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
