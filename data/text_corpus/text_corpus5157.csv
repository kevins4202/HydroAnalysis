index,text
25785,the multi basin dynamic reservoir simulation model mb dyresm has been developed to bridge the gap between the computationally cheap one dimensional lake models and the computationally expensive three dimensional lake models it was extended from the classic one dimensional lake model dyresm by incorporating a horizontal water exchange algorithm to simulate gravitational mass transport between adjacent sub basins numerical experiments with increasing complexity were carried out for capacity demonstration in which mb dyresm successfully reproduced the thermal structure and mass distribution in each sub basin with accuracy comparable to and computational speed approximately 300 times faster than the three dimensional model applied in parallel in situations where turbulence was suppressed by stratification it actually provided better simulation results the combination of low computational demand reasonable spatial resolution of mass flux path and constrained numerical diffusion makes mb dyresm a more applicable option for simulations where the balance between affordable computational power and proper spatial resolution is concerned keywords pseudo three dimensional lake model stratified lakes flux path gravitational exchange 1 introduction one dimensional 1d lagrangian mass conserving hydrodynamic numerical models have been used to simulate the evolution of thermal structure and mixing regime in lakes on decadal temporal scales danis et al 2004 fenocchi et al 2018 matzinger et al 2007 trolle et al 2012 valerio et al 2015 these models distribute all vertical mass exchanges instantly across the area of the simulated basin and so do not correctly account for horizontal mass transport imberger et al 1978 of state variables other than those associated with the temperature stratification such basin wide averaging models are unsuitable for coupling with ecological models as they do not conserve horizontal heterogeneity of ecological state variables in lakes where horizontal homogeneity is not maintained marti et al 2016 horizontal transport can be accounted for by three dimensional 3d numerical models but such models are not suitable for long term simulations of mass flux path in strongly stratified lakes due to two major limitations first full navier stokes solvers resolve 3d spatial variability with three major types of coordinates griffies et al 2000 chen et al 2007 holleman et al 2013 depth coordinate z models terrain following coordinate σ models and isopycnal coordinate ρ models the z models suffer from excessive vertical mixing in lake interiors where stratification is strong leading to a false mass flux of the biogeochemical state variables that tends to dominate the actual signal in long term simulations griffies et al 2000 laval et al 2003 in σ models water layers are parallel to the boundary and this introduces numerical diffusion processes such as numerical baroclinic pressure gradient and numerical vorticity where the neutral direction is not parallel to the boundary mellor et al 1997 the ρ models provide natural frameworks for representing nutrient and tracer transportation along neutral directions which evidently reduce the numerical diffusion as in z models but ρ models are not suitable for representing surface mixed layer or benthic boundary layer bbl where high resolution of structure is required though water is well mixed second higher order 3d difference schemes have been developed that conserve mass and include more detailed closure schemes that can correctly mimic the effect of the stratification on the turbulence intensity and scale but they are computationally far too expensive to be a realistic option for coupling with biogeochemical models acosta et al 2015 bruggeman and bolding 2014 leon et al 2007 mooij et al 2010 rajar et al 1997 swayne et al 2005 and applied to large lakes over long times the approach adopted to address limitations of 1d models and 3d models is to divide a lake into a number of sub basins and incorporate an exchange algorithm to the existing 1d lagrangian parametric hydrodynamic model dynamic reservoir simulation model dyresm imberger et al 1978 yeates and imberger 2003 that simulates the horizontal exchange across a partition between two sub basins here we present the case where this exchange is driven by gravitational forcing as gravitational flows induced by differential deepening okely and imberger 2007 differential absorption macintyre and melack 1995 and differential cooling heating imberger and parker 1985 macintyre et al 2002 verburg et al 2011 woodward et al 2017 are commonly observed in lakes with variable bathymetry and or complex topography this approach is sufficient to establish a pseudo 3d framework that has the advantage that vertical mass exchange is conserved the model retains the computational advantages of dyresm and the horizontal flux path may be correctly modelled by the choice of the shape and number of sub basins the new pseudo 3d hydrodynamic model presented here multi basin dyresm mb dyresm represents a natural extension of the 1d dyresm a commonly used lake simulation model trolle et al 2012 in the following sections we first present a brief summary of the lagrangian scheme of dyresm followed by a description of the horizontal exchange algorithm between sub basins the new model is then evaluated by comparison with field data and results from full 3d numerical simulations derived with the aquatic ecosystem model 3d aem3d hodges et al 2000 hodges and dallimore 2019 2 methods 2 1 the predecessor 1d lagrangian model dyresm dyresm was originally developed in 1978 imberger et al 1978 for the simulation of changes in the vertical thermal structure in small or medium sized stratified lakes and reservoirs it was later upgraded by yeates and imberger 2003 to incorporate parameterization of internal mixing and boundary fluxes imberger and ivey 1993 marti and imberger 2006 generally the development of dyresm focused on strongly stratified lakes allowing the use of a lagrangian computational layer structure fig 1 a with an effective explicit vertical eddy diffusion coefficient provided for the basin average vertical mass flux this coefficient was considered as a calibration coefficient determined by comparing the model output with data from laboratory experiments or inferred from field observations with such a model structure only few formulas needed numerical solutions to update the layer structure at each time step thus dyresm runs extremely fast and its accuracy is excellent with respect to water temperature vertical numerical diffusion resulted from the amalgamation of neighboring layers when they became thinner than the user specified minimum thickness thus the model accuracy as influenced by numerical diffusion could be constrained by the user however because such a model does not account for any horizontal variations the effective implied horizontal numerical diffusion coefficient relevant for all biogeochemical state variables when the model was coupled with a water quality model was very large o l 2 δ t where l is a measure of the size of the plain area of the lake imberger and patterson 1981 this is a particular problem when it comes to the ecological state variables such as nitrogen or phosphorus which generally contribute only negligibly to the density field and so horizontal gradients of these variables remain unchanged until influenced by the ecology itself or by advection driven by forces not provided for in a 1d model for a model to adequately simulate those state variables that do not contribute significantly to the density of the water column it must account for horizontal transport that is equal to or slower than the characteristic ecological growth rate scale 2 2 development of mb dyresm 2 2 1 framework of mb dyresm operationally mb dyresm divides the lake of interest into sub basins with the number and shape being determined by the desired resolution fig 1b the basin topography and the expected hydrodynamic features it is convenient to think of two types of sub basins the central one sub basin 3 in fig 1b and the side ones s sub basins 1 2 4 5 in fig 1b dyresm is run for each sub basin and at each time step fluxes across each interface at each layer depth are then calculated using the parametric intrusion model described below the interfacial fluxes are then injected into the sub basin lagrangian layer structures these steps are then repeated for each time step mb dyresm may be thought of as consisting of two sets of functional computational routines fig 2 the lagrangian dyresm module simulating the vertical hydrodynamic processes in each sub basin at each time step and the eulerian exchange module simulating the horizontal gravitational water exchange between any two adjacent sub basins the dyresm module is the same as the original dyresm model yeates and imberger 2003 except that the daily loop has been removed to allow more flexibility for coupling with the exchange module this was motivated by the possibilities that a large daily inflow volume could overfill a sub basin and that a large daily discharge outflow volume could completely drain a sub basin 2 2 2 intrusion algorithm for the horizontal gravitational exchange in the current version of mb dyresm horizontal density gradients across any of the interfaces result from dyresm runs in adjacent sub basins yielding different vertical density structures the horizontal density gradients so formed across the separation curtains are assumed to drive a gravitational mass exchange across the interfaces at each time step the water exchanged due to horizontal density gradients is modelled as a lock exchange process cheong et al 2006 flynn and linden 2006 holyer and huppert 1980 maurer 2011 as shown in fig 3 in which the intrusive exchange gravity currents are assumed to be governed by an inertia buoyancy balance and no mixing the intrusion velocity u i can be calculated following cheong et al 2006 1 u i α ρ l ρ u ρ 0 g h h h 2 2 h h ρ i ρ u ρ l ρ u ρ i ρ u ρ l ρ u where α is a constant equal to 0 5 g 9 81 m s 2 is the gravitational acceleration ρ 0 1000 kg m 3 is the reference density ρ i is the density of the intruding layer ρ u and ρ l are respectively the densities of the upper and lower ambient layers h is the total fluid height and h is the interface height the parameters h e the equilibrium interface height and d the displacement of the upper lower layer interface from h e shown in fig 3b were not considered in the calculation of u i in mb dyresm as flynn and linden 2006 pointed out that the energy lost to the displacement of the interface made negligible difference to u i these two parameters are presented in fig 3b because the displacement of the interface was observed in the lock exchange experiments and also captured by the aem3d simulation as will be shown in section 3 1 given that the thickness of each layer may change during the operation of the dyresm module it is common that layer depths in the two adjacent sub basins do not stay aligned vertically making the application of the above equation to all the layers infeasible to solve this issue any layer across the layer depth at the other side of the interface marked as i to iv in fig 1b is split at that depth so that layers in one sub basin are horizontally aligned with the layers in the adjacent sub basin as a result intrusion conducted by the exchange module at each of the new layer depths is simplified as intrusion between two layers with identical thickness in turn with the parameter h set to zero eq 1 reduced to 2 u i α ρ 1 ρ 2 ρ 0 g h where ρ 1 and ρ 2 are the densities of the layers in the two adjacent sub basins at the same depth along with the intruding flow a return flow is conducted above it if the density of the return flow is higher or below it if the density of the return flow is lower the velocity of the return flow also computed with eq 2 is the same as the velocity of the intruding flow in magnitude as a result of identical layer thickness and horizontal density gradient magnitude volumes of the intruding flow and return flow may then be calculated through multiplying u i by the cross sectional area of the layers step initialization of the exchange module in fig 2 with the thicknesses of the intruding flow and the return flow taken as half the thickness of the layers which leads to the same volume of water to be exchanged between adjacent sub basins this volume is then inserted into the adjacent sub basin as a new layer at the depth of neutral buoyancy with no mixing steps water exchange with no mixing and gravitational adjustment of the exchange module in fig 2 followed by the adjustments of depth and thickness of the layers remained but with reduced volume and the layers newly inserted according to the volume of these layers step morphometric adjustment of the exchange module in fig 2 finally at the end of the gravitational water exchange the density of all layers is adjusted by applying a density filter to the water column to account for the mixing induced by the intrusion step mixing due to gravitational intrusion of the exchange module in fig 2 2 2 3 mixing induced by the intrusion the water exchange across the interface between adjacent sub basins is first carried out assuming no mixing as detailed above the associated mixing of the intruded water with water in layers immediately above and below the intrusion level is simulated with an effective vertical diffusion coefficient using the density filter adopted from laval et al 2003 the changes brought about by diffusion are modelled by 3 s k 1 e δ z k λ s k e δ z k λ s k 1 where s k and s k 1 are the temperature or salinity of layers k and k 1 after applying the filter s k is the temperature or salinity of layer k before applying the filter δ z k is the layer thickness of layer k and λ is the smoothing length scale in meters the filter defined by eq 3 smooths the density profile directly if applied to the density profile of the water column or indirectly if applied to the temperature or salinity profile step mixing due to gravitational intrusion of the exchange module in fig 2 reducing the density gradient is equivalent to converting the available potential energy to kinetic energy for mixing which coincides with the mechanism behind the gravitational intrusion the smoothing effect the generation of intrusion induced mixing is more significant when the vertical density gradient is stronger and increases with λ laval et al 2003 stronger vertical density gradients normally result from greater horizontal density difference ρ 1 ρ 2 in eq 2 and thereby intrusion velocity between the adjacent sub basins is stronger hence the filter acts preferentially at depths where gravitational intrusion is relatively intensive 2 3 numerical experiments the performance of mb dyresm was evaluated by comparing simulation results with those from aem3d on domains of increasingly complex topography and external forcing as seen in fig 4 these numerical experiments were of three types experiment 1 two rectangular domains divided into two sub basins of identical bathymetry i e uniform depth firstly aem3d simulations were carried out on the smaller domain fig 4a1 for validation against experimental data for the lock exchange flow cheong et al 2006 flynn and linden 2006 this was performed for completeness because the lock exchange algorithm is fundamental in characterizing the gravitational water exchange in mb dyresm then both mb dyresm and aem3d simulations were carried out on the larger domain fig 4a2 to validate the water exchange process described above to launch the water exchange in the larger domain one sub basin was initialized with a fully mixed water column and the other with a stratified water column the sharp horizontal density gradient drove a relatively intensive gravitational intrusion similar to the lock exchange experiments as shown in fig 3 experiment 2 a rectangular domain divided into two sub basins one shallow and one deep fig 4b unlike the previous numerical experiment the temperature and salinity of the entire domain was initialized to be homogeneous everywhere and the gravitational water exchange was established gradually by differential absorption as one sub basin was configured with higher turbidity gravitational water exchange is much milder than in the previous case which represents a more realistic situation commonly found in the natural environment experiment 3 lake argyle a large artificial tropical lake in australia fig 4c simulations mimicking an inflow were carried out to determine whether or not mb dyresm can capture the path of a river borne tracer in the lake 3 results 3 1 experiment 1 two sub basin rectangular domain with uniform depth 3 1 1 validation of aem3d for lock exchange flow experimental data the series of three lock exchange laboratory experiments a b and c performed by cheong et al 2006 and flynn and linden 2006 contained sufficient parameter variations in interface depth ratios h h 0 0 0 5 1 0 and density differences to provide a good validation table 1 for the aem3d simulation the rectangular domain 1 82 m 0 23 m 0 3 m fig 4a1 was gridded 0 01 m 0 023 m 0 002 m table 2 compares the dimensionless intruding velocities u i ˆ u i g l u h measured in the experiments of cheong et al 2006 with those obtained with application of aem3d with the same configurations the aem3d simulated u i ˆ agreed reasonably well with the experimental results especially in the low variability at h h 0 5 and the general variation pattern with respect to h h cheong et al 2006 also carried out extra experiments with two ambient layers of equal depths h h 0 5 and confirmed that the intruding velocity varied weakly with different combinations of initial intruding density and ambient density snapshots of the aem3d simulated density fields in all three series with h h 0 5 fig 5 show that the aem3d successfully reproduced the intrusion generated long wave indicated by the vertical displacement of the interface between the upper and lower layers in the non equilibrium cases ρ i ρ e series a and c while simulating no intrusion generated long wave in the equilibrium case ρ i ρ e series b therefore it is appropriate to verify the water exchange algorithm in mb dyresm with aem3d simulations 3 1 2 validation of the water exchange process algorithm to demonstrate the efficiency of mb dyresm water exchange algorithm numerical experiments were first carried out with mb dyresm and aem3d simulations over a rectangular domain 10 000 m in length 1000 m in width and 60 m in depth which was correspondingly gridded 50 m 100 m 0 4 m in aem3d in mb dyresm the domain was divided into two sub basins of identical bathymetry and identical initial water levels fig 4a2 initially sub basin 1 was well mixed while sub basin 2 was thermally stratified fig 6 therefore water in sub basin 1 should intrude between the two layers of sub basin 2 with a returning current of the upper layer water of sub basin 2 at the surface and another returning current of the lower layer water of sub basin 2 at the bottom eventually forming a three layer structure in both sub basins dyresm simulations were also carried out as a reference situation of instantaneous water exchange between the two sub basins table 3 shows the four simulation cases carried out with dyresm mb dyresm and aem3d in all cases the horizontal surface water temperature variation was forced to generate gravitational flows between the two sub basins in two ways surface heat fluxes activated or not and different turbidities in the two sub basins with such configurations the efficiency of mb dyresm in capturing the structure of surface layer under water exchange could be tested note that surface fluxes of radiation heat and momentum were activated deactivated by switching on off meteorological forcing in the simulations for simulations with surface fluxes activated the meteorological data were adopted from the lds measured data at lake argyle between 2nd and 4th august 2012 with mean values of incoming shortwave radiation net longwave radiation air temperature and relative humidity being 232 w m 2 70 w m 2 35 c initial surface water temperature 25 c and 43 respectively other meteorological inputs including wind speed and rainfall were all set to zero as a result the surface water was constantly heated up when surface fluxes were activated moreover given such a short simulation the heating effect of surface fluxes was well contained in the top 1 m of the water column since there is no explicit water exchange between the two sub basins in the dyresm simulations the simulated density profiles deviated from the initial profiles only at the surface when surface fluxes were activated cases 1 2 and 4 in fig 6 comparing the density profiles of mb dyresm to those of aem3d mb dyresm successfully captured the three layer structure resulting from the gravitational exchange in each sub basin with the thermoclines at about 10 m and 30 m well matched in both depth and thickness with those simulated by aem3d the density gradients across thermoclines simulated by mb dyresm were slightly sharper than those by aem3d indicating less mixing induced by the intrusive current characterized by mb dyresm this is acceptable given that as will be shown in section 4 3 the mixing in aem3d tended to be overestimated the results obtained for surface densities using mb dyresm matched those obtained using aem3d in all cases 3 2 experiment 2 two sub basin rectangular domain with non uniform depth mb dyresm model was evaluated for the case of differential absorption macintyre and melack 1995 in a two sub basin rectangular domain with non uniform depth fig 4b the domain consists of two adjacent sub basins of different depths 10 m in sub basin 1 and 40 m in sub basin 2 with no inflow or outflow fig 7 the entire domain was initialized with uniform water temperature 25 c and salinity 0 1 psu a passive tracer with concentration of 1 0 inserted into the entire sub basin 1 and a η p a r higher in sub basin 1 2 5 m 1 than sub basin 2 1 0 m 1 both mb dyresm and aem3d were forced with meteorological data averaged from the lds measured data at lake argyle between 18th and 29th september 2012 that kept both sub basins being constantly heated throughout the simulation fig 7 which included constant incoming shortwave radiation of 240 w m 2 and constant net outgoing longwave radiation of 70 w m 2 constant air temperature of 35 c 10 c warmer than the initial water temperature constant relative humidity of 48 and zero wind speed to eliminate the disturbance of horizontal momentum input discussed in section 4 4 the result of this configuration was a horizontal gravitational water exchange as described by imberger 1974 because more heat was trapped in the surface water of sub basin 1 than that of sub basin 2 and in turn set up the horizontal water temperature gradient between the sub basins the resulted horizontal water exchange was confirmed by the tracer and water temperature profiles simulated by mb dyresm and aem3d fig 8 tracer profiles simulated by both mb dyresm and aem3d have clearly shown the pattern of the gravitational water exchange between sub basins 1 and 2 tracer occurrence at 2 3 m and 6 10 m depth in sub basin 2 fig 8a2 and b2 implies that there was a surface flow and an intruding flow at depth from sub basin 1 to sub basin 2 in the opposite direction the dilution of tracer at 3 4 m depth in sub basin 1 implies that there was a return flow from sub basin 2 to sub basin 1 such a pattern of gravitational water exchange was driven by the horizontal water temperature gradient as being configured which can be confirmed by the simulated water temperature profiles at the beginning of the simulations surface water temperature in sub basin 1 started to increase notably on day 1 5 half a day earlier than that in sub basin 2 fig 8c1 2 and d1 2 more exactly the simulated surface water temperature difference between the two sub basins increased from 0 to around 1 5 c during the first 3 days and then maintained at this level until the end of simulation fig 9 a and from then it constantly needed about 1 extra day for the surface water temperature in sub basin 2 to reach the same level as in sub basin 1 fig 9b due to higher turbidity in sub basin 1 than in sub basin 2 water below the thermocline in sub basin 1 received less shortwave radiation than water at the same depth in sub basin 2 which can be confirmed by the fact that both models simulated a slightly shallower 26 c isotherm in sub basin 1 than sub basin 2 fig 8c1 c2 d1 and d2 as a result water below the thermocline in sub basin 1 was slightly cooler than that in sub basin 2 and the so formed horizontal temperature gradients drove intrusion flows and transferred tracer from sub basin 1 to sub basin 2 at about 6 10 m in depth fig 8a2 and b2 in general the features of tracer and temperature profiles and the revealed water exchange pattern simulated by mb dyresm have been well confirmed by the aem3d simulation which implies that mb dyresm was able to capture the intensity as well as the timing of the differential heating resulting from the configured differential absorption 3 3 experiment 3 application to a prototype case lake argyle australia lake argyle is a large artificial lake that was formed by the ord river dam in the 1970s fig 4c it is located in the kimberley region a tropical area in the northwest of australia where the climate is characterized by a semi arid to arid monsoon cycle made up of a dry season from april to october and a very wet season from november to march lake argyle has large expanses of shallow littoral zones which give rise to very strong gravitational exchange currents in the dry season as shown by woodward et al 2017 in the wet season rainfall is highly concentrated leading to impulsive inflows that set up correspondingly strong gravitational intrusions propagating all the way to the dam wall marti and imberger 2015 to achieve a quantification of the simulation accuracy of mb dyresm in a prototype setting simulation of an inflow event from 20th november to 4th december 2013 was carried out to ascertain the accuracy of the exchange algorithm as applied to a gravitational inflow intrusion as shown in fig 4c for the mb dyresm application lake argyle was divided into 3 sub basins sub basin 1 the shallow embayment receiving the ord river inflow sub basin 2 the central part of lake argyle connecting the inflow to the dam wall sub basin 3 peripheral sub basin not directly connected with the path of the inflow the simulations were forced with data fig 10 including meteorological forcing measured by the lds station located in sub basin 2 see fig 4c except that the rainfall data were obtained from the australian government bureau of meteorology http www bom gov au climate data ref ftr for the three stations rosewood lake argyle resort and argyle aerodrome averaged with inverse distance weighting inflow and outflow discharge rates were provided by department of water government of western australia australia and water temperature of the inflow was estimated as the averaged air temperature 4 days before a detailed description of the configuration of the lds and aem3d used here is presented in woodward et al 2017 the aem3d performance over the simulation period was assessed using the root mean squared error rmse between the measured and simulated water temperature profiles at the lds the rmse was 0 96 c similar to those reported in other applications of aem3d including lake argyle dissanayake et al 2019 tranmer et al 2018 woodward et al 2017 zamani and koch 2020 this provides confidence that aem3d simulations could be used for comparison with mb dyresm simulations a simulation with dyresm was also carried out on the whole lake i e single basin serving as a minimum reference for all simulations a passive tracer of constant concentration equal to 1 0 was continuously released into the ord river flow the simulated water temperature and tracer profiles are summarized in fig 11 among the water temperature profiles the appearance of inflow water can be observed only in the mb dyresm result fig 11 c1 29 c water accumulating below 10 m and above the 26 c bottom water in sub basin 1 at the end of day 6 it could not be observed in the aem3d result fig 11 d1 due to its relatively small volume in one simulation time step as compared to the capacity of the sub basin 105 m3 vs 109 m3 neither could it be observed in other sub basins in any simulation results or at the lds site fig 11a because the inflow water had already diffused in sub basin 1 note that the mb dyresm results also showed that water in the shallower sub basin 1 heated more than that in deeper sub basin 2 especially after day 8 fig 11c1 and c2 such differential heating was clearly confirmed by the aem3d results fig 11d1 and d2 which highlights the necessity of horizontal resolution for reproducing the correct flux path of mass at the same time the mb dyresm simulated tracer profiles were well matched by the aem3d simulated ones showing more straightforward appearance of inflow water in sub basin 1 and more evident horizontal and vertical variability among the three sub basins fig 11f and g for example both mb dyresm and aem3d simulated tracer arrived near the 10 m depth in sub basin 1 at the end of day 5 nearly 36 h after the start of the inflow event see fig 10f with concentration values around 0 2 20 of tracer input concentration and maintained a distribution between 10 m and 20 m until the end of simulation fig 11f1 and g1 also in both models the tracer reached sub basin 3 but the concentration values were negligibly small 10 9 so that a different tracer scale had to be used as shown in fig 11f3 and g3 the main discrepancy between mb dyresm and aem3d was that tracer reached sub basins 2 and 3 several days earlier in mb dyresm than in aem3d in sub basin 2 mb dyresm simulated tracer concentration values less than 10 3 can be seen from days 2 6 before the arrival of the clearly defined inflow intrusion on day 6 with a maximum concentration value of 10 1 10 of tracer input concentration fig 11f2 while aem3d simulated tracer concentration values less than 10 3 can be seen from days 8 9 before the arrival of the inflow intrusion at the end of day 9 with a maximum concentration value of 10 1 as well fig 11g2 in both models the intrusion remained until the end of the simulation and centred at the depth of 15 m aem3d results showed the tracer vertically mixed into the lake surface layer on day 8 fig 11g2 in sub basin 3 the simulated tracer arrived on days 5 and 12 in mb dyresm and aem3d respectively with concentration values less than 10 9 fig 11f3 and g3 part of this discrepancy can be explained by the algorithm used for the intrusive exchange gravity currents these are assumed to be governed by an inertia buoyancy balance and no mixing see section 2 2 this choice was motivated by the observation that most of the horizontal exchange takes place at depths of strong stratification that collapses the turbulence imberger et al 1976 this assumption is valid when the ratio of the inertia buoyancy layer thickness δ i q n b 1 2 to that of the mixing buoyancy layer δ k ε n 3 1 2 is larger than 1 i e inertia buoyancy balance dominates q being the discharge n the buoyancy frequency b the width and ε the dissipation rate of turbulent kinetic energy taking the values from lake argyle inflow intrusion q 350 m3 s 1 n 0 03 s 1 b 5000 m and ε 10 6 m2 s 3 and substituting these into the ratio δ i δ k q n 2 b ε 1 2 leads to δ i 1 53 m δ k 0 19 m and δ i δ k 8 05 1 this indicates that mb dyresm predicts a faster intrusion than aem3d as it can be seen in fig 11f2 g2 f3 and g3 the dyresm simulated tracer profile fig 11e did not match with the aem3d results in any sub basin again clearly highlighting the need for the accounting of horizontal resolution to obtain any flux path prediction overall mb dyresm satisfactorily demonstrated the ability to reproduce the mass flux characteristics of the inflow event at lake argyle with relatively low horizontal resolution with a huge saving in computational time over aem3d results show that mb drysem should be the model of choice when the horizontal exchange takes place at relatively large values of the ratio of inertia buoyancy layer thickness to mixing buoyancy layer thickness 4 discussion 4 1 lagrangian mass flux path to compare the tracer flux across each interface simulated by mb dyresm and aem3d tracer flux rates between sub basins at the two sub basin rectangular domain with non uniform depth experiment 2 and lake argyle experiment 3 were calculated the tracer flux from sub basin i to sub basin j f i j arrows in fig 4b and c m3 s 1 was defined as 4 f i j 0 s z b x h v x z c x z d z d x where s is the length of the interface between sub basins m z is the layer height m z b x and h are the bottom height m and surface height m of water column at accumulative distance x m along the sub basin interface respectively v x z is intrusion velocity at distance x and height z positive from sub basin i to sub basin j m s 1 and c x z is the tracer concentration fraction of 1 0 at distance x and height z as shown in fig 12 tracer fluxes simulated by mb dyresm generally reached steady state faster than those by aem3d owing to the 1d representation of each sub basin by mb dyresm and to the fact that the value of the ratio of inertia buoyancy layer thickness to mixing buoyancy layer thickness is larger than 1 however such discrepancy has not led to unacceptable deviations between mb dyresm and aem3d tracer concentration profiles for the differential absorption experiment 2 fig 8 and the inflow event at lake argyle experiment 3 fig 11 this is because mb dyresm was able to maintain a balanced water exchange between sub basins equivalent to but more dynamic than aem3d note that at lake argyle the mb dyresm simulated tracer fluxes f 1 2 and f 2 1 reached a relative steady state quickly after the inflow event started on day 5 while it took another 4 days for the aem3d simulated f 1 2 and f 2 1 to arrive at similar levels fig 12a2 and 12b2 such discrepancy was not observed in experiment 2 fig 12a1 and b1 and we believe this was associated with the more complicated external forcing fig 10 and irregular shape of lake argyle fig 4c as compared to the rectangular domain figs 4b and 7 and also the ratio of inertia buoyancy layer thickness to mixing buoyancy layer thickness being larger than 1 after the aem3d simulated tracer fluxes achieved a relative steady state they were matched well by the mb dyresm simulations a more evident discrepancy was observed at the interface between sub basins 2 and 3 during the inflow event at lake argyle fig 12c2 and d2 but greater disturbance in simulated f 2 3 and f 3 2 would be expected as they were so weak and intermittent compared to f 1 2 and f 2 1 the resultant tracer concentration in sub basins 3 being negligibly small fig 11f3 and g3 4 2 advantage of mb dyresm over dyresm the most significant advantage of mb dyresm over its predecessor dyresm is the introduction of horizontal spatial variability such an improvement firstly brings more accuracy in the simulation of lake hydrodynamics for example mb dyresm successfully captured the intrusion of an inflow event at lake argyle fig 11 from an ecological perspective the introduction of horizontal variability is a major advance especially when considering long term changes species nurtured in one part of a lake spread to other parts of the lake and although such issues are not addressed directly here mb dyresm is capable of reproducing such exchanges in dyresm i e single basin as shown in fig 11e tracer that entered the basin carried by the inflow was immediately transported into the entire lake which was not indicated by the aem3d simulation results but mb dyresm successfully captured the spatial and temporal patterns of the tracer distribution with primary accumulation in sub basin 1 and negligibly small accumulation in sub basin 3 however it should be recognized that the success of any parametric model depends on the relevance of the underlying model in context and the relative importance of mechanisms not included in the model in the case of a climate change model the underlying behaviour may change over time so that adjustments in the parameters of the model may be needed and even a change of model may be indicated this suggests that a continual checking and updating of parameters may be required perhaps over periods of years and that under over predictions should be made 4 3 advantages of mb dyresm over models like aem3d there are three major advantages of mb dyresm over models like aem3d first the numerical diffusion in mb dyresm is the same as that in the lagrangian dyresm and is totally controlled by the amalgamation of thin layers in contrast the numerical diffusion in aem3d results from the discretization of the navier stokes equations as described by laval et al 2003 the control of the numerical diffusion is an important feature because in a strongly stratified lake the actual vertical mixing is close to molecular imberger and marti 2014 to illustrate the superior numerical mass conservation characteristics of mb dyresm compared to aem3d both models were applied to the simulation of small amplitude seiching in a single basin bowl shaped domain fig 13 a a periodic western wind along the y axis of 5 m s 1 was activated for 5 h much shorter than the 30 h fundamental internal wave period every 30 days to excite periodic seiching the water column stratification consisted of a thermocline of mean temperature gradient 0 8 c m 1 between the depths of 12 m and 16 m fig 13 b a passive tracer of concentration of 1 0 was introduced below the thermocline fig 13 c and its upward flux with time was used as a measure of the numerical mixing error in each model heat fluxes and all types of physical mixing were turned off in both mb dyresm and aem3d by setting relevant coefficients to zero and using free slip boundary conditions with such configurations the maximum gradient richardson number simulated by aem3d in the central water column reached minimum values between 102 and 104 at depths around 20 m indicating that the seiching in the bowel shaped lake was laminar yeates et al 2013 as the richardson number was always greater than 0 25 there should be no diffusion of temperature gradient or tracer gradient and the mean temperature and tracer profiles should be maintained at the initial values in both simulations any deviation from this would be due to numerical diffusion as shown in fig 13c as well as fig 14 a and b only mb dyresm reproduces the unchanged profiles while aem3d generates a definite upward and downward diffusion of both temperature and tracer gradients across the thermocline comparison of this output to a simple 1d diffusion model showed that the net basin average numerical error diffusion coefficient fig 14c reached high levels of the order of a factor of 100 greater than the molecular diffusion o 10 6 m2 s 1 second given that most horizontal exchanges in large deep stratified lakes have relatively large values of the ratio of inertia buoyancy layer thickness to mixing buoyancy layer thickness associated with the exchange intrusion velocity mb dyresm will provide a better estimate of the mass flux path and so is a more appropriate model for coupling with any biogeochemical model third mb dyresm has shown a better balance between computational efficiency and model resolution in long term simulations key features in judging whether or not a hydrodynamic model is suitable for simulations of climatic temporal scales as shown in table 4 based on the model efficiency calculated from the inflow event simulations at lake argyle the computational time for dyresm mb dyresm and aem3d to carry out a 100 year simulation at lake argyle would be 5 2 h 13 h and 3900 h 162 5 days respectively mb dyresm was about 300 times faster than aem3d and at the same time was able to provide sufficient details of the horizontal variability 4 4 limitations of mb dyresm through the numerical experiments above we also recognize a limitation of the current mb dyresm the effect of wind induced horizontal momentum transport woodward et al 2017 is not accounted for in the present version as mb dyresm characterizes sub basins as 1d water columns information including shape and topological relationships of sub basins is not explicitly available however for stratified lakes the modifications introduced by yeates and imberger 2003 do provide the basin lake numbers which may be used to evaluate the internal wave amplitudes from which follows the wind induced horizontal transport and the elevation change of the thermocline which controls the light climate important for phytoplankton growth both the effect of wind induced horizontal momentum transport and the limitation of requiring a large value of the ratio of inertia buoyancy layer thickness to mixing buoyancy layer thickness will form a major part of the future improvements of mb dyresm the latter has implications for lakes that are vertically homogenous 5 conclusions the pseudo 3d hydrodynamic lake model mb dyresm was developed to overcome the deficiencies of existing 1d and 3d models the key features include incorporating sufficient process complexity to produce a good representation of the mass flux path in a lake and at the same time maintain very high computational efficiency and well controlled cumulative numerical diffusion these features were achieved by 1 taking the lagrangian scheme inherited from dyresm as the core module to represent each sub basin and 2 characterizing the mass transport between adjacent sub basins by gravitational exchange thus with the respect to hydrodynamic simulations for lakes that have horizontal exchange flows with large values of the ratio of inertia buoyancy layer thickness to mixing buoyancy layer thickness mb dyresm provides a better option than 1d and 3d models within the established pseudo 3d framework it is natural to consider future improvements of mb dyresm from two perspectives first the model is currently being further upgraded by including shape and topological relationships of the sub basins and incorporating the horizontal momentum transfer induced by wind and other forcings second mb dyresm will be coupled with a biogeochemical model that can be used to assess the long term impact of global warming and nutrient enrichment in lakes author contributions w z carried out all the coding designed and conducted the numerical experiments and wrote the first draft of the manuscript j i initially prompted the idea to build a new model based on dyresm to overcome the absence of a fast model suitable for the assessment of climate change scenarios j i and w z configured the gravitational exchange algorithm j i did a major revision of the first draft and formulated the non dimensional ratio c l m ensured that scientific integrity was maintained interpreted the results and did a major edit of the manuscript all authors approve the content of this manuscript software availability the multi basin dynamic reservoir simulation model mb dyresm was coded in fotran 95 language by w z earth research institute university of california santa barbara 6832 ellison hall santa barbara ca 93101 usa wencaizhou ucsb edu 1805 895 9633 instructions executable codes and an example are freely available at https github com wencaizhou mb dyresm version 1 blob master mb dyresm version 1 zip declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the lake argyle data acquisition was funded by the royalties for regions kimberley regional grants scheme and the australian research council dp130103394 w z was supported by the harold clough scholarship during the initial research work of this manuscript and the nasa grant nnx17ak49g during manuscript preparation neville fowkes is acknowledged for providing critic thoughts and helping with the edition of the manuscript john melack kevin boland and sandya nanayakkara are acknowledged for helpful suggestions on the manuscript the reviewer is also acknowledged for providing valuable comments that triggered the need to formulate a non dimensional ratio that would allow a simple quantitative measure of the validity of the new model this helped to improve and clarify the manuscript considerably for which the authors are grateful 
25785,the multi basin dynamic reservoir simulation model mb dyresm has been developed to bridge the gap between the computationally cheap one dimensional lake models and the computationally expensive three dimensional lake models it was extended from the classic one dimensional lake model dyresm by incorporating a horizontal water exchange algorithm to simulate gravitational mass transport between adjacent sub basins numerical experiments with increasing complexity were carried out for capacity demonstration in which mb dyresm successfully reproduced the thermal structure and mass distribution in each sub basin with accuracy comparable to and computational speed approximately 300 times faster than the three dimensional model applied in parallel in situations where turbulence was suppressed by stratification it actually provided better simulation results the combination of low computational demand reasonable spatial resolution of mass flux path and constrained numerical diffusion makes mb dyresm a more applicable option for simulations where the balance between affordable computational power and proper spatial resolution is concerned keywords pseudo three dimensional lake model stratified lakes flux path gravitational exchange 1 introduction one dimensional 1d lagrangian mass conserving hydrodynamic numerical models have been used to simulate the evolution of thermal structure and mixing regime in lakes on decadal temporal scales danis et al 2004 fenocchi et al 2018 matzinger et al 2007 trolle et al 2012 valerio et al 2015 these models distribute all vertical mass exchanges instantly across the area of the simulated basin and so do not correctly account for horizontal mass transport imberger et al 1978 of state variables other than those associated with the temperature stratification such basin wide averaging models are unsuitable for coupling with ecological models as they do not conserve horizontal heterogeneity of ecological state variables in lakes where horizontal homogeneity is not maintained marti et al 2016 horizontal transport can be accounted for by three dimensional 3d numerical models but such models are not suitable for long term simulations of mass flux path in strongly stratified lakes due to two major limitations first full navier stokes solvers resolve 3d spatial variability with three major types of coordinates griffies et al 2000 chen et al 2007 holleman et al 2013 depth coordinate z models terrain following coordinate σ models and isopycnal coordinate ρ models the z models suffer from excessive vertical mixing in lake interiors where stratification is strong leading to a false mass flux of the biogeochemical state variables that tends to dominate the actual signal in long term simulations griffies et al 2000 laval et al 2003 in σ models water layers are parallel to the boundary and this introduces numerical diffusion processes such as numerical baroclinic pressure gradient and numerical vorticity where the neutral direction is not parallel to the boundary mellor et al 1997 the ρ models provide natural frameworks for representing nutrient and tracer transportation along neutral directions which evidently reduce the numerical diffusion as in z models but ρ models are not suitable for representing surface mixed layer or benthic boundary layer bbl where high resolution of structure is required though water is well mixed second higher order 3d difference schemes have been developed that conserve mass and include more detailed closure schemes that can correctly mimic the effect of the stratification on the turbulence intensity and scale but they are computationally far too expensive to be a realistic option for coupling with biogeochemical models acosta et al 2015 bruggeman and bolding 2014 leon et al 2007 mooij et al 2010 rajar et al 1997 swayne et al 2005 and applied to large lakes over long times the approach adopted to address limitations of 1d models and 3d models is to divide a lake into a number of sub basins and incorporate an exchange algorithm to the existing 1d lagrangian parametric hydrodynamic model dynamic reservoir simulation model dyresm imberger et al 1978 yeates and imberger 2003 that simulates the horizontal exchange across a partition between two sub basins here we present the case where this exchange is driven by gravitational forcing as gravitational flows induced by differential deepening okely and imberger 2007 differential absorption macintyre and melack 1995 and differential cooling heating imberger and parker 1985 macintyre et al 2002 verburg et al 2011 woodward et al 2017 are commonly observed in lakes with variable bathymetry and or complex topography this approach is sufficient to establish a pseudo 3d framework that has the advantage that vertical mass exchange is conserved the model retains the computational advantages of dyresm and the horizontal flux path may be correctly modelled by the choice of the shape and number of sub basins the new pseudo 3d hydrodynamic model presented here multi basin dyresm mb dyresm represents a natural extension of the 1d dyresm a commonly used lake simulation model trolle et al 2012 in the following sections we first present a brief summary of the lagrangian scheme of dyresm followed by a description of the horizontal exchange algorithm between sub basins the new model is then evaluated by comparison with field data and results from full 3d numerical simulations derived with the aquatic ecosystem model 3d aem3d hodges et al 2000 hodges and dallimore 2019 2 methods 2 1 the predecessor 1d lagrangian model dyresm dyresm was originally developed in 1978 imberger et al 1978 for the simulation of changes in the vertical thermal structure in small or medium sized stratified lakes and reservoirs it was later upgraded by yeates and imberger 2003 to incorporate parameterization of internal mixing and boundary fluxes imberger and ivey 1993 marti and imberger 2006 generally the development of dyresm focused on strongly stratified lakes allowing the use of a lagrangian computational layer structure fig 1 a with an effective explicit vertical eddy diffusion coefficient provided for the basin average vertical mass flux this coefficient was considered as a calibration coefficient determined by comparing the model output with data from laboratory experiments or inferred from field observations with such a model structure only few formulas needed numerical solutions to update the layer structure at each time step thus dyresm runs extremely fast and its accuracy is excellent with respect to water temperature vertical numerical diffusion resulted from the amalgamation of neighboring layers when they became thinner than the user specified minimum thickness thus the model accuracy as influenced by numerical diffusion could be constrained by the user however because such a model does not account for any horizontal variations the effective implied horizontal numerical diffusion coefficient relevant for all biogeochemical state variables when the model was coupled with a water quality model was very large o l 2 δ t where l is a measure of the size of the plain area of the lake imberger and patterson 1981 this is a particular problem when it comes to the ecological state variables such as nitrogen or phosphorus which generally contribute only negligibly to the density field and so horizontal gradients of these variables remain unchanged until influenced by the ecology itself or by advection driven by forces not provided for in a 1d model for a model to adequately simulate those state variables that do not contribute significantly to the density of the water column it must account for horizontal transport that is equal to or slower than the characteristic ecological growth rate scale 2 2 development of mb dyresm 2 2 1 framework of mb dyresm operationally mb dyresm divides the lake of interest into sub basins with the number and shape being determined by the desired resolution fig 1b the basin topography and the expected hydrodynamic features it is convenient to think of two types of sub basins the central one sub basin 3 in fig 1b and the side ones s sub basins 1 2 4 5 in fig 1b dyresm is run for each sub basin and at each time step fluxes across each interface at each layer depth are then calculated using the parametric intrusion model described below the interfacial fluxes are then injected into the sub basin lagrangian layer structures these steps are then repeated for each time step mb dyresm may be thought of as consisting of two sets of functional computational routines fig 2 the lagrangian dyresm module simulating the vertical hydrodynamic processes in each sub basin at each time step and the eulerian exchange module simulating the horizontal gravitational water exchange between any two adjacent sub basins the dyresm module is the same as the original dyresm model yeates and imberger 2003 except that the daily loop has been removed to allow more flexibility for coupling with the exchange module this was motivated by the possibilities that a large daily inflow volume could overfill a sub basin and that a large daily discharge outflow volume could completely drain a sub basin 2 2 2 intrusion algorithm for the horizontal gravitational exchange in the current version of mb dyresm horizontal density gradients across any of the interfaces result from dyresm runs in adjacent sub basins yielding different vertical density structures the horizontal density gradients so formed across the separation curtains are assumed to drive a gravitational mass exchange across the interfaces at each time step the water exchanged due to horizontal density gradients is modelled as a lock exchange process cheong et al 2006 flynn and linden 2006 holyer and huppert 1980 maurer 2011 as shown in fig 3 in which the intrusive exchange gravity currents are assumed to be governed by an inertia buoyancy balance and no mixing the intrusion velocity u i can be calculated following cheong et al 2006 1 u i α ρ l ρ u ρ 0 g h h h 2 2 h h ρ i ρ u ρ l ρ u ρ i ρ u ρ l ρ u where α is a constant equal to 0 5 g 9 81 m s 2 is the gravitational acceleration ρ 0 1000 kg m 3 is the reference density ρ i is the density of the intruding layer ρ u and ρ l are respectively the densities of the upper and lower ambient layers h is the total fluid height and h is the interface height the parameters h e the equilibrium interface height and d the displacement of the upper lower layer interface from h e shown in fig 3b were not considered in the calculation of u i in mb dyresm as flynn and linden 2006 pointed out that the energy lost to the displacement of the interface made negligible difference to u i these two parameters are presented in fig 3b because the displacement of the interface was observed in the lock exchange experiments and also captured by the aem3d simulation as will be shown in section 3 1 given that the thickness of each layer may change during the operation of the dyresm module it is common that layer depths in the two adjacent sub basins do not stay aligned vertically making the application of the above equation to all the layers infeasible to solve this issue any layer across the layer depth at the other side of the interface marked as i to iv in fig 1b is split at that depth so that layers in one sub basin are horizontally aligned with the layers in the adjacent sub basin as a result intrusion conducted by the exchange module at each of the new layer depths is simplified as intrusion between two layers with identical thickness in turn with the parameter h set to zero eq 1 reduced to 2 u i α ρ 1 ρ 2 ρ 0 g h where ρ 1 and ρ 2 are the densities of the layers in the two adjacent sub basins at the same depth along with the intruding flow a return flow is conducted above it if the density of the return flow is higher or below it if the density of the return flow is lower the velocity of the return flow also computed with eq 2 is the same as the velocity of the intruding flow in magnitude as a result of identical layer thickness and horizontal density gradient magnitude volumes of the intruding flow and return flow may then be calculated through multiplying u i by the cross sectional area of the layers step initialization of the exchange module in fig 2 with the thicknesses of the intruding flow and the return flow taken as half the thickness of the layers which leads to the same volume of water to be exchanged between adjacent sub basins this volume is then inserted into the adjacent sub basin as a new layer at the depth of neutral buoyancy with no mixing steps water exchange with no mixing and gravitational adjustment of the exchange module in fig 2 followed by the adjustments of depth and thickness of the layers remained but with reduced volume and the layers newly inserted according to the volume of these layers step morphometric adjustment of the exchange module in fig 2 finally at the end of the gravitational water exchange the density of all layers is adjusted by applying a density filter to the water column to account for the mixing induced by the intrusion step mixing due to gravitational intrusion of the exchange module in fig 2 2 2 3 mixing induced by the intrusion the water exchange across the interface between adjacent sub basins is first carried out assuming no mixing as detailed above the associated mixing of the intruded water with water in layers immediately above and below the intrusion level is simulated with an effective vertical diffusion coefficient using the density filter adopted from laval et al 2003 the changes brought about by diffusion are modelled by 3 s k 1 e δ z k λ s k e δ z k λ s k 1 where s k and s k 1 are the temperature or salinity of layers k and k 1 after applying the filter s k is the temperature or salinity of layer k before applying the filter δ z k is the layer thickness of layer k and λ is the smoothing length scale in meters the filter defined by eq 3 smooths the density profile directly if applied to the density profile of the water column or indirectly if applied to the temperature or salinity profile step mixing due to gravitational intrusion of the exchange module in fig 2 reducing the density gradient is equivalent to converting the available potential energy to kinetic energy for mixing which coincides with the mechanism behind the gravitational intrusion the smoothing effect the generation of intrusion induced mixing is more significant when the vertical density gradient is stronger and increases with λ laval et al 2003 stronger vertical density gradients normally result from greater horizontal density difference ρ 1 ρ 2 in eq 2 and thereby intrusion velocity between the adjacent sub basins is stronger hence the filter acts preferentially at depths where gravitational intrusion is relatively intensive 2 3 numerical experiments the performance of mb dyresm was evaluated by comparing simulation results with those from aem3d on domains of increasingly complex topography and external forcing as seen in fig 4 these numerical experiments were of three types experiment 1 two rectangular domains divided into two sub basins of identical bathymetry i e uniform depth firstly aem3d simulations were carried out on the smaller domain fig 4a1 for validation against experimental data for the lock exchange flow cheong et al 2006 flynn and linden 2006 this was performed for completeness because the lock exchange algorithm is fundamental in characterizing the gravitational water exchange in mb dyresm then both mb dyresm and aem3d simulations were carried out on the larger domain fig 4a2 to validate the water exchange process described above to launch the water exchange in the larger domain one sub basin was initialized with a fully mixed water column and the other with a stratified water column the sharp horizontal density gradient drove a relatively intensive gravitational intrusion similar to the lock exchange experiments as shown in fig 3 experiment 2 a rectangular domain divided into two sub basins one shallow and one deep fig 4b unlike the previous numerical experiment the temperature and salinity of the entire domain was initialized to be homogeneous everywhere and the gravitational water exchange was established gradually by differential absorption as one sub basin was configured with higher turbidity gravitational water exchange is much milder than in the previous case which represents a more realistic situation commonly found in the natural environment experiment 3 lake argyle a large artificial tropical lake in australia fig 4c simulations mimicking an inflow were carried out to determine whether or not mb dyresm can capture the path of a river borne tracer in the lake 3 results 3 1 experiment 1 two sub basin rectangular domain with uniform depth 3 1 1 validation of aem3d for lock exchange flow experimental data the series of three lock exchange laboratory experiments a b and c performed by cheong et al 2006 and flynn and linden 2006 contained sufficient parameter variations in interface depth ratios h h 0 0 0 5 1 0 and density differences to provide a good validation table 1 for the aem3d simulation the rectangular domain 1 82 m 0 23 m 0 3 m fig 4a1 was gridded 0 01 m 0 023 m 0 002 m table 2 compares the dimensionless intruding velocities u i ˆ u i g l u h measured in the experiments of cheong et al 2006 with those obtained with application of aem3d with the same configurations the aem3d simulated u i ˆ agreed reasonably well with the experimental results especially in the low variability at h h 0 5 and the general variation pattern with respect to h h cheong et al 2006 also carried out extra experiments with two ambient layers of equal depths h h 0 5 and confirmed that the intruding velocity varied weakly with different combinations of initial intruding density and ambient density snapshots of the aem3d simulated density fields in all three series with h h 0 5 fig 5 show that the aem3d successfully reproduced the intrusion generated long wave indicated by the vertical displacement of the interface between the upper and lower layers in the non equilibrium cases ρ i ρ e series a and c while simulating no intrusion generated long wave in the equilibrium case ρ i ρ e series b therefore it is appropriate to verify the water exchange algorithm in mb dyresm with aem3d simulations 3 1 2 validation of the water exchange process algorithm to demonstrate the efficiency of mb dyresm water exchange algorithm numerical experiments were first carried out with mb dyresm and aem3d simulations over a rectangular domain 10 000 m in length 1000 m in width and 60 m in depth which was correspondingly gridded 50 m 100 m 0 4 m in aem3d in mb dyresm the domain was divided into two sub basins of identical bathymetry and identical initial water levels fig 4a2 initially sub basin 1 was well mixed while sub basin 2 was thermally stratified fig 6 therefore water in sub basin 1 should intrude between the two layers of sub basin 2 with a returning current of the upper layer water of sub basin 2 at the surface and another returning current of the lower layer water of sub basin 2 at the bottom eventually forming a three layer structure in both sub basins dyresm simulations were also carried out as a reference situation of instantaneous water exchange between the two sub basins table 3 shows the four simulation cases carried out with dyresm mb dyresm and aem3d in all cases the horizontal surface water temperature variation was forced to generate gravitational flows between the two sub basins in two ways surface heat fluxes activated or not and different turbidities in the two sub basins with such configurations the efficiency of mb dyresm in capturing the structure of surface layer under water exchange could be tested note that surface fluxes of radiation heat and momentum were activated deactivated by switching on off meteorological forcing in the simulations for simulations with surface fluxes activated the meteorological data were adopted from the lds measured data at lake argyle between 2nd and 4th august 2012 with mean values of incoming shortwave radiation net longwave radiation air temperature and relative humidity being 232 w m 2 70 w m 2 35 c initial surface water temperature 25 c and 43 respectively other meteorological inputs including wind speed and rainfall were all set to zero as a result the surface water was constantly heated up when surface fluxes were activated moreover given such a short simulation the heating effect of surface fluxes was well contained in the top 1 m of the water column since there is no explicit water exchange between the two sub basins in the dyresm simulations the simulated density profiles deviated from the initial profiles only at the surface when surface fluxes were activated cases 1 2 and 4 in fig 6 comparing the density profiles of mb dyresm to those of aem3d mb dyresm successfully captured the three layer structure resulting from the gravitational exchange in each sub basin with the thermoclines at about 10 m and 30 m well matched in both depth and thickness with those simulated by aem3d the density gradients across thermoclines simulated by mb dyresm were slightly sharper than those by aem3d indicating less mixing induced by the intrusive current characterized by mb dyresm this is acceptable given that as will be shown in section 4 3 the mixing in aem3d tended to be overestimated the results obtained for surface densities using mb dyresm matched those obtained using aem3d in all cases 3 2 experiment 2 two sub basin rectangular domain with non uniform depth mb dyresm model was evaluated for the case of differential absorption macintyre and melack 1995 in a two sub basin rectangular domain with non uniform depth fig 4b the domain consists of two adjacent sub basins of different depths 10 m in sub basin 1 and 40 m in sub basin 2 with no inflow or outflow fig 7 the entire domain was initialized with uniform water temperature 25 c and salinity 0 1 psu a passive tracer with concentration of 1 0 inserted into the entire sub basin 1 and a η p a r higher in sub basin 1 2 5 m 1 than sub basin 2 1 0 m 1 both mb dyresm and aem3d were forced with meteorological data averaged from the lds measured data at lake argyle between 18th and 29th september 2012 that kept both sub basins being constantly heated throughout the simulation fig 7 which included constant incoming shortwave radiation of 240 w m 2 and constant net outgoing longwave radiation of 70 w m 2 constant air temperature of 35 c 10 c warmer than the initial water temperature constant relative humidity of 48 and zero wind speed to eliminate the disturbance of horizontal momentum input discussed in section 4 4 the result of this configuration was a horizontal gravitational water exchange as described by imberger 1974 because more heat was trapped in the surface water of sub basin 1 than that of sub basin 2 and in turn set up the horizontal water temperature gradient between the sub basins the resulted horizontal water exchange was confirmed by the tracer and water temperature profiles simulated by mb dyresm and aem3d fig 8 tracer profiles simulated by both mb dyresm and aem3d have clearly shown the pattern of the gravitational water exchange between sub basins 1 and 2 tracer occurrence at 2 3 m and 6 10 m depth in sub basin 2 fig 8a2 and b2 implies that there was a surface flow and an intruding flow at depth from sub basin 1 to sub basin 2 in the opposite direction the dilution of tracer at 3 4 m depth in sub basin 1 implies that there was a return flow from sub basin 2 to sub basin 1 such a pattern of gravitational water exchange was driven by the horizontal water temperature gradient as being configured which can be confirmed by the simulated water temperature profiles at the beginning of the simulations surface water temperature in sub basin 1 started to increase notably on day 1 5 half a day earlier than that in sub basin 2 fig 8c1 2 and d1 2 more exactly the simulated surface water temperature difference between the two sub basins increased from 0 to around 1 5 c during the first 3 days and then maintained at this level until the end of simulation fig 9 a and from then it constantly needed about 1 extra day for the surface water temperature in sub basin 2 to reach the same level as in sub basin 1 fig 9b due to higher turbidity in sub basin 1 than in sub basin 2 water below the thermocline in sub basin 1 received less shortwave radiation than water at the same depth in sub basin 2 which can be confirmed by the fact that both models simulated a slightly shallower 26 c isotherm in sub basin 1 than sub basin 2 fig 8c1 c2 d1 and d2 as a result water below the thermocline in sub basin 1 was slightly cooler than that in sub basin 2 and the so formed horizontal temperature gradients drove intrusion flows and transferred tracer from sub basin 1 to sub basin 2 at about 6 10 m in depth fig 8a2 and b2 in general the features of tracer and temperature profiles and the revealed water exchange pattern simulated by mb dyresm have been well confirmed by the aem3d simulation which implies that mb dyresm was able to capture the intensity as well as the timing of the differential heating resulting from the configured differential absorption 3 3 experiment 3 application to a prototype case lake argyle australia lake argyle is a large artificial lake that was formed by the ord river dam in the 1970s fig 4c it is located in the kimberley region a tropical area in the northwest of australia where the climate is characterized by a semi arid to arid monsoon cycle made up of a dry season from april to october and a very wet season from november to march lake argyle has large expanses of shallow littoral zones which give rise to very strong gravitational exchange currents in the dry season as shown by woodward et al 2017 in the wet season rainfall is highly concentrated leading to impulsive inflows that set up correspondingly strong gravitational intrusions propagating all the way to the dam wall marti and imberger 2015 to achieve a quantification of the simulation accuracy of mb dyresm in a prototype setting simulation of an inflow event from 20th november to 4th december 2013 was carried out to ascertain the accuracy of the exchange algorithm as applied to a gravitational inflow intrusion as shown in fig 4c for the mb dyresm application lake argyle was divided into 3 sub basins sub basin 1 the shallow embayment receiving the ord river inflow sub basin 2 the central part of lake argyle connecting the inflow to the dam wall sub basin 3 peripheral sub basin not directly connected with the path of the inflow the simulations were forced with data fig 10 including meteorological forcing measured by the lds station located in sub basin 2 see fig 4c except that the rainfall data were obtained from the australian government bureau of meteorology http www bom gov au climate data ref ftr for the three stations rosewood lake argyle resort and argyle aerodrome averaged with inverse distance weighting inflow and outflow discharge rates were provided by department of water government of western australia australia and water temperature of the inflow was estimated as the averaged air temperature 4 days before a detailed description of the configuration of the lds and aem3d used here is presented in woodward et al 2017 the aem3d performance over the simulation period was assessed using the root mean squared error rmse between the measured and simulated water temperature profiles at the lds the rmse was 0 96 c similar to those reported in other applications of aem3d including lake argyle dissanayake et al 2019 tranmer et al 2018 woodward et al 2017 zamani and koch 2020 this provides confidence that aem3d simulations could be used for comparison with mb dyresm simulations a simulation with dyresm was also carried out on the whole lake i e single basin serving as a minimum reference for all simulations a passive tracer of constant concentration equal to 1 0 was continuously released into the ord river flow the simulated water temperature and tracer profiles are summarized in fig 11 among the water temperature profiles the appearance of inflow water can be observed only in the mb dyresm result fig 11 c1 29 c water accumulating below 10 m and above the 26 c bottom water in sub basin 1 at the end of day 6 it could not be observed in the aem3d result fig 11 d1 due to its relatively small volume in one simulation time step as compared to the capacity of the sub basin 105 m3 vs 109 m3 neither could it be observed in other sub basins in any simulation results or at the lds site fig 11a because the inflow water had already diffused in sub basin 1 note that the mb dyresm results also showed that water in the shallower sub basin 1 heated more than that in deeper sub basin 2 especially after day 8 fig 11c1 and c2 such differential heating was clearly confirmed by the aem3d results fig 11d1 and d2 which highlights the necessity of horizontal resolution for reproducing the correct flux path of mass at the same time the mb dyresm simulated tracer profiles were well matched by the aem3d simulated ones showing more straightforward appearance of inflow water in sub basin 1 and more evident horizontal and vertical variability among the three sub basins fig 11f and g for example both mb dyresm and aem3d simulated tracer arrived near the 10 m depth in sub basin 1 at the end of day 5 nearly 36 h after the start of the inflow event see fig 10f with concentration values around 0 2 20 of tracer input concentration and maintained a distribution between 10 m and 20 m until the end of simulation fig 11f1 and g1 also in both models the tracer reached sub basin 3 but the concentration values were negligibly small 10 9 so that a different tracer scale had to be used as shown in fig 11f3 and g3 the main discrepancy between mb dyresm and aem3d was that tracer reached sub basins 2 and 3 several days earlier in mb dyresm than in aem3d in sub basin 2 mb dyresm simulated tracer concentration values less than 10 3 can be seen from days 2 6 before the arrival of the clearly defined inflow intrusion on day 6 with a maximum concentration value of 10 1 10 of tracer input concentration fig 11f2 while aem3d simulated tracer concentration values less than 10 3 can be seen from days 8 9 before the arrival of the inflow intrusion at the end of day 9 with a maximum concentration value of 10 1 as well fig 11g2 in both models the intrusion remained until the end of the simulation and centred at the depth of 15 m aem3d results showed the tracer vertically mixed into the lake surface layer on day 8 fig 11g2 in sub basin 3 the simulated tracer arrived on days 5 and 12 in mb dyresm and aem3d respectively with concentration values less than 10 9 fig 11f3 and g3 part of this discrepancy can be explained by the algorithm used for the intrusive exchange gravity currents these are assumed to be governed by an inertia buoyancy balance and no mixing see section 2 2 this choice was motivated by the observation that most of the horizontal exchange takes place at depths of strong stratification that collapses the turbulence imberger et al 1976 this assumption is valid when the ratio of the inertia buoyancy layer thickness δ i q n b 1 2 to that of the mixing buoyancy layer δ k ε n 3 1 2 is larger than 1 i e inertia buoyancy balance dominates q being the discharge n the buoyancy frequency b the width and ε the dissipation rate of turbulent kinetic energy taking the values from lake argyle inflow intrusion q 350 m3 s 1 n 0 03 s 1 b 5000 m and ε 10 6 m2 s 3 and substituting these into the ratio δ i δ k q n 2 b ε 1 2 leads to δ i 1 53 m δ k 0 19 m and δ i δ k 8 05 1 this indicates that mb dyresm predicts a faster intrusion than aem3d as it can be seen in fig 11f2 g2 f3 and g3 the dyresm simulated tracer profile fig 11e did not match with the aem3d results in any sub basin again clearly highlighting the need for the accounting of horizontal resolution to obtain any flux path prediction overall mb dyresm satisfactorily demonstrated the ability to reproduce the mass flux characteristics of the inflow event at lake argyle with relatively low horizontal resolution with a huge saving in computational time over aem3d results show that mb drysem should be the model of choice when the horizontal exchange takes place at relatively large values of the ratio of inertia buoyancy layer thickness to mixing buoyancy layer thickness 4 discussion 4 1 lagrangian mass flux path to compare the tracer flux across each interface simulated by mb dyresm and aem3d tracer flux rates between sub basins at the two sub basin rectangular domain with non uniform depth experiment 2 and lake argyle experiment 3 were calculated the tracer flux from sub basin i to sub basin j f i j arrows in fig 4b and c m3 s 1 was defined as 4 f i j 0 s z b x h v x z c x z d z d x where s is the length of the interface between sub basins m z is the layer height m z b x and h are the bottom height m and surface height m of water column at accumulative distance x m along the sub basin interface respectively v x z is intrusion velocity at distance x and height z positive from sub basin i to sub basin j m s 1 and c x z is the tracer concentration fraction of 1 0 at distance x and height z as shown in fig 12 tracer fluxes simulated by mb dyresm generally reached steady state faster than those by aem3d owing to the 1d representation of each sub basin by mb dyresm and to the fact that the value of the ratio of inertia buoyancy layer thickness to mixing buoyancy layer thickness is larger than 1 however such discrepancy has not led to unacceptable deviations between mb dyresm and aem3d tracer concentration profiles for the differential absorption experiment 2 fig 8 and the inflow event at lake argyle experiment 3 fig 11 this is because mb dyresm was able to maintain a balanced water exchange between sub basins equivalent to but more dynamic than aem3d note that at lake argyle the mb dyresm simulated tracer fluxes f 1 2 and f 2 1 reached a relative steady state quickly after the inflow event started on day 5 while it took another 4 days for the aem3d simulated f 1 2 and f 2 1 to arrive at similar levels fig 12a2 and 12b2 such discrepancy was not observed in experiment 2 fig 12a1 and b1 and we believe this was associated with the more complicated external forcing fig 10 and irregular shape of lake argyle fig 4c as compared to the rectangular domain figs 4b and 7 and also the ratio of inertia buoyancy layer thickness to mixing buoyancy layer thickness being larger than 1 after the aem3d simulated tracer fluxes achieved a relative steady state they were matched well by the mb dyresm simulations a more evident discrepancy was observed at the interface between sub basins 2 and 3 during the inflow event at lake argyle fig 12c2 and d2 but greater disturbance in simulated f 2 3 and f 3 2 would be expected as they were so weak and intermittent compared to f 1 2 and f 2 1 the resultant tracer concentration in sub basins 3 being negligibly small fig 11f3 and g3 4 2 advantage of mb dyresm over dyresm the most significant advantage of mb dyresm over its predecessor dyresm is the introduction of horizontal spatial variability such an improvement firstly brings more accuracy in the simulation of lake hydrodynamics for example mb dyresm successfully captured the intrusion of an inflow event at lake argyle fig 11 from an ecological perspective the introduction of horizontal variability is a major advance especially when considering long term changes species nurtured in one part of a lake spread to other parts of the lake and although such issues are not addressed directly here mb dyresm is capable of reproducing such exchanges in dyresm i e single basin as shown in fig 11e tracer that entered the basin carried by the inflow was immediately transported into the entire lake which was not indicated by the aem3d simulation results but mb dyresm successfully captured the spatial and temporal patterns of the tracer distribution with primary accumulation in sub basin 1 and negligibly small accumulation in sub basin 3 however it should be recognized that the success of any parametric model depends on the relevance of the underlying model in context and the relative importance of mechanisms not included in the model in the case of a climate change model the underlying behaviour may change over time so that adjustments in the parameters of the model may be needed and even a change of model may be indicated this suggests that a continual checking and updating of parameters may be required perhaps over periods of years and that under over predictions should be made 4 3 advantages of mb dyresm over models like aem3d there are three major advantages of mb dyresm over models like aem3d first the numerical diffusion in mb dyresm is the same as that in the lagrangian dyresm and is totally controlled by the amalgamation of thin layers in contrast the numerical diffusion in aem3d results from the discretization of the navier stokes equations as described by laval et al 2003 the control of the numerical diffusion is an important feature because in a strongly stratified lake the actual vertical mixing is close to molecular imberger and marti 2014 to illustrate the superior numerical mass conservation characteristics of mb dyresm compared to aem3d both models were applied to the simulation of small amplitude seiching in a single basin bowl shaped domain fig 13 a a periodic western wind along the y axis of 5 m s 1 was activated for 5 h much shorter than the 30 h fundamental internal wave period every 30 days to excite periodic seiching the water column stratification consisted of a thermocline of mean temperature gradient 0 8 c m 1 between the depths of 12 m and 16 m fig 13 b a passive tracer of concentration of 1 0 was introduced below the thermocline fig 13 c and its upward flux with time was used as a measure of the numerical mixing error in each model heat fluxes and all types of physical mixing were turned off in both mb dyresm and aem3d by setting relevant coefficients to zero and using free slip boundary conditions with such configurations the maximum gradient richardson number simulated by aem3d in the central water column reached minimum values between 102 and 104 at depths around 20 m indicating that the seiching in the bowel shaped lake was laminar yeates et al 2013 as the richardson number was always greater than 0 25 there should be no diffusion of temperature gradient or tracer gradient and the mean temperature and tracer profiles should be maintained at the initial values in both simulations any deviation from this would be due to numerical diffusion as shown in fig 13c as well as fig 14 a and b only mb dyresm reproduces the unchanged profiles while aem3d generates a definite upward and downward diffusion of both temperature and tracer gradients across the thermocline comparison of this output to a simple 1d diffusion model showed that the net basin average numerical error diffusion coefficient fig 14c reached high levels of the order of a factor of 100 greater than the molecular diffusion o 10 6 m2 s 1 second given that most horizontal exchanges in large deep stratified lakes have relatively large values of the ratio of inertia buoyancy layer thickness to mixing buoyancy layer thickness associated with the exchange intrusion velocity mb dyresm will provide a better estimate of the mass flux path and so is a more appropriate model for coupling with any biogeochemical model third mb dyresm has shown a better balance between computational efficiency and model resolution in long term simulations key features in judging whether or not a hydrodynamic model is suitable for simulations of climatic temporal scales as shown in table 4 based on the model efficiency calculated from the inflow event simulations at lake argyle the computational time for dyresm mb dyresm and aem3d to carry out a 100 year simulation at lake argyle would be 5 2 h 13 h and 3900 h 162 5 days respectively mb dyresm was about 300 times faster than aem3d and at the same time was able to provide sufficient details of the horizontal variability 4 4 limitations of mb dyresm through the numerical experiments above we also recognize a limitation of the current mb dyresm the effect of wind induced horizontal momentum transport woodward et al 2017 is not accounted for in the present version as mb dyresm characterizes sub basins as 1d water columns information including shape and topological relationships of sub basins is not explicitly available however for stratified lakes the modifications introduced by yeates and imberger 2003 do provide the basin lake numbers which may be used to evaluate the internal wave amplitudes from which follows the wind induced horizontal transport and the elevation change of the thermocline which controls the light climate important for phytoplankton growth both the effect of wind induced horizontal momentum transport and the limitation of requiring a large value of the ratio of inertia buoyancy layer thickness to mixing buoyancy layer thickness will form a major part of the future improvements of mb dyresm the latter has implications for lakes that are vertically homogenous 5 conclusions the pseudo 3d hydrodynamic lake model mb dyresm was developed to overcome the deficiencies of existing 1d and 3d models the key features include incorporating sufficient process complexity to produce a good representation of the mass flux path in a lake and at the same time maintain very high computational efficiency and well controlled cumulative numerical diffusion these features were achieved by 1 taking the lagrangian scheme inherited from dyresm as the core module to represent each sub basin and 2 characterizing the mass transport between adjacent sub basins by gravitational exchange thus with the respect to hydrodynamic simulations for lakes that have horizontal exchange flows with large values of the ratio of inertia buoyancy layer thickness to mixing buoyancy layer thickness mb dyresm provides a better option than 1d and 3d models within the established pseudo 3d framework it is natural to consider future improvements of mb dyresm from two perspectives first the model is currently being further upgraded by including shape and topological relationships of the sub basins and incorporating the horizontal momentum transfer induced by wind and other forcings second mb dyresm will be coupled with a biogeochemical model that can be used to assess the long term impact of global warming and nutrient enrichment in lakes author contributions w z carried out all the coding designed and conducted the numerical experiments and wrote the first draft of the manuscript j i initially prompted the idea to build a new model based on dyresm to overcome the absence of a fast model suitable for the assessment of climate change scenarios j i and w z configured the gravitational exchange algorithm j i did a major revision of the first draft and formulated the non dimensional ratio c l m ensured that scientific integrity was maintained interpreted the results and did a major edit of the manuscript all authors approve the content of this manuscript software availability the multi basin dynamic reservoir simulation model mb dyresm was coded in fotran 95 language by w z earth research institute university of california santa barbara 6832 ellison hall santa barbara ca 93101 usa wencaizhou ucsb edu 1805 895 9633 instructions executable codes and an example are freely available at https github com wencaizhou mb dyresm version 1 blob master mb dyresm version 1 zip declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the lake argyle data acquisition was funded by the royalties for regions kimberley regional grants scheme and the australian research council dp130103394 w z was supported by the harold clough scholarship during the initial research work of this manuscript and the nasa grant nnx17ak49g during manuscript preparation neville fowkes is acknowledged for providing critic thoughts and helping with the edition of the manuscript john melack kevin boland and sandya nanayakkara are acknowledged for helpful suggestions on the manuscript the reviewer is also acknowledged for providing valuable comments that triggered the need to formulate a non dimensional ratio that would allow a simple quantitative measure of the validity of the new model this helped to improve and clarify the manuscript considerably for which the authors are grateful 
25786,environmental models involve inherent uncertainties the understanding of which is required for use by practitioners one method of uncertainty quantification is global sensitivity analysis gsa which has been extensively used in environmental modeling the suitability of gsa methods depends on the model implementation and computational complexity thus we present a comparative analysis of different gsa methods morris sobol fast and pawn applied to empirical fire spread models dry eucalypt and rothermel and explain their implications gsa methods such as pawn may not be able to explain all the interactions whereas methods such as sobol can result in high computational costs for models with several parameters we found that the morris or the pawn method should be prioritized over the sobol and the fast methods for a balanced trade off between convergence and robustness under computational constraints additionally the sobol method should be chosen for more detailed sensitivity information keywords forest fire fire behavior modeling uncertainty quantification global sensitivity analysis wildfires 1 introduction environmental models provide a mathematical description of natural phenomena in terms of the relationships dynamic or static between their driving factors recent advancements in observational science and related technologies such as computing and engineering have allowed natural phenomena to be studied in greater detail and consequently environmental models are becoming more complex razavi et al 2012 kaizer et al 2015 kc et al 2019 such models require parameters to simulate physical processes for example fire models predict the fire spread rate at a particular point using several parameters that include land topography fuel load meteorological data fire history and so on in such models all the parameters may not significantly affect the model output moreover some of the parameters have to be estimated indirectly using different calibration techniques as they cannot be measured directly vrugt et al 2002 each input parameter has associated uncertainties and consequently can affect the model outcome to a widely varying degree deriving accurate metrics from these models by quantifying the associated uncertainties can require a significantly high model runs under a wide range of possible scenarios kc et al 2020 as a result parameter estimation model calibration and uncertainty quantification have become a major challenge in environmental models used for operational disaster management sensitivity analysis sa is the study of the uncertainties in model output caused by the variation in the model inputs in local sa the impact of the parameters is studied around a local point saltelli et al 2008 in global sensitivity analysis gsa the entire range of the input parameters is taken into consideration while analyzing the model outputs gsa methods are one of the most significant quantitative techniques in risk modeling and analysis baker et al 1999 delen et al 2017 alyami et al 2019 koks et al 2019 kc et al 2021 gsa of natural hazard models can help identify the factor s or scenarios that pose a significant risk in an event of the outbreak of the disaster such identification can prioritize strategic plans for effective risk management christopher frey and patil 2002 for example gsa of fire spread models can help the authorities identify adverse weather scenarios or conditions that may contribute to dangerous wildfires gsa has recently gained attention in environmental modeling in areas such as wildfire hydrology decomposition and crops kc et al 2020a pianosi and wagener 2015 qin et al 2013 lamboni et al 2009 gsa helps to identify influential and non influential factors in the model fix the non influential factors to a known value and treat uncertainties that contribute to better understanding and interpretation of the model nossent et al 2011 saltelli et al 2008 there are several different gsa methods in the literature in the morris method morris 1991 the influence of a parameter is estimated by assessing the variation in the model output caused by varying values of the parameter within its entire range when other parameters are kept constant several works cai et al 2019 brohus et al 2007 menberg et al 2016 wang et al 2019 have conducted sensitivity analyses of different environmental models using the morris method as it provides a good trade off between the efficiency and accuracy for compute intensive model jiang et al 2017 the morris method cannot explain the pair wise interactions between the input factors for models with non linear input output relationships and cannot be used for non orthogonal input factors i e any correlated factors as the correlation cannot be induced ekstrom 2005 nossent et al 2011 the sobol method sobol 1990 and the fast method as proposed in schaibly and shuler 1973 are two widely used variance based sa methods in environmental models saltelli et al 2008 2019 nossent et al 2011 cai et al 2019 in the variance based approach the sensitivity of an uncertain input factor is estimated by investigating the factor s contribution to the model output variance based methods give a good measure of the contribution made by the input factor and its interaction with other factors for robust results these methods require many runs of the model which can be computationally costly if the number of model runs is significantly high baroni and tarantola 2014 furthermore variance is not a sensible measure of model output uncertainty when the model has multi modal or highly skewed output distribution borgonovo 2007 the pawn method pianosi and wagener 2015 is a density based method that estimates the sensitivity indices based on the density function of the model output applications of density based approach include hydmod model pianosi and wagener 2015 fire spread modeling hilton et al 2017 probabilistic risk assessment model borgonovo 2007 and engineering design system liu et al 2005 the density based approach can overcome the limitations of other approaches but its adaptation has been fairly limited as it is difficult to implement as one requires the knowledge of the conditional pdfs of the input factors pianosi and wagener 2015 the existing literature details several gsa methods and instances where they have been applied to fire spread models an uncertainty analysis study was carried out on wildfire models of boreal forests using morris monte carlo and first order analysis methods in the work cai et al 2019 the spitfire fire model thonicke et al 2010 was studied in a similar study where the morris and the sobol methods were used by gomez dans 2018 to study different forests boreal savanna temperate and tropical a global sensitivity analysis of the rothermel model over the mediterranean region was conducted by salvador et al 2001 which established low heat content particle density and mineral content as the parameters with negligible influence on fire spread rate moreover liu et al 2015b used variance based methods to reduce the number of parameters in the rothermel model chaparral fuel model and used quasi monte carlo methods for parametric uncertainties quantification in the reduced model the choice of the approach used in these studies was dependent on the compromise between accuracy computational cost and objectives there are a few studies zadeh et al 2017 hamby 1994 saltelli et al 1999 that have presented comparative analyses of different gsa methods in this paper we expand the scope of such analyses to the wildfire domain by presenting a comprehensive comparative analysis of various gsa methods applied to fire spread models given the inherent dangers wildfires pose to lives and infrastructure determining the intrinsic uncertainties of wildfire models is crucial for their use in operational wildfire management as such we draw a clear picture of four different gsa methods morris sobol fast and pawn applied to two different fire models the dry eucalypt model cheney et al 2012 used mainly in australia and rothermel 1972 used widely in north america the choice of fire spread models has been made based on the number of the model parameters to draw a clearer picture of the comparative analysis between different gsa methods applied to models with different numbers of parameters we further discuss the implications of the findings of the analysis on the model uses and optimization through factor fixing prioritization and uncertainty treatment we also present an investigative analysis of all four methods applied to fire models for an ability to guide the use of the model and treat different kinds of uncertainties in such models the specific contributions of this paper are 1 a comprehensive sensitivity analysis of input parameters in the widely used rothermel fire spread model and dry eucalypt fire spread model 2 a comparative analysis of four different sensitivity methods and their suitability when applied to uncertainty quantification in fire models 3 an insight into the implications of results on the understanding and interpretation of the fire models the rest of the paper is organized as follows section 2 explains the workflow and experimental setup section 3 presents the results while section 4 discusses the findings and their implications in detail section 5 concludes the paper with future works 2 workflow and experimental setup in this section we first describe the sa methods and wildfire models briefly before explaining the steps followed to conduct the sensitivity analysis of the fire models using the described methods 2 1 sa methods we considered four different sa methods namely the morris the sobol the fast and the pawn for quantifying uncertainties in wildfire models the morris method is an elementary effects based method that can be used in determining a few important input parameters with high influence screening in a model saltelli et al 2008 the sensitivity of the model output to input parameters is measured by two indices mean μ and standard deviation σ of the absolute values of the elementary effects the mean μ assesses the direct influence of the parameter on the model output while the standard deviation σ estimates the ensemble of the input parameter s non linear effects with other parameters a higher value of μ for a parameter indicates a greater influence of the parameter on the variability of the output similarly a higher value of σ for a parameter signifies higher interaction effects of the parameter with at least one other input parameter and or non linearities consequently the morris method classifies the input parameters into three distinct categories negligible effect large linear effects without interactions and large linear and or interaction effects the idea of μ replacing the original μ as proposed by morris was introduced by campolongo et al 2007 that solves the problem of type ii errors as μ is vulnerable to failing to identify a parameter with considerable influence on the model output the sobol method sobol 1990 is a variance based sa method that estimates the influence of input parameters on the model output in terms of the first order index s 1 i second order index s2 i j and the total sensitivity index st i s 1 i defines the direct influence of parameter i on the model output without considering any interaction with other input parameters s2 i j assesses the influence of pairwise interaction between two parameters i and j on the model output st i estimates the total influence of parameter i and its non linear interactions on the model output higher values of these indices indicate a higher influence of the parameter on the model output fourier amplitude sensitivity test fast is a variance based global sensitivity analysis method the fast method estimates the sensitivity of the model output to input parameters in terms of the first order index fo and total effect index total fo is the quantitative measure of the standalone influence of an input parameter while total is the measure of the overall influence of the parameter on the model output including the effects of its non linear interactions higher the values of these indices the higher the influence of the parameter on the model output the pawn method is a moment independent measure of the uncertainty of input factors in this method the sensitivity index of parameter p i is the distance between the unconditional pdf of a parameter obtained by varying all other parameters simultaneously and its conditional pdfs obtained by all other parameters but itself fixed at nominal values as highlighted by bekele and nicklow 2007 and liu et al 2015b the pdfs of the model outputs are usually unknown and are approximated using the experimental data pianosi and wagener 2015 demonstrated the effectiveness of choosing cumulative density function over pdfs as the empirical cdf from the data sample does not require any parameter tuning and developed a cdf based method that they named pawn pawn was shown to be very easy to implement robust and convergent with a lower number of model runs when compared to other methods the value of p i ranges between 0 and 1 such that the values closer to 1 signifies the higher influence of the paramter i on the model output 2 2 wildfire models we considered the dry eucalypt model and the rothermel model for this work the dry eucalypt model is widely used for australian eucalypt forests the model was developed from a sequence of experiments called project vesta cheney et al 2012 carried out in south western australia aimed at updating an older model for the fuel type burrows 1994 the mathematical equations in the model are explained in appendix a the rothermel wildfire model which is widely used in north america was developed by rothermel in 1972 based on the principle of conservation of energy and experimental tests carried out with different fuel models in the us rothermel 1972 the model describes the fire behavior in terms of the rate of spread flame length and intensity the fuel models are used to define the fuel input parameters while dynamic fuel models and other models are used to define live fuel curing and the effects of cross slope wind in fire spread in this paper we adapted the model as described in plischke et al 2020 for the mathematical equations input parameters and their distributions the mathematical equations in the rothermel model are described in appendix b 2 3 parameter selection based on the working of the dry eucalypt fire model temperature relative humidity fuel age and wind speed were selected as input parameters for the sensitivity analysis these parameters are considered to be the major input parameters of the fire model by wildfire communities as well for the rothermel fire spread model the input parameters were selected in reference to the work of plischke et al 2020 2 4 determination of input parameter distribution function to define the range for each input parameter in the dry eucalypt model we considered the ranges detailed in mcarthur 1966 for example the experimental fires were carried under the temperature range 21 c 32 5 c but the overall applicable temperature range for the model is 10 c 40 c and this latter range is used in the paper for this analysis we assigned uniform distributions to all the parameters to account for the variation within the range as shown in table 1 the range and the distribution assigned to each input parameter could easily be changed during the analyses if required for the rothermel fire spread model we chose the ranges and distributions table 2 as defined in plischke et al 2020 for testing the estimated shapley effects for the parameter slope the tangent of the angle of steepness in degrees o is considered as the input as per the experimental design in the same work and the input includes the values of the angle of slope up to about 39 2 5 sample generation for the morris method for k input parameters in the model and the base sample size of n number of trajectories within the range of a parameter the total number of model runs required for the estimation of the sensitivity indices is n k 1 morris 1991 for the sobol method as explained by saltelli et al 2010 and implemented in herman and usher 2017 n 2k 2 model evaluations are required to calculate the sensitivity indices for a model with k input parameters and n samples when second order indices are considered the model evaluation runs decrease to n k 2 times when second order indices are not considered these n samples were created using sobol s lp τ sequences lp τ sequences are quasi random sequences that produce uniformly distributed points in a unit hypercube sobol 2001 for the fast method the number of model runs required for estimation of the indices is nk with n as the base sample size and k as the number of the parameter in the model saltelli et al 1999 the samples were generated as uniformly distributed samples for each input parameter in the unit hypercube for the pawn method the samples were generated uniformly in the unit hypercube for each input parameter the total number of model evaluations required to calculate st j for k parameters is n u n n c k where n u model evaluations are required to calculate the unconditional cdf while n c n model evaluations are required to calculate the statistics on the distance between the conditioned and unconditional cdfs 2 6 calculation of sensitivity indices the sample generation and model evaluations of empirical fire models for morris sobol and fast methods were carried out in python using salib herman and usher 2017 salib is a library in python that supports different sensitivity analysis methods the sensitivity indices were also calculated using the library for different values of sample size which gave a different number of total samples for different sa methods for the pawn method a matlab based safe toolbox pianosi et al 2015 was used for sample generation cdf generation model runs and calculation of the indices through statistical measures for this we considered maximum as the statistic to calculate the pawn indices for the input parameters in the fire models as used in pianosi and wagener 2015 2 6 1 convergence of sensitivity indices gsa usually explores the entire permissible range of input parameters to analyze the variability of the model output and hence is a computationally complex process gsa implementations are sample based and the choice of the sample size to perform the analysis is crucial sarrazin et al 2016 the small values of sample size may not be able to cover the entire input space and thus produce non robust results the choice of larger values for sample size may incur very a high computational cost without significantly improving the precision of the results for environmental models like fire models it is imperative to find a suitable trade off between the level of the robustness of the results and the computational costs sarrazin et al 2016 as such we tested the convergence of the sensitivity indices calculated during our work more intuitively based on the three different types of convergence ranking sa indices and screening as explained in detail in the work of sarrazin et al 2016 accordingly the rankings the order based on the value of the indices of the input parameters have to be consistent while the difference between the calculated values of indices with an increasing number of model runs should not be more than 0 05 moreover the gap between the most influential and the least influential parameters should be consistent 2 7 robustness check bootstrapping to assess the robustness of the indices calculated in different methods adopted in the paper we used the bootstrapping technique efron and tibshirani 1994 to calculate 95 confidence intervals for all the indices for all the methods we used the bootstrap technique using b 1000 with replacement as suggested in archer et al 1997 the base sample size is chosen because that bootstrapping yields weaker results with smaller sample sizes mai and tolson 2019 the confidence interval was calculated based on the minimum number of model runs concluded to have met the convergence criteria the robustness of any sensitivity index is dependent on the width of the confidence interval a narrow ci signifies a more robust index and sa method while a wider ci signifies a less robust index and method 2 8 repeatability test after analyzing the sensitivity of each parameter on the model output the parameters can be sorted based on the values of estimated indices from the most important parameters to the least important parameters in the repeatability test experiments are reconducted with the same sets of parameters but with a fixed nominal value for some of the parameters based on the choice of the parameters the correlation between the original output variables and the output variables obtained after the repeatability test can vary significantly if the least important parameter is kept constant in the repeatability test the correlation coefficient of the output sets should ideally be 1 any values of correlation coefficient closer to 1 signify that the parameter which is kept constant does not have a significant influence and can be ignored for operational models conversely if the most important parameter is kept constant and the repeatability test is conducted the correlation coefficient between the output sets should be ideally 0 this signifies that the parameter is the most sensitive and cannot be ignored at all for operational models in our experiments we conducted the repeatability test by keeping the parameter with the least effect on the model output fixed at the mean value then we calculated the value of the correlation coefficient between the original and conditioned model output we kept the sample size for the repeatability test for all the methods at 10 000 3 results in this section we present the results obtained from the sensitivity analyses of the fire models and discuss the implications of the values of sensitivity indices on the fire models 3 1 sensitivity analysis results fig 1 shows the sensitivity indices calculated using four different methods for the dry eucalypt and the rothermel fire spread models respectively the results are presented in tabulated forms tables 3 and 4 as well to disclose the indices not included in the figures σ for the morris method s i for the sobol method and fo for the fast method for the dry eucalypt fire spread model as estimated by all four sa methods relative humidity was the most influential input parameter while temperature had the least influence the rank of the parameter based on their influence on the fire spread rate was consistent for all the methods see table 3 relative humidity had a mean value of 0 293 closely followed by wind with a relative value of 0 242 as given by the morris method fuel age and temperature had mean values of 0 172 and 0 031 respectively in terms of non linear interactions the temperature had the least interactions with a value of σ 0 055 highest value 0 327 similar findings were obtained with the sobol and the fast methods the values of the first order sensitivity index for relative humidity were 0 509 and 0 554 sobol s i and fast fo respectively the values of first order sensitivity indices for wind and fuel age stood at 0 238 and 0 209 and 0 097 and 0 165 respectively the estimated total sensitivity indices showed similar results as relative humidity contributed 54 55 65 and 54 28 sobol and fast respectively in the variability of fire spread rate while temperature accounted for only 4 4 2 and 1 32 wind closely followed the relative humidity in terms of effects on fire spread rate as it contributed 25 30 34 and 26 02 sobol and fast as estimated by the density based pawn method the pawn index for relative humidity was the highest with a value closer to 0 6 the values of the pawn index estimated for wind and fuel age stood at around 0 46 and 0 39 respectively thereby establishing the three parameters as influential ones in the model the pawn index estimated for temperature was less than 0 06 which classifies the temperature as the least influential parameter in the fire spread model for the rothermel model as estimated by the morris method moisture content of dead fuel md had the highest influence on the rate of fire spread while fuel particle total mineral content mc had the least influence apart from md wind speed u moisture content of live fuel ml and dead fuel loading to total fuel loading p had a significant influence on the output on the other hand low heat content h and slope tp had a negligible influence on the fire spread rate the rank for the parameters based on their influence on the fire spread rate was consistent for the highly and lowly influential parameters see table 4 in terms of non linear interactions the parameters followed the same trend as established by the values of the indices and consequently μ as md u and ml had higher non linear interactions while tp and h had the least interactions similar findings were obtained with the variance based methods as md u and ml were the parameters with dominant impact on the fire spread rate with an aggregate of about 78 as estimated using the sobol and the fast methods from sobol analysis it is clear that h tp and s t had a negligible impact on the model outcome as they had less than 1 contribution to the variation similarly the same analysis obtained from the fast method showed the contribution of the three parameters to be less than 2 quantitatively as estimated by the pawn method the values of pawn indices for md and u stood at 0 41 and 0 36 respectively the same for mc tp and h stood at 0 04 0 05 and 0 06 respectively ml was another parameter with a significant impact on the output variation with an index value of 0 26 it can thus be concluded that the fuel particle total mineral content mc slope tp and fuel particle low heat content h as the parameters in the rothermel model with the least influence on the fire spread rate 3 2 convergence of sa indices in our convergence analysis of sa indices we tested the convergence more intuitively based on the ranking values and screening of μ for morris total effect for sobol st and fast total and pawn indices for pawn figs 2 5 represent the values of sa indices calculated using different sa methods for varying the number of model runs the rank order of the parameters with the highest to the lowest impact on fire spread rate of the input parameters in terms of their effects on the model outputs was consistent for all the methods in the dry eucalypt model for the condition of convergence based on the values of the indices we followed the maximum difference between the indices calculated in successive model runs which should be less than the threshold of 0 05 as defined by sarrazin et al 2016 in our analysis the morris method took at least 44000 model runs to converge 25 000 for dry eucalypt and 44 000 for rothermel as shown in fig 2 while the sobol method took 110 000 for convergence 50 000 for dry eucalypt and 110 000 for rothermel as shown in fig 3 the fast method took at least 220 000 model runs 100 000 for dry eucalypt and 220 000 for the rothermel as shown in fig 4 to converge the pawn method took at least 33 000 22 000 for dry eucalypt and 33 000 for rothermel to converge for lesser model runs the rank and value of the indices for the input parameters kept changing beyond the limit of the threshold required for the convergence the gap between the most sensitive and least sensitive parameters remained consistent for the morris and sobol methods beyond 44 000 and 110 000 model runs respectively while the same for the fast method was at beyond 220 000 model runs for the pawn method the indices converged at relatively lesser model runs after 22 000 model runs the value of pawn indices converged for all the parameters in the dry eucalypt model the rank of the parameters based on their influence only marginally changed before 6 600 model runs after which the rank remained constant throughout the analysis as shown in fig 5b for the rothermel model the rank of least significant parameters changed until 33 000 model runs after which the rank started staying consistent for the least influential parameters the values of pawn indices decreased with an increase in the model runs but the difference was well within the threshold of 0 05 the distance between the values of indices for the most and least significant parameters stayed consistent beyond the model run of 33 000 for all the models based on these experimental findings it can be concluded that the minimum number of the model runs required to produce robust results for the sensitivity analysis of fire spread models vary based on both the sa method chosen as well as the wildfire spread model under consideration the pawn method took fewer model runs to converge when compared to variance based sobol and fast and the morris methods 3 3 robustness check fig 6 shows the 95 confidence interval ci of the sensitivity indices calculated using different gsa methods the indices calculated in the morris method had narrow confidence intervals in all the fire spread models the width of the cis was proportional to the values of the indices calculated for each parameter for both models for the dry eucalypt model the morris and the fast methods were more robust compared to the sobol and the pawn methods the 95 ci was the widest for the relative humidity and the narrowest for the temperature the maximum widths of 95 cis for the sobol and the pawn methods stood at 0 024 and 0 048 compared to 0 006 and 0 002 for the morris and the fast methods respectively for the rothermel model md had the widest 95 ci while mc and tp had the narrowest width the 95 cis for the morris and the fast methods were quite narrow with a maximum width of 0 004 and thus these methods can be labeled as robust the sobol method had a maximum 95 ci width of 0 028 while the same for the pawn method was 0 035 it can thus be concluded that the elementary effects based and variance based approach are more robust than the density based approach to increase the robustness of the methods the bootstrapping technique can be coupled together with convergence analysis as the cis become narrower with the increase in the number of model runs nevertheless the trade offs between the computational complexities of the increased model runs and the desired robustness have to be considered 3 4 repeatability analysis figs 7 8 show the repeatability analyses conducted for all the gsa methods for the dry eucalypt model the values of the correlation coefficient for the rate of fire spread ros with varying and constant values of temperature were greater than 0 99 for all the methods the high value of the correlation coefficient indicates that uncertainty associated with the temperature can be treated by reducing the parameter space of the parameter on the fire spread rate for the rothermel model the values of mc h and tp were kept constant at average values obtained from the samples the values of correlation coefficients for the samples obtained from all four methods are well above 0 98 thereby confirming mc h and tp as the parameters with insignificant effects on the variation of fire spread rate as such it can be concluded that temperature has insignificant effects on the fire spread rate in the dry eucalypt fire spread model while fuel particle total mineral content fuel particle low heat constant and slope have insignificant effects on the fire spread rate calculated in the rothermel model such findings could lead to optimization of parameter spaces chosen in ensemble based operational applications of these models 4 discussions our comparative analysis of different sa methods applied to fire spread models has consistent results all of the four sa methods established relative humidity as the parameter with the highest influence and temperature as the parameter with the least influence on the fire spread rate in the dry eucalypt model additionally the rank of the parameters based on their influence on the model output is consistent with all the four sa methods these findings align with the results we obtained in our previous work kc et al 2020a where we conducted the sensitivity analysis of the fire simulation tool spark miller et al 2015 in spark the dry eucalypt model is one of the fire models considered within the framework to estimate the fire spread rate for eucalypt forests for determining the total area burned by fire after a particular time the consistency and the similarity of the results obtained to the previous findings in the dry eucalypt model verify the correctness of our experimental setups for the rothermel model the estimation of sa indices as done in our experiment is consistent for the parameters with the highest and strongest influence on the fire spread rate the three parameters namely moisture content of dead fuel md wind speed u and moisture content of live fuel ml were the top ranked parameters based on their influence on the fire spread rate for all the four sa methods similarly fuel particle total mineral content mc slope tp and fuel particle low heat content h were the parameters found to have the least influence on the fire spread rate with consistent rank in all four sa methods the influence of the parameters on the fire spread rate in the rothermel model was studied in plischke et al 2020 using shapley values owen 2014 under three categories independence weak dependency and strong dependency between md and u indicating the fact that stronger the winds drier the fuel gets the parameters mc tp and h were found to have the highest influence and the parameters tp and h had the weakest influence on the fire spread rate in all the cases the influence of mc was found to decrease with the introduction of the dependency between md and u these findings on the parameters with the highest and the lowest influence on the model output are consistent with the results obtained in our analysis interestingly in the same work od was found tohave no shapley effects thereby establishing the parameter as one of the least influential parameters in our analysis od was found to have some influence on the fire spread rate but with a rank in the bottom half 6 and 7 and based on its influence on the fire spread rate od can still be labeled as one of the least influential parameters our findings are also consistent with the results reported in salvador et al 2001 liu et al 2015a 2015b ökten and liu 2021 under our objective to establish the suitability of sa methods based on several assessment factors we performed convergence and robustness check analyses the results obtained from those analyses are interesting with implications on how sensitivity analysis should be applied to wildfire spread models from our convergence analysis it is clear that the pawn method converges quickly compared to other sa methods similarly the morris method is also one of the computationally efficient methods when it comes to quick convergence the fast method took unusually long to converge which could be due to interference between the frequencies considered in the algorithm used to estimate the indices our convergence analysis for the two fire spread models shows that despite the increase in the number of parameters there is no change in the convergence patterns of the sa methods similarly during our robustness check the fast and the morris methods were found in general to be the most robust sa methods for converged indices on the other hand the pawn and the sobol methods were the least robust methods as the 95 cis for the indices were wider as a general trend the 95 cis for highly influential parameters were found to be wider compared to the least influential parameters one of the interesting findings in our analysis is the cis of md where the ci with the sobol method is wider than the ci with the pawn method these findings did not follow the usual finding where the sobol indices were found to be more robust than the pawn indices thus the robustness of sa methods may change when the number of parameters in fire models increases and this fact should be considered while choosing the method for any analysis fig 9 summarizes our findings in determining the suitability of sa methods while applying them to wildfire models the suitability of sa methods was assessed under four factors namely robustness convergence the number of parameters model runs required for base sample size and details of sensitivity information as seen in the figure each assessment factor has a pecking order for the four sa methods the fast and the pawn are the two methods to be prioritized for high robustness while the sobol and the morris methods are the methods suitable for more details on the sensitivity information similarly for more parameters in the model based on the same base sample size the pawn and the fast methods are suitable for the estimation of the sensitivity indices for quicker convergence the pawn and the morris methods are more suitable methods while the fast is the least suitable method the choice of the sa methods depends on a balanced trade off between these assessment factors and such choice can be quickly made in reference to fig 9 nevertheless the morris method should be prioritized for initial parameter screening in wildfire models under limited computational resources as the method quickly estimates robust indices with fewer model runs for the additional information on the influence of the second order interactions between parameters on the model output for worst scenario analyses the sobol method should be prioritized where the computational resources do not pose to be a significant constraint the sensitivity analysis results as obtained in our work have further implications on how the fire models can be optimized for operational uses as verified by our repeatability analysis the dry eucalypt model can be further optimized by prioritizing relative humidity wind and fuel age while the complexity of the rothermel model can be reduced by fixing the least influential parameters mc tp and h to nominal values for operational uses these findings as obtained during the sensitivity analysis can lead to new operational tools by cutting down the parameter space of the least influential models or dropping the least influential parameters 5 conclusion and future works deriving accurate risk metrics for wildfires can be challenging due to the inherent uncertainties associated with several aspects of wildfire spread models global sensitivity analysis gsa has been widely used to quantify parametric uncertainties in many environmental models we performed a comprehensive parametric sensitivity analysis of the widely used rothermel fire spread model and dry eucalypt fire spread model using four different sa methods to quantify the uncertainties in these models with the comparative analysis of different gsa methods our investigation presented and explained their suitability for uncertainty quantification for different factors our assessment determined that temperature has the parameter with the least impact on the rate of spread of fire in the dry eucalypt model while fuel particle low heat content h and slope tp had a negligible influence on the variation of fire spread rate in the rothermel model relative humidity was found to have the greatest impact on the spread rate in the dry eucalypt fire spread model for the rothermel model moisture content of dead fuel md and wind speed u were found to have a strong influence on the variation of spread rate such findings could be useful in reducing the parameter space of the least significant parameters to construct new operational tools for better informed decision making additionally concerning the choice and suitability of sa methods the morris and the pawn methods should be prioritized over the sobol and the fast methods for uncertainty quantification in wildfire models for a balanced trade off between convergence and robustness with computational constraints for more detailed information on the associated uncertainties the sobol method should be prioritized over the others where computational resources do not pose a significant constraint as the method gives additional information on second order interactions between the parameters and their influence on the model output such information can be used to determine the worst case scenarios for operational fire management this work was carried out for operational wildfire spread models but could easily be transferred to other environmental models in the future we intend to investigate the completeness of the existing methods and the requirement of coupling one or more methods for uncertainty quantification in complex environmental models we also plan to use gsa approaches to quantify other types of uncertainty in other complex environmental modeling systems declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was done as a part of first author s phd studies at university of tasmania hobart the authors would like to extend their heartfelt gratitude to everyone helped improve the quality of the paper at different stages of the work appendices a dry eucalypt forest fire model this wildfire model is used for predicting the spread of fire behaviors in dry eucalypt forest the model was developed from a sequence of experiments called project vesta cheney et al 2012 carried out in south western australia aimed at update an older model for the fuel type burrows 1994 the rate of spread r m h is expressed as follows 1 r 30 φ m f u 10 5 k m h 30 1 531 u 10 5 0 858 f h s s 0 93 f h s n s h n s 0 637 b 1 φ m f u 10 5 k m h note the term b 1 is the model correction for bias and is taken as 1 03 for this fhs ns is the near surface fuel hazard score fhss is the surface fuel hazard score and hns is near surface height and are derived from fuel age fa using the tall shrubs regression equations as explained in gould et al 2011 u 10 is the average 10 m open wind speed km h and φm f is the fuel moisture function 2 φ m f 18 35 m c 1 495 mc is the moisture content 3 m c 2 76 0 124 r h 0 0187 t t is the air temperature o c and rh is the relative humidity 4 f h s s 3 39 1 e 1 0 030 f u e l a g e 12 5 f h s n s 2 5 1 e 1 0 22 f u e l a g e 12 6 h n s 23 33 1 e 1 0 025 f u e l a g e 12 b rothermel surface fire spread model the rothermel wildfire model which is widely used in north america was developed by rothermel in 1972 the model was based on the principle of conservation of energy and experimental tests carried out with different fuel models in the us rothermel 1972 the model describes the fire behavior in terms of the rate of spread flame length and intensity the fuel models are used to define the fuel input parameters while dynamic fuel models and other models are used to define live fuel curing and the effects of cross slope wind in fire spread in this we adapt the model as described in plischke et al 2020 for the mathematical equations input parameters and their distributions the rate of spread r ft min is defined as follows 7 r i r ξ 1 ψ w ψ s ρ ε q i g the value of r is calculated using the following sub equations the fuel loading w 0 lb ft 2 is calculated as 8 w 0 0 2048 1 e x p 15 30 48 f d 2 where fd ft is the fuel depth the maximum reaction velocity t max l min is calculated as 9 t m a x a 2 v 1 5 495 0594 a 2 v 1 5 where a2v ft 1 is fuel particle area to volume ratio the optimum packing ratio β op is calculated as 10 β o p 3 348 a 2 v 0 8189 11 a 133 0 a 2 v 0 7913 12 θ 301 4 305 87 m l m d 2260 m d 2260 m l where ml is the moisture content of live fuel while md is the moisture content of dead fuel 13 θ m i n 1 m a x θ 0 the moisture damping coefficient μ m is calculated as 14 μ m e x p 7 3 p m d 7 3 θ 2 13 1 p m l where p is dead fuel loading to total fuel loading the mineral damping coefficient mu s is calculated as 15 m u s 0 174 m c 0 19 where mc is fuel particle total mineral content 16 c 7 47 e x p 0 133 a 2 v 0 55 17 b 0 02526 a 2 v 0 54 18 e 0 715 e x p 3 59 1 0 4 a 2 v the net fuel loading w n is calculated as 19 w n w 0 1 m c the oven dry bulk density ρ b is calculated as 20 ρ b w 0 f d the effective heating number ε is calculated as 21 ε e x p 138 a 2 v the heat of pre ignition q ig is calculated as 22 q i g 130 87 1054 43 m d the packing ratio β is calculated as 23 β ρ b o d where od is oven dry particle density the optimum reaction velocity t is calculated as 24 t t m a x β β o p a e x p a 1 β β o p the propagating flux ratio ξ is calculated as 25 ξ e x p 0 792 0 681 a 2 v β 1 192 0 2595 a 2 v the wind coefficient ψ w is calculated as 26 ψ w c u b β β o p e where u ft min is the wind speed at midflame height the slope factor ψ s is calculated as 27 ψ s 5 275 β 0 3 t p 2 where tp is slope the reaction intensity i r is calculated as 28 i r t w n h μ m μ s where h is fuel particle low heat content 
25786,environmental models involve inherent uncertainties the understanding of which is required for use by practitioners one method of uncertainty quantification is global sensitivity analysis gsa which has been extensively used in environmental modeling the suitability of gsa methods depends on the model implementation and computational complexity thus we present a comparative analysis of different gsa methods morris sobol fast and pawn applied to empirical fire spread models dry eucalypt and rothermel and explain their implications gsa methods such as pawn may not be able to explain all the interactions whereas methods such as sobol can result in high computational costs for models with several parameters we found that the morris or the pawn method should be prioritized over the sobol and the fast methods for a balanced trade off between convergence and robustness under computational constraints additionally the sobol method should be chosen for more detailed sensitivity information keywords forest fire fire behavior modeling uncertainty quantification global sensitivity analysis wildfires 1 introduction environmental models provide a mathematical description of natural phenomena in terms of the relationships dynamic or static between their driving factors recent advancements in observational science and related technologies such as computing and engineering have allowed natural phenomena to be studied in greater detail and consequently environmental models are becoming more complex razavi et al 2012 kaizer et al 2015 kc et al 2019 such models require parameters to simulate physical processes for example fire models predict the fire spread rate at a particular point using several parameters that include land topography fuel load meteorological data fire history and so on in such models all the parameters may not significantly affect the model output moreover some of the parameters have to be estimated indirectly using different calibration techniques as they cannot be measured directly vrugt et al 2002 each input parameter has associated uncertainties and consequently can affect the model outcome to a widely varying degree deriving accurate metrics from these models by quantifying the associated uncertainties can require a significantly high model runs under a wide range of possible scenarios kc et al 2020 as a result parameter estimation model calibration and uncertainty quantification have become a major challenge in environmental models used for operational disaster management sensitivity analysis sa is the study of the uncertainties in model output caused by the variation in the model inputs in local sa the impact of the parameters is studied around a local point saltelli et al 2008 in global sensitivity analysis gsa the entire range of the input parameters is taken into consideration while analyzing the model outputs gsa methods are one of the most significant quantitative techniques in risk modeling and analysis baker et al 1999 delen et al 2017 alyami et al 2019 koks et al 2019 kc et al 2021 gsa of natural hazard models can help identify the factor s or scenarios that pose a significant risk in an event of the outbreak of the disaster such identification can prioritize strategic plans for effective risk management christopher frey and patil 2002 for example gsa of fire spread models can help the authorities identify adverse weather scenarios or conditions that may contribute to dangerous wildfires gsa has recently gained attention in environmental modeling in areas such as wildfire hydrology decomposition and crops kc et al 2020a pianosi and wagener 2015 qin et al 2013 lamboni et al 2009 gsa helps to identify influential and non influential factors in the model fix the non influential factors to a known value and treat uncertainties that contribute to better understanding and interpretation of the model nossent et al 2011 saltelli et al 2008 there are several different gsa methods in the literature in the morris method morris 1991 the influence of a parameter is estimated by assessing the variation in the model output caused by varying values of the parameter within its entire range when other parameters are kept constant several works cai et al 2019 brohus et al 2007 menberg et al 2016 wang et al 2019 have conducted sensitivity analyses of different environmental models using the morris method as it provides a good trade off between the efficiency and accuracy for compute intensive model jiang et al 2017 the morris method cannot explain the pair wise interactions between the input factors for models with non linear input output relationships and cannot be used for non orthogonal input factors i e any correlated factors as the correlation cannot be induced ekstrom 2005 nossent et al 2011 the sobol method sobol 1990 and the fast method as proposed in schaibly and shuler 1973 are two widely used variance based sa methods in environmental models saltelli et al 2008 2019 nossent et al 2011 cai et al 2019 in the variance based approach the sensitivity of an uncertain input factor is estimated by investigating the factor s contribution to the model output variance based methods give a good measure of the contribution made by the input factor and its interaction with other factors for robust results these methods require many runs of the model which can be computationally costly if the number of model runs is significantly high baroni and tarantola 2014 furthermore variance is not a sensible measure of model output uncertainty when the model has multi modal or highly skewed output distribution borgonovo 2007 the pawn method pianosi and wagener 2015 is a density based method that estimates the sensitivity indices based on the density function of the model output applications of density based approach include hydmod model pianosi and wagener 2015 fire spread modeling hilton et al 2017 probabilistic risk assessment model borgonovo 2007 and engineering design system liu et al 2005 the density based approach can overcome the limitations of other approaches but its adaptation has been fairly limited as it is difficult to implement as one requires the knowledge of the conditional pdfs of the input factors pianosi and wagener 2015 the existing literature details several gsa methods and instances where they have been applied to fire spread models an uncertainty analysis study was carried out on wildfire models of boreal forests using morris monte carlo and first order analysis methods in the work cai et al 2019 the spitfire fire model thonicke et al 2010 was studied in a similar study where the morris and the sobol methods were used by gomez dans 2018 to study different forests boreal savanna temperate and tropical a global sensitivity analysis of the rothermel model over the mediterranean region was conducted by salvador et al 2001 which established low heat content particle density and mineral content as the parameters with negligible influence on fire spread rate moreover liu et al 2015b used variance based methods to reduce the number of parameters in the rothermel model chaparral fuel model and used quasi monte carlo methods for parametric uncertainties quantification in the reduced model the choice of the approach used in these studies was dependent on the compromise between accuracy computational cost and objectives there are a few studies zadeh et al 2017 hamby 1994 saltelli et al 1999 that have presented comparative analyses of different gsa methods in this paper we expand the scope of such analyses to the wildfire domain by presenting a comprehensive comparative analysis of various gsa methods applied to fire spread models given the inherent dangers wildfires pose to lives and infrastructure determining the intrinsic uncertainties of wildfire models is crucial for their use in operational wildfire management as such we draw a clear picture of four different gsa methods morris sobol fast and pawn applied to two different fire models the dry eucalypt model cheney et al 2012 used mainly in australia and rothermel 1972 used widely in north america the choice of fire spread models has been made based on the number of the model parameters to draw a clearer picture of the comparative analysis between different gsa methods applied to models with different numbers of parameters we further discuss the implications of the findings of the analysis on the model uses and optimization through factor fixing prioritization and uncertainty treatment we also present an investigative analysis of all four methods applied to fire models for an ability to guide the use of the model and treat different kinds of uncertainties in such models the specific contributions of this paper are 1 a comprehensive sensitivity analysis of input parameters in the widely used rothermel fire spread model and dry eucalypt fire spread model 2 a comparative analysis of four different sensitivity methods and their suitability when applied to uncertainty quantification in fire models 3 an insight into the implications of results on the understanding and interpretation of the fire models the rest of the paper is organized as follows section 2 explains the workflow and experimental setup section 3 presents the results while section 4 discusses the findings and their implications in detail section 5 concludes the paper with future works 2 workflow and experimental setup in this section we first describe the sa methods and wildfire models briefly before explaining the steps followed to conduct the sensitivity analysis of the fire models using the described methods 2 1 sa methods we considered four different sa methods namely the morris the sobol the fast and the pawn for quantifying uncertainties in wildfire models the morris method is an elementary effects based method that can be used in determining a few important input parameters with high influence screening in a model saltelli et al 2008 the sensitivity of the model output to input parameters is measured by two indices mean μ and standard deviation σ of the absolute values of the elementary effects the mean μ assesses the direct influence of the parameter on the model output while the standard deviation σ estimates the ensemble of the input parameter s non linear effects with other parameters a higher value of μ for a parameter indicates a greater influence of the parameter on the variability of the output similarly a higher value of σ for a parameter signifies higher interaction effects of the parameter with at least one other input parameter and or non linearities consequently the morris method classifies the input parameters into three distinct categories negligible effect large linear effects without interactions and large linear and or interaction effects the idea of μ replacing the original μ as proposed by morris was introduced by campolongo et al 2007 that solves the problem of type ii errors as μ is vulnerable to failing to identify a parameter with considerable influence on the model output the sobol method sobol 1990 is a variance based sa method that estimates the influence of input parameters on the model output in terms of the first order index s 1 i second order index s2 i j and the total sensitivity index st i s 1 i defines the direct influence of parameter i on the model output without considering any interaction with other input parameters s2 i j assesses the influence of pairwise interaction between two parameters i and j on the model output st i estimates the total influence of parameter i and its non linear interactions on the model output higher values of these indices indicate a higher influence of the parameter on the model output fourier amplitude sensitivity test fast is a variance based global sensitivity analysis method the fast method estimates the sensitivity of the model output to input parameters in terms of the first order index fo and total effect index total fo is the quantitative measure of the standalone influence of an input parameter while total is the measure of the overall influence of the parameter on the model output including the effects of its non linear interactions higher the values of these indices the higher the influence of the parameter on the model output the pawn method is a moment independent measure of the uncertainty of input factors in this method the sensitivity index of parameter p i is the distance between the unconditional pdf of a parameter obtained by varying all other parameters simultaneously and its conditional pdfs obtained by all other parameters but itself fixed at nominal values as highlighted by bekele and nicklow 2007 and liu et al 2015b the pdfs of the model outputs are usually unknown and are approximated using the experimental data pianosi and wagener 2015 demonstrated the effectiveness of choosing cumulative density function over pdfs as the empirical cdf from the data sample does not require any parameter tuning and developed a cdf based method that they named pawn pawn was shown to be very easy to implement robust and convergent with a lower number of model runs when compared to other methods the value of p i ranges between 0 and 1 such that the values closer to 1 signifies the higher influence of the paramter i on the model output 2 2 wildfire models we considered the dry eucalypt model and the rothermel model for this work the dry eucalypt model is widely used for australian eucalypt forests the model was developed from a sequence of experiments called project vesta cheney et al 2012 carried out in south western australia aimed at updating an older model for the fuel type burrows 1994 the mathematical equations in the model are explained in appendix a the rothermel wildfire model which is widely used in north america was developed by rothermel in 1972 based on the principle of conservation of energy and experimental tests carried out with different fuel models in the us rothermel 1972 the model describes the fire behavior in terms of the rate of spread flame length and intensity the fuel models are used to define the fuel input parameters while dynamic fuel models and other models are used to define live fuel curing and the effects of cross slope wind in fire spread in this paper we adapted the model as described in plischke et al 2020 for the mathematical equations input parameters and their distributions the mathematical equations in the rothermel model are described in appendix b 2 3 parameter selection based on the working of the dry eucalypt fire model temperature relative humidity fuel age and wind speed were selected as input parameters for the sensitivity analysis these parameters are considered to be the major input parameters of the fire model by wildfire communities as well for the rothermel fire spread model the input parameters were selected in reference to the work of plischke et al 2020 2 4 determination of input parameter distribution function to define the range for each input parameter in the dry eucalypt model we considered the ranges detailed in mcarthur 1966 for example the experimental fires were carried under the temperature range 21 c 32 5 c but the overall applicable temperature range for the model is 10 c 40 c and this latter range is used in the paper for this analysis we assigned uniform distributions to all the parameters to account for the variation within the range as shown in table 1 the range and the distribution assigned to each input parameter could easily be changed during the analyses if required for the rothermel fire spread model we chose the ranges and distributions table 2 as defined in plischke et al 2020 for testing the estimated shapley effects for the parameter slope the tangent of the angle of steepness in degrees o is considered as the input as per the experimental design in the same work and the input includes the values of the angle of slope up to about 39 2 5 sample generation for the morris method for k input parameters in the model and the base sample size of n number of trajectories within the range of a parameter the total number of model runs required for the estimation of the sensitivity indices is n k 1 morris 1991 for the sobol method as explained by saltelli et al 2010 and implemented in herman and usher 2017 n 2k 2 model evaluations are required to calculate the sensitivity indices for a model with k input parameters and n samples when second order indices are considered the model evaluation runs decrease to n k 2 times when second order indices are not considered these n samples were created using sobol s lp τ sequences lp τ sequences are quasi random sequences that produce uniformly distributed points in a unit hypercube sobol 2001 for the fast method the number of model runs required for estimation of the indices is nk with n as the base sample size and k as the number of the parameter in the model saltelli et al 1999 the samples were generated as uniformly distributed samples for each input parameter in the unit hypercube for the pawn method the samples were generated uniformly in the unit hypercube for each input parameter the total number of model evaluations required to calculate st j for k parameters is n u n n c k where n u model evaluations are required to calculate the unconditional cdf while n c n model evaluations are required to calculate the statistics on the distance between the conditioned and unconditional cdfs 2 6 calculation of sensitivity indices the sample generation and model evaluations of empirical fire models for morris sobol and fast methods were carried out in python using salib herman and usher 2017 salib is a library in python that supports different sensitivity analysis methods the sensitivity indices were also calculated using the library for different values of sample size which gave a different number of total samples for different sa methods for the pawn method a matlab based safe toolbox pianosi et al 2015 was used for sample generation cdf generation model runs and calculation of the indices through statistical measures for this we considered maximum as the statistic to calculate the pawn indices for the input parameters in the fire models as used in pianosi and wagener 2015 2 6 1 convergence of sensitivity indices gsa usually explores the entire permissible range of input parameters to analyze the variability of the model output and hence is a computationally complex process gsa implementations are sample based and the choice of the sample size to perform the analysis is crucial sarrazin et al 2016 the small values of sample size may not be able to cover the entire input space and thus produce non robust results the choice of larger values for sample size may incur very a high computational cost without significantly improving the precision of the results for environmental models like fire models it is imperative to find a suitable trade off between the level of the robustness of the results and the computational costs sarrazin et al 2016 as such we tested the convergence of the sensitivity indices calculated during our work more intuitively based on the three different types of convergence ranking sa indices and screening as explained in detail in the work of sarrazin et al 2016 accordingly the rankings the order based on the value of the indices of the input parameters have to be consistent while the difference between the calculated values of indices with an increasing number of model runs should not be more than 0 05 moreover the gap between the most influential and the least influential parameters should be consistent 2 7 robustness check bootstrapping to assess the robustness of the indices calculated in different methods adopted in the paper we used the bootstrapping technique efron and tibshirani 1994 to calculate 95 confidence intervals for all the indices for all the methods we used the bootstrap technique using b 1000 with replacement as suggested in archer et al 1997 the base sample size is chosen because that bootstrapping yields weaker results with smaller sample sizes mai and tolson 2019 the confidence interval was calculated based on the minimum number of model runs concluded to have met the convergence criteria the robustness of any sensitivity index is dependent on the width of the confidence interval a narrow ci signifies a more robust index and sa method while a wider ci signifies a less robust index and method 2 8 repeatability test after analyzing the sensitivity of each parameter on the model output the parameters can be sorted based on the values of estimated indices from the most important parameters to the least important parameters in the repeatability test experiments are reconducted with the same sets of parameters but with a fixed nominal value for some of the parameters based on the choice of the parameters the correlation between the original output variables and the output variables obtained after the repeatability test can vary significantly if the least important parameter is kept constant in the repeatability test the correlation coefficient of the output sets should ideally be 1 any values of correlation coefficient closer to 1 signify that the parameter which is kept constant does not have a significant influence and can be ignored for operational models conversely if the most important parameter is kept constant and the repeatability test is conducted the correlation coefficient between the output sets should be ideally 0 this signifies that the parameter is the most sensitive and cannot be ignored at all for operational models in our experiments we conducted the repeatability test by keeping the parameter with the least effect on the model output fixed at the mean value then we calculated the value of the correlation coefficient between the original and conditioned model output we kept the sample size for the repeatability test for all the methods at 10 000 3 results in this section we present the results obtained from the sensitivity analyses of the fire models and discuss the implications of the values of sensitivity indices on the fire models 3 1 sensitivity analysis results fig 1 shows the sensitivity indices calculated using four different methods for the dry eucalypt and the rothermel fire spread models respectively the results are presented in tabulated forms tables 3 and 4 as well to disclose the indices not included in the figures σ for the morris method s i for the sobol method and fo for the fast method for the dry eucalypt fire spread model as estimated by all four sa methods relative humidity was the most influential input parameter while temperature had the least influence the rank of the parameter based on their influence on the fire spread rate was consistent for all the methods see table 3 relative humidity had a mean value of 0 293 closely followed by wind with a relative value of 0 242 as given by the morris method fuel age and temperature had mean values of 0 172 and 0 031 respectively in terms of non linear interactions the temperature had the least interactions with a value of σ 0 055 highest value 0 327 similar findings were obtained with the sobol and the fast methods the values of the first order sensitivity index for relative humidity were 0 509 and 0 554 sobol s i and fast fo respectively the values of first order sensitivity indices for wind and fuel age stood at 0 238 and 0 209 and 0 097 and 0 165 respectively the estimated total sensitivity indices showed similar results as relative humidity contributed 54 55 65 and 54 28 sobol and fast respectively in the variability of fire spread rate while temperature accounted for only 4 4 2 and 1 32 wind closely followed the relative humidity in terms of effects on fire spread rate as it contributed 25 30 34 and 26 02 sobol and fast as estimated by the density based pawn method the pawn index for relative humidity was the highest with a value closer to 0 6 the values of the pawn index estimated for wind and fuel age stood at around 0 46 and 0 39 respectively thereby establishing the three parameters as influential ones in the model the pawn index estimated for temperature was less than 0 06 which classifies the temperature as the least influential parameter in the fire spread model for the rothermel model as estimated by the morris method moisture content of dead fuel md had the highest influence on the rate of fire spread while fuel particle total mineral content mc had the least influence apart from md wind speed u moisture content of live fuel ml and dead fuel loading to total fuel loading p had a significant influence on the output on the other hand low heat content h and slope tp had a negligible influence on the fire spread rate the rank for the parameters based on their influence on the fire spread rate was consistent for the highly and lowly influential parameters see table 4 in terms of non linear interactions the parameters followed the same trend as established by the values of the indices and consequently μ as md u and ml had higher non linear interactions while tp and h had the least interactions similar findings were obtained with the variance based methods as md u and ml were the parameters with dominant impact on the fire spread rate with an aggregate of about 78 as estimated using the sobol and the fast methods from sobol analysis it is clear that h tp and s t had a negligible impact on the model outcome as they had less than 1 contribution to the variation similarly the same analysis obtained from the fast method showed the contribution of the three parameters to be less than 2 quantitatively as estimated by the pawn method the values of pawn indices for md and u stood at 0 41 and 0 36 respectively the same for mc tp and h stood at 0 04 0 05 and 0 06 respectively ml was another parameter with a significant impact on the output variation with an index value of 0 26 it can thus be concluded that the fuel particle total mineral content mc slope tp and fuel particle low heat content h as the parameters in the rothermel model with the least influence on the fire spread rate 3 2 convergence of sa indices in our convergence analysis of sa indices we tested the convergence more intuitively based on the ranking values and screening of μ for morris total effect for sobol st and fast total and pawn indices for pawn figs 2 5 represent the values of sa indices calculated using different sa methods for varying the number of model runs the rank order of the parameters with the highest to the lowest impact on fire spread rate of the input parameters in terms of their effects on the model outputs was consistent for all the methods in the dry eucalypt model for the condition of convergence based on the values of the indices we followed the maximum difference between the indices calculated in successive model runs which should be less than the threshold of 0 05 as defined by sarrazin et al 2016 in our analysis the morris method took at least 44000 model runs to converge 25 000 for dry eucalypt and 44 000 for rothermel as shown in fig 2 while the sobol method took 110 000 for convergence 50 000 for dry eucalypt and 110 000 for rothermel as shown in fig 3 the fast method took at least 220 000 model runs 100 000 for dry eucalypt and 220 000 for the rothermel as shown in fig 4 to converge the pawn method took at least 33 000 22 000 for dry eucalypt and 33 000 for rothermel to converge for lesser model runs the rank and value of the indices for the input parameters kept changing beyond the limit of the threshold required for the convergence the gap between the most sensitive and least sensitive parameters remained consistent for the morris and sobol methods beyond 44 000 and 110 000 model runs respectively while the same for the fast method was at beyond 220 000 model runs for the pawn method the indices converged at relatively lesser model runs after 22 000 model runs the value of pawn indices converged for all the parameters in the dry eucalypt model the rank of the parameters based on their influence only marginally changed before 6 600 model runs after which the rank remained constant throughout the analysis as shown in fig 5b for the rothermel model the rank of least significant parameters changed until 33 000 model runs after which the rank started staying consistent for the least influential parameters the values of pawn indices decreased with an increase in the model runs but the difference was well within the threshold of 0 05 the distance between the values of indices for the most and least significant parameters stayed consistent beyond the model run of 33 000 for all the models based on these experimental findings it can be concluded that the minimum number of the model runs required to produce robust results for the sensitivity analysis of fire spread models vary based on both the sa method chosen as well as the wildfire spread model under consideration the pawn method took fewer model runs to converge when compared to variance based sobol and fast and the morris methods 3 3 robustness check fig 6 shows the 95 confidence interval ci of the sensitivity indices calculated using different gsa methods the indices calculated in the morris method had narrow confidence intervals in all the fire spread models the width of the cis was proportional to the values of the indices calculated for each parameter for both models for the dry eucalypt model the morris and the fast methods were more robust compared to the sobol and the pawn methods the 95 ci was the widest for the relative humidity and the narrowest for the temperature the maximum widths of 95 cis for the sobol and the pawn methods stood at 0 024 and 0 048 compared to 0 006 and 0 002 for the morris and the fast methods respectively for the rothermel model md had the widest 95 ci while mc and tp had the narrowest width the 95 cis for the morris and the fast methods were quite narrow with a maximum width of 0 004 and thus these methods can be labeled as robust the sobol method had a maximum 95 ci width of 0 028 while the same for the pawn method was 0 035 it can thus be concluded that the elementary effects based and variance based approach are more robust than the density based approach to increase the robustness of the methods the bootstrapping technique can be coupled together with convergence analysis as the cis become narrower with the increase in the number of model runs nevertheless the trade offs between the computational complexities of the increased model runs and the desired robustness have to be considered 3 4 repeatability analysis figs 7 8 show the repeatability analyses conducted for all the gsa methods for the dry eucalypt model the values of the correlation coefficient for the rate of fire spread ros with varying and constant values of temperature were greater than 0 99 for all the methods the high value of the correlation coefficient indicates that uncertainty associated with the temperature can be treated by reducing the parameter space of the parameter on the fire spread rate for the rothermel model the values of mc h and tp were kept constant at average values obtained from the samples the values of correlation coefficients for the samples obtained from all four methods are well above 0 98 thereby confirming mc h and tp as the parameters with insignificant effects on the variation of fire spread rate as such it can be concluded that temperature has insignificant effects on the fire spread rate in the dry eucalypt fire spread model while fuel particle total mineral content fuel particle low heat constant and slope have insignificant effects on the fire spread rate calculated in the rothermel model such findings could lead to optimization of parameter spaces chosen in ensemble based operational applications of these models 4 discussions our comparative analysis of different sa methods applied to fire spread models has consistent results all of the four sa methods established relative humidity as the parameter with the highest influence and temperature as the parameter with the least influence on the fire spread rate in the dry eucalypt model additionally the rank of the parameters based on their influence on the model output is consistent with all the four sa methods these findings align with the results we obtained in our previous work kc et al 2020a where we conducted the sensitivity analysis of the fire simulation tool spark miller et al 2015 in spark the dry eucalypt model is one of the fire models considered within the framework to estimate the fire spread rate for eucalypt forests for determining the total area burned by fire after a particular time the consistency and the similarity of the results obtained to the previous findings in the dry eucalypt model verify the correctness of our experimental setups for the rothermel model the estimation of sa indices as done in our experiment is consistent for the parameters with the highest and strongest influence on the fire spread rate the three parameters namely moisture content of dead fuel md wind speed u and moisture content of live fuel ml were the top ranked parameters based on their influence on the fire spread rate for all the four sa methods similarly fuel particle total mineral content mc slope tp and fuel particle low heat content h were the parameters found to have the least influence on the fire spread rate with consistent rank in all four sa methods the influence of the parameters on the fire spread rate in the rothermel model was studied in plischke et al 2020 using shapley values owen 2014 under three categories independence weak dependency and strong dependency between md and u indicating the fact that stronger the winds drier the fuel gets the parameters mc tp and h were found to have the highest influence and the parameters tp and h had the weakest influence on the fire spread rate in all the cases the influence of mc was found to decrease with the introduction of the dependency between md and u these findings on the parameters with the highest and the lowest influence on the model output are consistent with the results obtained in our analysis interestingly in the same work od was found tohave no shapley effects thereby establishing the parameter as one of the least influential parameters in our analysis od was found to have some influence on the fire spread rate but with a rank in the bottom half 6 and 7 and based on its influence on the fire spread rate od can still be labeled as one of the least influential parameters our findings are also consistent with the results reported in salvador et al 2001 liu et al 2015a 2015b ökten and liu 2021 under our objective to establish the suitability of sa methods based on several assessment factors we performed convergence and robustness check analyses the results obtained from those analyses are interesting with implications on how sensitivity analysis should be applied to wildfire spread models from our convergence analysis it is clear that the pawn method converges quickly compared to other sa methods similarly the morris method is also one of the computationally efficient methods when it comes to quick convergence the fast method took unusually long to converge which could be due to interference between the frequencies considered in the algorithm used to estimate the indices our convergence analysis for the two fire spread models shows that despite the increase in the number of parameters there is no change in the convergence patterns of the sa methods similarly during our robustness check the fast and the morris methods were found in general to be the most robust sa methods for converged indices on the other hand the pawn and the sobol methods were the least robust methods as the 95 cis for the indices were wider as a general trend the 95 cis for highly influential parameters were found to be wider compared to the least influential parameters one of the interesting findings in our analysis is the cis of md where the ci with the sobol method is wider than the ci with the pawn method these findings did not follow the usual finding where the sobol indices were found to be more robust than the pawn indices thus the robustness of sa methods may change when the number of parameters in fire models increases and this fact should be considered while choosing the method for any analysis fig 9 summarizes our findings in determining the suitability of sa methods while applying them to wildfire models the suitability of sa methods was assessed under four factors namely robustness convergence the number of parameters model runs required for base sample size and details of sensitivity information as seen in the figure each assessment factor has a pecking order for the four sa methods the fast and the pawn are the two methods to be prioritized for high robustness while the sobol and the morris methods are the methods suitable for more details on the sensitivity information similarly for more parameters in the model based on the same base sample size the pawn and the fast methods are suitable for the estimation of the sensitivity indices for quicker convergence the pawn and the morris methods are more suitable methods while the fast is the least suitable method the choice of the sa methods depends on a balanced trade off between these assessment factors and such choice can be quickly made in reference to fig 9 nevertheless the morris method should be prioritized for initial parameter screening in wildfire models under limited computational resources as the method quickly estimates robust indices with fewer model runs for the additional information on the influence of the second order interactions between parameters on the model output for worst scenario analyses the sobol method should be prioritized where the computational resources do not pose to be a significant constraint the sensitivity analysis results as obtained in our work have further implications on how the fire models can be optimized for operational uses as verified by our repeatability analysis the dry eucalypt model can be further optimized by prioritizing relative humidity wind and fuel age while the complexity of the rothermel model can be reduced by fixing the least influential parameters mc tp and h to nominal values for operational uses these findings as obtained during the sensitivity analysis can lead to new operational tools by cutting down the parameter space of the least influential models or dropping the least influential parameters 5 conclusion and future works deriving accurate risk metrics for wildfires can be challenging due to the inherent uncertainties associated with several aspects of wildfire spread models global sensitivity analysis gsa has been widely used to quantify parametric uncertainties in many environmental models we performed a comprehensive parametric sensitivity analysis of the widely used rothermel fire spread model and dry eucalypt fire spread model using four different sa methods to quantify the uncertainties in these models with the comparative analysis of different gsa methods our investigation presented and explained their suitability for uncertainty quantification for different factors our assessment determined that temperature has the parameter with the least impact on the rate of spread of fire in the dry eucalypt model while fuel particle low heat content h and slope tp had a negligible influence on the variation of fire spread rate in the rothermel model relative humidity was found to have the greatest impact on the spread rate in the dry eucalypt fire spread model for the rothermel model moisture content of dead fuel md and wind speed u were found to have a strong influence on the variation of spread rate such findings could be useful in reducing the parameter space of the least significant parameters to construct new operational tools for better informed decision making additionally concerning the choice and suitability of sa methods the morris and the pawn methods should be prioritized over the sobol and the fast methods for uncertainty quantification in wildfire models for a balanced trade off between convergence and robustness with computational constraints for more detailed information on the associated uncertainties the sobol method should be prioritized over the others where computational resources do not pose a significant constraint as the method gives additional information on second order interactions between the parameters and their influence on the model output such information can be used to determine the worst case scenarios for operational fire management this work was carried out for operational wildfire spread models but could easily be transferred to other environmental models in the future we intend to investigate the completeness of the existing methods and the requirement of coupling one or more methods for uncertainty quantification in complex environmental models we also plan to use gsa approaches to quantify other types of uncertainty in other complex environmental modeling systems declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was done as a part of first author s phd studies at university of tasmania hobart the authors would like to extend their heartfelt gratitude to everyone helped improve the quality of the paper at different stages of the work appendices a dry eucalypt forest fire model this wildfire model is used for predicting the spread of fire behaviors in dry eucalypt forest the model was developed from a sequence of experiments called project vesta cheney et al 2012 carried out in south western australia aimed at update an older model for the fuel type burrows 1994 the rate of spread r m h is expressed as follows 1 r 30 φ m f u 10 5 k m h 30 1 531 u 10 5 0 858 f h s s 0 93 f h s n s h n s 0 637 b 1 φ m f u 10 5 k m h note the term b 1 is the model correction for bias and is taken as 1 03 for this fhs ns is the near surface fuel hazard score fhss is the surface fuel hazard score and hns is near surface height and are derived from fuel age fa using the tall shrubs regression equations as explained in gould et al 2011 u 10 is the average 10 m open wind speed km h and φm f is the fuel moisture function 2 φ m f 18 35 m c 1 495 mc is the moisture content 3 m c 2 76 0 124 r h 0 0187 t t is the air temperature o c and rh is the relative humidity 4 f h s s 3 39 1 e 1 0 030 f u e l a g e 12 5 f h s n s 2 5 1 e 1 0 22 f u e l a g e 12 6 h n s 23 33 1 e 1 0 025 f u e l a g e 12 b rothermel surface fire spread model the rothermel wildfire model which is widely used in north america was developed by rothermel in 1972 the model was based on the principle of conservation of energy and experimental tests carried out with different fuel models in the us rothermel 1972 the model describes the fire behavior in terms of the rate of spread flame length and intensity the fuel models are used to define the fuel input parameters while dynamic fuel models and other models are used to define live fuel curing and the effects of cross slope wind in fire spread in this we adapt the model as described in plischke et al 2020 for the mathematical equations input parameters and their distributions the rate of spread r ft min is defined as follows 7 r i r ξ 1 ψ w ψ s ρ ε q i g the value of r is calculated using the following sub equations the fuel loading w 0 lb ft 2 is calculated as 8 w 0 0 2048 1 e x p 15 30 48 f d 2 where fd ft is the fuel depth the maximum reaction velocity t max l min is calculated as 9 t m a x a 2 v 1 5 495 0594 a 2 v 1 5 where a2v ft 1 is fuel particle area to volume ratio the optimum packing ratio β op is calculated as 10 β o p 3 348 a 2 v 0 8189 11 a 133 0 a 2 v 0 7913 12 θ 301 4 305 87 m l m d 2260 m d 2260 m l where ml is the moisture content of live fuel while md is the moisture content of dead fuel 13 θ m i n 1 m a x θ 0 the moisture damping coefficient μ m is calculated as 14 μ m e x p 7 3 p m d 7 3 θ 2 13 1 p m l where p is dead fuel loading to total fuel loading the mineral damping coefficient mu s is calculated as 15 m u s 0 174 m c 0 19 where mc is fuel particle total mineral content 16 c 7 47 e x p 0 133 a 2 v 0 55 17 b 0 02526 a 2 v 0 54 18 e 0 715 e x p 3 59 1 0 4 a 2 v the net fuel loading w n is calculated as 19 w n w 0 1 m c the oven dry bulk density ρ b is calculated as 20 ρ b w 0 f d the effective heating number ε is calculated as 21 ε e x p 138 a 2 v the heat of pre ignition q ig is calculated as 22 q i g 130 87 1054 43 m d the packing ratio β is calculated as 23 β ρ b o d where od is oven dry particle density the optimum reaction velocity t is calculated as 24 t t m a x β β o p a e x p a 1 β β o p the propagating flux ratio ξ is calculated as 25 ξ e x p 0 792 0 681 a 2 v β 1 192 0 2595 a 2 v the wind coefficient ψ w is calculated as 26 ψ w c u b β β o p e where u ft min is the wind speed at midflame height the slope factor ψ s is calculated as 27 ψ s 5 275 β 0 3 t p 2 where tp is slope the reaction intensity i r is calculated as 28 i r t w n h μ m μ s where h is fuel particle low heat content 
25787,modelling river physical processes is of critical importance for flood protection river management and restoration of riverine environments developments in algorithms and computational power have led to a wider spread of river simulation tools however the use of two dimensional models can still be hindered by complexity in the setup and the high computational costs here we present the freeware basement version 3 a flexible tool for two dimensional river simulations that bundles solvers for hydrodynamic morphodynamic and scalar advection diffusion processes basement leverages different computational platforms multi core cpus and graphics processing units gpus to enable the simulation of large domains and long term river processes the adoption of a fully costless workflow and a light gui facilitate its broad utilization we test its robustness and efficiency in a selection of benchmarks results confirm that basement could be an efficient and versatile tool for research engineering practice and education in river modelling keywords gpu cuda river modelling unstructured grid shallow water sediment transport pollutant transport software availability name of software basement version 3 v3 website www basement ethz ch e mail basement ethz ch developer numerical modelling division at the laboratory of hydraulics hydrology and glaziology vaw eth zürich language c c cuda interface graphical user interface gui command line interface cli hardware cpus cuda enabled gpus optional os windows linux ubuntu availability freeware test cases repository available at eth zürich research collection https doi org 10 3929 ethz b 000482308 1 introduction in the last decades the usage of numerical tools widely spread in several subjects of the environmental sciences river science sensu gilvear et al 2016 is no exception in this trend with a number of tools been developed to address variegate research questions e g brewer et al 2018 shimizu et al 2019 modelled river physical processes span from flood simulation hydraulic and sediment dynamics pollutant and temperature transport to vegetation and flow interactions just to mention a few e g crosato and saleh 2011 sharma and kansal 2012 williams et al 2016 dugdale et al 2017 teng et al 2017 such river processes occur at different spatial and temporal scales hence influencing the development and choice of suitable modelling tools advances in computational power numerical algorithms and optimization routines that occurred in the last decades allowed for the spread of more and more sophisticated numerical tools in the context of river science two dimensional depth averaged hereinafter 2d models are nowadays of common use in research and engineering practice this is particularly true for some applications such as flood modelling and river morphodynamics e g shimizu et al 2019 zischg et al 2018 the increasing usage of 2d river models is also closely bonded with the growing availability of high resolution river datasets in particular advances in lidar uav photography and others remote sensed survey technologies enable river topographic scans at an unprecedented level of detail e g marcus and fonstad 2010 savage et al 2016 the increased computational capabilities and refined datasets open the gates for near census sensu pasternack 2011 numerical modelling of several river processes indeed 2d river models have the capability to simulate fine spatial centimeters to meters and temporal scales seconds to days at such scales relevant hydro morphodynamic processes e g bar formation and also ecohydraulic processes e g habitat dynamics can hence be modelled e g maddock 1999 siviglia et al 2013 wyrick et al 2014 guan et al 2016 nevertheless 2d river models can still be computationally demanding with simulations lasting several days this is particularly true when complex physical processes such as morphodynamics are accounted for siviglia and crosato 2016 moreover large scale or near census applications i e with millions of computational cells and or long term simulations i e years all concur to increase overall computational costs such drawbacks particularly apply for the investigation of highly unsteady river processes such as artificial or natural flood waves where explicit numerical schemes are preferable in such cases the overall computational time scales exponentially with the number of computational cells due to stability constraints e g toro 2001 increasing the efficiency and the computational performance of river models represents yet a challenge pitfalls arise with the number of computational cells but also with the inherent complexity of 2d models for example challenges are to be found in the setup of the computational domain nahorniak et al 2018 but also in the definition of particular boundary conditions costabile and macchione 2015 dazzi et al 2020 increasing the computational performance is also sought by developing alternative numerical solution strategies for the underlying physical governing equations examples are variegate spanning from the adoption of a local timestep for the numerical integration e g sanders 2008 dazzi et al 2018 the automatic adaptation of the computational mesh e g powell et al 1993 the use of acceleration factors for the hydro morphodynamic problem e g carraro et al 2018 morgan et al 2020 to the reformulation of the governing mathematical equations for water quality simulations e g vanzo et al 2016 to mention a few parallel computing solutions are the most popular strategies to reduce computational time they historically benefit from the continuous improvements of parallel performance of both single cpus and clusters and the decrease of their unitary price the general aim of parallelization techniques is to split the total computational load into tasks that can be executed simultaneously by different computational units e g afzal et al 2016 the use of the graphics processing unit gpu as a general purpose computational resource developed rapidly in the last decade owens et al 2008 for many parallelizable workloads offloading work to gpus is a relatively cheap and efficient high performance computing strategy that is also easily upgradable in standard desktop workstations by means of gpu parallelization numerical models can potentially be accelerated by a factor of tens and more e g lacasta et al 2014 the efficiency of such a parallelization however depends on the data exchange between the main memory and the processors with a complex memory hierarchy and bandwidth bottlenecks mudalige et al 2012 these low level constraints can limit computational speedup and depend on the model data memory handling the underlying model complexity and the nature of the governing equations to be solved in applications such as 2d river models the type of computational mesh i e structured or unstructured has a significant influence on the final computational speedup in the last decade river simulation models have benefited from gpu parallelization specific and ad hoc implementations of gpu based models for 2d hydrodynamic e g brodtkorb et al 2012 smith and liang 2013 vacondio et al 2014 horváth et al 2016 vacondio et al 2017 and occasionally morphodynamic simulations e g hou et al 2020 have become available the vast majority of these models are based on structured grids which allow for an easier implementation and for relatively higher computational speedups this is due to the fact that for structured meshes the data structure is inherently simpler which reduces the need for mappings and indirections to the best of the authors knowledge few hydrodynamic models implement gpu acceleration on unstructured meshes lacasta et al 2014 2015 castro et al 2011 petaccia et al 2016 with very limited ad hoc implementations for transient flows morphodynamics e g juez et al 2016 bundled river modelling software that support gpu acceleration are available for commercial use e g riverflow2d hydronia com riverflow2d tuflow tuflow com but costless ones are still few see garcía feal et al 2018 an increase in availability of freeware gpu based river models would be beneficial for environmental modelers in academic research and education but also in consultancy and engineering offices in this paper we introduce the basement software version 3 a freeware application developed at the laboratory of hydraulics hydrology and glaciology of eth zürich the software can simulate two dimensional hydrodynamic morphodynamic and scalar advection diffusion processes of scientific and practical interest it can seamlessly run on gpu enabled workstations as well as on more standard multi core cpus this flexibility in the choice of the backend i e of the final computational hardware is achieved by integrating the op2 framework mudalige et al 2012 reguly et al 2016 giles et al 2012 this framework provides an additional abstract layer for the acceleration of numerical models on unstructured computational meshes and has been successfully implemented in similar modelling context reguly et al 2018 the obtained parallelization performance alleviates the computational limitations when simulating high resolution or large computational domains and or long term processes e g giles et al 2020 this is particularly relevant when aiming at the calibration beckers et al 2020 or at the uncertainty evaluation savage et al 2016 jung and merwade 2014 of deterministic models as proof of concept a flood wave uncertainty propagation analysis with basement has been proposed in peter 2017 in the current version basement is available for both windows and linux based ubuntu environments it is provided with a command line interface cli to easily perform batch simulations but also with a light graphical user interface gui the basement software aims to enable a broad range of potential users to skilfully simulate river processes in the domain of river engineering and research on state of the art computational hardware moreover with accompanying scholar programs and extended documentation material and tutorials the software is designed to be a valuable didactic tool for engineering and river science students the paper is structured as follows 2 provides the software application context that justifies the adopted mathematical and numerical strategies 3 to 5 report the mathematical basis the numerical strategies and main features of the basic modules of basement the software design the modelling workflow and the parallelization solutions are presented in 6 a selection of benchmarks are reported in 7 whilst conclusions and outlooks are drawn in 8 2 application context one of the main goals of the novel software design of version 3 is the capability to tackle river processes at different spatial and temporal scales for example basement can be used to simulate large scale i e basin scale flood propagation but also reach scale morphodynamic processes such as formation and evolution of fluvial bars moreover it can be applied together with high resolution topographies in the order of centimeters to simulate ecohydraulic processes at different ecological scales e g habitat modelling this range of application possibilities is enabled by specific characteristics of the software in particular unsteady and transitional flows basement can deal with strongly unsteady flows and different flow regimes sub and super critical for this reason basement is particularly suitable for simulations of river flows in alpine contexts the propagation of natural flood waves as well as hydropeaking events this is ensured by the adoption of a robust and accurate shock capturing explicit solver for the hydrodynamic problem 4 2 accurate front propagation it is possible to simulate extreme events such as dam break induced floods but also ecologically relevant processes such as the wetting drying of riparian areas and in channel morphologies due to artificial flow alterations this is achieved by an implemented shock wave capturing numerical scheme complemented with a robust treatment of wet dry interfaces 4 2 complex river topographies the use of unstructured grid for the computational domain discretization enables for an accurate description of complex river morphologies and riverine structures 4 1 the adoption of an unstructured mesh also reduces the strong anisotropy of structured meshes which can be crucial for particular applications large problems the software adopts a parallelization strategy tailored to the acceleration of problems on unstructured meshes 6 4 moreover basement simulations can efficiently be executed on different computational backends those backends include gpu cards therefore allowing for the simulation of large domains millions of computational cells on standard workstations having a limited cost multiple river processes the software is designed in a modular way so different river processes such as hydrodynamics sediment transport or advection diffusion of a scalar e g a non reactive pollutant can be simulated by activating specific modules at setup time 6 different types of boundary conditions 5 and closure relationships are available to simulate for example simple hydraulic structures e g weirs or flow inputs outputs e g water intakes the modular design 6 allows to retain good parallelization performances in the simulation of different river processes as shown in 7 7 the basic modules available in basement are i hydrodynamics ii morphodynamics and iii advection diffusion of scalar quantities each module is composed by different sets of hyperbolic equations describing the conservation and evolution of the water flow hydrodynamics the fluvial sediment morphodynamics and the concentration of passive solutes scalar advection diffusion the governing equations represent a so called initial boundary value problem toro 2001 where process specific initial and boundary conditions are required to be set the following sections present the main governing equations and closure relationships 3 the numerical strategies 4 and finally the initial and boundary conditions 5 for the three basic modules the main module features are also listed in table 1 of the supplementary material 3 mathematical formulation 3 1 hydrodynamics the hydrodynamic module solves the so called shallow water equations hereinafter swe e g toro 2001 the two dimensional swe are of practical interest with regard to water flows with a free surface under the influence of gravity considering a cartesian reference system x y z where the z axis is vertical and the x y plane is horizontal fig 1 a the system of governing equations can be written as 1 t h x q x y q y s h t q x x q x 2 h 1 2 g h 2 g h z b y q x q y h g h x z b g h s f x t q y x q x q y h y q y 2 h 1 2 g h 2 g h z b g h y z b g h s f y where the system unknowns are the water surface elevation h m and the two directional components of q q x q y m2 s representing the flow discharge per unit width with z b m we indicate the bottom elevation whilst h h z b m is the water depth and g m s2 the acceleration due to gravity note that the depth averaged velocity vector can be consequently expressed as u u v q x h q y h m s finally s fx and s fy represent the dimensionless friction terms in x and y direction whilst s h m s represents potential external contribution subtraction of flow discharge to the mass conservation equation 3 1 1 hydrodynamic closure relationships to solve the system 1 closure relationships for the friction terms s fx s fy and the contribution of external inflow outflow discharge s h must be provided friction terms under the hypothesis of turbulent flow hence under the assumption that the energy line slope is proportional to the square of the flow velocity the friction terms s fx s fy can be written as 2 s f x u u g h c f 2 s f y v u g h c f 2 where c f is the dimensionless friction coefficient and u is the norm of the velocity vector several formulae are available for c f basement implements four well known formulations of power or logarithmic type given in table 1 external inflow outflow discharge the term s h m s represents additional sources of water like rainfall and springs or water abstraction sink and can be defined over subsets of the computational domain the external water source can be provided by the user as total discharge m3 s or as intensity mm h per squared meter different behaviour can be imposed for each external source sink area exact source sink the exact given water volume is added source or extracted sink from the surface this is the only option for water addition in case of water abstraction the simulation might end abruptly if the available water volume is smaller than the volume prescribed for subtraction this option allows to have the full control on the water entering leaving the computational domain and is useful to simulate e g managed hydraulic structures such as regulated water intakes available sink the given water volume to extract is limited by the available water volume in the single element i e computational cell with this abstraction option the simulation proceeds with no interruptions because the water conservation is ensured this option is useful to simulate particular unmanaged hydraulic structures such as diversion spillways infinity sink all available water will be abstracted from the computational domain 3 2 morphodynamics the basic morphodynamic module solves the so called exner equation exner 1925 it describes the bed evolution due to erosion or deposition which results in the elevation change of the actual bed level z b assuming the same coordinate reference of fig 1a it reads 3 1 p t z b x q b x y q b y s b where p is the bed sediment porosity assumed constant in space and time s b m s is an external source term specifying local inputs or outputs of sediment material e g slope collapse or excavation and q b q b x q b y m2 s is the specific sediment transport flux the morphodynamic module in its basic form accounts only for sediment transport occurring in the form of bed load or total load armanini 2018 parker the simulation of sediment transport as suspended load is delegated to a specific module planned in future versions of the software 3 2 1 morphodynamic closure relationships two closure relationships are needed to numerically solve the governing equation 3 a sediment transport formula and the external source sink of sediments 3 2 1 1 sediment transport formulae basement implements four different types of sediment transport formulae as given in table 2 the first two expressions meyer peter and müller like mpm like and grass like grass like are adequate to simulate bed load dominated sediment transport conditions the engelund and hansen formula allows for the estimation of total sediment transport i e suspended and bed load whilst smart and jäggi is used for bedload transport in steep channels the expressions and the typical parameter values to calculate the specific sediment transport magnitude q b are given in table 2 in the mpm like formulation θ is the dimensionless bed shear stress i e shields parameter armanini 2018 θ cr is the critical dimensionless bed shear stress d m is the representative grain diameter s ρ s ρ is the relative density of the sediment with respect to water the coefficients α m and the critical threshold θ cr can be assigned by the user or adopted from literature see table 2 the grass like model proposes a simple bedload transport formula where q b is a function of the flow velocity magnitude with u cr as critical threshold velocity the coefficients α m and the critical threshold u cr can be assigned by the user or adopted from literature table 2 it is worth remarking that the engelund and hansen formula engelund and hansen 1972 that quantifies the total sediment transport does not prescribe a threshold condition for incipient motion 3 2 1 2 local corrections of the sediment transport the morphodynamic module implements three corrections to the basic exner equation 3 to account for the influence of local characteristics of the flow and the bottom on the sediment transport namely i the influence of local slope on incipient motion ii the effect of lateral bed slope and iii of the flow curvature on the sediment transport direction the threshold condition for incipient motion of grains by shields 1936 is valid for an almost horizontal bed in case of a sloped bed in flow direction or transverse to it the stability of grains is either increased or reduced due to the gravity the critical shear stress value can be adapted consequently to account for the influence of local longitudinal and transversal slopes a common approach is to scale the critical shear stress for almost horizontal bed θ cr with a correction factor k 4 θ c r k θ c r basement implements the correction factor k as proposed in van rijn 1989 and chen et al 2010 implementation details are given in the official documentation the bedload direction can be corrected to account for two relevant morphodynamic processes linked to the slope of the bed and the curvature of the flow the deviation of the bedload direction from the flow direction can thus be modelled as a deviation angle φ φ b φ c sum of the correction angle for bed slope φ b and curvature φ c as depicted in fig 1 b and c the bedload vector is then rotated with the rotation matrix t φ being 5 t c o s φ s i n φ sin φ c o s φ where the angle is positive counterclockwise the angle φ b is estimated with the approach proposed in ikeda 1982 and talmon et al 1995 for the effect of the local transversal bed slope in particular the bedload direction deviates from the flow direction in presence of a local transversal bed slope due to the gravity acting on the bedload sediment particles fig 1b the bed load deviation φ b with respect to the flow is therefore evaluated as 6 tan φ b n l θ c r θ s n q for s n q 0 where n l is an experimental lateral transport factor 0 75 n l 2 63 s x z b y z b is the local bed slope and n q is the unit vector perpendicular to q in downhill direction fig 1b the angle φ c accounts for the effect of a marked flow curvature due to three dimensional spiral flow motion that establishes in curved flows the bed load direction tends to point towards the inner side of the curve while the flow direction points towards the outer side fig 1c this curvature effect is taken into account according to an approach proposed by engelund 1974 where the deviation angle φ c is determined as 7 tan φ c n h r c where h is the water depth n is a curvature factor and r c denotes the radius of the river bend positive for curvature in counterclockwise direction the curvature factor n mainly depends on bed roughness and assumes values n 7 for natural streams engelund 1974 and values up n 11 for laboratory channels rozovskii 1961 3 2 1 4 external sediment input output the source term s b represents additional sediment mass input or output sink that can be defined on subsets of the computational domain the source can be specified as total volume flux including porosity m3 s similarly to the hydrodynamic case 3 1 1 different approaches are adopted for the sediment sink namely exact available and infinity 3 2 2 fixed bed concept morphodynamic simulations generate deposition and erosion patterns of the riverbed erosion processes if not limited can proceed indefinitely in the vertical direction to account for the presence of non erodible river bottom as in case of bedrock or concrete cover a non erodible fixed bed depth ζ rel fig 1a can be set this threshold also determines the volume of sediment available for transport the fixed bed elevation is defined relative to the initial bottom elevation z b with ζ rel 0 3 2 3 gravitational transport several algorithms and approaches have been proposed to simulate the sediment flux contribution generated by gravitational collapse and bank erosion stecca et al 2017 in this version of the software we employed a simple geometrical approximation for gravitational transport assuming that it occurs when the local bed slope expressed as an angle γ between two neighbour computational cells exceeds a given critical angle γ cr this results in a sediment redistribution due to gravity towards the adjacent cells to restore a stable local slope i e γ γ cr further implementation details are provided in the official documentation 3 3 scalar advection diffusion a number of environmental processes such as pollutant temperature or nutrient transport can be modelled assuming the passive advection and diffusion of a scalar quantity in the form of dissolved or particulated species e g vanzo et al 2016 the scalar advection diffusion module allows for the simultaneous simulation of multiple passive species up to a maximum of 5 the maximum number of species is limited for purely computational efficiency reasons i e to limit the memory requirements of this module based on our experience we consider 5 species a suitable limit for a broad range of applications the transport of a generic species c can be described by the following advection diffusion equation 8 t q c x q x q c h h k x x x ϕ c k x y y ϕ c y q y q c h h k y x x ϕ c k y y y ϕ c s ϕ c with c 1 5 where the unknown is q c the specific mass of the species c it can be expressed as q c h ϕ c with ϕ c the volumetric concentration and h the water depth the term s ϕ c is a net source of c and k ij m2 s are the components of the 2d diffusion tensor 3 3 1 scalar advection diffusion closure relationships for the scalar advection diffusion module the closure relationships are used to model the contribution of external scalar input and output in particular the term s ϕ c represents an additional scalar mass flux that can be added within portions regions of the computational domain the source can be specified either as an imposed concentration value or a total volumetric flux m3 s the behavior is analogous to the case of hydro and morphodynamic sources s h and s b 3 1 1 and 3 2 1 the terms k ij of the diffusion tensor vary considerably with respect to the physical nature of the transported species diffusive transport is modelled in terms of both molecular diffusion k m and turbulent dispersion k i j t such that k i j k m i i j k i j t with i ij the identity matrix the molecular diffusion is assumed as an isotropic fickian process with constant coefficient k m turbulent dispersion is anisotropic k i j t and scales with the friction velocity u u c f and water depth via a longitudinal α l and transversal α t non dimensional coefficients suitable values for open channel flows in natural environments are α l 13 and α t 1 2 vanzo et al 2016 4 numerical solution the numerical solution of the governing equations 1 3 and 8 is sought in a finite volume framework with a spatial discretization based on unstructured meshes 4 1 for the temporal integration an explicit first order euler scheme is used in its basic configuration the temporal integration proceeds in a synchronous decoupled way for all the modules meaning that the modules are independently integrated in time with the same timestep 4 5 the following sections detail the domain discretization strategy and the adopted numerical solver for the fluxes calculation of the three basic modules the interested reader should refer to the provided references for specific implementation details 4 1 domain discretization the problem is discretized adopting a finite volume approach over unstructured triangular meshes a conforming triangulation t ω of the computational domain ω r 2 by elements ω i such that t ω ω i is assumed given a finite volume element ω i fig 2 j 1 2 3 is the set of indexes such that ω j is a neighbour of ω i γ ij is the common edge of two neighbour cells ω i and ω j and l ij its length n ij n ij x n ij y is the unit vector which is normal to the edge γ ij and points toward the cell ω j 4 2 hydrodynamics the system of governing equation 1 can be cast in vectorial form as 9 t u x f x y f y s where left handside terms of 9 are 10 u h q x q y f x q x q x 2 h 1 2 g h 2 g h z b q x q y h f y q y q x q y h q y 2 h 1 2 g h 2 g h z b the vector of source terms can be written as s u s h s f r u s b e d u where 11 s h s h 0 0 s f r 0 g h s f x g h s f y s b e d 0 g h x z b g h y z b by integrating the governing system of equation 9 in the control volume v ω i t n t n 1 we obtain the general update formula for the triangular element i 12 u i n 1 u i n δ t ω i j 1 3 l i j f i j δ t s i problem unknowns at cell i and discrete time n are represented by cell averages u i n the numerical solution sought at time t n 1 t n δt is denoted by u i n 1 in 12 f ij are the hydrodynamic fluxes estimated at the cell interface ij fig 2 to compute the fluxes f ij for the hydrodynamic system 9 several well established solvers are available here we adopt the well known hllc approximate riemann solver toro et al 1994 which is a modification of the basic hll scheme to account for the influence of intermediate contact waves further details on the hllc approach are available in chapter 10 of toro 2001 the solver is proved to be robust and efficient in simulating unsteady flows and the advection of passive tracers vanzo et al 2016 the numerical discretization of the three terms of s u 11 is conducted separately according to the nature of each term the external inflow outflow contribution s h is added explicitly to the continuity equation as it is not a function of the problem unknowns the stiff friction source terms s fr u are integrated with runge kutta 2 e g toro 2009 in a semi implicit fashion after adopting a splitting technique the implementation is analogous to the ones proposed in siviglia et al 2013 vanzo et al 2016 vanzo 2015 the topographical terms s bed u are discretized using the modified state approach proposed by duran et al 2013 this results in an easy and robust treatment of complex topographies and wetting and drying problems vanzo et al 2016 4 3 morphodynamics the exner equation is solved in a synchronous decoupled way with respect to the shallow water problem 4 2 meaning that the numerical integration of the exner equation 3 uses the same integration timestep δt of the hydrodynamic problem the general update formula for the exner problem reads 13 z b i n 1 z b i n 1 1 p δ t ω i j 1 3 l i j q b i j δ t s b i with the same symbols introduced for 3 the term q bij represents the normal sediment flux at the cell interface ij fig 2 for the numerical estimation of the term q bij a number of approaches are available in literature in the current version basement implements an approximate riemann solver of hll type sensu toro 2001 as in soares frazão and zech 2011 the sediment flux is thus calculated as 14 q b i j λ s q b i λ s q b j λ s λ s z b j z b i λ s λ s where pedix i j refers to quantities evaluated at the corresponding cell fig 2 and λ s λ s are speed estimations of the morphological problem we adopt the following speed estimates soares frazão and zech 2011 15 λ s m i n λ 1 i λ 1 j λ s m a x λ 2 i λ 2 j the expression for the terms λ 1 and λ 2 calculated for both cell i or j reads 16 λ 1 2 1 2 u n c u n c 2 4 q b n q n c 2 where u n is the normal velocity at the cell interface ij fig 2 and c g h is the so called wave celerity 4 4 scalar advection diffusion the scalar advection diffusion problem is solved in a synchronous decoupled way with respect to the shallow water problem 4 2 we reformulate the governing equation 8 via a cattaneo type relaxation technique as proposed by vanzo et al 2016 two additional scalar conservation equations are then added to 8 namely 17 t ψ x c x ϕ c ε ψ x c ε t ψ y c y ϕ c ε ψ y c ε where ε is a positive and small relaxation time whilst ψ x c and ψ y c are two auxiliary variables that recover x ϕ c and y ϕ c respectively for a sufficiently small ε vanzo et al 2016 after a trivial substitution of ψ x c x ϕ c and ψ y c y ϕ c into 8 the system composed by 8 and 17 can be rewritten in vectorial form as 18 t q x a x y a y x d x y d y s c s r e l where the vectors q a x d x s c and s rel read 19 q q c ψ x c ψ y c a x q c q x h 0 0 d x h k x x ψ x c k x y ψ y c q c ε h 0 s c s ϕ c 0 0 s r e l 0 ψ x c ε ψ y c ε with q representing the conserved scalar quantities whilst a x is the advective fluxes vector and d x is the diffusive relaxed fluxes vector both in x direction the scalar source terms are s c whilst the source terms arising form the relaxation are s rel for brevity we omit the formulation for the y direction a y and d y which is analogous the interested reader can refer to vanzo et al 2016 for a step by step derivation the scalar fluxes in 19 are solved through the svt solver introduced by vanzo et al 2016 the scheme presents a flux splitting approach combining the advective and diffusive relaxed fluxes evaluated with different solvers the hllc solver applied for the hydrodynamic fluxes 4 2 provides the advective component of the scalar fluxes at the cell interface a ij for the diffusive relaxed component the svt technique derives the fluxes at the interface d ij directly from the riemann invariants of a two non linear waves riemann problem similarly to the hydro and morphodynamic problems the control volume v ω i t n t n 1 is used to integrate the governing system 18 in order to obtain the following scalar update formula at the element i 20 q i n 1 q i n δ t ω i j 1 3 l i j a i j d i j δ t s c s r e l i where the fluxes a ij and d ij are computed at each cell interface ij fig 2 the numerical integration of the two source term vectors is conducted separately according to the nature of the terms the scalar sources s c are computed with a first order explicit euler scheme while the stiff relaxation source terms s rel q are integrated by means of a locally implicit euler method 4 5 stability condition numerical integration proceeds with a dynamic timestep δt evaluated at each time loop fig 6b that fulfills the well known courant friedrichs lewy stability condition toro 2001 in the current implementation the condition is expressed as 21 δ t c f l min 1 i n min 1 j 3 ρ i j λ i j where ρ ij is twice the distance between the edge j and the centroid of the cell i fig 2 and n is the total number of domain elements the term λ ij is an estimation of the largest eigenvalue of the hydrodynamic problem 1 namely λ i j u n g h with the symbology already introduced the cfl coefficient ranges between 0 and 1 by default it is set to 0 9 if not specified otherwise 5 initial and boundary conditions 5 1 initial conditions all modules require the user to define the initial conditions of the simulation two types of initial conditions are similarly available for all the modules region defined user explicitly defines the initial values of the problem unknowns e g water depth and specific discharge for hydrodynamics different values can be assigned to different region of the computational domain continue values are taken from the result file of previous simulations in addition the hydrodynamic module allows also to set dry conditions no water in the domain as initial conditions in this case the domain will progressively fill with water in relation to the assigned inflow boundary conditions or internal sources 3 1 1 5 2 boundary conditions the boundary conditions hereinafter bcs have different specifications for each core module see following sections but they all classify in three common types external standard external linked and internal bcs fig 3 exemplifies the main concepts adopted for the bcs the computational domain ω is defined by the domain boundaries as γ1 2 3 an external standard bc is dependent only on the local flow conditions and on some user defined rules this represents the most common case for example to define impermeable walls or river inflows and outflows by default all the external boundaries are set as wall the wall bc consists of a fixed frictionless inviscid reflective impermeable wall in external linked bcs instead the local bcs are defined also with information from a linked boundary typical example is a weir where the flow discharge at the downstream side of the weir depends on the water stage on the upstream side the third type of bcs internal are defined within the computational domain ω and not at the edges fig 3 this bc type comes in handy in case of very large domain application because it allows to test different configurations of hydraulic structures e g different locations of a weir or training wall without the need of regenerate the entire computational mesh for every configuration a summary of the main features of the bcs for the three core modules follows here the interested reader can refer to the official documentation for further details 5 2 1 hydrodynamic bcs the hydrodynamic module implements different types of bcs with a different level of customization depending on the bc type user assigned data is requested as single constant value in time e g lake level constant discharge as time series e g hydrograph or as set of parameters describing a dynamic behaviour e g weir activation rule in particular standard bcs in addition to wall bc inflows upstream bcs and outflows downstream bcs can be assigned as standard inflows three options are provided with uniform or explicit options the user provides a total volume discharge q m3 s whilst with zhydrograph the water surface elevation m must be provided for the standard outflows a value for the water depth h must be specified possible options are uniform conditions hydraulic weir rating curve zhydrograph i e water surface elevation and zero gradient i e neumann bc it is worth remarking that the specific type of upstream and downstream bcs should be selected depending on the local flow conditions i e sub or super critical linked bcs this type of boundaries establishes a link between two certain region of the domain where the governing equations are not solved it is specifically designed to simulate the behaviour of hydraulic structures within the river channel such as weirs gates bridges spillways internal bcs they are fictitious boundaries defined as segments at the interfaces of some computational cells on these segments three different conditions can be enforced instead of the solution of the swe 1 options are static walls dynamic walls and rating curve with the static wall the standard wall condition is applied on both sides of the internal boundary this option is useful for example for easily testing the presence of barriers e g training wall in fig 3 without the need of reconstructing the numerical domain with the dynamic wall the wall conditions are applied until a given threshold is reached after which the wall is removed and the swe are solved instead the threshold can be set as a time value i e wall removal at a given time or as water depth removal when a given water depth is reached on one side of the bc this feature comes in handy to simulate for example the collapse of some hydraulic structures with the rating curve option i e h q relation an unidirectional flow is applied through the internal boundary based on the water stage in the upstream side of the boundary as for the standard and linked rating curve bcs 5 2 2 morphodynamic bcs the sediment flow is defined as a specific bedload flux which is averaged and evenly distributed at the domain boundary conditions over the boundary length in analogy with the hydrodynamic module the morphodynamic boundaries are of type external standard and linked standard bcs for the upstream bcs basement implements three versions that allow to simulate i a given input of sediment as time series i e sedimentograph ii a sediment input derived from the hydrodynamic conditions under transport capacity conditions or iii bed equilibrium condition where the upstream bed elevation is kept constant two downstream bcs are available allowing the simulation of i equilibrium condition and ii check dam in this second option an equilibrium boundary condition is activated only if the bed level reaches a given threshold value otherwise a wall type boundary is assumed linked bcs one bc is available it allows for the simulation of sediment transport through given hydrodynamic linked conditions hence to ensure sediment continuity in the simulated channel 5 2 3 scalar advection diffusion bcs scalar bcs are defined in terms of concentration of total volumetric rate m3 s evenly distributed throughout the length of the relevant domain boundary the implemented types are standard bcs three types are available i scalar inflow as a constant value ii scalar inflow as a time series and iii zero gradient i e neumann bc outflow 6 software design 6 1 modelling workflow the standard modelling procedure involves three phases the pre processing the numerical simulation and the post processing phase fig 4 basement is designed to integrate into this workflow moreover the entire workflow relies on open source or freeware tools in the following we list the phases and provide a short description of the different configuration and results file formats as used by basement 1 pre processing in this phase the user is required to define the model domain and the input data the mesh file customized 2dm format mymesh 2dm in fig 4 contains the description of the triangular unstructured computational mesh the file can be generated with basemesh an open source python module as well as a qgis plugin see the official repository for details or via grid generator software that supports the 2dm format in addition further input data such as time series of water and sediment discharge or other quantities to be used as bcs can be provided ascii format mydata txt in fig 4 2 numerical simulation the actual simulation can be run via either cli or gui in basement version 3 the numerical simulation is split into three steps description follows in 6 2 the final simulation results are stored in a general purpose binary container hierarchical data format hdf5 www hdfgroup org basement generates also an xdmf file extensible data model and format http www xdmf org which contains a machine readable description of the data stored in the hdf5 file 3 post processing the xdmf file output xdmf in fig 4 can be opened with the crayfish plugin for qgis or with paraview for final results visualization and further post processing in addition ad hoc python scripts can be used to manipulate results directly from the binary container some scripts are provided at the software website 6 2 simulation steps the numerical simulation phase consists of three steps the pre simulation the simulation and the post simulation fig 5 each step can be completed by running a corresponding basement executable via gui or cli this modular design allows a customization of the simulation workflow by the user and an efficient batch processing of basement steps for instance the programs can be run from a scripting language like python the different executables are configured using a dedicated command file in standardized json file format javascript object notation fig 5 the basement gui is designed to support the user with creating the command files and running and monitoring the three simulation steps in particular the gui validates the configuration parameters and automatically adds required parameters where default values are available the three simulation steps are detailed as follows 1 the pre simulation step focuses on the model definition in particular the model json command file contains i physical properties ii initial conditions and iii boundary conditions of the physical problem and further iv numerical parameters the setup executable first reads the computational mesh mymesh 2dm the external required data mydata txt and the command file model json then it validates and stores the model inside the binary container setup h5 2 the simulation is carried out on a selected computational backend 6 4 it is driven by the command file simulation json that contains the simulation parameters such as the total simulation time the output timestep constant and the desired output quantities the program reads and executes the model setup h5 generated in the previous step the results of the simulation are stored in a second binary container results h5 the user can monitor the simulation execution on the gui log terminal where the simulation progress as the current integration timestep and an estimation of the real time speed rts of the simulation ratio of simulated time over computational time are provided 3 the post simulation step is configured using the command file results json that contains the selected output format currently only xdmf is supported the output is then available for the post processing phase 6 1 when it is necessary to run a new simulation starting from the results of a previous one two options are available restart and re run when performing a restart the pre simulation step is executed again i e a new model is generated from scratch with potentially a new set of parameters the user indicated an existing results h5 file that is to be used to fetch the initial conditions for the new model the re run option does not generate a new model but uses the existing model setup h5 with initial conditions taken from the current results file in this scenario the user can only modify the simulation and results parameters i e duration output timestep and output type but not the model parameters this second option is particularly useful in the case of large models i e millions of computational cells because the pre simulation step can take up to tens of minutes if the user only needs to extend the simulation duration for example then the re run option allows to skip the pre simulation step 6 3 modularity and sequencing basement aims to simulate different river processes with a high level of flexibility and efficiency with this in mind we designed the software adopting a modular approach the two core concepts of this design are modules and kernels described as follows modules take care of the simulation of specific river processes e g module hydrodynamics in fig 6a they can be nested to simulate processes with an increasing level of detail complexity e g module hyd external source fig 6a modules are activated by the user in the pre simulation step an activated module triggers the execution of a number of kernels throughout the simulation a kernel is a set of operations to be executed on each entity e g a cell i or an edge j fig 2 of the computational domain or a subset of it depending on the specific task kernels can be scheduled for a single execution i e initialization kernels or for repeated execution in each iteration of the integration time loop fig 6b the global time loop is executed with a timestep δt that satisfies the stability condition of the hydrodynamic problem 4 5 some modules and their associated kernels can be scheduled by the user for a delayed start or for execution at different time intervals larger than the global integration timestep δt for example the user can set a delayed starting time in seconds of morphodynamic and scalar transport modules typical usage is to ensure steady hydrodynamic initial conditions before starting the morphodynamic or advection diffusion simulation for other modules such as the gravitational transport or the flow curvature calculation the user can define also the execution interval in seconds this feature allows to reduced the computational efforts when the integrated processes are not subject to the hydrodynamic timestep constrain 4 5 by default all modules are executed from the beginning of the simulation and at each global integration timestep the architecture based on modules and kernels has two main advantages first it is flexible in that it allows users and software developers to easily add or remove specific modules without interfering with other existing modules in particular this permits an integration of further modules as development continues 8 second it is efficient because only the necessary kernels are scheduled for execution at setup time pre simulation step 6 4 parallelization strategy and computational backends the parallelization strategy of the basement numerical core addresses two main aspects i the use of different technologies i e computational backends generated from the same unique software source code this allows for an easier source code maintenance and integration of future different backends ii an efficient and heavy parallelization of the numerical core following the concept of data parallelism to this end the numerical core of basement integrates op2 mudalige et al 2012 giles et al 2012 which is an open source framework for the development of unstructured grid applications using source to source translation op2 generates the appropriate code for different target platforms by introducing an additional level of abstraction between the numerical algorithm and its execution it supports multi core cpus gpus and even clusters via mpi message passing interface http www mpi forum org basement currently supports multi core cpus and gpus when starting the simulation the user can select to compute on the cpu the gpu or a combination of both all the currently supported backends table 3 are available for both windows and linux ubuntu operating systems it is important to note that the choice of graphics processing units is currently limited to nvidia cuda cards the precise requirements are provided in the official documentation all the backends can execute the numerical simulations in double default or single precision with different performance 7 7 7 results a set of selected test cases t1 t6 are proposed here to test the robustness accuracy and efficiency of the three basic modules table 4 summarizes the key features of each test case the interested reader can refer to the official software documentation for further examples finally 7 7 focuses on the software performance and scalability all the test cases are freely available at eth zürich research collection doi https doi org 10 3929 ethz b 000482308 7 1 t1 malpasset dam collapse the scope of this test is to assess the robustness and accuracy of the hydrodynamic solver when simulating a shock type hydrodynamic wave travelling on a highly irregular and dry domain the collapse of the malpasset dam in the reyran river valley fréjus france represents a well established hydrodynamic benchmark for numerical models e g hervouet and petitjean 1999 singh et al 2011 valiani et al 2002 in 1959 the 66 5 m high dam collapsed almost instantaneously generating an up to 40 m high flood wave that propagated down the reyran valley destroyed the two villages malpasset and bozon and reached the mediterranean gulf 21 min later valiani et al 2002 the propagation of the flood wave was reconstructed via the maximum water level and the flood arrival time recorded at multiple locations in particular the maximum water level is available from a police survey for 17 survey points marked as p1 to p17 in fig 7 and the flood arrival time is known from three electric transformer stations which have been destroyed by the flood wave the locations of the transformer stations are indicated as a b and c in fig 7 coordinates and recorded arrival times are listed in table 5 we make use of such field data to test the performance of the hydrodynamic module the computational domain is discretized with 499 059 triangular elements the domain boundaries are set to walls 5 2 1 with exception of the downstream boundary located in the mediterranean gulf where a fixed water level was set to 0 m zhydrograph the initial conditions are a fixed water surface elevation of 100 m in the reservoir and dry conditions in the rest of inland domain region defined ics the initial velocity was set to 0 0 m s in the entire domain in accordance with hervouet and petitjean 1999 the manning s friction coefficient was set to 0 033 m 1 3s for the whole domain the cfl number was set to 0 9 the simulated maximum water levels are compared to the 17 field observations in fig 8 with overall good agreement the average relative error is 7 15 with the largest observed at p13 with an overestimation of 30 6 to highlight the effects of the topographical approximations of the digital elevation model and hence of the computational mesh we compared recorded and simulated water level values as follows for each punctual maximum water level recorded in field observations blue triangles in fig 8 we compared the maximum simulated values of the spatial mean maximum and minimum among the computational cell containing the observation point and its three neighbours black and red series in fig 8 we expect lower discrepancies between recorded and simulated values where the numerical values hence the topographical elevations are spatially homogeneous as a matter of fact points p1 p7 and p13 fig 8 have the large discrepancy between measured and simulated values but also the largest spatial variability of the numerical values red shaded area this suggests that such discrepancies relates more to the local topographical approximations of the dtm rather than to the numerical model observed and simulated times of flood arrival are given in table 5 simulated values are in good agreement with measured ones for all the electrical transformer stations et simulated arrival times have a maximum relative error of 3 8 for et b corresponding to an absolute delay of 47 s it is worth mentioning that the friction value influences the simulated arrival times 7 2 t2 propagation of a sediment bore scope of the test is to assess the robustness of the de coupled hydro morphodynamic solver approach particularly when simulating the sediment transport over a transcritical flow this represents a critical test especially when adopting de coupled approaches e g cordier et al 2011 moreover the simulation tests the morphological solver capability in well reproducing the dynamics of an advancing sediment bore in this test case the flume experiment proposed in bellal et al 2003 run 2 is reproduced numerically the computational domain is a composed by a straight 6 9 times 0 5 m channel representing the lower part of the original experimental flume and it is discretized with 24 612 triangular elements the sediment has a characteristic diameter of 1 65 mm the water and sediment discharge at the upstream boundary are set to 0 012 m3 s 1 uniform bc and 0 196 m3 s 1 with porosity sedimentograph bc respectively the flume is at initial uniform flow conditions characterized by a supercritical flow at t 0 s a fixed water level of 0 2093 m is imposed at the downstream boundary zhydrograph and sediment transport out of the domain is stopped wall this results in the formation of an hydraulic jump moving upstream in the flume and a subsequent downstream propagation of a sediment bore the cfl number is set to 0 9 the bed porosity is assumed constant and equal to 0 42 and the simulation duration is 500 s fig 9 shows the initial and final profiles of the simulated bed and water elevations the solver reproduces well the sharp transition between super and sub critical flow conditions the position of the sediment front in time is shown in fig 10 with good agreement between simulated and experimental values 7 3 t3 dam break over a mobile bed with a sudden enlargement scope of the test is to assess code robustness in simulating sediment transport at wet dry interface and the accuracy in reproducing scour deposition patterns the experiment illustrated in goutiere et al 2011 represents a well know morphodynamic test for numerical models e g juez et al 2014 siviglia et al 2013 soares frazão and zech 2011 the domain consists of a flat flume with a non symmetrical sudden enlargement fig 11 the bed is composed of a coarse uniform sand with a median diameter of d m 1 82 mm the initial conditions are defined by an horizontal layer of fully saturated sand of thickness 0 1 m over the whole domain and an initial water storage of depth 0 25 m upstream of the dam located at section x 3 0 m region defined ics at time t 0 s the dam is suddenly removed resulting in the propagation of a dam break wave with consequent sediment transport the computational domain is discretized by unstructured triangular cells at different resolutions follows in table 6 inviscid wall boundary conditions are set at the upstream and lateral domain boundaries while a free outflow condition zero gradient and constant bed elevation equilibrium conditions are used at the downstream outlet the manning coefficient is set to 0 0167 m 1 3s the sediment density and porosity are set to 2680 kg m3 and 0 47 respectively the sediment transport is evaluated with the mpm like formula table 2 setting θ cr 0 0495 for the critical shields stress α 3 97 and m 1 5 for the remaining parameters wong and parker 2006 the cfl number is set to 0 9 the numerical simulations last 12 s the evolution of the water elevation during the simulation is shown in fig 12 for the six survey points the simulated series show a fairly good agreement with the experimental values the dam break wave arrival time is well captured and the maximum elevation values are comparable with the measured ones moreover the simulated series show minor discrepancies with the experimental ones after the arrival of the first wave as already pointed out by previous works siviglia et al 2013 xia et al 2010 discrepancies are due to the extremely complex flow pattern generate by multiple wave reflections while simulation proceeds in time which potentially generate tri dimensional flow structures nevertheless obtained series are coherent with the ones of siviglia et al 2013 where a second order accuracy model was employed numerical bed elevations after 12 s are compared to the experimental results in fig 13 the simulated scour and deposition patterns are well reproduced the magnitude of the scour at cross section cs1 at y 0 25 m matches well with an underestimation of the deposition pattern at y 0 35 m at cross section cs2 the simulated deposition magnitude matches well with the experimental one but with a small shift toward the lateral boundary 7 4 t4 scour and deposition on a channel bend the scope of the simulation is to test the correct reproduction both in term of positioning and magnitude of a river point bar generated by a channel bend in this test we numerically reproduced one experiment from yen and lee 1995 already adopted as morphodynamic benchmark test e g villaret et al 2013 the flume is u shaped with a bend of 180 having a constant radius along the center line of r c 4 m the cross section is rectangular with width w 1 m and slope s 0 2 the two straight reaches before and after the bend are 11 5 m long the median diameter of the bed material was d m 1 mm in the experimental run the flume was fed with a simplified triangular flood hydrograph having a base flow of 0 02 m3 s 1 and peak flow of 0 053 m3 s 1 the rising and falling limbs last 100 and 200 min respectively afterwards a constant baseflow was kept for another 100 min during the experimental run a steady point bar in the inner side of the bed develops and grows with a corresponding erosion on the outer side in the numerical setup the domain is discretized with 24 523 computational cells we set the porosity to 0 4 and used the mpm like formula with parameters from wong and parker 2006 as in table 2 the lateral slope factor n l is equal to 1 4 and the curvature factor n is set to 11 6 and 7 at the numerical domain boundaries uniform flow and equilibrium sediment transport conditions are imposed the simulation as the experimental run lasts 400 min a planar view comparison between numerical and experimental run is depicted in fig 14 the final bed change with respect to the initial flat configuration δz b is scaled with the approaching i e upstream reach flow depth h 0 the magnitude of scours and depositions for the numerical run ranges between 0 75 and 0 75 matching fairly well with the experimental values also the positioning of the point bar with the maximum deposition anticipating the middle of the bend 90 is well reproduced numerically fig 15 shows the cross sectional profile of the relative bed change for the numerical simulation and the experimental run in the middle of the flume bend at 90 the numerical profile reproduces well the experimental trend this test demonstrates the software capability in simulating an unsteady morphological process i e a point bar development in a meander during a flood such process can be well reproduced only by implementing suitable corrections of the sediment transport direction due to gravity and curvature as presented in 3 2 1 7 5 t5 steady scalar discontinuity with two diverging hydrodynamic waves the test assesses the correct advection of scalar concentration this is assessed with a challenging test a steady discontinuity of a scalar concentration subjected to strongly variable flow conditions the chosen test is an idealized one dimensional problem but nevertheless it is particularly challenging for a pletora of numerical schemes toro 2001 the domain is a simple flat bed channel 100 m long and 0 1 m wide the domain is deliberately chosen very narrow to mimic a 1d setup given that the exact solution of the problem is available in one dimension case as initial conditions region defined ics the water depth h is set even in all the domain whilst the initial longitudinal specific discharge q x and the concentration of a generic scalar ϕ present a discontinuity 22 q x 3 0 m 2 s if x 50 m q x 3 0 m 2 s otherwise ϕ 1 0 if x 50 m ϕ 0 0 otherwise h 1 0 m x the domain is discretized with 1362 computational cells lateral walls are reflective and inviscid whilst transparent boundary conditions zero gradient are set at beginning and end of the channel for both hydrodynamic and scalar transport modules the cfl is set to 0 95 and the simulation timeout is t 2 5 s as the simulation starts two strong rarefaction waves start diverging from the center of the domain towards the two extremities suddenly forming a water depression in the center despite the strong unsteadiness of the hydrodynamic quantities during the simulation a steady contact wave persists in the domain avoiding the scalar quantity to mix in the domain the numerical solution at simulation timeout is compared with the exact solution of the problem in fig 16 the hydrodynamic exact solution is obtained by resolving the two rarefaction riemann problem toro 2001 whilst the exact solution for the scalar advection is identical to the given initial conditions fig 16 underlines how the numerical solution correctly approximates the exact solution in all the domain sections the scalar discontinuity is perfectly maintained throughout the simulation confirming the accurate resolution of the steady contact wave 7 6 t6 scalar advection and diffusion in a dam break over a complex domain with this test we assess the solver capability in correctly preserving the liquid and scalar species mass when simulating the advection and diffusion of species during a dam break phenomena the test is particularly harsh due to the presence of fast wetting drying fronts and multiple discontinuous flow regions the test is an ad hoc setup inspired to a common benchmark for hydrodynamic codes e g brufau et al 2002 vanzo et al 2016 the domain is composed by a rectangle 0 75 15 15 m the bottom η x y is fixed during the simulation and defined as 23 η x y max 0 η 1 x y η 2 x y η 3 x y with η 1 x y 1 1 8 x 30 2 y 9 2 η 2 x y 1 1 8 x 30 2 y 9 2 η 3 x y 3 3 10 x 47 5 2 y 2 the initial conditions are given by 24 h 1 0 m if x 16 m h 0 125 m otherwise ϕ 1 0 5 if x 16 m ϕ 1 0 otherwise ϕ 2 0 if x 16 m ϕ 2 0 1 otherwise q x q y 0 m 2 s x y presenting a virtual dam at x 16 m separating two discontinuous volumes of water and scalar mass here the domain is discretized with 492 277 triangular cells with a maximum of characteristic length of 0 1 m the hydrodynamics setup features reflective wall boundaries a cfl of 0 95 and frictional sources compatible with a manning coefficient of n 0 01 m 1 3s the scalar setup features two initially unmixed species both with a constant and isotropic diffusion coefficient k c 0 25 m2 s fig 17 a illustrates the hydrodynamic left and scalar solutions right at the initial condition t 0 s at simulation start the virtual dam collapses instantaneously with an advancing wave that overtops the two small lateral humps fully circumvents the larger hump and reaches the opposite wall in about t 20 s at this time the interface between the two species in what would otherwise be a contact discontinuity on flat topography is still lagging by approximately 15 m fig 17b at this point the reflected bores propagate upstream and further mix both species symmetrically around the x axis by t 50 s these bores overcome the two smaller obstacles and propagate upstream on flat ground fig 17c after a continuous sloshing and interaction of reflected waves topography and lateral walls the friction sources gain relevance and dissipate most of the kinetic energy in the flow with a near static solution being obtained at approximately t 20 min the scalars continue to mix now due mostly to molecular diffusion in what is a much slower process that only vanishes at around 3 5 h as both scalars become fully homogeneous across the domain fig 18 the model is fully conservative with the total liquid and scalar mass preserved during the entire simulation as the simulation approaches the lake at rest conditions the observed quantities correctly converge to their resting values of h 0 364 m ϕ 1 0 386 and ϕ 2 0 023 fig 18 7 7 performance and scalability the performance and scalability of the software depends not only on the implemented parallelization strategies but also on the physical model to be reproduced in general models with only few simulated physical processes are likely to show higher computational performances to test basement s computational performance we selected the benchmarks t1 hydrodynamic t3 morphodynamic and t6 scalar advection diffusion each of the selected numerical experiments t1 t3 t6 has been conducted with four different computational meshes the sizes of these meshes are given in table 6 and have been chosen to cover a broad range of spatial resolutions ranging from thousands to hundred of thousands computational cells the simulations have been run with a set of computational backends in particular cpu based simulations have been performed on an intel xeon gold 6154 3 00 ghz workstation equipped with 36 cores two sockets with 18 physical cores each whilst gpu based simulations have been run on three gpus geforce gtx 1050 ti geforce gtx 1080 ti and tesla p100 see table 7 for the main characteristics moreover the simulations have been benchmarked in both single and double precision mode the gpus were integrated in a workstation with a 32 core intel xeon gold 5218 2 30 ghz processor two sockets with 16 physical cores each for a given mesh size the speedup achieved by a parallelized backend p is computed using the formula speedup t s t p where t s t p is the total computational time used by the serial parallel backend the results for all the investigated benchmarks are depicted in fig 19 as anticipated the speedup depends on the simulated processes comparing the speedup values among different benchmarks in fig 19 cases t1 hydrodynamics and t3 morphodynamics show on average higher values than t6 advection diffusion such results are expected given the increased complexity number of equations and operations to be solved of t6 the performance benefits of basement s parallelization can be evaluated in more detail by comparing the speedup values along the vertical axis of the plots in the following we focus on benchmark t3 morphodynamics which shows an intermediate scalability among the three benchmarks fig 19b looking at the mesh with 47k elements as an example the cpu based family i e openmp on multiple cores shows a speedup efficiency i e speedup over number of cores of 87 with 2 cpu cores speedup 1 7 and of 58 with 32 cores speedup 18 5 with an average efficiency of 74 basement performs even better on some gpu cards the least performing card gtx 1050 ti with double precision has a speedup of about 8 however note that speedup jumps to 20 when using the single precision version overall the speedup provided by the tested gpus ranges between 7 and 60 the benchmarks in fig 19 also show how the maximum speedup changes with mesh size computational backend and simulated processes for all three cases the cpu based parallelizations show a mild speedup increase with an increasing number of computational cells the dependency on the mesh size is slightly more pronounced when the number of computational cores is increased this reflects the fact that cpu based solutions have shared memory and minimal overhead for multi threading handling thus the domain size i e the data size does not represent a potential performance bottleneck focusing on t3 fig 19b the parallelization efficiency for 2 4 cpu cores is almost constant for all mesh sizes and above 80 on the other hand the efficiency for 16 32 cpu cores is larger than 70 only for mesh 4 218k the speedup of the gpu accelerated solutions shows not only a more marked dependency on the problem size but also on the simulated processes in benchmark t1 fig 19a the speedup clearly increases with problem size this can be explained with the overhead of gpu parallelization which becomes more and more negligible with increasing domain size conversely benchmark t6 fig 19c shows little impact of the domain size on the speedup in this case the scalability is limited by data transfers i e data bandwidth test t6 has the largest amount of data compared to test t1 and t3 because of the simulation of the 5 species 3 3 due to the current design of the op2 framework dataset dimensions e g the maximum number of species are statically assigned at compile time increasing this maximum value would be at the expense of scalability of the computational backends it is worth remarking that the gpu accelerated backends show an average speedup difference greater than 10 between the single and double precision versions of course the adequate choice depends on the requirements of the specific application the analysis above shows basement s performance on different computational backends and underlines the differences when simulating different processes the results summarized in fig 19 can also serve as a guideline for the interested reader user when choosing an appropriate computational configuration for a given application finally it is worth highlighting that all the tested hardware configurations can be easily installed in standard office workstations 8 conclusions in this paper we introduced the main features of basement version 3 a freeware tool for river simulation basement allows the simulation of a wide variety of hydro morphodynamic and scalar advection diffusion scenarios as illustrated with the test cases the software is able to efficiently capture large scale hydrodynamic processes modelled with several hundreds of thousand elements in good agreement with the measurements on the opposite end of the spectrum the morphological solver is able to handle demanding sediment transport scenarios well albeit with known limitations with the scalar advection diffusion module a further set of physical processes such as the fate of river pollutants can be accurately modelled the impact of this flexibility on the software performance is minimized by activating modules on request in basement s pre simulation step the advantage of this approach is that only the required kernels are scheduled for execution this together with op2 s ability to generate executable code for both multi core cpus and gpus permit basement to scale with both available features and available computational power such advantages are reflected in the presented benchmarks given a large enough domain the software shows a good parallel efficiency on the cpu and an even higher speedup when using gpus the basement project is in continuous advancement to optimize and include further features in the existing basic modules as an example the modelling of the sediment transport in presence of non uniform sediment size and the simulation of water temperature dynamics are in implementation phase on the other hand efforts are dedicated also to develop novel modelling solutions for river processes such as the bio morphodynamic feedback between vegetation and sediment transport table 2 of the supplementary material provides an overview of under development features the modularity of the development framework allows also for further refinement of single specific numerical solvers and the implementation of high order schemes when needed overall the combination of different river processes that can be modelled the computational efficiency the flexibility in the backend choice but also the availability of a light gui make basement a valuable tool for a broad family of river modelers in both academia and practice funding the development of the software basement is financially supported by the swiss federal office for the environment foen declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors thank the reviewers for the fruitful comments and suggestions the authors greatly thank the many former collaborators and developers within the basement project particular thanks to aurélie koch for her valuable contribution in testing and documenting the software the design of basement was conducted by dfv dv sp lv the software prototyping development and implementation was done by dv sp lv mb mw with the coordination and supervision of dfv and as implementation and testing of the reported features was done by mb dv mw and dc the manuscript was conceptualized by dv as and dfv and drafted by dv with support of mb and mw all authors contributed to the manuscript review appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105102 
25787,modelling river physical processes is of critical importance for flood protection river management and restoration of riverine environments developments in algorithms and computational power have led to a wider spread of river simulation tools however the use of two dimensional models can still be hindered by complexity in the setup and the high computational costs here we present the freeware basement version 3 a flexible tool for two dimensional river simulations that bundles solvers for hydrodynamic morphodynamic and scalar advection diffusion processes basement leverages different computational platforms multi core cpus and graphics processing units gpus to enable the simulation of large domains and long term river processes the adoption of a fully costless workflow and a light gui facilitate its broad utilization we test its robustness and efficiency in a selection of benchmarks results confirm that basement could be an efficient and versatile tool for research engineering practice and education in river modelling keywords gpu cuda river modelling unstructured grid shallow water sediment transport pollutant transport software availability name of software basement version 3 v3 website www basement ethz ch e mail basement ethz ch developer numerical modelling division at the laboratory of hydraulics hydrology and glaziology vaw eth zürich language c c cuda interface graphical user interface gui command line interface cli hardware cpus cuda enabled gpus optional os windows linux ubuntu availability freeware test cases repository available at eth zürich research collection https doi org 10 3929 ethz b 000482308 1 introduction in the last decades the usage of numerical tools widely spread in several subjects of the environmental sciences river science sensu gilvear et al 2016 is no exception in this trend with a number of tools been developed to address variegate research questions e g brewer et al 2018 shimizu et al 2019 modelled river physical processes span from flood simulation hydraulic and sediment dynamics pollutant and temperature transport to vegetation and flow interactions just to mention a few e g crosato and saleh 2011 sharma and kansal 2012 williams et al 2016 dugdale et al 2017 teng et al 2017 such river processes occur at different spatial and temporal scales hence influencing the development and choice of suitable modelling tools advances in computational power numerical algorithms and optimization routines that occurred in the last decades allowed for the spread of more and more sophisticated numerical tools in the context of river science two dimensional depth averaged hereinafter 2d models are nowadays of common use in research and engineering practice this is particularly true for some applications such as flood modelling and river morphodynamics e g shimizu et al 2019 zischg et al 2018 the increasing usage of 2d river models is also closely bonded with the growing availability of high resolution river datasets in particular advances in lidar uav photography and others remote sensed survey technologies enable river topographic scans at an unprecedented level of detail e g marcus and fonstad 2010 savage et al 2016 the increased computational capabilities and refined datasets open the gates for near census sensu pasternack 2011 numerical modelling of several river processes indeed 2d river models have the capability to simulate fine spatial centimeters to meters and temporal scales seconds to days at such scales relevant hydro morphodynamic processes e g bar formation and also ecohydraulic processes e g habitat dynamics can hence be modelled e g maddock 1999 siviglia et al 2013 wyrick et al 2014 guan et al 2016 nevertheless 2d river models can still be computationally demanding with simulations lasting several days this is particularly true when complex physical processes such as morphodynamics are accounted for siviglia and crosato 2016 moreover large scale or near census applications i e with millions of computational cells and or long term simulations i e years all concur to increase overall computational costs such drawbacks particularly apply for the investigation of highly unsteady river processes such as artificial or natural flood waves where explicit numerical schemes are preferable in such cases the overall computational time scales exponentially with the number of computational cells due to stability constraints e g toro 2001 increasing the efficiency and the computational performance of river models represents yet a challenge pitfalls arise with the number of computational cells but also with the inherent complexity of 2d models for example challenges are to be found in the setup of the computational domain nahorniak et al 2018 but also in the definition of particular boundary conditions costabile and macchione 2015 dazzi et al 2020 increasing the computational performance is also sought by developing alternative numerical solution strategies for the underlying physical governing equations examples are variegate spanning from the adoption of a local timestep for the numerical integration e g sanders 2008 dazzi et al 2018 the automatic adaptation of the computational mesh e g powell et al 1993 the use of acceleration factors for the hydro morphodynamic problem e g carraro et al 2018 morgan et al 2020 to the reformulation of the governing mathematical equations for water quality simulations e g vanzo et al 2016 to mention a few parallel computing solutions are the most popular strategies to reduce computational time they historically benefit from the continuous improvements of parallel performance of both single cpus and clusters and the decrease of their unitary price the general aim of parallelization techniques is to split the total computational load into tasks that can be executed simultaneously by different computational units e g afzal et al 2016 the use of the graphics processing unit gpu as a general purpose computational resource developed rapidly in the last decade owens et al 2008 for many parallelizable workloads offloading work to gpus is a relatively cheap and efficient high performance computing strategy that is also easily upgradable in standard desktop workstations by means of gpu parallelization numerical models can potentially be accelerated by a factor of tens and more e g lacasta et al 2014 the efficiency of such a parallelization however depends on the data exchange between the main memory and the processors with a complex memory hierarchy and bandwidth bottlenecks mudalige et al 2012 these low level constraints can limit computational speedup and depend on the model data memory handling the underlying model complexity and the nature of the governing equations to be solved in applications such as 2d river models the type of computational mesh i e structured or unstructured has a significant influence on the final computational speedup in the last decade river simulation models have benefited from gpu parallelization specific and ad hoc implementations of gpu based models for 2d hydrodynamic e g brodtkorb et al 2012 smith and liang 2013 vacondio et al 2014 horváth et al 2016 vacondio et al 2017 and occasionally morphodynamic simulations e g hou et al 2020 have become available the vast majority of these models are based on structured grids which allow for an easier implementation and for relatively higher computational speedups this is due to the fact that for structured meshes the data structure is inherently simpler which reduces the need for mappings and indirections to the best of the authors knowledge few hydrodynamic models implement gpu acceleration on unstructured meshes lacasta et al 2014 2015 castro et al 2011 petaccia et al 2016 with very limited ad hoc implementations for transient flows morphodynamics e g juez et al 2016 bundled river modelling software that support gpu acceleration are available for commercial use e g riverflow2d hydronia com riverflow2d tuflow tuflow com but costless ones are still few see garcía feal et al 2018 an increase in availability of freeware gpu based river models would be beneficial for environmental modelers in academic research and education but also in consultancy and engineering offices in this paper we introduce the basement software version 3 a freeware application developed at the laboratory of hydraulics hydrology and glaciology of eth zürich the software can simulate two dimensional hydrodynamic morphodynamic and scalar advection diffusion processes of scientific and practical interest it can seamlessly run on gpu enabled workstations as well as on more standard multi core cpus this flexibility in the choice of the backend i e of the final computational hardware is achieved by integrating the op2 framework mudalige et al 2012 reguly et al 2016 giles et al 2012 this framework provides an additional abstract layer for the acceleration of numerical models on unstructured computational meshes and has been successfully implemented in similar modelling context reguly et al 2018 the obtained parallelization performance alleviates the computational limitations when simulating high resolution or large computational domains and or long term processes e g giles et al 2020 this is particularly relevant when aiming at the calibration beckers et al 2020 or at the uncertainty evaluation savage et al 2016 jung and merwade 2014 of deterministic models as proof of concept a flood wave uncertainty propagation analysis with basement has been proposed in peter 2017 in the current version basement is available for both windows and linux based ubuntu environments it is provided with a command line interface cli to easily perform batch simulations but also with a light graphical user interface gui the basement software aims to enable a broad range of potential users to skilfully simulate river processes in the domain of river engineering and research on state of the art computational hardware moreover with accompanying scholar programs and extended documentation material and tutorials the software is designed to be a valuable didactic tool for engineering and river science students the paper is structured as follows 2 provides the software application context that justifies the adopted mathematical and numerical strategies 3 to 5 report the mathematical basis the numerical strategies and main features of the basic modules of basement the software design the modelling workflow and the parallelization solutions are presented in 6 a selection of benchmarks are reported in 7 whilst conclusions and outlooks are drawn in 8 2 application context one of the main goals of the novel software design of version 3 is the capability to tackle river processes at different spatial and temporal scales for example basement can be used to simulate large scale i e basin scale flood propagation but also reach scale morphodynamic processes such as formation and evolution of fluvial bars moreover it can be applied together with high resolution topographies in the order of centimeters to simulate ecohydraulic processes at different ecological scales e g habitat modelling this range of application possibilities is enabled by specific characteristics of the software in particular unsteady and transitional flows basement can deal with strongly unsteady flows and different flow regimes sub and super critical for this reason basement is particularly suitable for simulations of river flows in alpine contexts the propagation of natural flood waves as well as hydropeaking events this is ensured by the adoption of a robust and accurate shock capturing explicit solver for the hydrodynamic problem 4 2 accurate front propagation it is possible to simulate extreme events such as dam break induced floods but also ecologically relevant processes such as the wetting drying of riparian areas and in channel morphologies due to artificial flow alterations this is achieved by an implemented shock wave capturing numerical scheme complemented with a robust treatment of wet dry interfaces 4 2 complex river topographies the use of unstructured grid for the computational domain discretization enables for an accurate description of complex river morphologies and riverine structures 4 1 the adoption of an unstructured mesh also reduces the strong anisotropy of structured meshes which can be crucial for particular applications large problems the software adopts a parallelization strategy tailored to the acceleration of problems on unstructured meshes 6 4 moreover basement simulations can efficiently be executed on different computational backends those backends include gpu cards therefore allowing for the simulation of large domains millions of computational cells on standard workstations having a limited cost multiple river processes the software is designed in a modular way so different river processes such as hydrodynamics sediment transport or advection diffusion of a scalar e g a non reactive pollutant can be simulated by activating specific modules at setup time 6 different types of boundary conditions 5 and closure relationships are available to simulate for example simple hydraulic structures e g weirs or flow inputs outputs e g water intakes the modular design 6 allows to retain good parallelization performances in the simulation of different river processes as shown in 7 7 the basic modules available in basement are i hydrodynamics ii morphodynamics and iii advection diffusion of scalar quantities each module is composed by different sets of hyperbolic equations describing the conservation and evolution of the water flow hydrodynamics the fluvial sediment morphodynamics and the concentration of passive solutes scalar advection diffusion the governing equations represent a so called initial boundary value problem toro 2001 where process specific initial and boundary conditions are required to be set the following sections present the main governing equations and closure relationships 3 the numerical strategies 4 and finally the initial and boundary conditions 5 for the three basic modules the main module features are also listed in table 1 of the supplementary material 3 mathematical formulation 3 1 hydrodynamics the hydrodynamic module solves the so called shallow water equations hereinafter swe e g toro 2001 the two dimensional swe are of practical interest with regard to water flows with a free surface under the influence of gravity considering a cartesian reference system x y z where the z axis is vertical and the x y plane is horizontal fig 1 a the system of governing equations can be written as 1 t h x q x y q y s h t q x x q x 2 h 1 2 g h 2 g h z b y q x q y h g h x z b g h s f x t q y x q x q y h y q y 2 h 1 2 g h 2 g h z b g h y z b g h s f y where the system unknowns are the water surface elevation h m and the two directional components of q q x q y m2 s representing the flow discharge per unit width with z b m we indicate the bottom elevation whilst h h z b m is the water depth and g m s2 the acceleration due to gravity note that the depth averaged velocity vector can be consequently expressed as u u v q x h q y h m s finally s fx and s fy represent the dimensionless friction terms in x and y direction whilst s h m s represents potential external contribution subtraction of flow discharge to the mass conservation equation 3 1 1 hydrodynamic closure relationships to solve the system 1 closure relationships for the friction terms s fx s fy and the contribution of external inflow outflow discharge s h must be provided friction terms under the hypothesis of turbulent flow hence under the assumption that the energy line slope is proportional to the square of the flow velocity the friction terms s fx s fy can be written as 2 s f x u u g h c f 2 s f y v u g h c f 2 where c f is the dimensionless friction coefficient and u is the norm of the velocity vector several formulae are available for c f basement implements four well known formulations of power or logarithmic type given in table 1 external inflow outflow discharge the term s h m s represents additional sources of water like rainfall and springs or water abstraction sink and can be defined over subsets of the computational domain the external water source can be provided by the user as total discharge m3 s or as intensity mm h per squared meter different behaviour can be imposed for each external source sink area exact source sink the exact given water volume is added source or extracted sink from the surface this is the only option for water addition in case of water abstraction the simulation might end abruptly if the available water volume is smaller than the volume prescribed for subtraction this option allows to have the full control on the water entering leaving the computational domain and is useful to simulate e g managed hydraulic structures such as regulated water intakes available sink the given water volume to extract is limited by the available water volume in the single element i e computational cell with this abstraction option the simulation proceeds with no interruptions because the water conservation is ensured this option is useful to simulate particular unmanaged hydraulic structures such as diversion spillways infinity sink all available water will be abstracted from the computational domain 3 2 morphodynamics the basic morphodynamic module solves the so called exner equation exner 1925 it describes the bed evolution due to erosion or deposition which results in the elevation change of the actual bed level z b assuming the same coordinate reference of fig 1a it reads 3 1 p t z b x q b x y q b y s b where p is the bed sediment porosity assumed constant in space and time s b m s is an external source term specifying local inputs or outputs of sediment material e g slope collapse or excavation and q b q b x q b y m2 s is the specific sediment transport flux the morphodynamic module in its basic form accounts only for sediment transport occurring in the form of bed load or total load armanini 2018 parker the simulation of sediment transport as suspended load is delegated to a specific module planned in future versions of the software 3 2 1 morphodynamic closure relationships two closure relationships are needed to numerically solve the governing equation 3 a sediment transport formula and the external source sink of sediments 3 2 1 1 sediment transport formulae basement implements four different types of sediment transport formulae as given in table 2 the first two expressions meyer peter and müller like mpm like and grass like grass like are adequate to simulate bed load dominated sediment transport conditions the engelund and hansen formula allows for the estimation of total sediment transport i e suspended and bed load whilst smart and jäggi is used for bedload transport in steep channels the expressions and the typical parameter values to calculate the specific sediment transport magnitude q b are given in table 2 in the mpm like formulation θ is the dimensionless bed shear stress i e shields parameter armanini 2018 θ cr is the critical dimensionless bed shear stress d m is the representative grain diameter s ρ s ρ is the relative density of the sediment with respect to water the coefficients α m and the critical threshold θ cr can be assigned by the user or adopted from literature see table 2 the grass like model proposes a simple bedload transport formula where q b is a function of the flow velocity magnitude with u cr as critical threshold velocity the coefficients α m and the critical threshold u cr can be assigned by the user or adopted from literature table 2 it is worth remarking that the engelund and hansen formula engelund and hansen 1972 that quantifies the total sediment transport does not prescribe a threshold condition for incipient motion 3 2 1 2 local corrections of the sediment transport the morphodynamic module implements three corrections to the basic exner equation 3 to account for the influence of local characteristics of the flow and the bottom on the sediment transport namely i the influence of local slope on incipient motion ii the effect of lateral bed slope and iii of the flow curvature on the sediment transport direction the threshold condition for incipient motion of grains by shields 1936 is valid for an almost horizontal bed in case of a sloped bed in flow direction or transverse to it the stability of grains is either increased or reduced due to the gravity the critical shear stress value can be adapted consequently to account for the influence of local longitudinal and transversal slopes a common approach is to scale the critical shear stress for almost horizontal bed θ cr with a correction factor k 4 θ c r k θ c r basement implements the correction factor k as proposed in van rijn 1989 and chen et al 2010 implementation details are given in the official documentation the bedload direction can be corrected to account for two relevant morphodynamic processes linked to the slope of the bed and the curvature of the flow the deviation of the bedload direction from the flow direction can thus be modelled as a deviation angle φ φ b φ c sum of the correction angle for bed slope φ b and curvature φ c as depicted in fig 1 b and c the bedload vector is then rotated with the rotation matrix t φ being 5 t c o s φ s i n φ sin φ c o s φ where the angle is positive counterclockwise the angle φ b is estimated with the approach proposed in ikeda 1982 and talmon et al 1995 for the effect of the local transversal bed slope in particular the bedload direction deviates from the flow direction in presence of a local transversal bed slope due to the gravity acting on the bedload sediment particles fig 1b the bed load deviation φ b with respect to the flow is therefore evaluated as 6 tan φ b n l θ c r θ s n q for s n q 0 where n l is an experimental lateral transport factor 0 75 n l 2 63 s x z b y z b is the local bed slope and n q is the unit vector perpendicular to q in downhill direction fig 1b the angle φ c accounts for the effect of a marked flow curvature due to three dimensional spiral flow motion that establishes in curved flows the bed load direction tends to point towards the inner side of the curve while the flow direction points towards the outer side fig 1c this curvature effect is taken into account according to an approach proposed by engelund 1974 where the deviation angle φ c is determined as 7 tan φ c n h r c where h is the water depth n is a curvature factor and r c denotes the radius of the river bend positive for curvature in counterclockwise direction the curvature factor n mainly depends on bed roughness and assumes values n 7 for natural streams engelund 1974 and values up n 11 for laboratory channels rozovskii 1961 3 2 1 4 external sediment input output the source term s b represents additional sediment mass input or output sink that can be defined on subsets of the computational domain the source can be specified as total volume flux including porosity m3 s similarly to the hydrodynamic case 3 1 1 different approaches are adopted for the sediment sink namely exact available and infinity 3 2 2 fixed bed concept morphodynamic simulations generate deposition and erosion patterns of the riverbed erosion processes if not limited can proceed indefinitely in the vertical direction to account for the presence of non erodible river bottom as in case of bedrock or concrete cover a non erodible fixed bed depth ζ rel fig 1a can be set this threshold also determines the volume of sediment available for transport the fixed bed elevation is defined relative to the initial bottom elevation z b with ζ rel 0 3 2 3 gravitational transport several algorithms and approaches have been proposed to simulate the sediment flux contribution generated by gravitational collapse and bank erosion stecca et al 2017 in this version of the software we employed a simple geometrical approximation for gravitational transport assuming that it occurs when the local bed slope expressed as an angle γ between two neighbour computational cells exceeds a given critical angle γ cr this results in a sediment redistribution due to gravity towards the adjacent cells to restore a stable local slope i e γ γ cr further implementation details are provided in the official documentation 3 3 scalar advection diffusion a number of environmental processes such as pollutant temperature or nutrient transport can be modelled assuming the passive advection and diffusion of a scalar quantity in the form of dissolved or particulated species e g vanzo et al 2016 the scalar advection diffusion module allows for the simultaneous simulation of multiple passive species up to a maximum of 5 the maximum number of species is limited for purely computational efficiency reasons i e to limit the memory requirements of this module based on our experience we consider 5 species a suitable limit for a broad range of applications the transport of a generic species c can be described by the following advection diffusion equation 8 t q c x q x q c h h k x x x ϕ c k x y y ϕ c y q y q c h h k y x x ϕ c k y y y ϕ c s ϕ c with c 1 5 where the unknown is q c the specific mass of the species c it can be expressed as q c h ϕ c with ϕ c the volumetric concentration and h the water depth the term s ϕ c is a net source of c and k ij m2 s are the components of the 2d diffusion tensor 3 3 1 scalar advection diffusion closure relationships for the scalar advection diffusion module the closure relationships are used to model the contribution of external scalar input and output in particular the term s ϕ c represents an additional scalar mass flux that can be added within portions regions of the computational domain the source can be specified either as an imposed concentration value or a total volumetric flux m3 s the behavior is analogous to the case of hydro and morphodynamic sources s h and s b 3 1 1 and 3 2 1 the terms k ij of the diffusion tensor vary considerably with respect to the physical nature of the transported species diffusive transport is modelled in terms of both molecular diffusion k m and turbulent dispersion k i j t such that k i j k m i i j k i j t with i ij the identity matrix the molecular diffusion is assumed as an isotropic fickian process with constant coefficient k m turbulent dispersion is anisotropic k i j t and scales with the friction velocity u u c f and water depth via a longitudinal α l and transversal α t non dimensional coefficients suitable values for open channel flows in natural environments are α l 13 and α t 1 2 vanzo et al 2016 4 numerical solution the numerical solution of the governing equations 1 3 and 8 is sought in a finite volume framework with a spatial discretization based on unstructured meshes 4 1 for the temporal integration an explicit first order euler scheme is used in its basic configuration the temporal integration proceeds in a synchronous decoupled way for all the modules meaning that the modules are independently integrated in time with the same timestep 4 5 the following sections detail the domain discretization strategy and the adopted numerical solver for the fluxes calculation of the three basic modules the interested reader should refer to the provided references for specific implementation details 4 1 domain discretization the problem is discretized adopting a finite volume approach over unstructured triangular meshes a conforming triangulation t ω of the computational domain ω r 2 by elements ω i such that t ω ω i is assumed given a finite volume element ω i fig 2 j 1 2 3 is the set of indexes such that ω j is a neighbour of ω i γ ij is the common edge of two neighbour cells ω i and ω j and l ij its length n ij n ij x n ij y is the unit vector which is normal to the edge γ ij and points toward the cell ω j 4 2 hydrodynamics the system of governing equation 1 can be cast in vectorial form as 9 t u x f x y f y s where left handside terms of 9 are 10 u h q x q y f x q x q x 2 h 1 2 g h 2 g h z b q x q y h f y q y q x q y h q y 2 h 1 2 g h 2 g h z b the vector of source terms can be written as s u s h s f r u s b e d u where 11 s h s h 0 0 s f r 0 g h s f x g h s f y s b e d 0 g h x z b g h y z b by integrating the governing system of equation 9 in the control volume v ω i t n t n 1 we obtain the general update formula for the triangular element i 12 u i n 1 u i n δ t ω i j 1 3 l i j f i j δ t s i problem unknowns at cell i and discrete time n are represented by cell averages u i n the numerical solution sought at time t n 1 t n δt is denoted by u i n 1 in 12 f ij are the hydrodynamic fluxes estimated at the cell interface ij fig 2 to compute the fluxes f ij for the hydrodynamic system 9 several well established solvers are available here we adopt the well known hllc approximate riemann solver toro et al 1994 which is a modification of the basic hll scheme to account for the influence of intermediate contact waves further details on the hllc approach are available in chapter 10 of toro 2001 the solver is proved to be robust and efficient in simulating unsteady flows and the advection of passive tracers vanzo et al 2016 the numerical discretization of the three terms of s u 11 is conducted separately according to the nature of each term the external inflow outflow contribution s h is added explicitly to the continuity equation as it is not a function of the problem unknowns the stiff friction source terms s fr u are integrated with runge kutta 2 e g toro 2009 in a semi implicit fashion after adopting a splitting technique the implementation is analogous to the ones proposed in siviglia et al 2013 vanzo et al 2016 vanzo 2015 the topographical terms s bed u are discretized using the modified state approach proposed by duran et al 2013 this results in an easy and robust treatment of complex topographies and wetting and drying problems vanzo et al 2016 4 3 morphodynamics the exner equation is solved in a synchronous decoupled way with respect to the shallow water problem 4 2 meaning that the numerical integration of the exner equation 3 uses the same integration timestep δt of the hydrodynamic problem the general update formula for the exner problem reads 13 z b i n 1 z b i n 1 1 p δ t ω i j 1 3 l i j q b i j δ t s b i with the same symbols introduced for 3 the term q bij represents the normal sediment flux at the cell interface ij fig 2 for the numerical estimation of the term q bij a number of approaches are available in literature in the current version basement implements an approximate riemann solver of hll type sensu toro 2001 as in soares frazão and zech 2011 the sediment flux is thus calculated as 14 q b i j λ s q b i λ s q b j λ s λ s z b j z b i λ s λ s where pedix i j refers to quantities evaluated at the corresponding cell fig 2 and λ s λ s are speed estimations of the morphological problem we adopt the following speed estimates soares frazão and zech 2011 15 λ s m i n λ 1 i λ 1 j λ s m a x λ 2 i λ 2 j the expression for the terms λ 1 and λ 2 calculated for both cell i or j reads 16 λ 1 2 1 2 u n c u n c 2 4 q b n q n c 2 where u n is the normal velocity at the cell interface ij fig 2 and c g h is the so called wave celerity 4 4 scalar advection diffusion the scalar advection diffusion problem is solved in a synchronous decoupled way with respect to the shallow water problem 4 2 we reformulate the governing equation 8 via a cattaneo type relaxation technique as proposed by vanzo et al 2016 two additional scalar conservation equations are then added to 8 namely 17 t ψ x c x ϕ c ε ψ x c ε t ψ y c y ϕ c ε ψ y c ε where ε is a positive and small relaxation time whilst ψ x c and ψ y c are two auxiliary variables that recover x ϕ c and y ϕ c respectively for a sufficiently small ε vanzo et al 2016 after a trivial substitution of ψ x c x ϕ c and ψ y c y ϕ c into 8 the system composed by 8 and 17 can be rewritten in vectorial form as 18 t q x a x y a y x d x y d y s c s r e l where the vectors q a x d x s c and s rel read 19 q q c ψ x c ψ y c a x q c q x h 0 0 d x h k x x ψ x c k x y ψ y c q c ε h 0 s c s ϕ c 0 0 s r e l 0 ψ x c ε ψ y c ε with q representing the conserved scalar quantities whilst a x is the advective fluxes vector and d x is the diffusive relaxed fluxes vector both in x direction the scalar source terms are s c whilst the source terms arising form the relaxation are s rel for brevity we omit the formulation for the y direction a y and d y which is analogous the interested reader can refer to vanzo et al 2016 for a step by step derivation the scalar fluxes in 19 are solved through the svt solver introduced by vanzo et al 2016 the scheme presents a flux splitting approach combining the advective and diffusive relaxed fluxes evaluated with different solvers the hllc solver applied for the hydrodynamic fluxes 4 2 provides the advective component of the scalar fluxes at the cell interface a ij for the diffusive relaxed component the svt technique derives the fluxes at the interface d ij directly from the riemann invariants of a two non linear waves riemann problem similarly to the hydro and morphodynamic problems the control volume v ω i t n t n 1 is used to integrate the governing system 18 in order to obtain the following scalar update formula at the element i 20 q i n 1 q i n δ t ω i j 1 3 l i j a i j d i j δ t s c s r e l i where the fluxes a ij and d ij are computed at each cell interface ij fig 2 the numerical integration of the two source term vectors is conducted separately according to the nature of the terms the scalar sources s c are computed with a first order explicit euler scheme while the stiff relaxation source terms s rel q are integrated by means of a locally implicit euler method 4 5 stability condition numerical integration proceeds with a dynamic timestep δt evaluated at each time loop fig 6b that fulfills the well known courant friedrichs lewy stability condition toro 2001 in the current implementation the condition is expressed as 21 δ t c f l min 1 i n min 1 j 3 ρ i j λ i j where ρ ij is twice the distance between the edge j and the centroid of the cell i fig 2 and n is the total number of domain elements the term λ ij is an estimation of the largest eigenvalue of the hydrodynamic problem 1 namely λ i j u n g h with the symbology already introduced the cfl coefficient ranges between 0 and 1 by default it is set to 0 9 if not specified otherwise 5 initial and boundary conditions 5 1 initial conditions all modules require the user to define the initial conditions of the simulation two types of initial conditions are similarly available for all the modules region defined user explicitly defines the initial values of the problem unknowns e g water depth and specific discharge for hydrodynamics different values can be assigned to different region of the computational domain continue values are taken from the result file of previous simulations in addition the hydrodynamic module allows also to set dry conditions no water in the domain as initial conditions in this case the domain will progressively fill with water in relation to the assigned inflow boundary conditions or internal sources 3 1 1 5 2 boundary conditions the boundary conditions hereinafter bcs have different specifications for each core module see following sections but they all classify in three common types external standard external linked and internal bcs fig 3 exemplifies the main concepts adopted for the bcs the computational domain ω is defined by the domain boundaries as γ1 2 3 an external standard bc is dependent only on the local flow conditions and on some user defined rules this represents the most common case for example to define impermeable walls or river inflows and outflows by default all the external boundaries are set as wall the wall bc consists of a fixed frictionless inviscid reflective impermeable wall in external linked bcs instead the local bcs are defined also with information from a linked boundary typical example is a weir where the flow discharge at the downstream side of the weir depends on the water stage on the upstream side the third type of bcs internal are defined within the computational domain ω and not at the edges fig 3 this bc type comes in handy in case of very large domain application because it allows to test different configurations of hydraulic structures e g different locations of a weir or training wall without the need of regenerate the entire computational mesh for every configuration a summary of the main features of the bcs for the three core modules follows here the interested reader can refer to the official documentation for further details 5 2 1 hydrodynamic bcs the hydrodynamic module implements different types of bcs with a different level of customization depending on the bc type user assigned data is requested as single constant value in time e g lake level constant discharge as time series e g hydrograph or as set of parameters describing a dynamic behaviour e g weir activation rule in particular standard bcs in addition to wall bc inflows upstream bcs and outflows downstream bcs can be assigned as standard inflows three options are provided with uniform or explicit options the user provides a total volume discharge q m3 s whilst with zhydrograph the water surface elevation m must be provided for the standard outflows a value for the water depth h must be specified possible options are uniform conditions hydraulic weir rating curve zhydrograph i e water surface elevation and zero gradient i e neumann bc it is worth remarking that the specific type of upstream and downstream bcs should be selected depending on the local flow conditions i e sub or super critical linked bcs this type of boundaries establishes a link between two certain region of the domain where the governing equations are not solved it is specifically designed to simulate the behaviour of hydraulic structures within the river channel such as weirs gates bridges spillways internal bcs they are fictitious boundaries defined as segments at the interfaces of some computational cells on these segments three different conditions can be enforced instead of the solution of the swe 1 options are static walls dynamic walls and rating curve with the static wall the standard wall condition is applied on both sides of the internal boundary this option is useful for example for easily testing the presence of barriers e g training wall in fig 3 without the need of reconstructing the numerical domain with the dynamic wall the wall conditions are applied until a given threshold is reached after which the wall is removed and the swe are solved instead the threshold can be set as a time value i e wall removal at a given time or as water depth removal when a given water depth is reached on one side of the bc this feature comes in handy to simulate for example the collapse of some hydraulic structures with the rating curve option i e h q relation an unidirectional flow is applied through the internal boundary based on the water stage in the upstream side of the boundary as for the standard and linked rating curve bcs 5 2 2 morphodynamic bcs the sediment flow is defined as a specific bedload flux which is averaged and evenly distributed at the domain boundary conditions over the boundary length in analogy with the hydrodynamic module the morphodynamic boundaries are of type external standard and linked standard bcs for the upstream bcs basement implements three versions that allow to simulate i a given input of sediment as time series i e sedimentograph ii a sediment input derived from the hydrodynamic conditions under transport capacity conditions or iii bed equilibrium condition where the upstream bed elevation is kept constant two downstream bcs are available allowing the simulation of i equilibrium condition and ii check dam in this second option an equilibrium boundary condition is activated only if the bed level reaches a given threshold value otherwise a wall type boundary is assumed linked bcs one bc is available it allows for the simulation of sediment transport through given hydrodynamic linked conditions hence to ensure sediment continuity in the simulated channel 5 2 3 scalar advection diffusion bcs scalar bcs are defined in terms of concentration of total volumetric rate m3 s evenly distributed throughout the length of the relevant domain boundary the implemented types are standard bcs three types are available i scalar inflow as a constant value ii scalar inflow as a time series and iii zero gradient i e neumann bc outflow 6 software design 6 1 modelling workflow the standard modelling procedure involves three phases the pre processing the numerical simulation and the post processing phase fig 4 basement is designed to integrate into this workflow moreover the entire workflow relies on open source or freeware tools in the following we list the phases and provide a short description of the different configuration and results file formats as used by basement 1 pre processing in this phase the user is required to define the model domain and the input data the mesh file customized 2dm format mymesh 2dm in fig 4 contains the description of the triangular unstructured computational mesh the file can be generated with basemesh an open source python module as well as a qgis plugin see the official repository for details or via grid generator software that supports the 2dm format in addition further input data such as time series of water and sediment discharge or other quantities to be used as bcs can be provided ascii format mydata txt in fig 4 2 numerical simulation the actual simulation can be run via either cli or gui in basement version 3 the numerical simulation is split into three steps description follows in 6 2 the final simulation results are stored in a general purpose binary container hierarchical data format hdf5 www hdfgroup org basement generates also an xdmf file extensible data model and format http www xdmf org which contains a machine readable description of the data stored in the hdf5 file 3 post processing the xdmf file output xdmf in fig 4 can be opened with the crayfish plugin for qgis or with paraview for final results visualization and further post processing in addition ad hoc python scripts can be used to manipulate results directly from the binary container some scripts are provided at the software website 6 2 simulation steps the numerical simulation phase consists of three steps the pre simulation the simulation and the post simulation fig 5 each step can be completed by running a corresponding basement executable via gui or cli this modular design allows a customization of the simulation workflow by the user and an efficient batch processing of basement steps for instance the programs can be run from a scripting language like python the different executables are configured using a dedicated command file in standardized json file format javascript object notation fig 5 the basement gui is designed to support the user with creating the command files and running and monitoring the three simulation steps in particular the gui validates the configuration parameters and automatically adds required parameters where default values are available the three simulation steps are detailed as follows 1 the pre simulation step focuses on the model definition in particular the model json command file contains i physical properties ii initial conditions and iii boundary conditions of the physical problem and further iv numerical parameters the setup executable first reads the computational mesh mymesh 2dm the external required data mydata txt and the command file model json then it validates and stores the model inside the binary container setup h5 2 the simulation is carried out on a selected computational backend 6 4 it is driven by the command file simulation json that contains the simulation parameters such as the total simulation time the output timestep constant and the desired output quantities the program reads and executes the model setup h5 generated in the previous step the results of the simulation are stored in a second binary container results h5 the user can monitor the simulation execution on the gui log terminal where the simulation progress as the current integration timestep and an estimation of the real time speed rts of the simulation ratio of simulated time over computational time are provided 3 the post simulation step is configured using the command file results json that contains the selected output format currently only xdmf is supported the output is then available for the post processing phase 6 1 when it is necessary to run a new simulation starting from the results of a previous one two options are available restart and re run when performing a restart the pre simulation step is executed again i e a new model is generated from scratch with potentially a new set of parameters the user indicated an existing results h5 file that is to be used to fetch the initial conditions for the new model the re run option does not generate a new model but uses the existing model setup h5 with initial conditions taken from the current results file in this scenario the user can only modify the simulation and results parameters i e duration output timestep and output type but not the model parameters this second option is particularly useful in the case of large models i e millions of computational cells because the pre simulation step can take up to tens of minutes if the user only needs to extend the simulation duration for example then the re run option allows to skip the pre simulation step 6 3 modularity and sequencing basement aims to simulate different river processes with a high level of flexibility and efficiency with this in mind we designed the software adopting a modular approach the two core concepts of this design are modules and kernels described as follows modules take care of the simulation of specific river processes e g module hydrodynamics in fig 6a they can be nested to simulate processes with an increasing level of detail complexity e g module hyd external source fig 6a modules are activated by the user in the pre simulation step an activated module triggers the execution of a number of kernels throughout the simulation a kernel is a set of operations to be executed on each entity e g a cell i or an edge j fig 2 of the computational domain or a subset of it depending on the specific task kernels can be scheduled for a single execution i e initialization kernels or for repeated execution in each iteration of the integration time loop fig 6b the global time loop is executed with a timestep δt that satisfies the stability condition of the hydrodynamic problem 4 5 some modules and their associated kernels can be scheduled by the user for a delayed start or for execution at different time intervals larger than the global integration timestep δt for example the user can set a delayed starting time in seconds of morphodynamic and scalar transport modules typical usage is to ensure steady hydrodynamic initial conditions before starting the morphodynamic or advection diffusion simulation for other modules such as the gravitational transport or the flow curvature calculation the user can define also the execution interval in seconds this feature allows to reduced the computational efforts when the integrated processes are not subject to the hydrodynamic timestep constrain 4 5 by default all modules are executed from the beginning of the simulation and at each global integration timestep the architecture based on modules and kernels has two main advantages first it is flexible in that it allows users and software developers to easily add or remove specific modules without interfering with other existing modules in particular this permits an integration of further modules as development continues 8 second it is efficient because only the necessary kernels are scheduled for execution at setup time pre simulation step 6 4 parallelization strategy and computational backends the parallelization strategy of the basement numerical core addresses two main aspects i the use of different technologies i e computational backends generated from the same unique software source code this allows for an easier source code maintenance and integration of future different backends ii an efficient and heavy parallelization of the numerical core following the concept of data parallelism to this end the numerical core of basement integrates op2 mudalige et al 2012 giles et al 2012 which is an open source framework for the development of unstructured grid applications using source to source translation op2 generates the appropriate code for different target platforms by introducing an additional level of abstraction between the numerical algorithm and its execution it supports multi core cpus gpus and even clusters via mpi message passing interface http www mpi forum org basement currently supports multi core cpus and gpus when starting the simulation the user can select to compute on the cpu the gpu or a combination of both all the currently supported backends table 3 are available for both windows and linux ubuntu operating systems it is important to note that the choice of graphics processing units is currently limited to nvidia cuda cards the precise requirements are provided in the official documentation all the backends can execute the numerical simulations in double default or single precision with different performance 7 7 7 results a set of selected test cases t1 t6 are proposed here to test the robustness accuracy and efficiency of the three basic modules table 4 summarizes the key features of each test case the interested reader can refer to the official software documentation for further examples finally 7 7 focuses on the software performance and scalability all the test cases are freely available at eth zürich research collection doi https doi org 10 3929 ethz b 000482308 7 1 t1 malpasset dam collapse the scope of this test is to assess the robustness and accuracy of the hydrodynamic solver when simulating a shock type hydrodynamic wave travelling on a highly irregular and dry domain the collapse of the malpasset dam in the reyran river valley fréjus france represents a well established hydrodynamic benchmark for numerical models e g hervouet and petitjean 1999 singh et al 2011 valiani et al 2002 in 1959 the 66 5 m high dam collapsed almost instantaneously generating an up to 40 m high flood wave that propagated down the reyran valley destroyed the two villages malpasset and bozon and reached the mediterranean gulf 21 min later valiani et al 2002 the propagation of the flood wave was reconstructed via the maximum water level and the flood arrival time recorded at multiple locations in particular the maximum water level is available from a police survey for 17 survey points marked as p1 to p17 in fig 7 and the flood arrival time is known from three electric transformer stations which have been destroyed by the flood wave the locations of the transformer stations are indicated as a b and c in fig 7 coordinates and recorded arrival times are listed in table 5 we make use of such field data to test the performance of the hydrodynamic module the computational domain is discretized with 499 059 triangular elements the domain boundaries are set to walls 5 2 1 with exception of the downstream boundary located in the mediterranean gulf where a fixed water level was set to 0 m zhydrograph the initial conditions are a fixed water surface elevation of 100 m in the reservoir and dry conditions in the rest of inland domain region defined ics the initial velocity was set to 0 0 m s in the entire domain in accordance with hervouet and petitjean 1999 the manning s friction coefficient was set to 0 033 m 1 3s for the whole domain the cfl number was set to 0 9 the simulated maximum water levels are compared to the 17 field observations in fig 8 with overall good agreement the average relative error is 7 15 with the largest observed at p13 with an overestimation of 30 6 to highlight the effects of the topographical approximations of the digital elevation model and hence of the computational mesh we compared recorded and simulated water level values as follows for each punctual maximum water level recorded in field observations blue triangles in fig 8 we compared the maximum simulated values of the spatial mean maximum and minimum among the computational cell containing the observation point and its three neighbours black and red series in fig 8 we expect lower discrepancies between recorded and simulated values where the numerical values hence the topographical elevations are spatially homogeneous as a matter of fact points p1 p7 and p13 fig 8 have the large discrepancy between measured and simulated values but also the largest spatial variability of the numerical values red shaded area this suggests that such discrepancies relates more to the local topographical approximations of the dtm rather than to the numerical model observed and simulated times of flood arrival are given in table 5 simulated values are in good agreement with measured ones for all the electrical transformer stations et simulated arrival times have a maximum relative error of 3 8 for et b corresponding to an absolute delay of 47 s it is worth mentioning that the friction value influences the simulated arrival times 7 2 t2 propagation of a sediment bore scope of the test is to assess the robustness of the de coupled hydro morphodynamic solver approach particularly when simulating the sediment transport over a transcritical flow this represents a critical test especially when adopting de coupled approaches e g cordier et al 2011 moreover the simulation tests the morphological solver capability in well reproducing the dynamics of an advancing sediment bore in this test case the flume experiment proposed in bellal et al 2003 run 2 is reproduced numerically the computational domain is a composed by a straight 6 9 times 0 5 m channel representing the lower part of the original experimental flume and it is discretized with 24 612 triangular elements the sediment has a characteristic diameter of 1 65 mm the water and sediment discharge at the upstream boundary are set to 0 012 m3 s 1 uniform bc and 0 196 m3 s 1 with porosity sedimentograph bc respectively the flume is at initial uniform flow conditions characterized by a supercritical flow at t 0 s a fixed water level of 0 2093 m is imposed at the downstream boundary zhydrograph and sediment transport out of the domain is stopped wall this results in the formation of an hydraulic jump moving upstream in the flume and a subsequent downstream propagation of a sediment bore the cfl number is set to 0 9 the bed porosity is assumed constant and equal to 0 42 and the simulation duration is 500 s fig 9 shows the initial and final profiles of the simulated bed and water elevations the solver reproduces well the sharp transition between super and sub critical flow conditions the position of the sediment front in time is shown in fig 10 with good agreement between simulated and experimental values 7 3 t3 dam break over a mobile bed with a sudden enlargement scope of the test is to assess code robustness in simulating sediment transport at wet dry interface and the accuracy in reproducing scour deposition patterns the experiment illustrated in goutiere et al 2011 represents a well know morphodynamic test for numerical models e g juez et al 2014 siviglia et al 2013 soares frazão and zech 2011 the domain consists of a flat flume with a non symmetrical sudden enlargement fig 11 the bed is composed of a coarse uniform sand with a median diameter of d m 1 82 mm the initial conditions are defined by an horizontal layer of fully saturated sand of thickness 0 1 m over the whole domain and an initial water storage of depth 0 25 m upstream of the dam located at section x 3 0 m region defined ics at time t 0 s the dam is suddenly removed resulting in the propagation of a dam break wave with consequent sediment transport the computational domain is discretized by unstructured triangular cells at different resolutions follows in table 6 inviscid wall boundary conditions are set at the upstream and lateral domain boundaries while a free outflow condition zero gradient and constant bed elevation equilibrium conditions are used at the downstream outlet the manning coefficient is set to 0 0167 m 1 3s the sediment density and porosity are set to 2680 kg m3 and 0 47 respectively the sediment transport is evaluated with the mpm like formula table 2 setting θ cr 0 0495 for the critical shields stress α 3 97 and m 1 5 for the remaining parameters wong and parker 2006 the cfl number is set to 0 9 the numerical simulations last 12 s the evolution of the water elevation during the simulation is shown in fig 12 for the six survey points the simulated series show a fairly good agreement with the experimental values the dam break wave arrival time is well captured and the maximum elevation values are comparable with the measured ones moreover the simulated series show minor discrepancies with the experimental ones after the arrival of the first wave as already pointed out by previous works siviglia et al 2013 xia et al 2010 discrepancies are due to the extremely complex flow pattern generate by multiple wave reflections while simulation proceeds in time which potentially generate tri dimensional flow structures nevertheless obtained series are coherent with the ones of siviglia et al 2013 where a second order accuracy model was employed numerical bed elevations after 12 s are compared to the experimental results in fig 13 the simulated scour and deposition patterns are well reproduced the magnitude of the scour at cross section cs1 at y 0 25 m matches well with an underestimation of the deposition pattern at y 0 35 m at cross section cs2 the simulated deposition magnitude matches well with the experimental one but with a small shift toward the lateral boundary 7 4 t4 scour and deposition on a channel bend the scope of the simulation is to test the correct reproduction both in term of positioning and magnitude of a river point bar generated by a channel bend in this test we numerically reproduced one experiment from yen and lee 1995 already adopted as morphodynamic benchmark test e g villaret et al 2013 the flume is u shaped with a bend of 180 having a constant radius along the center line of r c 4 m the cross section is rectangular with width w 1 m and slope s 0 2 the two straight reaches before and after the bend are 11 5 m long the median diameter of the bed material was d m 1 mm in the experimental run the flume was fed with a simplified triangular flood hydrograph having a base flow of 0 02 m3 s 1 and peak flow of 0 053 m3 s 1 the rising and falling limbs last 100 and 200 min respectively afterwards a constant baseflow was kept for another 100 min during the experimental run a steady point bar in the inner side of the bed develops and grows with a corresponding erosion on the outer side in the numerical setup the domain is discretized with 24 523 computational cells we set the porosity to 0 4 and used the mpm like formula with parameters from wong and parker 2006 as in table 2 the lateral slope factor n l is equal to 1 4 and the curvature factor n is set to 11 6 and 7 at the numerical domain boundaries uniform flow and equilibrium sediment transport conditions are imposed the simulation as the experimental run lasts 400 min a planar view comparison between numerical and experimental run is depicted in fig 14 the final bed change with respect to the initial flat configuration δz b is scaled with the approaching i e upstream reach flow depth h 0 the magnitude of scours and depositions for the numerical run ranges between 0 75 and 0 75 matching fairly well with the experimental values also the positioning of the point bar with the maximum deposition anticipating the middle of the bend 90 is well reproduced numerically fig 15 shows the cross sectional profile of the relative bed change for the numerical simulation and the experimental run in the middle of the flume bend at 90 the numerical profile reproduces well the experimental trend this test demonstrates the software capability in simulating an unsteady morphological process i e a point bar development in a meander during a flood such process can be well reproduced only by implementing suitable corrections of the sediment transport direction due to gravity and curvature as presented in 3 2 1 7 5 t5 steady scalar discontinuity with two diverging hydrodynamic waves the test assesses the correct advection of scalar concentration this is assessed with a challenging test a steady discontinuity of a scalar concentration subjected to strongly variable flow conditions the chosen test is an idealized one dimensional problem but nevertheless it is particularly challenging for a pletora of numerical schemes toro 2001 the domain is a simple flat bed channel 100 m long and 0 1 m wide the domain is deliberately chosen very narrow to mimic a 1d setup given that the exact solution of the problem is available in one dimension case as initial conditions region defined ics the water depth h is set even in all the domain whilst the initial longitudinal specific discharge q x and the concentration of a generic scalar ϕ present a discontinuity 22 q x 3 0 m 2 s if x 50 m q x 3 0 m 2 s otherwise ϕ 1 0 if x 50 m ϕ 0 0 otherwise h 1 0 m x the domain is discretized with 1362 computational cells lateral walls are reflective and inviscid whilst transparent boundary conditions zero gradient are set at beginning and end of the channel for both hydrodynamic and scalar transport modules the cfl is set to 0 95 and the simulation timeout is t 2 5 s as the simulation starts two strong rarefaction waves start diverging from the center of the domain towards the two extremities suddenly forming a water depression in the center despite the strong unsteadiness of the hydrodynamic quantities during the simulation a steady contact wave persists in the domain avoiding the scalar quantity to mix in the domain the numerical solution at simulation timeout is compared with the exact solution of the problem in fig 16 the hydrodynamic exact solution is obtained by resolving the two rarefaction riemann problem toro 2001 whilst the exact solution for the scalar advection is identical to the given initial conditions fig 16 underlines how the numerical solution correctly approximates the exact solution in all the domain sections the scalar discontinuity is perfectly maintained throughout the simulation confirming the accurate resolution of the steady contact wave 7 6 t6 scalar advection and diffusion in a dam break over a complex domain with this test we assess the solver capability in correctly preserving the liquid and scalar species mass when simulating the advection and diffusion of species during a dam break phenomena the test is particularly harsh due to the presence of fast wetting drying fronts and multiple discontinuous flow regions the test is an ad hoc setup inspired to a common benchmark for hydrodynamic codes e g brufau et al 2002 vanzo et al 2016 the domain is composed by a rectangle 0 75 15 15 m the bottom η x y is fixed during the simulation and defined as 23 η x y max 0 η 1 x y η 2 x y η 3 x y with η 1 x y 1 1 8 x 30 2 y 9 2 η 2 x y 1 1 8 x 30 2 y 9 2 η 3 x y 3 3 10 x 47 5 2 y 2 the initial conditions are given by 24 h 1 0 m if x 16 m h 0 125 m otherwise ϕ 1 0 5 if x 16 m ϕ 1 0 otherwise ϕ 2 0 if x 16 m ϕ 2 0 1 otherwise q x q y 0 m 2 s x y presenting a virtual dam at x 16 m separating two discontinuous volumes of water and scalar mass here the domain is discretized with 492 277 triangular cells with a maximum of characteristic length of 0 1 m the hydrodynamics setup features reflective wall boundaries a cfl of 0 95 and frictional sources compatible with a manning coefficient of n 0 01 m 1 3s the scalar setup features two initially unmixed species both with a constant and isotropic diffusion coefficient k c 0 25 m2 s fig 17 a illustrates the hydrodynamic left and scalar solutions right at the initial condition t 0 s at simulation start the virtual dam collapses instantaneously with an advancing wave that overtops the two small lateral humps fully circumvents the larger hump and reaches the opposite wall in about t 20 s at this time the interface between the two species in what would otherwise be a contact discontinuity on flat topography is still lagging by approximately 15 m fig 17b at this point the reflected bores propagate upstream and further mix both species symmetrically around the x axis by t 50 s these bores overcome the two smaller obstacles and propagate upstream on flat ground fig 17c after a continuous sloshing and interaction of reflected waves topography and lateral walls the friction sources gain relevance and dissipate most of the kinetic energy in the flow with a near static solution being obtained at approximately t 20 min the scalars continue to mix now due mostly to molecular diffusion in what is a much slower process that only vanishes at around 3 5 h as both scalars become fully homogeneous across the domain fig 18 the model is fully conservative with the total liquid and scalar mass preserved during the entire simulation as the simulation approaches the lake at rest conditions the observed quantities correctly converge to their resting values of h 0 364 m ϕ 1 0 386 and ϕ 2 0 023 fig 18 7 7 performance and scalability the performance and scalability of the software depends not only on the implemented parallelization strategies but also on the physical model to be reproduced in general models with only few simulated physical processes are likely to show higher computational performances to test basement s computational performance we selected the benchmarks t1 hydrodynamic t3 morphodynamic and t6 scalar advection diffusion each of the selected numerical experiments t1 t3 t6 has been conducted with four different computational meshes the sizes of these meshes are given in table 6 and have been chosen to cover a broad range of spatial resolutions ranging from thousands to hundred of thousands computational cells the simulations have been run with a set of computational backends in particular cpu based simulations have been performed on an intel xeon gold 6154 3 00 ghz workstation equipped with 36 cores two sockets with 18 physical cores each whilst gpu based simulations have been run on three gpus geforce gtx 1050 ti geforce gtx 1080 ti and tesla p100 see table 7 for the main characteristics moreover the simulations have been benchmarked in both single and double precision mode the gpus were integrated in a workstation with a 32 core intel xeon gold 5218 2 30 ghz processor two sockets with 16 physical cores each for a given mesh size the speedup achieved by a parallelized backend p is computed using the formula speedup t s t p where t s t p is the total computational time used by the serial parallel backend the results for all the investigated benchmarks are depicted in fig 19 as anticipated the speedup depends on the simulated processes comparing the speedup values among different benchmarks in fig 19 cases t1 hydrodynamics and t3 morphodynamics show on average higher values than t6 advection diffusion such results are expected given the increased complexity number of equations and operations to be solved of t6 the performance benefits of basement s parallelization can be evaluated in more detail by comparing the speedup values along the vertical axis of the plots in the following we focus on benchmark t3 morphodynamics which shows an intermediate scalability among the three benchmarks fig 19b looking at the mesh with 47k elements as an example the cpu based family i e openmp on multiple cores shows a speedup efficiency i e speedup over number of cores of 87 with 2 cpu cores speedup 1 7 and of 58 with 32 cores speedup 18 5 with an average efficiency of 74 basement performs even better on some gpu cards the least performing card gtx 1050 ti with double precision has a speedup of about 8 however note that speedup jumps to 20 when using the single precision version overall the speedup provided by the tested gpus ranges between 7 and 60 the benchmarks in fig 19 also show how the maximum speedup changes with mesh size computational backend and simulated processes for all three cases the cpu based parallelizations show a mild speedup increase with an increasing number of computational cells the dependency on the mesh size is slightly more pronounced when the number of computational cores is increased this reflects the fact that cpu based solutions have shared memory and minimal overhead for multi threading handling thus the domain size i e the data size does not represent a potential performance bottleneck focusing on t3 fig 19b the parallelization efficiency for 2 4 cpu cores is almost constant for all mesh sizes and above 80 on the other hand the efficiency for 16 32 cpu cores is larger than 70 only for mesh 4 218k the speedup of the gpu accelerated solutions shows not only a more marked dependency on the problem size but also on the simulated processes in benchmark t1 fig 19a the speedup clearly increases with problem size this can be explained with the overhead of gpu parallelization which becomes more and more negligible with increasing domain size conversely benchmark t6 fig 19c shows little impact of the domain size on the speedup in this case the scalability is limited by data transfers i e data bandwidth test t6 has the largest amount of data compared to test t1 and t3 because of the simulation of the 5 species 3 3 due to the current design of the op2 framework dataset dimensions e g the maximum number of species are statically assigned at compile time increasing this maximum value would be at the expense of scalability of the computational backends it is worth remarking that the gpu accelerated backends show an average speedup difference greater than 10 between the single and double precision versions of course the adequate choice depends on the requirements of the specific application the analysis above shows basement s performance on different computational backends and underlines the differences when simulating different processes the results summarized in fig 19 can also serve as a guideline for the interested reader user when choosing an appropriate computational configuration for a given application finally it is worth highlighting that all the tested hardware configurations can be easily installed in standard office workstations 8 conclusions in this paper we introduced the main features of basement version 3 a freeware tool for river simulation basement allows the simulation of a wide variety of hydro morphodynamic and scalar advection diffusion scenarios as illustrated with the test cases the software is able to efficiently capture large scale hydrodynamic processes modelled with several hundreds of thousand elements in good agreement with the measurements on the opposite end of the spectrum the morphological solver is able to handle demanding sediment transport scenarios well albeit with known limitations with the scalar advection diffusion module a further set of physical processes such as the fate of river pollutants can be accurately modelled the impact of this flexibility on the software performance is minimized by activating modules on request in basement s pre simulation step the advantage of this approach is that only the required kernels are scheduled for execution this together with op2 s ability to generate executable code for both multi core cpus and gpus permit basement to scale with both available features and available computational power such advantages are reflected in the presented benchmarks given a large enough domain the software shows a good parallel efficiency on the cpu and an even higher speedup when using gpus the basement project is in continuous advancement to optimize and include further features in the existing basic modules as an example the modelling of the sediment transport in presence of non uniform sediment size and the simulation of water temperature dynamics are in implementation phase on the other hand efforts are dedicated also to develop novel modelling solutions for river processes such as the bio morphodynamic feedback between vegetation and sediment transport table 2 of the supplementary material provides an overview of under development features the modularity of the development framework allows also for further refinement of single specific numerical solvers and the implementation of high order schemes when needed overall the combination of different river processes that can be modelled the computational efficiency the flexibility in the backend choice but also the availability of a light gui make basement a valuable tool for a broad family of river modelers in both academia and practice funding the development of the software basement is financially supported by the swiss federal office for the environment foen declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors thank the reviewers for the fruitful comments and suggestions the authors greatly thank the many former collaborators and developers within the basement project particular thanks to aurélie koch for her valuable contribution in testing and documenting the software the design of basement was conducted by dfv dv sp lv the software prototyping development and implementation was done by dv sp lv mb mw with the coordination and supervision of dfv and as implementation and testing of the reported features was done by mb dv mw and dc the manuscript was conceptualized by dv as and dfv and drafted by dv with support of mb and mw all authors contributed to the manuscript review appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105102 
25788,in the era of ubiquitous data collection and generation demands are high to make these data accessible as widely as possible with as little effort and as much power and flexibility as ever possible on earth data this holds in particular for pixel data and point clouds some of the main big data today coverages represent a unifying concept for space time varying data especially for spatio temporal gridded data nowadays often called datacubes coverage standards exist however their fundaments appear in places technically outdated imprecise and not suitable for the full spectrum of data due to this lack there is a danger of missing interoperability goals and impeding future directed big earth data services we introduce the conceptual coverage model of the forthcoming iso 19123 1 standard it is generic supporting all spatio temporal dimensions in a unified manner and is compatible with the existing coverage implementation standards of ogc and iso we demonstrate feasibility through concrete service examples keywords coverage spatio temporal field datacube conceptual model standards iso ogc 1 introduction we are living in an era where it is as easy and inexpensive as never before to collect data about our physical environment at a large scale both over large regions and over time the same holds for artificially generated data such as climate simulation output storage and processing capabilities the big data volume and velocity today are less of a problem than variety and veracity to continue using the v terms gaining insight from data requires intensive work on finding collecting homogenizing and combining them activities aiming to move from files to pixels and towards analysis ready data wulderet al 2016 herald the need for having data readily available for analysis and gaining insights without spending the major part of the effort on data preparation high level general concepts help and give guidance for such data preparation and based on them effective scalable interfaces and complete services can be built standards can help to give guidance to implementers but also foster interoperability of tools once widely adopted mathematically the realm of spatio temporally extended phenomena can be captured by tensor fields the corresponding concept in it world establishing a data and subsequently service model is given by coverages coverages are multi dimensional by nature such as 1 d sensor timeseries 2 d x y images 3 d x y t image timeseries and x y z subsurface information and 4 d x y z t atmospheric and oceanographic information the dimension axes spanning the coverage s extent can be of spatial temporal or abstract nature where abstract is understood in the sense of being neither spatial nor temporal or any combination of these as such a coverage forms a digital representation of some space time varying phenomenon for example a satellite image timeseries has two spatial and one temporal axis whereas a geo statistical datacube may have a temporal axis spatial extents as well as thematic axes such as gender and age categories providing the count of the relevant subset of the population as the range value so the spaces to be considered definitely reach beyond 4 d x y z t coverages essentially serve to describe spatially and possibly temporally extended phenomena the difference to a vector representation is that in such a coverage observations vary with location and time for example a highway represented by a polyline will have a name like a27 along all its extent invariably just one property is associated with the entire object in a coverage view of the same highway conversely its degree of surface humidity might be captured to serve for example networked cars underway obviously humidity varies individually along the extent of the highway and additionally changes over time conceptually this is continuous with an infinite amount of information but as sensors can deliver only a finite number of values the humidity technically is available only for a finite number of positions along the highway with the help of some method of interpolation humidity can be obtained also for positions between those where values are available directly creating an infinite continuous approximation of reality from a finite data set with all the well known caveats 1 1 it is virtually impossible for a representation based on a discretization to agree perfectly with the real world so the choice of a comparatively accurate discretization is clearly important iso 2004b this small scenario highlights that coverages regularly are substantially more complex and voluminous data structures than vector data in common terminology coverages typically constitute big data establishing a conceptual model for coverages is challenging for several reasons first the intrinsic complexity of this concept is contrasted with the need of a handy model that is easy to use also for non experts further each of the diverse application domains bring along their own requirements and conventions which need to be matched by the generic underlying model otherwise cross domain data communication remains theory multiple problems need to be addressed including how to achieve a unified handling of multi dimensional coordinates with space time or other semantics and the corresponding coordinate reference systems how to integrate different description methods such as raster points point clouds and geometric models how to deal with all the discretization problems that occur in the context of raster data the variety of encodings which diverge in particular on the metadata captured and many more over the last twenty years driven by substantial progress made in science and technology the coverage concept has evolved significantly today it is widely accepted among standardization bodies such as iso ogc and inspire which in turn was a trigger for tool implementers to adopt coverages a who s who of open source and proprietary tools support the ogc coverage implementation schema cis and web coverage service wcs standards suites dozens of petabytes of earth data are being served operationally through standalone services as well as critical mass data center federations however to the best of our knowledge today there is no implementation spanning the complete universe of coverage structures and even less so supporting combining all different coverage types typically coverage oriented tools concentrate on raster data while point clouds and meshes are dealt with independently raster oriented coverage implementations traditionally address 2 d horizontal data such as orthoimages dems and thematic raster maps basically 3d grid data have been present in weather forecast and climate modelling since the early days of supercomputers however these were accessible only by a few experts having logins on the storage servers and deep technical knowledge on the specialized formats used with the advent of array databases showing the potential of multi dimensional analysis such datacube services are increasingly getting in focus attempts towards a model for spatio temporally extended phenomena have been undertaken before as we will discuss in detail in section 2 in particular the iso 19123 standard written in about 2000 and adopted in 2004 defines coverages the coverage model of 19123 developed around 2000 deserves renovation and cleanup in several respects while it has been seminal in guiding coverage standardization it no longer represents the state of the art among the many shortcomings are some items are not described optimally with sometimes naïve definitions such as the term raster which is defined as a usually rectangular pattern of parallel scanning lines forming or corresponding to the display on a cathode ray tube treatment of grids is impractical and missing important cases treatment of multi dimensionality is neither homogeneous nor sufficiently general the top level distinction into discrete and continuous coverages blurs the picture and is not to the point making the standard well known as a hard read finally recent practice to split into fundamentals and implementation specifications requires a disentangling of concepts which in 19123 provide a confusing mix of concepts and implementation details goal of our work is not to propose an entirely new model but to accomplish consolidation the article builds on work done by the editor of the forthcoming abstract coverage model standard iso 19123 1 iso 2020c based on initial work presented to iso with din spec 18114 baumann and merticariu 2018 which forms the basis for the forthcoming 19123 1 coverage fundamentals standard as a companion to the adopted 19123 2 coverage implementation schema iso 2019c we base our coverage work on the concepts of 19123 but give more concise and correct definitions of the underlying concepts and altogether provide a framework which focuses on the concepts rather than realization and reorganizes presentation to ease overview and understanding for the same reason a semi formal approach is adopted which ensures succinctness while remaining understandable to readers without a background in formal semantics specification bauer and wössner 1982 as done by us e g in sql mda iso 2019d and ogc wcps baumann 2010 baumann 2021 note that we do not attempt at establishing an api for web services nor a query model as in databases however we attempt at preparing the scene by establishing an expressive interoperable coverage data model which is sufficiently complete and tractable for such next steps as such this contribution aims at allowing sharing and discussion of the forthcoming standard but also at discussing background theory and design rationales which do not fit into a standards specification document the remainder of this article is structured as follows in section 2 we inspect the state of the art the conceptual coverage model is presented in section 3 followed by a classification of key coverage types in section 4 an overview on the impact of the model is given in section 5 and section 6 concludes the plot 2 state of the art substantial work has been devoted in the coverage field on both conceptual and implementation level even though it often was not labelled as coverages we inspect the state of the art in mathematics physics computer science and standardization as this contribution deals with the fundamentals we put the main emphasis on conceptual work and less on implementation which will be the subject of a forthcoming companion paper 2 1 mathematics and physics we start with the common abstract notion of coverages as digital representations of space time varying phenomena ogc 2021 various methods are known for representing phenomena that vary across some given space 2 2 we will use space as a summary term for space time and other types of dimensions whenever unambiguous see discussion later in this contribution including analytical geometry boundary representations and the whole field of discretized pixel and voxel modelling a very generic model of spatially extended phenomena is provided by function representation f rep pasko et al 1995 implemented in hyperfun n n 2021 01a f rep represents shapes in n d space as single real valued functions f x 1 x n by definition points with a function result above zero are inside the object a value equal to zero indicates the border called isosurface values below zero define the outside of the object this model is capable of handling non manifold models as well as a mix of lower dimensional entities such as surfaces curves and points in a unified manner discrete fields such as voxel objects will appear as discretized functions which can be interpolated to obtain a continuous function this concept is complemented by r functions describing set theoretic operations r functions form the basis of the hyperfun programming language further a generalization to a constructive hypervolume has been proposed pasko et al 1995 which allows for modelling n d point sets with arbitrary attributes as such f rep is very close to coverages it seems like a worthwhile endeavour to describe coverages in terms of f rep and study operations with the goal of formally deriving coverage services this might even lead to new coverage service paradigms a very interesting formalization of fields is undertaken by liang nittel and hah mann using second order functionals liang et al 2016 this approach has proven successful already earlier on a subtype of fields discrete multi dimensional arrays baumann 1999 obviously such formalizations are particularly suitable for defining the semantics of declarative query languages on such structures baumann 2010 iso 2019d likewise relevant work has been published by galton galton 2004 who establishes a unified theory along a very similar line but considering both fields and discrete objects in space and time the general mathematical concept behind all of those is that of a tensor field mcconnell 1957 in linear algebra a tensor is known to be an n dimensional array in 1 d called a vector and in 2 d a matrix in practice vectors often are replaced by records to offer the convenience of named instead of numbered components the set of points making up a tensor field s space resembles a manifold rehmann 2020 munkres 1991 meaning that around every point that n dimensional space behaves like a multi dimensional euclidean space which simply put means each point has neighbouring points as we know it from physical space in a grid for example which spans such a space every point has exactly two neighbours above and below the point s position on that axis fig 1 in practice where we deal with finite grids the boundary points where no neighbours exist outside the grid limits constitute an exception to this rule generalizations of such a space such as general riemann spaces do carmo 1992 have been discussed however for practical relevance we stick with euclidean space finally points in n d space may be aligned along some grid of points which may be regular with constant distances among neighbours or irregular this actually is the prevailing structure for imagery climate model output and many more as especially square grids are convenient to handle through arrays in programming knuth 1968 the most well known grids have a quadrilateral shape 3 3 a fundamentally flawed term as quadrilateral i e with four corners is adequate only for the 2d regular case with identical spacing on both axes and the imaginary connection lines are neither in a 90 angle nor necessarily straight let us determine what other grid types are possible in n d space skipping the trivial 1 d case we find that in 2 d space there exist exactly three regular edge to edge tilings based on triangles squares or hexagons resp grünbaum and shephard 1977 coxeter 1989 see fig 2 in 3 d the only useful grid is given by the cuboid and likewise in 4 d space we find only the hypercube also called tesseract also for higher dimensions cuboid equivalents are the only grids possible the description of a field s mapping from points in space to their associated values can be done in many ways if the space is continuous then even a limited region as coverages normally are can contain a transfinite number of points which is impossible to represent in finite computer memory therefore other means have to be found one approach is to finite or transfinite cluster sets of points into a finite number of regions and store some finite description of each region as a representation of the field depending on the spatio temporal dimensions under consideration these regions can be given as 1 d curves 2 d surfaces 3 d solids or 4 d time variant solids for example iso 2019b note that such objects may well be embedded in some higher dimensional space such as a 2 d terrain surface draped on an elevation model in 3 d space another method for describing a transfinite continuous field is to use a finite set of 0 d points together with an interpolation method for obtaining values for points in between the ones stored sheppardchisholm 1911 grids also occur naturally e g at microscopic level in crystallography a regular repeated lineup of molecules atoms or ions is modelled by the mathematical structure of a geometric group theory lattice bravais 1850 such a lattice may be viewed as a regular tiling of some space the periodicity of regular grids which allows computing storage location of a cell through a linear addressing scheme corresponds to the periodic arrangement of the same particle or some regular patterns from a small number of particle types called motif in an ideal crystal the crystallographic restriction theorem as per coxeter coxeter 1989 has it that in any dimension there is only a finite number of distinct lattice patterns closest to regular grids as discussed in our geo context are bravais lattices bravais 1850 which are generated by translations corresponding to resolution in geo grids along each axis in euclidean 2 d space there exist 17 different types of periodic tilings based on groups of isometries however under the restriction that all cells have the same shape there are only three regular tiling types square triangular and hexagonal tesselations grünbaum and shephard 1977 hence the abstractions known in crystallography can provide a useful tool to determine the set of possible regular grids in 2d and higher dimensions aside from that it is not a current interest of research to unify geodesy and crystallography so we do not follow this approach further just note in passing that spatial grids are known in a variety of disciplines and treated there in a diversity of formalizations in computational fluid dynamics cfd and its applications like climate weather modelling and aerodynamics simulation grids are common partial differential equations as per navier navier 1822 and stokes stokes 1851 can describe the behaviour of gases and liquids subsumed under fluids and so numerical solver codes are common in these disciplines harlow and welch 1965 in the earth sciences we find many examples such as weather forecast air quality monitoring wildfire simulation ocean surface modelling and several more specifically in numerical weather prediction grids are often used which do not align with the grids common in giss differences include a rotated latitude longitude grid as well as using an icosahedron which is not in our above list of suitable grids instead of rectangular grids for the horizontal coordinates reinert et al 2021 vertical coordinates may rely on proxies such as pressure coordinate systems and additionally reduce resolution for higher altitudes which all make the vertical axis irregular lynch 2006 a further noteworthy technique is ensemble modelling where various simulations are run in parallel among other techniques older simulations are continued to deliver forecasts for further time in future while in parallel new runs are started based on the latest actual observations this leads to a second time axis named wallclock time in addition to simulated time a coverage therefore must be able to hold more than one time axis differentiated appropriately such as through unique axis name aliases while the time axis is regular in weather forecast experience shows it may be irregular with satellite imagery hence a mix of regular and irregular axes is common in coverage universe which will give rise to a specific grid modelling in section 4 3 along a different line arakawa arakawa and lamb 1977 has studied grid types which we will come back to later when introducing pixel in corner and pixel in area concepts 2 2 computer representations analytical geometry describing inside and outside of areas in the sense that positions inside such a geometric object which might be a line surface or solid own the properties associated with the object whereas locations outside don t the stated target of coverages is regular and irregular grids point clouds and meshes baumann a variety of representation schemes for coverages fields is in common use depending on the nature of the data and the operations applied to them we inspect them in order of their topological dimension going from 0 d points up to 3 d solids a classification from an atmospheric research viewpoint is presented by balaji and liang balaji and liang 2007 balaji et al 2006 they use grid as the overarching term and avoid mesh at all generally their definition is not underpinned by mathematical treatment but follows a more informal intuitive approach general point clouds basically are sets of coordinate value pairs an example for a common transformation of point clouds is thinning where too similar points get eliminated to condense the data set frequently though point clouds serve to extract higher level information through surface reconstruction and object recognition berger et al 2016 visualization levoy and whitted 1985 etc points that sit on some kind of grid are referred to as gridded or raster data as there is substantial confusion about the terms grids and meshes let us first define both a grid in our context means all points involved have pairwise distinct positions in n d space time further a connectivity criterion requires that every point has exactly two direct neighbours per dimension one with a lower and another one with a higher coordinate value in a cartesian grid also referred to as raster the minimum distance in a grid is 1 only at the outer boundaries of a grid may the number of neighbours per axis may reduce from two to one due to this regularity of grid data storage and processing is substantially more efficient than with point clouds which explains the success of imagery in all earth sciences some grid mappings come at the expense of discontinuities for example the mapping to a cuboid presented in fig 3 contains some cells like the ones in the center of the image which do not have diagonal neighbours in all directions in practice software iterating over such a structure will have to implement expensive case distinctions for the discontinuities and neighbourhood operations like convolutions may not return the expected result but generate artefacts hence such models tend to be not widely adopted an example being the discrete global grid system dggs specification purss 2017 relying on space filling curves on hexagonal primitives 4 4 specifically ddgs as adopted in ogc has additional disadvantages such as requiring the user to know the storage structure as opposed to the principle of data independence proven highly useful in data management in general walking up the dimension of the objects collected in a coverage we find 1 d curves 2 d surfaces and 3 d solids we collectively name these meshes there is a natural hierarchy in their modelling points delimit curves curves enclose surfaces and surfaces constitute the boundaries of solids these have been studied extensively in computer aided design and related domains but also in the gis area goodchild et al 2007 in computer aided design cad boundary representations b rep are standard which describe solids through hierarchical graphs whose nodes represent vertices edges faces and solids to delineate arbitrary bodies typically with the help of line and surface approximations like nurbs b rep has been studied extensively see for example braid et al 1980 mäntylä 1984 baumann 1988 mahdiraji 2015 constructive solid geometry csg is a method of describing general 3d bodies through parametrized primitives like sphere box cylinder cone and ring together with set like operations for building hierarchical combinations of such primitives thereby allowing generation of more complex shapes foley 1996 requicha and voelcker 1982 in the coverage model such objects might be represented through expressions in some suitable language such as mathml given this wide range of variations we tentatively do not fix a particular one but rather define an abstract dependency on some geometric model be it b rep csg or something else due to the normative character this reference needs to target a standard in this case is iso 19107 iso 2019b further relevant standards in the geographic domain include iso 19125 1 common architecture iso 2004b and 19 5 2 sql binding iso 2004c together with the ogc counterpart herring 2011 spatial data types are also defined in iso 13249 3 sql mm spatial iso 2016 generally speaking in the coverage context the main emphasis so far has been on gridded data such as satellite imagery atmospheric simulations and geophysical voxel data mesh data types have not yet been addressed systematically from a coverage perspective not to speak about issues of integrating them and converting between different representations starting with regular versus irregular grids up to grids versus meshes notably all these issues have been addressed in science and technology indeed see worboys 1995 for just a few examples it is an open research question to establish a common model suitable for standardization 2 3 standards 2 3 1 ogc and iso ogc abstract topic 6 is ogc s abstract specification of coverages developed around the year 2000 subsequently it has been adopted verbatim by iso as 19123 2004 iso 2004a the specification introduces data types which first of all are differentiated into discrete and continuous coverages discrete coverages fall into the subcategories discrete point coverage discrete grid point coverage discrete curve coverage discrete surface coverage and discrete solid coverage where the coverage domain is described by points curves surfaces or solids resp continuous coverages fall into thiessen polygon coverage hexagonal grid coverage segmented curve coverage tin coverage and continuous quadrilateral grid coverage this grouping is not entirely motivated for example there is no continuous solid coverage the relation between discrete grid coverage and a continuous quadrilateral grid coverage is not explained irregular grids seem missing in general the selection of grid types appears neither complete nor orthogonal interpolation is defined incoherently and only for 2 d several definitions are outdated at best in places unnecessarily involved and often inexact as the following examples illustrate raster usually rectangular pattern of parallel scanning lines forming or corresponding to the display on a cathode ray tube aside from the partial definition applicable only usually this relies on some particular in our days completely obsolete hardware and constrains raster data to 2 d a more modern definition would simply base on cartesian grids grid network composed of two or more sets of curves in which the members of each set intersect the members of the other sets in an algorithmic way this leaves open what exactly is to be understood by a network and what the algorithmic way mentioned should look like finally it excludes the 1 d case another criticism of 19123 is that in places it grounds on implementation oriented data structures rather than on high level concepts this reflects the then modern approach of packing all aspects from concepts to data storage into one specification whereas the modern approach in standardization establishes pairs of synchronized companion standards addressing concepts and implementation respectively such issues have contributed to an often heard impression that coverages are complicating facts and of less practical use one effect was that interpretations emerged which claimed conformance with 19123 while not being mutually compatible and interoperable which also did not send a message of suitability into the stakeholder communities hence while seminal in guiding coverage standardization for 15 years iso 19123 is not any longer representing the state of the art however the interoperability issue of the implementation standards was felt to be most pressing and consequently these were addressed first coming to the fundaments only recently as the big picture was clarified in the beginning of this renovation effort it is ensured that all specifications including forthcoming 19123 1 remain in sync in ogc the web coverage service standards working group wcs swg recently renamed to coverages swg is in charge of the coverage standards after a lack of acceptance of the wcs 1 x versions peter baumann mainly due to undue complexity a complete restart was launched starting from scratch and abandoning the entire existing coverage approach a separation into explicit data and service models was agreed to enable provision via services beyond wcs further the coverage model was based on the then recent gml 3 2 1 model portele 2007 as an improved corrected but backwards compatible version of its coverage model thereby harmonizing coverage and gml worlds for example the original distinction into rectified and referenceable grid coverages is kept the outcome was gml 3 2 1 application schema coverages gmlcov 1 0 baumann and its service companion wcs 2 0 baumann 2012b more on this in section 5 after some time unfortunately it turned out that the gmlcov title caused significant confusion because it suggested this was just a gml encoding not a general implementation model and so in 2015 ogc decided to rename gmlcov 1 0 to coverage implementation schema cis 1 0 cis offers several grid types grid coverage rectified grid coverage referenceable grid coverage point clouds multi point coverage and meshes multi curve coverage multi surface coverage multi solid coverage the often unnecessarily complicated grid types have been augmented with cis 1 1 general grid coverage and wcs 2 1 baumann 2019 as companion which can also handle coverage situations not previously addressed at the same time cis 1 1 dropped some ballast inherited from gml finally encodings for json and rdf were added altogether cis and wcs in combination ensure interoperability down to the level of single pixels through concise definitions accompanied by automated conformance tests note that the relevant standards both in iso and ogc are not always disjoint in all aspects for example an alternative and different realization of iso 19123 data type multipointcoverage is given by the iso 19107 data type pointcloud iso 2019b 2 3 2 inspire inspire is the legal framework for a homogenized european spatial data infrastructure in its three annexes it defines 34 spatial data themes and service requirements for practically all relevant geo data offered by agencies these requirements have been further broken down to conceptual data models and concrete service specification guidelines in 11 of these themes coverages play a key role fig 4 several issues have been spotted in the inspire coverage definition since its publication baumann and escriu 2019 however a de facto re harmonization has happened through inspire s use of ogc wcs normatively referencing ogc cis and not the inspire coverage model 2 3 3 w3c qb4st the world wide web consortium w3c spatial data on the web group in collaboration with some ogc members has established its own definition of a geo datacube qb4st w3c this approach is independent from and incompatible with ogc iso datacubes which uniformly are modelled through grid coverages main emphasis is on a list of metadata considered of particular importance less so the data i e direct positions and their associated values themselves qb4st expresses n d datacubes in terms of rdf triples through a concrete rdf data cube ontology the semantics of spatial axis coordinate values is given by crss as in coverages the exact handling of time coordinates is not further detailed grids are constrained to be regular only according to the specification document w3c qb4st has draft status and should not be considered endorsed by the w3c membership at large several concepts are marked as this is defined here pending availability of a canonical definition of spatial concepts at which point an equivalence will be declared as furthermore no complete example is provided the potential merits of this model are not easy to assess 3 a unified abstract coverage model 3 1 anatomy of coverages the term coverage in line with the definitions of ogc abstract topic 6 ogc 2006 and iso 19123 iso 2004a refers to a data representation that assigns values to positions in some multi dimensional space as such a coverage can be viewed conceptually as a mathematical function which for every value of its domain set provides a particular value taken from its defined range set as such a coverage c is given by a function c d p v where the domain set d of c is a non empty set of direct positions short for directly stored values for a particular position as opposed to interpolation derived in some space and v is a non empty set of values called the range set from which the function range values are taken p v denotes the power set of v each direct position always has at least one value assigned more than one value might correspond to one and the same direct position as several observation results in a coverage overlap or even share the identical position for example in a point cloud two points may share the same positions in a grid on the other hand by definition a grid cell contains exactly one value which in practice may be null the uml diagram of fig 5 sketches on a high abstract level a coverage data structure that can represent domain and range set plus further information discussed below in case of a finite domain set d the coverage can alternatively be written in pair set notation c p 1 v 1 p n v n for n d p n d v n v this notation of the coverage function often prompts an interpretation of coverages as just being sets of objects in standardization typically referred to as features while this view is not wrong and coverages do not make a statement about any potential independent existence it can be misleading first the coverage knows no other identification mechanism than the direct position second despite the set notation we need to bear in mind that this set adheres to some specific rules it cannot be concluded that naïve set operations will always work on coverages and actually they often do not a simple example is the union of two grids which requires application of a transitive hull to the union set to again end up with a valid grid for our discussion we rely on the domain range notation of a coverage while keeping in mind that further equivalent descriptions exist this situation is shown in the uml diagram of fig 5 which is extended with a range type description discussed later additionally an optional metadata container is added which allows augmenting the coverage data with any kind of metadata the coverage will not understand these data structures but transport them duly thereby keeping connection between data and metadata in practice this often will be simply a reference into some catalog while mathematically this is perfectly equivalent to our function based definition it is our experience that using this definition can lead to considerable confusion among practitioners as too often it is forgotten that such sets come with additional rules for the various subtypes which critically shape behaviour simply put a coverage is not just a set for example geometric features like curves surfaces and solids are assumed to have an interchangeable grid coverage representation consisting of a raster image obviously the mapping between raster and geometry is far from unambiguous and in particular during rendering of a geometric shape into a grid a substantial loss of accuracy and information content happens for these reasons we prefer the functional view as it allows conveniently expressing important coverage properties that said we will come back to this in section 5 when discussing representation alternatives in computer storage further a collection of concrete coverages encoded in xml json and rdf are available with ogc ogc 2021 01a in the remainder of this section we first abstractly define coverages through a probing function and then model each of the coverage constituents in turn 3 2 probing coverages we make use of a probing function for defining the data with the help of such a function which delivers the coverage s value for each of its direct positions we can systematically observe the behaviour of an object a principle dating back to the theory of algebraic specification of abstract data types adts bauer and wössner 1982 where the objects under discussion remain opaque defined only through their externally observable behaviour in case of coverages conceived as functions associating values with coordinates the appropriate probing function for extracting information about the data structure will retrieve the value associated with a particular point consider a coverage c with domain d and range set v for every direct position within d expressed in the coverage s native crs the function evaluate returns the set of values associated with this position evaluate c d v evaluate c p v p v c note that this probing function only serves for definition purposes it is not required to be implemented in practice higher level retrieval functionality is desirable such as bounding box subsetting in the ogc web coverage service wcs core baumann 2012b 3 3 coverage domain 3 3 1 definition the coverage s domain set describes for which positions in the coverage s multi dimensional space values are available these are the directly stored positions therefore they are called direct positions further values may be generated via interpolation the coverage domain describes some n dimensional space for some n 1 for closure reasons we could consider single position less scalars as 0 d coverages but this is not relevant for the purpose of this paper the multi dimensional space the coverage provides data for is defined through the domain s coordinate reference system crs every coverage is described by exactly one such crs its native crs which defines the domain space through a sequence of axes sometimes predefined crss contain everything needed such as epsg 4979 which contains two horizontal axes and one vertical axis but if such a predefined crs is not readily available then it can also be built on the fly through a compound crs where axes as well as potential sub crss are lined up in proper sequence such as a composition of said epsg 4979 with a time axis to describe a 4d space time crs in ogc crss like other entities are defined through urls an example for a compound crs is the following http www opengis net def crs compound 1 http www opengis net def crs epsg 0 4326 2 http www opengis net def crs ogc 0 ansidate discussion is underway in ogc to allow simplified representations such as the alias below as an equivalent to the url based identifier epsg 4326 ogc datetime the epsg catalog ogp 2021 contains several thousand mostly horizontal crss further crss and axes are under consideration by ogc such as pressure altitudes various calendars and more planetary bodies have shown to be amenable to coverage standards as well oosthoek et al 2014 rossi and hare 2016 and there are about 2500 celestial crss maintained by the international astronomical union iau currently as per iso 19111 iso 2019a ogc additionally foresees for a modular composition of crss from single axes and other crss misev et al 2012 the crs defines the theoretically available space for direct positions in practice bounded by minimum and maximum coordinates for each axis together forming the coverage s minimal bounding box note that if not stored explicitly it may be computationally expensive to determine the bounds from the domain set as we will see later when inspecting various coverage types in a service which potentially offers a large number of coverages such as mundi datacubes with more than 2500 objects n n b it is helpful if the coverage gives a hint about its location in a common rather than its native crs to this end in addition to its concise domain set a coverage carries a so called envelope which gives the approximate location and shape of the coverage through a not necessarily minimal bounding box for horizontal geo coordinates this is typically provided in wgs84 one possible reason for the envelope being larger than the domain set is that the envelope bounding box which must completely contain the coverage domain usually is rotated and distorted relative to the native crs bounding box this allows for an efficient rough search across large coverage sets for example to determine overlap with a particular region 3 3 2 mathematical vs physical coordinates the coverage concepts presented in this work are agnostic of the real life semantics of its dimensions geographic data typically have a subset of the following axes two horizontal axes one height axis expressing elevation or bathymetry and time climate and weather modelling add another time axis for differentiating model run time from the time modelled though these are tightly correlated a height axis can be replaced by a proxy such as pressure altitude in aeronautics pressure altitudes measured in hekto pascal hpa are considered but also more abstract proxies such as flight levels expressed in 100 ft steps like fl150 for 15 000 ft above some reference point mean sea level under the icao standard conditions icao 1993 non spatio temporal axes occur in practice as well for example bands in hyperspectral imagery make sense as a numbered sequence in cases where there are hundreds of bands such as the hyperion instrument on board of eo 1 with its 220 bands pearlman 2001 also in geostatistics non spatio temporal dimensions are frequent note also that originally spatial dimensions might become non spatial at some level of generalization for example cities in europe might originally be expressed through coordinates but at some higher level get abstracted to be in bavaria in the alps region etc which is at a symbolic level generally speaking abstract in the sense of non spatio temporal axes may occur as well like in olap where e g time product subsidiary axes are common for high performance optical sensors as used in remote sensing and astronomy spectral frequency can define a coverage axis as well spatial temporal and abstract axes may be mixed freely in a coverage domain definition in applications such as in spatial olap we are aiming at a level of abstraction which allows sufficient spatio temporal semantics while not excluding abstract dimensions and any mix of all of those 3 3 3 topological vs geometric dimension we distinguish the topological dimension of a coverage from its geometric dimension the topological dimension is given by the degrees of freedom the domain set offers for example the topological dimension of an orthoimage is 2 given by its flat grid underlying the domain set the geometric dimension aims at the length of the coordinate vector required to describe the coverage coordinates for a flat orthoimage this will be 2 d coordinates so topological and geometric dimension coincide but the coordinates could also be e g 3 d with a vertical component in this case the geometric dimension of the coverage is 3 hence larger than its topological dimension in general the topological dimension dt of a coverage is less or equal to its geometric dimension dg dt dg 3 4 common point rule except in grids it may happen that coverage cells coincide points in point clouds may incidentally share the same direct position multi curve and multi surface coverages may contain points where several lines meet and curves surfaces and solids in a coverage may intersect or even be identical many applications however are not prepared for such a set oriented treatment and can only deal with one value per position for such cases the coverage may provide a preference rule the so called common point rule to pick one value in case of co location of direct positions and declare it as the coverage value of this direct position generally a common point rule cptc is a function pertaining to a coverage c and a particular direct position p within the domain d and delivering a single value out of the options offered by the coverage at that position cptc d v cptc p evaluatec p the choice of the rule may be controlled through an extra parameter for example iso 19123 provides a code list with common options as follows adapted to match this article s terminology average mean of the cell values low minimum of the cell values high maximum of the cell values all set of all of the cell values start start value of the second segment in a segmented curve coverage end end value of the first segment in a segmented curve coverage beyond this 19123 definition the following additional options have been proposed oldest value with the least future directed time coordinate only applicable to coverages with a temporal axis newest value with the most future directed time coordinate only applicable to coverages with a temporal axis this seems due to a misinterpretation of spatio temporal coordinates time is just another axis and hence direct positions which differ in the time component actually are distinct and do not need any choice method in ogc wcs baumann et al 2018 for example a timeseries extraction at one spatial point would naturally yield the sequence of points with their proper time coordinates associated as a 1 d coverage see section 5 1 for an example the intended effect to pick a particular value along the time axis is accomplished through subsetting see wcs discussion in section 5 2 therefore we disregard oldest and newest iso 19123 extends the common point rule to coverages which allow interpolation as follows at positions in between direct positions the interpolation is performed first and the common point rule is applied on the resulting values we have some general reservations over this common point rule first and foremost making a choice out of existing values appears forceful and effectively hides some values in the coverage from the user further the options provided seem arbitrary and lack a common concept one option returns a set of values all others return single elements without motivation also start and end address only multi curve coverages as one very specific category while other relevant cases such as surfaces and solids remain unaddressed also average low and high might be augmented with say median as further option so this is by far not closed and final in passing we note that we are not aware of any implementation supporting this functionality for these reasons and despite being aware that the forthcoming 19123 1 will keep it we abandon the concept of a common point rule altogether and define the coverage evaluation function to always return a set of values in case this is undesirable such as with grids where the grid points are expected to have a non zero pairwise distance we express this through explicit constraints algorithmic aspects such as in overlay operations which require a choice to hide values of lower layers in presence of non null values in higher layers of a map stack get adequately addressed in the corresponding service operation definition 3 5 coverage range and coverage range type mathematically the range set equals its type so v says it all in practice however this is insufficient first coverages must carry type information for proper decoding in an application it might not be entirely clear what values like fl050 or 1 0 5 0 mean is the latter an imaginary number or wind components in computer science syntax coverages require dynamic typing and so the receiving tool must get this information which may or may not be deducible from the values themselves second applications require substantially more semantics than just the programming language s data type among the essential information are units of measure meters degrees etc null values such as 9999 which is often used in bathymetry accuracy measurement methodology and more an example for a high level semantic description of the value set is given by the ogc sensor web enablement swe common datarecord robin 2011 which supports the description of any data structure that sensors or data generators may deliver the datarecord structure is used in the implementable companion standard of 19123 1 coverage implementation schema baumann et al 2019 iso 2019c thereby enabling seamlessly passing of sensor information captured upstream to downstream coverage services 3 6 interpolation interpolation is a method for constructing new range values for coordinates within the domain set of a coverage which are aligned with direct positions the only positions for which the coverage explicitly provides values this can be useful for estimating values where none are present notably interpolation as opposed to extrapolation derives values for positions between direct positions for example in an image timeseries having latitude longitude and time axes several ways of interpolation may become relevant linear interpolation along latitude and longitude bilinear interpolation with temporal resolution unchanged in plain words all existing timeslices get extracted meaning no interpolation occurs along time interpolate each pixel s history i e along time using linear interpolation without any spatial interpolation linear interpolation along latitude longitude and time simultaneously trilinear interpolation in principle any point in the transitive hull of the domain set is amenable to interpolation we denote the transitive hull of some coverage c which in general is a superset of domain c as d c and define function interpolate as an extension to evaluate where interpolatec p evaluatec p for all p domain c interpolate c d v with d d c v range c many interpolation methods are in use the most prominent being nearest neighbour and polynomial such as linear quadratic and cubic less used methods include barycentric on tins and lost area for thiessen polygons and hexagonal grids 5 5 the firm grounding of iso 19123 in 2 d remote sensing shines through by mentioning bi linear bi quadratic and bi cubic methods rather than their general n d counterparts as the interpolation function is based on the coverage domain and range type these may impose natural restrictions on the type of interpolation applied first we inspect the domain set impact note that the domain set type as an analogy to range type is given by the crs more specifically the axis definitions 6 6 although today s available crs definitions do not convey this information explicitly neither human nor machine readable the fact is recorded that e g latitude and longitude are represented by real values index grid coordinates representing integer numbers do not allow expressing any in between value as it would not be an integer anymore therefore interpolation is not possible simply due to the lack of points that are not direct positions physical coordinates like latitude longitude height and time are expressed through real numbers and consequently allow expressing values between direct positions interpolation methods may go over a single axis or over several axes in combination in an x y t image timeseries datacube one might want to apply bilinear interpolation along lat long and independently perform nearest neighbour interpolation along time next let us see examples of how the range type choice can affect interpolation polynomial interpolation on a real valued range type such as radiometric intensities would deliver as interpolation result some value between the contributing direct positions delivering more or less smooth transitions a categorical range type such as land use does not allow arbitrary values as interpolation results thereby restricting the choice of interpolation methods e g polynomial interpolation is not applicable whereas nearest neighbour is bottom line while the choice of applicable interpolation methods is to some extent determined by the domain and range data type there is still a degree of choice therefore we associate with a coverage a possibly empty set of applicable interpolation methods any tool processing a coverage should only apply those methods listed to not violate the coverage semantics conceptually a coverage with several interpolation methods which all will yield different interpolation results defines a family of coverages where all members contain the same values at the direct positions themselves but will convey potentially different values for the interpolated non direct positions on a side note one might also consider interpolation between components of the range values such as in a combined handling of red green and blue channels together however we feel that this is entering into the field of general analytics and do not include such methods specifically under interpolation 3 7 discrete and continuous coverages based on the previous elaboration we now can define discrete and continuous coverages an axis is called discrete if every possible interval with finite bounds describes a finite set of values otherwise such an axis is called continuous a coverage is called discrete if its axis list contains only discrete axes a coverage is called continuous if its axis list contains at least one continuous axis the new iso 19123 1 lists a series of examples for illustration iso 2020c a map of postal code zones is a coverage which is discrete in its range the postal code zones cover an entire country and at every location in the country one can evaluate the coverage function and get a value that represents the postal code for that location within a postal code zone the value is constant one cannot interpolate such a discrete coverage a coverage that maps a set of polygons to the soil type found within each polygon is a coverage which is discrete in its range a point set representing a set of measurements that are only valid at the position of each point and which cannot be interpolated is a discrete coverage discrete in domain an image sampled by a sensor may be represented as a grid coverage consisting of a set of pixels corresponding to grid cells in the domain of the coverage a value is associated with each grid cell however since the coverage is continuous an interpolation function such as linear quadratic or cubic may be applied so that a continuously variable value may be determined at any location within the domain extent of the image in a coverage that maps direct positions in san diego county to their temperature at noon on a specific day both domain and range may take an infinite number of different values this continuous coverage would be associated with a discrete coverage that holds the temperature values observed at a set of weather stations discrete in domain whereby the measured values correspond to a point set coverage this point set coverage is discrete because each point can only have one value the associated continuous coverage uses the point values as driving values for the coverage function and allows interpolation between the points a set of bathymetric soundings is a discrete point set coverage with a single measured water depth value at each point location discrete in domain an associated continuous coverage allows one to interpolate between the measured depth soundings to determine the bottom surface of a body of water evaluation of a triangulated irregular network involves interpolation of values within a triangle composed of three neighbouring point value pairs hence the distinction between discrete and continuous is not a primary first class criterion but a consequence of particular coverage properties historically in iso 19123 this has been the first level differentiation of coverages a well known reason for sustained confusion 3 8 metadata in practice more data is necessary for providing relevant context think of the satellite instrument pixels are coming from spectral characteristics of the sensor processing steps applied etc in common terminology further metadata need to be associated with a coverage given the foundational nature of coverage data such metadata have to satisfy a wide range of domains unfortunately despite standards like iso 19115 iso 2020a and inspire metadata inspire there is not a generic concise agreed definition of what this metadata should comprise even worse even within focused domains frequently there is no accepted consensus confusion about metadata is as old as the term itself yielding sentences like your metadata are my data we have encountered the same in discussions about coverages for example sometimes the domain set is considered metadata sometimes data as it is part of the coverage concept in reality we observe several distinct levels of metadata fig 6 pertaining to coverages at the lowest level the range set sits as the data of concern these are described by the second level that we call technical metadata in the sense that they are indispensable for accessing the range set values among these are for example the underlying domain definition with its crs and the range data type one level above geo descriptors add location and temporal information to the coverage s range values here we find geographic and temporal coordinates crss and units of measure of the range values these levels together constitute the coverage concept on top of that applications may add any type of further metadata such as provenance information therefore as a way forward in the concrete coverage implementation schema baumann et al 2019 we have foreseen a slot for metadata in general with an undefined structure to allow any type of data to be stored there from simple references to a corresponding catalog record up to elaborate records describing the contributing footprint for each patch inside the coverage the coverage will not know about the semantics of these metadata but it will duly carry them along thereby at least maintaining connection between data and metadata 3 9 internal structuring of coverages the abstract concept of a function mapping direct positions to values can be modelled through various different data structures while this may rightfully be considered an implementation detail and hence out of scope on the level of abstraction of this paper and iso 19123 1 we still provide it as a guidance for possible concretizations and as a bridge to the concretization of iso 19123 2 and ogc cis where several alternatives are broken down to concrete encodings in xml json and rdf the uml diagram in fig 7 summarizes them one method of storing the domain range function mapping is through a set of coordinate value pairs this approach is particularly convenient for timeseries streaming a complementary approach makes use of two collections one for the direct positions of the domain set and the other for the corresponding range values both can be correlated unambiguously via their position in the collection this scheme is attractive because there are very efficient methods for representing the domain set particularly in case of regular grids the values in this case form a multi dimensional array where value positions can be computed for retrieval in combination with range set compression this leads to very storage efficient encodings the two modes of organization described above represent extremes of a wide space of possible schemes while coordinate value pairs have the finest granularity single positions and consequently the largest number of items the domain range representation consists of one single monolithic item in between is the universe of tilings i e partitioning following some regular or irregular pattern specifically for very large gridded data sets tiling is highly advantageous be it for efficient retrieval in datacube engines baumann et al 2010 or for efficient extraction from large files in formats like tiff n n 2021 01b and netcdf n n 2021 0a where it is often referred to as chunking often such tilings are simple with regular tile sizes and shapes but they can be irregular as well such as for adjusting to particular access patterns a comprehensive study of tiling patterns has been accomplished by furtado furtado and baumann 1999 ogc iso cis version 1 1 supports arbitrary tiling and even organization into sub coverages under certain homogeneity conditions baumann et al 2019 a completely different approach is possible if the coverage function can be described analytically in this case the range set does not have to be materialized instead a definition of a generating function is stored in some suitable format like mathml while there is little practical relevance for grid coverages and point clouds it is a widely used method in 3d csg modelling in cad cam 4 coverage typology by dimension 4 1 overview in this section we establish classes of coverages aligned with the relevant application areas of coverages these types in a nutshell are grids point clouds and general meshes instead of using this directly we employ a more structurally justified classification based on the topological dimension of the elements describing the direct positions within a coverage starting from ordered and non ordered 0 d points with gridded data being a special case and moving up to higher dimensions fig 8 we stop at full 3d x y z space acknowledging that higher dimensions are possible however to the best of our knowledge there are no practically relevant applications that said the hierarchy and data structures can be extended at any time without affecting the lower dimensional levels for example the spatial dimensions and their characteristic behaviour in the coverage types may well be augmented with temporal and other axes such as to model time varying solids in the following subsections coverage types are introduced sorted along their topological dimension 4 2 multi point coverages a point cloud or multi point coverage is a coverage whose domain set consists of a set of points in other words direct positions are unconnected points in the domain set given by their point coordinates the evaluation function consequently specializes as follows for some coverage c and position p evaluatec p v point x domain c x p v c x 4 3 grid coverages 4 3 1 anatomy of grids by far the most important application of coverages today is given by raster data i e data sitting on some grid long known as raster images and more recently as earth datacubes baumann et al 2021 which have first been introduced in baumann 1992 baumann 1994 in standardization these are all modelled through the family of grid coverages a special case of a multi point coverage where all direct positions are constrained to sit on some grid in the coverage domain formalization of regular and irregular grids is straightforward we call ac a1 an an axis sequence remembering it is determined by the coverage s native crs just as is the dimension n of the coverage domain along each axis ai we pick a non empty subset of points gi in practice each such set will be finite with ki 0 elements so we can write it as gi gi 1 gi ki we now can define the grid coverage domain as a set of direct positions g g1 gn g1 gn gi gi for 1 i n topologically every inner point in a grid has exactly two neighbours along each axis one with a lower coordinate value and another one with a higher coordinate value along this axis border cells have exactly one such neighbour cell each in direction of the grid s border corner cells have exactly one neighbour in each direction this neighbourhood relation can be considered indicative to distinguish grids from meshes in particular in discussions where mathematical language is better avoided in comparison the definition of a grid in 19123 was a network composed of one or more sets of curves in which the members of each set intersect the members of the other sets obviously this excludes the 1 d case and further does not prohibit touching and crossing of the generating curves as a first classification such an n dimensional grid is said to be regular if for all pairwise neighbouring direct positions the distance in each axis is constant otherwise the grid is called irregular this corresponds to the definition of rectified grid coverage and georeferenceable grid coverage resp in the iso 19123 standard under revision a more fine grain distinction adopted in 19123 1 we will be introduce below in passing we note that the lower and upper bound along each domain axis defines the minimum bounding box as expressed in the coverage s native crs often portele 2007 baumann the minimum and maximum values respectively are grouped into diagonal corner points for defining the grid extent by construction direct positions in a grid are pairwise disjoint for each direct position there is exactly one associated value in the range of the function a direct consequence is that in a raster image every pixel can contain only one value overlaps resulting in several pixel values for some given coordinate are not admissible we argue that this is actually natural from the viewpoint of analysis ready data while overlaps say of satellite scenes is common during acquisition and processing both human users and image processing tools get confused and have to pay extra attention to artefacts resulting from acquisition such as overlapping tracks which get cleared during generation of more analysis ready products such as for datacubes raw satellite swath data can be modelled as individual irregular grid coverages as well they just cannot be combined into one coverage due to their non matching grids 4 3 2 axis based grid specification the classical subdivision into rectified i e regular grid coverages and referenceable i e irregular grid coverages is rather coarse and does not allow expressing relevant details such as all grid types shown in fig 9 for example a case which we frequently meet in practice when building datacubes n na is timeseries on orthoimagery in this case the horizontal part of the 3 d grid is regular whereas distribution along time typically is irregular fig 9 center in traditional nomenclature this would simply be a general referenceable grid neglecting the regularity of the spatial axes thus causing excessive storage and processing overhead in practice as the more expensive general representations for irregular grids will be used by the system rather than the efficient regular grid mechanics in the classical rectified grid coverage of the original 19123 standard this cannot be represented as one axis is irregular rather this needs to be modelled as a 19123 referenceable grid coverage where all axes are considered irregular leading to a massive storage and processing overhead for those axes which actually are regular and can be described by a simple resolution value therefore we propose to abandon the classification into several grid types and instead advocate a more fine grain distinction which considers each axis individually for the direct position s regularity properties this leads us to the following axis types cf fig 10 an index axis is a 1 d cartesian axis there is no geo reference admissible coordinates for direct positions are at discrete integer positions and unit less technically expressed by a unit of 1 and there is no datum mapping defined which could relate to some physical position as a consequence of the discrete nature of the axis no coverage interpolation is possible between adjacent coordinates a regular axis has an equi distant spacing like an index axis but is continuous and not constrained to integer positions and distances such an axis can be georeferenced i e it can have a spatial or temporal semantics attached units of measure typically degrees meters seconds years etc are defined with the axis crs which also should specify meaningful interpolation due to the regularity valid coordinate positions can be expressed as multiples of a given distance the resolution added to an initial offset the lower bound coordinate an irregular axis is continuous possibly geo referenced the distances between neighbouring points are non zero but otherwise arbitrary a displacement axis nest or warped nest is a set of continuous possibly georeferenced axes forming a subset of the grid s axes relative to a regular grid each direct position is shifted by some individual offset within the crs space spanned by the axes participating in the extreme case such a nest spans all axes of the grid which resembles the referenceable grid of earlier standards algorithmic axes are given by a set of discrete or continuous possibly geo referenced axes where the actual coordinates are not indicated but have to be derived algorithmically from some otherwise abstract parameter set provided hence the alternative name transformation model nature data type and evaluation of such parameter types is tentatively left unspecified but must be defined on an application specific basis examples of such algorithmic axes include sensor models where instead of the coordinate information a set of sensor parameters such as ground control parameters is provided which must be fed into the model for deriving the actual direct positions by combining all the above axis types freely any type of grid can be modelled we can substantiate this claim with the observation that not only are the common well known cases represented but the warped nest allows to effectively assign individual positions to each direct position in the grid as long as the basic grid definition is not violated ordered positions positive distance between grid lines still the list of possible axis types is not comprehensive some standards or applications may define their own specialized axis types comparing this with the previous classification of grid coverages baumann we can easily map them into this new schema rectified grid coverages are those where the underlying grid consists of only regular axes referenceable grid coverages are all those where the underlying grid consists of at least one irregular axis it is easy to see that the axis based classification allows substantially more fine grained differentiation in the universe of regular and irregular axes furthermore several classes of coverages can be described now which were not supported by the earlier classification namely warped nests algorithmic axes and all combinations of the various axis types described 4 3 3 further grid types in section 2 we have discussed the grid types possible in euclidean n space and found that cuboid grids exist in any dimension while in 2 d triangular and hexagonal grids additionally exist constrained by the fact that the axes for coordinate expression must be straight uninterrupted lines in the tiling pattern in each dimension which includes skewed non orthogonal axes since simple coordinate translations can be found the hexagonal case at first glance seems to contradict this however with a trick this case can be mapped back to the square case by considering as direct positions not the grid corners but the hexagon centers fig 11 this situation can be described through a rectified grid in which the two offset vectors are of equal length but differ in direction by 60 the length l of a side of the hexagon is l s tan 30 where s is the length of the offset vector in a computer the values of such a coverage range can therefore be stored as a multi dimensional array with an adjusted index addressing scheme by way of background the hexagons are the thiessen polygons generated around the grid points altogether after this inspection we can confirm that square and with less relevance triangular and hexagonal grids are the only grid generating patterns possible for coverages 4 3 4 varying cell location interpretation independently from the coordinates and their arrangement through crss there is often an interpretation of a value as sitting next to the coordinate point in our terminology direct position this seems to result from a visual misunderstanding of a grid it is commonly accepted that the cell values in a cartesian grid are associated with the exact coordinate whereas in earth sciences cell contents often is associated with a position between neighbouring direct positions in our approach and commonly in mathematics values are associated with the exact coordinates of their direct position in remote sensing this is referred to as pixel in corner also referred to as pixel is area however a common practice in gis is to consider a pixel in center position also referred to as pixel is point typically meaning the geometric center of the grid cuboid in 2 d square in a regular grid fig 12 visualizes both interpretations technically this is typically described by metadata such as tags in a tiff file indicating an interpretation of pixel in center versus pixel in corner implementations such as libtiff n n 2021 use code lists which guide implementation of the processing tool indeed this split interpretation is mainly driven by the geotiff format which has picked these two special cases while ignoring all others a more general systematic overview on a number of such value shift cases has been established by arakawa arakawa and lamb 1977 but it can be generalized even more to give the value a center of gravity in any direction and distance of its position as indicated by its coordinate in the most general case the grid value is determined by a sensor specific intensity distribution integral associated with the value s coordinate position a typical case is the instantaneous field of view of a sensor collecting radiosity on some area on ground based on this view we prefer an explicit modelling which does not hide the actual algorithmic treatment of data one way of doing so is to describe a pixel in center situation by a coordinate shift of half a pixel size remember this discussion only applies to regular grids which in turn can be modelled as a concatenation of the given crs with an engineering crs having a correspondingly shifted origin by half the pixel size such a crs concatenation is explicitly foreseen in iso 19111 iso 2019a an additional characterization would be to capture the intensity distribution something which currently is not considered in the earth sciences except in very special situations of sensor calibration and validation in some situations this point versus area distinction can become particularly cumbersome for example in atmospheric research different physical parameters may be subject to different regimes with respect to position assignment relative to the coordinates concretely mass of a cell may be associated with the center between neighbouring coordinates pixel in center whereas wind speed may be associated with corners potentially even split into different corners for the u and v horizontal components generally this idea begs several questions the concept is discussed on regular grids only what would it mean in case of irregular grids discussion seems to focus on lat long what about pixel in x for time etc why is it assumed that pixels have clearly delineated polygons and do not overlap from sensor physics it rather seems that generally a pixel is obtained through a weighted integral conceptually going from the ccd element s center to infinity but at least involving some neighbourhood possibly larger than the distance to the next element why is it assumed that the value of a pixel is spatially extended and constant over some neighbourhood interpolation will yield different values even close to a pixel the pixel areas discussed seem to apply only to nearest neighbour interpolation where nearest already is hard to define consider irregular grids krieging etc pixel in corner refers to what corner in general 2 d has 4 corners 3 d has 8 corners etc not to mention triangular and hexagonal grids as discussed above further why does the principle have to be confined to a corner vs the center in the first place and not at 75 supporting just corners seems random see ogc 2021 01b for a recent documented version of this recurring discussion altogether we consider such variations of the grid theme to be implementation details and do not address them on this contribution s level of abstraction 4 4 multi curve coverages after addressing 0 dimensional multi point coverages with their special case grid coverage we move on to coverages whose delineating shapes have a non zero topological dimension such coverages essentially align with geometric structures as they appear for example in 3d city modelling and computer aided design cad a multi curve coverage is a coverage consisting of a collection of curves that is the behaviour of the field described by the coverage is modelled through curves a curve is given by an ordered list of two or more points in which case it resembles a single or poly line with straight connections or a point list with additional curve information such as splines ferguson 1964 by way of example a multi curve coverage might describe routes that have numbers and names a pavement width and a pavement material type assigned to each segment of some road network formally the coverage evaluation function is given for some coverage c and position p as follows evaluatec p v curve x domain c x contains p the containment predicate on which this definition relies contains refers to the corresponding probing predicate on the geometric elements contained in the coverage such as the ones defined in iso 19107 iso 2019b obviously it is heavily dependent on the way the direct positions are described through direct enumeration of the direct positions example point clouds containment descriptions example curves areas and volumes or some algorithmically involved algorithm example ground control points in sensor models while we allow any method on principle to specify the direct positions we focus on the most common methods used in practice see section 4 from a multi curve coverage a corresponding multi point coverage can be constructed from the bounding points of the curves often the result is unambiguously defined but in general it depends on the representation mechanism of the curve see next conversely a multi curve coverage can be derived from a multi point coverage but in the general case will not be unique depending on the application the evaluation function may return at most one element no crossing routes in our example or may return several values in case of crossings and bridges as the geometric description of curves is dependent on their endpoints although maybe not completely such as in non straight cases the coverage modelling becomes a 2 level hierarchy where 1 d curves rely on 0 d points mappings from multi curve to multi point coverages are conceivable a trivial mapping is by down propagating all coverage values from the curves to the points and dropping the curve information this is of less practical use than other mappings where the curves get rendered into series of points at some given distance i e resolution as lidar returns such point clouds of objects it is of interest to study multi curve and multi point coverages in combination for applications such as object reconstruction this will be of even more importance with the multi surface and multi solid coverages direct positions on such a curve take on the value associated with the curve object other direct positions of the coverage are undefined 4 5 multi surface coverages continuing up the hierarchy of topological dimensions we next address surfaces where 2 d elements determine the coverage domain set a multi surface coverage is a coverage where the direct positions are described through surfaces the formal definition is structurally identical to the previous one just with curves replaced by surfaces evaluatec p v surface x domain c x contains p from a multi surface coverage a corresponding multi curve coverage can be constructed from the bounding curves of the surfaces often the result is unambiguously defined but in general it depends on the representation mechanism of the surface see below conversely a multi surface coverage can be derived from a multi curve coverage but in the general case it will not be unique the description of a surface relies on a bounding curve ring with or without holes which in turn relies on points at both ends of each line segment to remain unique the bounding polygons should be planar thereby defining a surface plane the de facto standard for describing curved surfaces is nurbs rogers 2000 based on control points and polygons direct positions on such a surface take on the value associated with the surface other direct positions of the coverage are undefined various practically relevant subtypes of multi surface coverages exist including polyhedral surfaces and their special case of triangulated irregular networks tins which often are used for digital elevation models dems kumler 1994 iso 19107 offers a data type triangulatedsurfacedata iso 2019b as an example land use can be represented by a multi surface coverage where the spatial domain is composed of one surface per cadastral parcel the coverage values associated with such parcels then may constitute soil types ownership etc a 3 d example is given by iso surfaces of atmospheric data such as temperature wind speed etc fig 13 multiple values again may be forbidden by design such as iso surfaces which by definition do not cross touch or overlap in practice multi surface coverages can be used to model bundles of equi potential surfaces such as air pressure in atmospheric data 4 6 multi solid coverages with coverages representing spatial bodies we reach the final hierarchy level 3 d solids a multi solid coverage is a coverage where the direct positions are described through solids the formal definition again is structurally identical to the previous ones albeit based on solids evaluatec p v solid x domain c x contains p from a multi solid coverage a corresponding multi surface coverage can be constructed from the bounding surfaces of the solids often the result is unambiguously defined but in general it depends on the representation mechanism of the solid see next conversely a multi solid coverage can be derived from a multi surface coverage but in the general case it will not be unique solids can be described in various ways such as constructive solid geometry csg and boundary representation b rep csg requicha and voelcker 1977 relies on a small set of analytically described standard volumes such as sphere cylinder cone and torus a concrete object consists of instantiations of these positioned and scaled as required by combining solids through regularized set operations arbitrarily complex objects can be built recursively constructive hypervolume modelling is an extension to this pasko et al 2001 conversely b rep braid et al 1980 models solids through their boundary surfaces edges and vertices such graphs need to adhere to several nontrivial geometric and topological integrity constraints including non self intersection singularity avoidance planarity of the surface graph etc b rep solids can be constructed piecewise through so called euler operations which maintain all these constraints mäntyla baumgart 1972 an example is shown in fig 14 from a coverage perspective b rep is a natural way of describing solids csg modelling parametric representations etc comprise alternatives which easily might be added 4 7 mixed coverages so far we have established coverages of different types sorted along the topological dimension of the spatio temporal objects describing the mapping to values plus specific extra constraints that allow modelling grids as special cases of point clouds it is a natural next step to allow coverages with a mixed description this bears significant practical relevance for example data formats like gmljp2 colaiacomo et al 2018 gdb esri 2021 geopackage ogc 2021 and safe esa 2021 01 all allow incorporating different data structures within the same package not to speak about common container formats like zip and tar which are allowed by ogc cis 1 1 as well formally let ci d v for 1 i n n 0 be coverages of any of the types introduced above all sharing the same domain and range then a mixed coverage c can be defined in a straightforward manner as c d v c p u i 1 n c i p for p d mixed coverages are not currently addressed by standardization in a formal manner which may have to do with the intrinsic complexity of the corresponding services however pragmatically several data formats exist such as gmljp2 colaiacomo et al 2000 which support such mixed data without any particular conceptual underpinning 5 coverage services although the service model is not at the heart of this contribution we exemplarily discuss how the coverage data model can benefit encoding provision access and analytics of coverages this also gives some impression about the practical relevance and impact of the overall coverage work of which we only report a specific part here following the thrust of this paper we prioritize standardized approaches further as grid coverages form the by far most relevant category these days we focus on those 5 1 coverage representation and encoding the abstract concepts presented in this contribution define a foundation for coverage structures but not to the level of concrete interoperability this is achieved via a concrete implementation schema for example the ogc iso coverage implementation schema cis derived from the 19123 1 model among others cis is concrete enough to define an extensible ecosystem of encodings allowing to express coverages in common formats like geotiff netcdf and many more a recurring problem with the plethora of data formats is their varying capability of representing metadata even the purely technical metadata like domain crs and range type often cannot be encoded still such formats are practically important as they often offer efficient binary representations compression etc conversely informationally complete encodings where all coverage information can be carried along such as cis xml json and rdf tend to be inefficient in storage footprint and access to combine the best of these extremes cis has introduced a mixed representation where a container format is used for gathering a canonical header in an informationally complete format such as xml json or rdf paired with one or more files containing range values in any suitable format by factoring out high volume parts such as the range sets from the header it is possible to store them efficiently in some binary format while retaining the complete coverage contents in cis encodings for xml json and rdf are already included as optional packages further formats are defined separately for geotiff netcdf jpeg 2000 grib2 etc ogc 2021 5 2 coverage access the purposeful step of wcs 2 0 to separate the data from service model has substantially broadened the opportunities for accessing coverages making it possible to utilize wcs wfs vretanos 2014 wms de la beaujardiere 2006 wps mueller and pross 2015 sos bröring et al 2012 and further service application programming interfaces apis however the most comprehensive and streamlined functionality is available with the wcs suite of standards see baumann 2019 for the authoritative specifications baumann 2019 and baumann et al 2018 for an overview in contrast wfs only allows retrieving a complete coverage as it does not support multi dimensional subsetting capabilities wcs centers around a mandatory core which allows download of a coverage bounding box extraction and format encoding further bespoke functionality is specified in optional extensions including range band subsetting reprojection data upload to the server and datacube analytics see next section request encoding in wcs speak protocol binding is uniformly through the http web protocol but can follow different flavours get kvp xml post soap and more recently and still heavily under development oapi we present get and oapi for comparison both with a request on a 3 d x y t image timeseries from which subsetting extracts a reduced footprint trim in lat and long at a particular position in time slice the get request signals that it is a getcoverage belonging to the wcs family followed by the identifier of the coverage addressed and the subsetting coordinates one per axis and is finalized by specification of the return format image 2 conversely the following request does a slicing only in space thereby focusing on a specific point on earth while not mentioning time at all thereby extracting everything available along time in case of a 2d lat long coverage the result would be a single point in case of a 3d lat long time datacube as is the case here the result is the history of that point conveniently expressed as comma separated values image 3 oapi tries to partially avoid query parameters preferring a pseudo restful syntax to accomplish the same here a rephrasing of the first wcs example above image 4 with the rasdaman array dbms the concept of internet accessible datacubes amenable to individual queries and not requiring deep technical skills was pioneered some of the epigons provide some sort of query language with n n c or without spatio temporal coordinate support paradigm4 others just a direct programming interface nowadays typically in python which represents a serious security issue any code can be submitted to the server from anywhere and gets executed in an unsupervised manner coverage based apis on the contrary offer high level access and processing with streamlined functionality hence easier and safer to use an overview of 19 different datacube technologies is presented in the research data alliance report rda 2018 the aforementioned rasdaman engine which is the ogc coverage reference implementation gives access to multi dimensional coverages via wms for visualization wcs for data extraction reformatting and download and wcps for safe server side analytics internally all these operations are translated into queries of rasdaman s array language rasql which has been the blueprint for the array extension to the iso sql query language sql mda multi dimensional arrays iso 2019d see misev and baumann 2015 for an overview 5 3 coverage analytics ogc web coverage processing service wcps is a high level datacube analytics language with built in spatio temporal semantics crafted tightly around the ogc iso coverage model through the wcs processing extension wcps is tied into the overall wps ecosystem a wcps request sends a query string usually with the identifiers of one or more available coverages as input parameters and returns a set of result scalars in case of aggregation or coverages the idea is to allow any query any time on any size rather than having just a predefined set of functions which is either too limiting or too hard to oversee such wcps expressions can range from simple extraction to any complexity and length processing in the server includes data fusion with any number of coverages filtering of coverages based on their data properties multi coverage analytics and encoding results for shipping them back to the client a simple example would deliver the average temperature from an era5 climate model at a particular geo position across all times and all heights image 1 due to the high level nature of the language complex analytics can be tasked without any programming further the language is amenable to heavy optimization parallelization distributed processing etc in a fully transparent manner without user or administrator intervention this has been demonstrated on dozens of petabytes with the rasdaman engine n n b baumann et al 2017 which also boosts the earthserver datacube federation n n 2021 0b the largest of its kind worldwide the same underlying formal framework is standardized as the sql datacube extension mda multi dimensional arrays iso 2019d which resembles the rasdaman array query language the underlying algebraic foundation baumann 1999 and operational capability is the same as with wcps modulo the space time semantics built into wcps in fact rasdaman internally translates wcps requests to md queries for processing an integrated analytics language supporting all coverage types in the spirit of database languages like e g secondo güting et al 1999 nidzwetzki and güting 2017 mql mahdiraji 2015 sql mda iso 2019d and ogc wcps baumann 2010 baumann 2021 is a topic of further research and a forthcoming paper 6 conclusion we have presented a unifying abstract model for coverages which is in advanced status of adoption becoming iso 19123 1 and updated abstract topic 6 in ogc and compatible with the existing implementation standard 19123 2 while a standard has to respect diverse constraints such as requiring little domain knowledge and backwards compatibility across multiple specifications an article like this has the scientific freedom to adopt an abstract independent position hence a fresh view we hope that it fosters discussion of the forthcoming 19123 1 standard and can act as meaningful input stimulating future evolution of coverage concepts and technology our key contributions are as follows we have developed a coherent framework for the representation of spatio temporally extended phenomena regardless of their dimension but particularly suitable for spatio temporal data the conceptualization established to the best of our knowledge has not been done before in a way that is equally comprehensive and suitable for deriving implementation standards for scalable interoperable services this framework is modernizing clarifying and enhancing the existing standards coverage data model while remaining essentially compatible moreover concepts are simple and can also be easily communicated to non geo experts a comprehensive treatment of all possible grid types is proposed including mixed type grids where different axes follow different patterns such as cartesian versus regular versus irregular we hope that this paper can act as a companion document to the standard giving background on design decisions and explaining concepts something which is not appropriate in a standard itself the unified concepts can contribute to closing gaps in coverage modelling and in particular for devising flexible easy to use but comprehensive and powerful earth data services for increasing insight on the big earth data being continuously collected the coverage concept is at the heart of big earth data analytics it unifies handling across all spatio temporal dimensions it integrates a number of essential data structures namely regular and irregular grids point clouds and general meshes it is matured through extensive discussion among a large number of data generating and consuming communities from upstream satellite operators to downstream precision farming to name but two and there is not only a concise yet flexible data model but also a versatile powerful service model ranging from simple extraction to high end analytics and fusion both of which are streamlined and establish interoperability across all coverage types and formats as more and more services emerge and data volumes grow by multi terabytes per day the issues of interoperability and easy to use yet powerful access and processing becomes more and more urgent the recent quest for analysis ready data underlines this a common well founded coverage model therefore is a key pillar for powerful user friendly services practical impact of 19123 1 which this paper anticipates is substantial establishing the foundations of a whole ecosystem of further standards within iso we first find 19123 2 the coverage implementation schema which is a concretization of the 19123 1 concepts ready for implementation and conformance testing further there is a series of additional standards using coverages iso 19111 geographic referencing normatively references coverages iso 2019e iso 19115 metadata iso 2014 iso 19129 imagery gridded and coverage data framework iso 2009a iso 19130 imagery sensor models for geopositioning iso 2018a iso 19131 data product specifications iso 2007 iso 19136 geographic markup language gml including coverage encoding iso 2020d iso 19144 classification systems iso 2009b iso 19157 data quality utilizing coverages for representing quality information iso 2018b iso 19156 observations and measurements o m iso 1915 iso 19163 2 metadata and encoding rules for imagery and gridded data iso 2020b most of these have their parallel in ogc the inspire legal framework for a common european spatial data infrastructure relies on iso ogc coverages as the basis for the conceptual models defined for various inspire themes as well as the inspire wcs specification inspire 2021 0 not surprisingly a large number of open source and proprietary tools implement coverages their interoperability relies on a concise adequate conceptualization our own work focuses on flexible scalable services on massive heterogeneous and distributed multi dimensional datacubes manifest in the rasdaman array database system as editors of the ogc and iso coverage standards we continuously use rasdaman as our platform for evaluating implementability and so rasdaman naturally has become reference implementation various services have been deployed on basis of rasdaman strictly based on the ogc coverage standards wcps style processing at large has been demonstrated on earth data assets exceeding 50 pb single queries have been parallelized across more than 1000 amazon cloud nodes and data fusion on coverages is routine in the earthserver federation hence it is safe to say that the ogc coverage data and service models are practice proven and allow for fast scalable and flexible implementations a long avenue of further research remains to be done with the abstract coverage model presented here hopefully acting as a stable basis for further innovation most importantly this work establishes only the data model leaving processing and service models for future research we list some of the challenges remaining current wcs core defines subsetting only on 0 d coverages that is grids and point clouds tentatively this core has been kept as simple as possible to have a low entry barrier for implementers more complex functionality can always be put into optional extensions such an extension could consider higher dimensional subsetting commonly also called clipping there when considering higher dimensional coverages there are two possible avenues first subsetting could be defined on multi curve surface solid coverages in addition implementation wise this is substantially more complex as it involves polygon clipping and objects intersected will need to be substantially reshaped as opposed to the simple 0 d case where a point grid or not is either inside or outside the subsetting area along the same line the current restriction to bounding box subsetting could be lifted to allow general polygons first steps have been undertaken by the ogc wcs metocean application schema trevelyan 2021 which allows among others polygon corridors for subsetting this is driven by an aviation use case where atmospheric conditions during a flight need to be retrieved from an x y z t weather datacube to obtain the weather along the airplane s trajectory and exactly at the time the plane passes a point with some margin along all dimensions this can be generalized further to intersection between arbitrary objects where the subsetting description can be of any shape and the subsetted object can be any type of coverage from this it is only a small step to allow general coverage joins where multiple coverages sitting on a server get combined through set operations like in cad another open field of research is coverage to coverage transformation simple cases like scaling a grid coverage are well understood however transformations between different coverage types are also common and important specifically between grid coverages and multi surface coverages object reconstruction transforms from sampled grids to vectorial representations rendering conversely produces gridded representations from vectors the famous marching cube algorithm lorensen and cline 1987 with its many improvements over time is one way of obtaining a surface model from gridded data however although vector raster integration is common in giss piwowar 1990 such conversion at the same time is frowned upon by some researchers such as van der knaap van der knaap 1992 while by nature inexact in the general case such conversions are implemented in an ad hoc manner not based on a common agreed mathematical framework thereby sometimes leading to unexpected results carver 1994 and often not guaranteeing the same result across different tools altogether there is a huge body of research and implementation which might be consolidated into a unified coverage transformation theory and possibly standard we believe that coverages contribute essentially to better leveraging the potential of big earth data and are instrumental for understanding our planet and acting adequately across all fields from academia to industry from governments to citizens we hope that the coverage fundamentals presented in this work and under standardization by iso and subsequently ogc will help better our understanding of our environment declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the author gratefully acknowledges valuable discussion and contribution by douglas o brien emmanuel devys graham wilkes liping di kathi schleidt jordi escriu and many more colleagues in ogc iso industry and academia who took patience in many discussions on all aspects of coverages the reviewer comments greatly have helped to improve the paper which is gratefully acknowledged this work has been supported through german bmbf deeprain eu h2020 parsec and eu h2020 centurion 
25788,in the era of ubiquitous data collection and generation demands are high to make these data accessible as widely as possible with as little effort and as much power and flexibility as ever possible on earth data this holds in particular for pixel data and point clouds some of the main big data today coverages represent a unifying concept for space time varying data especially for spatio temporal gridded data nowadays often called datacubes coverage standards exist however their fundaments appear in places technically outdated imprecise and not suitable for the full spectrum of data due to this lack there is a danger of missing interoperability goals and impeding future directed big earth data services we introduce the conceptual coverage model of the forthcoming iso 19123 1 standard it is generic supporting all spatio temporal dimensions in a unified manner and is compatible with the existing coverage implementation standards of ogc and iso we demonstrate feasibility through concrete service examples keywords coverage spatio temporal field datacube conceptual model standards iso ogc 1 introduction we are living in an era where it is as easy and inexpensive as never before to collect data about our physical environment at a large scale both over large regions and over time the same holds for artificially generated data such as climate simulation output storage and processing capabilities the big data volume and velocity today are less of a problem than variety and veracity to continue using the v terms gaining insight from data requires intensive work on finding collecting homogenizing and combining them activities aiming to move from files to pixels and towards analysis ready data wulderet al 2016 herald the need for having data readily available for analysis and gaining insights without spending the major part of the effort on data preparation high level general concepts help and give guidance for such data preparation and based on them effective scalable interfaces and complete services can be built standards can help to give guidance to implementers but also foster interoperability of tools once widely adopted mathematically the realm of spatio temporally extended phenomena can be captured by tensor fields the corresponding concept in it world establishing a data and subsequently service model is given by coverages coverages are multi dimensional by nature such as 1 d sensor timeseries 2 d x y images 3 d x y t image timeseries and x y z subsurface information and 4 d x y z t atmospheric and oceanographic information the dimension axes spanning the coverage s extent can be of spatial temporal or abstract nature where abstract is understood in the sense of being neither spatial nor temporal or any combination of these as such a coverage forms a digital representation of some space time varying phenomenon for example a satellite image timeseries has two spatial and one temporal axis whereas a geo statistical datacube may have a temporal axis spatial extents as well as thematic axes such as gender and age categories providing the count of the relevant subset of the population as the range value so the spaces to be considered definitely reach beyond 4 d x y z t coverages essentially serve to describe spatially and possibly temporally extended phenomena the difference to a vector representation is that in such a coverage observations vary with location and time for example a highway represented by a polyline will have a name like a27 along all its extent invariably just one property is associated with the entire object in a coverage view of the same highway conversely its degree of surface humidity might be captured to serve for example networked cars underway obviously humidity varies individually along the extent of the highway and additionally changes over time conceptually this is continuous with an infinite amount of information but as sensors can deliver only a finite number of values the humidity technically is available only for a finite number of positions along the highway with the help of some method of interpolation humidity can be obtained also for positions between those where values are available directly creating an infinite continuous approximation of reality from a finite data set with all the well known caveats 1 1 it is virtually impossible for a representation based on a discretization to agree perfectly with the real world so the choice of a comparatively accurate discretization is clearly important iso 2004b this small scenario highlights that coverages regularly are substantially more complex and voluminous data structures than vector data in common terminology coverages typically constitute big data establishing a conceptual model for coverages is challenging for several reasons first the intrinsic complexity of this concept is contrasted with the need of a handy model that is easy to use also for non experts further each of the diverse application domains bring along their own requirements and conventions which need to be matched by the generic underlying model otherwise cross domain data communication remains theory multiple problems need to be addressed including how to achieve a unified handling of multi dimensional coordinates with space time or other semantics and the corresponding coordinate reference systems how to integrate different description methods such as raster points point clouds and geometric models how to deal with all the discretization problems that occur in the context of raster data the variety of encodings which diverge in particular on the metadata captured and many more over the last twenty years driven by substantial progress made in science and technology the coverage concept has evolved significantly today it is widely accepted among standardization bodies such as iso ogc and inspire which in turn was a trigger for tool implementers to adopt coverages a who s who of open source and proprietary tools support the ogc coverage implementation schema cis and web coverage service wcs standards suites dozens of petabytes of earth data are being served operationally through standalone services as well as critical mass data center federations however to the best of our knowledge today there is no implementation spanning the complete universe of coverage structures and even less so supporting combining all different coverage types typically coverage oriented tools concentrate on raster data while point clouds and meshes are dealt with independently raster oriented coverage implementations traditionally address 2 d horizontal data such as orthoimages dems and thematic raster maps basically 3d grid data have been present in weather forecast and climate modelling since the early days of supercomputers however these were accessible only by a few experts having logins on the storage servers and deep technical knowledge on the specialized formats used with the advent of array databases showing the potential of multi dimensional analysis such datacube services are increasingly getting in focus attempts towards a model for spatio temporally extended phenomena have been undertaken before as we will discuss in detail in section 2 in particular the iso 19123 standard written in about 2000 and adopted in 2004 defines coverages the coverage model of 19123 developed around 2000 deserves renovation and cleanup in several respects while it has been seminal in guiding coverage standardization it no longer represents the state of the art among the many shortcomings are some items are not described optimally with sometimes naïve definitions such as the term raster which is defined as a usually rectangular pattern of parallel scanning lines forming or corresponding to the display on a cathode ray tube treatment of grids is impractical and missing important cases treatment of multi dimensionality is neither homogeneous nor sufficiently general the top level distinction into discrete and continuous coverages blurs the picture and is not to the point making the standard well known as a hard read finally recent practice to split into fundamentals and implementation specifications requires a disentangling of concepts which in 19123 provide a confusing mix of concepts and implementation details goal of our work is not to propose an entirely new model but to accomplish consolidation the article builds on work done by the editor of the forthcoming abstract coverage model standard iso 19123 1 iso 2020c based on initial work presented to iso with din spec 18114 baumann and merticariu 2018 which forms the basis for the forthcoming 19123 1 coverage fundamentals standard as a companion to the adopted 19123 2 coverage implementation schema iso 2019c we base our coverage work on the concepts of 19123 but give more concise and correct definitions of the underlying concepts and altogether provide a framework which focuses on the concepts rather than realization and reorganizes presentation to ease overview and understanding for the same reason a semi formal approach is adopted which ensures succinctness while remaining understandable to readers without a background in formal semantics specification bauer and wössner 1982 as done by us e g in sql mda iso 2019d and ogc wcps baumann 2010 baumann 2021 note that we do not attempt at establishing an api for web services nor a query model as in databases however we attempt at preparing the scene by establishing an expressive interoperable coverage data model which is sufficiently complete and tractable for such next steps as such this contribution aims at allowing sharing and discussion of the forthcoming standard but also at discussing background theory and design rationales which do not fit into a standards specification document the remainder of this article is structured as follows in section 2 we inspect the state of the art the conceptual coverage model is presented in section 3 followed by a classification of key coverage types in section 4 an overview on the impact of the model is given in section 5 and section 6 concludes the plot 2 state of the art substantial work has been devoted in the coverage field on both conceptual and implementation level even though it often was not labelled as coverages we inspect the state of the art in mathematics physics computer science and standardization as this contribution deals with the fundamentals we put the main emphasis on conceptual work and less on implementation which will be the subject of a forthcoming companion paper 2 1 mathematics and physics we start with the common abstract notion of coverages as digital representations of space time varying phenomena ogc 2021 various methods are known for representing phenomena that vary across some given space 2 2 we will use space as a summary term for space time and other types of dimensions whenever unambiguous see discussion later in this contribution including analytical geometry boundary representations and the whole field of discretized pixel and voxel modelling a very generic model of spatially extended phenomena is provided by function representation f rep pasko et al 1995 implemented in hyperfun n n 2021 01a f rep represents shapes in n d space as single real valued functions f x 1 x n by definition points with a function result above zero are inside the object a value equal to zero indicates the border called isosurface values below zero define the outside of the object this model is capable of handling non manifold models as well as a mix of lower dimensional entities such as surfaces curves and points in a unified manner discrete fields such as voxel objects will appear as discretized functions which can be interpolated to obtain a continuous function this concept is complemented by r functions describing set theoretic operations r functions form the basis of the hyperfun programming language further a generalization to a constructive hypervolume has been proposed pasko et al 1995 which allows for modelling n d point sets with arbitrary attributes as such f rep is very close to coverages it seems like a worthwhile endeavour to describe coverages in terms of f rep and study operations with the goal of formally deriving coverage services this might even lead to new coverage service paradigms a very interesting formalization of fields is undertaken by liang nittel and hah mann using second order functionals liang et al 2016 this approach has proven successful already earlier on a subtype of fields discrete multi dimensional arrays baumann 1999 obviously such formalizations are particularly suitable for defining the semantics of declarative query languages on such structures baumann 2010 iso 2019d likewise relevant work has been published by galton galton 2004 who establishes a unified theory along a very similar line but considering both fields and discrete objects in space and time the general mathematical concept behind all of those is that of a tensor field mcconnell 1957 in linear algebra a tensor is known to be an n dimensional array in 1 d called a vector and in 2 d a matrix in practice vectors often are replaced by records to offer the convenience of named instead of numbered components the set of points making up a tensor field s space resembles a manifold rehmann 2020 munkres 1991 meaning that around every point that n dimensional space behaves like a multi dimensional euclidean space which simply put means each point has neighbouring points as we know it from physical space in a grid for example which spans such a space every point has exactly two neighbours above and below the point s position on that axis fig 1 in practice where we deal with finite grids the boundary points where no neighbours exist outside the grid limits constitute an exception to this rule generalizations of such a space such as general riemann spaces do carmo 1992 have been discussed however for practical relevance we stick with euclidean space finally points in n d space may be aligned along some grid of points which may be regular with constant distances among neighbours or irregular this actually is the prevailing structure for imagery climate model output and many more as especially square grids are convenient to handle through arrays in programming knuth 1968 the most well known grids have a quadrilateral shape 3 3 a fundamentally flawed term as quadrilateral i e with four corners is adequate only for the 2d regular case with identical spacing on both axes and the imaginary connection lines are neither in a 90 angle nor necessarily straight let us determine what other grid types are possible in n d space skipping the trivial 1 d case we find that in 2 d space there exist exactly three regular edge to edge tilings based on triangles squares or hexagons resp grünbaum and shephard 1977 coxeter 1989 see fig 2 in 3 d the only useful grid is given by the cuboid and likewise in 4 d space we find only the hypercube also called tesseract also for higher dimensions cuboid equivalents are the only grids possible the description of a field s mapping from points in space to their associated values can be done in many ways if the space is continuous then even a limited region as coverages normally are can contain a transfinite number of points which is impossible to represent in finite computer memory therefore other means have to be found one approach is to finite or transfinite cluster sets of points into a finite number of regions and store some finite description of each region as a representation of the field depending on the spatio temporal dimensions under consideration these regions can be given as 1 d curves 2 d surfaces 3 d solids or 4 d time variant solids for example iso 2019b note that such objects may well be embedded in some higher dimensional space such as a 2 d terrain surface draped on an elevation model in 3 d space another method for describing a transfinite continuous field is to use a finite set of 0 d points together with an interpolation method for obtaining values for points in between the ones stored sheppardchisholm 1911 grids also occur naturally e g at microscopic level in crystallography a regular repeated lineup of molecules atoms or ions is modelled by the mathematical structure of a geometric group theory lattice bravais 1850 such a lattice may be viewed as a regular tiling of some space the periodicity of regular grids which allows computing storage location of a cell through a linear addressing scheme corresponds to the periodic arrangement of the same particle or some regular patterns from a small number of particle types called motif in an ideal crystal the crystallographic restriction theorem as per coxeter coxeter 1989 has it that in any dimension there is only a finite number of distinct lattice patterns closest to regular grids as discussed in our geo context are bravais lattices bravais 1850 which are generated by translations corresponding to resolution in geo grids along each axis in euclidean 2 d space there exist 17 different types of periodic tilings based on groups of isometries however under the restriction that all cells have the same shape there are only three regular tiling types square triangular and hexagonal tesselations grünbaum and shephard 1977 hence the abstractions known in crystallography can provide a useful tool to determine the set of possible regular grids in 2d and higher dimensions aside from that it is not a current interest of research to unify geodesy and crystallography so we do not follow this approach further just note in passing that spatial grids are known in a variety of disciplines and treated there in a diversity of formalizations in computational fluid dynamics cfd and its applications like climate weather modelling and aerodynamics simulation grids are common partial differential equations as per navier navier 1822 and stokes stokes 1851 can describe the behaviour of gases and liquids subsumed under fluids and so numerical solver codes are common in these disciplines harlow and welch 1965 in the earth sciences we find many examples such as weather forecast air quality monitoring wildfire simulation ocean surface modelling and several more specifically in numerical weather prediction grids are often used which do not align with the grids common in giss differences include a rotated latitude longitude grid as well as using an icosahedron which is not in our above list of suitable grids instead of rectangular grids for the horizontal coordinates reinert et al 2021 vertical coordinates may rely on proxies such as pressure coordinate systems and additionally reduce resolution for higher altitudes which all make the vertical axis irregular lynch 2006 a further noteworthy technique is ensemble modelling where various simulations are run in parallel among other techniques older simulations are continued to deliver forecasts for further time in future while in parallel new runs are started based on the latest actual observations this leads to a second time axis named wallclock time in addition to simulated time a coverage therefore must be able to hold more than one time axis differentiated appropriately such as through unique axis name aliases while the time axis is regular in weather forecast experience shows it may be irregular with satellite imagery hence a mix of regular and irregular axes is common in coverage universe which will give rise to a specific grid modelling in section 4 3 along a different line arakawa arakawa and lamb 1977 has studied grid types which we will come back to later when introducing pixel in corner and pixel in area concepts 2 2 computer representations analytical geometry describing inside and outside of areas in the sense that positions inside such a geometric object which might be a line surface or solid own the properties associated with the object whereas locations outside don t the stated target of coverages is regular and irregular grids point clouds and meshes baumann a variety of representation schemes for coverages fields is in common use depending on the nature of the data and the operations applied to them we inspect them in order of their topological dimension going from 0 d points up to 3 d solids a classification from an atmospheric research viewpoint is presented by balaji and liang balaji and liang 2007 balaji et al 2006 they use grid as the overarching term and avoid mesh at all generally their definition is not underpinned by mathematical treatment but follows a more informal intuitive approach general point clouds basically are sets of coordinate value pairs an example for a common transformation of point clouds is thinning where too similar points get eliminated to condense the data set frequently though point clouds serve to extract higher level information through surface reconstruction and object recognition berger et al 2016 visualization levoy and whitted 1985 etc points that sit on some kind of grid are referred to as gridded or raster data as there is substantial confusion about the terms grids and meshes let us first define both a grid in our context means all points involved have pairwise distinct positions in n d space time further a connectivity criterion requires that every point has exactly two direct neighbours per dimension one with a lower and another one with a higher coordinate value in a cartesian grid also referred to as raster the minimum distance in a grid is 1 only at the outer boundaries of a grid may the number of neighbours per axis may reduce from two to one due to this regularity of grid data storage and processing is substantially more efficient than with point clouds which explains the success of imagery in all earth sciences some grid mappings come at the expense of discontinuities for example the mapping to a cuboid presented in fig 3 contains some cells like the ones in the center of the image which do not have diagonal neighbours in all directions in practice software iterating over such a structure will have to implement expensive case distinctions for the discontinuities and neighbourhood operations like convolutions may not return the expected result but generate artefacts hence such models tend to be not widely adopted an example being the discrete global grid system dggs specification purss 2017 relying on space filling curves on hexagonal primitives 4 4 specifically ddgs as adopted in ogc has additional disadvantages such as requiring the user to know the storage structure as opposed to the principle of data independence proven highly useful in data management in general walking up the dimension of the objects collected in a coverage we find 1 d curves 2 d surfaces and 3 d solids we collectively name these meshes there is a natural hierarchy in their modelling points delimit curves curves enclose surfaces and surfaces constitute the boundaries of solids these have been studied extensively in computer aided design and related domains but also in the gis area goodchild et al 2007 in computer aided design cad boundary representations b rep are standard which describe solids through hierarchical graphs whose nodes represent vertices edges faces and solids to delineate arbitrary bodies typically with the help of line and surface approximations like nurbs b rep has been studied extensively see for example braid et al 1980 mäntylä 1984 baumann 1988 mahdiraji 2015 constructive solid geometry csg is a method of describing general 3d bodies through parametrized primitives like sphere box cylinder cone and ring together with set like operations for building hierarchical combinations of such primitives thereby allowing generation of more complex shapes foley 1996 requicha and voelcker 1982 in the coverage model such objects might be represented through expressions in some suitable language such as mathml given this wide range of variations we tentatively do not fix a particular one but rather define an abstract dependency on some geometric model be it b rep csg or something else due to the normative character this reference needs to target a standard in this case is iso 19107 iso 2019b further relevant standards in the geographic domain include iso 19125 1 common architecture iso 2004b and 19 5 2 sql binding iso 2004c together with the ogc counterpart herring 2011 spatial data types are also defined in iso 13249 3 sql mm spatial iso 2016 generally speaking in the coverage context the main emphasis so far has been on gridded data such as satellite imagery atmospheric simulations and geophysical voxel data mesh data types have not yet been addressed systematically from a coverage perspective not to speak about issues of integrating them and converting between different representations starting with regular versus irregular grids up to grids versus meshes notably all these issues have been addressed in science and technology indeed see worboys 1995 for just a few examples it is an open research question to establish a common model suitable for standardization 2 3 standards 2 3 1 ogc and iso ogc abstract topic 6 is ogc s abstract specification of coverages developed around the year 2000 subsequently it has been adopted verbatim by iso as 19123 2004 iso 2004a the specification introduces data types which first of all are differentiated into discrete and continuous coverages discrete coverages fall into the subcategories discrete point coverage discrete grid point coverage discrete curve coverage discrete surface coverage and discrete solid coverage where the coverage domain is described by points curves surfaces or solids resp continuous coverages fall into thiessen polygon coverage hexagonal grid coverage segmented curve coverage tin coverage and continuous quadrilateral grid coverage this grouping is not entirely motivated for example there is no continuous solid coverage the relation between discrete grid coverage and a continuous quadrilateral grid coverage is not explained irregular grids seem missing in general the selection of grid types appears neither complete nor orthogonal interpolation is defined incoherently and only for 2 d several definitions are outdated at best in places unnecessarily involved and often inexact as the following examples illustrate raster usually rectangular pattern of parallel scanning lines forming or corresponding to the display on a cathode ray tube aside from the partial definition applicable only usually this relies on some particular in our days completely obsolete hardware and constrains raster data to 2 d a more modern definition would simply base on cartesian grids grid network composed of two or more sets of curves in which the members of each set intersect the members of the other sets in an algorithmic way this leaves open what exactly is to be understood by a network and what the algorithmic way mentioned should look like finally it excludes the 1 d case another criticism of 19123 is that in places it grounds on implementation oriented data structures rather than on high level concepts this reflects the then modern approach of packing all aspects from concepts to data storage into one specification whereas the modern approach in standardization establishes pairs of synchronized companion standards addressing concepts and implementation respectively such issues have contributed to an often heard impression that coverages are complicating facts and of less practical use one effect was that interpretations emerged which claimed conformance with 19123 while not being mutually compatible and interoperable which also did not send a message of suitability into the stakeholder communities hence while seminal in guiding coverage standardization for 15 years iso 19123 is not any longer representing the state of the art however the interoperability issue of the implementation standards was felt to be most pressing and consequently these were addressed first coming to the fundaments only recently as the big picture was clarified in the beginning of this renovation effort it is ensured that all specifications including forthcoming 19123 1 remain in sync in ogc the web coverage service standards working group wcs swg recently renamed to coverages swg is in charge of the coverage standards after a lack of acceptance of the wcs 1 x versions peter baumann mainly due to undue complexity a complete restart was launched starting from scratch and abandoning the entire existing coverage approach a separation into explicit data and service models was agreed to enable provision via services beyond wcs further the coverage model was based on the then recent gml 3 2 1 model portele 2007 as an improved corrected but backwards compatible version of its coverage model thereby harmonizing coverage and gml worlds for example the original distinction into rectified and referenceable grid coverages is kept the outcome was gml 3 2 1 application schema coverages gmlcov 1 0 baumann and its service companion wcs 2 0 baumann 2012b more on this in section 5 after some time unfortunately it turned out that the gmlcov title caused significant confusion because it suggested this was just a gml encoding not a general implementation model and so in 2015 ogc decided to rename gmlcov 1 0 to coverage implementation schema cis 1 0 cis offers several grid types grid coverage rectified grid coverage referenceable grid coverage point clouds multi point coverage and meshes multi curve coverage multi surface coverage multi solid coverage the often unnecessarily complicated grid types have been augmented with cis 1 1 general grid coverage and wcs 2 1 baumann 2019 as companion which can also handle coverage situations not previously addressed at the same time cis 1 1 dropped some ballast inherited from gml finally encodings for json and rdf were added altogether cis and wcs in combination ensure interoperability down to the level of single pixels through concise definitions accompanied by automated conformance tests note that the relevant standards both in iso and ogc are not always disjoint in all aspects for example an alternative and different realization of iso 19123 data type multipointcoverage is given by the iso 19107 data type pointcloud iso 2019b 2 3 2 inspire inspire is the legal framework for a homogenized european spatial data infrastructure in its three annexes it defines 34 spatial data themes and service requirements for practically all relevant geo data offered by agencies these requirements have been further broken down to conceptual data models and concrete service specification guidelines in 11 of these themes coverages play a key role fig 4 several issues have been spotted in the inspire coverage definition since its publication baumann and escriu 2019 however a de facto re harmonization has happened through inspire s use of ogc wcs normatively referencing ogc cis and not the inspire coverage model 2 3 3 w3c qb4st the world wide web consortium w3c spatial data on the web group in collaboration with some ogc members has established its own definition of a geo datacube qb4st w3c this approach is independent from and incompatible with ogc iso datacubes which uniformly are modelled through grid coverages main emphasis is on a list of metadata considered of particular importance less so the data i e direct positions and their associated values themselves qb4st expresses n d datacubes in terms of rdf triples through a concrete rdf data cube ontology the semantics of spatial axis coordinate values is given by crss as in coverages the exact handling of time coordinates is not further detailed grids are constrained to be regular only according to the specification document w3c qb4st has draft status and should not be considered endorsed by the w3c membership at large several concepts are marked as this is defined here pending availability of a canonical definition of spatial concepts at which point an equivalence will be declared as furthermore no complete example is provided the potential merits of this model are not easy to assess 3 a unified abstract coverage model 3 1 anatomy of coverages the term coverage in line with the definitions of ogc abstract topic 6 ogc 2006 and iso 19123 iso 2004a refers to a data representation that assigns values to positions in some multi dimensional space as such a coverage can be viewed conceptually as a mathematical function which for every value of its domain set provides a particular value taken from its defined range set as such a coverage c is given by a function c d p v where the domain set d of c is a non empty set of direct positions short for directly stored values for a particular position as opposed to interpolation derived in some space and v is a non empty set of values called the range set from which the function range values are taken p v denotes the power set of v each direct position always has at least one value assigned more than one value might correspond to one and the same direct position as several observation results in a coverage overlap or even share the identical position for example in a point cloud two points may share the same positions in a grid on the other hand by definition a grid cell contains exactly one value which in practice may be null the uml diagram of fig 5 sketches on a high abstract level a coverage data structure that can represent domain and range set plus further information discussed below in case of a finite domain set d the coverage can alternatively be written in pair set notation c p 1 v 1 p n v n for n d p n d v n v this notation of the coverage function often prompts an interpretation of coverages as just being sets of objects in standardization typically referred to as features while this view is not wrong and coverages do not make a statement about any potential independent existence it can be misleading first the coverage knows no other identification mechanism than the direct position second despite the set notation we need to bear in mind that this set adheres to some specific rules it cannot be concluded that naïve set operations will always work on coverages and actually they often do not a simple example is the union of two grids which requires application of a transitive hull to the union set to again end up with a valid grid for our discussion we rely on the domain range notation of a coverage while keeping in mind that further equivalent descriptions exist this situation is shown in the uml diagram of fig 5 which is extended with a range type description discussed later additionally an optional metadata container is added which allows augmenting the coverage data with any kind of metadata the coverage will not understand these data structures but transport them duly thereby keeping connection between data and metadata in practice this often will be simply a reference into some catalog while mathematically this is perfectly equivalent to our function based definition it is our experience that using this definition can lead to considerable confusion among practitioners as too often it is forgotten that such sets come with additional rules for the various subtypes which critically shape behaviour simply put a coverage is not just a set for example geometric features like curves surfaces and solids are assumed to have an interchangeable grid coverage representation consisting of a raster image obviously the mapping between raster and geometry is far from unambiguous and in particular during rendering of a geometric shape into a grid a substantial loss of accuracy and information content happens for these reasons we prefer the functional view as it allows conveniently expressing important coverage properties that said we will come back to this in section 5 when discussing representation alternatives in computer storage further a collection of concrete coverages encoded in xml json and rdf are available with ogc ogc 2021 01a in the remainder of this section we first abstractly define coverages through a probing function and then model each of the coverage constituents in turn 3 2 probing coverages we make use of a probing function for defining the data with the help of such a function which delivers the coverage s value for each of its direct positions we can systematically observe the behaviour of an object a principle dating back to the theory of algebraic specification of abstract data types adts bauer and wössner 1982 where the objects under discussion remain opaque defined only through their externally observable behaviour in case of coverages conceived as functions associating values with coordinates the appropriate probing function for extracting information about the data structure will retrieve the value associated with a particular point consider a coverage c with domain d and range set v for every direct position within d expressed in the coverage s native crs the function evaluate returns the set of values associated with this position evaluate c d v evaluate c p v p v c note that this probing function only serves for definition purposes it is not required to be implemented in practice higher level retrieval functionality is desirable such as bounding box subsetting in the ogc web coverage service wcs core baumann 2012b 3 3 coverage domain 3 3 1 definition the coverage s domain set describes for which positions in the coverage s multi dimensional space values are available these are the directly stored positions therefore they are called direct positions further values may be generated via interpolation the coverage domain describes some n dimensional space for some n 1 for closure reasons we could consider single position less scalars as 0 d coverages but this is not relevant for the purpose of this paper the multi dimensional space the coverage provides data for is defined through the domain s coordinate reference system crs every coverage is described by exactly one such crs its native crs which defines the domain space through a sequence of axes sometimes predefined crss contain everything needed such as epsg 4979 which contains two horizontal axes and one vertical axis but if such a predefined crs is not readily available then it can also be built on the fly through a compound crs where axes as well as potential sub crss are lined up in proper sequence such as a composition of said epsg 4979 with a time axis to describe a 4d space time crs in ogc crss like other entities are defined through urls an example for a compound crs is the following http www opengis net def crs compound 1 http www opengis net def crs epsg 0 4326 2 http www opengis net def crs ogc 0 ansidate discussion is underway in ogc to allow simplified representations such as the alias below as an equivalent to the url based identifier epsg 4326 ogc datetime the epsg catalog ogp 2021 contains several thousand mostly horizontal crss further crss and axes are under consideration by ogc such as pressure altitudes various calendars and more planetary bodies have shown to be amenable to coverage standards as well oosthoek et al 2014 rossi and hare 2016 and there are about 2500 celestial crss maintained by the international astronomical union iau currently as per iso 19111 iso 2019a ogc additionally foresees for a modular composition of crss from single axes and other crss misev et al 2012 the crs defines the theoretically available space for direct positions in practice bounded by minimum and maximum coordinates for each axis together forming the coverage s minimal bounding box note that if not stored explicitly it may be computationally expensive to determine the bounds from the domain set as we will see later when inspecting various coverage types in a service which potentially offers a large number of coverages such as mundi datacubes with more than 2500 objects n n b it is helpful if the coverage gives a hint about its location in a common rather than its native crs to this end in addition to its concise domain set a coverage carries a so called envelope which gives the approximate location and shape of the coverage through a not necessarily minimal bounding box for horizontal geo coordinates this is typically provided in wgs84 one possible reason for the envelope being larger than the domain set is that the envelope bounding box which must completely contain the coverage domain usually is rotated and distorted relative to the native crs bounding box this allows for an efficient rough search across large coverage sets for example to determine overlap with a particular region 3 3 2 mathematical vs physical coordinates the coverage concepts presented in this work are agnostic of the real life semantics of its dimensions geographic data typically have a subset of the following axes two horizontal axes one height axis expressing elevation or bathymetry and time climate and weather modelling add another time axis for differentiating model run time from the time modelled though these are tightly correlated a height axis can be replaced by a proxy such as pressure altitude in aeronautics pressure altitudes measured in hekto pascal hpa are considered but also more abstract proxies such as flight levels expressed in 100 ft steps like fl150 for 15 000 ft above some reference point mean sea level under the icao standard conditions icao 1993 non spatio temporal axes occur in practice as well for example bands in hyperspectral imagery make sense as a numbered sequence in cases where there are hundreds of bands such as the hyperion instrument on board of eo 1 with its 220 bands pearlman 2001 also in geostatistics non spatio temporal dimensions are frequent note also that originally spatial dimensions might become non spatial at some level of generalization for example cities in europe might originally be expressed through coordinates but at some higher level get abstracted to be in bavaria in the alps region etc which is at a symbolic level generally speaking abstract in the sense of non spatio temporal axes may occur as well like in olap where e g time product subsidiary axes are common for high performance optical sensors as used in remote sensing and astronomy spectral frequency can define a coverage axis as well spatial temporal and abstract axes may be mixed freely in a coverage domain definition in applications such as in spatial olap we are aiming at a level of abstraction which allows sufficient spatio temporal semantics while not excluding abstract dimensions and any mix of all of those 3 3 3 topological vs geometric dimension we distinguish the topological dimension of a coverage from its geometric dimension the topological dimension is given by the degrees of freedom the domain set offers for example the topological dimension of an orthoimage is 2 given by its flat grid underlying the domain set the geometric dimension aims at the length of the coordinate vector required to describe the coverage coordinates for a flat orthoimage this will be 2 d coordinates so topological and geometric dimension coincide but the coordinates could also be e g 3 d with a vertical component in this case the geometric dimension of the coverage is 3 hence larger than its topological dimension in general the topological dimension dt of a coverage is less or equal to its geometric dimension dg dt dg 3 4 common point rule except in grids it may happen that coverage cells coincide points in point clouds may incidentally share the same direct position multi curve and multi surface coverages may contain points where several lines meet and curves surfaces and solids in a coverage may intersect or even be identical many applications however are not prepared for such a set oriented treatment and can only deal with one value per position for such cases the coverage may provide a preference rule the so called common point rule to pick one value in case of co location of direct positions and declare it as the coverage value of this direct position generally a common point rule cptc is a function pertaining to a coverage c and a particular direct position p within the domain d and delivering a single value out of the options offered by the coverage at that position cptc d v cptc p evaluatec p the choice of the rule may be controlled through an extra parameter for example iso 19123 provides a code list with common options as follows adapted to match this article s terminology average mean of the cell values low minimum of the cell values high maximum of the cell values all set of all of the cell values start start value of the second segment in a segmented curve coverage end end value of the first segment in a segmented curve coverage beyond this 19123 definition the following additional options have been proposed oldest value with the least future directed time coordinate only applicable to coverages with a temporal axis newest value with the most future directed time coordinate only applicable to coverages with a temporal axis this seems due to a misinterpretation of spatio temporal coordinates time is just another axis and hence direct positions which differ in the time component actually are distinct and do not need any choice method in ogc wcs baumann et al 2018 for example a timeseries extraction at one spatial point would naturally yield the sequence of points with their proper time coordinates associated as a 1 d coverage see section 5 1 for an example the intended effect to pick a particular value along the time axis is accomplished through subsetting see wcs discussion in section 5 2 therefore we disregard oldest and newest iso 19123 extends the common point rule to coverages which allow interpolation as follows at positions in between direct positions the interpolation is performed first and the common point rule is applied on the resulting values we have some general reservations over this common point rule first and foremost making a choice out of existing values appears forceful and effectively hides some values in the coverage from the user further the options provided seem arbitrary and lack a common concept one option returns a set of values all others return single elements without motivation also start and end address only multi curve coverages as one very specific category while other relevant cases such as surfaces and solids remain unaddressed also average low and high might be augmented with say median as further option so this is by far not closed and final in passing we note that we are not aware of any implementation supporting this functionality for these reasons and despite being aware that the forthcoming 19123 1 will keep it we abandon the concept of a common point rule altogether and define the coverage evaluation function to always return a set of values in case this is undesirable such as with grids where the grid points are expected to have a non zero pairwise distance we express this through explicit constraints algorithmic aspects such as in overlay operations which require a choice to hide values of lower layers in presence of non null values in higher layers of a map stack get adequately addressed in the corresponding service operation definition 3 5 coverage range and coverage range type mathematically the range set equals its type so v says it all in practice however this is insufficient first coverages must carry type information for proper decoding in an application it might not be entirely clear what values like fl050 or 1 0 5 0 mean is the latter an imaginary number or wind components in computer science syntax coverages require dynamic typing and so the receiving tool must get this information which may or may not be deducible from the values themselves second applications require substantially more semantics than just the programming language s data type among the essential information are units of measure meters degrees etc null values such as 9999 which is often used in bathymetry accuracy measurement methodology and more an example for a high level semantic description of the value set is given by the ogc sensor web enablement swe common datarecord robin 2011 which supports the description of any data structure that sensors or data generators may deliver the datarecord structure is used in the implementable companion standard of 19123 1 coverage implementation schema baumann et al 2019 iso 2019c thereby enabling seamlessly passing of sensor information captured upstream to downstream coverage services 3 6 interpolation interpolation is a method for constructing new range values for coordinates within the domain set of a coverage which are aligned with direct positions the only positions for which the coverage explicitly provides values this can be useful for estimating values where none are present notably interpolation as opposed to extrapolation derives values for positions between direct positions for example in an image timeseries having latitude longitude and time axes several ways of interpolation may become relevant linear interpolation along latitude and longitude bilinear interpolation with temporal resolution unchanged in plain words all existing timeslices get extracted meaning no interpolation occurs along time interpolate each pixel s history i e along time using linear interpolation without any spatial interpolation linear interpolation along latitude longitude and time simultaneously trilinear interpolation in principle any point in the transitive hull of the domain set is amenable to interpolation we denote the transitive hull of some coverage c which in general is a superset of domain c as d c and define function interpolate as an extension to evaluate where interpolatec p evaluatec p for all p domain c interpolate c d v with d d c v range c many interpolation methods are in use the most prominent being nearest neighbour and polynomial such as linear quadratic and cubic less used methods include barycentric on tins and lost area for thiessen polygons and hexagonal grids 5 5 the firm grounding of iso 19123 in 2 d remote sensing shines through by mentioning bi linear bi quadratic and bi cubic methods rather than their general n d counterparts as the interpolation function is based on the coverage domain and range type these may impose natural restrictions on the type of interpolation applied first we inspect the domain set impact note that the domain set type as an analogy to range type is given by the crs more specifically the axis definitions 6 6 although today s available crs definitions do not convey this information explicitly neither human nor machine readable the fact is recorded that e g latitude and longitude are represented by real values index grid coordinates representing integer numbers do not allow expressing any in between value as it would not be an integer anymore therefore interpolation is not possible simply due to the lack of points that are not direct positions physical coordinates like latitude longitude height and time are expressed through real numbers and consequently allow expressing values between direct positions interpolation methods may go over a single axis or over several axes in combination in an x y t image timeseries datacube one might want to apply bilinear interpolation along lat long and independently perform nearest neighbour interpolation along time next let us see examples of how the range type choice can affect interpolation polynomial interpolation on a real valued range type such as radiometric intensities would deliver as interpolation result some value between the contributing direct positions delivering more or less smooth transitions a categorical range type such as land use does not allow arbitrary values as interpolation results thereby restricting the choice of interpolation methods e g polynomial interpolation is not applicable whereas nearest neighbour is bottom line while the choice of applicable interpolation methods is to some extent determined by the domain and range data type there is still a degree of choice therefore we associate with a coverage a possibly empty set of applicable interpolation methods any tool processing a coverage should only apply those methods listed to not violate the coverage semantics conceptually a coverage with several interpolation methods which all will yield different interpolation results defines a family of coverages where all members contain the same values at the direct positions themselves but will convey potentially different values for the interpolated non direct positions on a side note one might also consider interpolation between components of the range values such as in a combined handling of red green and blue channels together however we feel that this is entering into the field of general analytics and do not include such methods specifically under interpolation 3 7 discrete and continuous coverages based on the previous elaboration we now can define discrete and continuous coverages an axis is called discrete if every possible interval with finite bounds describes a finite set of values otherwise such an axis is called continuous a coverage is called discrete if its axis list contains only discrete axes a coverage is called continuous if its axis list contains at least one continuous axis the new iso 19123 1 lists a series of examples for illustration iso 2020c a map of postal code zones is a coverage which is discrete in its range the postal code zones cover an entire country and at every location in the country one can evaluate the coverage function and get a value that represents the postal code for that location within a postal code zone the value is constant one cannot interpolate such a discrete coverage a coverage that maps a set of polygons to the soil type found within each polygon is a coverage which is discrete in its range a point set representing a set of measurements that are only valid at the position of each point and which cannot be interpolated is a discrete coverage discrete in domain an image sampled by a sensor may be represented as a grid coverage consisting of a set of pixels corresponding to grid cells in the domain of the coverage a value is associated with each grid cell however since the coverage is continuous an interpolation function such as linear quadratic or cubic may be applied so that a continuously variable value may be determined at any location within the domain extent of the image in a coverage that maps direct positions in san diego county to their temperature at noon on a specific day both domain and range may take an infinite number of different values this continuous coverage would be associated with a discrete coverage that holds the temperature values observed at a set of weather stations discrete in domain whereby the measured values correspond to a point set coverage this point set coverage is discrete because each point can only have one value the associated continuous coverage uses the point values as driving values for the coverage function and allows interpolation between the points a set of bathymetric soundings is a discrete point set coverage with a single measured water depth value at each point location discrete in domain an associated continuous coverage allows one to interpolate between the measured depth soundings to determine the bottom surface of a body of water evaluation of a triangulated irregular network involves interpolation of values within a triangle composed of three neighbouring point value pairs hence the distinction between discrete and continuous is not a primary first class criterion but a consequence of particular coverage properties historically in iso 19123 this has been the first level differentiation of coverages a well known reason for sustained confusion 3 8 metadata in practice more data is necessary for providing relevant context think of the satellite instrument pixels are coming from spectral characteristics of the sensor processing steps applied etc in common terminology further metadata need to be associated with a coverage given the foundational nature of coverage data such metadata have to satisfy a wide range of domains unfortunately despite standards like iso 19115 iso 2020a and inspire metadata inspire there is not a generic concise agreed definition of what this metadata should comprise even worse even within focused domains frequently there is no accepted consensus confusion about metadata is as old as the term itself yielding sentences like your metadata are my data we have encountered the same in discussions about coverages for example sometimes the domain set is considered metadata sometimes data as it is part of the coverage concept in reality we observe several distinct levels of metadata fig 6 pertaining to coverages at the lowest level the range set sits as the data of concern these are described by the second level that we call technical metadata in the sense that they are indispensable for accessing the range set values among these are for example the underlying domain definition with its crs and the range data type one level above geo descriptors add location and temporal information to the coverage s range values here we find geographic and temporal coordinates crss and units of measure of the range values these levels together constitute the coverage concept on top of that applications may add any type of further metadata such as provenance information therefore as a way forward in the concrete coverage implementation schema baumann et al 2019 we have foreseen a slot for metadata in general with an undefined structure to allow any type of data to be stored there from simple references to a corresponding catalog record up to elaborate records describing the contributing footprint for each patch inside the coverage the coverage will not know about the semantics of these metadata but it will duly carry them along thereby at least maintaining connection between data and metadata 3 9 internal structuring of coverages the abstract concept of a function mapping direct positions to values can be modelled through various different data structures while this may rightfully be considered an implementation detail and hence out of scope on the level of abstraction of this paper and iso 19123 1 we still provide it as a guidance for possible concretizations and as a bridge to the concretization of iso 19123 2 and ogc cis where several alternatives are broken down to concrete encodings in xml json and rdf the uml diagram in fig 7 summarizes them one method of storing the domain range function mapping is through a set of coordinate value pairs this approach is particularly convenient for timeseries streaming a complementary approach makes use of two collections one for the direct positions of the domain set and the other for the corresponding range values both can be correlated unambiguously via their position in the collection this scheme is attractive because there are very efficient methods for representing the domain set particularly in case of regular grids the values in this case form a multi dimensional array where value positions can be computed for retrieval in combination with range set compression this leads to very storage efficient encodings the two modes of organization described above represent extremes of a wide space of possible schemes while coordinate value pairs have the finest granularity single positions and consequently the largest number of items the domain range representation consists of one single monolithic item in between is the universe of tilings i e partitioning following some regular or irregular pattern specifically for very large gridded data sets tiling is highly advantageous be it for efficient retrieval in datacube engines baumann et al 2010 or for efficient extraction from large files in formats like tiff n n 2021 01b and netcdf n n 2021 0a where it is often referred to as chunking often such tilings are simple with regular tile sizes and shapes but they can be irregular as well such as for adjusting to particular access patterns a comprehensive study of tiling patterns has been accomplished by furtado furtado and baumann 1999 ogc iso cis version 1 1 supports arbitrary tiling and even organization into sub coverages under certain homogeneity conditions baumann et al 2019 a completely different approach is possible if the coverage function can be described analytically in this case the range set does not have to be materialized instead a definition of a generating function is stored in some suitable format like mathml while there is little practical relevance for grid coverages and point clouds it is a widely used method in 3d csg modelling in cad cam 4 coverage typology by dimension 4 1 overview in this section we establish classes of coverages aligned with the relevant application areas of coverages these types in a nutshell are grids point clouds and general meshes instead of using this directly we employ a more structurally justified classification based on the topological dimension of the elements describing the direct positions within a coverage starting from ordered and non ordered 0 d points with gridded data being a special case and moving up to higher dimensions fig 8 we stop at full 3d x y z space acknowledging that higher dimensions are possible however to the best of our knowledge there are no practically relevant applications that said the hierarchy and data structures can be extended at any time without affecting the lower dimensional levels for example the spatial dimensions and their characteristic behaviour in the coverage types may well be augmented with temporal and other axes such as to model time varying solids in the following subsections coverage types are introduced sorted along their topological dimension 4 2 multi point coverages a point cloud or multi point coverage is a coverage whose domain set consists of a set of points in other words direct positions are unconnected points in the domain set given by their point coordinates the evaluation function consequently specializes as follows for some coverage c and position p evaluatec p v point x domain c x p v c x 4 3 grid coverages 4 3 1 anatomy of grids by far the most important application of coverages today is given by raster data i e data sitting on some grid long known as raster images and more recently as earth datacubes baumann et al 2021 which have first been introduced in baumann 1992 baumann 1994 in standardization these are all modelled through the family of grid coverages a special case of a multi point coverage where all direct positions are constrained to sit on some grid in the coverage domain formalization of regular and irregular grids is straightforward we call ac a1 an an axis sequence remembering it is determined by the coverage s native crs just as is the dimension n of the coverage domain along each axis ai we pick a non empty subset of points gi in practice each such set will be finite with ki 0 elements so we can write it as gi gi 1 gi ki we now can define the grid coverage domain as a set of direct positions g g1 gn g1 gn gi gi for 1 i n topologically every inner point in a grid has exactly two neighbours along each axis one with a lower coordinate value and another one with a higher coordinate value along this axis border cells have exactly one such neighbour cell each in direction of the grid s border corner cells have exactly one neighbour in each direction this neighbourhood relation can be considered indicative to distinguish grids from meshes in particular in discussions where mathematical language is better avoided in comparison the definition of a grid in 19123 was a network composed of one or more sets of curves in which the members of each set intersect the members of the other sets obviously this excludes the 1 d case and further does not prohibit touching and crossing of the generating curves as a first classification such an n dimensional grid is said to be regular if for all pairwise neighbouring direct positions the distance in each axis is constant otherwise the grid is called irregular this corresponds to the definition of rectified grid coverage and georeferenceable grid coverage resp in the iso 19123 standard under revision a more fine grain distinction adopted in 19123 1 we will be introduce below in passing we note that the lower and upper bound along each domain axis defines the minimum bounding box as expressed in the coverage s native crs often portele 2007 baumann the minimum and maximum values respectively are grouped into diagonal corner points for defining the grid extent by construction direct positions in a grid are pairwise disjoint for each direct position there is exactly one associated value in the range of the function a direct consequence is that in a raster image every pixel can contain only one value overlaps resulting in several pixel values for some given coordinate are not admissible we argue that this is actually natural from the viewpoint of analysis ready data while overlaps say of satellite scenes is common during acquisition and processing both human users and image processing tools get confused and have to pay extra attention to artefacts resulting from acquisition such as overlapping tracks which get cleared during generation of more analysis ready products such as for datacubes raw satellite swath data can be modelled as individual irregular grid coverages as well they just cannot be combined into one coverage due to their non matching grids 4 3 2 axis based grid specification the classical subdivision into rectified i e regular grid coverages and referenceable i e irregular grid coverages is rather coarse and does not allow expressing relevant details such as all grid types shown in fig 9 for example a case which we frequently meet in practice when building datacubes n na is timeseries on orthoimagery in this case the horizontal part of the 3 d grid is regular whereas distribution along time typically is irregular fig 9 center in traditional nomenclature this would simply be a general referenceable grid neglecting the regularity of the spatial axes thus causing excessive storage and processing overhead in practice as the more expensive general representations for irregular grids will be used by the system rather than the efficient regular grid mechanics in the classical rectified grid coverage of the original 19123 standard this cannot be represented as one axis is irregular rather this needs to be modelled as a 19123 referenceable grid coverage where all axes are considered irregular leading to a massive storage and processing overhead for those axes which actually are regular and can be described by a simple resolution value therefore we propose to abandon the classification into several grid types and instead advocate a more fine grain distinction which considers each axis individually for the direct position s regularity properties this leads us to the following axis types cf fig 10 an index axis is a 1 d cartesian axis there is no geo reference admissible coordinates for direct positions are at discrete integer positions and unit less technically expressed by a unit of 1 and there is no datum mapping defined which could relate to some physical position as a consequence of the discrete nature of the axis no coverage interpolation is possible between adjacent coordinates a regular axis has an equi distant spacing like an index axis but is continuous and not constrained to integer positions and distances such an axis can be georeferenced i e it can have a spatial or temporal semantics attached units of measure typically degrees meters seconds years etc are defined with the axis crs which also should specify meaningful interpolation due to the regularity valid coordinate positions can be expressed as multiples of a given distance the resolution added to an initial offset the lower bound coordinate an irregular axis is continuous possibly geo referenced the distances between neighbouring points are non zero but otherwise arbitrary a displacement axis nest or warped nest is a set of continuous possibly georeferenced axes forming a subset of the grid s axes relative to a regular grid each direct position is shifted by some individual offset within the crs space spanned by the axes participating in the extreme case such a nest spans all axes of the grid which resembles the referenceable grid of earlier standards algorithmic axes are given by a set of discrete or continuous possibly geo referenced axes where the actual coordinates are not indicated but have to be derived algorithmically from some otherwise abstract parameter set provided hence the alternative name transformation model nature data type and evaluation of such parameter types is tentatively left unspecified but must be defined on an application specific basis examples of such algorithmic axes include sensor models where instead of the coordinate information a set of sensor parameters such as ground control parameters is provided which must be fed into the model for deriving the actual direct positions by combining all the above axis types freely any type of grid can be modelled we can substantiate this claim with the observation that not only are the common well known cases represented but the warped nest allows to effectively assign individual positions to each direct position in the grid as long as the basic grid definition is not violated ordered positions positive distance between grid lines still the list of possible axis types is not comprehensive some standards or applications may define their own specialized axis types comparing this with the previous classification of grid coverages baumann we can easily map them into this new schema rectified grid coverages are those where the underlying grid consists of only regular axes referenceable grid coverages are all those where the underlying grid consists of at least one irregular axis it is easy to see that the axis based classification allows substantially more fine grained differentiation in the universe of regular and irregular axes furthermore several classes of coverages can be described now which were not supported by the earlier classification namely warped nests algorithmic axes and all combinations of the various axis types described 4 3 3 further grid types in section 2 we have discussed the grid types possible in euclidean n space and found that cuboid grids exist in any dimension while in 2 d triangular and hexagonal grids additionally exist constrained by the fact that the axes for coordinate expression must be straight uninterrupted lines in the tiling pattern in each dimension which includes skewed non orthogonal axes since simple coordinate translations can be found the hexagonal case at first glance seems to contradict this however with a trick this case can be mapped back to the square case by considering as direct positions not the grid corners but the hexagon centers fig 11 this situation can be described through a rectified grid in which the two offset vectors are of equal length but differ in direction by 60 the length l of a side of the hexagon is l s tan 30 where s is the length of the offset vector in a computer the values of such a coverage range can therefore be stored as a multi dimensional array with an adjusted index addressing scheme by way of background the hexagons are the thiessen polygons generated around the grid points altogether after this inspection we can confirm that square and with less relevance triangular and hexagonal grids are the only grid generating patterns possible for coverages 4 3 4 varying cell location interpretation independently from the coordinates and their arrangement through crss there is often an interpretation of a value as sitting next to the coordinate point in our terminology direct position this seems to result from a visual misunderstanding of a grid it is commonly accepted that the cell values in a cartesian grid are associated with the exact coordinate whereas in earth sciences cell contents often is associated with a position between neighbouring direct positions in our approach and commonly in mathematics values are associated with the exact coordinates of their direct position in remote sensing this is referred to as pixel in corner also referred to as pixel is area however a common practice in gis is to consider a pixel in center position also referred to as pixel is point typically meaning the geometric center of the grid cuboid in 2 d square in a regular grid fig 12 visualizes both interpretations technically this is typically described by metadata such as tags in a tiff file indicating an interpretation of pixel in center versus pixel in corner implementations such as libtiff n n 2021 use code lists which guide implementation of the processing tool indeed this split interpretation is mainly driven by the geotiff format which has picked these two special cases while ignoring all others a more general systematic overview on a number of such value shift cases has been established by arakawa arakawa and lamb 1977 but it can be generalized even more to give the value a center of gravity in any direction and distance of its position as indicated by its coordinate in the most general case the grid value is determined by a sensor specific intensity distribution integral associated with the value s coordinate position a typical case is the instantaneous field of view of a sensor collecting radiosity on some area on ground based on this view we prefer an explicit modelling which does not hide the actual algorithmic treatment of data one way of doing so is to describe a pixel in center situation by a coordinate shift of half a pixel size remember this discussion only applies to regular grids which in turn can be modelled as a concatenation of the given crs with an engineering crs having a correspondingly shifted origin by half the pixel size such a crs concatenation is explicitly foreseen in iso 19111 iso 2019a an additional characterization would be to capture the intensity distribution something which currently is not considered in the earth sciences except in very special situations of sensor calibration and validation in some situations this point versus area distinction can become particularly cumbersome for example in atmospheric research different physical parameters may be subject to different regimes with respect to position assignment relative to the coordinates concretely mass of a cell may be associated with the center between neighbouring coordinates pixel in center whereas wind speed may be associated with corners potentially even split into different corners for the u and v horizontal components generally this idea begs several questions the concept is discussed on regular grids only what would it mean in case of irregular grids discussion seems to focus on lat long what about pixel in x for time etc why is it assumed that pixels have clearly delineated polygons and do not overlap from sensor physics it rather seems that generally a pixel is obtained through a weighted integral conceptually going from the ccd element s center to infinity but at least involving some neighbourhood possibly larger than the distance to the next element why is it assumed that the value of a pixel is spatially extended and constant over some neighbourhood interpolation will yield different values even close to a pixel the pixel areas discussed seem to apply only to nearest neighbour interpolation where nearest already is hard to define consider irregular grids krieging etc pixel in corner refers to what corner in general 2 d has 4 corners 3 d has 8 corners etc not to mention triangular and hexagonal grids as discussed above further why does the principle have to be confined to a corner vs the center in the first place and not at 75 supporting just corners seems random see ogc 2021 01b for a recent documented version of this recurring discussion altogether we consider such variations of the grid theme to be implementation details and do not address them on this contribution s level of abstraction 4 4 multi curve coverages after addressing 0 dimensional multi point coverages with their special case grid coverage we move on to coverages whose delineating shapes have a non zero topological dimension such coverages essentially align with geometric structures as they appear for example in 3d city modelling and computer aided design cad a multi curve coverage is a coverage consisting of a collection of curves that is the behaviour of the field described by the coverage is modelled through curves a curve is given by an ordered list of two or more points in which case it resembles a single or poly line with straight connections or a point list with additional curve information such as splines ferguson 1964 by way of example a multi curve coverage might describe routes that have numbers and names a pavement width and a pavement material type assigned to each segment of some road network formally the coverage evaluation function is given for some coverage c and position p as follows evaluatec p v curve x domain c x contains p the containment predicate on which this definition relies contains refers to the corresponding probing predicate on the geometric elements contained in the coverage such as the ones defined in iso 19107 iso 2019b obviously it is heavily dependent on the way the direct positions are described through direct enumeration of the direct positions example point clouds containment descriptions example curves areas and volumes or some algorithmically involved algorithm example ground control points in sensor models while we allow any method on principle to specify the direct positions we focus on the most common methods used in practice see section 4 from a multi curve coverage a corresponding multi point coverage can be constructed from the bounding points of the curves often the result is unambiguously defined but in general it depends on the representation mechanism of the curve see next conversely a multi curve coverage can be derived from a multi point coverage but in the general case will not be unique depending on the application the evaluation function may return at most one element no crossing routes in our example or may return several values in case of crossings and bridges as the geometric description of curves is dependent on their endpoints although maybe not completely such as in non straight cases the coverage modelling becomes a 2 level hierarchy where 1 d curves rely on 0 d points mappings from multi curve to multi point coverages are conceivable a trivial mapping is by down propagating all coverage values from the curves to the points and dropping the curve information this is of less practical use than other mappings where the curves get rendered into series of points at some given distance i e resolution as lidar returns such point clouds of objects it is of interest to study multi curve and multi point coverages in combination for applications such as object reconstruction this will be of even more importance with the multi surface and multi solid coverages direct positions on such a curve take on the value associated with the curve object other direct positions of the coverage are undefined 4 5 multi surface coverages continuing up the hierarchy of topological dimensions we next address surfaces where 2 d elements determine the coverage domain set a multi surface coverage is a coverage where the direct positions are described through surfaces the formal definition is structurally identical to the previous one just with curves replaced by surfaces evaluatec p v surface x domain c x contains p from a multi surface coverage a corresponding multi curve coverage can be constructed from the bounding curves of the surfaces often the result is unambiguously defined but in general it depends on the representation mechanism of the surface see below conversely a multi surface coverage can be derived from a multi curve coverage but in the general case it will not be unique the description of a surface relies on a bounding curve ring with or without holes which in turn relies on points at both ends of each line segment to remain unique the bounding polygons should be planar thereby defining a surface plane the de facto standard for describing curved surfaces is nurbs rogers 2000 based on control points and polygons direct positions on such a surface take on the value associated with the surface other direct positions of the coverage are undefined various practically relevant subtypes of multi surface coverages exist including polyhedral surfaces and their special case of triangulated irregular networks tins which often are used for digital elevation models dems kumler 1994 iso 19107 offers a data type triangulatedsurfacedata iso 2019b as an example land use can be represented by a multi surface coverage where the spatial domain is composed of one surface per cadastral parcel the coverage values associated with such parcels then may constitute soil types ownership etc a 3 d example is given by iso surfaces of atmospheric data such as temperature wind speed etc fig 13 multiple values again may be forbidden by design such as iso surfaces which by definition do not cross touch or overlap in practice multi surface coverages can be used to model bundles of equi potential surfaces such as air pressure in atmospheric data 4 6 multi solid coverages with coverages representing spatial bodies we reach the final hierarchy level 3 d solids a multi solid coverage is a coverage where the direct positions are described through solids the formal definition again is structurally identical to the previous ones albeit based on solids evaluatec p v solid x domain c x contains p from a multi solid coverage a corresponding multi surface coverage can be constructed from the bounding surfaces of the solids often the result is unambiguously defined but in general it depends on the representation mechanism of the solid see next conversely a multi solid coverage can be derived from a multi surface coverage but in the general case it will not be unique solids can be described in various ways such as constructive solid geometry csg and boundary representation b rep csg requicha and voelcker 1977 relies on a small set of analytically described standard volumes such as sphere cylinder cone and torus a concrete object consists of instantiations of these positioned and scaled as required by combining solids through regularized set operations arbitrarily complex objects can be built recursively constructive hypervolume modelling is an extension to this pasko et al 2001 conversely b rep braid et al 1980 models solids through their boundary surfaces edges and vertices such graphs need to adhere to several nontrivial geometric and topological integrity constraints including non self intersection singularity avoidance planarity of the surface graph etc b rep solids can be constructed piecewise through so called euler operations which maintain all these constraints mäntyla baumgart 1972 an example is shown in fig 14 from a coverage perspective b rep is a natural way of describing solids csg modelling parametric representations etc comprise alternatives which easily might be added 4 7 mixed coverages so far we have established coverages of different types sorted along the topological dimension of the spatio temporal objects describing the mapping to values plus specific extra constraints that allow modelling grids as special cases of point clouds it is a natural next step to allow coverages with a mixed description this bears significant practical relevance for example data formats like gmljp2 colaiacomo et al 2018 gdb esri 2021 geopackage ogc 2021 and safe esa 2021 01 all allow incorporating different data structures within the same package not to speak about common container formats like zip and tar which are allowed by ogc cis 1 1 as well formally let ci d v for 1 i n n 0 be coverages of any of the types introduced above all sharing the same domain and range then a mixed coverage c can be defined in a straightforward manner as c d v c p u i 1 n c i p for p d mixed coverages are not currently addressed by standardization in a formal manner which may have to do with the intrinsic complexity of the corresponding services however pragmatically several data formats exist such as gmljp2 colaiacomo et al 2000 which support such mixed data without any particular conceptual underpinning 5 coverage services although the service model is not at the heart of this contribution we exemplarily discuss how the coverage data model can benefit encoding provision access and analytics of coverages this also gives some impression about the practical relevance and impact of the overall coverage work of which we only report a specific part here following the thrust of this paper we prioritize standardized approaches further as grid coverages form the by far most relevant category these days we focus on those 5 1 coverage representation and encoding the abstract concepts presented in this contribution define a foundation for coverage structures but not to the level of concrete interoperability this is achieved via a concrete implementation schema for example the ogc iso coverage implementation schema cis derived from the 19123 1 model among others cis is concrete enough to define an extensible ecosystem of encodings allowing to express coverages in common formats like geotiff netcdf and many more a recurring problem with the plethora of data formats is their varying capability of representing metadata even the purely technical metadata like domain crs and range type often cannot be encoded still such formats are practically important as they often offer efficient binary representations compression etc conversely informationally complete encodings where all coverage information can be carried along such as cis xml json and rdf tend to be inefficient in storage footprint and access to combine the best of these extremes cis has introduced a mixed representation where a container format is used for gathering a canonical header in an informationally complete format such as xml json or rdf paired with one or more files containing range values in any suitable format by factoring out high volume parts such as the range sets from the header it is possible to store them efficiently in some binary format while retaining the complete coverage contents in cis encodings for xml json and rdf are already included as optional packages further formats are defined separately for geotiff netcdf jpeg 2000 grib2 etc ogc 2021 5 2 coverage access the purposeful step of wcs 2 0 to separate the data from service model has substantially broadened the opportunities for accessing coverages making it possible to utilize wcs wfs vretanos 2014 wms de la beaujardiere 2006 wps mueller and pross 2015 sos bröring et al 2012 and further service application programming interfaces apis however the most comprehensive and streamlined functionality is available with the wcs suite of standards see baumann 2019 for the authoritative specifications baumann 2019 and baumann et al 2018 for an overview in contrast wfs only allows retrieving a complete coverage as it does not support multi dimensional subsetting capabilities wcs centers around a mandatory core which allows download of a coverage bounding box extraction and format encoding further bespoke functionality is specified in optional extensions including range band subsetting reprojection data upload to the server and datacube analytics see next section request encoding in wcs speak protocol binding is uniformly through the http web protocol but can follow different flavours get kvp xml post soap and more recently and still heavily under development oapi we present get and oapi for comparison both with a request on a 3 d x y t image timeseries from which subsetting extracts a reduced footprint trim in lat and long at a particular position in time slice the get request signals that it is a getcoverage belonging to the wcs family followed by the identifier of the coverage addressed and the subsetting coordinates one per axis and is finalized by specification of the return format image 2 conversely the following request does a slicing only in space thereby focusing on a specific point on earth while not mentioning time at all thereby extracting everything available along time in case of a 2d lat long coverage the result would be a single point in case of a 3d lat long time datacube as is the case here the result is the history of that point conveniently expressed as comma separated values image 3 oapi tries to partially avoid query parameters preferring a pseudo restful syntax to accomplish the same here a rephrasing of the first wcs example above image 4 with the rasdaman array dbms the concept of internet accessible datacubes amenable to individual queries and not requiring deep technical skills was pioneered some of the epigons provide some sort of query language with n n c or without spatio temporal coordinate support paradigm4 others just a direct programming interface nowadays typically in python which represents a serious security issue any code can be submitted to the server from anywhere and gets executed in an unsupervised manner coverage based apis on the contrary offer high level access and processing with streamlined functionality hence easier and safer to use an overview of 19 different datacube technologies is presented in the research data alliance report rda 2018 the aforementioned rasdaman engine which is the ogc coverage reference implementation gives access to multi dimensional coverages via wms for visualization wcs for data extraction reformatting and download and wcps for safe server side analytics internally all these operations are translated into queries of rasdaman s array language rasql which has been the blueprint for the array extension to the iso sql query language sql mda multi dimensional arrays iso 2019d see misev and baumann 2015 for an overview 5 3 coverage analytics ogc web coverage processing service wcps is a high level datacube analytics language with built in spatio temporal semantics crafted tightly around the ogc iso coverage model through the wcs processing extension wcps is tied into the overall wps ecosystem a wcps request sends a query string usually with the identifiers of one or more available coverages as input parameters and returns a set of result scalars in case of aggregation or coverages the idea is to allow any query any time on any size rather than having just a predefined set of functions which is either too limiting or too hard to oversee such wcps expressions can range from simple extraction to any complexity and length processing in the server includes data fusion with any number of coverages filtering of coverages based on their data properties multi coverage analytics and encoding results for shipping them back to the client a simple example would deliver the average temperature from an era5 climate model at a particular geo position across all times and all heights image 1 due to the high level nature of the language complex analytics can be tasked without any programming further the language is amenable to heavy optimization parallelization distributed processing etc in a fully transparent manner without user or administrator intervention this has been demonstrated on dozens of petabytes with the rasdaman engine n n b baumann et al 2017 which also boosts the earthserver datacube federation n n 2021 0b the largest of its kind worldwide the same underlying formal framework is standardized as the sql datacube extension mda multi dimensional arrays iso 2019d which resembles the rasdaman array query language the underlying algebraic foundation baumann 1999 and operational capability is the same as with wcps modulo the space time semantics built into wcps in fact rasdaman internally translates wcps requests to md queries for processing an integrated analytics language supporting all coverage types in the spirit of database languages like e g secondo güting et al 1999 nidzwetzki and güting 2017 mql mahdiraji 2015 sql mda iso 2019d and ogc wcps baumann 2010 baumann 2021 is a topic of further research and a forthcoming paper 6 conclusion we have presented a unifying abstract model for coverages which is in advanced status of adoption becoming iso 19123 1 and updated abstract topic 6 in ogc and compatible with the existing implementation standard 19123 2 while a standard has to respect diverse constraints such as requiring little domain knowledge and backwards compatibility across multiple specifications an article like this has the scientific freedom to adopt an abstract independent position hence a fresh view we hope that it fosters discussion of the forthcoming 19123 1 standard and can act as meaningful input stimulating future evolution of coverage concepts and technology our key contributions are as follows we have developed a coherent framework for the representation of spatio temporally extended phenomena regardless of their dimension but particularly suitable for spatio temporal data the conceptualization established to the best of our knowledge has not been done before in a way that is equally comprehensive and suitable for deriving implementation standards for scalable interoperable services this framework is modernizing clarifying and enhancing the existing standards coverage data model while remaining essentially compatible moreover concepts are simple and can also be easily communicated to non geo experts a comprehensive treatment of all possible grid types is proposed including mixed type grids where different axes follow different patterns such as cartesian versus regular versus irregular we hope that this paper can act as a companion document to the standard giving background on design decisions and explaining concepts something which is not appropriate in a standard itself the unified concepts can contribute to closing gaps in coverage modelling and in particular for devising flexible easy to use but comprehensive and powerful earth data services for increasing insight on the big earth data being continuously collected the coverage concept is at the heart of big earth data analytics it unifies handling across all spatio temporal dimensions it integrates a number of essential data structures namely regular and irregular grids point clouds and general meshes it is matured through extensive discussion among a large number of data generating and consuming communities from upstream satellite operators to downstream precision farming to name but two and there is not only a concise yet flexible data model but also a versatile powerful service model ranging from simple extraction to high end analytics and fusion both of which are streamlined and establish interoperability across all coverage types and formats as more and more services emerge and data volumes grow by multi terabytes per day the issues of interoperability and easy to use yet powerful access and processing becomes more and more urgent the recent quest for analysis ready data underlines this a common well founded coverage model therefore is a key pillar for powerful user friendly services practical impact of 19123 1 which this paper anticipates is substantial establishing the foundations of a whole ecosystem of further standards within iso we first find 19123 2 the coverage implementation schema which is a concretization of the 19123 1 concepts ready for implementation and conformance testing further there is a series of additional standards using coverages iso 19111 geographic referencing normatively references coverages iso 2019e iso 19115 metadata iso 2014 iso 19129 imagery gridded and coverage data framework iso 2009a iso 19130 imagery sensor models for geopositioning iso 2018a iso 19131 data product specifications iso 2007 iso 19136 geographic markup language gml including coverage encoding iso 2020d iso 19144 classification systems iso 2009b iso 19157 data quality utilizing coverages for representing quality information iso 2018b iso 19156 observations and measurements o m iso 1915 iso 19163 2 metadata and encoding rules for imagery and gridded data iso 2020b most of these have their parallel in ogc the inspire legal framework for a common european spatial data infrastructure relies on iso ogc coverages as the basis for the conceptual models defined for various inspire themes as well as the inspire wcs specification inspire 2021 0 not surprisingly a large number of open source and proprietary tools implement coverages their interoperability relies on a concise adequate conceptualization our own work focuses on flexible scalable services on massive heterogeneous and distributed multi dimensional datacubes manifest in the rasdaman array database system as editors of the ogc and iso coverage standards we continuously use rasdaman as our platform for evaluating implementability and so rasdaman naturally has become reference implementation various services have been deployed on basis of rasdaman strictly based on the ogc coverage standards wcps style processing at large has been demonstrated on earth data assets exceeding 50 pb single queries have been parallelized across more than 1000 amazon cloud nodes and data fusion on coverages is routine in the earthserver federation hence it is safe to say that the ogc coverage data and service models are practice proven and allow for fast scalable and flexible implementations a long avenue of further research remains to be done with the abstract coverage model presented here hopefully acting as a stable basis for further innovation most importantly this work establishes only the data model leaving processing and service models for future research we list some of the challenges remaining current wcs core defines subsetting only on 0 d coverages that is grids and point clouds tentatively this core has been kept as simple as possible to have a low entry barrier for implementers more complex functionality can always be put into optional extensions such an extension could consider higher dimensional subsetting commonly also called clipping there when considering higher dimensional coverages there are two possible avenues first subsetting could be defined on multi curve surface solid coverages in addition implementation wise this is substantially more complex as it involves polygon clipping and objects intersected will need to be substantially reshaped as opposed to the simple 0 d case where a point grid or not is either inside or outside the subsetting area along the same line the current restriction to bounding box subsetting could be lifted to allow general polygons first steps have been undertaken by the ogc wcs metocean application schema trevelyan 2021 which allows among others polygon corridors for subsetting this is driven by an aviation use case where atmospheric conditions during a flight need to be retrieved from an x y z t weather datacube to obtain the weather along the airplane s trajectory and exactly at the time the plane passes a point with some margin along all dimensions this can be generalized further to intersection between arbitrary objects where the subsetting description can be of any shape and the subsetted object can be any type of coverage from this it is only a small step to allow general coverage joins where multiple coverages sitting on a server get combined through set operations like in cad another open field of research is coverage to coverage transformation simple cases like scaling a grid coverage are well understood however transformations between different coverage types are also common and important specifically between grid coverages and multi surface coverages object reconstruction transforms from sampled grids to vectorial representations rendering conversely produces gridded representations from vectors the famous marching cube algorithm lorensen and cline 1987 with its many improvements over time is one way of obtaining a surface model from gridded data however although vector raster integration is common in giss piwowar 1990 such conversion at the same time is frowned upon by some researchers such as van der knaap van der knaap 1992 while by nature inexact in the general case such conversions are implemented in an ad hoc manner not based on a common agreed mathematical framework thereby sometimes leading to unexpected results carver 1994 and often not guaranteeing the same result across different tools altogether there is a huge body of research and implementation which might be consolidated into a unified coverage transformation theory and possibly standard we believe that coverages contribute essentially to better leveraging the potential of big earth data and are instrumental for understanding our planet and acting adequately across all fields from academia to industry from governments to citizens we hope that the coverage fundamentals presented in this work and under standardization by iso and subsequently ogc will help better our understanding of our environment declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the author gratefully acknowledges valuable discussion and contribution by douglas o brien emmanuel devys graham wilkes liping di kathi schleidt jordi escriu and many more colleagues in ogc iso industry and academia who took patience in many discussions on all aspects of coverages the reviewer comments greatly have helped to improve the paper which is gratefully acknowledged this work has been supported through german bmbf deeprain eu h2020 parsec and eu h2020 centurion 
25789,a richards based soil water model was implemented in the apex and epic terrestrial ecosystem models to improve their hydrologic modeling capabilities the richards model together with two existing soil water models were calibrated and evaluated to assess their performance for simulating watershed level hydrology under scenarios of landscape conversion to bioenergy crop production the richards model was shown to better reflect observed soil water dynamics in grain corn and cellulosic switchgrass bioenergy agroecosystems whereas all three models simulated historic streamflows comparably application of the models to understand the impacts of widespread landscape conversion from traditional agriculture to bioenergy producing landscapes indicated disparate conclusions with the richards based simulations indicating a modest 1 0 reduction in streamflow whereas the existing models simulated sizable reductions of 10 6 16 1 this study clearly demonstrates the impact of model methodology on system understanding and contextualizes the wide range of simulated streamflow impacts from bioenergy conversions reported in the literature keywords richards equation soil water modeling bioenergy landscape conversion watershed maize switchgrass 1 introduction cellulosic feedstocks are capable of providing substantial low carbon energy and spurring local bio economies dale et al 2014 jones et al 2017 kim et al 2018 robertson et al 2017 among the more promising avenues for providing cellulosic feedstocks is the harvesting of crop residues such as corn a k a maize zea mays stover and the cultivation of dedicated perennial energy crops such as switchgrass panicum virgatum particularly on marginal lands gelfand et al 2013 so as to avoid displacement of food crop production and produce bioenergy feedstocks on presently underutilized lands these two feedstock production pathways have divergent environmental impacts with residue harvest incurring risks including soil carbon loss and elevated susceptibility for erosion compared to cultivation of perennials which tend to provide ecosystem services including soil carbon sequestration and reduced erosion however outcomes are not simplistic as options exist to improve the environmental impacts of residue harvest jones et al 2018 among the primary concerns regarding the expansion of dedicated bioenergy cropping systems is the potential for increased crop water use with resultant decreases in groundwater recharge and stream flow berndes 2002 le et al 2011 perennial energy crops are generally expected to have greater evapotranspiration et than traditional annual crops due to factors including extended seasonal canopy coverage and root activity however comparative studies have shown disparate water use patterns with perennials showing both elevated et as well as comparable et relative to annuals daigh et al 2014 hamilton et al 2015 le et al 2011 suggesting that relative et responses may depend on context and specifically soil plant climate interactions incorporation of cover crops into rotations can similarly result in increased water use by extending the period of active vegetative growth gabriel et al 2012 zhang and schilling 2006 yet alterations to the water balance can be more complex as vegetative cover root biomass and long term impacts on organic matter can have impacts such as reduced overland runoff reduced evaporation and increased soil water storage basche et al 2016 blanco canqui et al 2015 p drury et al 2014 which could offset or even reverse water use increases from expected increases in crop transpiration conversely harvesting of residues such as corn stover is expected to have the opposite effect inducing reduction in soil cover and depletion of soil organic matter resulting in increased runoff increased evaporation and reduced soil water retention demissie et al 2012 johnson et al 2016 n l klocke et al 2009 due to these complex soil plant climate management interactions models are needed to assess system level responses to bioenergy production scenarios in a manner that capably considers site specific characteristics and drivers to characterize system behavior existing modeling studies have investigated the impacts of bioenergy production on system behaviors and outcomes chen et al 2017 cibin et al 2012 2016 demissie et al 2012 ha et al 2020 wu and liu 2012 however the capacity of such models to skillfully simulate soil water dynamics under novel bioenergy cropping systems has not been assessed here we present improvements to the methodology of soil water modeling and calibration of bioenergy crop production systems judging model performance against soil water data from field experiments and applying the models at the landscape scale as a representative bioenergy cropping system landscape we chose the north raccoon river basin a huc 8 subbasin of the des moines river watershed in iowa usa this sub basin was chosen as it is an intensive agricultural area that shows promise for cellulosic feedstock production jones et al 2017 and feeds into the des moines river the predominant source of water for the city of des moines the specific objectives of this work are to 1 describe a richards based soil water flow model that we have incorporated into apex and epic two widely used watershed and field scale terrestrial ecosystem models 2 assess the efficacy of the existing and richards based soil water flow models at site and watershed scales and 3 utilize the models to assess the implications of conversion to perennial bioenergy feedstock production for watershed level water balances 2 methods to quantitatively compare system behavior and responses under current and potential future bioenergy producing scenarios at management and policy relevant scales we utilized the agricultural policy environmental extender apex model gassman et al 2009 the apex model which is designed for large farm to small watershed scale applications and the environmental policy integrated climate epic model williams 1995 which represents the field scale version of apex have been widely used for simulating biophysical and biogeochemical processes in managed terrestrial ecosystems including bioenergy production systems and watershed level environmental assessments francesconi et al 2014 gassman et al 2004 jones et al 2018 wang et al 2012 these models are continually being updated to improve model performance or expand applicability as dictated by the needs and interests of the model development and user community here we describe improvements made to the soil water modeling methods to allow more skillful assessment of the watershed level water use implications of potential bioenergy crop production scenarios 2 1 improved hydrologic modeling the default saturation hydraulic conductivity soil water flow model in apex and epic utilizes a tipping bucket approach herein referred to as the original soil water flow model which has been shown to overestimate the rate of soil water drainage doro et al 2017 an improved variable saturation hydraulic conductivity model was implemented referred to here as the slug soil water flow model that utilized an empirical approach with inputs and complexity harmonized with the standard soil inputs required for apex and epic simulations doro et al 2017 however despite achieving improvement limitations in skill remained to improve simulation of soil water dynamics we incorporated a richards based richards 1931 soil water flow model into apex and epic which will be referred to as the richards soil water flow model richards based approaches are considered state of the art for simulating soil water flow twarakavi et al 2008 vereecken et al 2016 the main drawbacks of the richards approach are the computational time required as the equation can only be solved analytically under limiting assumptions and parameterization which requires measurements at a level of detail exceeding what is commonly available at large scales e g traditional soil surveys since the apex and epic models are traditionally applied at coarser spatial resolution and larger time and space scales than typical richards based model applications we sought to implement a solution to the richards equation that would minimize the added computational burden an approach developed by ross crevoisier et al 2009 ross 2003 which has demonstrated computational efficiency accuracy robustness and reliable convergence solves the mixed form of richards equation making it suitable for variably saturated soils 1 θ t z k h h z 1 where θ is the soil water content cm cm 1 t is time hr z is depth cm k is the unsaturated hydraulic conductivity cm hr 1 and h is the soil matric pressure cm h2o the soil profile is discretized into n soil layers and the model is applied to simulate vertical water flows through soil layers the richards equation is solved non iteratively utilizing temporal linearization of fluxes as 2 q i σ q i 0 σ q i s i 0 δ s i q i s i 1 0 δ s i 1 f o r i 1 t o n s o i l 1 a i δ s i 1 b i δ s i c i δ s i 1 d i f o r i 1 t o n s o i l where a i q i 1 s i 1 0 b i q i 1 s i 0 q i s i 0 δ z i θ s i θ r i σ δ t c i q i s i 1 0 d i q i 1 0 q i 0 σ s i θ i θ r i θ s i θ r i where q is the water flux cm hr 1 i is the soil layer number σ is the fraction of the timestep s is the degree of saturation a d are equation coefficients δz is the layer thickness cm θ is the soil water content cm cm 1 θr is the residual soil water content cm cm 1 and θs is the saturated soil water content cm cm 1 note that σ 1 in the presence of saturated conditions and σ 0 5 under unsaturated conditions also a1 and cn equal zero as s0 and sn soil 1 do not exist the flow of water into a layer can then be calculated as 3 δ q i δ t q i 1 σ q i σ where q is the flow of water cm and t is the timestep hr the equations are solved on a sub daily time step with the time step varying to limit the maximum allowable change in degree of saturation 4 δ t δ s m a x q i 1 q i θ s i θ r i δ z t m a x where smax is the largest allowable change in degree of saturation iteration is incurred as necessary to ensure the change in s in all soil layers falls below the smax threshold if this change threshold is exceeded a smaller timestep is implemented as 5 δ t i t δ t δ s m a x δ s where δtit is the updated time step hr percolation from the lowest soil layer is assumed to occur as either free gravitational drainage seepage or a constant head boundary condition allowing flexibility as well as incorporation of elevated groundwater tables or subirrigation practices infiltration and evaporation from the surface soil layer as well as root water extraction from root penetrated soil layers are simulated daily following standard apex and epic methodologies gassman et al 2009 subsurface horizontal water flows similar to warrick et al 2008 were simulated assuming zero pressure gradients across horizontal boundaries such that gravity driven darcian flow occurs in the horizontal direction as a function of slope and soil hydraulic characteristics 6 j x k h m l a n d where jx is the horizontal subsurface flow cm hr 1 and m land is the slope gradient cm cm 1 in order to parameterize the model soil water retention and unsaturated hydraulic conductivity functions were characterized using a modification similar to van genuchten mualem vgm soil hydraulic models schaap and van genuchten 2006 but instead retaining the occurrence of saturation at zero matric pressure and allowing macropore flow in the unsaturated zone ross 2006 hence the soil water retention model is characterized as 7 s h 1 h h g n m f o r h h t h r e s h s h 1 2 s t h r e s h 1 h t h r e s h a 1 a 1 2 4 a 2 h t h r e s h f o r h h t h r e s h m 1 1 n a 1 2 h t h r e s h s t h r e s h 1 d h d s s t h r e s h a 2 h t h r e s h a 1 where hg cm water is a scaling parameter m and n are shape parameters sthresh is the degree of saturation threshold for saturated conditions hthresh cm water is the soil matric pressure when s sthresh and a1 and a2 are equation coefficients here it is assumed that sthresh equals 0 99 the hydraulic conductivity model is defined as 8 k h k s k v h r h k v h r h 1 1 r m a c 1 h h m a c 1 f o r 0 h h m a c 1 r h r m a c 1 h h m a c 2 h m a c 1 h m a c 2 f o r h m a c 1 h h m a c 2 r h 0 f o r h h m a c 2 k v h k s x m p 1 1 x m 2 f o r h h s k v h k s f o r h h s x 1 1 h n where ks is the saturated hydraulic conductivity cm hr 1 kv is the macropore adjusted hydraulic conductivity cm hr 1 r is an equation coefficient hmac1 cm water and hmac2 cm water are soil matric pressure thresholds distinguishing between exponential macropore flow non exponential macropore flow and soil matrix flow rmac1 is the r at a soil matric pressure of hmac1 x is an equation coefficient and p is a pore connectivity parameter here it is assumed that hmac1 equals 4 cm water hmac2 equals 40 cm water rmac1 equals 0 25 and p equals 0 5 these models avoid highly non linear hydraulic property changes near soil water saturation improving the efficiency and stability of numerical solutions since the vgm models require parameters not included in the standard apex and epic soil inputs the capacity to specify these parameters by soil layer was added however most apex and epic model applications utilize cardinal soil water thresholds or texture based pedotransfer functions ptfs to characterize the soil hydraulic characteristics to align parameterization with the needs of the model user base ptfs were included to estimate the vgm parameters based on soil texture and cardinal water content thresholds hence existing ptfs developed by jones et al 2014 weynants et al 2009 and wösten et al 1999 were incorporated as well as the option to enter the vgm parameters directly 2 2 improved parameterization of bioenergy cropping systems to evaluate and improve the performance of the updated model for simulating soil water dynamics under bioenergy cropping systems a field experiment in southwestern michigan including various candidate bioenergy cropping systems hamilton et al 2015 was utilized for model calibration and independent assessment of model skill utilizing similar methodologies to jones et al 2018 the experiment was selected due to its detailed monitoring of soil water content at seven depths using time domain reflectometry tdr probes under continuous corn and switchgrass treatments we used soil water content measurements from 2010 to 2013 with data from the even years used for calibration purposes and the data from the odd years was reserved for independent model evaluation the watershed is the relevant scale for assessing the net impact of landscape changes in crop water use on streamflow as a representative agricultural watershed in the midwest u s we chose the north raccoon river basin which is a huc 8 subbasin of the des moines river watershed fig 1 this watershed was chosen as it is a highly agricultural area lies in a promising area for cellulosic feedstock production jones et al 2017 and feeds into the des moines river which is the predominant source of water for the city of des moines where the des moines water works have dealt with a long history of water quality issues at least partially linked to agricultural practices hatfield et al 2009 daily streamflow data were available from the united states geological survey stream gage station 05482500 dating back to 1940 here we simulated the 1980 2016 period with data from even years utilized for calibration and data from odd years used for independent model evaluation the apex model was set up to represent the land characteristics and management in the watershed the arcapex tool tuppad et al 2009 was utilized to delineate 99 individual subareas within the watershed according to land use soil type and topography soils were characterized using the united states department of agriculture usda state soil geographic statsgo database selecting the most dominant soil type within a subarea the dominant land use in each subarea was characterized using a crop rotation product derived from the usda cropland data layer cdl from 2012 to 2014 according to sahajpal et al 2014 planting and harvesting dates for row crop production were derived from usda estimates of typical dates usda nass 2010 daily meteorological data including maximum and minimum temperature precipitation solar radiation relative humidity and wind speed were obtained from the north american land data assimilation system project phase 2 nldas 2 xia et al this is a gridded reanalysis product that resulted in 21 unique grids within or adjacent to the watershed weather data within subareas were defined by the nearest grid according to the centroid of the subarea and the nldas 2 grid parameters were selected for calibration based on expert knowledge of apex and the most influential parameters for soil water content and subsurface and surface water flow a parameter screening step was conducted to eliminate the least influential parameters through implementation of the method of morris campolongo et al 2007 considering the influence μ of a parameter on model skill for simulating soil water content according to the nash sutcliffe coefficient of efficiency nse with the average of the nses calculated from the soil water content and streamflow simulations utilized for a balanced assessment each soil water model was executed for 570 iterations at the site and watershed levels for the sensitivity analysis the reduced set of parameters were calibrated utilizing the differential evolution adaptive metropolis vrugt and braak 2011 algorithm dream utilizing minimization of the root mean squared error rmse as the objective function to balance unit magnitude differences soil water content and streamflow were normalized according to their respective observed means and standard deviations the model was executed for 3 000 iterations at both the site and watershed scales to implement the calibration for each soil water model both the dream and method of morris methods were implemented using the r software r core team 2015 together these steps were implemented to better characterize parameter sensitivities and estimate effective parameter values for accurate simulation of soil water dynamics to contextualize the performance of the richards model relative to apex and epic this model evaluation process was conducted for each of the three soil water flow models subsequently we utilized the parameterized apex model to assess the impact of conversion to bioenergy producing landscapes within the watershed on water fluxes and streamflow the baseline scenario was represented by the unmodified setup utilized for the calibration and evaluation procedures while bioenergy producing landscapes were created to convert varying proportions of the row crop subareas from corn or corn soybean rotation to switchgrass to simulate the impacts of varying intensities of landscape conversion row cropped subareas were selected for conversion to switchgrass in increments of three comprising 3 8 of the 78 subareas under row crop agriculture per iteration switchgrass biomass was assumed to be harvested each fall while grain crops were assumed to be under no till management to ensure the baseline scenarios reflected aspirational management practices scenarios were assessed in terms of impacts on streamflow as well as water fluxes and biophysical drivers it should be noted that while model improvements were made to both the apex and epic models apex was selected for parameterization evaluation and application purposes because while both models are comparable at the plot to field level only apex is capable of simulating watershed level processes 3 results and discussion 3 1 improved parameterization of bioenergy cropping systems the method of morris screening procedure resulted in similar parameter importance metrics across soil water models figure a1 resulting in retention of similar parameters for the calibration procedure table a1 layer level lower limit of soil water content and field capacity were calibrated for all soil water models to ensure inaccurate soil characterization data did not incur biases and force parameters towards extremes to compensate in addition to these soil water holding limits an additional 11 15 and 10 parameters were retained for the original richards and slug methods respectively the calibration procedure resulted in reasonable fits r2 0 49 for soil water content simulations across soil water models and cropping systems fig 2 table 1 simulations were notably more skillful under corn r2 0 63 0 81 than under switchgrass r2 0 49 0 63 and the richards soil water model produced the best fits of the three models for both corn and switchgrass cropping systems notable residual trends can be observed in fig 2 particularly for the original and slug submodels whereas the richards submodel demonstrated more balanced distribution of residuals across the range of vwc investigating the model fits by soil layer fig 3 indicates an inability of the original model to capture soil water dynamics near the soil surface this known model inadequacy in part motivated implementation of the slug method into epic doro et al 2017 which here is shown to improve this shortcoming under corn cropping whereas the insufficiency remains under switchgrass additional patterns of model bias are observed for many of the soil layers for the original and slug submodels under both crops but to a greater severity under switchgrass the richards model demonstrated some consistent bias at certain layers particularly under switchgrass but proved consistently more aligned with the observed soil water dynamics in the streamflow simulations the three soil water models performed quite similarly fig 4 table 2 with r2 ranging only between 0 64 and 0 65 the slug model compared most favorably with observed streamflows rmse 31 4 m3 s 1 while the richards model differed the most rmse 32 3 m3 s 1 albeit with the lowest percent bias pbias overall the evaluation of the soil water models indicated each was quite comparable in terms of streamflow simulation whereas the richards submodel was notably more skilled for simulating soil water dynamics under both corn and switchgrass cropping this similarity among the submodels in simulating historic streamflows for which the dominant land use was grain cropping with no adoption of switchgrass cultivation and the divergence among the submodels for simulating root zone soil water under switchgrass cultivation indicate the watershed level hydrology simulated by the submodels will be more divergent under switchgrass cultivation than under traditional grain production systems 3 2 watershed level assessment of bioenergy crop production scenarios the crop rotations product derived from the cdl characterized the watershed area as comprised of 65 corn soybean rotation 13 continuous corn 9 urban developed land 7 grassland pasture and the remaining 6 as water fig 1 hence 78 of the area was under row crop agriculture and 85 under managed agriculture mean simulated dry matter corn yields from the 1980 2016 period were 11 9 12 7 and 10 2 mg ha 1 based on the original slug and richards methods respectively compared to a mean reported usda nass yield from 2007 to 2016 in hardin county iowa of 9 4 mg ha 1 while the periods of comparison differ they were selected as the simulated cultivars and management technologies are better aligned with modern production practices which evolve over time sacks and kucharik 2011 and would be expected to influence harvestable yield more than water consumption or streamflow which in rain fed cropping systems can be quite temporally stable in the face of changing agricultural practices because different rainfed cropping systems as well as fallow fields may use all available soil water during the growing season hamilton et al 2018 mean simulated dry matter soybean yields were 3 5 3 7 and 2 8 mg ha 1 for the original slug and richards methods respectively compared to a mean reported usda nass yield from 2007 to 2016 in hardin county iowa of 3 0 mg ha 1 mean simulated dry matter switchgrass yields were 11 5 11 9 and 10 2 mg ha 1 for the original slug and richards methods respectively whereas switchgrass yields in similar regions in north central iowa have been reported in the 12 0 12 3 mg ha 1 range gassman et al 2017 trybula et al 2015 hence simulated yields in the watershed were comparable to expected yields in the region simulated yields for all crops were consistently highest with the slug method and lowest with the richards method the latter difference largely driven by lower available soil water in the root zone due to increased simulated percolation utilizing the richards method compared to the other methods simulation of the grain crop to switchgrass conversion scenarios revealed that while the three soil water models performed comparably for the historical streamflow evaluation marked differences between the submodels manifested under land use conversion fig 5 the simulated reductions in streamflow as a result of cropping system conversion to switchgrass was consistently highest with the original model and simulated reductions with both the original and slug models were much higher than with the richards model under the most extreme scenario of complete conversion of agricultural croplands to switchgrass apex simulations utilizing the richards submodel produced a modest 1 0 reduction in streamflow compared to 10 6 and 16 1 reductions simulated utilizing the slug and original submodels respectively streamflow reduction increased monotonically with increasing cropland conversion for the original and slug submodels whereas reductions demonstrated site specific directionality dependence as conversion of some groups of subareas resulted in less streamflow reduction the disparity in simulated streamflow reductions utilizing the richards submodel compared to the original or slug submodels is driven by inverse simulation of the impacts of the land use conversion on percolation and et fig 6 table 3 while all three submodels simulate reduced runoff under greater area in the perennial switchgrass crop the original and slug submodels simulate increased et and reduced percolation whereas the richards submodel simulates reduced et and increased percolation this is partly explained by the underestimation of near surface soil water content by the original model and to a lesser degree by the slug model perennial switchgrass cropping systems result in greater surface vegetative and residue cover than annual cropping systems reducing evaporative losses from the near soil surface hence the underestimation of near surface soil water content in the original and slug models mitigates the impact of the increased surface cover under switchgrass towards reduced et relative to grain cropping this contributed to water consumption in the two cropping systems being more similar throughout the year with richards based simulations than with original or slug based simulations fig 7 the longer growing season of perennials compared to annuals does result in elevated water consumption when perennials are active prior to annual crop establishment and following harvest however while the original and slug submodels simulated elevated water consumption and hence lowered streamflow under perennial conversion across the calendar year the richards submodel predicted that perennial conversion would increase water consumption outside of peak annual growing season but result in reduced water consumption during peak annual growth months in june and july when et rates of corn considerably exceed that of perennials these intra annual water consumption patterns with the richards submodel align more closely with experimental observations than with the original or slug submodels abraha et al 2020 eichelmann et al 2016 the increased percolation under perennial cropping systems simulated utilizing the richards submodel is also supported in the literature parish et al 2019 stenjem et al 2019 as increased soil organic carbon and a deeper and denser rooting structure enable greater soil water infiltration and percolation zaibon et al 2017 the large disparity in simulated outcomes among the soil water models highlights the importance of soil hydraulic modeling methodology for understanding system behavior and informing management or policy here richards based simulations indicate modest water quantity impacts from widespread perennial conversion whereas the impacts simulated utilizing the original or slug submodels would be quite consequential this methodological disparity aligns with uncertainty in the literature surrounding the water quantity impacts of conversion to bioenergy production landscapes where the magnitude and directionality of impacts vary widely robertson et al 2017 the focus here on the implementation of a richards based soil water model in the apex and epic models is particularly relevant as these models comprise a family of related models that also includes the soil and water assessment tool swat a model that has been widely used for assessing the hydrologic impacts of landscape conversions including to bioenergy crop production and can actually ingest apex simulations for integrating smaller agricultural watersheds into larger scale mixed land cover studies the richards based methodology is planned for implementation into the swat model replacing the default soil water model that has heretofore been used in swat past studies using the default soil water model may have reached different conclusions with the richards based methodology for instance a swat modeling study in the skunk creek watershed of south dakota reported streamflow reductions of 19 following conversion of agricultural lands which comprised 64 of the land area to perennial grassland ahiablame et al 2019 similarly another swat modeling study in the skeleton creek watershed of oklahoma reported a 27 7 reduction in streamflow following conversion of grasslands which comprised 35 of the land area to switchgrass yimam et al 2017 while these impacts on streamflow would be quite concerning those studies implemented the default swat soil water model analogous to the original soil water model in apex and epic which may prove similarly incapable of reflecting the differential soil water dynamics under annual and perennial cropping nevertheless it is important to note that those studies were conducted in regions of drier climate and hence greater potential soil water limitation than the present study the findings presented here indicating minimal impact on annual streamflow from grain crop to switchgrass conversion when utilizing the richards soil water model align well with other observational studies in humid climates of the upper midwest for instance eddy covariance and soil moisture monitoring studies in southwestern michigan have indicated comparable annual et rates under annual corn and perennial switchgrass or mixed species grassland cropping systems abraha et al 2020 hamilton et al 2015 similarly a study in southwestern ontario reported eddy covariance estimates of annual et that were lower under switchgrass than under corn eichelmann et al 2016 in a more comprehensive assessment of surface subsurface and lateral fluxes an assessment of streamflows in the augusta creek watershed in southwest michigan reported stable streamflows across a 50 year period despite abandonment of 27 of the land area from row cropping to perennial vegetation and 20 of the land area from row cropping to deciduous forest hamilton et al 2018 not surprisingly however context matters and observational studies in the literature also indicate the reverse for instance a soil water monitoring study in east central illinois at a location with a high water table reported et increases of 104 mm and drainage water reduction of 32 following conversion of corn soybean rotation to switchgrass mcisaac et al 2010 zeri et al 2013 biophysical phenomena can be highly spatially variable as a lysimeter study in southern wisconsin reported between a 38 reduction and 78 increase in water drainage relative to a corn treatment under various perennial bioenergy treatments which the authors attributed to the interaction of root development with macropore flow parish et al 2019 more broadly it should be noted that while widespread landscape conversion to perennial bioenergy production may have modest impacts on annual water consumption in some ecosystems the scenarios presented here are intended to demonstrate methodological improvements rather than to influence policy for bioenergy production a plethora of environmental and socioeconomic factors needs to be considered to inform viable strategies for perennial cellulosic biomass production robertson et al 2017 tactical approaches such as cultivation on marginal lands or in vegetative buffer strips are generally seen as more viable options to realize meaningful environmental benefits while avoiding displacement of food production on croplands while further research is needed to better understand the hydrological effects of different cropping systems and implications of potential landscape conversion to perennial bioenergy crop production implementation and utilization of more refined hydrologic models with capabilities to account for macropore flow such as the model implemented here will be beneficial for clearer scientific understanding and better informing policies and decision makers improved modeling techniques might alter response dimensions from indicating more extreme changes towards more modest alterations for instance a swat based simulation study in the little vermilion river watershed in east central illinois utilized drainmod based tile drainage parameterization and reported only small reductions in streamflow upon conversion of row crops to switchgrass and other perennials with the most severe scenario resulting in only a 0 76 streamflow reduction guo et al 2018 similarly a simulation study in the mississippi atchafalaya river basin utilizing the agro ibis dynamic global vegetation model and terrestrial hydrology model with biogeochemistry indicated stream discharge reductions of less than 1 5 under the most extreme conversion scenario of row crops to switchgrass or miscanthus vanloocke et al 2017 hence care must be taken to ensure models are capable of reflecting the scenarios they are being used to assess otherwise such simulation exercises may obfuscate understanding in the literature when the state of knowledge derived from empirical observations is clearer 4 conclusions the updated richards based soil water model was implemented into the apex and epic terrestrial ecosystem models evaluation of the richards model compared with two existing soil water models indicated that while all could suitably simulate streamflow under traditional land use the richards model was better able to reflect observed soil water dynamics particularly under switchgrass cropping systems independent application of the apex model utilizing the three soil water submodels to understand the implications of potential conversion of traditional agriculture towards bioenergy production revealed widely different implications for streamflow the richards based estimate indicated only slight 1 reductions in streamflow under widespread cropland conversion whereas the original and slug based simulations estimated considerable 10 16 streamflow reductions experimental and observational studies in similar climates tend to support more modest estimates of hydrological impacts of land conversion from grain crops to cellulosic bioenergy crops although contradictory findings have been observed these findings highlight the importance of modeling methodologies for enabling meaningful understanding of complex systems software availability name of software apex and epic developers curtis jones cujo umd edu jimmy williams retired jaehak jeong jjeong brc tamus edu hardware required pc software required windows or linux availability available under gnu license upon request from https epicapex tamu edu cost free program language fortran first available 1980 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the us doe office of science de fc02 07er64494 to the great lakes bioenergy research center the usda nifa 2019 68012 29888 and texas agrilife research texas a m university r c i appendices table a1 parameters selected for calibration of the apex model table a1 parameter definition original richards slug rft0 groundwater residence time x x x rfp0 return flow deep percolation x x x iwtb duration of antecedent period for rainfall and pet to drive water table qg channel capacity flow rate qcf exponent in watershed area flow rate equation x chs0 average upland slope bwd channel botom width depth x fcw floodplain width channel width x fps0 floodplain saturated hydraulic conductivity gws0 maximum groundwater residence time x x x parm1 canopy pet factor x parm2 root growth soil strength parm5 soil water lower limit x x parm12 soil evaporation coefficient x parm16 cn retention parameter parm17 soil evaporation plant cover factor x x x parm20 runoff cn initial abstraction x x x parm38 water stress weighting coefficient parm40 groundwater storage threshold x x x parm42 scs cn index coefficient parm49 maximum rainfall interception by plant canopy mm x x x parm50 rainfall interception coefficient x x x parm51 water stored in litter residue x x parm82 permeability parameter x x parm87 water table recession coefficient parm88 daily water table movement limit x parm89 water table recession exponent parm90 subsurface flow factor u g1 soil lower limit for corn plot x x x fc g1 soil field capacity for corn plot x x x u g5 soil lower limit for switchgrass plot x x x fc g5 soil field capacity for switchgrass plot x x x fig a1 method of morris parameter importance for the three soil water models fig a1 
25789,a richards based soil water model was implemented in the apex and epic terrestrial ecosystem models to improve their hydrologic modeling capabilities the richards model together with two existing soil water models were calibrated and evaluated to assess their performance for simulating watershed level hydrology under scenarios of landscape conversion to bioenergy crop production the richards model was shown to better reflect observed soil water dynamics in grain corn and cellulosic switchgrass bioenergy agroecosystems whereas all three models simulated historic streamflows comparably application of the models to understand the impacts of widespread landscape conversion from traditional agriculture to bioenergy producing landscapes indicated disparate conclusions with the richards based simulations indicating a modest 1 0 reduction in streamflow whereas the existing models simulated sizable reductions of 10 6 16 1 this study clearly demonstrates the impact of model methodology on system understanding and contextualizes the wide range of simulated streamflow impacts from bioenergy conversions reported in the literature keywords richards equation soil water modeling bioenergy landscape conversion watershed maize switchgrass 1 introduction cellulosic feedstocks are capable of providing substantial low carbon energy and spurring local bio economies dale et al 2014 jones et al 2017 kim et al 2018 robertson et al 2017 among the more promising avenues for providing cellulosic feedstocks is the harvesting of crop residues such as corn a k a maize zea mays stover and the cultivation of dedicated perennial energy crops such as switchgrass panicum virgatum particularly on marginal lands gelfand et al 2013 so as to avoid displacement of food crop production and produce bioenergy feedstocks on presently underutilized lands these two feedstock production pathways have divergent environmental impacts with residue harvest incurring risks including soil carbon loss and elevated susceptibility for erosion compared to cultivation of perennials which tend to provide ecosystem services including soil carbon sequestration and reduced erosion however outcomes are not simplistic as options exist to improve the environmental impacts of residue harvest jones et al 2018 among the primary concerns regarding the expansion of dedicated bioenergy cropping systems is the potential for increased crop water use with resultant decreases in groundwater recharge and stream flow berndes 2002 le et al 2011 perennial energy crops are generally expected to have greater evapotranspiration et than traditional annual crops due to factors including extended seasonal canopy coverage and root activity however comparative studies have shown disparate water use patterns with perennials showing both elevated et as well as comparable et relative to annuals daigh et al 2014 hamilton et al 2015 le et al 2011 suggesting that relative et responses may depend on context and specifically soil plant climate interactions incorporation of cover crops into rotations can similarly result in increased water use by extending the period of active vegetative growth gabriel et al 2012 zhang and schilling 2006 yet alterations to the water balance can be more complex as vegetative cover root biomass and long term impacts on organic matter can have impacts such as reduced overland runoff reduced evaporation and increased soil water storage basche et al 2016 blanco canqui et al 2015 p drury et al 2014 which could offset or even reverse water use increases from expected increases in crop transpiration conversely harvesting of residues such as corn stover is expected to have the opposite effect inducing reduction in soil cover and depletion of soil organic matter resulting in increased runoff increased evaporation and reduced soil water retention demissie et al 2012 johnson et al 2016 n l klocke et al 2009 due to these complex soil plant climate management interactions models are needed to assess system level responses to bioenergy production scenarios in a manner that capably considers site specific characteristics and drivers to characterize system behavior existing modeling studies have investigated the impacts of bioenergy production on system behaviors and outcomes chen et al 2017 cibin et al 2012 2016 demissie et al 2012 ha et al 2020 wu and liu 2012 however the capacity of such models to skillfully simulate soil water dynamics under novel bioenergy cropping systems has not been assessed here we present improvements to the methodology of soil water modeling and calibration of bioenergy crop production systems judging model performance against soil water data from field experiments and applying the models at the landscape scale as a representative bioenergy cropping system landscape we chose the north raccoon river basin a huc 8 subbasin of the des moines river watershed in iowa usa this sub basin was chosen as it is an intensive agricultural area that shows promise for cellulosic feedstock production jones et al 2017 and feeds into the des moines river the predominant source of water for the city of des moines the specific objectives of this work are to 1 describe a richards based soil water flow model that we have incorporated into apex and epic two widely used watershed and field scale terrestrial ecosystem models 2 assess the efficacy of the existing and richards based soil water flow models at site and watershed scales and 3 utilize the models to assess the implications of conversion to perennial bioenergy feedstock production for watershed level water balances 2 methods to quantitatively compare system behavior and responses under current and potential future bioenergy producing scenarios at management and policy relevant scales we utilized the agricultural policy environmental extender apex model gassman et al 2009 the apex model which is designed for large farm to small watershed scale applications and the environmental policy integrated climate epic model williams 1995 which represents the field scale version of apex have been widely used for simulating biophysical and biogeochemical processes in managed terrestrial ecosystems including bioenergy production systems and watershed level environmental assessments francesconi et al 2014 gassman et al 2004 jones et al 2018 wang et al 2012 these models are continually being updated to improve model performance or expand applicability as dictated by the needs and interests of the model development and user community here we describe improvements made to the soil water modeling methods to allow more skillful assessment of the watershed level water use implications of potential bioenergy crop production scenarios 2 1 improved hydrologic modeling the default saturation hydraulic conductivity soil water flow model in apex and epic utilizes a tipping bucket approach herein referred to as the original soil water flow model which has been shown to overestimate the rate of soil water drainage doro et al 2017 an improved variable saturation hydraulic conductivity model was implemented referred to here as the slug soil water flow model that utilized an empirical approach with inputs and complexity harmonized with the standard soil inputs required for apex and epic simulations doro et al 2017 however despite achieving improvement limitations in skill remained to improve simulation of soil water dynamics we incorporated a richards based richards 1931 soil water flow model into apex and epic which will be referred to as the richards soil water flow model richards based approaches are considered state of the art for simulating soil water flow twarakavi et al 2008 vereecken et al 2016 the main drawbacks of the richards approach are the computational time required as the equation can only be solved analytically under limiting assumptions and parameterization which requires measurements at a level of detail exceeding what is commonly available at large scales e g traditional soil surveys since the apex and epic models are traditionally applied at coarser spatial resolution and larger time and space scales than typical richards based model applications we sought to implement a solution to the richards equation that would minimize the added computational burden an approach developed by ross crevoisier et al 2009 ross 2003 which has demonstrated computational efficiency accuracy robustness and reliable convergence solves the mixed form of richards equation making it suitable for variably saturated soils 1 θ t z k h h z 1 where θ is the soil water content cm cm 1 t is time hr z is depth cm k is the unsaturated hydraulic conductivity cm hr 1 and h is the soil matric pressure cm h2o the soil profile is discretized into n soil layers and the model is applied to simulate vertical water flows through soil layers the richards equation is solved non iteratively utilizing temporal linearization of fluxes as 2 q i σ q i 0 σ q i s i 0 δ s i q i s i 1 0 δ s i 1 f o r i 1 t o n s o i l 1 a i δ s i 1 b i δ s i c i δ s i 1 d i f o r i 1 t o n s o i l where a i q i 1 s i 1 0 b i q i 1 s i 0 q i s i 0 δ z i θ s i θ r i σ δ t c i q i s i 1 0 d i q i 1 0 q i 0 σ s i θ i θ r i θ s i θ r i where q is the water flux cm hr 1 i is the soil layer number σ is the fraction of the timestep s is the degree of saturation a d are equation coefficients δz is the layer thickness cm θ is the soil water content cm cm 1 θr is the residual soil water content cm cm 1 and θs is the saturated soil water content cm cm 1 note that σ 1 in the presence of saturated conditions and σ 0 5 under unsaturated conditions also a1 and cn equal zero as s0 and sn soil 1 do not exist the flow of water into a layer can then be calculated as 3 δ q i δ t q i 1 σ q i σ where q is the flow of water cm and t is the timestep hr the equations are solved on a sub daily time step with the time step varying to limit the maximum allowable change in degree of saturation 4 δ t δ s m a x q i 1 q i θ s i θ r i δ z t m a x where smax is the largest allowable change in degree of saturation iteration is incurred as necessary to ensure the change in s in all soil layers falls below the smax threshold if this change threshold is exceeded a smaller timestep is implemented as 5 δ t i t δ t δ s m a x δ s where δtit is the updated time step hr percolation from the lowest soil layer is assumed to occur as either free gravitational drainage seepage or a constant head boundary condition allowing flexibility as well as incorporation of elevated groundwater tables or subirrigation practices infiltration and evaporation from the surface soil layer as well as root water extraction from root penetrated soil layers are simulated daily following standard apex and epic methodologies gassman et al 2009 subsurface horizontal water flows similar to warrick et al 2008 were simulated assuming zero pressure gradients across horizontal boundaries such that gravity driven darcian flow occurs in the horizontal direction as a function of slope and soil hydraulic characteristics 6 j x k h m l a n d where jx is the horizontal subsurface flow cm hr 1 and m land is the slope gradient cm cm 1 in order to parameterize the model soil water retention and unsaturated hydraulic conductivity functions were characterized using a modification similar to van genuchten mualem vgm soil hydraulic models schaap and van genuchten 2006 but instead retaining the occurrence of saturation at zero matric pressure and allowing macropore flow in the unsaturated zone ross 2006 hence the soil water retention model is characterized as 7 s h 1 h h g n m f o r h h t h r e s h s h 1 2 s t h r e s h 1 h t h r e s h a 1 a 1 2 4 a 2 h t h r e s h f o r h h t h r e s h m 1 1 n a 1 2 h t h r e s h s t h r e s h 1 d h d s s t h r e s h a 2 h t h r e s h a 1 where hg cm water is a scaling parameter m and n are shape parameters sthresh is the degree of saturation threshold for saturated conditions hthresh cm water is the soil matric pressure when s sthresh and a1 and a2 are equation coefficients here it is assumed that sthresh equals 0 99 the hydraulic conductivity model is defined as 8 k h k s k v h r h k v h r h 1 1 r m a c 1 h h m a c 1 f o r 0 h h m a c 1 r h r m a c 1 h h m a c 2 h m a c 1 h m a c 2 f o r h m a c 1 h h m a c 2 r h 0 f o r h h m a c 2 k v h k s x m p 1 1 x m 2 f o r h h s k v h k s f o r h h s x 1 1 h n where ks is the saturated hydraulic conductivity cm hr 1 kv is the macropore adjusted hydraulic conductivity cm hr 1 r is an equation coefficient hmac1 cm water and hmac2 cm water are soil matric pressure thresholds distinguishing between exponential macropore flow non exponential macropore flow and soil matrix flow rmac1 is the r at a soil matric pressure of hmac1 x is an equation coefficient and p is a pore connectivity parameter here it is assumed that hmac1 equals 4 cm water hmac2 equals 40 cm water rmac1 equals 0 25 and p equals 0 5 these models avoid highly non linear hydraulic property changes near soil water saturation improving the efficiency and stability of numerical solutions since the vgm models require parameters not included in the standard apex and epic soil inputs the capacity to specify these parameters by soil layer was added however most apex and epic model applications utilize cardinal soil water thresholds or texture based pedotransfer functions ptfs to characterize the soil hydraulic characteristics to align parameterization with the needs of the model user base ptfs were included to estimate the vgm parameters based on soil texture and cardinal water content thresholds hence existing ptfs developed by jones et al 2014 weynants et al 2009 and wösten et al 1999 were incorporated as well as the option to enter the vgm parameters directly 2 2 improved parameterization of bioenergy cropping systems to evaluate and improve the performance of the updated model for simulating soil water dynamics under bioenergy cropping systems a field experiment in southwestern michigan including various candidate bioenergy cropping systems hamilton et al 2015 was utilized for model calibration and independent assessment of model skill utilizing similar methodologies to jones et al 2018 the experiment was selected due to its detailed monitoring of soil water content at seven depths using time domain reflectometry tdr probes under continuous corn and switchgrass treatments we used soil water content measurements from 2010 to 2013 with data from the even years used for calibration purposes and the data from the odd years was reserved for independent model evaluation the watershed is the relevant scale for assessing the net impact of landscape changes in crop water use on streamflow as a representative agricultural watershed in the midwest u s we chose the north raccoon river basin which is a huc 8 subbasin of the des moines river watershed fig 1 this watershed was chosen as it is a highly agricultural area lies in a promising area for cellulosic feedstock production jones et al 2017 and feeds into the des moines river which is the predominant source of water for the city of des moines where the des moines water works have dealt with a long history of water quality issues at least partially linked to agricultural practices hatfield et al 2009 daily streamflow data were available from the united states geological survey stream gage station 05482500 dating back to 1940 here we simulated the 1980 2016 period with data from even years utilized for calibration and data from odd years used for independent model evaluation the apex model was set up to represent the land characteristics and management in the watershed the arcapex tool tuppad et al 2009 was utilized to delineate 99 individual subareas within the watershed according to land use soil type and topography soils were characterized using the united states department of agriculture usda state soil geographic statsgo database selecting the most dominant soil type within a subarea the dominant land use in each subarea was characterized using a crop rotation product derived from the usda cropland data layer cdl from 2012 to 2014 according to sahajpal et al 2014 planting and harvesting dates for row crop production were derived from usda estimates of typical dates usda nass 2010 daily meteorological data including maximum and minimum temperature precipitation solar radiation relative humidity and wind speed were obtained from the north american land data assimilation system project phase 2 nldas 2 xia et al this is a gridded reanalysis product that resulted in 21 unique grids within or adjacent to the watershed weather data within subareas were defined by the nearest grid according to the centroid of the subarea and the nldas 2 grid parameters were selected for calibration based on expert knowledge of apex and the most influential parameters for soil water content and subsurface and surface water flow a parameter screening step was conducted to eliminate the least influential parameters through implementation of the method of morris campolongo et al 2007 considering the influence μ of a parameter on model skill for simulating soil water content according to the nash sutcliffe coefficient of efficiency nse with the average of the nses calculated from the soil water content and streamflow simulations utilized for a balanced assessment each soil water model was executed for 570 iterations at the site and watershed levels for the sensitivity analysis the reduced set of parameters were calibrated utilizing the differential evolution adaptive metropolis vrugt and braak 2011 algorithm dream utilizing minimization of the root mean squared error rmse as the objective function to balance unit magnitude differences soil water content and streamflow were normalized according to their respective observed means and standard deviations the model was executed for 3 000 iterations at both the site and watershed scales to implement the calibration for each soil water model both the dream and method of morris methods were implemented using the r software r core team 2015 together these steps were implemented to better characterize parameter sensitivities and estimate effective parameter values for accurate simulation of soil water dynamics to contextualize the performance of the richards model relative to apex and epic this model evaluation process was conducted for each of the three soil water flow models subsequently we utilized the parameterized apex model to assess the impact of conversion to bioenergy producing landscapes within the watershed on water fluxes and streamflow the baseline scenario was represented by the unmodified setup utilized for the calibration and evaluation procedures while bioenergy producing landscapes were created to convert varying proportions of the row crop subareas from corn or corn soybean rotation to switchgrass to simulate the impacts of varying intensities of landscape conversion row cropped subareas were selected for conversion to switchgrass in increments of three comprising 3 8 of the 78 subareas under row crop agriculture per iteration switchgrass biomass was assumed to be harvested each fall while grain crops were assumed to be under no till management to ensure the baseline scenarios reflected aspirational management practices scenarios were assessed in terms of impacts on streamflow as well as water fluxes and biophysical drivers it should be noted that while model improvements were made to both the apex and epic models apex was selected for parameterization evaluation and application purposes because while both models are comparable at the plot to field level only apex is capable of simulating watershed level processes 3 results and discussion 3 1 improved parameterization of bioenergy cropping systems the method of morris screening procedure resulted in similar parameter importance metrics across soil water models figure a1 resulting in retention of similar parameters for the calibration procedure table a1 layer level lower limit of soil water content and field capacity were calibrated for all soil water models to ensure inaccurate soil characterization data did not incur biases and force parameters towards extremes to compensate in addition to these soil water holding limits an additional 11 15 and 10 parameters were retained for the original richards and slug methods respectively the calibration procedure resulted in reasonable fits r2 0 49 for soil water content simulations across soil water models and cropping systems fig 2 table 1 simulations were notably more skillful under corn r2 0 63 0 81 than under switchgrass r2 0 49 0 63 and the richards soil water model produced the best fits of the three models for both corn and switchgrass cropping systems notable residual trends can be observed in fig 2 particularly for the original and slug submodels whereas the richards submodel demonstrated more balanced distribution of residuals across the range of vwc investigating the model fits by soil layer fig 3 indicates an inability of the original model to capture soil water dynamics near the soil surface this known model inadequacy in part motivated implementation of the slug method into epic doro et al 2017 which here is shown to improve this shortcoming under corn cropping whereas the insufficiency remains under switchgrass additional patterns of model bias are observed for many of the soil layers for the original and slug submodels under both crops but to a greater severity under switchgrass the richards model demonstrated some consistent bias at certain layers particularly under switchgrass but proved consistently more aligned with the observed soil water dynamics in the streamflow simulations the three soil water models performed quite similarly fig 4 table 2 with r2 ranging only between 0 64 and 0 65 the slug model compared most favorably with observed streamflows rmse 31 4 m3 s 1 while the richards model differed the most rmse 32 3 m3 s 1 albeit with the lowest percent bias pbias overall the evaluation of the soil water models indicated each was quite comparable in terms of streamflow simulation whereas the richards submodel was notably more skilled for simulating soil water dynamics under both corn and switchgrass cropping this similarity among the submodels in simulating historic streamflows for which the dominant land use was grain cropping with no adoption of switchgrass cultivation and the divergence among the submodels for simulating root zone soil water under switchgrass cultivation indicate the watershed level hydrology simulated by the submodels will be more divergent under switchgrass cultivation than under traditional grain production systems 3 2 watershed level assessment of bioenergy crop production scenarios the crop rotations product derived from the cdl characterized the watershed area as comprised of 65 corn soybean rotation 13 continuous corn 9 urban developed land 7 grassland pasture and the remaining 6 as water fig 1 hence 78 of the area was under row crop agriculture and 85 under managed agriculture mean simulated dry matter corn yields from the 1980 2016 period were 11 9 12 7 and 10 2 mg ha 1 based on the original slug and richards methods respectively compared to a mean reported usda nass yield from 2007 to 2016 in hardin county iowa of 9 4 mg ha 1 while the periods of comparison differ they were selected as the simulated cultivars and management technologies are better aligned with modern production practices which evolve over time sacks and kucharik 2011 and would be expected to influence harvestable yield more than water consumption or streamflow which in rain fed cropping systems can be quite temporally stable in the face of changing agricultural practices because different rainfed cropping systems as well as fallow fields may use all available soil water during the growing season hamilton et al 2018 mean simulated dry matter soybean yields were 3 5 3 7 and 2 8 mg ha 1 for the original slug and richards methods respectively compared to a mean reported usda nass yield from 2007 to 2016 in hardin county iowa of 3 0 mg ha 1 mean simulated dry matter switchgrass yields were 11 5 11 9 and 10 2 mg ha 1 for the original slug and richards methods respectively whereas switchgrass yields in similar regions in north central iowa have been reported in the 12 0 12 3 mg ha 1 range gassman et al 2017 trybula et al 2015 hence simulated yields in the watershed were comparable to expected yields in the region simulated yields for all crops were consistently highest with the slug method and lowest with the richards method the latter difference largely driven by lower available soil water in the root zone due to increased simulated percolation utilizing the richards method compared to the other methods simulation of the grain crop to switchgrass conversion scenarios revealed that while the three soil water models performed comparably for the historical streamflow evaluation marked differences between the submodels manifested under land use conversion fig 5 the simulated reductions in streamflow as a result of cropping system conversion to switchgrass was consistently highest with the original model and simulated reductions with both the original and slug models were much higher than with the richards model under the most extreme scenario of complete conversion of agricultural croplands to switchgrass apex simulations utilizing the richards submodel produced a modest 1 0 reduction in streamflow compared to 10 6 and 16 1 reductions simulated utilizing the slug and original submodels respectively streamflow reduction increased monotonically with increasing cropland conversion for the original and slug submodels whereas reductions demonstrated site specific directionality dependence as conversion of some groups of subareas resulted in less streamflow reduction the disparity in simulated streamflow reductions utilizing the richards submodel compared to the original or slug submodels is driven by inverse simulation of the impacts of the land use conversion on percolation and et fig 6 table 3 while all three submodels simulate reduced runoff under greater area in the perennial switchgrass crop the original and slug submodels simulate increased et and reduced percolation whereas the richards submodel simulates reduced et and increased percolation this is partly explained by the underestimation of near surface soil water content by the original model and to a lesser degree by the slug model perennial switchgrass cropping systems result in greater surface vegetative and residue cover than annual cropping systems reducing evaporative losses from the near soil surface hence the underestimation of near surface soil water content in the original and slug models mitigates the impact of the increased surface cover under switchgrass towards reduced et relative to grain cropping this contributed to water consumption in the two cropping systems being more similar throughout the year with richards based simulations than with original or slug based simulations fig 7 the longer growing season of perennials compared to annuals does result in elevated water consumption when perennials are active prior to annual crop establishment and following harvest however while the original and slug submodels simulated elevated water consumption and hence lowered streamflow under perennial conversion across the calendar year the richards submodel predicted that perennial conversion would increase water consumption outside of peak annual growing season but result in reduced water consumption during peak annual growth months in june and july when et rates of corn considerably exceed that of perennials these intra annual water consumption patterns with the richards submodel align more closely with experimental observations than with the original or slug submodels abraha et al 2020 eichelmann et al 2016 the increased percolation under perennial cropping systems simulated utilizing the richards submodel is also supported in the literature parish et al 2019 stenjem et al 2019 as increased soil organic carbon and a deeper and denser rooting structure enable greater soil water infiltration and percolation zaibon et al 2017 the large disparity in simulated outcomes among the soil water models highlights the importance of soil hydraulic modeling methodology for understanding system behavior and informing management or policy here richards based simulations indicate modest water quantity impacts from widespread perennial conversion whereas the impacts simulated utilizing the original or slug submodels would be quite consequential this methodological disparity aligns with uncertainty in the literature surrounding the water quantity impacts of conversion to bioenergy production landscapes where the magnitude and directionality of impacts vary widely robertson et al 2017 the focus here on the implementation of a richards based soil water model in the apex and epic models is particularly relevant as these models comprise a family of related models that also includes the soil and water assessment tool swat a model that has been widely used for assessing the hydrologic impacts of landscape conversions including to bioenergy crop production and can actually ingest apex simulations for integrating smaller agricultural watersheds into larger scale mixed land cover studies the richards based methodology is planned for implementation into the swat model replacing the default soil water model that has heretofore been used in swat past studies using the default soil water model may have reached different conclusions with the richards based methodology for instance a swat modeling study in the skunk creek watershed of south dakota reported streamflow reductions of 19 following conversion of agricultural lands which comprised 64 of the land area to perennial grassland ahiablame et al 2019 similarly another swat modeling study in the skeleton creek watershed of oklahoma reported a 27 7 reduction in streamflow following conversion of grasslands which comprised 35 of the land area to switchgrass yimam et al 2017 while these impacts on streamflow would be quite concerning those studies implemented the default swat soil water model analogous to the original soil water model in apex and epic which may prove similarly incapable of reflecting the differential soil water dynamics under annual and perennial cropping nevertheless it is important to note that those studies were conducted in regions of drier climate and hence greater potential soil water limitation than the present study the findings presented here indicating minimal impact on annual streamflow from grain crop to switchgrass conversion when utilizing the richards soil water model align well with other observational studies in humid climates of the upper midwest for instance eddy covariance and soil moisture monitoring studies in southwestern michigan have indicated comparable annual et rates under annual corn and perennial switchgrass or mixed species grassland cropping systems abraha et al 2020 hamilton et al 2015 similarly a study in southwestern ontario reported eddy covariance estimates of annual et that were lower under switchgrass than under corn eichelmann et al 2016 in a more comprehensive assessment of surface subsurface and lateral fluxes an assessment of streamflows in the augusta creek watershed in southwest michigan reported stable streamflows across a 50 year period despite abandonment of 27 of the land area from row cropping to perennial vegetation and 20 of the land area from row cropping to deciduous forest hamilton et al 2018 not surprisingly however context matters and observational studies in the literature also indicate the reverse for instance a soil water monitoring study in east central illinois at a location with a high water table reported et increases of 104 mm and drainage water reduction of 32 following conversion of corn soybean rotation to switchgrass mcisaac et al 2010 zeri et al 2013 biophysical phenomena can be highly spatially variable as a lysimeter study in southern wisconsin reported between a 38 reduction and 78 increase in water drainage relative to a corn treatment under various perennial bioenergy treatments which the authors attributed to the interaction of root development with macropore flow parish et al 2019 more broadly it should be noted that while widespread landscape conversion to perennial bioenergy production may have modest impacts on annual water consumption in some ecosystems the scenarios presented here are intended to demonstrate methodological improvements rather than to influence policy for bioenergy production a plethora of environmental and socioeconomic factors needs to be considered to inform viable strategies for perennial cellulosic biomass production robertson et al 2017 tactical approaches such as cultivation on marginal lands or in vegetative buffer strips are generally seen as more viable options to realize meaningful environmental benefits while avoiding displacement of food production on croplands while further research is needed to better understand the hydrological effects of different cropping systems and implications of potential landscape conversion to perennial bioenergy crop production implementation and utilization of more refined hydrologic models with capabilities to account for macropore flow such as the model implemented here will be beneficial for clearer scientific understanding and better informing policies and decision makers improved modeling techniques might alter response dimensions from indicating more extreme changes towards more modest alterations for instance a swat based simulation study in the little vermilion river watershed in east central illinois utilized drainmod based tile drainage parameterization and reported only small reductions in streamflow upon conversion of row crops to switchgrass and other perennials with the most severe scenario resulting in only a 0 76 streamflow reduction guo et al 2018 similarly a simulation study in the mississippi atchafalaya river basin utilizing the agro ibis dynamic global vegetation model and terrestrial hydrology model with biogeochemistry indicated stream discharge reductions of less than 1 5 under the most extreme conversion scenario of row crops to switchgrass or miscanthus vanloocke et al 2017 hence care must be taken to ensure models are capable of reflecting the scenarios they are being used to assess otherwise such simulation exercises may obfuscate understanding in the literature when the state of knowledge derived from empirical observations is clearer 4 conclusions the updated richards based soil water model was implemented into the apex and epic terrestrial ecosystem models evaluation of the richards model compared with two existing soil water models indicated that while all could suitably simulate streamflow under traditional land use the richards model was better able to reflect observed soil water dynamics particularly under switchgrass cropping systems independent application of the apex model utilizing the three soil water submodels to understand the implications of potential conversion of traditional agriculture towards bioenergy production revealed widely different implications for streamflow the richards based estimate indicated only slight 1 reductions in streamflow under widespread cropland conversion whereas the original and slug based simulations estimated considerable 10 16 streamflow reductions experimental and observational studies in similar climates tend to support more modest estimates of hydrological impacts of land conversion from grain crops to cellulosic bioenergy crops although contradictory findings have been observed these findings highlight the importance of modeling methodologies for enabling meaningful understanding of complex systems software availability name of software apex and epic developers curtis jones cujo umd edu jimmy williams retired jaehak jeong jjeong brc tamus edu hardware required pc software required windows or linux availability available under gnu license upon request from https epicapex tamu edu cost free program language fortran first available 1980 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the us doe office of science de fc02 07er64494 to the great lakes bioenergy research center the usda nifa 2019 68012 29888 and texas agrilife research texas a m university r c i appendices table a1 parameters selected for calibration of the apex model table a1 parameter definition original richards slug rft0 groundwater residence time x x x rfp0 return flow deep percolation x x x iwtb duration of antecedent period for rainfall and pet to drive water table qg channel capacity flow rate qcf exponent in watershed area flow rate equation x chs0 average upland slope bwd channel botom width depth x fcw floodplain width channel width x fps0 floodplain saturated hydraulic conductivity gws0 maximum groundwater residence time x x x parm1 canopy pet factor x parm2 root growth soil strength parm5 soil water lower limit x x parm12 soil evaporation coefficient x parm16 cn retention parameter parm17 soil evaporation plant cover factor x x x parm20 runoff cn initial abstraction x x x parm38 water stress weighting coefficient parm40 groundwater storage threshold x x x parm42 scs cn index coefficient parm49 maximum rainfall interception by plant canopy mm x x x parm50 rainfall interception coefficient x x x parm51 water stored in litter residue x x parm82 permeability parameter x x parm87 water table recession coefficient parm88 daily water table movement limit x parm89 water table recession exponent parm90 subsurface flow factor u g1 soil lower limit for corn plot x x x fc g1 soil field capacity for corn plot x x x u g5 soil lower limit for switchgrass plot x x x fc g5 soil field capacity for switchgrass plot x x x fig a1 method of morris parameter importance for the three soil water models fig a1 
