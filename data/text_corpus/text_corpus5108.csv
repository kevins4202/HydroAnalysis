index,text
25540,land use land cover lulc change is widely recognised as one of the most important factors impacting the hydrological response of river basins swat the latest version of the soil and water assessment tool has been used extensively to assess the hydrological impacts of lulc change however the process of making and assessing such changes in swat is often cumbersome and non intuitive thereby reducing its usability amongst a wider pool of applied users we address this issue by developing a user friendly toolkit land use change swat toolkit lucst that will 1 allow the end user to define various lulc change scenarios in their study catchment 2 run the swat model with the specified lulc changes and 3 enable interactive visualisation of the different swat output variables a good system usability score 79 8 and positive feedback from end users promises the potential for adopting lucst in future lulc change studies video abstract https youtu be qygbidyr4cq keywords swat land use change hydrological modelling open source javascript package flooding visual analytics data availability data will be made available on request 1 introduction land use land cover lulc change is recognised as one of the most important factors impacting overland runoff the hydrological response of river basins and fluvial flooding defries and eshleman 2004 sajikumar and remya 2015 consequently many studies have linked flooding directly to lulc change e g apollonio et al 2016 zope et al 2016 and posthumus et al 2008 in the cervaro basin italy the oshiwara basin india and ure basin england respectively anthropogenic lulc changes have the capacity to propagate the adverse impacts of climate change on the hydrological response of catchments marhaento et al 2018 van roosmalen et al 2009 however properly implemented catchment management has the potential to be a useful impact mitigation measure against climate change burby and french 1981 branca et al 2013 parker 2014 it is therefore imperative that catchments are managed carefully to help mitigate and not exacerbate the impacts of climate change the first step towards good catchment management as recognised by the united nations sendai framework for disaster risk reduction 2015 2039 is to understand the disaster risk at hand unisdr 2018 the first principles of hydrological modelling were introduced by mulvaney 1851 and were used in the rational formula of kuichling 1889 in what could be one of the very earliest hydrological models montanari 2011 later sherman 1932 proposed the unit hydrograph model a method of hydrological assessment commonly used to this day however since the attempt by crawford and linsley 1966 hydrological modelling has predominantly become an ever more complex digital activity by the end of the 20th century advances in computing power and increased understanding of hydrological processes led to complex process based hydrological models these process based hydrological models are an idealised mathematical representation of a given catchment that calculate numerous physical outputs e g flow sediment water quality lulc change impact assessments can be made using these models by altering the land use input and comparing the outputs to that of other scenarios topography based hydrological model topmodel beven et al 1995 2021 scanlon et al 2000 gao et al 2017 the european hydrology system mike she refshaard and storm 1995 graham and butts 2005 rujner et al 2018 and the soil and water assessment tool swat arnold et al 1998 2012 are just a few examples of the most popular process based models swat is a deterministic semi distributed process based hydrological model it is described as a multi scale time continuous catchment model arnold et al 1998 and over the last 30 years it has become one of the most widely used hydrological models in the world wu et al 2020 bieger et al 2017 originally developed to quantify the impact of land management practices in small to large multi complexity catchments fao 2021 swat is considered one of the most suitable models for predicting the impact of lulc on catchment processes arnold and fohrer 2005 gassman et al 2007 ullrich and volk 2009 consequently swat is a useful catchment management tool gassman et al 2007 ahn and kim 2019 the model is used by several governmental organisations particularly in the united states such as the us department of agriculture s conservation effects assessment project ceap wu et al 2020 scavia et al 2017 white et al 2014 and the us environmental protection agency yen et al 2016 fu et al 2019 added the names of 42 catchment models or modelling platforms to the search terms on scopus and found that between 2003 and 2018 swat accounted for 44 of papers published proving the model s popularity despite swat s many merits there has been a growing understanding of its shortcomings in recent years with many experts highlighting pollutant routeing and a lack of flexibility in reference to catchment configuration as major limitations arnold and fohrer 2005 gassman et al 2007 krysanova and arnold 2008 swat a revised version of swat was developed to address such issues improve code maintenance and foster the development and integration of new tools into the model by external researchers bieger et al 2017 released in 2018 swat already has a strong pool of users and is gaining popularity over previous versions of swat whilst maintaining the same core algorithms and same input data swat was designed to streamline and simplify the modelling process whilst offering the user more flexibility regarding the spatial representation of interactions and processes bieger et al 2017 when assessing the impact of lulc change in swat a method often referred to as the fixing changing method yan et al 2013 woldesenbet et al 2017 awotwi et al 2019 shukla and gedam 2019 is employed also referred to as the one factor at a time analysis zhang et al 2020 or delta approach shukla and gedam 2019 this method involves changing a single input or factor in this case land use whilst all other inputs remain fixed thereby isolating the cause of the impact s to a single factor i e land use there are two main ways of implementing the fixing changing method when studying the impact of land use change using the swat model the first and seemingly the most widely used is to produce a new lulc map for each scenario which essentially re delineates the hrus within the catchment lee et al 2008 subedi et al 2013 cec√≠lio et al 2019 tavangar et al 2021 the second is to alter the swat input files adding new land uses and plant communities to the model as well as changing the hrus land use values in hru data hru but maintaining the original spatial set up of the hrus fig 1 mwangi 2016 ahiablame et al 2019 each method comes with its own inherent challenges and both can be time consuming and non intuitive in previous versions of swat mid simulation land use changes could be made via swat s luc module swat landuse update tool swat lut an interface developed by moriasi et al 2019 interacts with the luc module to facilitate mid simulation land use changes in swat decision tables dtls can be used by expert users in a similar way arnold et al 2018 however as far as we are aware at the time of writing there are no plug ins or programs that specifically aid in the construction of lulc change scenarios and the analysis of their impact in swat visual analytics have transformed how we process and understand data in several domains as indicated by keim et al 2008 visual analytics aim to make the processing of data more transparent for analytic discourse through visual representations of the said data visual analytics have been used in various domains such as urban planning karduni et al 2017 coastal monitoring george et al 2014 and earth system climate change sensitivity analysis steed et al 2013 to name a few with success in environmental and scientific information analysis thomas and cook 2006 the potential benefits of visual analytics in hydrological modelling can be utilised to advance of catchment management swatonline has already successfully utilised visual analytics to enhance understanding of the response of catchments to climate change mcdonald et al 2019 this paper introduces the land use change swat toolkit lucst a web based and open source visual analytics application to streamline lulc change assessment in swat modelled catchments the array of fluxes including flow sediment nh3 no3 temperature etc simulated by swat means that the model can be used to assess not only flooding impacts but also hydrological balance water quality and a range of other catchment processes here we have discussed the utility of lucst primarily as a flood risk management support tool specifically with an example of a small catchment in wales however its research use cases are not merely confined to this application and have the potential to aid in wider catchment management 2 methods 2 1 lucst programming lucst is a web based application written in html css and javascript a list of the javascript libraries used in the application can be found in table 1 currently to use lucst the application files are downloaded from github and a locally hosted server makes them accessible through a web browser a python server handles the front end functions whereas an express server handles the back end functions node js is used as an application programming interface api between the web browser and swat by using node js the constraints of the browser can be bypassed to allow lucst to interact with swat files lucst makes lulc changes by interacting with a number of swat input and output files from the txtinout directory table 2 the files that lucst interacts with can be categorised into two types passive and interactive passive files are read but their contents are never altered whereas the interactive files are both read and their contents modified to interpret and edit the txtinout files using javascript they are uploaded to the server and converted to a javascript object notation json file lucst uses functions from the javascript library d3 js to parse and convert both csv and tsv formatted files to json format text files outputted by swat are not always in a consistent format in these cases javascript s regex regular expression is used to format the text files to tsv before parsing with d3 js for passive files the converted data is read from the json files and displayed in various ways in the browser for interactive files those where edits are made and saved by a specific action e g clicking save all in the land use change table the newly edited json file is sent to the server where it is converted back to tsv or csv format then using the node js api the file is written to disk in the current scenarios txtinout directory under its correct name e g landuse lum plant ini and replaces the previous version 2 2 user interface 2 2 1 scenario management the swat scenario directory created during catchment set up contains all the user defined scenarios i e all variations of a modelled catchment default is the first scenario created to represent the actual catchment conditions for the modelled period the default scenario must remain unchanged to provide a baseline for meaningful comparison lucst ensures the preservation of the default scenario in 2 ways the first is through control settings on the interface that disable any active toolkit features when default is selected the second is through coded in safety checks that ensure the default scenario is not selected when any interactive interface feature is activated e g selecting a land use and clicking save all in the land use change table by clicking the create new scenario button a new scenario is written to the scenarios directory which other than its unique name is an exact copy of the default scenario each scenario in the scenarios directory is added to the scenario tab on the lucst interface when a scenario is selected the directory s name is passed to all javascript functions as the directory path for lucst to both read from and write to both passive and interactive functions are enabled when the selected scenario is any other than the default thereby allowing lulc changes to be made 2 2 2 spatial selection map and land use change table leaflet js an open source javascript library for map rendering is used to generate the map and its various components the channels hrus and sub basins shapefiles from the swat catchments watershed file are converted to geojson format using the leaflet js plugins leaflet shpfile js and shp js the geojson objects are added as layers to the leaflet js map the original shapefile attributes except for landuse are encoded as properties of the geojson objects and map layers the individual lu mgt land use values from the scenarios hru data hru file are assigned to the toolkits hru layers as land use property s the leaflet js plugin leaflet lasso js is used by lucst for hru selection leaflet lasso js allows a spatial selection to be made by clicking and dragging the mouse over the desired map area layers with the hrus property i e only hru layers that fall into the lassoed area are assigned to an array list of selected hrus each hru has the property hrus which corresponds to the id value from hru data hru therefore the map layer s hrus values are used to identify the selected hrus and their land uses by their id value in the hru data hru file the land use change table is generated and populated with the array of selected hru id values and their lu mgt values a drop down list of all the name values from swat s landuse lum allows a new land use to be selected the land use of individual hrus or all selected hrus can be changed at once when the change is saved by clicking save or save all lucst loops over each data point in hru data hru and changes the lu mgt of the selected hrus to the new user selected land use the new hru data hru file automatically replaces the old one to update the selected scenario 2 2 3 new land use and plant community forms the lucst interface with an input form allows new land uses to be written to the landuse lum file each input field in the form corresponds to a landuse lum parameter for swat to run correctly no parameter can be left valueless so the new land use will not be saved unless all input fields are populated with a value table 3 gives a list of all new land use type input fields and descriptions each parameter in landuse lum is defined in a connecting file the available values from connecting files are presented as drop down lists in the corresponding input fields swat land uses can include either a plant community or an urban and urban runoff value so when one is selected in the form the other is automatically set to null an accepted swat parameter value some parameters whose connecting files are not usually generated automatically by swat are automatically set to null although they can be defined manually by an expert user the new land use is automatically assigned a name based on the selected plant community or urban land use with the suffix lum adding a new plant community is generally the first step to generate a new land use similar to landuse lum an input form in lucst is used to create a new plant community which must be written in the plant ini file all swat pre defined plant types are available to choose from in a drop down list all other input fields are automatically set to the typical swat plant community values but can be altered if a more specific plant community is required if the landcover status lc status is set to n meaning the plant hasn t grown at the start of the model the initial leaf area index lai ini is automatically set to 0 similarly if the landcover status is y yes the initial leaf area index cannot be set to 0 the plant community name value is automatically set as the value for plt name with the added comm suffix as per swat standards table 4 gives a list and description of the new plant community form input fields similar to land use no input field can be left valueless at its current stage of development the toolkit does not provide the capability to add multiple plant types to a community in consideration of this limitation the input field for plt cnt plant count was not added to the interface instead the plt cnt value is automatically set as 1 as there can only be one plant type in the new plant community 2 2 4 model run and result visualisation lucst runs swat rev60 5 2 64rel exe executable which we have made available to download on github with the toolkit when the run swat button is clicked the exe file is activated and runs swat for the selected scenario once the model run is complete the outputs are automatically uploaded for visualisation both the plots that can be seen in the user interface are generated using vega lite js an open source javascript library for data manipulation and visualisation data for the time series plot is read from the swat output channel sd day csv channel data at a daily timestep in the current version of lucst channel data can only be visualised at a daily timestep the output headers from channel sd day csv are available to choose from a drop down list in the plot time series box as well as a list of the catchment channels a json object is created from the values of the selected output of the selected channel per day along with their corresponding dates the json object is updated when the output or channel options are changed the data from this json object is plotted on a time series plot as well as the corresponding data from the default scenario for output comparison digital images of both plots can be downloaded alternatively the raw data from the current time series plot can be downloaded automatically titled as a csv file geojson data is required by vega lite js to render geographical areas the shp js plugin is used to convert the hrus shapefile from the swat watershed directory into a geojson object this geojson object is the choropleth plot s primary data source thereby enabling hru rendering spatial data plotted on the choropleth is read from the swat output file hru wb mon csv hru water balance data at a monthly timestep in the plot choropleth box all hru wb mon headers are available to choose from a drop down list as well as each month that the simulation has been run the values of chosen output for the chosen month are added to a json object along with the corresponding hru numbers the hru numbers from the json object are matched to the hru numbers on the choropleth plot and the data are plotted 2 2 5 tooltips some elements on the lucst interface need further description to be clearly understood especially for less technical users tooltips were used to provide these descriptions without taking up window space descriptions of these elements were either taken from the swat input documentation or the description column found in some of the swat input files 2 3 system usability scale sus and usability survey the system usability scale sus is a method developed for low cost and reliable assessments of the usability of systems and applications brooke 1996 the usability of a system must be viewed and measured in terms of how appropriate it is for the task for which it is used however usability is a subjective concept and the components which fall under its umbrella in the context of system evaluation are hard to define quantitatively these components are effectiveness the ability of users to complete tasks using the system and the quality of the output of those tasks efficiency the level of resource consumed in performing tasks and satisfaction users subjective reactions to using the system sus is an instrument designed to respond to these issues and assess the usability of a system with a simple but carefully designed set of 10 standard questions answered using an agree disagree likert scale for this study similar to the method used by bangor et al 2009 the original word system was substituted for application in every question to make the questions more answerable for the participants individual sessions were conducted with 12 possible lucst end users a mix of hydrologists environmental scientists postgraduates and various relevant graduate degree holders during the sessions the participants were asked to use lucst to make a new plant community and land use type to implement a land use change in a catchment and then run the model analyse the plots and download the results when the experience was still fresh in their mind the participants were asked to fill out the sus questionnaire the results of which were compiled once all sessions were complete for a more holistic evaluation of the application commensurate with the sus methodology participants were asked to answer two open ended questions which gave them an opportunity to provide textual feedback on specific likes dislikes a specific feature analysis section was also included where the participants rated each feature overall on a scale of 0 4 finally the participants were asked to rate their familiarity with the swat model 1 being not familiar and 5 being very familiar to investigate how this affected their answers in other areas of the study 3 results the lucst graphical user interface gui fig 2 is designed to allow for an intuitive flow between each step of lulc change in the swat model the interface allows for spatial selection and interactive editing of the hru land uses the swat model can also be run within lucst and the visual analytics are incorporated to allow for easy scenario comparison this section will explore in details how the individual lucst gui features are used and what purpose they serve 3 1 scenario management a new scenario is created by clicking the create new scenario button in the top right corner of the toolkit window this opens a pop up menu with an automatically generated scenario name e g scenario 1 that can be customised if desired the new scenario is displayed alongside all other available scenarios in the scenario tab when a scenario is selected its name turns green see fig 2 and that scenario s data is displayed all the changes that are then made only affect the selected scenario 3 2 spatial selection map and land use change table three choices of a background map are available in lucst satellite streets and terrain thus providing quick access to multiple methods of spatial identification three swat map layers are also available the hrus and channels layers are essential for identification of hrus and of the catchment drainage network a sub basins layer although not essential as its properties are never used by the application is included to aid in the identification of separate drainage areas within a catchment all layers can be turned off and on depending on the needs of the study however the hrus layer must be visible when making a selection landscape unit lsu shapefiles were excluded from the interface to reduce the interface complexity however this layer may be added in subsequent lucst versions the map automatically centres to the coordinates of hru 1 this ensures that no matter the geographical location of the study catchment it will appear in the map window when lucst loads each layer s properties are accessible by clicking on the individual layer the most relevant of which are channel hrus and landuse when a land use change is made the hru layers landuse property is updated clicking the lasso icon in the map window turns the cursor icon into a crosshair indicating that the lasso tool is activated the tool is used by clicking and dragging the cursor over an area of the map two selection options are available contain and intersect the contain method selects all hrus which fall within the bounds of the lassoed area whereas the intersect method only selects those hrus where boundaries have been intersected by the cursor path the intersect method is generally better suited for following a specific geographic feature e g channel road whereas the contain method is better suited for making large selections e g entire sub basins selected hrus become red and then revert to their original colour when a new selection is made spatial selection and by extension land use change is confined to the hrus once a map selection is made a land use change table is generated that displays the selected hrus and their land use values the land uses are displayed in the swat format to minimise operational disconnect between swat and lucst the entire selection can be cleared by pressing the clear button although when a new selection is made the table is automatically re populated if a different scenario is selected then the table is removed from the window until a new selection is made a drop down list of all currently available land uses in the modelled catchment populates the tables new land use column a bulk lulc change can be made to all selected hrus by choosing a land use in the top row and clicking the save all button alternatively land uses of individual hrus within the selection can be changed by choosing a land use in their row and clicking save values in the current landuse column are updated when a change is made and an alert stating that a new hru data hru file has been written indicates that the change has been saved the change will not be saved if no land use has been selected 3 3 new land use and plant community forms forms were decided upon as the most efficient and user friendly way of gathering and handling the new plant community and land use parameters generally speaking land uses in swat are centred around a plant community therefore in order to make a new land use a new plant community needs to be written although there are some exceptions to this rule the input forms are opened by clicking on their corresponding button and are closed by clicking on the map for both forms clicking make writes the new item and clicking reset clears the form the plant community name is automatically set as the chosen plant name with the suffix comm as per swat nomenclature for plant communities although this can be changed if required a standard value automatically populates each new plant community input field to help simplify the process for non technical users without specific plant community parameters in mind however it is advisable that some research is done for more accurate simulations the land use name is automatically set to the plant name from the plant community form with the suffix lum as per swat nomenclature for land use types where applicable each input in the land use form has a drop down list of the available parameter values both new land use and new plant community forms employ several checks described in detail in section 2 2 3 to ensure that all swat formatting and rules are adhered to 3 4 model run and result visualisation clicking the run swat for scenario button updated with the current scenario name initiates a swat run for the selected scenario this process is indicated by a loading spinner in the visualisation window once the model has run its outputs are displayed in the visualisation window which is designed to provide a quick and easy assessment of the impact of land use scenarios on catchment processes having been designed primarily as a flood assessment tool when flo out is loaded flow out in m3 s for the main channel is automatically plotted on the time series plot in the plot time series control box any output variable from channel sd day can be selected from a drop down list so that it can be plotted for any of the modelled channels specific channel numbers can be identified by clicking the map layers the plot title indicates which data is currently plotted two datasets are plotted on the time series the selected data for the default scenario orange dotted line and the current scenario blue solid line this gives an instant indication of the impact of the land use change on channel output the plot can be downloaded as a svg or png file or alternatively the plotted data can be downloaded as a raw csv file for further interrogation by clicking the download csv button to complement the time series plot a spatial choropleth plot is generated displaying hru wb mon data hru water balance on a monthly time step precipitation for the first month of the model is displayed by default on the plot in the plot choropleth control box any of the months of the model can be chosen and like the time series plot any of the hru wb mon output variables can be chosen from the output drop down list the chosen output and month are displayed as the plot s title the raw choropleth data is not downloadable because the data per hru would bear little relevance without a spatial representation of the catchment for reference however a svg or png file of the plot can be downloaded 3 5 tooltips a defining and often overwhelming feature of swat is the volume of files and consequent parameters and parameter values that are needed to properly implement land use changes the approach taken to reduce the need for in depth knowledge of these aspects of swat is the incorporation of a commonly used gui feature known as the tooltip a tooltip is a short description displayed on screen when the cursor is hovered over an interface element tooltips were added to lucst elements where a deeper knowledge of a parameter parameter value or connection file could be beneficial to the task at hand these elements include the available new land use values in the land use change table each input field and each item in the drop down lists in the new plant community and new land use type forms 3 6 system usability scale sus survey bangor et al 2009 compiled and compared over 2000 sus surveys in over 200 studies for a range of user interface types and concluded that a mean score of around 72 constitutes good and around 85 excellent from a total of 12 participants lucst achieved an average sus score of 79 8 table 5 placing it firmly in the good range of usability furthermore bangor et al 2009 found first products to score a mean of around 63 well below the first release of lucst it was hypothesised that those users who had previous experience using swat and the challenges involved in making lulc changes would rate lucst higher than non swat users reinforcing this assumption participant 3 who was most familiar with swat gave a sus score of 95 unfortunately a lack of swat literate participants meant that no meaningful comparison could be made between swat users and non users for example participant 5 who considered themselves to be not familiar with the swat model also gave an overall score of 95 however the results from table 5 clearly suggest that non swat users did find the application to be useable participants of the study were also asked two open ended questions describe one positive feature about this application and describe one negative feature about this application these questions were designed to gain a deeper understanding of what features helped and hindered lucst s usability table 6 displays the positive comments the interface s simplicity and user friendliness were a recurring observation commented on by nine out of twelve participants 2 3 5 7 8 9 10 11 and 12 comments from four participants 4 7 10 and 12 suggest that lucst can improve accessibility of swat for non technical users participant 1 mentioned that lucst had the potential for use in their own project despite being not familiar with swat further suggesting its increased accessibility for non technical users two participants 3 5 positively commented on the result visualisation table 7 displays the negative comments that eight out of twelve participants provided for this section out of the participants who responded five 2 5 6 7 and 12 suggested negative features relating to the complexity of swat itself as opposed to the lucst user interface two of these participants 5 and 6 directly mention the technicality of the swat language and specific habitat assumed to mean land use and plant communities naming only participant 9 negatively commented on the interface table 8 displays the results of the usability study s feature analysis section the participants ranked each feature on a scale of 0 4 giving a maximum possible score of 48 12 x 4 the score as a percentage of the maximum was calculated the land use change table scored the highest with 83 hru selection and output visualisation came joint second with 81 the mechanism for adding new plant communities and land use types scored lowest with 79 which suggests that it was perceived to be the least impressive feature 4 discussion 4 1 integration into the swat environment the restructuring of swat to swat was in part done to foster and encourage new innovations by external researchers bieger et al 2017 there are currently various swat add ons and supporting applications developed by different user groups available publicly some examples include swat toolbox chawanda 2022 ipeat yen et al 2019 swat aw chawanda et al 2020 and swat2lake molina navarro et al 2018 although these add ons and applications all help with various tasks they all have the common goal of making the running of swat and its associated tasks simpler yen et al 2016 2019 this is also the fundamental goal of lucst in relation to lulc change assessment lucst will add to the growing arsenal of swat add ons and can either be incorporated as a key component of swat studies or as an additional investigation tool with little need for prior planning by accepting a calibrated catchment as its input data lucst slots perfectly behind the calibration tool swat toolbox in the swat workflow 4 2 improvements on current methods by feature 4 2 1 scenario management lucst removes the need for manual management of scenario folders through this application safety checks are in place to ensure that the integrity of the default scenario is maintained in contrast when making changes through the swat editor or manually in the swat text files there are no such barriers lucst s scenarios tab enables a frictionless transition between the scenarios whereas current methods require the user to traverse multiple platforms folders and files with the system applied by lucst at the click of a button all data relevant to lulc change impact assessment is displayed in one window for the chosen scenario this reduces both the necessary knowledge of swat files and time it takes to make multi scenario lulc change impact assessments by guaranteeing no changes are made to the default scenario lucst ensures that a lulc change study will yield meaningful results 4 2 2 hru selection and land use change the hru shape files produced during catchment set up in qswat contain only the attributes of the default scenario lucst improves on this by updating map layers with land use attributes from the current scenario the updating of layer attributes displays changes within the catchment and removes the need for interpretation of hru data hru in conjunction with qgis additionally lucst provides background map options to aid in spatial referencing during the hru selection process whereas in qgis background maps require importing the method of hru selection in qgis and the one adopted by lucst are very similar in both selections are made by using the cursor to define an area of the catchment to select the hrus within it however the contain or intersect methods offered by lucst are tailored to suit specific selection requirements depending on the needs of the study hru selection scored 81 in the feature analysis section of the usability study suggesting that it is well received by possible end users the land use change table scored highest 83 in the feature analysis section of the usability study automatically populating the land use change table from a selection is no different from qgis automatically populating an attribute table from selected layers however in qgis this is where the automation ends the hru ids then need to be identified from the attribute table and compiled into a list which can be referred to when manually editing the hru data hru file available land uses need to be identified from landuse lum to ensure only viable land use changes are made adding to the workload and data management requirements lucst completely bypasses all these manual interactions providing all relevant data within one interface while in previous methods it is possible that hundreds of lu mgt values would need to be manually changed individually lucst achieves the same result with the selection of a land use code from the drop down list and the click of a button save all or save 4 2 3 new land use and plant types within lucst it is hard to mitigate for the number of technical parameters that make up both plant communities and land uses this quantity and technicality of parameters is an inherent characteristic of complex process based hydrological models yang et al 2000 devia et al 2015 in general a larger number of parameters although adding to the complexity improve the mathematical representation of the catchment yang et al 2000 devia et al 2015 although the input forms do nothing to reduce the number of parameters needed they help organise with the aid of tools and pointers the building of a land use or plant community in an intuitive way unfortunately the nature of swat means that at least some technical understanding of the model is needed to generate new plant communities and land uses to accurately represent their real life counterparts this complexity was reflected in the feedback from the usability study where the forms scored lowest of all the features 79 the largest group of negative comments referred to the complexity of the swat model and its nomenclature as the most negative feature of the application lucst automatically locates each relevant connecting file and extracts all parameter value names thereby reducing the need for interaction within the txtinout directory the names are presented in drop down lists in the input forms and where applicable the list items are anchored with their swat descriptions as tooltips all input fields are labelled with the full parameter name as opposed to their swat code e g manning s n instead of ov mann and also anchored with their swat description and connecting file name additionally checks ensuring all swat formats and rules are adhered to that are employed by lucst section 2 2 3 do not exist when manually writing in the swat text files 4 2 4 output visualisation clear visualisation is key to communicating results effectively van wijk 2005 and was therefore a key element in lucst s development the large quantity of data outputted by swat files and output parameters for each catchment channel and hru can be overwhelming and make manual extraction of the correct data a time consuming and cumbersome task the workload is then multiplied when it is necessary to plot the results of multiple scenarios for output comparison lucst locates both the channel file channel sd day and hru water balance file hru wb mon automatically and then filters and plots the chosen data instantaneously the channel data is plotted alongside the default scenario for instant result comparison a time series plot was chosen as one of the simplest plots for human interpretation of temporal data dunn 2019 the ready sorted channel data can also be downloaded for deeper interrogation if required having the result visualisation incorporated within the lucst interface in one window means that specific channel names can be identified from the desired catchment location by clicking on the map layer and selected for plotting in previous methods the easiest way of associating a channel with a spatial area was through qgis attribute tables lucst also incorporates visualisation of the spatial hru water balance data to complement the channel output data this enables easy association of channel output events to the catchment water balance e g high peak flows with high precipitation 4 3 improvements on current methods as a whole although each feature provides its own improvements to the individual stages of lulc change in swat improvements to the process as a whole need to be considered to fully understand the benefits of lucst at first glance the two significant improvements that lucst makes are to increase the speed and reduce the complexity of making lulc change impact assessments these two elements are non mutually exclusive and feed into one another to help improve the overall accessibility of the swat model as a lulc change impact assessment tool fig 3 that is the primary accomplishment of lucst 4 3 1 reduced complexity task complexity negatively impacts performance and behaviour which is something that needs to be seriously considered in system design liu and li 2012 furthermore complexity is assumed to influence mental workload jacko and ward 1996 thereby affecting performance valdeza et al 2015 by providing checks and helpful features as well as automating specific tasks in one user friendly interface lucst reduces the complexity of manual lulc change impact assessments 4 3 1 1 reduced need for technical knowledge the need for technical knowledge is a major barrier for new users when it comes to assessing the impact of lulc change in swat lucst removes some of this complexity firstly by automatically locating and uploading files essential to the task at hand e g channel sd day for result visualisation and secondly by reducing the need for knowledge of inter file connections the application does this by locating and providing all parameter options in drop down lists where applicable tooltips provide deeper descriptions of complex elements as and when they are needed as discussed later in this section the removal of cross platform interaction also reduces the need for technical knowledge of multiple programmes 4 3 1 2 reduced human error manually implementing changes to a swat catchment allows room for human error e g changing the land use of the wrong hru using parameter codes not available in the catchment or by writing data in the wrong format as workloads increase repetition can lead to tedium confusion and increased chances of human errors reason 1990 whilst it is true that an ill informed land use change or new plant type can be made through the lucst interface the data is always written in the correct swat format to the exact user specification if an undesired change is made the automation introduced by the application means that these changes are easily reversable 4 3 1 3 reduced need for cross platform interaction currently all methods of lulc change assessment using swat involve the integration and use of multiple platforms qgis qswat multiple text files along with input output swat documentation swat editor and a csv reader like microsoft excel this results in the need for trained technically competent swat users with a good understanding of these platforms and how they interact to implement an lulc change study even for technically competent users swapping between programmes text files and platforms can become cumbersome tedious and lead to confusion and mistakes the inherent complexities of swat further run the risk of alienating non technical users lucst provides all the steps of lulc change assessment on a single platform each step of the process after initial catchment set up and calibration can be implemented through the interface not only does this add to the user experience it along with the automation introduced by each feature also vastly reduces the time needed to study lulc change scenarios the toolkit provides an array of features all compiled in one application thereby reducing the level of expertise and data management needed to make fast and accurate assessments of the impact of lulc change on catchment processes 4 3 2 increased process speed in addition to the complexity the time it takes to make lulc change impact assessments in swat is another limiting factor for hydrological studies and can lead to the neglect of certain land use scenarios by automating specific tasks which otherwise would be done manually lucst vastly reduces the time it takes to conduct lulc studies using the swat model thereby increasing the potential for initial and further investigation as mentioned in section 4 3 1 a major time saver generated by lucst is the compilation of all lulc change features within a single application making complex changes to the model manually often involves swapping between files and programmes several times to check parameters hrus channels help documentations etc with the possibility of significantly increasing the length of time taken to make a study lucst provides all the files parameters descriptions and features in one application the primary time saving feature of lucst is the automation of file writing and output uploading automation of previously manual tasks allows lulc change scenarios to be constructed and their outputs compared in a relatively short period of time this enables the construction and comparison of more scenarios per study as well as the possibility of adding lulc change assessments to studies where previously the option would have been dismissed due to time constraints 4 3 3 improved accessibility lucst increases the speed and simplicity of making lulc change impact assessments in swat which in turn increases the model s accessibility the results of the preliminary usability study backed up this observation suggesting an improved accessibility for non swat users with lucst for the first time swat can be used for on the go lulc change studies without the need for meticulous planning data management or extensive technical knowledge to this end lucst will not only help make the model more accessible to non swat users but also allow technical users to conduct studies in a much more time effective and convenient manner than previous methods 4 4 facilitating hydrological investigations since the rise in environmental awareness at the end of the last century the need for a good understanding of the impacts of land use change on the hydrological response of catchments has become increasingly apparent grayson and bl√∂schl 2001 swat as an internationally recognised hydrological model that stands up to scientific scrutiny arnold and fohrer 2005 gassman et al 2007 ullrich and volk 2009 is the perfect tool to provide quantifiable evidence of the hydrological impact of different land uses lucst output accuracy depends on the swat model the accuracy of the input data at catchment set up and the accuracy of model calibration gassman et al 2014 all of which are independent of the toolkit s design as a result use of the application does not directly achieve greater model accuracy however lucst helps fill a gap in the research as it provides a platform for more research to be conducted at greater speed by increasing model accessibility to a wider range of user groups the likelihood of proper hydrological assessments being made will increase and likely have a positive knock on effect on catchment management 5 conclusion two main factors influence the hydrological response of catchments climate and land use both climate change and land use change are impacting catchment processes globally with no short term possibility of reversing the anthropogenic impact on the atmosphere and climate land must be managed appropriately to mitigate the effects of contemporary climate conditions one of the biggest obstacles to successful land management is a lack of scientific understanding and quantifiable evidence of its effects therefore it is imperative that local hydrological responses and the impact the lulc change will have on those responses are properly understood to help make informed catchment management decisions to improve understanding of local and regional catchment processes decision makers need access to tools which produce quantifiable and scientifically rigorous results in this paper lucst has been introduced as one of the only tools of its type that can streamline lulc change assessment in swat by utilising swat as its hydrological modelling engine it was possible to piggyback on the well laid foundations of swat in terms of its user group workflow and wealth of scientifically accredited studies although the list of possible additional functions is long the toolkit has met the studies original aim of making lulc change assessment easier in its current stage of development the improvements lucst has introduced to lulc change impact assessment in swat have shown that it has great promise as a tool to aid in catchment management software availability software name lucst land use change swat toolkit developers alexander rigby rigbya96 gmail com peter butcher p butcher bangor ac uk year of first release 2021 hardware requirements pc software requirements windows 10 npm 8 1 2 nodejs 14 15 4 python 3 9 7 swat rev60 5 2 64rel exe program language javascript css html program size 31 3 mb availability https github com alexrigby lucst git open source documentation full step by step installation setup and interface instructions are available in the documentation folder within the github package lucst runs swat rev60 5 2 64rel exe therefore some features may not work as expected with catchments modelled using other versions of swat declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research was funded by kess 2 in partnership with ymgynghoriaeth gwynedd consultancy ygc ygc is the consultancy branch of gwynedd county council wales knowledge economy skills scholarships kess 2 is a pan wales higher level skills initiative led by bangor university on behalf of the he sector in wales it is part funded by the welsh government s european social fund esf convergence programme for west wales and the valleys 
25540,land use land cover lulc change is widely recognised as one of the most important factors impacting the hydrological response of river basins swat the latest version of the soil and water assessment tool has been used extensively to assess the hydrological impacts of lulc change however the process of making and assessing such changes in swat is often cumbersome and non intuitive thereby reducing its usability amongst a wider pool of applied users we address this issue by developing a user friendly toolkit land use change swat toolkit lucst that will 1 allow the end user to define various lulc change scenarios in their study catchment 2 run the swat model with the specified lulc changes and 3 enable interactive visualisation of the different swat output variables a good system usability score 79 8 and positive feedback from end users promises the potential for adopting lucst in future lulc change studies video abstract https youtu be qygbidyr4cq keywords swat land use change hydrological modelling open source javascript package flooding visual analytics data availability data will be made available on request 1 introduction land use land cover lulc change is recognised as one of the most important factors impacting overland runoff the hydrological response of river basins and fluvial flooding defries and eshleman 2004 sajikumar and remya 2015 consequently many studies have linked flooding directly to lulc change e g apollonio et al 2016 zope et al 2016 and posthumus et al 2008 in the cervaro basin italy the oshiwara basin india and ure basin england respectively anthropogenic lulc changes have the capacity to propagate the adverse impacts of climate change on the hydrological response of catchments marhaento et al 2018 van roosmalen et al 2009 however properly implemented catchment management has the potential to be a useful impact mitigation measure against climate change burby and french 1981 branca et al 2013 parker 2014 it is therefore imperative that catchments are managed carefully to help mitigate and not exacerbate the impacts of climate change the first step towards good catchment management as recognised by the united nations sendai framework for disaster risk reduction 2015 2039 is to understand the disaster risk at hand unisdr 2018 the first principles of hydrological modelling were introduced by mulvaney 1851 and were used in the rational formula of kuichling 1889 in what could be one of the very earliest hydrological models montanari 2011 later sherman 1932 proposed the unit hydrograph model a method of hydrological assessment commonly used to this day however since the attempt by crawford and linsley 1966 hydrological modelling has predominantly become an ever more complex digital activity by the end of the 20th century advances in computing power and increased understanding of hydrological processes led to complex process based hydrological models these process based hydrological models are an idealised mathematical representation of a given catchment that calculate numerous physical outputs e g flow sediment water quality lulc change impact assessments can be made using these models by altering the land use input and comparing the outputs to that of other scenarios topography based hydrological model topmodel beven et al 1995 2021 scanlon et al 2000 gao et al 2017 the european hydrology system mike she refshaard and storm 1995 graham and butts 2005 rujner et al 2018 and the soil and water assessment tool swat arnold et al 1998 2012 are just a few examples of the most popular process based models swat is a deterministic semi distributed process based hydrological model it is described as a multi scale time continuous catchment model arnold et al 1998 and over the last 30 years it has become one of the most widely used hydrological models in the world wu et al 2020 bieger et al 2017 originally developed to quantify the impact of land management practices in small to large multi complexity catchments fao 2021 swat is considered one of the most suitable models for predicting the impact of lulc on catchment processes arnold and fohrer 2005 gassman et al 2007 ullrich and volk 2009 consequently swat is a useful catchment management tool gassman et al 2007 ahn and kim 2019 the model is used by several governmental organisations particularly in the united states such as the us department of agriculture s conservation effects assessment project ceap wu et al 2020 scavia et al 2017 white et al 2014 and the us environmental protection agency yen et al 2016 fu et al 2019 added the names of 42 catchment models or modelling platforms to the search terms on scopus and found that between 2003 and 2018 swat accounted for 44 of papers published proving the model s popularity despite swat s many merits there has been a growing understanding of its shortcomings in recent years with many experts highlighting pollutant routeing and a lack of flexibility in reference to catchment configuration as major limitations arnold and fohrer 2005 gassman et al 2007 krysanova and arnold 2008 swat a revised version of swat was developed to address such issues improve code maintenance and foster the development and integration of new tools into the model by external researchers bieger et al 2017 released in 2018 swat already has a strong pool of users and is gaining popularity over previous versions of swat whilst maintaining the same core algorithms and same input data swat was designed to streamline and simplify the modelling process whilst offering the user more flexibility regarding the spatial representation of interactions and processes bieger et al 2017 when assessing the impact of lulc change in swat a method often referred to as the fixing changing method yan et al 2013 woldesenbet et al 2017 awotwi et al 2019 shukla and gedam 2019 is employed also referred to as the one factor at a time analysis zhang et al 2020 or delta approach shukla and gedam 2019 this method involves changing a single input or factor in this case land use whilst all other inputs remain fixed thereby isolating the cause of the impact s to a single factor i e land use there are two main ways of implementing the fixing changing method when studying the impact of land use change using the swat model the first and seemingly the most widely used is to produce a new lulc map for each scenario which essentially re delineates the hrus within the catchment lee et al 2008 subedi et al 2013 cec√≠lio et al 2019 tavangar et al 2021 the second is to alter the swat input files adding new land uses and plant communities to the model as well as changing the hrus land use values in hru data hru but maintaining the original spatial set up of the hrus fig 1 mwangi 2016 ahiablame et al 2019 each method comes with its own inherent challenges and both can be time consuming and non intuitive in previous versions of swat mid simulation land use changes could be made via swat s luc module swat landuse update tool swat lut an interface developed by moriasi et al 2019 interacts with the luc module to facilitate mid simulation land use changes in swat decision tables dtls can be used by expert users in a similar way arnold et al 2018 however as far as we are aware at the time of writing there are no plug ins or programs that specifically aid in the construction of lulc change scenarios and the analysis of their impact in swat visual analytics have transformed how we process and understand data in several domains as indicated by keim et al 2008 visual analytics aim to make the processing of data more transparent for analytic discourse through visual representations of the said data visual analytics have been used in various domains such as urban planning karduni et al 2017 coastal monitoring george et al 2014 and earth system climate change sensitivity analysis steed et al 2013 to name a few with success in environmental and scientific information analysis thomas and cook 2006 the potential benefits of visual analytics in hydrological modelling can be utilised to advance of catchment management swatonline has already successfully utilised visual analytics to enhance understanding of the response of catchments to climate change mcdonald et al 2019 this paper introduces the land use change swat toolkit lucst a web based and open source visual analytics application to streamline lulc change assessment in swat modelled catchments the array of fluxes including flow sediment nh3 no3 temperature etc simulated by swat means that the model can be used to assess not only flooding impacts but also hydrological balance water quality and a range of other catchment processes here we have discussed the utility of lucst primarily as a flood risk management support tool specifically with an example of a small catchment in wales however its research use cases are not merely confined to this application and have the potential to aid in wider catchment management 2 methods 2 1 lucst programming lucst is a web based application written in html css and javascript a list of the javascript libraries used in the application can be found in table 1 currently to use lucst the application files are downloaded from github and a locally hosted server makes them accessible through a web browser a python server handles the front end functions whereas an express server handles the back end functions node js is used as an application programming interface api between the web browser and swat by using node js the constraints of the browser can be bypassed to allow lucst to interact with swat files lucst makes lulc changes by interacting with a number of swat input and output files from the txtinout directory table 2 the files that lucst interacts with can be categorised into two types passive and interactive passive files are read but their contents are never altered whereas the interactive files are both read and their contents modified to interpret and edit the txtinout files using javascript they are uploaded to the server and converted to a javascript object notation json file lucst uses functions from the javascript library d3 js to parse and convert both csv and tsv formatted files to json format text files outputted by swat are not always in a consistent format in these cases javascript s regex regular expression is used to format the text files to tsv before parsing with d3 js for passive files the converted data is read from the json files and displayed in various ways in the browser for interactive files those where edits are made and saved by a specific action e g clicking save all in the land use change table the newly edited json file is sent to the server where it is converted back to tsv or csv format then using the node js api the file is written to disk in the current scenarios txtinout directory under its correct name e g landuse lum plant ini and replaces the previous version 2 2 user interface 2 2 1 scenario management the swat scenario directory created during catchment set up contains all the user defined scenarios i e all variations of a modelled catchment default is the first scenario created to represent the actual catchment conditions for the modelled period the default scenario must remain unchanged to provide a baseline for meaningful comparison lucst ensures the preservation of the default scenario in 2 ways the first is through control settings on the interface that disable any active toolkit features when default is selected the second is through coded in safety checks that ensure the default scenario is not selected when any interactive interface feature is activated e g selecting a land use and clicking save all in the land use change table by clicking the create new scenario button a new scenario is written to the scenarios directory which other than its unique name is an exact copy of the default scenario each scenario in the scenarios directory is added to the scenario tab on the lucst interface when a scenario is selected the directory s name is passed to all javascript functions as the directory path for lucst to both read from and write to both passive and interactive functions are enabled when the selected scenario is any other than the default thereby allowing lulc changes to be made 2 2 2 spatial selection map and land use change table leaflet js an open source javascript library for map rendering is used to generate the map and its various components the channels hrus and sub basins shapefiles from the swat catchments watershed file are converted to geojson format using the leaflet js plugins leaflet shpfile js and shp js the geojson objects are added as layers to the leaflet js map the original shapefile attributes except for landuse are encoded as properties of the geojson objects and map layers the individual lu mgt land use values from the scenarios hru data hru file are assigned to the toolkits hru layers as land use property s the leaflet js plugin leaflet lasso js is used by lucst for hru selection leaflet lasso js allows a spatial selection to be made by clicking and dragging the mouse over the desired map area layers with the hrus property i e only hru layers that fall into the lassoed area are assigned to an array list of selected hrus each hru has the property hrus which corresponds to the id value from hru data hru therefore the map layer s hrus values are used to identify the selected hrus and their land uses by their id value in the hru data hru file the land use change table is generated and populated with the array of selected hru id values and their lu mgt values a drop down list of all the name values from swat s landuse lum allows a new land use to be selected the land use of individual hrus or all selected hrus can be changed at once when the change is saved by clicking save or save all lucst loops over each data point in hru data hru and changes the lu mgt of the selected hrus to the new user selected land use the new hru data hru file automatically replaces the old one to update the selected scenario 2 2 3 new land use and plant community forms the lucst interface with an input form allows new land uses to be written to the landuse lum file each input field in the form corresponds to a landuse lum parameter for swat to run correctly no parameter can be left valueless so the new land use will not be saved unless all input fields are populated with a value table 3 gives a list of all new land use type input fields and descriptions each parameter in landuse lum is defined in a connecting file the available values from connecting files are presented as drop down lists in the corresponding input fields swat land uses can include either a plant community or an urban and urban runoff value so when one is selected in the form the other is automatically set to null an accepted swat parameter value some parameters whose connecting files are not usually generated automatically by swat are automatically set to null although they can be defined manually by an expert user the new land use is automatically assigned a name based on the selected plant community or urban land use with the suffix lum adding a new plant community is generally the first step to generate a new land use similar to landuse lum an input form in lucst is used to create a new plant community which must be written in the plant ini file all swat pre defined plant types are available to choose from in a drop down list all other input fields are automatically set to the typical swat plant community values but can be altered if a more specific plant community is required if the landcover status lc status is set to n meaning the plant hasn t grown at the start of the model the initial leaf area index lai ini is automatically set to 0 similarly if the landcover status is y yes the initial leaf area index cannot be set to 0 the plant community name value is automatically set as the value for plt name with the added comm suffix as per swat standards table 4 gives a list and description of the new plant community form input fields similar to land use no input field can be left valueless at its current stage of development the toolkit does not provide the capability to add multiple plant types to a community in consideration of this limitation the input field for plt cnt plant count was not added to the interface instead the plt cnt value is automatically set as 1 as there can only be one plant type in the new plant community 2 2 4 model run and result visualisation lucst runs swat rev60 5 2 64rel exe executable which we have made available to download on github with the toolkit when the run swat button is clicked the exe file is activated and runs swat for the selected scenario once the model run is complete the outputs are automatically uploaded for visualisation both the plots that can be seen in the user interface are generated using vega lite js an open source javascript library for data manipulation and visualisation data for the time series plot is read from the swat output channel sd day csv channel data at a daily timestep in the current version of lucst channel data can only be visualised at a daily timestep the output headers from channel sd day csv are available to choose from a drop down list in the plot time series box as well as a list of the catchment channels a json object is created from the values of the selected output of the selected channel per day along with their corresponding dates the json object is updated when the output or channel options are changed the data from this json object is plotted on a time series plot as well as the corresponding data from the default scenario for output comparison digital images of both plots can be downloaded alternatively the raw data from the current time series plot can be downloaded automatically titled as a csv file geojson data is required by vega lite js to render geographical areas the shp js plugin is used to convert the hrus shapefile from the swat watershed directory into a geojson object this geojson object is the choropleth plot s primary data source thereby enabling hru rendering spatial data plotted on the choropleth is read from the swat output file hru wb mon csv hru water balance data at a monthly timestep in the plot choropleth box all hru wb mon headers are available to choose from a drop down list as well as each month that the simulation has been run the values of chosen output for the chosen month are added to a json object along with the corresponding hru numbers the hru numbers from the json object are matched to the hru numbers on the choropleth plot and the data are plotted 2 2 5 tooltips some elements on the lucst interface need further description to be clearly understood especially for less technical users tooltips were used to provide these descriptions without taking up window space descriptions of these elements were either taken from the swat input documentation or the description column found in some of the swat input files 2 3 system usability scale sus and usability survey the system usability scale sus is a method developed for low cost and reliable assessments of the usability of systems and applications brooke 1996 the usability of a system must be viewed and measured in terms of how appropriate it is for the task for which it is used however usability is a subjective concept and the components which fall under its umbrella in the context of system evaluation are hard to define quantitatively these components are effectiveness the ability of users to complete tasks using the system and the quality of the output of those tasks efficiency the level of resource consumed in performing tasks and satisfaction users subjective reactions to using the system sus is an instrument designed to respond to these issues and assess the usability of a system with a simple but carefully designed set of 10 standard questions answered using an agree disagree likert scale for this study similar to the method used by bangor et al 2009 the original word system was substituted for application in every question to make the questions more answerable for the participants individual sessions were conducted with 12 possible lucst end users a mix of hydrologists environmental scientists postgraduates and various relevant graduate degree holders during the sessions the participants were asked to use lucst to make a new plant community and land use type to implement a land use change in a catchment and then run the model analyse the plots and download the results when the experience was still fresh in their mind the participants were asked to fill out the sus questionnaire the results of which were compiled once all sessions were complete for a more holistic evaluation of the application commensurate with the sus methodology participants were asked to answer two open ended questions which gave them an opportunity to provide textual feedback on specific likes dislikes a specific feature analysis section was also included where the participants rated each feature overall on a scale of 0 4 finally the participants were asked to rate their familiarity with the swat model 1 being not familiar and 5 being very familiar to investigate how this affected their answers in other areas of the study 3 results the lucst graphical user interface gui fig 2 is designed to allow for an intuitive flow between each step of lulc change in the swat model the interface allows for spatial selection and interactive editing of the hru land uses the swat model can also be run within lucst and the visual analytics are incorporated to allow for easy scenario comparison this section will explore in details how the individual lucst gui features are used and what purpose they serve 3 1 scenario management a new scenario is created by clicking the create new scenario button in the top right corner of the toolkit window this opens a pop up menu with an automatically generated scenario name e g scenario 1 that can be customised if desired the new scenario is displayed alongside all other available scenarios in the scenario tab when a scenario is selected its name turns green see fig 2 and that scenario s data is displayed all the changes that are then made only affect the selected scenario 3 2 spatial selection map and land use change table three choices of a background map are available in lucst satellite streets and terrain thus providing quick access to multiple methods of spatial identification three swat map layers are also available the hrus and channels layers are essential for identification of hrus and of the catchment drainage network a sub basins layer although not essential as its properties are never used by the application is included to aid in the identification of separate drainage areas within a catchment all layers can be turned off and on depending on the needs of the study however the hrus layer must be visible when making a selection landscape unit lsu shapefiles were excluded from the interface to reduce the interface complexity however this layer may be added in subsequent lucst versions the map automatically centres to the coordinates of hru 1 this ensures that no matter the geographical location of the study catchment it will appear in the map window when lucst loads each layer s properties are accessible by clicking on the individual layer the most relevant of which are channel hrus and landuse when a land use change is made the hru layers landuse property is updated clicking the lasso icon in the map window turns the cursor icon into a crosshair indicating that the lasso tool is activated the tool is used by clicking and dragging the cursor over an area of the map two selection options are available contain and intersect the contain method selects all hrus which fall within the bounds of the lassoed area whereas the intersect method only selects those hrus where boundaries have been intersected by the cursor path the intersect method is generally better suited for following a specific geographic feature e g channel road whereas the contain method is better suited for making large selections e g entire sub basins selected hrus become red and then revert to their original colour when a new selection is made spatial selection and by extension land use change is confined to the hrus once a map selection is made a land use change table is generated that displays the selected hrus and their land use values the land uses are displayed in the swat format to minimise operational disconnect between swat and lucst the entire selection can be cleared by pressing the clear button although when a new selection is made the table is automatically re populated if a different scenario is selected then the table is removed from the window until a new selection is made a drop down list of all currently available land uses in the modelled catchment populates the tables new land use column a bulk lulc change can be made to all selected hrus by choosing a land use in the top row and clicking the save all button alternatively land uses of individual hrus within the selection can be changed by choosing a land use in their row and clicking save values in the current landuse column are updated when a change is made and an alert stating that a new hru data hru file has been written indicates that the change has been saved the change will not be saved if no land use has been selected 3 3 new land use and plant community forms forms were decided upon as the most efficient and user friendly way of gathering and handling the new plant community and land use parameters generally speaking land uses in swat are centred around a plant community therefore in order to make a new land use a new plant community needs to be written although there are some exceptions to this rule the input forms are opened by clicking on their corresponding button and are closed by clicking on the map for both forms clicking make writes the new item and clicking reset clears the form the plant community name is automatically set as the chosen plant name with the suffix comm as per swat nomenclature for plant communities although this can be changed if required a standard value automatically populates each new plant community input field to help simplify the process for non technical users without specific plant community parameters in mind however it is advisable that some research is done for more accurate simulations the land use name is automatically set to the plant name from the plant community form with the suffix lum as per swat nomenclature for land use types where applicable each input in the land use form has a drop down list of the available parameter values both new land use and new plant community forms employ several checks described in detail in section 2 2 3 to ensure that all swat formatting and rules are adhered to 3 4 model run and result visualisation clicking the run swat for scenario button updated with the current scenario name initiates a swat run for the selected scenario this process is indicated by a loading spinner in the visualisation window once the model has run its outputs are displayed in the visualisation window which is designed to provide a quick and easy assessment of the impact of land use scenarios on catchment processes having been designed primarily as a flood assessment tool when flo out is loaded flow out in m3 s for the main channel is automatically plotted on the time series plot in the plot time series control box any output variable from channel sd day can be selected from a drop down list so that it can be plotted for any of the modelled channels specific channel numbers can be identified by clicking the map layers the plot title indicates which data is currently plotted two datasets are plotted on the time series the selected data for the default scenario orange dotted line and the current scenario blue solid line this gives an instant indication of the impact of the land use change on channel output the plot can be downloaded as a svg or png file or alternatively the plotted data can be downloaded as a raw csv file for further interrogation by clicking the download csv button to complement the time series plot a spatial choropleth plot is generated displaying hru wb mon data hru water balance on a monthly time step precipitation for the first month of the model is displayed by default on the plot in the plot choropleth control box any of the months of the model can be chosen and like the time series plot any of the hru wb mon output variables can be chosen from the output drop down list the chosen output and month are displayed as the plot s title the raw choropleth data is not downloadable because the data per hru would bear little relevance without a spatial representation of the catchment for reference however a svg or png file of the plot can be downloaded 3 5 tooltips a defining and often overwhelming feature of swat is the volume of files and consequent parameters and parameter values that are needed to properly implement land use changes the approach taken to reduce the need for in depth knowledge of these aspects of swat is the incorporation of a commonly used gui feature known as the tooltip a tooltip is a short description displayed on screen when the cursor is hovered over an interface element tooltips were added to lucst elements where a deeper knowledge of a parameter parameter value or connection file could be beneficial to the task at hand these elements include the available new land use values in the land use change table each input field and each item in the drop down lists in the new plant community and new land use type forms 3 6 system usability scale sus survey bangor et al 2009 compiled and compared over 2000 sus surveys in over 200 studies for a range of user interface types and concluded that a mean score of around 72 constitutes good and around 85 excellent from a total of 12 participants lucst achieved an average sus score of 79 8 table 5 placing it firmly in the good range of usability furthermore bangor et al 2009 found first products to score a mean of around 63 well below the first release of lucst it was hypothesised that those users who had previous experience using swat and the challenges involved in making lulc changes would rate lucst higher than non swat users reinforcing this assumption participant 3 who was most familiar with swat gave a sus score of 95 unfortunately a lack of swat literate participants meant that no meaningful comparison could be made between swat users and non users for example participant 5 who considered themselves to be not familiar with the swat model also gave an overall score of 95 however the results from table 5 clearly suggest that non swat users did find the application to be useable participants of the study were also asked two open ended questions describe one positive feature about this application and describe one negative feature about this application these questions were designed to gain a deeper understanding of what features helped and hindered lucst s usability table 6 displays the positive comments the interface s simplicity and user friendliness were a recurring observation commented on by nine out of twelve participants 2 3 5 7 8 9 10 11 and 12 comments from four participants 4 7 10 and 12 suggest that lucst can improve accessibility of swat for non technical users participant 1 mentioned that lucst had the potential for use in their own project despite being not familiar with swat further suggesting its increased accessibility for non technical users two participants 3 5 positively commented on the result visualisation table 7 displays the negative comments that eight out of twelve participants provided for this section out of the participants who responded five 2 5 6 7 and 12 suggested negative features relating to the complexity of swat itself as opposed to the lucst user interface two of these participants 5 and 6 directly mention the technicality of the swat language and specific habitat assumed to mean land use and plant communities naming only participant 9 negatively commented on the interface table 8 displays the results of the usability study s feature analysis section the participants ranked each feature on a scale of 0 4 giving a maximum possible score of 48 12 x 4 the score as a percentage of the maximum was calculated the land use change table scored the highest with 83 hru selection and output visualisation came joint second with 81 the mechanism for adding new plant communities and land use types scored lowest with 79 which suggests that it was perceived to be the least impressive feature 4 discussion 4 1 integration into the swat environment the restructuring of swat to swat was in part done to foster and encourage new innovations by external researchers bieger et al 2017 there are currently various swat add ons and supporting applications developed by different user groups available publicly some examples include swat toolbox chawanda 2022 ipeat yen et al 2019 swat aw chawanda et al 2020 and swat2lake molina navarro et al 2018 although these add ons and applications all help with various tasks they all have the common goal of making the running of swat and its associated tasks simpler yen et al 2016 2019 this is also the fundamental goal of lucst in relation to lulc change assessment lucst will add to the growing arsenal of swat add ons and can either be incorporated as a key component of swat studies or as an additional investigation tool with little need for prior planning by accepting a calibrated catchment as its input data lucst slots perfectly behind the calibration tool swat toolbox in the swat workflow 4 2 improvements on current methods by feature 4 2 1 scenario management lucst removes the need for manual management of scenario folders through this application safety checks are in place to ensure that the integrity of the default scenario is maintained in contrast when making changes through the swat editor or manually in the swat text files there are no such barriers lucst s scenarios tab enables a frictionless transition between the scenarios whereas current methods require the user to traverse multiple platforms folders and files with the system applied by lucst at the click of a button all data relevant to lulc change impact assessment is displayed in one window for the chosen scenario this reduces both the necessary knowledge of swat files and time it takes to make multi scenario lulc change impact assessments by guaranteeing no changes are made to the default scenario lucst ensures that a lulc change study will yield meaningful results 4 2 2 hru selection and land use change the hru shape files produced during catchment set up in qswat contain only the attributes of the default scenario lucst improves on this by updating map layers with land use attributes from the current scenario the updating of layer attributes displays changes within the catchment and removes the need for interpretation of hru data hru in conjunction with qgis additionally lucst provides background map options to aid in spatial referencing during the hru selection process whereas in qgis background maps require importing the method of hru selection in qgis and the one adopted by lucst are very similar in both selections are made by using the cursor to define an area of the catchment to select the hrus within it however the contain or intersect methods offered by lucst are tailored to suit specific selection requirements depending on the needs of the study hru selection scored 81 in the feature analysis section of the usability study suggesting that it is well received by possible end users the land use change table scored highest 83 in the feature analysis section of the usability study automatically populating the land use change table from a selection is no different from qgis automatically populating an attribute table from selected layers however in qgis this is where the automation ends the hru ids then need to be identified from the attribute table and compiled into a list which can be referred to when manually editing the hru data hru file available land uses need to be identified from landuse lum to ensure only viable land use changes are made adding to the workload and data management requirements lucst completely bypasses all these manual interactions providing all relevant data within one interface while in previous methods it is possible that hundreds of lu mgt values would need to be manually changed individually lucst achieves the same result with the selection of a land use code from the drop down list and the click of a button save all or save 4 2 3 new land use and plant types within lucst it is hard to mitigate for the number of technical parameters that make up both plant communities and land uses this quantity and technicality of parameters is an inherent characteristic of complex process based hydrological models yang et al 2000 devia et al 2015 in general a larger number of parameters although adding to the complexity improve the mathematical representation of the catchment yang et al 2000 devia et al 2015 although the input forms do nothing to reduce the number of parameters needed they help organise with the aid of tools and pointers the building of a land use or plant community in an intuitive way unfortunately the nature of swat means that at least some technical understanding of the model is needed to generate new plant communities and land uses to accurately represent their real life counterparts this complexity was reflected in the feedback from the usability study where the forms scored lowest of all the features 79 the largest group of negative comments referred to the complexity of the swat model and its nomenclature as the most negative feature of the application lucst automatically locates each relevant connecting file and extracts all parameter value names thereby reducing the need for interaction within the txtinout directory the names are presented in drop down lists in the input forms and where applicable the list items are anchored with their swat descriptions as tooltips all input fields are labelled with the full parameter name as opposed to their swat code e g manning s n instead of ov mann and also anchored with their swat description and connecting file name additionally checks ensuring all swat formats and rules are adhered to that are employed by lucst section 2 2 3 do not exist when manually writing in the swat text files 4 2 4 output visualisation clear visualisation is key to communicating results effectively van wijk 2005 and was therefore a key element in lucst s development the large quantity of data outputted by swat files and output parameters for each catchment channel and hru can be overwhelming and make manual extraction of the correct data a time consuming and cumbersome task the workload is then multiplied when it is necessary to plot the results of multiple scenarios for output comparison lucst locates both the channel file channel sd day and hru water balance file hru wb mon automatically and then filters and plots the chosen data instantaneously the channel data is plotted alongside the default scenario for instant result comparison a time series plot was chosen as one of the simplest plots for human interpretation of temporal data dunn 2019 the ready sorted channel data can also be downloaded for deeper interrogation if required having the result visualisation incorporated within the lucst interface in one window means that specific channel names can be identified from the desired catchment location by clicking on the map layer and selected for plotting in previous methods the easiest way of associating a channel with a spatial area was through qgis attribute tables lucst also incorporates visualisation of the spatial hru water balance data to complement the channel output data this enables easy association of channel output events to the catchment water balance e g high peak flows with high precipitation 4 3 improvements on current methods as a whole although each feature provides its own improvements to the individual stages of lulc change in swat improvements to the process as a whole need to be considered to fully understand the benefits of lucst at first glance the two significant improvements that lucst makes are to increase the speed and reduce the complexity of making lulc change impact assessments these two elements are non mutually exclusive and feed into one another to help improve the overall accessibility of the swat model as a lulc change impact assessment tool fig 3 that is the primary accomplishment of lucst 4 3 1 reduced complexity task complexity negatively impacts performance and behaviour which is something that needs to be seriously considered in system design liu and li 2012 furthermore complexity is assumed to influence mental workload jacko and ward 1996 thereby affecting performance valdeza et al 2015 by providing checks and helpful features as well as automating specific tasks in one user friendly interface lucst reduces the complexity of manual lulc change impact assessments 4 3 1 1 reduced need for technical knowledge the need for technical knowledge is a major barrier for new users when it comes to assessing the impact of lulc change in swat lucst removes some of this complexity firstly by automatically locating and uploading files essential to the task at hand e g channel sd day for result visualisation and secondly by reducing the need for knowledge of inter file connections the application does this by locating and providing all parameter options in drop down lists where applicable tooltips provide deeper descriptions of complex elements as and when they are needed as discussed later in this section the removal of cross platform interaction also reduces the need for technical knowledge of multiple programmes 4 3 1 2 reduced human error manually implementing changes to a swat catchment allows room for human error e g changing the land use of the wrong hru using parameter codes not available in the catchment or by writing data in the wrong format as workloads increase repetition can lead to tedium confusion and increased chances of human errors reason 1990 whilst it is true that an ill informed land use change or new plant type can be made through the lucst interface the data is always written in the correct swat format to the exact user specification if an undesired change is made the automation introduced by the application means that these changes are easily reversable 4 3 1 3 reduced need for cross platform interaction currently all methods of lulc change assessment using swat involve the integration and use of multiple platforms qgis qswat multiple text files along with input output swat documentation swat editor and a csv reader like microsoft excel this results in the need for trained technically competent swat users with a good understanding of these platforms and how they interact to implement an lulc change study even for technically competent users swapping between programmes text files and platforms can become cumbersome tedious and lead to confusion and mistakes the inherent complexities of swat further run the risk of alienating non technical users lucst provides all the steps of lulc change assessment on a single platform each step of the process after initial catchment set up and calibration can be implemented through the interface not only does this add to the user experience it along with the automation introduced by each feature also vastly reduces the time needed to study lulc change scenarios the toolkit provides an array of features all compiled in one application thereby reducing the level of expertise and data management needed to make fast and accurate assessments of the impact of lulc change on catchment processes 4 3 2 increased process speed in addition to the complexity the time it takes to make lulc change impact assessments in swat is another limiting factor for hydrological studies and can lead to the neglect of certain land use scenarios by automating specific tasks which otherwise would be done manually lucst vastly reduces the time it takes to conduct lulc studies using the swat model thereby increasing the potential for initial and further investigation as mentioned in section 4 3 1 a major time saver generated by lucst is the compilation of all lulc change features within a single application making complex changes to the model manually often involves swapping between files and programmes several times to check parameters hrus channels help documentations etc with the possibility of significantly increasing the length of time taken to make a study lucst provides all the files parameters descriptions and features in one application the primary time saving feature of lucst is the automation of file writing and output uploading automation of previously manual tasks allows lulc change scenarios to be constructed and their outputs compared in a relatively short period of time this enables the construction and comparison of more scenarios per study as well as the possibility of adding lulc change assessments to studies where previously the option would have been dismissed due to time constraints 4 3 3 improved accessibility lucst increases the speed and simplicity of making lulc change impact assessments in swat which in turn increases the model s accessibility the results of the preliminary usability study backed up this observation suggesting an improved accessibility for non swat users with lucst for the first time swat can be used for on the go lulc change studies without the need for meticulous planning data management or extensive technical knowledge to this end lucst will not only help make the model more accessible to non swat users but also allow technical users to conduct studies in a much more time effective and convenient manner than previous methods 4 4 facilitating hydrological investigations since the rise in environmental awareness at the end of the last century the need for a good understanding of the impacts of land use change on the hydrological response of catchments has become increasingly apparent grayson and bl√∂schl 2001 swat as an internationally recognised hydrological model that stands up to scientific scrutiny arnold and fohrer 2005 gassman et al 2007 ullrich and volk 2009 is the perfect tool to provide quantifiable evidence of the hydrological impact of different land uses lucst output accuracy depends on the swat model the accuracy of the input data at catchment set up and the accuracy of model calibration gassman et al 2014 all of which are independent of the toolkit s design as a result use of the application does not directly achieve greater model accuracy however lucst helps fill a gap in the research as it provides a platform for more research to be conducted at greater speed by increasing model accessibility to a wider range of user groups the likelihood of proper hydrological assessments being made will increase and likely have a positive knock on effect on catchment management 5 conclusion two main factors influence the hydrological response of catchments climate and land use both climate change and land use change are impacting catchment processes globally with no short term possibility of reversing the anthropogenic impact on the atmosphere and climate land must be managed appropriately to mitigate the effects of contemporary climate conditions one of the biggest obstacles to successful land management is a lack of scientific understanding and quantifiable evidence of its effects therefore it is imperative that local hydrological responses and the impact the lulc change will have on those responses are properly understood to help make informed catchment management decisions to improve understanding of local and regional catchment processes decision makers need access to tools which produce quantifiable and scientifically rigorous results in this paper lucst has been introduced as one of the only tools of its type that can streamline lulc change assessment in swat by utilising swat as its hydrological modelling engine it was possible to piggyback on the well laid foundations of swat in terms of its user group workflow and wealth of scientifically accredited studies although the list of possible additional functions is long the toolkit has met the studies original aim of making lulc change assessment easier in its current stage of development the improvements lucst has introduced to lulc change impact assessment in swat have shown that it has great promise as a tool to aid in catchment management software availability software name lucst land use change swat toolkit developers alexander rigby rigbya96 gmail com peter butcher p butcher bangor ac uk year of first release 2021 hardware requirements pc software requirements windows 10 npm 8 1 2 nodejs 14 15 4 python 3 9 7 swat rev60 5 2 64rel exe program language javascript css html program size 31 3 mb availability https github com alexrigby lucst git open source documentation full step by step installation setup and interface instructions are available in the documentation folder within the github package lucst runs swat rev60 5 2 64rel exe therefore some features may not work as expected with catchments modelled using other versions of swat declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the research was funded by kess 2 in partnership with ymgynghoriaeth gwynedd consultancy ygc ygc is the consultancy branch of gwynedd county council wales knowledge economy skills scholarships kess 2 is a pan wales higher level skills initiative led by bangor university on behalf of the he sector in wales it is part funded by the welsh government s european social fund esf convergence programme for west wales and the valleys 
25541,with the advent of increasingly powerful computational architectures scientists use these possibilities to create simulations of ever increasing size and complexity large scale simulations of environmental systems require huge amounts of resources managing these in an operational way becomes increasingly complex and difficult to handle for individual scientists state of the art simulation infrastructures usually provide the necessary resources in a centralised setup which often results in an all or nothing choice for the user here we outline an alternative approach to handling this complexity while rendering the use of high performance hardware and large datasets still possible it retains a number of desirable properties i a decentralised structure ii easy sharing of resources to promote collaboration and iii secure access to everything including natural delegation of authority across levels and system boundaries we show that the object capability paradigm will cover these issues and present the first steps towards developing a simulation infrastructure based on these principles keywords cap n proto scientific collaboration co development communication protocol object capability 1 introduction 1 1 the environmental modelling context in a rapidly changing world science is increasingly called on to project future developments of our societal systems and the environmental systems in which they are embedded hofman et al 2017 mea 2005 meadows et al 1972 story telling and the development of narratives is a commonly agreed way of outlining desirable or undesirable futures in a qualitative manner while simulations are used to translate the rather fuzzy narratives into a quantitative universe mallampalli et al 2016 sch√∂nenberg et al 2017 there are already numerous feedback loops within environmental and societal systems and interactions between the two systems intensify the complexity a vast number of simulation models have been developed to represent various sub systems at different levels of detail their purposes range from enhancing the understanding of the system to predicting the behaviour of the system when surrounding conditions change hofman et al 2017 van nes and scheffer 2005 simulation models for environmental and societal systems include a range of methodological approaches and disciplinary foci addressing questions of public and scientific concern ihrig 2016 verburg et al 2016 the aim of integrated modelling is to combine individual modelling approaches into larger model frameworks and to utilise the feedback loops across the different models to gain a better understanding and predictability of the systems represented van beek et al 2020 van ittersum et al 2008 however the operation of individual models often requires a certain infrastructure computing facilities software environment access to data etc verweij et al 2010 and experience on the part of the user to produce the desired output confalonieri et al 2016 which is often limited to individual groups of developers semantic and sociological challenges remain but software hardware and data have undergone substantial improvements in recent decades these improvements enable increasingly complex simulations and new modelling approaches also across domains partially driving the development of even more powerful computers as the potential for increasing the speed of single processors has hit the ceiling for now parallel computing on multiple processors is on the rise shalf 2020 at the same time the advancement of sensors sensor platforms and other forms of continuous and large scale data creation has boosted the availability of big data for potential use in developing calibrating or driving simulation models basso and antle 2020 franz et al 2020 hampf et al 2021 weiss et al 2020 in many cases large institutionalised infrastructures are needed to harvest store and serve such data however high performance computing hpc facilities big data repositories and skilled model developers are unevenly distributed across research institutions the integration of different models and the utilisation of computing power and data availability therefore require cross institutional collaboration and a software infrastructure that supports such collaboration altschul et al 2013 ideally this infrastructure should integrate models data and people including model developers and model users drawing on the example of impact modelling in agriculture we present below the vision and rationale for a flexible cooperative simulation infrastructure and the underlying software mechanisms needed to support it 1 2 criteria for a simulation infrastructure no matter how it is done designing and developing a new model and simulation infrastructure is an extensive undertaking in light of past experiences and projects in the environmental modelling field an ideal infrastructure has the following desirable properties 1 flexibility the ideal infrastructure allows for easy integration of diverse hardware pc hpc cloud and software legacy different languages etc and can handle a wide range of heterogeneous models and data 2 efficiency the ideal infrastructure facilitates the fast and efficient execution of both simple and complex simulations and does not represent the bottleneck of the simulation 3 simplicity the simpler the infrastructure the higher the level of adoption an infrastructure must be easy for scientists to use and for developers to improve 4 peripherality to ease collaboration the infrastructure should support the distributed and decentralised use of models and data 5 openness a modern simulation infrastructure should be open to improvement by both scientists and developers involving the collaboration of many different people ideally such collaboration is voluntary unconditional easy and secure today s model and simulation infrastructures tend to be based on a centralised approach where a large storage and computational backend is accompanied by some user facing usually web based frontend that allows access to the system consequently models and data to be used within such an infrastructure must be prepared accordingly to fit into the system the more commonality can be enforced the more benefits can be gained by the easy use of models for new data or new models for existing data what these centralised infrastructures have in common is that they act as silos allowing only the models and data placed inside them to interact or utilise future enhancements of the system this means that existing models and data that are to be included in the infrastructure have to be adapted to the infrastructure and its prevailing rules depending on the properties of the system adjusting to the infrastructure requires a considerable amount of resources and skills while the trade off between costs and benefits can be very positive for large organisations major projects or long term needs it may discourage smaller stakeholders hamper agile experimentation and generally cause problems when models and or data do not fit easily into the concept of an infrastructure similarly it is difficult and expensive to cater for multiple possibly competing infrastructures in addition centralised infrastructures encourage a centralised user and rights management system which makes administrative control easy but external collaboration and participation more difficult in order to mitigate some of the aforementioned problems and to achieve the properties required by the scientific community we propose a different take on simulation infrastructure our goal is to create a potentially large ad hoc decentralised simulation infrastructure comprising networked components such as computing or storage resources all components are treated as equal members of the larger virtual infrastructure these distributed components communicate via a capability based remote procedure protocol rpc according to capability security principles this allows fine grained decentralised security and the easy and safe delegation of authority it removes the need for centralised user management enabling unconditional collaboration at all scales since all parts of the infrastructure are treated as equal there is no distinction between simple and complex models personal laptops and hpcs or small local and large cloud backed datasets what counts at the infrastructure level is communication between the components that make up a simulation it goes without saying that different needs will result in the creation of different setups an organisation s need for high performance large scale simulations will result in parts of the virtual infrastructure models and data being optimised for hpcs on the other hand a scientist might want to hook into the same setup and prototype a model change or experiment using a new locally crafted dataset in both cases users need access to the components representing the resources in order to run a particular simulation the authority to access such resources can be delegated by peers or granted by administrators as is the case in centralised systems decentralisation does not mean the complete abandonment of centralisation instead it seeks to establish multiple centres and potentially lots of them the costs of the virtual infrastructure are then reduced to the cost of self hosted systems and a negotiable part for remotely used systems it is likely that only simulations bound by performance needs will have to run in a purely centralised manner the simulation infrastructure we propose attempts to bridge the extremes local vs hpc by offering sufficient flexibility to fill the space between those extremes 1 3 the transition from centralised to decentralised systems decentralisation requires that all parts communicate since speed and latency are always important an efficient communication protocol a common language and shared communication interfaces both explicitly and externally specified are needed agreeing on a common set of interfaces for the infrastructure s components allows to abstract away some of the heterogeneity of real world systems and renders possible the creation of a virtual infrastructure stable communication interface specifications encourage model creators and data suppliers to implement them for their own resources equipped with sufficient tooling end users may also become model or data providers being able to hook into the infrastructure peripherally security is an even more important issue for collaboration in open distributed systems than it is in closed centralised infrastructures while the latter can employ well established user and rights management mechanisms controlled by an administrator with elevated rights a single point of control is difficult and ultimately not desirable in a decentralised system to allow as much freedom as possible within the infrastructure we selected the capability security paradigm miller et al 2003 as the underlying authority transfer and access mechanism capability security gives the user access to a particular resource and the authority to handle it together both will be defined later as capability without a third party e g an administrator being involved and the opportunity to pass on this capability to others this transfer of capabilities is part of the communication protocol and as such a software entity that can address both users and machines capabilities can be transferred via messages that are otherwise used to execute a simulation model for example and pass on access information and rights in the simulation model example the model can receive access information to a particular data set plus the authority to access it all these factors are easily applicable as long as all the components are connected however clients may go offline and servers and systems may fail or reboot users once authorised may want to retain the capabilities received between work sessions or pass them on to somebody else via infrastructure independent means such as an email to this end capabilities need to persist so that they can be stored offline and used at a later stage to gain access to the resources represented persistent capabilities are called sturdy references erights org 1998a or sturdy refs receiving a sturdy ref enables a user or a software program to connect to the resource represented and interact with that resource according to the capability s communication interface 1 4 choice of a particular technology so far we have made only an abstract reference to the foundational capability secure layer that we believe a decentralised simulation infrastructure needs to develop the desired properties since it is likely that a decentralised system will consist of very different hardware software programs programming languages platforms and frameworks the capability secure communication protocol should be agnostic to languages and platforms there is currently only one real world implementation of a capability secure communication protocol that meets our needs being multi language multi platform offering an external schema description and focusing on speed and efficiency cap n proto capnp 2022 is a fast data interchange format and a capability based rpc protocol it is an implementation of the captp protocol captp 2021 which allows distributed object programming over mutually suspicious networks meant to be combined with an object capability ocap style of programming spritely 2021 in other words it is a distributed object oriented program secured by capability principles capabilities provided to scripts or programs act like a distributed object oriented application programming interface api e g a script or program would start off with one or more sturdy refs persistent capabilities to some part of the infrastructure turning these into live references e g to model instances or datasets allows the program to access the currently available simulation api now that the entry point to the simulation infrastructure is mediated by capabilities software that uses and runs on top of the infrastructure can be secure even among mutually suspicious participants a capability granted to a participant in the infrastructure should therefore only have the authority required for their tasks moreover a capability should never allow unintended privilege escalation which would involve an owner being granted more authority than is needed for a given task this principle of least authority pola saltzer 1974 can be expressed very naturally in object capability systems the capability security principles are able to avoid many pitfalls of contemporary systems that are mainly based on ambient authority using access control lists acls which we describe in more detail in the next section 1 5 the example realm seeking to exemplify the concept of object capabilities in the context of a model and simulation infrastructure we use the case of process based agroecosystem modelling process based agroecosystem models build on a system of differential equations that represent biophysical processes in the soil plant atmosphere continuum and are typically used to simulate the impact of crop properties genotype soil and climate environment and alternative soil and crop management strategies management on a particular target variable or set of target variables these variables often include crop yields water use nutrient losses greenhouse gas emissions and many more besides due to their development legacy most available agroecosystem models are built on a one dimensional concept that does not represent any spatial relationships per se to overcome this limitation such models are often applied as a suite of individual simulations across a grid representing an area involving either sequential model runs or parallel runs on multiple computer cores in this case each grid cell must be supplied with the appropriate input data to drive the model and the simulation result of each grid cell must be collected in order to display the overall simulation result in the spatial context in which it was embedded fig 1 scenario runs may require alternative data and management rules finally the simulation results are often needed as a series of maps visualising the spatial and temporal distribution of the target variable e g crop yields 2 capability security and object capabilities a capability is a communicable unforgeable token of authority an object capability in turn is an unforgeable reference to an object the object s interface defines the authority that can be exercised by sending messages to the object dennis and van horn 1966 in this section we explain the concept of capability security and how it differs from the well established mechanism of access control lists acls and address object capabilities in particular the following examples are purposely reduced and simplified to allow focusing on the concepts in the accompanying github repository code 2022 the complete source code and the description how to run the examples can be found 2 1 access control lists versus capabilities both access control lists and capabilities are used to control access to resources within a computer system this requires a definition of which user or process has access to which resources an acl is a list of users and their access rights to a particular resource translated to a filesystem a control authority e g an administrator specifies which users are allowed to access a file and what are they allowed to do e g read the file in an acl based filesystem a user refers to the resources files by the name of the file the filename which does not by itself entail the authority to access the file instead authorisation is derived from the file s environment this concept is referred to as ambient authority it may be viewed as an automatic door that identifies an authorised person on the basis of prior registration and slides open when such a person approaches in contrast capabilities can be compared to keys equipped with the matching key a person can open the door in this case access authorisation is attached to the key itself and not to the environment a typical way of depicting the problem is to use a matrix showing the users along the vertical axis and the resources along the horizontal axis the matrix in fig 2 states that all users are allowed to read the file at etc passwd but only alice and carol are permitted to write to the file u markm foo whereas bob is denied access to this file acls and capabilities offer two different ways of looking at this matrix in an acl system the file etc motd would have to have a list attached stating that both bob and carol are allowed to read the file in contrast in a capability system bob and carol would need to have a key that allows them to read the file etc motd if the matrix is static both approaches to defining access to a resource are equivalent as soon as the matrix becomes dynamic things differ in the acl approach only the owner or privileged users administrators are usually allowed to change access lists if then alice also needs to read the file etc motd at a later point in time the administrator has to update the acl of the file etc motd in a capability system somebody with read access to the file etc motd has to give alice a copy of the key so that she too can read the file the important difference to note is that with capabilities all authorised users can easily delegate authority to somebody else see also sandstorm 2015 miller et al 2003 discussed this and other issues in detail and traced many of the computer security problems that still exist today back to the use of ambient authority in acl systems which impairs the safe delegation of authority however the dynamic ad hoc and safe sharing of resources is a prerequisite for collaboration the aforementioned door key analogy is easy to grasp but has issues of its own the most obvious one being the revocation of authority once granted in this real world analogy the only way to remove authority would be to replace the whole door lock this is often not realistic or desirable nevertheless many contemporary token based methods of accessing web services belong to this category although keys have their uses as capabilities a different kind of capability referred to as object capabilities solve this issue by offering richer semantics 2 2 object capabilities the concept of object capability proposed by dennis and van horn 1966 parallels the concept of object oriented programming oop in many aspects an object capability is a reference to an object and represents both the authority and the means to access a resource the authority is exercised by sending messages to the referenced object the messages that the object understands make up the authority it represents in software systems an object usually represents some real or abstract resource encapsulated by an interface the messages that the object understands for our purposes objects exist within computer programs on machines connected by networks thus a capability aka a reference to an object might span program and machine boundaries in any case possessing a reference to an object equals the authority to access it according to the object capabilities concept the owner of an object reference is able to share it with somebody else by sending them a message fig 3 granovetter 1973 miller et al 2001 which in our case depicts the relationships of computational objects capabilities in a network and their change over time fig 3 recalls the original communication between alice bob and carol in our case all three actors are objects alice has a reference a capability to bob and is therefore authorised to send the message foo to bob this message includes a reference a capability to carol authorising bob to talk to carol as well 2 2 1 a revocable object capability object capability systems operate under the assumption that the only way to communicate is through object references in other words if bob receives a capability to carol then bob has all the authority that this capability entails one of the common objections against capability systems is that once issued capabilities cannot be revoked in acl systems the operation and authority of revocation is external to the user who is supposed to access the resource for example an administrator can remove a user from the acl of a particular file or change the access rights to the file object capability systems allow new objects to be created that instantaneously forward all messages they receive facilitating the creation of a valid work around fig 4 in this case forwarder f is such a capability that alice sends to bob optionally f could even have a different perhaps limited read instead of read write interface to the one carol offers weakening the original capability to carol this alone would not yet allow alice to control bob s access to carol inserting another forwarder revoker r under alice s control that forwards f s messages to carol solves problem r s interface can include a revoke message which cuts the connection between r and carol effectively revoking bob s access to carol bob can still talk to f and f will still forward all messages to r but the communication line is broken at r as a result alice has revoked bob s authority to talk to carol 2 3 implementations of capability security and object capabilities considerable research has been undertaken on capabilities and different applications have been developed ranging from operating systems eros 1999 keykos 1988 programming languages erights org 1998b and web capabilities waterken 2009 to filesystems tahoe lafs 2021 and a web application platform sandstorm 2021 burtsev et al 2017 presented research on a capability enabled cloud infrastructure to facilitate secure collaboration at the infrastructure level while it is desirable to secure the underlying infrastructures using capabilities and a least authority approach the simulation infrastructure we propose operates at a different level just below the application layer from the perspective of safe collaboration the sandstorm platform is particularly interesting its goal was to create a web based self hostable productivity suite and platform with an app marketplace that allows untrusted applications to be run together in a cluster environment the whole system was built on capability principles to allow secure communication among untrusted apps and users the basis for sandstorm was the cap n proto protocol a decentralised contemporary simulation infrastructure needs to support a variety of systems and languages cap n proto is a data interchange format and capability based rpc protocol which allows disparate systems to communicate via object capabilities with an eye to efficiency and performance implementations of cap n proto protocols are available for many different languages enabling the integration of a wide range of existing systems even though cap n proto is not as widely used as other pure rpc protocols e g grpc 2021 it has proven to be stable and is also used in production systems for instance at cloudflare cloudflare 2021 capability security speed efficiency platform and language agnosticism suggest that cap n proto is a good foundation for a distributed simulation infrastructure 2 4 a simple example of a cap n proto api as a data interchange format the cap n proto protocol specifies the structure of messages being transferred from a sender to a receiver these messages are strongly typed but not self descriptive for this reason cap n proto defines a special schema language that is used to describe the message structure a compiler is then used to create code and data structures in the target language allowing messages to be manipulated cap n proto schemas contain two broad categories of structures data structures and interfaces the data structures represent the layout and type of data being sent interfaces are the entry point to the capability based rpc system and represent runtime objects and the messages these objects may receive messages in cap n proto implementations are always sent asynchronously and like a method call in an object oriented language can return values data structures or other interfaces due to its asynchronous nature the return of a value is depicted in the example below fig 5 as a separate message directing back from the receiver to the sender of the original message conceptually this is the same as sending a message to the receiver with an additional parameter representing the sender once the result is ready the receiver can return it to the sender in the example realm of environmental science fig 4 shows the use of a simple remote climate service to access data of a time series at a particular geo location of the period 2021 2050 we assume that our user possesses a reference a capability to a remote climate service that holds these data when the service is ready it will deliver a time series capability back to our user fig 5a this capability is authorised to send all messages to a time series object that the user understands the simplified interfaces for the climate service and the time series are shown below fig 6 words in bold represent capabilities interfaces words in italics denote data structures or simple types and underlined words depict the message names that an object implementing this interface understands the cap n proto schema language allows the use of many of the structures often found in object oriented programming languages such as inheritance nested structures and unions after receiving the time series capability the user only holds a reference and the potential to acquire the data or query meta information since our user only wants a 30 year data subset he sends the subrange message to the time series capability to receive another time series capability representing that 30 year subrange this process is illustrated in fig 5b in contrast fig 5c shows the user sending the final data message to the just received sub time range the result now is pure data the structure of which has been described in the schema above as list list float32 the documentation of the schema would describe it as a list the days of a list of floating point values the day s data the order of the day s data for instance is defined by the header message available in the timeseries interface a particular point to note is the seemingly inefficient interface that timeseries offers in order to obtain the required time series data the user had to do three full network round trips three requests to the remote interface to optimise this other interfaces fig 7 could be possible e g to do the job a climateservice object could understand only one data message with three parameters revealing the location and time range requested which then requires only one network round trip an obvious problem in the long run could be the proliferation of the messages and message combinations an interface needs to declare that all needs are met e g a version of the retrieved data within a certain time range but with a different set of climate elements the original api design did not have this problem because common object oriented principles could be applied as network round trips albeit seemingly at the cost of latency a more serious problem is that in this case the pola has been violated the second api version has no way to prevent a user from possessing this capability to acquire all data offered by the service an authority we might not want to give the user traditional rpc systems have to work around this problem by adding a further layer of user management or actual capability like access tokens as an additional parameter to the message to the system in our api the user is free to restrict authority at any level in which a capability is involved if really necessary our user could also just have received the capability in fig 5c to the sub time range from somebody with higher authority e g a co worker cap n proto and captp solve the network inefficiency by using a concept called promise pipelining erights org 1998d the user of an api can immediately continue sending messages to the data structures and to the capabilities received as a result of previous messages sending a message in cap n proto will return an object a promise that represents the future result this promise will resolve to the actual result whenever the remote side is ready and the result has been received in our example the following python code fig 8 could express fig 5a c using promise pipelining only when the python wait method is called on the returned promise to the actual data will the cap n proto implementation send the requested message this means that only one network request is made and all three messages are sent simultaneously the subrange message can be delivered whenever on the remote side the first time series promises are resolved when the sub range time series is finally available the data message can be delivered and the actual data returned to the sender note that depending on the computing process also referred to as a vat erights org 1998c in which these capabilities are active these messages subrange and data can almost be plain local method calls within the remote system nevertheless the mechanism is completely flexible and network transparent because either of the returned capabilities from closesttimeseriesat and or subrange could be remote to the vat in which the climate service object itself exists concluding the above cap n proto api can retain the advantages of a proper domain api support capability security principles and still be similarly efficient as an efficiency optimised custom rpc api 3 towards a model and simulation infrastructure in the previous sections we gave the reasons for creating a decentralised model and simulation infrastructure and described which means of communication are necessary within the infrastructure to allow secure collaboration between data models and users in the following we attempt to outline the structure of such a simulation infrastructure and showcase its envisioned use with a few simple examples while cap n proto apis can also be used to implement the mechanics of running models access data or couple models we initially focus on the creation of a coordination api and a set of simple mostly general services the presentation in this paper leaves the implementation of these services consciously open they can differ greatly between use cases and may comprise different methodologies ranging from single threaded scripting for example in python over data and work flow based approaches to highly parallel implementations in languages such as go 2021 in the first step we incrementally create a coordination api and simple general services essentially we can identify four different domains that an api has to cover administrative interfaces to the infrastructure which allow the management e g addition removal of resources models data model and data interfaces to allow access to different services representing a wide range of data and models such a service represents an instance or instance factory of a model set of models or dataset given a capability to the service the user or a program can control the service according to its available interfaces for models the obvious needs are to run it either completely or stepwise e g day by day with a given configuration some of the functionality will be similar to specifications such as openmi harpham et al 2019 which may be a way to bridge into external environments and runtimes we expect these apis to be deployed primarily by users requiring maximum flexibility either via scripting or dedicated graphical user interfaces to data and models capabilities to data will then practically evolve into parameters to model runs meaning that they never need to be accessed or transferred to the user s computer simulation interfaces offer a higher level concept for the assembly of models and data ready to be applied to a certain problem domain rather than having to care about individual model instances users of these apis will focus on the use of previously created sets of ready to run workflows which we call simulations later further simulation apis might allow the interactive creation of simulations scenario interfaces that complement the simulation interfaces acknowledge that at the top most level simulations are always used to explore scenarios these interfaces allow the creation manipulation and easy access to scenarios and simulations bundled configured with particular sets of data while the simulation apis backed by the model and data apis manage the technical aspect of how a simulation is to be performed under defined boundary conditions hardware software available data etc the scenario apis reflect the management and application of these simulations the overarching goal of these apis is to create a composable set of services since we abstract all resources via object capabilities we can write programs that interact with these remote objects the actual programs can become additional services extending the api as long as they implement a cap n proto interface this enables the organic and incremental creation of new services by anyone anywhere comparing this to a single system and a single programming language it is similar to object oriented libraries that are dependent on other libraries for their implementation systems like the java platform or net are considered powerful because they offer a huge set of functionality ready to be used and extended by composing library objects into new potentially more complex libraries 3 1 administrative interfaces the administrative interfaces within the infrastructure can be grouped into two categories i interfaces for managing the infrastructure this is usually the task of dedicated administrators using special apis implemented by graphical user interfaces to manage the actual infrastructure ii interfaces for managing the administrative part of services here the view becomes more blurred these apis actually belong to the service domain and are used to administer a service the domain and the task govern who is authorised to do this for example a service will usually have at least two faces one accessed by the general public and one to configure and maintain the service itself the following simplified example shows how a service can be accessed by two or more capabilities depending on the task or users involved when a service is first launched possibly via the administrative api it returns one or more persistent capabilities sturdy refs to the service if a user possesses one of these sturdy refs they can connect to the service fig 9 below shows the cap n proto schema of a general registry service which when launched returns a reference to the actual service registry a reference to the administrative interface admin and a reference to a registrar interface as a result three levels of authority have been introduced mediated by three capabilities the registry service allows other services to be registered a user can browse the registry and acquire a capability to a service needed for a particular task this capability can then be used to execute the task run a model retrieve data etc our example above supports freely configurable categories which are used when registering a service categories could be called climate soil model or crop management a user who possesses a registrar capability can use the register message to register their own service at the registry the definition of categories permitted for a registry instance is mediated by the admin interface a user who possesses a capability to a remote object with this interface can add and remove categories the registry interface is what users are expected to work with either at the api level using a programming language or in an abstract way using a graphical user interface users can send the message supported categories to get a list of the valid categories or the message entries to obtain a list of all entries on the basis of the category name supplied once a user program has obtained a capability to a required service it can be used directly or queried for a sturdy ref the latter allows direct connection to the service at a later time meaning that the registry is no longer needed from an administrative user s point of view starting a service registry at the command line might look like this fig 10 the started service creates a number of persistent capabilities sturdy refs which allow connection to the service the uri for the above sturdy refs can be read as follows capnp the protocol specifier insecure or public key of the service to connect securely to an encrypted service host port the ip or hostname of the server and the port number being used a sturdy ref token a secure unguessable token to identify the actual remote object being referenced 3 2 model and data interface the model and data interface represents the lowest layer of the user facing part of a model and simulation infrastructure these services are supposed to be used directly by end users and by the simulation interface layer above moreover we expect these services to be the building blocks of other more specialised services or model aggregates and model pipelines fig 11 illustrates the use of the model as well as the data interfaces the diagram assumes that the user has already obtained capabilities to a climate time series a soil profile and a set of management events via existing data services as well as a capability to the instance of a crop growth model via a model instance factory the user then sends the run message to the model instance capability supplying all three data capabilities as parameters the user finally receives the model run s result as pure data as can be seen in this and the previous examples everything boils down to the ingredients capabilities messages and data structures contrary to traditional rpc protocols capabilities are first class meaning that they cannot just be the receiver of a message but also the contents of a message and the result of a message sent this property allows the composability of services and enables the simulation api to build on the model and data interfaces 3 3 simulation interface the simulation api addresses the problem that for many use cases the actual model and data services are too low level the services are the building blocks for a simulation at the next higher aggregate level a simulation may be the application of an agroecosystem model to a larger region to simulate crop yields involving the allocation of many model instances and data sets while fig 11 illustrates running a model on a single climate time series a simulation for a larger region looks similar but involves a capability to a climate dataset in this case a climate dataset is a collection of time series used to acquire capabilities to time series at particular locations within the simulated region how a simulation is then actually configured often depends on the implementations e g scripts using mpi the mpi forum 1993 or mapreduce style dean and ghemawat 2004 pipelines finally fig 12 describes a more complex example at the level of the simulation api the figure also illustrates the flexibility offered by cap n proto for a cross infrastructure setup as mentioned in the introduction in our example scenario user a shares the authority to read data from infrastructure a first step with user b user b runs simulation x which needs an instance of model b and read access to datasets a and b second step after running simulation x user b wants to fix a perceived problem with version 1 of model b which has only been used and deployed in infrastructure b she finds and fixes the problem on her local development machine and tests the bug fix with an instance of model b running there instead of running simulation x with a capability to model b in infrastructure b she runs the simulation with version 2 on her local laptop third step 3 4 scenario interface scenarios can be considered as having the same relationship to simulations as simulations have to models data while a simulation is an aggregation of models and data at the technical level scenarios move closer to the actual application domain the setup of a simulation should address all the technicalities of the models and data to be used the setup must be flexible enough to cater for different models and datasets scenarios usually involve running a simulation with different configurations e g including different climate datasets or crop management strategies the organisation of the different runs belongs to the realm of the scenario api the scenario api is technically the same as all other layers except that the interfaces and their implementations change reflecting the domain to be described 4 discussion 4 1 barriers to scientific collaboration scientific collaboration in environmental modelling has grown in recent years climate systems modelling started long ago to build simulation models that consist of various modules each representing physical components and related processes of the global climate system manabe and wetherald 1967 phillips 1956 advancements were made along two lines i the integration of additional details and components bennartz et al 2013 bony et al 2006 sun and hansen 2003 and ii an increase in the spatial and temporal resolution iles et al 2020 jones et al 1997 the development of climate models and later earth system models friedlingstein et al 2006 always centred on a computing and data storage infrastructure and in turn contributed to fuelling the further development of supercomputers and data storage technology palmer 2014 observation infrastructures giard and bazile 2000 reichle et al 2010 that continuously feed large data streams into these facilities were subsequently added today a number of large environmental research institutions develop and host their own modelling systems applications of their models for short term weather forecast or long term climate scenarios for instance are collected in an ensemble approach to assure the most robust prediction as possible the coupled model intercomparison project eyring et al 2016 meehl et al 1997 is a prominent example from the field of climatology this idea was also adopted in the impact modelling community schellnhuber et al 2014 including hydrologists maxwell et al 2014 economists robinson et al 2014 and agronomists in the latter case the agricultural model intercomparison and improvement project rosenzweig et al 2014 brought together modellers to compare their models in how they represent the interaction of crop plants with their soil and atmosphere environment simulating changes of crop yields water consumption nutrient losses greenhouse gas emissions and other variables of interest in this case too the ensemble mean of a population of models proved to be the best predictor of many target variables martre et al 2015 palosuo et al 2011 however it turned out to be very difficult to improve such models because there were no common standards for code development in this community single modelling groups invested heavily in clipping out code from models to insert it into other models in order to test the consequences of the newly added set of algorithms on the prediction of a target variable maiorano et al 2017 wang et al 2017 at the same time much time and effort has been invested in developing common standards for interfaces buahin and horsburgh 2018 evans 2012 j√∂ckel et al 2005 or data formats porter et al 2014 in a bid to run models developed elsewhere under the roof of a common infrastructure what all these efforts have in common is that components models or data need to be transferred to a new host organisation which then becomes responsible for temporarily maintaining and running them knapen et al 2020 the success of such efforts measured by the number of items involved and used for creating end products reports studies publications seems to be moderate and strongly dependent on the commitment of the hosting person or group if complete models are transferred the problem often arises that the new host institution does not have the capacity to engage with model recalibration or code adjustments if required by a new research question this again hampers a straightforward simulation pipeline with the external models involved a distributed infrastructure would allow for such constructions for instance the models could remain temporarily with their developers and run on a common infrastructure elsewhere on a more granular scale model components could also be handled in the same way allowing for joint model development with distributed process components midingoyi et al 2021 4 2 organic growth a range of large collaborative model developments are under way in various science communities related to environmental research the largest probably being the community earth system model hurrell et al 2013 which involves a number of sub component development teams the concept builds on component based software engineering and software libraries that are developed peripherally and can be shared across multiple institutions host institutions have been specifically designed and equipped to meet the host function in the best possible manner including massive financial support from governmental level although the idea began as a grassroots effort it now relies on a major institutional commitment to keep the development along the desired trajectory this example has worked so far and produced a large number of research products that have a high impact on the community especially in the context of global climate projections and the understanding of the underlying concert of coupled processes however many more communities are under way that are less visible and consequently less heavily supported because their work does not address the most prominent societal challenges in such cases grassroots efforts may remain as such for a long time and require lower thresholds for collaboration especially if they are embedded in a co creation process with stakeholders collaborative modelling ulibarri 2018 for such research efforts easy access to powerful computing facilities as described above will surely be an advantage allowing them to grow organically on the ground of great ideas but with limited resources 4 3 the building and maintenance of large remote object graphs unlike an object oriented program whose analogy we used further above the actual application running is a not under the central control of a single computer process developer and b all components of such distributed programs are subject to failure risk thus while it is already difficult to create bug free programs in traditional environments on a single device a distributed application has to deal with all the possible issues that may arise for multi threaded concurrent applications including network related issues and the lack of central control over all the components while the object capability paradigm and its implementation offers the means consequent asynchronous promise based concurrency to create such an infrastructure it is still a challenging undertaking one particular problem is that of how to keep an existing remote object graph the set of remote services connected via capabilities stable in the advent of a failure if a service goes offline network outage server reboot software crash it has to be restarted automatically at places where many services are hosted and people rely on their availability some kind of supervisor system will be required to assure this automatic response mechanisms to design such fault tolerant systems have already existed for decades and are applied widely in production as exemplified by erlang based systems and otp supervision trees sloughter et al 2019 or the current world of container orchestration e g with kubernetes valim 2019 a further challenge is the evolution of software and services that are reliant on existing services the existence of persistent references to objects services within other systems is meant to allow the creation of new services that are reliant on a potentially large network of other services or remote objects in that network in the case of traditional software updates are released at some point restarting its runtime object world in a decentralised system the evolution of parts of the system requires migrating to newer versions of a remote service moving services to other physical locations or removing services that are considered outdated while keeping if possible clients unaffected a similar challenge arose for clients connected via the http protocol strategies have to be developed for all these cases starting small organic growth is expected to allow the incremental and agile development of these strategies along with the evolution of the distributed system 4 4 relationship to contemporary data and computational infrastructures the object capability approach presented here is considered a flexible layer on top of more foundational infrastructures allowing to ignore particular implementations of data storage representation and remote services execution as such the resulting distributed object graph can represent any imaginable api beyond our example from the environmental realm in any case the underlying infrastructures must be of computational nature unless cap n proto apis for acquiring these computational resources themselves are created the provisioning of actual hardware is completely external of the concepts presented here in most cases capabilities will allow to treat the runtime as an implementation detail no matter if a remote object is living on a personal laptop a hpc node or a virtual machine provided in a cloud the same applies to data infrastructures which facilitate efficient storing accessing synchronisation and sharing of potentially large amounts of data often infrastructure providers expose apis on their own to allow automatised access and retrieval of data in context of the capability approach it means that capability based apis can be created which directly use the exposed apis of these data infrastructures however the process of data management is often an external independent institutional process and outside of the considerations in this paper 4 5 range of applicability and high performance considerations many high performance applications need to access large amounts of data thus the question arises if and how distributed computing can fit into this picture in general the object capability concept is well suited to control and coordinate tasks this allows to abstract access to data and resources and thus increases the likelihood for interoperability between systems it also allows to build further abstractions on top of lower ones to ease the infrastructures use on the other hand any added layer of abstraction adds a layer of indirection and thus most likely reduces efficiency the most glaring problems are related to data locality and latency to process large amounts of data code has to have as much bandwidth to the data as possible often facilitated by having the data physically close to the code processing it consequently it makes a huge difference whether data is living on a remote system across the internet on a high performance network back end on a slow hard drive or in an in memory or in processor cache as a user of a capability does not have to know where the remote code is actually executed this approach is potentially less suited for bandwidth critical applications however up to a certain point the capability approach allows increasing the available bandwidth by moving a remote object closer to the data in need to be processed while in such case the abstraction introduced by the cap n proto layer may lose some of its efficiency it renders some use cases possible that did not exist before latency in the communication between two distant objects scales with distance and with the number of indirection layers being involved it becomes problematic if two models are to be tightly coupled and thus demand a large amount of communication between them similar to the bandwidth problem above it makes a big difference if the code of the two models to be coupled is located within a single thread process multiple threads machine multiple processes node network or within two machines across the internet in all these cases remote objects are interoperable but at some point latency will limit the practicability and the concept becomes useless as demonstrated in fig 12 the potential lies again in the emerging opportunities the user of model b was able to complete the task in step 3 because to the infrastructure the instance of model b was just another capability which allowed her to run version 2 on her laptop if the simulation would have had involved some tight coupling of model b to some other code it may have resulted in a fairly slow simulation but at least it would have worked in the end it all boils down to a balance between performance and flexibility an underlying trade off discussion that also dominates code development it remains to be seen how much ground can be covered in practice when applying the remote procedure call methodology for high performance computing for which it certainly poses some challenges 5 conclusion in the light of heterogeneously distributed computing facilities models data and skilled personnel we have highlighted the barriers that currently hamper scientific and engineering progress in the field of environmental modelling we have argued that collaborative scientific work would achieve faster more efficient progress and described in very general terms how a capability based decentralised model and simulation infrastructure could facilitate such collaboration efforts across individual institutions while existing or future designs and implementations of similar systems remain valid we have offered an alternative view on the interoperability of existing and newly built infrastructures starting with a simple set of service abstractions for models and data the creation of simple user interfaces will enable support for these services and the creation of programs and scripts that directly access data and models in recent years we have seen powerful examples of making data remotely available and it has become evident that we need continuously improving tools to work on these remote interfaces web based interfaces developed for this purpose already facilitate a large range of operations at this stage however they lack the property of composability and a decentralised method of managing security although any single deficiency can be worked around as a whole these concerns hinder seamless interoperability and thus collaboration at both the software and human level we have highlighted the existence of alternative approaches that act inclusively and have the potential to unite disparate efforts in a self organising way software availability section code open source https github com zalf rpm ems ocaps paper 2022 and https github com zalf rpm mas infrastructure languages c c python go java declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25541,with the advent of increasingly powerful computational architectures scientists use these possibilities to create simulations of ever increasing size and complexity large scale simulations of environmental systems require huge amounts of resources managing these in an operational way becomes increasingly complex and difficult to handle for individual scientists state of the art simulation infrastructures usually provide the necessary resources in a centralised setup which often results in an all or nothing choice for the user here we outline an alternative approach to handling this complexity while rendering the use of high performance hardware and large datasets still possible it retains a number of desirable properties i a decentralised structure ii easy sharing of resources to promote collaboration and iii secure access to everything including natural delegation of authority across levels and system boundaries we show that the object capability paradigm will cover these issues and present the first steps towards developing a simulation infrastructure based on these principles keywords cap n proto scientific collaboration co development communication protocol object capability 1 introduction 1 1 the environmental modelling context in a rapidly changing world science is increasingly called on to project future developments of our societal systems and the environmental systems in which they are embedded hofman et al 2017 mea 2005 meadows et al 1972 story telling and the development of narratives is a commonly agreed way of outlining desirable or undesirable futures in a qualitative manner while simulations are used to translate the rather fuzzy narratives into a quantitative universe mallampalli et al 2016 sch√∂nenberg et al 2017 there are already numerous feedback loops within environmental and societal systems and interactions between the two systems intensify the complexity a vast number of simulation models have been developed to represent various sub systems at different levels of detail their purposes range from enhancing the understanding of the system to predicting the behaviour of the system when surrounding conditions change hofman et al 2017 van nes and scheffer 2005 simulation models for environmental and societal systems include a range of methodological approaches and disciplinary foci addressing questions of public and scientific concern ihrig 2016 verburg et al 2016 the aim of integrated modelling is to combine individual modelling approaches into larger model frameworks and to utilise the feedback loops across the different models to gain a better understanding and predictability of the systems represented van beek et al 2020 van ittersum et al 2008 however the operation of individual models often requires a certain infrastructure computing facilities software environment access to data etc verweij et al 2010 and experience on the part of the user to produce the desired output confalonieri et al 2016 which is often limited to individual groups of developers semantic and sociological challenges remain but software hardware and data have undergone substantial improvements in recent decades these improvements enable increasingly complex simulations and new modelling approaches also across domains partially driving the development of even more powerful computers as the potential for increasing the speed of single processors has hit the ceiling for now parallel computing on multiple processors is on the rise shalf 2020 at the same time the advancement of sensors sensor platforms and other forms of continuous and large scale data creation has boosted the availability of big data for potential use in developing calibrating or driving simulation models basso and antle 2020 franz et al 2020 hampf et al 2021 weiss et al 2020 in many cases large institutionalised infrastructures are needed to harvest store and serve such data however high performance computing hpc facilities big data repositories and skilled model developers are unevenly distributed across research institutions the integration of different models and the utilisation of computing power and data availability therefore require cross institutional collaboration and a software infrastructure that supports such collaboration altschul et al 2013 ideally this infrastructure should integrate models data and people including model developers and model users drawing on the example of impact modelling in agriculture we present below the vision and rationale for a flexible cooperative simulation infrastructure and the underlying software mechanisms needed to support it 1 2 criteria for a simulation infrastructure no matter how it is done designing and developing a new model and simulation infrastructure is an extensive undertaking in light of past experiences and projects in the environmental modelling field an ideal infrastructure has the following desirable properties 1 flexibility the ideal infrastructure allows for easy integration of diverse hardware pc hpc cloud and software legacy different languages etc and can handle a wide range of heterogeneous models and data 2 efficiency the ideal infrastructure facilitates the fast and efficient execution of both simple and complex simulations and does not represent the bottleneck of the simulation 3 simplicity the simpler the infrastructure the higher the level of adoption an infrastructure must be easy for scientists to use and for developers to improve 4 peripherality to ease collaboration the infrastructure should support the distributed and decentralised use of models and data 5 openness a modern simulation infrastructure should be open to improvement by both scientists and developers involving the collaboration of many different people ideally such collaboration is voluntary unconditional easy and secure today s model and simulation infrastructures tend to be based on a centralised approach where a large storage and computational backend is accompanied by some user facing usually web based frontend that allows access to the system consequently models and data to be used within such an infrastructure must be prepared accordingly to fit into the system the more commonality can be enforced the more benefits can be gained by the easy use of models for new data or new models for existing data what these centralised infrastructures have in common is that they act as silos allowing only the models and data placed inside them to interact or utilise future enhancements of the system this means that existing models and data that are to be included in the infrastructure have to be adapted to the infrastructure and its prevailing rules depending on the properties of the system adjusting to the infrastructure requires a considerable amount of resources and skills while the trade off between costs and benefits can be very positive for large organisations major projects or long term needs it may discourage smaller stakeholders hamper agile experimentation and generally cause problems when models and or data do not fit easily into the concept of an infrastructure similarly it is difficult and expensive to cater for multiple possibly competing infrastructures in addition centralised infrastructures encourage a centralised user and rights management system which makes administrative control easy but external collaboration and participation more difficult in order to mitigate some of the aforementioned problems and to achieve the properties required by the scientific community we propose a different take on simulation infrastructure our goal is to create a potentially large ad hoc decentralised simulation infrastructure comprising networked components such as computing or storage resources all components are treated as equal members of the larger virtual infrastructure these distributed components communicate via a capability based remote procedure protocol rpc according to capability security principles this allows fine grained decentralised security and the easy and safe delegation of authority it removes the need for centralised user management enabling unconditional collaboration at all scales since all parts of the infrastructure are treated as equal there is no distinction between simple and complex models personal laptops and hpcs or small local and large cloud backed datasets what counts at the infrastructure level is communication between the components that make up a simulation it goes without saying that different needs will result in the creation of different setups an organisation s need for high performance large scale simulations will result in parts of the virtual infrastructure models and data being optimised for hpcs on the other hand a scientist might want to hook into the same setup and prototype a model change or experiment using a new locally crafted dataset in both cases users need access to the components representing the resources in order to run a particular simulation the authority to access such resources can be delegated by peers or granted by administrators as is the case in centralised systems decentralisation does not mean the complete abandonment of centralisation instead it seeks to establish multiple centres and potentially lots of them the costs of the virtual infrastructure are then reduced to the cost of self hosted systems and a negotiable part for remotely used systems it is likely that only simulations bound by performance needs will have to run in a purely centralised manner the simulation infrastructure we propose attempts to bridge the extremes local vs hpc by offering sufficient flexibility to fill the space between those extremes 1 3 the transition from centralised to decentralised systems decentralisation requires that all parts communicate since speed and latency are always important an efficient communication protocol a common language and shared communication interfaces both explicitly and externally specified are needed agreeing on a common set of interfaces for the infrastructure s components allows to abstract away some of the heterogeneity of real world systems and renders possible the creation of a virtual infrastructure stable communication interface specifications encourage model creators and data suppliers to implement them for their own resources equipped with sufficient tooling end users may also become model or data providers being able to hook into the infrastructure peripherally security is an even more important issue for collaboration in open distributed systems than it is in closed centralised infrastructures while the latter can employ well established user and rights management mechanisms controlled by an administrator with elevated rights a single point of control is difficult and ultimately not desirable in a decentralised system to allow as much freedom as possible within the infrastructure we selected the capability security paradigm miller et al 2003 as the underlying authority transfer and access mechanism capability security gives the user access to a particular resource and the authority to handle it together both will be defined later as capability without a third party e g an administrator being involved and the opportunity to pass on this capability to others this transfer of capabilities is part of the communication protocol and as such a software entity that can address both users and machines capabilities can be transferred via messages that are otherwise used to execute a simulation model for example and pass on access information and rights in the simulation model example the model can receive access information to a particular data set plus the authority to access it all these factors are easily applicable as long as all the components are connected however clients may go offline and servers and systems may fail or reboot users once authorised may want to retain the capabilities received between work sessions or pass them on to somebody else via infrastructure independent means such as an email to this end capabilities need to persist so that they can be stored offline and used at a later stage to gain access to the resources represented persistent capabilities are called sturdy references erights org 1998a or sturdy refs receiving a sturdy ref enables a user or a software program to connect to the resource represented and interact with that resource according to the capability s communication interface 1 4 choice of a particular technology so far we have made only an abstract reference to the foundational capability secure layer that we believe a decentralised simulation infrastructure needs to develop the desired properties since it is likely that a decentralised system will consist of very different hardware software programs programming languages platforms and frameworks the capability secure communication protocol should be agnostic to languages and platforms there is currently only one real world implementation of a capability secure communication protocol that meets our needs being multi language multi platform offering an external schema description and focusing on speed and efficiency cap n proto capnp 2022 is a fast data interchange format and a capability based rpc protocol it is an implementation of the captp protocol captp 2021 which allows distributed object programming over mutually suspicious networks meant to be combined with an object capability ocap style of programming spritely 2021 in other words it is a distributed object oriented program secured by capability principles capabilities provided to scripts or programs act like a distributed object oriented application programming interface api e g a script or program would start off with one or more sturdy refs persistent capabilities to some part of the infrastructure turning these into live references e g to model instances or datasets allows the program to access the currently available simulation api now that the entry point to the simulation infrastructure is mediated by capabilities software that uses and runs on top of the infrastructure can be secure even among mutually suspicious participants a capability granted to a participant in the infrastructure should therefore only have the authority required for their tasks moreover a capability should never allow unintended privilege escalation which would involve an owner being granted more authority than is needed for a given task this principle of least authority pola saltzer 1974 can be expressed very naturally in object capability systems the capability security principles are able to avoid many pitfalls of contemporary systems that are mainly based on ambient authority using access control lists acls which we describe in more detail in the next section 1 5 the example realm seeking to exemplify the concept of object capabilities in the context of a model and simulation infrastructure we use the case of process based agroecosystem modelling process based agroecosystem models build on a system of differential equations that represent biophysical processes in the soil plant atmosphere continuum and are typically used to simulate the impact of crop properties genotype soil and climate environment and alternative soil and crop management strategies management on a particular target variable or set of target variables these variables often include crop yields water use nutrient losses greenhouse gas emissions and many more besides due to their development legacy most available agroecosystem models are built on a one dimensional concept that does not represent any spatial relationships per se to overcome this limitation such models are often applied as a suite of individual simulations across a grid representing an area involving either sequential model runs or parallel runs on multiple computer cores in this case each grid cell must be supplied with the appropriate input data to drive the model and the simulation result of each grid cell must be collected in order to display the overall simulation result in the spatial context in which it was embedded fig 1 scenario runs may require alternative data and management rules finally the simulation results are often needed as a series of maps visualising the spatial and temporal distribution of the target variable e g crop yields 2 capability security and object capabilities a capability is a communicable unforgeable token of authority an object capability in turn is an unforgeable reference to an object the object s interface defines the authority that can be exercised by sending messages to the object dennis and van horn 1966 in this section we explain the concept of capability security and how it differs from the well established mechanism of access control lists acls and address object capabilities in particular the following examples are purposely reduced and simplified to allow focusing on the concepts in the accompanying github repository code 2022 the complete source code and the description how to run the examples can be found 2 1 access control lists versus capabilities both access control lists and capabilities are used to control access to resources within a computer system this requires a definition of which user or process has access to which resources an acl is a list of users and their access rights to a particular resource translated to a filesystem a control authority e g an administrator specifies which users are allowed to access a file and what are they allowed to do e g read the file in an acl based filesystem a user refers to the resources files by the name of the file the filename which does not by itself entail the authority to access the file instead authorisation is derived from the file s environment this concept is referred to as ambient authority it may be viewed as an automatic door that identifies an authorised person on the basis of prior registration and slides open when such a person approaches in contrast capabilities can be compared to keys equipped with the matching key a person can open the door in this case access authorisation is attached to the key itself and not to the environment a typical way of depicting the problem is to use a matrix showing the users along the vertical axis and the resources along the horizontal axis the matrix in fig 2 states that all users are allowed to read the file at etc passwd but only alice and carol are permitted to write to the file u markm foo whereas bob is denied access to this file acls and capabilities offer two different ways of looking at this matrix in an acl system the file etc motd would have to have a list attached stating that both bob and carol are allowed to read the file in contrast in a capability system bob and carol would need to have a key that allows them to read the file etc motd if the matrix is static both approaches to defining access to a resource are equivalent as soon as the matrix becomes dynamic things differ in the acl approach only the owner or privileged users administrators are usually allowed to change access lists if then alice also needs to read the file etc motd at a later point in time the administrator has to update the acl of the file etc motd in a capability system somebody with read access to the file etc motd has to give alice a copy of the key so that she too can read the file the important difference to note is that with capabilities all authorised users can easily delegate authority to somebody else see also sandstorm 2015 miller et al 2003 discussed this and other issues in detail and traced many of the computer security problems that still exist today back to the use of ambient authority in acl systems which impairs the safe delegation of authority however the dynamic ad hoc and safe sharing of resources is a prerequisite for collaboration the aforementioned door key analogy is easy to grasp but has issues of its own the most obvious one being the revocation of authority once granted in this real world analogy the only way to remove authority would be to replace the whole door lock this is often not realistic or desirable nevertheless many contemporary token based methods of accessing web services belong to this category although keys have their uses as capabilities a different kind of capability referred to as object capabilities solve this issue by offering richer semantics 2 2 object capabilities the concept of object capability proposed by dennis and van horn 1966 parallels the concept of object oriented programming oop in many aspects an object capability is a reference to an object and represents both the authority and the means to access a resource the authority is exercised by sending messages to the referenced object the messages that the object understands make up the authority it represents in software systems an object usually represents some real or abstract resource encapsulated by an interface the messages that the object understands for our purposes objects exist within computer programs on machines connected by networks thus a capability aka a reference to an object might span program and machine boundaries in any case possessing a reference to an object equals the authority to access it according to the object capabilities concept the owner of an object reference is able to share it with somebody else by sending them a message fig 3 granovetter 1973 miller et al 2001 which in our case depicts the relationships of computational objects capabilities in a network and their change over time fig 3 recalls the original communication between alice bob and carol in our case all three actors are objects alice has a reference a capability to bob and is therefore authorised to send the message foo to bob this message includes a reference a capability to carol authorising bob to talk to carol as well 2 2 1 a revocable object capability object capability systems operate under the assumption that the only way to communicate is through object references in other words if bob receives a capability to carol then bob has all the authority that this capability entails one of the common objections against capability systems is that once issued capabilities cannot be revoked in acl systems the operation and authority of revocation is external to the user who is supposed to access the resource for example an administrator can remove a user from the acl of a particular file or change the access rights to the file object capability systems allow new objects to be created that instantaneously forward all messages they receive facilitating the creation of a valid work around fig 4 in this case forwarder f is such a capability that alice sends to bob optionally f could even have a different perhaps limited read instead of read write interface to the one carol offers weakening the original capability to carol this alone would not yet allow alice to control bob s access to carol inserting another forwarder revoker r under alice s control that forwards f s messages to carol solves problem r s interface can include a revoke message which cuts the connection between r and carol effectively revoking bob s access to carol bob can still talk to f and f will still forward all messages to r but the communication line is broken at r as a result alice has revoked bob s authority to talk to carol 2 3 implementations of capability security and object capabilities considerable research has been undertaken on capabilities and different applications have been developed ranging from operating systems eros 1999 keykos 1988 programming languages erights org 1998b and web capabilities waterken 2009 to filesystems tahoe lafs 2021 and a web application platform sandstorm 2021 burtsev et al 2017 presented research on a capability enabled cloud infrastructure to facilitate secure collaboration at the infrastructure level while it is desirable to secure the underlying infrastructures using capabilities and a least authority approach the simulation infrastructure we propose operates at a different level just below the application layer from the perspective of safe collaboration the sandstorm platform is particularly interesting its goal was to create a web based self hostable productivity suite and platform with an app marketplace that allows untrusted applications to be run together in a cluster environment the whole system was built on capability principles to allow secure communication among untrusted apps and users the basis for sandstorm was the cap n proto protocol a decentralised contemporary simulation infrastructure needs to support a variety of systems and languages cap n proto is a data interchange format and capability based rpc protocol which allows disparate systems to communicate via object capabilities with an eye to efficiency and performance implementations of cap n proto protocols are available for many different languages enabling the integration of a wide range of existing systems even though cap n proto is not as widely used as other pure rpc protocols e g grpc 2021 it has proven to be stable and is also used in production systems for instance at cloudflare cloudflare 2021 capability security speed efficiency platform and language agnosticism suggest that cap n proto is a good foundation for a distributed simulation infrastructure 2 4 a simple example of a cap n proto api as a data interchange format the cap n proto protocol specifies the structure of messages being transferred from a sender to a receiver these messages are strongly typed but not self descriptive for this reason cap n proto defines a special schema language that is used to describe the message structure a compiler is then used to create code and data structures in the target language allowing messages to be manipulated cap n proto schemas contain two broad categories of structures data structures and interfaces the data structures represent the layout and type of data being sent interfaces are the entry point to the capability based rpc system and represent runtime objects and the messages these objects may receive messages in cap n proto implementations are always sent asynchronously and like a method call in an object oriented language can return values data structures or other interfaces due to its asynchronous nature the return of a value is depicted in the example below fig 5 as a separate message directing back from the receiver to the sender of the original message conceptually this is the same as sending a message to the receiver with an additional parameter representing the sender once the result is ready the receiver can return it to the sender in the example realm of environmental science fig 4 shows the use of a simple remote climate service to access data of a time series at a particular geo location of the period 2021 2050 we assume that our user possesses a reference a capability to a remote climate service that holds these data when the service is ready it will deliver a time series capability back to our user fig 5a this capability is authorised to send all messages to a time series object that the user understands the simplified interfaces for the climate service and the time series are shown below fig 6 words in bold represent capabilities interfaces words in italics denote data structures or simple types and underlined words depict the message names that an object implementing this interface understands the cap n proto schema language allows the use of many of the structures often found in object oriented programming languages such as inheritance nested structures and unions after receiving the time series capability the user only holds a reference and the potential to acquire the data or query meta information since our user only wants a 30 year data subset he sends the subrange message to the time series capability to receive another time series capability representing that 30 year subrange this process is illustrated in fig 5b in contrast fig 5c shows the user sending the final data message to the just received sub time range the result now is pure data the structure of which has been described in the schema above as list list float32 the documentation of the schema would describe it as a list the days of a list of floating point values the day s data the order of the day s data for instance is defined by the header message available in the timeseries interface a particular point to note is the seemingly inefficient interface that timeseries offers in order to obtain the required time series data the user had to do three full network round trips three requests to the remote interface to optimise this other interfaces fig 7 could be possible e g to do the job a climateservice object could understand only one data message with three parameters revealing the location and time range requested which then requires only one network round trip an obvious problem in the long run could be the proliferation of the messages and message combinations an interface needs to declare that all needs are met e g a version of the retrieved data within a certain time range but with a different set of climate elements the original api design did not have this problem because common object oriented principles could be applied as network round trips albeit seemingly at the cost of latency a more serious problem is that in this case the pola has been violated the second api version has no way to prevent a user from possessing this capability to acquire all data offered by the service an authority we might not want to give the user traditional rpc systems have to work around this problem by adding a further layer of user management or actual capability like access tokens as an additional parameter to the message to the system in our api the user is free to restrict authority at any level in which a capability is involved if really necessary our user could also just have received the capability in fig 5c to the sub time range from somebody with higher authority e g a co worker cap n proto and captp solve the network inefficiency by using a concept called promise pipelining erights org 1998d the user of an api can immediately continue sending messages to the data structures and to the capabilities received as a result of previous messages sending a message in cap n proto will return an object a promise that represents the future result this promise will resolve to the actual result whenever the remote side is ready and the result has been received in our example the following python code fig 8 could express fig 5a c using promise pipelining only when the python wait method is called on the returned promise to the actual data will the cap n proto implementation send the requested message this means that only one network request is made and all three messages are sent simultaneously the subrange message can be delivered whenever on the remote side the first time series promises are resolved when the sub range time series is finally available the data message can be delivered and the actual data returned to the sender note that depending on the computing process also referred to as a vat erights org 1998c in which these capabilities are active these messages subrange and data can almost be plain local method calls within the remote system nevertheless the mechanism is completely flexible and network transparent because either of the returned capabilities from closesttimeseriesat and or subrange could be remote to the vat in which the climate service object itself exists concluding the above cap n proto api can retain the advantages of a proper domain api support capability security principles and still be similarly efficient as an efficiency optimised custom rpc api 3 towards a model and simulation infrastructure in the previous sections we gave the reasons for creating a decentralised model and simulation infrastructure and described which means of communication are necessary within the infrastructure to allow secure collaboration between data models and users in the following we attempt to outline the structure of such a simulation infrastructure and showcase its envisioned use with a few simple examples while cap n proto apis can also be used to implement the mechanics of running models access data or couple models we initially focus on the creation of a coordination api and a set of simple mostly general services the presentation in this paper leaves the implementation of these services consciously open they can differ greatly between use cases and may comprise different methodologies ranging from single threaded scripting for example in python over data and work flow based approaches to highly parallel implementations in languages such as go 2021 in the first step we incrementally create a coordination api and simple general services essentially we can identify four different domains that an api has to cover administrative interfaces to the infrastructure which allow the management e g addition removal of resources models data model and data interfaces to allow access to different services representing a wide range of data and models such a service represents an instance or instance factory of a model set of models or dataset given a capability to the service the user or a program can control the service according to its available interfaces for models the obvious needs are to run it either completely or stepwise e g day by day with a given configuration some of the functionality will be similar to specifications such as openmi harpham et al 2019 which may be a way to bridge into external environments and runtimes we expect these apis to be deployed primarily by users requiring maximum flexibility either via scripting or dedicated graphical user interfaces to data and models capabilities to data will then practically evolve into parameters to model runs meaning that they never need to be accessed or transferred to the user s computer simulation interfaces offer a higher level concept for the assembly of models and data ready to be applied to a certain problem domain rather than having to care about individual model instances users of these apis will focus on the use of previously created sets of ready to run workflows which we call simulations later further simulation apis might allow the interactive creation of simulations scenario interfaces that complement the simulation interfaces acknowledge that at the top most level simulations are always used to explore scenarios these interfaces allow the creation manipulation and easy access to scenarios and simulations bundled configured with particular sets of data while the simulation apis backed by the model and data apis manage the technical aspect of how a simulation is to be performed under defined boundary conditions hardware software available data etc the scenario apis reflect the management and application of these simulations the overarching goal of these apis is to create a composable set of services since we abstract all resources via object capabilities we can write programs that interact with these remote objects the actual programs can become additional services extending the api as long as they implement a cap n proto interface this enables the organic and incremental creation of new services by anyone anywhere comparing this to a single system and a single programming language it is similar to object oriented libraries that are dependent on other libraries for their implementation systems like the java platform or net are considered powerful because they offer a huge set of functionality ready to be used and extended by composing library objects into new potentially more complex libraries 3 1 administrative interfaces the administrative interfaces within the infrastructure can be grouped into two categories i interfaces for managing the infrastructure this is usually the task of dedicated administrators using special apis implemented by graphical user interfaces to manage the actual infrastructure ii interfaces for managing the administrative part of services here the view becomes more blurred these apis actually belong to the service domain and are used to administer a service the domain and the task govern who is authorised to do this for example a service will usually have at least two faces one accessed by the general public and one to configure and maintain the service itself the following simplified example shows how a service can be accessed by two or more capabilities depending on the task or users involved when a service is first launched possibly via the administrative api it returns one or more persistent capabilities sturdy refs to the service if a user possesses one of these sturdy refs they can connect to the service fig 9 below shows the cap n proto schema of a general registry service which when launched returns a reference to the actual service registry a reference to the administrative interface admin and a reference to a registrar interface as a result three levels of authority have been introduced mediated by three capabilities the registry service allows other services to be registered a user can browse the registry and acquire a capability to a service needed for a particular task this capability can then be used to execute the task run a model retrieve data etc our example above supports freely configurable categories which are used when registering a service categories could be called climate soil model or crop management a user who possesses a registrar capability can use the register message to register their own service at the registry the definition of categories permitted for a registry instance is mediated by the admin interface a user who possesses a capability to a remote object with this interface can add and remove categories the registry interface is what users are expected to work with either at the api level using a programming language or in an abstract way using a graphical user interface users can send the message supported categories to get a list of the valid categories or the message entries to obtain a list of all entries on the basis of the category name supplied once a user program has obtained a capability to a required service it can be used directly or queried for a sturdy ref the latter allows direct connection to the service at a later time meaning that the registry is no longer needed from an administrative user s point of view starting a service registry at the command line might look like this fig 10 the started service creates a number of persistent capabilities sturdy refs which allow connection to the service the uri for the above sturdy refs can be read as follows capnp the protocol specifier insecure or public key of the service to connect securely to an encrypted service host port the ip or hostname of the server and the port number being used a sturdy ref token a secure unguessable token to identify the actual remote object being referenced 3 2 model and data interface the model and data interface represents the lowest layer of the user facing part of a model and simulation infrastructure these services are supposed to be used directly by end users and by the simulation interface layer above moreover we expect these services to be the building blocks of other more specialised services or model aggregates and model pipelines fig 11 illustrates the use of the model as well as the data interfaces the diagram assumes that the user has already obtained capabilities to a climate time series a soil profile and a set of management events via existing data services as well as a capability to the instance of a crop growth model via a model instance factory the user then sends the run message to the model instance capability supplying all three data capabilities as parameters the user finally receives the model run s result as pure data as can be seen in this and the previous examples everything boils down to the ingredients capabilities messages and data structures contrary to traditional rpc protocols capabilities are first class meaning that they cannot just be the receiver of a message but also the contents of a message and the result of a message sent this property allows the composability of services and enables the simulation api to build on the model and data interfaces 3 3 simulation interface the simulation api addresses the problem that for many use cases the actual model and data services are too low level the services are the building blocks for a simulation at the next higher aggregate level a simulation may be the application of an agroecosystem model to a larger region to simulate crop yields involving the allocation of many model instances and data sets while fig 11 illustrates running a model on a single climate time series a simulation for a larger region looks similar but involves a capability to a climate dataset in this case a climate dataset is a collection of time series used to acquire capabilities to time series at particular locations within the simulated region how a simulation is then actually configured often depends on the implementations e g scripts using mpi the mpi forum 1993 or mapreduce style dean and ghemawat 2004 pipelines finally fig 12 describes a more complex example at the level of the simulation api the figure also illustrates the flexibility offered by cap n proto for a cross infrastructure setup as mentioned in the introduction in our example scenario user a shares the authority to read data from infrastructure a first step with user b user b runs simulation x which needs an instance of model b and read access to datasets a and b second step after running simulation x user b wants to fix a perceived problem with version 1 of model b which has only been used and deployed in infrastructure b she finds and fixes the problem on her local development machine and tests the bug fix with an instance of model b running there instead of running simulation x with a capability to model b in infrastructure b she runs the simulation with version 2 on her local laptop third step 3 4 scenario interface scenarios can be considered as having the same relationship to simulations as simulations have to models data while a simulation is an aggregation of models and data at the technical level scenarios move closer to the actual application domain the setup of a simulation should address all the technicalities of the models and data to be used the setup must be flexible enough to cater for different models and datasets scenarios usually involve running a simulation with different configurations e g including different climate datasets or crop management strategies the organisation of the different runs belongs to the realm of the scenario api the scenario api is technically the same as all other layers except that the interfaces and their implementations change reflecting the domain to be described 4 discussion 4 1 barriers to scientific collaboration scientific collaboration in environmental modelling has grown in recent years climate systems modelling started long ago to build simulation models that consist of various modules each representing physical components and related processes of the global climate system manabe and wetherald 1967 phillips 1956 advancements were made along two lines i the integration of additional details and components bennartz et al 2013 bony et al 2006 sun and hansen 2003 and ii an increase in the spatial and temporal resolution iles et al 2020 jones et al 1997 the development of climate models and later earth system models friedlingstein et al 2006 always centred on a computing and data storage infrastructure and in turn contributed to fuelling the further development of supercomputers and data storage technology palmer 2014 observation infrastructures giard and bazile 2000 reichle et al 2010 that continuously feed large data streams into these facilities were subsequently added today a number of large environmental research institutions develop and host their own modelling systems applications of their models for short term weather forecast or long term climate scenarios for instance are collected in an ensemble approach to assure the most robust prediction as possible the coupled model intercomparison project eyring et al 2016 meehl et al 1997 is a prominent example from the field of climatology this idea was also adopted in the impact modelling community schellnhuber et al 2014 including hydrologists maxwell et al 2014 economists robinson et al 2014 and agronomists in the latter case the agricultural model intercomparison and improvement project rosenzweig et al 2014 brought together modellers to compare their models in how they represent the interaction of crop plants with their soil and atmosphere environment simulating changes of crop yields water consumption nutrient losses greenhouse gas emissions and other variables of interest in this case too the ensemble mean of a population of models proved to be the best predictor of many target variables martre et al 2015 palosuo et al 2011 however it turned out to be very difficult to improve such models because there were no common standards for code development in this community single modelling groups invested heavily in clipping out code from models to insert it into other models in order to test the consequences of the newly added set of algorithms on the prediction of a target variable maiorano et al 2017 wang et al 2017 at the same time much time and effort has been invested in developing common standards for interfaces buahin and horsburgh 2018 evans 2012 j√∂ckel et al 2005 or data formats porter et al 2014 in a bid to run models developed elsewhere under the roof of a common infrastructure what all these efforts have in common is that components models or data need to be transferred to a new host organisation which then becomes responsible for temporarily maintaining and running them knapen et al 2020 the success of such efforts measured by the number of items involved and used for creating end products reports studies publications seems to be moderate and strongly dependent on the commitment of the hosting person or group if complete models are transferred the problem often arises that the new host institution does not have the capacity to engage with model recalibration or code adjustments if required by a new research question this again hampers a straightforward simulation pipeline with the external models involved a distributed infrastructure would allow for such constructions for instance the models could remain temporarily with their developers and run on a common infrastructure elsewhere on a more granular scale model components could also be handled in the same way allowing for joint model development with distributed process components midingoyi et al 2021 4 2 organic growth a range of large collaborative model developments are under way in various science communities related to environmental research the largest probably being the community earth system model hurrell et al 2013 which involves a number of sub component development teams the concept builds on component based software engineering and software libraries that are developed peripherally and can be shared across multiple institutions host institutions have been specifically designed and equipped to meet the host function in the best possible manner including massive financial support from governmental level although the idea began as a grassroots effort it now relies on a major institutional commitment to keep the development along the desired trajectory this example has worked so far and produced a large number of research products that have a high impact on the community especially in the context of global climate projections and the understanding of the underlying concert of coupled processes however many more communities are under way that are less visible and consequently less heavily supported because their work does not address the most prominent societal challenges in such cases grassroots efforts may remain as such for a long time and require lower thresholds for collaboration especially if they are embedded in a co creation process with stakeholders collaborative modelling ulibarri 2018 for such research efforts easy access to powerful computing facilities as described above will surely be an advantage allowing them to grow organically on the ground of great ideas but with limited resources 4 3 the building and maintenance of large remote object graphs unlike an object oriented program whose analogy we used further above the actual application running is a not under the central control of a single computer process developer and b all components of such distributed programs are subject to failure risk thus while it is already difficult to create bug free programs in traditional environments on a single device a distributed application has to deal with all the possible issues that may arise for multi threaded concurrent applications including network related issues and the lack of central control over all the components while the object capability paradigm and its implementation offers the means consequent asynchronous promise based concurrency to create such an infrastructure it is still a challenging undertaking one particular problem is that of how to keep an existing remote object graph the set of remote services connected via capabilities stable in the advent of a failure if a service goes offline network outage server reboot software crash it has to be restarted automatically at places where many services are hosted and people rely on their availability some kind of supervisor system will be required to assure this automatic response mechanisms to design such fault tolerant systems have already existed for decades and are applied widely in production as exemplified by erlang based systems and otp supervision trees sloughter et al 2019 or the current world of container orchestration e g with kubernetes valim 2019 a further challenge is the evolution of software and services that are reliant on existing services the existence of persistent references to objects services within other systems is meant to allow the creation of new services that are reliant on a potentially large network of other services or remote objects in that network in the case of traditional software updates are released at some point restarting its runtime object world in a decentralised system the evolution of parts of the system requires migrating to newer versions of a remote service moving services to other physical locations or removing services that are considered outdated while keeping if possible clients unaffected a similar challenge arose for clients connected via the http protocol strategies have to be developed for all these cases starting small organic growth is expected to allow the incremental and agile development of these strategies along with the evolution of the distributed system 4 4 relationship to contemporary data and computational infrastructures the object capability approach presented here is considered a flexible layer on top of more foundational infrastructures allowing to ignore particular implementations of data storage representation and remote services execution as such the resulting distributed object graph can represent any imaginable api beyond our example from the environmental realm in any case the underlying infrastructures must be of computational nature unless cap n proto apis for acquiring these computational resources themselves are created the provisioning of actual hardware is completely external of the concepts presented here in most cases capabilities will allow to treat the runtime as an implementation detail no matter if a remote object is living on a personal laptop a hpc node or a virtual machine provided in a cloud the same applies to data infrastructures which facilitate efficient storing accessing synchronisation and sharing of potentially large amounts of data often infrastructure providers expose apis on their own to allow automatised access and retrieval of data in context of the capability approach it means that capability based apis can be created which directly use the exposed apis of these data infrastructures however the process of data management is often an external independent institutional process and outside of the considerations in this paper 4 5 range of applicability and high performance considerations many high performance applications need to access large amounts of data thus the question arises if and how distributed computing can fit into this picture in general the object capability concept is well suited to control and coordinate tasks this allows to abstract access to data and resources and thus increases the likelihood for interoperability between systems it also allows to build further abstractions on top of lower ones to ease the infrastructures use on the other hand any added layer of abstraction adds a layer of indirection and thus most likely reduces efficiency the most glaring problems are related to data locality and latency to process large amounts of data code has to have as much bandwidth to the data as possible often facilitated by having the data physically close to the code processing it consequently it makes a huge difference whether data is living on a remote system across the internet on a high performance network back end on a slow hard drive or in an in memory or in processor cache as a user of a capability does not have to know where the remote code is actually executed this approach is potentially less suited for bandwidth critical applications however up to a certain point the capability approach allows increasing the available bandwidth by moving a remote object closer to the data in need to be processed while in such case the abstraction introduced by the cap n proto layer may lose some of its efficiency it renders some use cases possible that did not exist before latency in the communication between two distant objects scales with distance and with the number of indirection layers being involved it becomes problematic if two models are to be tightly coupled and thus demand a large amount of communication between them similar to the bandwidth problem above it makes a big difference if the code of the two models to be coupled is located within a single thread process multiple threads machine multiple processes node network or within two machines across the internet in all these cases remote objects are interoperable but at some point latency will limit the practicability and the concept becomes useless as demonstrated in fig 12 the potential lies again in the emerging opportunities the user of model b was able to complete the task in step 3 because to the infrastructure the instance of model b was just another capability which allowed her to run version 2 on her laptop if the simulation would have had involved some tight coupling of model b to some other code it may have resulted in a fairly slow simulation but at least it would have worked in the end it all boils down to a balance between performance and flexibility an underlying trade off discussion that also dominates code development it remains to be seen how much ground can be covered in practice when applying the remote procedure call methodology for high performance computing for which it certainly poses some challenges 5 conclusion in the light of heterogeneously distributed computing facilities models data and skilled personnel we have highlighted the barriers that currently hamper scientific and engineering progress in the field of environmental modelling we have argued that collaborative scientific work would achieve faster more efficient progress and described in very general terms how a capability based decentralised model and simulation infrastructure could facilitate such collaboration efforts across individual institutions while existing or future designs and implementations of similar systems remain valid we have offered an alternative view on the interoperability of existing and newly built infrastructures starting with a simple set of service abstractions for models and data the creation of simple user interfaces will enable support for these services and the creation of programs and scripts that directly access data and models in recent years we have seen powerful examples of making data remotely available and it has become evident that we need continuously improving tools to work on these remote interfaces web based interfaces developed for this purpose already facilitate a large range of operations at this stage however they lack the property of composability and a decentralised method of managing security although any single deficiency can be worked around as a whole these concerns hinder seamless interoperability and thus collaboration at both the software and human level we have highlighted the existence of alternative approaches that act inclusively and have the potential to unite disparate efforts in a self organising way software availability section code open source https github com zalf rpm ems ocaps paper 2022 and https github com zalf rpm mas infrastructure languages c c python go java declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25542,an operational urban air quality model enfuser based on dispersion modelling and data assimilation lasse johansson a ari karppinen a mona kurppa a anu kousa b jarkko v niemi b jaakko kukkonen a c a atmospheric composition research finnish meteorological institute helsinki finland atmospheric composition research finnish meteorological institute helsinki finland atmospheric composition research finnish meteorological institute helsinki finland b helsinki region environmental services authority hsy ilmalantori 1 fi 00240 helsinki finland helsinki region environmental services authority hsy ilmalantori 1 fi 00240 helsinki finland helsinki region environmental services authority hsy ilmalantori 1 fi 00240 helsinki finland c centre for atmospheric and climate physics research and centre for climate change research university of hertfordshire college lane hatfield al10 9ab uk centre for atmospheric and climate physics research and centre for climate change research university of hertfordshire college lane hatfield al10 9ab uk centre for atmospheric and climate physics research and centre for climate change research university of hertfordshire college lane hatfield al10 9ab uk corresponding author an operational urban air quality modelling system enfuser is presented with an evaluation against measured data enfuser combines several dispersion modelling approaches uses data assimilation and continuously extracts information from online global open access sources the modelling area is described with a combination of geographic datasets these gis datasets are globally available with open access and therefore the model can be applied worldwide urban scale dispersion is addressed with a combination of gaussian puff and gaussian plume modelling and long range transport of pollutants is accounted for via a separate regional model the presented data assimilation method which supports the use of aq sensors and incorporates a longer term learning mechanism adjusts emission factors and the regional background values on an hourly basis the model can be used with reasonable accuracy also in urban areas for which detailed emissions inventories would not be available due to the data assimilation capabilities graphical abstract image 1 keywords air quality dispersion modelling data assimilation 1 introduction air pollution is a major environmental concern in many areas worldwide having a strong impact on public health and the economy world health organization 2016 guerreiro et al 2014 the spatial variability of air quality is especially pronounced in urban environments it is common for e g european cities to report exceedances of annual air quality limit values at urban monitoring stations eea 2018 therefore high quality up to date information on local air quality is needed to make it possible for citizens to assess their personal exposure to air pollutants and make informed decisions on e g their commuting options further local authorities also require such timely information to make decisions on effective interventions and countermeasures the amount and variety of data available to facilitate high quality local scale air quality aq modelling have been increasing steadily over the past decade due to technological advancement and through open access initiatives such as inspire eu 2007 for instance sentinel 2 from the copernicus programme gascon et al 2014 provides open access satellite imaging and sentinel 5 veefkind et al 2012 provides information on the atmospheric composition of air pollutants globally the voluntary participation of citizens may also result in the accumulation of useful information one prominent example of this trend is the use of openstreetmap as a source for detailed land use information estima and painho 2013 yang d smith a c and yu q 2017 arsanjani et al 2015 detailed traffic emissions can nowadays be estimated by deriving road specific driving cycles using gps data from vehicles and by combining this information with vehicular emission factors borrego et al 2016 in harbour areas the impact of shipping can be modelled in near real time using online ais data e g huang et al 2020 moreover new information can be derived when these datasets are processed further or combined intelligently with data fusion as an example misra et al 2020 used sentinel 2 satellite data successfully with object detection methods to assess brick kilns around delhi india the amount and availability of aq measurement data have also substantially increased recently this is partly due to the introduction of online services that gather and readily provide measurement data an example of these services is the air quality in china service aqicn 2022 for global online measurements this service has been used e g for the mapping of pollutant concentrations in china rohde and muller 2015 another reason for the improved availability of measurement data is the wider use of low cost air quality sensors which can be deployed in large quantities to complement the reference quality network of stations kuula et al 2019 pet√§j√§ et al 2021 however the quality of sensor measurements can change over time and be affected e g by meteorological conditions pet√§j√§ et al 2021 clearly assimilation of sensor data in air quality modelling must consider this quality issue the most common approach in urban scale modelling including downscaling systems is to utilize multisource gaussian modelling however these models cannot explicitly resolve the influence of urban buildings and other obstacles on the dispersion of pollution various street canyon dispersion models such as the operational street pollution model ospm berkowicz 2000 address this issue by approximating the circular motions of air moving within the street canyons the performance of the ospm has been widely evaluated against experimental data e g in various cities in denmark e g jensen et al 2017 and in helsinki e g kukkonen et al 2001a b denby et al 2020 presented a downscaling procedure starting from the output of a regional chemical transport model ctm and downscaling this into a local scale 50 m using gaussian modelling principles another example of a downscaling approach has been presented for barcelona by benavides et al 2019 their approach was based on a coupling of a regional scale model with an urban scale one caliope urban caliope urban like many other models avoids double counting emissions from the regional scale model while resolving a higher resolution concentration field upwind of the grid cells provided by the regional model computational fluid dynamics cfd models are the most accurate methods for simulating the complex wind field and dispersion of air pollutants in urban areas but these methods are also the most resource consuming ones as an example a large eddy simulation les model e g hellsten et al 2020 can provide a sub meter grid resolution allow for the effects of buildings and canopy and such models can be used in nested domains the les models cannot yet be operationally used for a full city scale using the currently available computer technology however there are downscaling applications using les methods for a limited urban area and duration e g nuterman et al 2021 in addition to gaussian and cfd models there are several different modelling approaches used in urban dispersion modelling including eulerian e g karl et al 2019 and lagrangian methods rotach 2001 karl et al 2019 have used a combination of different types of dispersion modelling techniques and a more advanced treatment for photochemistry there are several urban air quality models that utilize data assimilation or data fusion of air quality measurements schneider et al 2017 presented a method for the generation of high resolution urban aq mapping by including dispersion modelling coupled with aq measurement driven geostatistical data assimilation the basis of the method is the universal kriging technique goovaerts 1997 using a so called base map e g a longer term average concentration field the limitations of this method include the need for a substantial amount of measurement locations and an accurate base concentration field similarly gressent et al 2020 used sensor data in data fusion to enhance urban air quality mapping for pm10 in nantes france the adopted data fusion method was kriging it requires a base map in the form of modelled annual average pm10 concentrations the used sensor data is a large collection of fixed and mobile measurements and the model therefore applies pre processing and filtering methods the aim of this paper is to present an operational multi scale air quality modelling system called enfuser environmental information fusion service the modelling system combines several datasets regarding online information on geography meteorology regional background concentrations aq measurements and local activity datasets the model provides adaptive hourly high resolution air quality modelling output for the general public decision makers and other end users the novelty of the presented system is the combination of multi scale dispersion modelling in urban scale operative service and data assimilation that is not based on kriging the adopted data assimilation method which also supports aq sensor data to be used facilitates physically meaningful i e a detected change in emission patterns persistent learning mechanisms to guide the modelling this kind of an operative model has not been presented previously the first objective of this paper is to present the mathematical model within the operational modelling system including the approaches to combining gaussian plume and puff modelling techniques with data assimilation the second objective is to present the technical solution to perform the high resolution computations at a moderate computational cost the third objective is to present a model evaluation in the helsinki metropolitan area hma in finland for two selected years the fourth objective is to present a non kriging based data assimilation approach for urban scale air quality modelling 2 materials and methods 2 1 overview of the modelling system enfuser is an operational local scale air quality model a combination of gaussian puff plume used in the helsinki metropolitan area in finland hma the model has also been used in foreign installation sites such as nanjing china and delhi india the current set of modelled pollutant species in hma includes nitrogen dioxide no2 ozone o3 fine particles pm2 5 and thoracic particles pm10 for which the model provides hourly average concentrations at a breathing height of 2m above ground the model also produces derivative output such as the finnish national air quality index aqi based on the primary modelling output i e the pollutant concentrations the output provided by the model is publicly available via fmi s open data portal fmi 2021 hourly updating aqi visualizations based on the model results are available at hsy 2022 but also shown in public transportation displays in hma a caption of the service is shown in fig 1 further there is also a separate service providing annual average information based on historical model data hsy 2021 modelling results are updated each hour each time including a now casting period with measurements 12h in the past and a forecasting period to the future 12h in a resolution of 13 13m covering an area of approx 40 30 km historically enfuser is a predecessor of the land use regression based model presented in johansson et al 2015 with respect to this previous work the aim of incorporating air quality measurements for improved urban air quality predictions has remained the same however the land use regression based approach has been replaced with a more realistic dispersion modelling approach a schematic presentation of the modelling system has been shown in fig 2 more thorough descriptions of the individual components presented in the figure are given in the following sections of the paper in brief the input datasets which can be separated into static and dynamic ones are shown at the top of the diagram the static input data is used for the description of the modelling area and local emission inventories often the available emission inventory data is not directly useful for local scale modelling and therefore the modelling system may need to refine or downscale emission inventories based on gis data the dynamic input consists of meteorological data both numerical and measured regional aq forecasts and aq measurements this set of input is continuously extracted from online sources the raw meteorological input is not sufficient for local scale dispersion modelling and pre processing of meteorology is therefore used the dispersion modelling operates in three different layers gaussian puff urban scale regional background regional scale and local scale gaussian plume modelling for the gaussian puff and plume modelling the underlying emission information is the same and the model must index and represent the emission release data in different resolutions simultaneously the meteorological conditions may also affect hourly release rates of emissions e g ambient temperature and emissions from residential wood combustion the dispersion modelling in the three separate layers is combined to form model predictions at measurement locations these predictions will then be used in data assimilation a procedure described in section 2 4 which results in a set of hourly adjustments that will help the model to obtain a higher degree of agreement at the measurement locations this process requires a light database for measurement location specifics including device quality ratings exact coordinates and the measurement height once data assimilation based adjustments have been assessed for a given hour and a given pollutant species the final model predictions can be computed for the overall modelling area each data assimilation outcome that occurs during the now casting period i e the period for which the air quality measurement data is already available will update a more gradual persistent set of adjustment factors pa the pa factors are considered also during the forecasting period that extends up to 12h to the future clearly no measured air quality data is available for this period at the time of computing the forecast when a new modelling task launches the previous state of pa is loaded as input when a modelling task is completed the updated pa is stored locally to be used for subsequent modelling tasks 2 2 input data for the model the input datasets required by the model can be separated into static and dynamic input datasets to set up new modelling domains e g a collection of cities in a selected country certain gis datasets need to be extracted from various online sources these extracted online gis datasets fig 3 are updated periodically e g annually however due to the low frequency of updates these gis datasets are referred to as static input for this gis data extraction and the preparation of static datasets there is an automatic procedure built in enfuser a dedicated sub program is used to access various online sources for dynamic input this sub program continuously extracts and archives recent data locally for all modelling areas that have been defined in enfuser the sub program is always kept active and provides the model with up to date input data without temporal gaps when the model is used for aq predictions this local archive of dynamic data is accessed in hma the dynamic information includes the following reference quality aq data from 12 measurement stations fmi 2021 regional scale aq forecasts from the chemical transport model silam sofiev et al 2015 harmonie numerical weather prediction nwp data with 1 1 km resolution for meteorology bengtsson et al 2017 and road weather measurements digitraffic 2021 additionally traffic flow measurements digitraffic 2021 and traffic congestion data are being extracted continuously however these additional information sources are used in supporting offline tasks 1 1 traffic flow count data together with traffic congestion data from www here com are used for quality controlling traffic flow patterns the model uses and do not yet contribute to the operational modelling outcome 2 2 1 geographical information system gis data enfuser has a built in capability of extracting and processing global open access geographic information for its modelling areas there are several use cases for this information e g during the dispersion modelling information on ground elevation urban structures and surface roughness length estimates are needed gis based derivatives are often needed as global coarse resolution inventories such as edgarv5 crippa et al 2019 cannot be used without post processing and downscaling in local scale modelling the openstreetmap raw data osm 2021 can be considered as the foundation for the gis datasets that are used to describe the modelling areas this provides the model with a detailed high resolution land use mapping that is especially useful for the description of road networks and the layout of buildings the modelling area is described with osm data in two layers the functional land use e g parking space and the material land use e g asphalt for buildings and roads the model uses an object oriented mapping that can e g describe the road name number of lanes speed limits etc another source of information is the global human settlement eu 2021 which provides additional land use and population data the ground elevation is extracted from nasa srtm and contributes to surface roughness estimations and height parameters used in dispersion modelling 10 10m 2 a description in finnish for the emme model can be found from visited february 1st 2022 https www hsl fi sites default files helsingin seudun tyossakayntialueen liikenne ennuste kysyntamallit 5 2019 pdf multi spectral satellite imagery is extracted from copernicus sentinel 2 msi these above mentioned sources can be regarded as the primary sources for gis data for the model and based on them additional derivative datasets are proxied one such derivative is shown in fig 3 the building height dataset the building height dataset is estimated based on the osm building characteristics which often specify the height of the building in meters or the number of floors however for many buildings this characterization does not exist and therefore the height can only be roughly estimated based on the building type and building surface area 2 2 2 emission data it has previously been found that the most important urban source categories in hma are residential wood combustion and vehicular traffic kukkonen et al 2018 and 2020 for shipping and harbour activities the contribution to the total concentrations of pm2 5 in the three year period 2012 2014 has been estimated to exceed 10 only in the vicinity of major harbours kukkonen et al 2018 the contribution of power production and industrial activities to local pm2 5 concentrations has been negligible in most parts of the area leisure boat emissions for the coastal area of hma can be estimated with the beam model johansson et al 2020 but these presumably negligible emission contributions have not been included in enfuser the basis for most emission inventories is a gridded annual or monthly emission total dataset for which a temporal profile to sequence emission release rates has been defined in addition there are a road traffic emissions based on osm road network structure coupled with vehicle flow counts and b shipping emissions high resolution dynamic emission inventory given by the steam3 model johansson et al 2017 and c power plants and factories treated as individual entities that are modelled as elevated point sources the most notable emission sources for hma have been illustrated in fig 4 road traffic emissions the road traffic emission modelling is based on the openstreetmap road network each piece of a road of which there are tens of thousands in hma s dataset is treated as a separate entity with characteristics such as speed limit road type number of lanes and surface material hourly vehicle flow patterns are attached to these road elements and then an emission release rate as ¬µg m2 s is computed based on the vehicle type road speed limit and the number of vehicles flowing through the road at the time of assessment there are three classes for vehicle types cars buses and heavy vehicles to define these emission factors flow speed relationships from kristensson et al 2004 and vehicle category specific unit emission factors from vtt 2021 have been used an averaged vehicle flow pattern with 72 hourly values 24 for monday to friday 24 for saturday 24 for sunday as shown in fig 4 is implemented for each road segment separately for each vehicle class in addition a weekly correction factor can be applied to address e g holiday seasons a dataset given by local authorities in helsinki helsinki region transport hsl and the city of helsinki using the emme 2 model has been used for defining these averaged traffic flow patterns this material describes averaged hourly flow counts as line data 30 000 lines the vehicle flow information characterized by the line data has then been implemented to the road network described by the osm data for each osm road segment a nearby emme line element is searched with matching orientation and speed limits for lesser roads for which a corresponding emme characterization is not found a statistical approximation is used instead based on gis data and a generic temporal profile obtained from nearby roads this procedure of implementing vehicle flow patterns from line input data into the osm road network is a supporting feature of enfuser and can be used also in other areas in addition to helsinki road dust emissions complex deterministic models have previously been developed for the evaluation of suspended dust from the road and street surfaces e g kauhaniemi et al 2011 denby et al 2013 kauhaniemi et al 2014 stojiljkovic et al 2019 however such models require a comprehensive set of input data for instance data on the road sanding and the use of studded tires such data is commonly not operationally available we have therefore applied a simpler statistical method for the evaluation of suspended dust the modelling of road dust and resuspension of particles is based on the data on the road network vehicle flow speed and hourly flow counts also the effects of road surface moisture the usage of studded winter tires and other meteorological factors are considered to assess the resuspension of coarse particles defined as pm10 pm2 5 road surface moisture estimates are obtained via digitraffic 2021 and a generic weekly profile for the usage of studded tires in hma is used power plant emissions power plants are treated as a collection of elevated point sources each being modelled solely with gaussian puff dispersion in enfuser generalized profiles have been created for coal oil and natural gas power plants that specify emission factors in terms of g mws with monthly diurnal variability the set of power plants nearby the modelling area is based on the global energy observatory 2021 which specifies the location type and capacity mw of the power plants in the hma there are 5 combined heat and power plants that are shown in fig 4 more specifically 1 suomenoja coal and natural gas 360 mw 2 salmisaari coal and wood pellets 160 mw 3 hanasaari b coal and biomass 220 mw 4 vuosaari natural gas 630 mw and 5 martinlaakso biomass and natural gas 197 mw shipping emissions shipping emission inventories for helsinki are provided by the steam3 model johansson et al 2017 using historic helcom ais data by the courtesy of the riparian states of the baltic sea and a vessel database from ihs fairplay the shipping emission inventory given by steam3 describes ships their positions time stamps emission release rates g s and emission release heights the detail at which shipping emissions are represented and delivered to enfuser facilitates the modelling of shipping emissions as moving point sources while considering the effective emission height however in operational use a less ambitious resolution of approx 100 100m in 5 min temporal resolution is used to represent shipping emissions residential wood combustion emissions an unprecedentedly detailed emission inventory for residential wood combustion rwc has been compiled in the hma kukkonen et al 2020 used this emission inventory to model deterministically the pm2 5 concentrations including rwc this study also evaluated the concentrations of pm2 5 and the related contributions of rwc in three other nordic cities viz copenhagen oslo and ume√• for a more detailed description of this inventory and its use in dispersion modelling the reader is referred to kukkonen et al 2020 the above mentioned emission inventory for rwc was also used in the present study the inventory has three components viz heating saunas and fireplaces each of which has different diurnal patterns and different dependencies on the ambient temperature clearly residential heating activities are increased during low ambient temperatures in winter especially for the heating component when the ambient temperature is lower than 14 c the modelling assumes that the emission factor for residential wood combustion is increased in proportion to the difference of the actual temperature and the reference value of 14 c the modelled temperature dependencies have been derived based on the monthly total emissions from rwc and the monthly average temperatures the emissions are presented with a resolution of 20 20m fig 4 and the release height of the rwc emissions has been assumed to be 10m 2 3 atmospheric dispersion modelling the model applies a multi scale approach a local scale assessment using gaussian plume modelling b urban scale assessment using gaussian puff modelling and c regional scale background extracted from the outputs of a chemical transport model the contribution of each of these three is assessed and combined at a selected receptor point rp to avoid double counting of emissions while combining gaussian plume and puff modelling the distances of emission sources from rp and the travel distances of gaussian puffs are considered the pollutant concentration c x t for a selected pollutant species prior data assimilation at location x and time t at the receptor point rp is given by 1 c x t n n k k c l e k t x n x p p c p p t x c b g x t where n is a set of incremental locations around the rp covering the surface area up to a selected maximum distance of d c u t k is the set of incremental emission sources at the location n for which the emission release rate in terms of ¬µg s for the selected pollutant species is e k t x n e k also characterises an emission release height m and an emission source category it is associated with e g traffic rwc etc the concentration at rp caused by e k in terms of ¬µg m3 is given by c l c p refers to the concentration evaluated using the gaussian puff approach at rp for which there are a total of p puffs to address during the evaluation time t each puff has a specific state p t that stands for e g the centre point location and the carried pollutant mass ¬µg which are needed for the assessment of c p the final term c b g is the regional background concentration excluding the influence of the local and urban emissions the size of the set n depends on the resolution in which the local emission information around the rp is represented and from the maximum assessment distance d c u t in the model the finest used resolution to represent incremental emission sources around rp is 5 5 m and the size of n is of the order of several thousand for each of these incremental areas n there can be multiple emission sources k but this is often limited to only a few such emission sources most often the set k is empty i e no emissions the parameter d c u t is the cut off distance at which the modelling approach switches from gaussian plume into gaussian puff modelling therefore the set p includes only the puffs that have travelled at least the distance of d c u t a low value of d c u t makes the modelling more agile i e can more realistically address rapidly changing meteorological conditions but requires that the puff modelling is performed in higher detail this will result in an increase of the computational cost considering the technical hardware limitations of the modelling configuration and based on a collection of simulations with different values for d c u t a value of 2000m has been selected to be used in the hma 2 3 1 gaussian plume computations following the work in seinfeld and pandis 2016 the gaussian plume solution with steady state meteorology totally reflecting ground is given by 2 c l q 2 œÄ u œÉ y œÉ z exp y 2 2 œÉ y 2 exp z h 2 2 œÉ z 2 exp z h 2 2 œÉ z 2 where c l is the concentration at rp q is the emission rate ¬µg s the standard deviation œÉ y defines the spread of the dispersion horizontally and œÉ z vertically variable u is the wind speed m s y is the horizontal perpendicular distance from the line aligned with wind direction h is the emission height m and z is the height of the receptor point above ground m vertical profiles for wind speed or diffusivity are not considered eq 2 does not address the influence of a finite boundary layer height or the settling velocity of particles the model however includes these effects by using extended versions of eq 2 for readability these have been presented in appendix a computationally efficient ways to implement these equations in practice have been presented in supplements 2 3 2 estimation of gaussian standard deviations there are several empirical approaches for the assessment of the gaussian standard deviations œÉ z and œÉ z mcelroy 1968 smith 1973 mcmullen r w 1975 the approach from briggs 1973 p 124 is adopted in which the standard deviations are estimated as a function of distance x m from the emission source along the wind direction but depending on a stability classification these empirical formulas predict standard deviations œÉ y x and œÉ z x defined separately for a discrete set of different atmospheric stability conditions a b c d e f ranging from highly unstable a to extremely stable f the stability class is selected based on the meteorological conditions mohan and siddiqui 1998 specifically the monin obukhov length based method is adopted traffic produced turbulence tpt can become a dominant cause for mixing and the dilution of pollutant concentrations nearby roads especially in low wind scenarios the inclusion of tpt in an urban scale model can be challenging in solazzo et al 2007 several tpt schemes have been evaluated in h√§rk√∂nen et al 1996 a semi empirical treatment is used by introducing additional standard deviation terms œÉ y 0 and œÉ z 0 as a function of wind speed and the angular difference between the road and the wind however in h√§rk√∂nen et al 1996 the vehicular flow speed is ignored whereas it is considered in solazzo et al 2007 in enfuser tpt is addressed simply by enhancing the diffusion of pollution by increasing the values of œÉ y x and œÉ z x as a function of traffic flow speed while the vehicle density is not considered for example the value of x for the assessment of œÉ x is increased by 10m for a motorway emission source with a flow speed of 120 km h in essence this introduces additional mixing in a similar way that the addition of œÉ y 0 œÉ z 0 does the used parametrization requires further research however the primary aim of this simplified approach is to remove outliers overprediction in the immediate vicinity of main roads and the simplistic parametrization achieves this 2 3 3 wind profiling approach the following method is used for the assessment of wind speed at the height of the emission source u h first the model takes interpolated data from the harmonie nwp model at a height of 10m u 10 this interpolation considers both spatial and temporal dimensions and therefore provides smoothly variating preliminary wind fields then u h is evaluated based on a log wind profile holmes and bekele 2001 3 u h u 10 l n h d 0 z e f f l n 10 d 0 z e f f u h 1 where d 0 is the zero plane displacement m and z e f f is the effective upwind surface roughness length m for the assessment of z e f f an upwind directed assessment of land use information is used and the land use information is used for the selection of a roughness length estimate more information on the wind profiling approach and the estimation of surface roughness is presented in appendix b 2 3 4 gaussian plume computations in practice a concentration field for the modelling area for a given pollutant species and time is estimated by applying eq 1 repeatedly by changing the location for the rp each time the assessment of the n surrounding incremental areas around the rp and their potential concentration contribution at rp if the set k is not empty is critical for the computational performance of the model to make this process efficient a method has been developed to reduce the number of areas assessed this way a gaussian plume shaped footprint that consists of a large set 1000 of incremental area cells has been shown in fig 5 the footprint has been designed to be rotated upwind in a single operation and define a reduced number of incremental areas for the assessment of the first term of eq 1 the coordinate system is like the one used with the gaussian plume computations but reversed the footprint is rotated to the upwind direction the origin x 0 y 0 is at the rp as shown in the figure the incremental area cells that form the footprint have different assessment resolutions the highest resolution is used for the cells that are close to the rp along the wind direction to facilitate the variable assessment resolution this way the emission information is also presented in different aggregation resolutions for example the cells with high resolution can access emission information at a high resolution and the cells farther away can access aggregated emission information that matches the coarser cell resolution better further the footprint s cells fixed geometry with respect to rp facilitates the use of tabled precomputed values for the assessment eq 2 this process and how the footprint has been formulated has been presented in supplements briefly put each cell can instantly access the set k wherever it has been rotated on top of for each emission source revealed this way their concentration contribution at rp can be estimated using tabled precomputed values excluding the term q u the net contribution of all incremental emission sources is aggregated at the rp as is described in eq 1 2 3 5 methods for considering the influence of buildings and street canyons part of the receptor points of modelling including several measurement stations are in complex urban environments such as in street canyons or the vicinity of high buildings the model surveys the nearby environment for urban obstacles e g based on the locations of buildings in the gis dataset for each receptor point the model subsequently archives the distances to various buildings and other obstacles and the height of each obstacle in each direction with an estimation radius of 100m based on the scanned obstacles around the rp the model then evaluates the additional effect on dispersion caused by them specifically for the computation of c l with the gaussian footprint method a limited number of area cells close to rp can be re rotated within urban street canyons the rotation angle in this empirical method that uses the assumptions presented in berkowicz 2000 is based on the street canyon geometry and ambient wind direction to put it simply the re rotation of the footprint within street canyons is to guarantee that the original surface area for local pollution in the street canyon is being accounted for regardless of the wind direction more realistic concentration predictions can then be given for both the leeward and windward sides of the canyon the strongest impact of this method is observed when the wind direction is perpendicular to the street canyon conversely the method has a negligible effect when the wind direction is parallel to the street canyon as a limitation the method cannot achieve homogeneous concentration fields within the street canyon in low wind speed scenarios as is assumed to occur berkowicz 2000 2 3 6 gaussian puff computations a gaussian puff approach is used to represent a single emission mass parcel that moves dynamically in the modelling area the concentration caused by a gaussian puff can be computed with stockie 2011 seinfeld and pandis 2016 4 c p q m 8 1 2 œÄ œÉ y œÉ z 3 2 exp x u t 2 y 2 2 œÉ y 2 exp z h 2 2 œÉ z 2 exp z h 2 2 œÉ z 2 where q m is the emission mass ¬µg of a pollutant species within the puff and t is the travel time however eq 4 does not support the non linear movement determined according to the location and time dependent wind field therefore the following refinements that extend the applicability of eq 4 are adopted first the travel distance m of each puff is given by u t d t and this is used for the computation of standard deviations œÉ y œÉ z second a substitution x u t 2 y 2 d 2 is used for eq 4 the term d 2 is the squared euclidean distance from rp to the current centre point location of the puff fig 5 after these refinements the concentration contribution assessment of a gaussian puff at rp allows the meteorological conditions imposed for the puff to be updated on a minute basis appendix b but the travel distance and centre point need to be dynamically tracked further the variables x and y have been eliminated from the equation and the substituting variable d can be assessed in a fixed coordinate system instead of a rotated one the tracking of the puff travel distance has another important use case the selection of puffs to be considered in eq 1 set p are determined based on their travel distances to avoid double counting of emissions the set p can therefore be formed by selecting the puffs for which the travel distance exceeds d c u t as with gaussian plumes extended equations are applied with gaussian puff modelling which considers the influence of a finite mixing layer height and the gravitational settling velocity of coarse particles appendix a the management of emission source information for the gaussian puff modelling is different than was presented for gaussian plume first the high resolution emission source representation is aggregated to much larger 500 500m sized cells and these cells are used to frequently release emissions into the atmosphere in the form of puffs for each puff emitted this way the source category or several categories information is maintained secondly selected emission sources such as power plants are treated as point emission sources and dispersion modelling is solely based on gaussian puff no spatial aggregation is applied for such point sources and these are not considered with gaussian footprint based approach the efficiency of assessment of eq 4 is important for the model s performance and the set of puffs that are being assessed for each rp needs to be kept as low as possible usually the number of puffs set p being modelled in hma at any given time is in between 100 000 to 300 000 in supplements several techniques for reducing the computational burden for gaussian puff modelling in enfuser have been presented 2 3 7 regional background the regional background is included by using hourly predictions from the operational chemical transport model silam sofiev et al 2015 clearly a double counting of emissions must be avoided previously various methods for that have been presented e g by denby et al 2020 and benavides et al 2019 the combined use of chemical transport models and urban scale dispersion models for a wide range of models has also been discussed by kukkonen et al 2012 the adopted technique for assessing the regional background concentration is comparable to an upwind assessment of the background but utilizes the gaussian puff modelling technique the silam predictions are assessed at the modelling area boundaries in which numerical background concentration markers are being continuously released and their movement tracked afterwards these markers that contain the regional background concentration values for all modelled species at the time of release will travel to from and within the modelling area according to the wind fields for assessing the regional background for an rp at any given time a inverse distance weighted kriging interpolated value of the markers found nearby the rp are used 2 3 8 chemistry correction to model predictions methods have been presented in the literature for including chemical reactions to gaussian models for instance kukkonen et al 2001a b and karppinen et al 2000 presented a simplified solution for including the basic reactions of nitrogen oxides oxygen and ozone to a gaussian plume model this system of equations can be solved analytically the chemical transformation equations were included in the dispersion model based on the so called receptor oriented discrete parcel method kukkonen et al 2001a b karppinen et al 1998 this method considers air parcels in which the emissions and background air are assumed to be instantaneously uniformly mixed the chemical reactions in each parcel are then assumed to proceed independently of the dispersion process regarding the transformation of nitrogen oxides the influence of hydrocarbons is important on regional and long range transport scales but commonly insignificant on the urban scales however their influence may be substantial in episodic conditions during prevailing stable atmospheric stratification and low wind speed and in case of recirculation of the air masses over a city in the present study it was assumed that most but not all e g 75 depending on emission source category of emitted nox emissions are initially no there is uncertainty in the ratio of primary no2 emissions and in addition the ratio is gradually changing caused by technological advancement most detailed information is available for road traffic in grice et al 2009 the ratio of primary no2 to nox is estimated to be above 10 in 2005 and projected to exceed 30 by 2020 within the european union during the atmospheric transport no is converted into no2 and the reactions involving o3 are responsible for most of the oxidation however the conversion of no to no2 can be limited by the amount of o3 at the location a solution to this is to make the model predictions at a specific order i e the prediction for o3 at the rp is done before predicting no2 and no then a post processed adjustment can be applied to the modelled no2 and no given that the o3 concentration at the rp can be given as input further it is assumed that no released this way is immediately converted into no2 simultaneously depleting o3 through oxidation by introducing a modelled virtual negative o3 emission in this step we base the conversions on the molecular masses of the species involved after the assessment of eq 1 it is possible to check whether the initial assumptions were correct and if not balance the modelled no no2 and o3 so that o3 is non negative as an example near major roads a large no2 concentration is initially modelled coupled with a negative virtual total o3 concentration after the balancing measure some part of the modelled no2 is transformed back to no finally to address the atmospheric lifetime of no2 mass decay functions are used for instance according to liu et al 2016 the mean lifetime for no2 power plants may september usa and china was derived to be approximately 4 h and based on this 30 of the no2 mass is reduced each hour such decay functions are only applied to gaussian puff modelling in which the state and movement of the emission parcels are updated dynamically 2 4 data assimilation the data assimilation method presented in this paper variates the hourly emission factors for the included emission source categories effectively making the modelling to be constrained by the observations in addition to emission factors there are other explanations for having differences between measured and modelled observations these include meteorological chemistry or dispersion modelling related reasons but these are not considered in the data assimilation a model prediction for any pollutant species given by eq 1 can be expressed as the sum of its category specific contributions since emission source category distinction is maintained for both the gaussian puff and plume modelling c x t s s c s x t for a total of s emission source categories in which the regional background has been included the term c s x t for example stands for concentrations caused by traffic shipping or rwc at location x during time t an adjusted model prediction that is affected by recent and historic data assimilation results is given by 5 c x a t s s c s x t c s x t a s a s c s x t s b g c b g x t a b g a b g c b g x t c b g 0 where a a 1 a 2 a s is a set of hourly adjustments for emission source categories and a s is the persistent adjustment for category s the term persistent adjustment is used since a s encapsulates the outcome of many previous hourly adjustments and its effect to eq 5 persists even when no measurement data is available e g during forecasting in such cases when no measurement data is available a s 1 a b g 0 the hourly adjustment set a is optimized during data assimilation as a free parameter for each modelling hour and separately for each pollutant species the value of a s is considered constant for the estimation of eq 5 but each optimized a s will slightly update and modify the value of a s for subsequent modelling hours this update operation also considers the time of day and the day of week i e if an hourly adjustment a s for a particular local time is consistently high or consistently low then the persistent adjustment a s for that local time gradually adapts to match this behaviour more information is given in supplements the weighted squared error wse between an adjusted model prediction and the observation o m t is w m o m t c x m a t 2 where the weight factor w m relates to the measurements m reliability and variability the aim of the data assimilation is to minimize the weighted sum of squared errors wsse across all the measurements for the same pollutant species at a given hour this formulation is like the one in weighted least squares technique wls kiers 1997 in wls in which the best linear unbiased estimator is solved to minimize wsse the inverse variance of the measurement is often used for w m w m 1 œÉ 2 in this approach œÉ 2 is being estimated based on a fixed quality rating for the measurement device considering the hourly variability of measurements from the measurement source johansson et al 2015 the wls technique however is not adopted for solving in enfuser strict limitations for possible states for a need to be enforced e g minimum and maximum values for a s the system due to eq 5 is not easily expressed as linear expressions either further due to meteorological conditions infrequent calibration and technical issues w m needs to be treated as a free parameter that is also optimized during data assimilation for m measurements in m locations for a certain pollutant species the optimization problem for minimizing the wsse is defined as follows 6 m i n w s s e a p w s s e a p f p m 1 m p m w m o m t c x m a t 2 where p p 1 p 2 p m p m 0 1 is a factor that reduces the weight assigned to the wse for measurement m and f p is a penalty function smith and coit david 1996 that increases wsse for each p m 1 the function f p acts as a countermeasure when the values of p m are being reduced any attempt to minimize w s s e a p without incorporating a penalty function would include the trivial minimization of the values of p m an effective penalty function forces the algorithm to search for a balanced compromise of optimal adjustments to emission factors a while keeping p m collectively as high close to 1 as possible with this in mind we define 7 f p p m b where a value of 1 is used for b b 0 several other formulations for f p have been tested the suggested one is currently the simplest well performing parametrization found based on numerical test simulations 2 4 1 discrete descent algorithm for searching the optimum the optimization task of eq 6 is a nonlinear optimization problem ruszczynski 2011 the problem can be simplified by discretizing the variables however no completely satisfactory method exists to solve it even in that case olsen and vanderplaats 1989 gradient descent methods for which an overview is presented in ruder 2016 are one of the more popular approaches to perform this kind of optimization the logic is simple one can slide towards the minimum by moving to the opposite direction of the gradient of the objective function step by step in case the system is convex the algorithm terminates at a global minimum otherwise the potential issue of getting trapped in a local minimum must be considered a discrete gradient descent algorithm is used in enfuser to find optimal parametrization for a p that minimizes wsse starting from an arbitrary initial state fig 6 and step by step descending to optimum in terms of w s s e the possible states for each a s p m have been discretized for emission source adjustments each a s are allowed to have a finite number of states e g from 0 5 to 1 5 with a step size of 0 1 the discretized possible states for each p m include 1 0 6 0 4 0 2 0 1 0 05 in the descent algorithm the direction and step length offering the most impactful reduction to wsse are searched and taken once there are no directions left to reduce wsse the algorithm terminates to address the potential issue of terminating at a local minimum the following techniques are used first attempts are made to leap over the minimum with significantly larger step sizes when the algorithm is about to terminate secondly the iteration can be repeated using multiple different initial states and then select the optimal state out from several candidates the optimal state a that minimized wsse is used to update a s procedure has been described in supplements the computational burden of the data assimilation algorithm can be considered negligible with respect to the gaussian dispersion computations even with a high number m 100 of measurement data being included therefore the method supports the use of aq sensors in large quantities 3 results the model was applied for the prediction of hourly pollutant concentrations for no2 o3 pm2 5 and pm10 in the hma during 2017 and 2018 depending on pollutant species there are up to 12 aq measurement stations in hma of which several are relocated each year all model configuration parameters such as the minimum wind speed and minimum mixing layer height have been set as presented in the methods section with appendices the station locations are shown in fig 7 together with annual average concentrations for no2 2017 the geographical annual averages presented in this study have been averaged from modelled hourly concentrations for the same area further the modelling of each hour involves urban scale gaussian plume modelling minute by minute gaussian puff modelling incorporation of regional background and data assimilation the annual computations have been performed using the operational system architecture i e proceeding chronologically from january to december in 24 h modelling task batches completing a modelling task that covers a full year this way takes approximately three days of processing time with a modern 3 3 time estimate is based on data production rate for hma using 12 core 6 61 broadwell 2400 mhz cpu with 32 gb of memory pc the contribution of road traffic is dominant for no2 in hma but the concentrations near main roads are often limited by the availability of o3 the regional background is approx 7 Œºgm 3 across the modelling area and shipping contributes up to 5 Œºgm 3 the contribution of power plants is minor also for other pollutant species than no2 in fig 8 the annual average pm2 5 concentration for 2017 is shown with emission source contributions the most notable source is the regional background not shown approx 4 Œºgm 3 across the modelling area the contribution of traffic is up to 2 Œºgm 3 near the busiest roads in hma the modelled contribution of rwc emissions is lower than 1 Œºgm 3 the contribution of shipping for pm2 5 in 2017 is the highest near the helsinki central area coastline but lower than 0 5 Œºgm 3 the corresponding annual modelled average concentrations for pm10 and o3 are shown in fig 9 the annual average concentrations of pm10 and pm2 5 are naturally related the most notable difference being the added resuspension of particles that increases concentrations near roads for o3 there is no other source for pollution other than the regional background however lower annual average o3 concentrations can be observed nearby traffic and shipping emission sources due to adopted nox o3 chemistry 3 1 the evaluation of the model against measured data the model evaluation was performed using the leave one out cross validation procedure for each measurement site the model predictions were done excluding the values at that site during data assimilation due to model development measurement availability and model maintenance during 2017 and 2018 the evaluation time series does not cover a full two year period and there are some notable gaps in between the measurements used for the evaluation are uncorrected real time data extracted and archived as soon they have become available online this means that the air quality measurements have not yet been subject to the more stringent qa qc procedures and may contain outlier data for most stations participating in the evaluation there are more than 7500 hourly measurements available for comparison annually for all sites a measurement height of 3m above ground has been assumed using the exact coordinates as shown in table 2 in this study several standardized statistical measures were used to quantify the modelling performance these measures have been explained in the supplements in terms of fractional bias fb and normalized mean squared error nmse the model prediction accuracy has been presented in fig 10 for each station and species for 2017 and 2018 further details have been shown in tables 1 and 2 as it can be seen from fig 10 for o3 and pm2 5 most of the stations are within the target limits with respect to nmse and fb nmse 0 5 and 0 5 fb 0 5 however for no2 and pm10 there are a couple of outlier stations that cause the nmse and f2 to exceed these target values an evaluation of predicted monthly averages against observed ones have been shown in fig 11 in terms of correlation pcc the highest ones are obtained for no2 0 908 for 2017 and 0 927 for 2018 the second highest correlation is obtained for o3 previous deterministic model evaluations for pm2 5 in hma have been presented by kukkonen et al 2018 and 2020 for the years 2004 2014 most of the stations are the same as those used in this study over this assessment period the average f2 was approximately 0 68 and the index of agreement ia was 0 66 for the deterministic modelling system for comparison the average enfuser predictions for 2017 and 2018 for f2 were 0 84 and for ia these were 0 83 as expected the evaluation measures computed in this study are better than the corresponding ones obtained using solely deterministic methods 4 discussion the spatial distribution of annual average concentrations for 2017 was presented in the study the corresponding annual averages for 2018 were not shown as these are like the ones obtained for 2017 several outlier stations can be observed in fig 10 when model predictions are compared to measurements in terms of nmse and fb these outlier stations have been grouped as follows outlier 1 stands for an urban traffic station lep situated in between a busy outdoor parking area and a road for this site the model underpredicts pm10 concentrations especially during spring road dust events outlier 2 is a rural background station luu that provides valuable information for the model for regional background concentrations the added value comes from the fact that the model predictions at this rural background station can effectively be variated by adjusting a b g and the model prediction at luu is not sensitive to other a s when this measurement data source is being omitted in the leave one out validation procedure a collection of suburban and urban stations remains from the data assimilation method s perspective after the omission of luu there is no measurement data available to provide a pure signal for the regional background further for no2 the modelling results for luu are the poorest a clear over prediction can be observed especially when the wind direction is from south and south east with these wind directions the gaussian puff modelling carries no2 pollution to luu which in turn suggests that the used decay rate for no2 may be underestimated outlier group 3 is a collection of challenging urban suburban locations for pm10 modelling of which the hardest one to predict is an urban street canyon station mak for this group the nmse target is exceeded but the f2 and fb targets are met nearby construction work has also been suggested as one of the causes for high measured pm10 concentrations outlier 4 is an urban background station smear3 which provided very low concentration measurements for no2 from january to march 2017 these measurements were later confirmed to be erroneous by the data provider after mid july this station returned online after being offline for 3 months this kind of conflicting measurement input with respect to other measurement evidence however can be dealt with by the data assimilation method as is shown in fig 12 monthly average concentrations and average weight reduction factors for eq 6 are shown for the smear3 station in the figure in a modelling test run where every station participates in data assimilation in this modelling test from january to march the data assimilation algorithm often terminates at states that strongly reduce the weight put on smear3 measurements the only effective way to reduce the squared error se at smear3 would be to reduce a b g however such an adjustment would increase se in all other measurement locations simultaneously therefore a reduction of a b g does not occur at the optimum where data assimilation terminates but a reduction of weight put to the erroneous smear3 measurement occurs instead outlier 5 is a coastal measurement site near a shipping terminal e sat particularly the prediction of hourly no2 is challenging there especially due to berthing ships nearby for which the emissions of auxiliary engines are difficult to predict by the steam3 model the road traffic profiles near e sat seem also to be underestimated as they do not account for the additional traffic induced by the nearby shipping terminals similarly in 2017 one of the measurement stations was situated at the helsinki airport len while nmse and fb are acceptable for this location the model clearly under estimates no2 one possible explanation for this is the significant amount of taxi traffic with mostly diesel engines being used nearby the station which has not been accounted for in the vehicular traffic flow mappings in the area in fig 11 the predicted monthly average concentrations against measured concentrations were shown for 2018 high correlation of 0 837 is obtained for pm2 5 but a lower correlation is obtained for 2017 this is likely due to higher monthly variability for the average pm2 5 concentrations in 2018 and this variability is successfully being captured by the model between january and march 2018 the average temperature is more than 3 c lower than in 2017 which in turn results in higher emission output from the residential wood combustion sources during the winter months the lowest correlations are obtained for pm10 especially during spring 2018 the monthly variability is strong and there are notable differences from station to station due to road dust in hma the modelled regional background concentrations as given by the ctm for ozone have a clear seasonal bias during spring the concentrations are underestimated by the ctm the adopted data assimilation procedure can correct the regional background to more appropriate levels namely a b g for ozone begins to rise during spring reaching 5 Œºgm 3 after summer a b g reduces gradually to a value close to 0 Œºgm 3 for other modelled pollutant species such a distinguished seasonal behaviour is not observed for example a b g for no2 varies in between 1 5 and 0 5 Œºgm 3 throughout 2017 and 2018 without clear seasonal patterns occasional overfitting by the data assimilation procedure can also be observed for several pollutant species during winter months no2 traffic emissions are being scaled upwards and downwards during summer modelled pm10 background is being increased by pa during spring time road dust episodes fortunately these occurrences of overfitting can be analysed and used to guide further model improvement for example the adjustments made by the data assimilation method can be analysed in terms of conditional averages i e as a function of meteorological variables such as wind speed boundary layer height and ambient temperature if the data assimilation systematically assigns strong adjustments in specific atmospheric conditions then this is a clear indication of overfitting 4 1 limitations the key sources of gis information are extracted from openstreetmap however the quality of this information varies between countries in some areas such as in asia additional supplemental gis information may be needed further information in addition to the gis data is commonly needed to reliably evaluate traffic flow patterns the model has limited capability to address atmospheric chemistry the model currently includes solely a simplified scheme for the nox ozone chemistry a more comprehensive treatment of the chemical and physical transformations would be needed for geographically more extensive modelling domains and in case of more complex meso and microscale meteorological processes e g recirculation of air masses over a city while the data assimilation method provides a mechanism for potentially improving the emission strengths of the local emission modelling the method cannot correct any spatial heterogeneous inaccuracies within the used emission inventories the model also cannot explicitly correct for any inaccuracies in the dispersion modelling and the evaluation of other properties of the emission sources for instance the release height estimated for the emissions from residential wood combustion the modelling can in some cases result in an over fitting to the measured air quality data due to the adopted data assimilation procedure for example during spring road dust episodes the modelling may excessively increase the regional background concentrations to counterbalance the lack of sufficiently realistic modelling for dust accumulation and resuspension in addition the modelling requires the data of an air quality measurement network in the modelling area the data assimilation method can function effectively only if this measurement network includes a wide range of stations in different environments accurate coordinates and measurement heights are also needed for each station or sensor which will be included in the data assimilation the gaussian dispersion modelling methods cannot address explicitly either the effects of buildings and other obstacles or the effects of complex terrain such as mountainous areas valleys and fjords the modelling has applied simple methods for estimating gaussian standard deviations atmospheric stability classes and urban micrometeorology and upwind roughness length these simplifications were needed partly due to the computational efficiency requirements and the need to support offline pre computations for gaussian dispersion modelling 4 2 future development topics the modelling of pm10 could be improved in the future especially regarding the modelling of road dust during spring this development work could include the use of machine learning methods neural networks or random forest models machine learning methods may also be adopted in the future to assist in the modelling of traffic flow patterns within the osm road network provided that enough traffic flow measurements will be available for training such models detailed traffic flow datasets such as the ones used in the helsinki metropolitan area may not be available for other urban areas the operational modelling service for hma will be improved by extending the forecasting period up to 24h in the future black carbon and the lung deposited surface area ldsa of particles will be included in the modelling measurement data is available for both of these pollutant species in the helsinki region in addition an existing complementary network of aq sensors will be utilized finally the model could continuously use the predicted near real time shipping emissions instead of historic annual emission data to facilitate this the steam3 model will be coupled with a continuous supply of ais data for the modelling area 5 conclusions we have presented a multi scale air quality modelling system called enfuser the model uses a combination of gaussian plume and puff modelling techniques incorporates regional scale modelling and utilizes data fusion and aq measurement based data assimilation this novel modelling system has provided open access information on air quality for the general public continuously since march 2018 clearly the model can be used also for other regions in addition to the hma the model takes advantage of both a global open access source for gis information and various time dependent dynamic input datasets the extracted gis data and the time dependent datasets are used in the model in various ways including a the refinement and downscaling of emission inventories b description of urban buildings and other obstacles for local dispersion modelling c the assessment of surface properties for the processing of meteorological information and d the detailed modelling of vehicular flow patterns and the emissions of road traffic the model can in some cases be used with reasonable accuracy even in regions for which reliable emission inventories are not available due to the data fusion and data assimilation capabilities however some information sources used in this study are not available at all locations internationally to guarantee the transferability of the model to another city than helsinki the following key datasets and sources of information should be available first there should be an aq measurement network that provides measurements for each modelled pollutant species there should be several air quality measuring stations and preferably the stations should be distributed in different measurement environments e g one or more in rural background areas and several in urban areas in case the number of stations in the area is low a complementary network of sensors may be needed if the measurement data is not accessible from global open source data portals then a timely access to the measurement data needs to be provided second there should be information available to support the customization of traffic flow patterns in the road network possibly in the form of a previous study or a collection of traffic count data third in case there are special local emission sources in the area e g industrial areas that are known to substantially affect the air quality then emission inventories for these sources should be provided fourth the current provider of harmonie nwp data for enfuser covers only the baltic sea region and a different source for meteorological input is needed elsewhere in addition to these necessary sources of information there are certain lower priority datasets that may be provided to improve the modelling quality but are not considered mandatory for example a building height dataset improves the modelling capabilities in view of urban micrometeorology and can be used to address the concentrations in urban street canyons a novel data assimilation method was presented to support urban scale modelling this method variates the local emission factors under the constraints of the measured air quality information the method can continuously adjust the modelling of emissions over time it will adapt to the changes in the temporal patterns of emissions and the regional background concentration the method is computationally efficient and can be used for the simultaneous assimilation of large amounts of measurement data the model was evaluated in this study in a now casting mode using a two year evaluation dataset for the helsinki metropolitan area during 2017 and 2018 the modelling accuracy can be considered good in terms of monthly averages there was a high correlation pcc for the predicted and measured concentrations for no2 0 908 and 0 927 and o3 0 855 and 0 901 for both years in the case of pm2 5 the achieved correlation for 2018 was 0 837 although the correlation for monthly averages for no2 was high the model on the average systematically overestimated the relatively lower measured no2 concentrations and underestimated the higher no2 concentrations in the case of the pm10 concentrations there was a relatively lower correlation pcc in terms of monthly averages for both years 0 801 and 0 738 in terms of hourly evaluation and specifically in terms of fractional bias and normalized mean squared error the best modelling performance was obtained for pm2 5 and o3 while the model performance measured for pm10 was relatively worse the model performance measures were on the average clearly better compared with those found in previous studies in this region using deterministic modelling without data assimilation author contributions lj is the lead author of the article and the developer of the enfuser model lj also prepared results using the model ari k mk jn anu k and jk internally reviewed and contributed to the writing of the article jn and anu k provided emission inventory input for the model with regard road traffic and rwc software and data availability the enfuser model developed by the lead author has been written in java more than 75 000 lines of code during the years 2012 2022 the model at its earlier stages of development has been presented in johansson et al 2015 enfuser has been designed to be used in a modern pc e g 8 core cpu and 32 gb of ram or in a virtual environment both linux and windows are supported the model utilizes parallel computations multi threading and therefore benefits from the availability of a high number of processing cores data produced for the helsinki metropolitan area by the model is publicly available via the open data portal of fmi fmi 2021 source code for the enfuser model is publicly available via github under the mit license https github com johanssl enfusermit git the repository contains the necessary code and input data for operative modelling of air quality in the helsinki region declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was financially supported by the projects uia hope healthy outdoor premises for everyone project no uia03 240 smart clean haqt helsinki air quality test bed business finland cityzer services for effective decision making and environmental resilience the accc flagship the atmosphere and climate competence centre academy grant no 337552 this work has also received funding from the european union s horizon 2020 research and innovation program under grant agreement no 874990 emerge project appendix c supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105460 appendix a application of dispersion modelling formulas the extended gaussian puff and plume formulas that are utilized separately for gaseous species and pm2 5 finite mixing height adjusted form and coarse particles settling velocity adjusted form are presented in this appendix the formulas and notations from stockie 2011 are followed in which variables r y œÉ 2 y 2 r z œÉ 2 z 2 and r r y r z for the expression of the extended formulas eq 2 is written as a1 c l q 4 œÄ u r y b y exp y 2 4 r y b exp z h 2 4 r z exp z h 2 4 r z the gaussian plume formula does not account for a finite mixing layer height fmh for the pollutant concentrations which can range from several kilometres down to several tens of meters extended gaussian plume formula that considers fmh is presented e g in stockie 2011 and is given by a2 c l m q u d œÄ r y b m b m 1 2 n 1 c o s n œÄ z d c o s n œÄ h d e x p r z n œÄ d 2 where d is the value of fmh in meters low value for fmh forces the pollutant concentrations to mix into a smaller volume of air causing larger ground level concentrations due to this sensitivity of modelled concentrations with low values of fmh a minimum value of 100m is imposed for assessing b m numerically the infinite trigonometric series needs to be cut into a finite number of n terms the choice of n has clearly a notable impact on the computational cost of solving eq a2 however when using a low value of n e g n 10 a clear bias in the concentration distribution can be seen close to the emission source on the other hand close by the emission source c l eq 2 is an accurate approximation for eq a2 especially when d is large by taking advantage of these properties a cost efficient solution for assessing c l m can be summarized as follows as a function of x d the computation method switches from c l to c l m x is the distance to the emission source along the wind direction o if x d 2 case 1 assess eq a2 with 3 first terms in the series o if x d 1 case 2 c l is used to approximate eq a2 o otherwise use a linear combination of cases 1 and 2 in trivial cases in which the emission source and the receptor point are separated by the fmh a value of 0 is used regardless of x d this solution of combining c l and c l m cost effectively is referred to as the hybrid solution and has been illustrated in fig a1 it is used for gaussian plume computations for gaseous species and pm2 5 settling velocity to take the effect of particle settling velocity into account the ermak formula 4 4 the ermak solution has been presented in stockie 2011 at section 3 6 p365 is used given by a3 c l e q 4 œÄ u r y b e 1 e 1 e x p w s e t z h 2 k w s e t 2 r 4 k 2 a value of 1 m2 s is used for eddy diffusion coefficient k and the settling velocity w s e t is approximated with stoke s law for typical coarse particles the original ermak solution supports for deposition velocity to be included however this has been assumed to be zero deposition effects may still be included in the form of mass decay functions in the model gaussian puff solution extensions following eq a1 and that the distance d from the receptor point to the centre of the puff is x u t 2 y 2 eq 4 can be written as a4 c p q m 8 œÄ r 3 2 y p b y p e x p d 2 4 r y considering that when d is large then c l c l m and thus b 4 œÄ r d 1 b m based on this an extended gaussian puff solution formula that considers finite mixing height when d is large is approximated by a5 c p m q m 2 œÄ r d b m y p it can be numerically verified that for a continuous stream of incrementally small puff releases the use of eq a5 for c p m yields a concentration field that corresponds to the field given by c l m regardless of the value of d one such comparison has been shown in fig a1 using d 180m further in analogy to gaussian plume computations the gaussian puff computations can also be performed using a cost effective hybrid solution the concentrations for puffs are computed based on eq a5 only when x d is low x corresponding to the travel distance of the puff finally the gaussian puff solution formula that considers particle settling velocity is given by a6 c p e q m 8 œÄ r 3 2 y p b e 1 fig a1 comparison of concentration fields as given by the different modelling approaches using a single elevated unit emitter 1 g s as a source in each example the vertical axis corresponds to z 0 300m and the horizontal axis corresponds to x downwind distance from the emission source stability class c has been used a very high settling velocity has been used for demonstrative purposes fig a1 appendix b wind profiling the available meteorological input for the model is harmone nwp hourly data with a resolution of 1 1 km for the height 10m above ground this data is currently available only for the baltic sea region and in other modelling areas a different often with lower resolution is used for gaussian modelling in enfuser for both plume and puff modelling the meteorological input is downscaled to match the needs of the multi scaled modelling approach and this process has been outlined in fig b1 fig b1 an illustration of the wind profiling methodology adopted in enfuser nwp wind speed and direction are interpolated spatially and temporally and used as input 6 00 and 7 00 refer to the local time of day used for the shown example for helsinki region upwind roughness length url estimates are produced based on the wind direction and gis data for the area wind speed profiles as a function of height are then estimated based on interpolated meteorology and url and this assessment is done in different scales to match the use case they are used for u 10 and u 2 refer to profiled wind speed at 10m and 2m above ground fig b1 the first stage of the downscaling process is to obtain an interpolated wind speed and direction values for a given location at a given time based on the nwp data these are obtained with spatial temporal interpolation from the raw nwp data spatially an inverse distance weighted kriging is used a linear interpolation is used for time turning of wind direction as a function of height is not considered the second stage involves the estimation of upwind roughness length url or z e f f the assessment needs to be wind directed as measured turbulent fluxes can be assumed to depend on an upwind source area at the typical modelling heights used in urban scale modelling schmid and oke 1990 schmid 1994 rooney 2001 like the approach presented in hammond et al 2012 an upwind fetch distance of 500m is used and z e f f assessed this way is taken as the average value of multiple roughness length estimates z i encountered within the defined area of assessment the area of assessment is set to be a narrow cone 6 covering the area upwind up to the selected maximum distance of 500m the distance of each z i for the location of z e f f is considered we estimate z e f f as a simplistic distance weighted average given by c1 z e f f w i z i w i w i 1 d where d i is the distance of z i to the assessment location for z e f f for the estimation of z i the available gis datasets are used to define a closest matching davenport classification wieringa 1992 the resolution used for this assessment is 5 5 m2 that matches the resolution in which openstreetmap data is described for the modelling area using this resolution z e f f gives estimates to be used in gaussian plume modelling with the footprint based approach however this scale is not suitable for the gaussian puff modelling for which further averaging up to 100 100m is done technically the assessment of z e f f is computationally costly and therefore z e f f values for the modelling area have been computed and archived prior operational use for each wind direction in 10 degree intervals the assessment of z i involves the gis datasets shown in fig 3 e g openstreetmap but requires complementary information as well such as sentinel 2 based estimates for vegetation coverage details are not presented here generally speaking the approach yields url values of 1 2 at urban areas and provides low values 0 0002 for sea areas and lakes due to the upwind directed approach however low url values are also obtained for roads when they are aligned parallel to the wind which therefore impacts the modelling of e g street canyons the third and final step of wind profiling involves the use of eq 3 to obtain a wind speed estimate for emission sources using their effective release height above ground further the meteorology for each gaussian puff being modelled is updated frequently once every minute or for every 100m travelled by the puff since the gaussian dispersion equation is inaccurate for low wind speeds a minimum wind speed of 1 m s has been defined as an additional simplification and to guarantee non zero wind speeds at low heights above ground a value of 0 is used for zero plane displacement d 0 the gaussian dispersion formulas for the estimation of c l eq 2 do not address the influence of the change of the wind velocity with height between the emission source and the observation point limitations for the wind profiling approach as noted in hammond et al 2012 the estimation of z i is challenging in real world conditions even the definition for the surface area that is assessed in the aggregation is complicated the use of a nested set of different elliptical upstream areas is suggested schmid and oke 1990 therefore the method presented here can be considered as a crude approximation the performance of the presented method needs to be further developed and evaluated against wind measurements additionally comparison against les modelled wind fields hellsten et al 2020 may provide useful information for further development 
25542,an operational urban air quality model enfuser based on dispersion modelling and data assimilation lasse johansson a ari karppinen a mona kurppa a anu kousa b jarkko v niemi b jaakko kukkonen a c a atmospheric composition research finnish meteorological institute helsinki finland atmospheric composition research finnish meteorological institute helsinki finland atmospheric composition research finnish meteorological institute helsinki finland b helsinki region environmental services authority hsy ilmalantori 1 fi 00240 helsinki finland helsinki region environmental services authority hsy ilmalantori 1 fi 00240 helsinki finland helsinki region environmental services authority hsy ilmalantori 1 fi 00240 helsinki finland c centre for atmospheric and climate physics research and centre for climate change research university of hertfordshire college lane hatfield al10 9ab uk centre for atmospheric and climate physics research and centre for climate change research university of hertfordshire college lane hatfield al10 9ab uk centre for atmospheric and climate physics research and centre for climate change research university of hertfordshire college lane hatfield al10 9ab uk corresponding author an operational urban air quality modelling system enfuser is presented with an evaluation against measured data enfuser combines several dispersion modelling approaches uses data assimilation and continuously extracts information from online global open access sources the modelling area is described with a combination of geographic datasets these gis datasets are globally available with open access and therefore the model can be applied worldwide urban scale dispersion is addressed with a combination of gaussian puff and gaussian plume modelling and long range transport of pollutants is accounted for via a separate regional model the presented data assimilation method which supports the use of aq sensors and incorporates a longer term learning mechanism adjusts emission factors and the regional background values on an hourly basis the model can be used with reasonable accuracy also in urban areas for which detailed emissions inventories would not be available due to the data assimilation capabilities graphical abstract image 1 keywords air quality dispersion modelling data assimilation 1 introduction air pollution is a major environmental concern in many areas worldwide having a strong impact on public health and the economy world health organization 2016 guerreiro et al 2014 the spatial variability of air quality is especially pronounced in urban environments it is common for e g european cities to report exceedances of annual air quality limit values at urban monitoring stations eea 2018 therefore high quality up to date information on local air quality is needed to make it possible for citizens to assess their personal exposure to air pollutants and make informed decisions on e g their commuting options further local authorities also require such timely information to make decisions on effective interventions and countermeasures the amount and variety of data available to facilitate high quality local scale air quality aq modelling have been increasing steadily over the past decade due to technological advancement and through open access initiatives such as inspire eu 2007 for instance sentinel 2 from the copernicus programme gascon et al 2014 provides open access satellite imaging and sentinel 5 veefkind et al 2012 provides information on the atmospheric composition of air pollutants globally the voluntary participation of citizens may also result in the accumulation of useful information one prominent example of this trend is the use of openstreetmap as a source for detailed land use information estima and painho 2013 yang d smith a c and yu q 2017 arsanjani et al 2015 detailed traffic emissions can nowadays be estimated by deriving road specific driving cycles using gps data from vehicles and by combining this information with vehicular emission factors borrego et al 2016 in harbour areas the impact of shipping can be modelled in near real time using online ais data e g huang et al 2020 moreover new information can be derived when these datasets are processed further or combined intelligently with data fusion as an example misra et al 2020 used sentinel 2 satellite data successfully with object detection methods to assess brick kilns around delhi india the amount and availability of aq measurement data have also substantially increased recently this is partly due to the introduction of online services that gather and readily provide measurement data an example of these services is the air quality in china service aqicn 2022 for global online measurements this service has been used e g for the mapping of pollutant concentrations in china rohde and muller 2015 another reason for the improved availability of measurement data is the wider use of low cost air quality sensors which can be deployed in large quantities to complement the reference quality network of stations kuula et al 2019 pet√§j√§ et al 2021 however the quality of sensor measurements can change over time and be affected e g by meteorological conditions pet√§j√§ et al 2021 clearly assimilation of sensor data in air quality modelling must consider this quality issue the most common approach in urban scale modelling including downscaling systems is to utilize multisource gaussian modelling however these models cannot explicitly resolve the influence of urban buildings and other obstacles on the dispersion of pollution various street canyon dispersion models such as the operational street pollution model ospm berkowicz 2000 address this issue by approximating the circular motions of air moving within the street canyons the performance of the ospm has been widely evaluated against experimental data e g in various cities in denmark e g jensen et al 2017 and in helsinki e g kukkonen et al 2001a b denby et al 2020 presented a downscaling procedure starting from the output of a regional chemical transport model ctm and downscaling this into a local scale 50 m using gaussian modelling principles another example of a downscaling approach has been presented for barcelona by benavides et al 2019 their approach was based on a coupling of a regional scale model with an urban scale one caliope urban caliope urban like many other models avoids double counting emissions from the regional scale model while resolving a higher resolution concentration field upwind of the grid cells provided by the regional model computational fluid dynamics cfd models are the most accurate methods for simulating the complex wind field and dispersion of air pollutants in urban areas but these methods are also the most resource consuming ones as an example a large eddy simulation les model e g hellsten et al 2020 can provide a sub meter grid resolution allow for the effects of buildings and canopy and such models can be used in nested domains the les models cannot yet be operationally used for a full city scale using the currently available computer technology however there are downscaling applications using les methods for a limited urban area and duration e g nuterman et al 2021 in addition to gaussian and cfd models there are several different modelling approaches used in urban dispersion modelling including eulerian e g karl et al 2019 and lagrangian methods rotach 2001 karl et al 2019 have used a combination of different types of dispersion modelling techniques and a more advanced treatment for photochemistry there are several urban air quality models that utilize data assimilation or data fusion of air quality measurements schneider et al 2017 presented a method for the generation of high resolution urban aq mapping by including dispersion modelling coupled with aq measurement driven geostatistical data assimilation the basis of the method is the universal kriging technique goovaerts 1997 using a so called base map e g a longer term average concentration field the limitations of this method include the need for a substantial amount of measurement locations and an accurate base concentration field similarly gressent et al 2020 used sensor data in data fusion to enhance urban air quality mapping for pm10 in nantes france the adopted data fusion method was kriging it requires a base map in the form of modelled annual average pm10 concentrations the used sensor data is a large collection of fixed and mobile measurements and the model therefore applies pre processing and filtering methods the aim of this paper is to present an operational multi scale air quality modelling system called enfuser environmental information fusion service the modelling system combines several datasets regarding online information on geography meteorology regional background concentrations aq measurements and local activity datasets the model provides adaptive hourly high resolution air quality modelling output for the general public decision makers and other end users the novelty of the presented system is the combination of multi scale dispersion modelling in urban scale operative service and data assimilation that is not based on kriging the adopted data assimilation method which also supports aq sensor data to be used facilitates physically meaningful i e a detected change in emission patterns persistent learning mechanisms to guide the modelling this kind of an operative model has not been presented previously the first objective of this paper is to present the mathematical model within the operational modelling system including the approaches to combining gaussian plume and puff modelling techniques with data assimilation the second objective is to present the technical solution to perform the high resolution computations at a moderate computational cost the third objective is to present a model evaluation in the helsinki metropolitan area hma in finland for two selected years the fourth objective is to present a non kriging based data assimilation approach for urban scale air quality modelling 2 materials and methods 2 1 overview of the modelling system enfuser is an operational local scale air quality model a combination of gaussian puff plume used in the helsinki metropolitan area in finland hma the model has also been used in foreign installation sites such as nanjing china and delhi india the current set of modelled pollutant species in hma includes nitrogen dioxide no2 ozone o3 fine particles pm2 5 and thoracic particles pm10 for which the model provides hourly average concentrations at a breathing height of 2m above ground the model also produces derivative output such as the finnish national air quality index aqi based on the primary modelling output i e the pollutant concentrations the output provided by the model is publicly available via fmi s open data portal fmi 2021 hourly updating aqi visualizations based on the model results are available at hsy 2022 but also shown in public transportation displays in hma a caption of the service is shown in fig 1 further there is also a separate service providing annual average information based on historical model data hsy 2021 modelling results are updated each hour each time including a now casting period with measurements 12h in the past and a forecasting period to the future 12h in a resolution of 13 13m covering an area of approx 40 30 km historically enfuser is a predecessor of the land use regression based model presented in johansson et al 2015 with respect to this previous work the aim of incorporating air quality measurements for improved urban air quality predictions has remained the same however the land use regression based approach has been replaced with a more realistic dispersion modelling approach a schematic presentation of the modelling system has been shown in fig 2 more thorough descriptions of the individual components presented in the figure are given in the following sections of the paper in brief the input datasets which can be separated into static and dynamic ones are shown at the top of the diagram the static input data is used for the description of the modelling area and local emission inventories often the available emission inventory data is not directly useful for local scale modelling and therefore the modelling system may need to refine or downscale emission inventories based on gis data the dynamic input consists of meteorological data both numerical and measured regional aq forecasts and aq measurements this set of input is continuously extracted from online sources the raw meteorological input is not sufficient for local scale dispersion modelling and pre processing of meteorology is therefore used the dispersion modelling operates in three different layers gaussian puff urban scale regional background regional scale and local scale gaussian plume modelling for the gaussian puff and plume modelling the underlying emission information is the same and the model must index and represent the emission release data in different resolutions simultaneously the meteorological conditions may also affect hourly release rates of emissions e g ambient temperature and emissions from residential wood combustion the dispersion modelling in the three separate layers is combined to form model predictions at measurement locations these predictions will then be used in data assimilation a procedure described in section 2 4 which results in a set of hourly adjustments that will help the model to obtain a higher degree of agreement at the measurement locations this process requires a light database for measurement location specifics including device quality ratings exact coordinates and the measurement height once data assimilation based adjustments have been assessed for a given hour and a given pollutant species the final model predictions can be computed for the overall modelling area each data assimilation outcome that occurs during the now casting period i e the period for which the air quality measurement data is already available will update a more gradual persistent set of adjustment factors pa the pa factors are considered also during the forecasting period that extends up to 12h to the future clearly no measured air quality data is available for this period at the time of computing the forecast when a new modelling task launches the previous state of pa is loaded as input when a modelling task is completed the updated pa is stored locally to be used for subsequent modelling tasks 2 2 input data for the model the input datasets required by the model can be separated into static and dynamic input datasets to set up new modelling domains e g a collection of cities in a selected country certain gis datasets need to be extracted from various online sources these extracted online gis datasets fig 3 are updated periodically e g annually however due to the low frequency of updates these gis datasets are referred to as static input for this gis data extraction and the preparation of static datasets there is an automatic procedure built in enfuser a dedicated sub program is used to access various online sources for dynamic input this sub program continuously extracts and archives recent data locally for all modelling areas that have been defined in enfuser the sub program is always kept active and provides the model with up to date input data without temporal gaps when the model is used for aq predictions this local archive of dynamic data is accessed in hma the dynamic information includes the following reference quality aq data from 12 measurement stations fmi 2021 regional scale aq forecasts from the chemical transport model silam sofiev et al 2015 harmonie numerical weather prediction nwp data with 1 1 km resolution for meteorology bengtsson et al 2017 and road weather measurements digitraffic 2021 additionally traffic flow measurements digitraffic 2021 and traffic congestion data are being extracted continuously however these additional information sources are used in supporting offline tasks 1 1 traffic flow count data together with traffic congestion data from www here com are used for quality controlling traffic flow patterns the model uses and do not yet contribute to the operational modelling outcome 2 2 1 geographical information system gis data enfuser has a built in capability of extracting and processing global open access geographic information for its modelling areas there are several use cases for this information e g during the dispersion modelling information on ground elevation urban structures and surface roughness length estimates are needed gis based derivatives are often needed as global coarse resolution inventories such as edgarv5 crippa et al 2019 cannot be used without post processing and downscaling in local scale modelling the openstreetmap raw data osm 2021 can be considered as the foundation for the gis datasets that are used to describe the modelling areas this provides the model with a detailed high resolution land use mapping that is especially useful for the description of road networks and the layout of buildings the modelling area is described with osm data in two layers the functional land use e g parking space and the material land use e g asphalt for buildings and roads the model uses an object oriented mapping that can e g describe the road name number of lanes speed limits etc another source of information is the global human settlement eu 2021 which provides additional land use and population data the ground elevation is extracted from nasa srtm and contributes to surface roughness estimations and height parameters used in dispersion modelling 10 10m 2 a description in finnish for the emme model can be found from visited february 1st 2022 https www hsl fi sites default files helsingin seudun tyossakayntialueen liikenne ennuste kysyntamallit 5 2019 pdf multi spectral satellite imagery is extracted from copernicus sentinel 2 msi these above mentioned sources can be regarded as the primary sources for gis data for the model and based on them additional derivative datasets are proxied one such derivative is shown in fig 3 the building height dataset the building height dataset is estimated based on the osm building characteristics which often specify the height of the building in meters or the number of floors however for many buildings this characterization does not exist and therefore the height can only be roughly estimated based on the building type and building surface area 2 2 2 emission data it has previously been found that the most important urban source categories in hma are residential wood combustion and vehicular traffic kukkonen et al 2018 and 2020 for shipping and harbour activities the contribution to the total concentrations of pm2 5 in the three year period 2012 2014 has been estimated to exceed 10 only in the vicinity of major harbours kukkonen et al 2018 the contribution of power production and industrial activities to local pm2 5 concentrations has been negligible in most parts of the area leisure boat emissions for the coastal area of hma can be estimated with the beam model johansson et al 2020 but these presumably negligible emission contributions have not been included in enfuser the basis for most emission inventories is a gridded annual or monthly emission total dataset for which a temporal profile to sequence emission release rates has been defined in addition there are a road traffic emissions based on osm road network structure coupled with vehicle flow counts and b shipping emissions high resolution dynamic emission inventory given by the steam3 model johansson et al 2017 and c power plants and factories treated as individual entities that are modelled as elevated point sources the most notable emission sources for hma have been illustrated in fig 4 road traffic emissions the road traffic emission modelling is based on the openstreetmap road network each piece of a road of which there are tens of thousands in hma s dataset is treated as a separate entity with characteristics such as speed limit road type number of lanes and surface material hourly vehicle flow patterns are attached to these road elements and then an emission release rate as ¬µg m2 s is computed based on the vehicle type road speed limit and the number of vehicles flowing through the road at the time of assessment there are three classes for vehicle types cars buses and heavy vehicles to define these emission factors flow speed relationships from kristensson et al 2004 and vehicle category specific unit emission factors from vtt 2021 have been used an averaged vehicle flow pattern with 72 hourly values 24 for monday to friday 24 for saturday 24 for sunday as shown in fig 4 is implemented for each road segment separately for each vehicle class in addition a weekly correction factor can be applied to address e g holiday seasons a dataset given by local authorities in helsinki helsinki region transport hsl and the city of helsinki using the emme 2 model has been used for defining these averaged traffic flow patterns this material describes averaged hourly flow counts as line data 30 000 lines the vehicle flow information characterized by the line data has then been implemented to the road network described by the osm data for each osm road segment a nearby emme line element is searched with matching orientation and speed limits for lesser roads for which a corresponding emme characterization is not found a statistical approximation is used instead based on gis data and a generic temporal profile obtained from nearby roads this procedure of implementing vehicle flow patterns from line input data into the osm road network is a supporting feature of enfuser and can be used also in other areas in addition to helsinki road dust emissions complex deterministic models have previously been developed for the evaluation of suspended dust from the road and street surfaces e g kauhaniemi et al 2011 denby et al 2013 kauhaniemi et al 2014 stojiljkovic et al 2019 however such models require a comprehensive set of input data for instance data on the road sanding and the use of studded tires such data is commonly not operationally available we have therefore applied a simpler statistical method for the evaluation of suspended dust the modelling of road dust and resuspension of particles is based on the data on the road network vehicle flow speed and hourly flow counts also the effects of road surface moisture the usage of studded winter tires and other meteorological factors are considered to assess the resuspension of coarse particles defined as pm10 pm2 5 road surface moisture estimates are obtained via digitraffic 2021 and a generic weekly profile for the usage of studded tires in hma is used power plant emissions power plants are treated as a collection of elevated point sources each being modelled solely with gaussian puff dispersion in enfuser generalized profiles have been created for coal oil and natural gas power plants that specify emission factors in terms of g mws with monthly diurnal variability the set of power plants nearby the modelling area is based on the global energy observatory 2021 which specifies the location type and capacity mw of the power plants in the hma there are 5 combined heat and power plants that are shown in fig 4 more specifically 1 suomenoja coal and natural gas 360 mw 2 salmisaari coal and wood pellets 160 mw 3 hanasaari b coal and biomass 220 mw 4 vuosaari natural gas 630 mw and 5 martinlaakso biomass and natural gas 197 mw shipping emissions shipping emission inventories for helsinki are provided by the steam3 model johansson et al 2017 using historic helcom ais data by the courtesy of the riparian states of the baltic sea and a vessel database from ihs fairplay the shipping emission inventory given by steam3 describes ships their positions time stamps emission release rates g s and emission release heights the detail at which shipping emissions are represented and delivered to enfuser facilitates the modelling of shipping emissions as moving point sources while considering the effective emission height however in operational use a less ambitious resolution of approx 100 100m in 5 min temporal resolution is used to represent shipping emissions residential wood combustion emissions an unprecedentedly detailed emission inventory for residential wood combustion rwc has been compiled in the hma kukkonen et al 2020 used this emission inventory to model deterministically the pm2 5 concentrations including rwc this study also evaluated the concentrations of pm2 5 and the related contributions of rwc in three other nordic cities viz copenhagen oslo and ume√• for a more detailed description of this inventory and its use in dispersion modelling the reader is referred to kukkonen et al 2020 the above mentioned emission inventory for rwc was also used in the present study the inventory has three components viz heating saunas and fireplaces each of which has different diurnal patterns and different dependencies on the ambient temperature clearly residential heating activities are increased during low ambient temperatures in winter especially for the heating component when the ambient temperature is lower than 14 c the modelling assumes that the emission factor for residential wood combustion is increased in proportion to the difference of the actual temperature and the reference value of 14 c the modelled temperature dependencies have been derived based on the monthly total emissions from rwc and the monthly average temperatures the emissions are presented with a resolution of 20 20m fig 4 and the release height of the rwc emissions has been assumed to be 10m 2 3 atmospheric dispersion modelling the model applies a multi scale approach a local scale assessment using gaussian plume modelling b urban scale assessment using gaussian puff modelling and c regional scale background extracted from the outputs of a chemical transport model the contribution of each of these three is assessed and combined at a selected receptor point rp to avoid double counting of emissions while combining gaussian plume and puff modelling the distances of emission sources from rp and the travel distances of gaussian puffs are considered the pollutant concentration c x t for a selected pollutant species prior data assimilation at location x and time t at the receptor point rp is given by 1 c x t n n k k c l e k t x n x p p c p p t x c b g x t where n is a set of incremental locations around the rp covering the surface area up to a selected maximum distance of d c u t k is the set of incremental emission sources at the location n for which the emission release rate in terms of ¬µg s for the selected pollutant species is e k t x n e k also characterises an emission release height m and an emission source category it is associated with e g traffic rwc etc the concentration at rp caused by e k in terms of ¬µg m3 is given by c l c p refers to the concentration evaluated using the gaussian puff approach at rp for which there are a total of p puffs to address during the evaluation time t each puff has a specific state p t that stands for e g the centre point location and the carried pollutant mass ¬µg which are needed for the assessment of c p the final term c b g is the regional background concentration excluding the influence of the local and urban emissions the size of the set n depends on the resolution in which the local emission information around the rp is represented and from the maximum assessment distance d c u t in the model the finest used resolution to represent incremental emission sources around rp is 5 5 m and the size of n is of the order of several thousand for each of these incremental areas n there can be multiple emission sources k but this is often limited to only a few such emission sources most often the set k is empty i e no emissions the parameter d c u t is the cut off distance at which the modelling approach switches from gaussian plume into gaussian puff modelling therefore the set p includes only the puffs that have travelled at least the distance of d c u t a low value of d c u t makes the modelling more agile i e can more realistically address rapidly changing meteorological conditions but requires that the puff modelling is performed in higher detail this will result in an increase of the computational cost considering the technical hardware limitations of the modelling configuration and based on a collection of simulations with different values for d c u t a value of 2000m has been selected to be used in the hma 2 3 1 gaussian plume computations following the work in seinfeld and pandis 2016 the gaussian plume solution with steady state meteorology totally reflecting ground is given by 2 c l q 2 œÄ u œÉ y œÉ z exp y 2 2 œÉ y 2 exp z h 2 2 œÉ z 2 exp z h 2 2 œÉ z 2 where c l is the concentration at rp q is the emission rate ¬µg s the standard deviation œÉ y defines the spread of the dispersion horizontally and œÉ z vertically variable u is the wind speed m s y is the horizontal perpendicular distance from the line aligned with wind direction h is the emission height m and z is the height of the receptor point above ground m vertical profiles for wind speed or diffusivity are not considered eq 2 does not address the influence of a finite boundary layer height or the settling velocity of particles the model however includes these effects by using extended versions of eq 2 for readability these have been presented in appendix a computationally efficient ways to implement these equations in practice have been presented in supplements 2 3 2 estimation of gaussian standard deviations there are several empirical approaches for the assessment of the gaussian standard deviations œÉ z and œÉ z mcelroy 1968 smith 1973 mcmullen r w 1975 the approach from briggs 1973 p 124 is adopted in which the standard deviations are estimated as a function of distance x m from the emission source along the wind direction but depending on a stability classification these empirical formulas predict standard deviations œÉ y x and œÉ z x defined separately for a discrete set of different atmospheric stability conditions a b c d e f ranging from highly unstable a to extremely stable f the stability class is selected based on the meteorological conditions mohan and siddiqui 1998 specifically the monin obukhov length based method is adopted traffic produced turbulence tpt can become a dominant cause for mixing and the dilution of pollutant concentrations nearby roads especially in low wind scenarios the inclusion of tpt in an urban scale model can be challenging in solazzo et al 2007 several tpt schemes have been evaluated in h√§rk√∂nen et al 1996 a semi empirical treatment is used by introducing additional standard deviation terms œÉ y 0 and œÉ z 0 as a function of wind speed and the angular difference between the road and the wind however in h√§rk√∂nen et al 1996 the vehicular flow speed is ignored whereas it is considered in solazzo et al 2007 in enfuser tpt is addressed simply by enhancing the diffusion of pollution by increasing the values of œÉ y x and œÉ z x as a function of traffic flow speed while the vehicle density is not considered for example the value of x for the assessment of œÉ x is increased by 10m for a motorway emission source with a flow speed of 120 km h in essence this introduces additional mixing in a similar way that the addition of œÉ y 0 œÉ z 0 does the used parametrization requires further research however the primary aim of this simplified approach is to remove outliers overprediction in the immediate vicinity of main roads and the simplistic parametrization achieves this 2 3 3 wind profiling approach the following method is used for the assessment of wind speed at the height of the emission source u h first the model takes interpolated data from the harmonie nwp model at a height of 10m u 10 this interpolation considers both spatial and temporal dimensions and therefore provides smoothly variating preliminary wind fields then u h is evaluated based on a log wind profile holmes and bekele 2001 3 u h u 10 l n h d 0 z e f f l n 10 d 0 z e f f u h 1 where d 0 is the zero plane displacement m and z e f f is the effective upwind surface roughness length m for the assessment of z e f f an upwind directed assessment of land use information is used and the land use information is used for the selection of a roughness length estimate more information on the wind profiling approach and the estimation of surface roughness is presented in appendix b 2 3 4 gaussian plume computations in practice a concentration field for the modelling area for a given pollutant species and time is estimated by applying eq 1 repeatedly by changing the location for the rp each time the assessment of the n surrounding incremental areas around the rp and their potential concentration contribution at rp if the set k is not empty is critical for the computational performance of the model to make this process efficient a method has been developed to reduce the number of areas assessed this way a gaussian plume shaped footprint that consists of a large set 1000 of incremental area cells has been shown in fig 5 the footprint has been designed to be rotated upwind in a single operation and define a reduced number of incremental areas for the assessment of the first term of eq 1 the coordinate system is like the one used with the gaussian plume computations but reversed the footprint is rotated to the upwind direction the origin x 0 y 0 is at the rp as shown in the figure the incremental area cells that form the footprint have different assessment resolutions the highest resolution is used for the cells that are close to the rp along the wind direction to facilitate the variable assessment resolution this way the emission information is also presented in different aggregation resolutions for example the cells with high resolution can access emission information at a high resolution and the cells farther away can access aggregated emission information that matches the coarser cell resolution better further the footprint s cells fixed geometry with respect to rp facilitates the use of tabled precomputed values for the assessment eq 2 this process and how the footprint has been formulated has been presented in supplements briefly put each cell can instantly access the set k wherever it has been rotated on top of for each emission source revealed this way their concentration contribution at rp can be estimated using tabled precomputed values excluding the term q u the net contribution of all incremental emission sources is aggregated at the rp as is described in eq 1 2 3 5 methods for considering the influence of buildings and street canyons part of the receptor points of modelling including several measurement stations are in complex urban environments such as in street canyons or the vicinity of high buildings the model surveys the nearby environment for urban obstacles e g based on the locations of buildings in the gis dataset for each receptor point the model subsequently archives the distances to various buildings and other obstacles and the height of each obstacle in each direction with an estimation radius of 100m based on the scanned obstacles around the rp the model then evaluates the additional effect on dispersion caused by them specifically for the computation of c l with the gaussian footprint method a limited number of area cells close to rp can be re rotated within urban street canyons the rotation angle in this empirical method that uses the assumptions presented in berkowicz 2000 is based on the street canyon geometry and ambient wind direction to put it simply the re rotation of the footprint within street canyons is to guarantee that the original surface area for local pollution in the street canyon is being accounted for regardless of the wind direction more realistic concentration predictions can then be given for both the leeward and windward sides of the canyon the strongest impact of this method is observed when the wind direction is perpendicular to the street canyon conversely the method has a negligible effect when the wind direction is parallel to the street canyon as a limitation the method cannot achieve homogeneous concentration fields within the street canyon in low wind speed scenarios as is assumed to occur berkowicz 2000 2 3 6 gaussian puff computations a gaussian puff approach is used to represent a single emission mass parcel that moves dynamically in the modelling area the concentration caused by a gaussian puff can be computed with stockie 2011 seinfeld and pandis 2016 4 c p q m 8 1 2 œÄ œÉ y œÉ z 3 2 exp x u t 2 y 2 2 œÉ y 2 exp z h 2 2 œÉ z 2 exp z h 2 2 œÉ z 2 where q m is the emission mass ¬µg of a pollutant species within the puff and t is the travel time however eq 4 does not support the non linear movement determined according to the location and time dependent wind field therefore the following refinements that extend the applicability of eq 4 are adopted first the travel distance m of each puff is given by u t d t and this is used for the computation of standard deviations œÉ y œÉ z second a substitution x u t 2 y 2 d 2 is used for eq 4 the term d 2 is the squared euclidean distance from rp to the current centre point location of the puff fig 5 after these refinements the concentration contribution assessment of a gaussian puff at rp allows the meteorological conditions imposed for the puff to be updated on a minute basis appendix b but the travel distance and centre point need to be dynamically tracked further the variables x and y have been eliminated from the equation and the substituting variable d can be assessed in a fixed coordinate system instead of a rotated one the tracking of the puff travel distance has another important use case the selection of puffs to be considered in eq 1 set p are determined based on their travel distances to avoid double counting of emissions the set p can therefore be formed by selecting the puffs for which the travel distance exceeds d c u t as with gaussian plumes extended equations are applied with gaussian puff modelling which considers the influence of a finite mixing layer height and the gravitational settling velocity of coarse particles appendix a the management of emission source information for the gaussian puff modelling is different than was presented for gaussian plume first the high resolution emission source representation is aggregated to much larger 500 500m sized cells and these cells are used to frequently release emissions into the atmosphere in the form of puffs for each puff emitted this way the source category or several categories information is maintained secondly selected emission sources such as power plants are treated as point emission sources and dispersion modelling is solely based on gaussian puff no spatial aggregation is applied for such point sources and these are not considered with gaussian footprint based approach the efficiency of assessment of eq 4 is important for the model s performance and the set of puffs that are being assessed for each rp needs to be kept as low as possible usually the number of puffs set p being modelled in hma at any given time is in between 100 000 to 300 000 in supplements several techniques for reducing the computational burden for gaussian puff modelling in enfuser have been presented 2 3 7 regional background the regional background is included by using hourly predictions from the operational chemical transport model silam sofiev et al 2015 clearly a double counting of emissions must be avoided previously various methods for that have been presented e g by denby et al 2020 and benavides et al 2019 the combined use of chemical transport models and urban scale dispersion models for a wide range of models has also been discussed by kukkonen et al 2012 the adopted technique for assessing the regional background concentration is comparable to an upwind assessment of the background but utilizes the gaussian puff modelling technique the silam predictions are assessed at the modelling area boundaries in which numerical background concentration markers are being continuously released and their movement tracked afterwards these markers that contain the regional background concentration values for all modelled species at the time of release will travel to from and within the modelling area according to the wind fields for assessing the regional background for an rp at any given time a inverse distance weighted kriging interpolated value of the markers found nearby the rp are used 2 3 8 chemistry correction to model predictions methods have been presented in the literature for including chemical reactions to gaussian models for instance kukkonen et al 2001a b and karppinen et al 2000 presented a simplified solution for including the basic reactions of nitrogen oxides oxygen and ozone to a gaussian plume model this system of equations can be solved analytically the chemical transformation equations were included in the dispersion model based on the so called receptor oriented discrete parcel method kukkonen et al 2001a b karppinen et al 1998 this method considers air parcels in which the emissions and background air are assumed to be instantaneously uniformly mixed the chemical reactions in each parcel are then assumed to proceed independently of the dispersion process regarding the transformation of nitrogen oxides the influence of hydrocarbons is important on regional and long range transport scales but commonly insignificant on the urban scales however their influence may be substantial in episodic conditions during prevailing stable atmospheric stratification and low wind speed and in case of recirculation of the air masses over a city in the present study it was assumed that most but not all e g 75 depending on emission source category of emitted nox emissions are initially no there is uncertainty in the ratio of primary no2 emissions and in addition the ratio is gradually changing caused by technological advancement most detailed information is available for road traffic in grice et al 2009 the ratio of primary no2 to nox is estimated to be above 10 in 2005 and projected to exceed 30 by 2020 within the european union during the atmospheric transport no is converted into no2 and the reactions involving o3 are responsible for most of the oxidation however the conversion of no to no2 can be limited by the amount of o3 at the location a solution to this is to make the model predictions at a specific order i e the prediction for o3 at the rp is done before predicting no2 and no then a post processed adjustment can be applied to the modelled no2 and no given that the o3 concentration at the rp can be given as input further it is assumed that no released this way is immediately converted into no2 simultaneously depleting o3 through oxidation by introducing a modelled virtual negative o3 emission in this step we base the conversions on the molecular masses of the species involved after the assessment of eq 1 it is possible to check whether the initial assumptions were correct and if not balance the modelled no no2 and o3 so that o3 is non negative as an example near major roads a large no2 concentration is initially modelled coupled with a negative virtual total o3 concentration after the balancing measure some part of the modelled no2 is transformed back to no finally to address the atmospheric lifetime of no2 mass decay functions are used for instance according to liu et al 2016 the mean lifetime for no2 power plants may september usa and china was derived to be approximately 4 h and based on this 30 of the no2 mass is reduced each hour such decay functions are only applied to gaussian puff modelling in which the state and movement of the emission parcels are updated dynamically 2 4 data assimilation the data assimilation method presented in this paper variates the hourly emission factors for the included emission source categories effectively making the modelling to be constrained by the observations in addition to emission factors there are other explanations for having differences between measured and modelled observations these include meteorological chemistry or dispersion modelling related reasons but these are not considered in the data assimilation a model prediction for any pollutant species given by eq 1 can be expressed as the sum of its category specific contributions since emission source category distinction is maintained for both the gaussian puff and plume modelling c x t s s c s x t for a total of s emission source categories in which the regional background has been included the term c s x t for example stands for concentrations caused by traffic shipping or rwc at location x during time t an adjusted model prediction that is affected by recent and historic data assimilation results is given by 5 c x a t s s c s x t c s x t a s a s c s x t s b g c b g x t a b g a b g c b g x t c b g 0 where a a 1 a 2 a s is a set of hourly adjustments for emission source categories and a s is the persistent adjustment for category s the term persistent adjustment is used since a s encapsulates the outcome of many previous hourly adjustments and its effect to eq 5 persists even when no measurement data is available e g during forecasting in such cases when no measurement data is available a s 1 a b g 0 the hourly adjustment set a is optimized during data assimilation as a free parameter for each modelling hour and separately for each pollutant species the value of a s is considered constant for the estimation of eq 5 but each optimized a s will slightly update and modify the value of a s for subsequent modelling hours this update operation also considers the time of day and the day of week i e if an hourly adjustment a s for a particular local time is consistently high or consistently low then the persistent adjustment a s for that local time gradually adapts to match this behaviour more information is given in supplements the weighted squared error wse between an adjusted model prediction and the observation o m t is w m o m t c x m a t 2 where the weight factor w m relates to the measurements m reliability and variability the aim of the data assimilation is to minimize the weighted sum of squared errors wsse across all the measurements for the same pollutant species at a given hour this formulation is like the one in weighted least squares technique wls kiers 1997 in wls in which the best linear unbiased estimator is solved to minimize wsse the inverse variance of the measurement is often used for w m w m 1 œÉ 2 in this approach œÉ 2 is being estimated based on a fixed quality rating for the measurement device considering the hourly variability of measurements from the measurement source johansson et al 2015 the wls technique however is not adopted for solving in enfuser strict limitations for possible states for a need to be enforced e g minimum and maximum values for a s the system due to eq 5 is not easily expressed as linear expressions either further due to meteorological conditions infrequent calibration and technical issues w m needs to be treated as a free parameter that is also optimized during data assimilation for m measurements in m locations for a certain pollutant species the optimization problem for minimizing the wsse is defined as follows 6 m i n w s s e a p w s s e a p f p m 1 m p m w m o m t c x m a t 2 where p p 1 p 2 p m p m 0 1 is a factor that reduces the weight assigned to the wse for measurement m and f p is a penalty function smith and coit david 1996 that increases wsse for each p m 1 the function f p acts as a countermeasure when the values of p m are being reduced any attempt to minimize w s s e a p without incorporating a penalty function would include the trivial minimization of the values of p m an effective penalty function forces the algorithm to search for a balanced compromise of optimal adjustments to emission factors a while keeping p m collectively as high close to 1 as possible with this in mind we define 7 f p p m b where a value of 1 is used for b b 0 several other formulations for f p have been tested the suggested one is currently the simplest well performing parametrization found based on numerical test simulations 2 4 1 discrete descent algorithm for searching the optimum the optimization task of eq 6 is a nonlinear optimization problem ruszczynski 2011 the problem can be simplified by discretizing the variables however no completely satisfactory method exists to solve it even in that case olsen and vanderplaats 1989 gradient descent methods for which an overview is presented in ruder 2016 are one of the more popular approaches to perform this kind of optimization the logic is simple one can slide towards the minimum by moving to the opposite direction of the gradient of the objective function step by step in case the system is convex the algorithm terminates at a global minimum otherwise the potential issue of getting trapped in a local minimum must be considered a discrete gradient descent algorithm is used in enfuser to find optimal parametrization for a p that minimizes wsse starting from an arbitrary initial state fig 6 and step by step descending to optimum in terms of w s s e the possible states for each a s p m have been discretized for emission source adjustments each a s are allowed to have a finite number of states e g from 0 5 to 1 5 with a step size of 0 1 the discretized possible states for each p m include 1 0 6 0 4 0 2 0 1 0 05 in the descent algorithm the direction and step length offering the most impactful reduction to wsse are searched and taken once there are no directions left to reduce wsse the algorithm terminates to address the potential issue of terminating at a local minimum the following techniques are used first attempts are made to leap over the minimum with significantly larger step sizes when the algorithm is about to terminate secondly the iteration can be repeated using multiple different initial states and then select the optimal state out from several candidates the optimal state a that minimized wsse is used to update a s procedure has been described in supplements the computational burden of the data assimilation algorithm can be considered negligible with respect to the gaussian dispersion computations even with a high number m 100 of measurement data being included therefore the method supports the use of aq sensors in large quantities 3 results the model was applied for the prediction of hourly pollutant concentrations for no2 o3 pm2 5 and pm10 in the hma during 2017 and 2018 depending on pollutant species there are up to 12 aq measurement stations in hma of which several are relocated each year all model configuration parameters such as the minimum wind speed and minimum mixing layer height have been set as presented in the methods section with appendices the station locations are shown in fig 7 together with annual average concentrations for no2 2017 the geographical annual averages presented in this study have been averaged from modelled hourly concentrations for the same area further the modelling of each hour involves urban scale gaussian plume modelling minute by minute gaussian puff modelling incorporation of regional background and data assimilation the annual computations have been performed using the operational system architecture i e proceeding chronologically from january to december in 24 h modelling task batches completing a modelling task that covers a full year this way takes approximately three days of processing time with a modern 3 3 time estimate is based on data production rate for hma using 12 core 6 61 broadwell 2400 mhz cpu with 32 gb of memory pc the contribution of road traffic is dominant for no2 in hma but the concentrations near main roads are often limited by the availability of o3 the regional background is approx 7 Œºgm 3 across the modelling area and shipping contributes up to 5 Œºgm 3 the contribution of power plants is minor also for other pollutant species than no2 in fig 8 the annual average pm2 5 concentration for 2017 is shown with emission source contributions the most notable source is the regional background not shown approx 4 Œºgm 3 across the modelling area the contribution of traffic is up to 2 Œºgm 3 near the busiest roads in hma the modelled contribution of rwc emissions is lower than 1 Œºgm 3 the contribution of shipping for pm2 5 in 2017 is the highest near the helsinki central area coastline but lower than 0 5 Œºgm 3 the corresponding annual modelled average concentrations for pm10 and o3 are shown in fig 9 the annual average concentrations of pm10 and pm2 5 are naturally related the most notable difference being the added resuspension of particles that increases concentrations near roads for o3 there is no other source for pollution other than the regional background however lower annual average o3 concentrations can be observed nearby traffic and shipping emission sources due to adopted nox o3 chemistry 3 1 the evaluation of the model against measured data the model evaluation was performed using the leave one out cross validation procedure for each measurement site the model predictions were done excluding the values at that site during data assimilation due to model development measurement availability and model maintenance during 2017 and 2018 the evaluation time series does not cover a full two year period and there are some notable gaps in between the measurements used for the evaluation are uncorrected real time data extracted and archived as soon they have become available online this means that the air quality measurements have not yet been subject to the more stringent qa qc procedures and may contain outlier data for most stations participating in the evaluation there are more than 7500 hourly measurements available for comparison annually for all sites a measurement height of 3m above ground has been assumed using the exact coordinates as shown in table 2 in this study several standardized statistical measures were used to quantify the modelling performance these measures have been explained in the supplements in terms of fractional bias fb and normalized mean squared error nmse the model prediction accuracy has been presented in fig 10 for each station and species for 2017 and 2018 further details have been shown in tables 1 and 2 as it can be seen from fig 10 for o3 and pm2 5 most of the stations are within the target limits with respect to nmse and fb nmse 0 5 and 0 5 fb 0 5 however for no2 and pm10 there are a couple of outlier stations that cause the nmse and f2 to exceed these target values an evaluation of predicted monthly averages against observed ones have been shown in fig 11 in terms of correlation pcc the highest ones are obtained for no2 0 908 for 2017 and 0 927 for 2018 the second highest correlation is obtained for o3 previous deterministic model evaluations for pm2 5 in hma have been presented by kukkonen et al 2018 and 2020 for the years 2004 2014 most of the stations are the same as those used in this study over this assessment period the average f2 was approximately 0 68 and the index of agreement ia was 0 66 for the deterministic modelling system for comparison the average enfuser predictions for 2017 and 2018 for f2 were 0 84 and for ia these were 0 83 as expected the evaluation measures computed in this study are better than the corresponding ones obtained using solely deterministic methods 4 discussion the spatial distribution of annual average concentrations for 2017 was presented in the study the corresponding annual averages for 2018 were not shown as these are like the ones obtained for 2017 several outlier stations can be observed in fig 10 when model predictions are compared to measurements in terms of nmse and fb these outlier stations have been grouped as follows outlier 1 stands for an urban traffic station lep situated in between a busy outdoor parking area and a road for this site the model underpredicts pm10 concentrations especially during spring road dust events outlier 2 is a rural background station luu that provides valuable information for the model for regional background concentrations the added value comes from the fact that the model predictions at this rural background station can effectively be variated by adjusting a b g and the model prediction at luu is not sensitive to other a s when this measurement data source is being omitted in the leave one out validation procedure a collection of suburban and urban stations remains from the data assimilation method s perspective after the omission of luu there is no measurement data available to provide a pure signal for the regional background further for no2 the modelling results for luu are the poorest a clear over prediction can be observed especially when the wind direction is from south and south east with these wind directions the gaussian puff modelling carries no2 pollution to luu which in turn suggests that the used decay rate for no2 may be underestimated outlier group 3 is a collection of challenging urban suburban locations for pm10 modelling of which the hardest one to predict is an urban street canyon station mak for this group the nmse target is exceeded but the f2 and fb targets are met nearby construction work has also been suggested as one of the causes for high measured pm10 concentrations outlier 4 is an urban background station smear3 which provided very low concentration measurements for no2 from january to march 2017 these measurements were later confirmed to be erroneous by the data provider after mid july this station returned online after being offline for 3 months this kind of conflicting measurement input with respect to other measurement evidence however can be dealt with by the data assimilation method as is shown in fig 12 monthly average concentrations and average weight reduction factors for eq 6 are shown for the smear3 station in the figure in a modelling test run where every station participates in data assimilation in this modelling test from january to march the data assimilation algorithm often terminates at states that strongly reduce the weight put on smear3 measurements the only effective way to reduce the squared error se at smear3 would be to reduce a b g however such an adjustment would increase se in all other measurement locations simultaneously therefore a reduction of a b g does not occur at the optimum where data assimilation terminates but a reduction of weight put to the erroneous smear3 measurement occurs instead outlier 5 is a coastal measurement site near a shipping terminal e sat particularly the prediction of hourly no2 is challenging there especially due to berthing ships nearby for which the emissions of auxiliary engines are difficult to predict by the steam3 model the road traffic profiles near e sat seem also to be underestimated as they do not account for the additional traffic induced by the nearby shipping terminals similarly in 2017 one of the measurement stations was situated at the helsinki airport len while nmse and fb are acceptable for this location the model clearly under estimates no2 one possible explanation for this is the significant amount of taxi traffic with mostly diesel engines being used nearby the station which has not been accounted for in the vehicular traffic flow mappings in the area in fig 11 the predicted monthly average concentrations against measured concentrations were shown for 2018 high correlation of 0 837 is obtained for pm2 5 but a lower correlation is obtained for 2017 this is likely due to higher monthly variability for the average pm2 5 concentrations in 2018 and this variability is successfully being captured by the model between january and march 2018 the average temperature is more than 3 c lower than in 2017 which in turn results in higher emission output from the residential wood combustion sources during the winter months the lowest correlations are obtained for pm10 especially during spring 2018 the monthly variability is strong and there are notable differences from station to station due to road dust in hma the modelled regional background concentrations as given by the ctm for ozone have a clear seasonal bias during spring the concentrations are underestimated by the ctm the adopted data assimilation procedure can correct the regional background to more appropriate levels namely a b g for ozone begins to rise during spring reaching 5 Œºgm 3 after summer a b g reduces gradually to a value close to 0 Œºgm 3 for other modelled pollutant species such a distinguished seasonal behaviour is not observed for example a b g for no2 varies in between 1 5 and 0 5 Œºgm 3 throughout 2017 and 2018 without clear seasonal patterns occasional overfitting by the data assimilation procedure can also be observed for several pollutant species during winter months no2 traffic emissions are being scaled upwards and downwards during summer modelled pm10 background is being increased by pa during spring time road dust episodes fortunately these occurrences of overfitting can be analysed and used to guide further model improvement for example the adjustments made by the data assimilation method can be analysed in terms of conditional averages i e as a function of meteorological variables such as wind speed boundary layer height and ambient temperature if the data assimilation systematically assigns strong adjustments in specific atmospheric conditions then this is a clear indication of overfitting 4 1 limitations the key sources of gis information are extracted from openstreetmap however the quality of this information varies between countries in some areas such as in asia additional supplemental gis information may be needed further information in addition to the gis data is commonly needed to reliably evaluate traffic flow patterns the model has limited capability to address atmospheric chemistry the model currently includes solely a simplified scheme for the nox ozone chemistry a more comprehensive treatment of the chemical and physical transformations would be needed for geographically more extensive modelling domains and in case of more complex meso and microscale meteorological processes e g recirculation of air masses over a city while the data assimilation method provides a mechanism for potentially improving the emission strengths of the local emission modelling the method cannot correct any spatial heterogeneous inaccuracies within the used emission inventories the model also cannot explicitly correct for any inaccuracies in the dispersion modelling and the evaluation of other properties of the emission sources for instance the release height estimated for the emissions from residential wood combustion the modelling can in some cases result in an over fitting to the measured air quality data due to the adopted data assimilation procedure for example during spring road dust episodes the modelling may excessively increase the regional background concentrations to counterbalance the lack of sufficiently realistic modelling for dust accumulation and resuspension in addition the modelling requires the data of an air quality measurement network in the modelling area the data assimilation method can function effectively only if this measurement network includes a wide range of stations in different environments accurate coordinates and measurement heights are also needed for each station or sensor which will be included in the data assimilation the gaussian dispersion modelling methods cannot address explicitly either the effects of buildings and other obstacles or the effects of complex terrain such as mountainous areas valleys and fjords the modelling has applied simple methods for estimating gaussian standard deviations atmospheric stability classes and urban micrometeorology and upwind roughness length these simplifications were needed partly due to the computational efficiency requirements and the need to support offline pre computations for gaussian dispersion modelling 4 2 future development topics the modelling of pm10 could be improved in the future especially regarding the modelling of road dust during spring this development work could include the use of machine learning methods neural networks or random forest models machine learning methods may also be adopted in the future to assist in the modelling of traffic flow patterns within the osm road network provided that enough traffic flow measurements will be available for training such models detailed traffic flow datasets such as the ones used in the helsinki metropolitan area may not be available for other urban areas the operational modelling service for hma will be improved by extending the forecasting period up to 24h in the future black carbon and the lung deposited surface area ldsa of particles will be included in the modelling measurement data is available for both of these pollutant species in the helsinki region in addition an existing complementary network of aq sensors will be utilized finally the model could continuously use the predicted near real time shipping emissions instead of historic annual emission data to facilitate this the steam3 model will be coupled with a continuous supply of ais data for the modelling area 5 conclusions we have presented a multi scale air quality modelling system called enfuser the model uses a combination of gaussian plume and puff modelling techniques incorporates regional scale modelling and utilizes data fusion and aq measurement based data assimilation this novel modelling system has provided open access information on air quality for the general public continuously since march 2018 clearly the model can be used also for other regions in addition to the hma the model takes advantage of both a global open access source for gis information and various time dependent dynamic input datasets the extracted gis data and the time dependent datasets are used in the model in various ways including a the refinement and downscaling of emission inventories b description of urban buildings and other obstacles for local dispersion modelling c the assessment of surface properties for the processing of meteorological information and d the detailed modelling of vehicular flow patterns and the emissions of road traffic the model can in some cases be used with reasonable accuracy even in regions for which reliable emission inventories are not available due to the data fusion and data assimilation capabilities however some information sources used in this study are not available at all locations internationally to guarantee the transferability of the model to another city than helsinki the following key datasets and sources of information should be available first there should be an aq measurement network that provides measurements for each modelled pollutant species there should be several air quality measuring stations and preferably the stations should be distributed in different measurement environments e g one or more in rural background areas and several in urban areas in case the number of stations in the area is low a complementary network of sensors may be needed if the measurement data is not accessible from global open source data portals then a timely access to the measurement data needs to be provided second there should be information available to support the customization of traffic flow patterns in the road network possibly in the form of a previous study or a collection of traffic count data third in case there are special local emission sources in the area e g industrial areas that are known to substantially affect the air quality then emission inventories for these sources should be provided fourth the current provider of harmonie nwp data for enfuser covers only the baltic sea region and a different source for meteorological input is needed elsewhere in addition to these necessary sources of information there are certain lower priority datasets that may be provided to improve the modelling quality but are not considered mandatory for example a building height dataset improves the modelling capabilities in view of urban micrometeorology and can be used to address the concentrations in urban street canyons a novel data assimilation method was presented to support urban scale modelling this method variates the local emission factors under the constraints of the measured air quality information the method can continuously adjust the modelling of emissions over time it will adapt to the changes in the temporal patterns of emissions and the regional background concentration the method is computationally efficient and can be used for the simultaneous assimilation of large amounts of measurement data the model was evaluated in this study in a now casting mode using a two year evaluation dataset for the helsinki metropolitan area during 2017 and 2018 the modelling accuracy can be considered good in terms of monthly averages there was a high correlation pcc for the predicted and measured concentrations for no2 0 908 and 0 927 and o3 0 855 and 0 901 for both years in the case of pm2 5 the achieved correlation for 2018 was 0 837 although the correlation for monthly averages for no2 was high the model on the average systematically overestimated the relatively lower measured no2 concentrations and underestimated the higher no2 concentrations in the case of the pm10 concentrations there was a relatively lower correlation pcc in terms of monthly averages for both years 0 801 and 0 738 in terms of hourly evaluation and specifically in terms of fractional bias and normalized mean squared error the best modelling performance was obtained for pm2 5 and o3 while the model performance measured for pm10 was relatively worse the model performance measures were on the average clearly better compared with those found in previous studies in this region using deterministic modelling without data assimilation author contributions lj is the lead author of the article and the developer of the enfuser model lj also prepared results using the model ari k mk jn anu k and jk internally reviewed and contributed to the writing of the article jn and anu k provided emission inventory input for the model with regard road traffic and rwc software and data availability the enfuser model developed by the lead author has been written in java more than 75 000 lines of code during the years 2012 2022 the model at its earlier stages of development has been presented in johansson et al 2015 enfuser has been designed to be used in a modern pc e g 8 core cpu and 32 gb of ram or in a virtual environment both linux and windows are supported the model utilizes parallel computations multi threading and therefore benefits from the availability of a high number of processing cores data produced for the helsinki metropolitan area by the model is publicly available via the open data portal of fmi fmi 2021 source code for the enfuser model is publicly available via github under the mit license https github com johanssl enfusermit git the repository contains the necessary code and input data for operative modelling of air quality in the helsinki region declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was financially supported by the projects uia hope healthy outdoor premises for everyone project no uia03 240 smart clean haqt helsinki air quality test bed business finland cityzer services for effective decision making and environmental resilience the accc flagship the atmosphere and climate competence centre academy grant no 337552 this work has also received funding from the european union s horizon 2020 research and innovation program under grant agreement no 874990 emerge project appendix c supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105460 appendix a application of dispersion modelling formulas the extended gaussian puff and plume formulas that are utilized separately for gaseous species and pm2 5 finite mixing height adjusted form and coarse particles settling velocity adjusted form are presented in this appendix the formulas and notations from stockie 2011 are followed in which variables r y œÉ 2 y 2 r z œÉ 2 z 2 and r r y r z for the expression of the extended formulas eq 2 is written as a1 c l q 4 œÄ u r y b y exp y 2 4 r y b exp z h 2 4 r z exp z h 2 4 r z the gaussian plume formula does not account for a finite mixing layer height fmh for the pollutant concentrations which can range from several kilometres down to several tens of meters extended gaussian plume formula that considers fmh is presented e g in stockie 2011 and is given by a2 c l m q u d œÄ r y b m b m 1 2 n 1 c o s n œÄ z d c o s n œÄ h d e x p r z n œÄ d 2 where d is the value of fmh in meters low value for fmh forces the pollutant concentrations to mix into a smaller volume of air causing larger ground level concentrations due to this sensitivity of modelled concentrations with low values of fmh a minimum value of 100m is imposed for assessing b m numerically the infinite trigonometric series needs to be cut into a finite number of n terms the choice of n has clearly a notable impact on the computational cost of solving eq a2 however when using a low value of n e g n 10 a clear bias in the concentration distribution can be seen close to the emission source on the other hand close by the emission source c l eq 2 is an accurate approximation for eq a2 especially when d is large by taking advantage of these properties a cost efficient solution for assessing c l m can be summarized as follows as a function of x d the computation method switches from c l to c l m x is the distance to the emission source along the wind direction o if x d 2 case 1 assess eq a2 with 3 first terms in the series o if x d 1 case 2 c l is used to approximate eq a2 o otherwise use a linear combination of cases 1 and 2 in trivial cases in which the emission source and the receptor point are separated by the fmh a value of 0 is used regardless of x d this solution of combining c l and c l m cost effectively is referred to as the hybrid solution and has been illustrated in fig a1 it is used for gaussian plume computations for gaseous species and pm2 5 settling velocity to take the effect of particle settling velocity into account the ermak formula 4 4 the ermak solution has been presented in stockie 2011 at section 3 6 p365 is used given by a3 c l e q 4 œÄ u r y b e 1 e 1 e x p w s e t z h 2 k w s e t 2 r 4 k 2 a value of 1 m2 s is used for eddy diffusion coefficient k and the settling velocity w s e t is approximated with stoke s law for typical coarse particles the original ermak solution supports for deposition velocity to be included however this has been assumed to be zero deposition effects may still be included in the form of mass decay functions in the model gaussian puff solution extensions following eq a1 and that the distance d from the receptor point to the centre of the puff is x u t 2 y 2 eq 4 can be written as a4 c p q m 8 œÄ r 3 2 y p b y p e x p d 2 4 r y considering that when d is large then c l c l m and thus b 4 œÄ r d 1 b m based on this an extended gaussian puff solution formula that considers finite mixing height when d is large is approximated by a5 c p m q m 2 œÄ r d b m y p it can be numerically verified that for a continuous stream of incrementally small puff releases the use of eq a5 for c p m yields a concentration field that corresponds to the field given by c l m regardless of the value of d one such comparison has been shown in fig a1 using d 180m further in analogy to gaussian plume computations the gaussian puff computations can also be performed using a cost effective hybrid solution the concentrations for puffs are computed based on eq a5 only when x d is low x corresponding to the travel distance of the puff finally the gaussian puff solution formula that considers particle settling velocity is given by a6 c p e q m 8 œÄ r 3 2 y p b e 1 fig a1 comparison of concentration fields as given by the different modelling approaches using a single elevated unit emitter 1 g s as a source in each example the vertical axis corresponds to z 0 300m and the horizontal axis corresponds to x downwind distance from the emission source stability class c has been used a very high settling velocity has been used for demonstrative purposes fig a1 appendix b wind profiling the available meteorological input for the model is harmone nwp hourly data with a resolution of 1 1 km for the height 10m above ground this data is currently available only for the baltic sea region and in other modelling areas a different often with lower resolution is used for gaussian modelling in enfuser for both plume and puff modelling the meteorological input is downscaled to match the needs of the multi scaled modelling approach and this process has been outlined in fig b1 fig b1 an illustration of the wind profiling methodology adopted in enfuser nwp wind speed and direction are interpolated spatially and temporally and used as input 6 00 and 7 00 refer to the local time of day used for the shown example for helsinki region upwind roughness length url estimates are produced based on the wind direction and gis data for the area wind speed profiles as a function of height are then estimated based on interpolated meteorology and url and this assessment is done in different scales to match the use case they are used for u 10 and u 2 refer to profiled wind speed at 10m and 2m above ground fig b1 the first stage of the downscaling process is to obtain an interpolated wind speed and direction values for a given location at a given time based on the nwp data these are obtained with spatial temporal interpolation from the raw nwp data spatially an inverse distance weighted kriging is used a linear interpolation is used for time turning of wind direction as a function of height is not considered the second stage involves the estimation of upwind roughness length url or z e f f the assessment needs to be wind directed as measured turbulent fluxes can be assumed to depend on an upwind source area at the typical modelling heights used in urban scale modelling schmid and oke 1990 schmid 1994 rooney 2001 like the approach presented in hammond et al 2012 an upwind fetch distance of 500m is used and z e f f assessed this way is taken as the average value of multiple roughness length estimates z i encountered within the defined area of assessment the area of assessment is set to be a narrow cone 6 covering the area upwind up to the selected maximum distance of 500m the distance of each z i for the location of z e f f is considered we estimate z e f f as a simplistic distance weighted average given by c1 z e f f w i z i w i w i 1 d where d i is the distance of z i to the assessment location for z e f f for the estimation of z i the available gis datasets are used to define a closest matching davenport classification wieringa 1992 the resolution used for this assessment is 5 5 m2 that matches the resolution in which openstreetmap data is described for the modelling area using this resolution z e f f gives estimates to be used in gaussian plume modelling with the footprint based approach however this scale is not suitable for the gaussian puff modelling for which further averaging up to 100 100m is done technically the assessment of z e f f is computationally costly and therefore z e f f values for the modelling area have been computed and archived prior operational use for each wind direction in 10 degree intervals the assessment of z i involves the gis datasets shown in fig 3 e g openstreetmap but requires complementary information as well such as sentinel 2 based estimates for vegetation coverage details are not presented here generally speaking the approach yields url values of 1 2 at urban areas and provides low values 0 0002 for sea areas and lakes due to the upwind directed approach however low url values are also obtained for roads when they are aligned parallel to the wind which therefore impacts the modelling of e g street canyons the third and final step of wind profiling involves the use of eq 3 to obtain a wind speed estimate for emission sources using their effective release height above ground further the meteorology for each gaussian puff being modelled is updated frequently once every minute or for every 100m travelled by the puff since the gaussian dispersion equation is inaccurate for low wind speeds a minimum wind speed of 1 m s has been defined as an additional simplification and to guarantee non zero wind speeds at low heights above ground a value of 0 is used for zero plane displacement d 0 the gaussian dispersion formulas for the estimation of c l eq 2 do not address the influence of the change of the wind velocity with height between the emission source and the observation point limitations for the wind profiling approach as noted in hammond et al 2012 the estimation of z i is challenging in real world conditions even the definition for the surface area that is assessed in the aggregation is complicated the use of a nested set of different elliptical upstream areas is suggested schmid and oke 1990 therefore the method presented here can be considered as a crude approximation the performance of the presented method needs to be further developed and evaluated against wind measurements additionally comparison against les modelled wind fields hellsten et al 2020 may provide useful information for further development 
25543,we investigated the ability of the process based 1 dimensional hydrodynamic ecosystem lake model gotm wet to reproduce the fluctuating dynamics and shifts between turbid and clear water states following restoration in temperate shallow lake arreskov denmark the lake model was calibrated on a comprehensive 12 year dataset with a multiple single model ensemble approach to address model parameter and performance metric uncertainty compared to earlier modelling attempts on this lake the gotm wet model which enables simulation of water column hydrodynamics such as stratification events and two zooplankton groups improved the simulation of the maximum chlorophyll a concentrations during cyanobacteria blooms and zooplankton dynamics however the timing of shifts between phytoplankton and submerged macrophytes dominance following fish removal was not well reproduced although both states were simulated we discuss potential improvements of the model to enhance the ability to simulate the effects of restoration on the food web and ecological states in this and similar lakes keywords aquatic ecosystem modelling water ecosystems tool shallow lake lake restoration general ocean turbulence model water quality 1 introduction shallow lakes are the most abundant lake type in the world meerhoff and jeppesen 2009 and provide essential ecosystem services such as drinking water supply habitats for aquatic flora and fauna and recreational services hilt et al 2017 however shallow lake ecosystems have undergone a continuing deterioration worldwide caused particularly by eutrophication smith and schindler 2009 with the symptoms being further exacerbated by the global changes in climate jeppesen et al 2020 to mitigate these negative impacts restoration measures are implemented for instance by decreasing the external nutrient loads through improved waste water treatment or broader catchment management jeppesen et al 2005 occasionally followed by in lake restoration methods s√∏ndergaard et al 2007 however management interventions can be costly and lead to uncertain outcomes abell et al 2020 therefore before implementing restoration actions it is important to make an assessment of how a specific lake ecosystem is likely to respond to available restoration measures and take into account the characteristics of a given system and its history to inform lake management and stewardship of the best practices in restoration planning process based lake ecosystem models can be powerful predictive tools but accurate simulation of specific restoration effects is required the in lake restoration method biomanipulation which can include removal of plankti and benthivorous fish and or stocking of piscivorous fish has been extensively used in northern temperate lakes in europe s√∏ndergaard et al 2007 its efficiency relies on the trophic cascade hypothesis according to which manipulation of the fish stock leads to increased zooplankton grazing which again results in decreased phytoplankton concentrations and increased water clarity carpenter and kitchell 1996 jeppesen et al 2012 some widely used process based lake ecosystem models include fish groups e g caedym and pclake mooij et al 2010 or have been coupled to an individual based fish model makler pick et al 2011 allowing simulation of the effects of fisheries and stocking however fish groups are rarely included in case studies soares and calijuri 2021 and if included the model results on fish are rarely presented e g ula≈Ñczyk et al 2021 zhang et al 2022 of the widely used models capable of fish simulation only a few e g pclake by janse 2005 are able to simulate shifts in ecological state between turbid and clear water for shallow lakes the models that do typically assume a homogenous water column by simulating biogeochemical processes in a box model 0 dimensional approach or they rely on forcing functions or simple empirical relationships that impose stratification but have not yet been quantitatively validated janssen et al 2019a however many shallow lakes undergo multiple short term thermal stratification events lewis jr 1983 that routine monitoring campaigns are not likely to capture even under short term stratification the distribution of biota and rates of biogeochemical processes may vary substantially across the depth thus impacting the overall assessment of the ecosystem e g bartosiewicz et al 2019 kangur et al 2018 several phytoplankton species can actively seek optimal placement in a stratified water column resulting in specific depth chlorophyll a chl a maxima hamilton et al 2010 and phosphorus sediment release rates depend on the redox potential and dissolved oxygen concentrations that can vary with depth jensen and andersen 1992 furthermore to increase confidence and trust in the capability of the lake models to provide reliable insights into the effects of restoration measures the models should be able to accurately reproduce the observed impacts of restoration efforts a few case studies exist where shallow lake models have been applied in an attempt to reproduce shifts in ecological state after in lake restoration often with mixed success janse et al 1995 kong et al 2017 nielsen et al 2014 for example the 0d pclake model could not adequately reproduce water quality dynamics during short term stratification events and shifts between clear water and turbid conditions before and after an external nutrient load reduction and fish harvests in shallow lake arreskov denmark nielsen et al 2014 nielsen et al 2014 suggested that a key reason for the model s underestimation of phytoplankton bloom concentrations was inability to simulate short term stratification events and the effect that this could have via the buoyancy of phytoplankton due to conceptual shortcomings in this study our aim was to reproduce the effects of external and in lake restoration measures on water quality dynamics and shifts in ecological state in a shallow lake with an improved state of the art lake ecosystem model we applied the new 1 dimensional 1d coupled hydrodynamic ecosystem model gotm wet general ocean turbulence model gotm coupled to the water ecosystems tool wet which includes water quality and food web dynamics ranging from nutrients to fish with updated meteorological forcing and nutrient load boundary conditions on shallow lake arreskov denmark relative to nielsen et al 2014 wet is redesigned from the original 0d pclake model janse 2005 and the 1d fabm pclake model hu et al 2016 and now features fish harvest it has a flexible food web configuration and when coupled to gotm a vertically resolved water column enabling simulation of stratification and phytoplankton movement the latter previously being identified as a key shortcoming of the 0d pclake model nielsen et al 2014 for an in depth analysis of the model capabilities we applied several single model ensembles based on different model performance evaluation strategies the term ensembles is used to represent a set of models with identical gotm wet configuration but varying parameter values considering the results of the updated lake ecosystem model for lake arreskov we highlight the benefits of applying a 1d water column model and discuss the model s ability to capture shifting ecological states and the potential for further improving the model for use in shallow lake management and planning 2 methods and materials 2 1 study site and sampling lake arreskov is a shallow lake located on the island of funen denmark mean depth 1 9 m maximum depth 3 6 m surface area 3 17 km2 the climate is temperate with an annual mean precipitation of 753 mm 1990 2010 and a mean winter october april and summer may september temperature of 4 8 c and 15 7 c respectively dee et al 2011 riddersholm and scharling 2010 the lake receives water from 13 tributaries of which half is artificially constructed to drain lowland areas and it has one outlet towards east fig s1 the lake has a hydraulic retention time of 1 1 1 6 yr the catchment area 28 km2 comprises mainly agriculture 58 and forest 36 with some urban areas as well fig s1 nielsen et al 2000 monitoring of the ecological state of lake arreskov and the external nutrient load from its catchment has been undertaken since 1989 as part of the national monitoring and assessment program for the aquatic and terrestrial environment in denmark novana kronvang et al 1993 lauridsen et al 2007 catchment monitoring encompasses the daily flow and bimonthly nutrient total nitrogen tn nitrate no3 ammonium nh4 total phosphorus tp and phosphate po4 concentrations in the three major tributaries draining approx 56 of the catchment area fig s1 thodsen et al 2019 estimates from the ungauged catchment area are based on the dk qnp model adapted to danish catchments windolf et al 2011 and estimates of inlet inorganic nutrient concentration fractions figs s2 and s3 the in lake water quality monitoring comprises 19 samples year of nutrients tn no3 tp po4 and chl a as a pooled sample from the surface to twice the measured secchi depth between 0 1 and 3 4 m as well as temperature and dissolved oxygen do measurements at 4 7 points evenly spaced from the surface to the bottom of the lake kristensen et al 1992 lauridsen et al 2007 phyto and zooplankton are sampled every 2 4 weeks from the surface to twice the secchi depth or from the entire water column respectively and usually identified to genus or species level kristensen et al 1992 lauridsen et al 2007 plankton dry weight biomasses are estimated based on phytoplankton taxa specific volume formula johansson and s√∏ndergaard 2017 or zooplankton taxa specific length to biomass relations hansen et al 1992 johansson 2011 and aggregated to diatoms cyanobacteria and other algae or to daphnia and other zooplankton respectively in the comparison for further details on phytoplankton sampling and handling see appendix a the mean depth of the pooled water quality and plankton samples represent the sampled depth of observations for model comparison additionally submerged macrophyte coverage was monitored once a year from 1993 to 1994 in mid august and from 1995 to 2005 in july to estimate uncertainty in in lake observations results from historical comparison projects of the participants samplers in the novana program were compiled and percentage coefficients of variation cv were calculated for nutrient concentrations submerged macrophytes phytoplankton biovolumes and zooplankton biomasses appendix a lake arreskov has been impacted by anthropogenic activity for the majority of the last hundred years in the 1920 1990s the lake was eutrophic with low secchi depth and frequent cyanobacteria blooms but a relatively rich aquatic vegetation was observed on some shores of the lake fyns amt 1990 petersen 1920 1950 in the 1950 1980s the lake received untreated and later mechanically treated sewage water from the nearby town of korinth 970 person equivalents the sewage was diverted in 1983 but the lake still received untreated wastewater from 124 rural households as well as runoff during storm flows through the 1990s fyns amt 1996 commercial eel fishery was established in the lake in 1963 which resulted in varied amounts of by catches mainly of cyprinid fish hansen and hansen 2007 from 1990 to 1997 a combination of by catches from commercial fishing and active harvest of cyprinids removed 52 57 mg wet weight ww fish mainly roach rutilus rutilus and bream abramis brama partly with the aim of restoring the ecological state of the lake sandby 1998 fyns amt 2006 s√∏ndergaard et al 2007 table 1 additionally stocking of 15 000 to 50 000 piscivore fingerlings pike esox lucius was undertaken each year in may in 1993 and 1995 1997 hansen and hansen 2007 two wetlands receiving water from some of the cultivated areas in the catchment were established adjacent to the largest tributaries in 2003 wetland area 0 39 km2 geddeb√¶kken and 2005 wetland area 0 1 km2 hammerdam to further reduce the external nutrient load hansen et al 2011 hoffmann et al 2004 also in 2005 to sustain discharges throughout summer when phosphorus concentrations were typically higher water level regulation at the outlet was initiated since the monitoring and restoration measures were implemented the ecological state of lake arreskov has shifted between turbid and clear water conditions several times johansson et al 2019 thus the lake has had three periods with high macrophyte coverage above 30 in 1997 1998 2005 2006 and 2011 usually with corresponding changes in secchi depth and chl a concentrations from 1989 to 2018 the lake experienced an overall improvement in water quality with significant decreases in summer mean tp and tn concentrations whereas no improvement of water quality has occurred after 2008 johansson et al 2019 additionally lake arreskov and its surroundings are an important site for a range of natural habitat types and is a resting ground for several protected bird species fyns amt 2006 2 2 model description and configuration the 1d process based lake hydrodynamic and ecosystem model complex gotm wet was applied and configured specifically to lake arreskov the framework for aquatic biogeochemical models fabm bruggeman and bolding 2014 provides the coupling between the gotm and wet models thereby ensuring feedback between the two models the lake version of the water column model gotm https gitlab com wet gotm tree au was applied to simulate a pelagic and a sediment domain in each model layer for lake arreskov gotm was configured with an initial maximum depth of 4 m and 16 layers in the water column with variable water level height throughout the simulation i e grid heights varied between 21 and 26 cm each water layer included a sediment layer of 10 cm and layer volumes and areas between layers and at the sediment water interface were derived from a lake specific hypsograph i e the relation between depth and horizontal area from the novana monitoring database we applied the positive definite and conservative extended modified patankar ordinary differential equation scheme for source and sink dynamics bruggeman et al 2007 to simulate lake ice thickness and cover implementation in gotm of the ice module from mylake saloranta and andersen 2007 was enabled water ecosystems tool wet is a process based lake ecosystem model capable of simulating biogeochemical and ecosystem interactions as well as dynamics from nutrients to multiple higher trophic levels within the water column and the top sediment schnedler meyer et al 2022 the model is a redesign of the aquatic ecosystem model fabm pclake hu et al 2016 which is based on the widely applied 0d aquatic ecosystem model pclake originally by janse van liere 1995 the model accounts for mass balances represented by total dry weight dw nitrogen n phosphorus p and silicon cycling between the various components of the ecosystem the lake arreskov model was configured with organic dissolved and particulate organic matter for dw n and p and inorganic nutrients no3 nh4 po4 and sio2 three phytoplankton groups diatoms cyanobacteria and other algae and one submerged rooted macrophyte group as well as with a food web consisting of two zooplankton groups daphnia and other zooplankton detritivorous macrozoobenthos juvenile zooplanktivorous and adult zoobenthivorous fish and piscivorous fish as well as the dynamics between these components for instance grazing predation nutrient uptake and shading fig 1 in the model primary producers may be limited by nitrogen phosphorus light and or temperature and additionally diatoms may be limited by silicate the three phytoplankton groups were chosen to represent the dominant phytoplankton genera observed in lake arreskov fig s4 the cyanobacteria group were configured to represent anabaena sp and aphanizomenon sp with active phototaxis i e cyanobacteria will swim upwards with a fixed speed whenever ambient light levels surpass a light detection threshold schnedler meyer et al 2022 whereas diatoms and the other algae group settle out of the water column with a fixed sinking rate the model assumes a fixed plant height of submerged macrophytes with a linear biomass dependent function from modelled biomass densities in dw to percentage coverage of the bottom sediment janse 2005 resuspension of particulate organic matter pom inorganic matter im and phytoplankton from the bottom sediment is reduced by a linear scale factor based on macrophyte biomass densities both phytoplankton and macrophytes influence the simulated light attenuation via a linear biomass dependent function zooplankton grazing efficiency is dependent on the specific zooplankton group and also varies between the phytoplankton groups by a phytoplankton specific preference factor hu et al 2016 janse 2005 which is often lowest for cyanobacteria light attenuation is affected by macrophyte biomass and pelagic concentrations of phytoplankton pom and im resuspension of particulates is dependent on wind speed and bioturbation by zoobenthos and adult zoobenthivorous fish for all three fish groups predation was modelled as a holling type iii response modified by water temperature and a macrophyte dependence assuming a linear decrease in feeding with increasing macrophytes biomass for plankti and benthivorous fish spawning of juvenile zooplanktivorous fish was simulated as a transfer of a fixed fraction of adult benthivorous fish biomass to juvenile zooplanktivorous fish every may likewise a biomass transfer from juvenile zooplanktivorous fish to adult benthivorous fish occurred at the end of every year to simulate aging for more details on the fish model description see hu et al 2016 and janse 2005 we also accounted for the fish harvest of juvenile and adult zooplanktivorous and zoobenthivorous fish 2 3 model forcing meteorological data on atmospheric forcing of gotm were obtained from the european ecmwf era5 dataset hersbach et al 2018 which provides data at an hourly resolution on air temperature o c air pressure hpa dew point temperature o c cloud cover and wind speed components m s in the north south and west east direction monthly averages of water inflow m3 s and nutrient concentrations mg l were used as boundary conditions see study site for more information initial values for the calibration of sediment inorganic matter particulate organic matter humus absorbed phosphate and the iron percentage of inorganic matter were estimated from sediment samples collected in 1992 by flindt et al 2015 with an estimated bulk density fish removal was applied as percentage removal of simulated fish dw p and n densities of juvenile planktivores and adult benthivores at specific dates dependent on cause of removal table 1 2 4 model calibration and performance the gotm wet model for lake arreskov was calibrated with two spin up years to initialise biogeochemical nutrient pools and a calibration period of 12 years 1992 2005 with a dataset including observed water temperatures in lake water quality and ecological variables we applied the entire dataset for calibration i e not including a validation period in an attempt to maximise the fit between observations and model simulations across periods both with and without macrophyte coverage this was also done in previous modelling efforts where challenges remained with reproducing the observed lake ecosystem dynamics e g nielsen et al 2014 the auto calibration program parsac version 0 5 7 bruggeman and bolding 2020 was used to perform an automatic global optimisation of a selected subset of model parameters parsac applies the parallel direct search method differential evolution storn and price 1997 to estimate the most optimal choice of model parameter values within predefined parameter specific ranges based on optimisation of a maximum likelihood multi objective function this allows a more thorough search for optimal parameter values in the global parameter space than previously achieved by manual trial and error calibration andersen et al 2020 the calibration procedure included 210 parameters selected from relevant sensitivity analyses andersen et al 2021 janse 2005 other temperate model studies andersen et al 2020 chen et al 2020 and user experience with the model during calibration initially the calibration procedure was based on a bottom up approach following seven consecutive steps with focus on calibrating specific state variable dynamics and model processes in each step 1 temperature and turbulence 2 do 3 nitrogen nitrification and denitrification 4 hypolimnetic phosphorus 5 all nutrients and plankton 6 macrophytes and 7 higher trophic levels and other relevant processes within each step several auto calibration iterations were undertaken with each iteration resulting in specific parameter value ranges being shifted and or narrowed this being determined by the modeller based on model performance evaluated by visual inspection of dotty plots relation between model parameter value and model performance metric performance metrics and the maximum likelihood multi objective function most of the calibration effort was spent on the last step with all relevant parameters and observations included in the auto calibration iterations to guide auto calibration of surface phytoplankton dynamics chl a concentrations with a mean depth of the pooled samples above 0 6 m were corrected to 0 2 m lastly the final auto calibration iteration including all selected model parameters against all state variables and their corresponding observations was conducted resulting in a model database of approx 350 000 model performances and their parameter values all auto calibration iterations were executed on a thinksystem sr850 with four intel xeon gold 6130 processors with a total of 64 cores the duration of one model run was often between 40 and 60 s and the final iteration ran for 10 days long term monitoring data from 600 danish shallow lakes mean depth 3 m showed an effect of increasing macrophytes coverage on chl a concentrations of up to 30 40 coverage s√∏ndergaard et al 2021 based on these findings we defined five ecological states based on macrophytes coverage no 10 sparse 10 30 and abundant macrophyte coverage 30 as well as between year shifts increase or reduction if coverage changed more than five percentage points in the following year for 10 30 macrophyte coverage each year in the observational data set and in model simulations was classified with an ecological state for an example see fig 3 model performance for the daily output of each state variable was evaluated by calculating pearson correlation r coefficient of determination r2 mean absolute relative error mare and percent bias pbias to describe both the correlation between observed and modelled values and also the bias or offset in the modelled values as recommended by bennett et al 2013 additionally we reported the percentage of observations and observations with estimated uncertainty within the simulated model ensemble span hit 2 5 model ensemble selection and uncertainty visualisation for the ensemble visualisation and evaluation a subset of model parameterisations from the final auto calibration iteration was selected from the model database results to explore the impacts of model performance criteria for model ensemble selection we tested several selection strategies based on bias reduction and or correlation maximisation on either a full water quality dataset water temperature tp po4 tn no3 nh4 chl a and zooplankton biomass concentrations or a limited water quality dataset water temperature tn tp and chl a concentrations in addition we tested a selection strategy focused only on simulating the observed shifts between phytoplankton and macrophyte coverage with a correlation maximisation of these this resulted in six different selection strategies 1 bias reduction of full water quality dataset 2 correlation maximisation of full water quality and macrophyte coverage dataset 3 correlation maximisation of chl a concentrations and macrophyte coverage 4 bias reduction of limited dataset 5 correlation maximisation of limited dataset and 6 combined bias reduction and correlation maximisation of limited dataset for each selection strategy the selection criteria for model performance were incrementally optimised until a subset sample of around 100 150 model parameterisations matched the criteria which required 5 10 trials of manually adjusting performance criteria to reach the desired sample size see table s1 for criteria for specific strategies for parameter value ranges for the calibrated parameters for ensembles see appendix b to visualise the range and centre of each single model ensemble set daily minimum maximum and volume or sediment area weighted means were extracted from each model in each ensemble from either the entire water column zooplankton macrophytes and fish variables or the surface layers 1 5 m temperature do nutrients and chl a for each ensemble set the ensemble span i e the extracted daily min and max values was set with a transparency of 90 to highlight agreement between ensemble sets and daily median values and daily model means from each ensemble were derived to represent the centre of the ensemble simulation range all model output was processed and visualised in python 3 6 with the packages xarray 0 15 1 hoyer and hamman 2017 pandas 0 1 5 mckinney 2011 matplotlib 3 1 2 hunter 2007 and seaborn 0 11 1 waskom 2021 to visualise in lake observation uncertainty we applied a percentage error to each state variable observation ranging between 25 phyto and zooplankton and 40 tp and tn based on a compilation of intercalibration exercises for the danish novana monitoring program and laboratory analysis uncertainty see appendix a for further details 2 6 model ensemble scenario simulations to investigate how the model ensembles captured shifting states at changes in nutrient load we performed two nutrient load change scenarios this also served as a test of some of the emergent properties to be expected in the lake arreskov model we defined a baseline period of the last four years of the model period 2000 2004 which closely resembled the mean external n and p load for the entire model period fig s5 to encompass the yearly minimum and maximum n and p loads of the model period for which the ecological state was observed to be either clear or turbid we defined increased and decreased external load scenarios as 75 or 75 respectively of the yearly external load in baseline baseline and nutrient load scenarios were looped five times creating a model period of 1992 2004 followed by baseline scenario period of 2004 2024 this scenario design allowed yearly variation between years in weather and inflow forcing and an adequate time period for most state variables to stabilise the two scenarios and baseline were run for all models in each ensemble in the last four years of each scenario the ecological state was classified as described in section 2 4 and volume weighted surface mean concentrations in summer were calculated for the water quality variables 3 results 3 1 model performance between evaluation strategies the six ensemble strategies included between 130 and 141 gotm wet model parameterisations in each ensemble based on adjusted selection criteria with a low e g po4 and tn to high e g temperature performance depending on the particular ensemble and state variable in general higher performance for ensemble elements was achieved for the state variables included in the selection strategy for the specific ensemble fig 2 for instance ensembles selected based on a limited water quality set temp tn tp and chl a with correlation as criterion ensembles 4 and 6 had the best correlation results for tn and ensemble 3 selected for improved correlation of primary producers performed best for chl a and macrophyte coverage however some ensembles had higher performance than ensembles that specifically included a state variable in their selection criteria for instance ensemble 5 and 6 had the highest quantiles and median for correlation performance for both daphnia and other zooplankton even though ensemble 1 and 2 included zooplankton as criteria the pbias reduction strategy clearly limited the ensemble span for targeted state variables compared to correlation maximisation not all selection criteria necessarily limited the amount of models in the ensemble for instance the lowest model performance for correlation of macrophyte coverage in ensemble 2 was r 0 18 but the correlation criteria used was r 0 05 3 2 temperature do and nutrients all ensembles reproduced the dynamics of observed water temperatures ranging between 1 and 29 c with high accuracy r2 0 98 to 0 99 and pbias 2 and agreement between ensemble strategies fig 3a with the inclusion of the water column model gotm several short term stratification events in temperature and do were simulated by the ensemble models in both summer and winter e g fig 4 a in most winters the simulated ice cover corresponded with actual lake observations fig s6 the model ensembles generally reproduced the observed seasonal patterns of do with agreement between median ensemble values fig 3b however the high variability in the observations was not fully captured ensemble median r2 between 0 02 and 0 22 but overall encapsulated by the ensemble spans 29 80 table s2 the overall seasonal dynamics of tp concentrations were captured by the ensembles albeit the seasonal dynamics of po4 concentrations were not reproduced well figs 3c and 5 c some models in all ensembles could reproduce the 1 2 fold increase in late summer tp concentrations summer peak concentrations between 0 3 and 0 6 mg p l in years with low to sparse macrophyte coverage but at the expense of underestimated po4 concentrations and overestimated summer tp concentrations in the years with macrophytes fig 3c the ensembles varied markedly in their ability to simulate po4 concentrations with ensemble 2 reproducing increased autumn concentrations and other ensembles e g ensemble 5 and 6 reproducing the magnitude of lower concentrations for the rest of the year fig 5c although decreased do concentrations in the deepest layers were simulated during short term stratification events this did not markedly impact the tp sediment release as the major part of the tp release was controlled by organic p pool mineralisation from the sediment the model included fe and al as a fixed fraction of the inorganic matter in the sediment set as parameter values to simulate p adsorption to fe and al dependent upon the anoxic to oxic fraction of the sediment for example in the model with highest r performance for tp in ensemble 1 the summer mean tp sediment release varied between 3 5 and 5 5 mg p m 2 day 1 with the majority of the p being released as pdom and the highest release occurring in august in all years data not shown the phytoplankton p uptake removed most of the available po4 summer mean uptake between 5 7 and 7 9 p m 2 day 1 compared to other fluxes while pelagic pdom mineralisation predominantly contributed to the po4 pool for nitrogen the ensembles encapsulated between 50 and 86 of the tn concentrations when accounting for observation errors table s2 with a general overestimation of tn ensemble median pbias between 2 and 75 especially in the years with high macrophyte coverage fig 3d all ensembles reproduced the seasonal and inter annual dynamics of no3 ensemble median r2 between 0 15 and 0 50 but most ensembles missed the timing with 1 3 months and overestimated the concentrations fig 5a the magnitude of nh4 concentrations was generally captured in spring and early summer which is also reflected in the summer mean concentrations across years in different ecological states but the variations observed in late summer concentrations of up to 25 90 fold within two months were not simulated by any of the ensembles fig 5b 3 3 primary producers and higher trophic levels none of the ensembles reproduced the timing and magnitude of the observed shifts in ecological state from no to high macrophyte coverage following the fish removal fig 6 a ensemble 1 and 3 simulated most accurately the mixed distribution of different ecological states fig 6a and ensemble 3 was the only ensemble with 39 out of 138 models simulating shifts between no and high macrophyte coverage in the model period the additional ensembles either simulated no ensemble 4 5 and 6 or high coverage ensemble 2 for the majority of models 82 and 92 99 respectively throughout the model period also the observed shift between submerged macrophytes and phytoplankton was not captured by any single ensemble as none of the ensembles reproduced the observed magnitudes of summer chl a for all periods with different macrophyte coverage fig 3e only ensemble 3 reproduced the observed chl a concentration ranges in model years with high coverage but also underestimated summer chl a concentrations at no to sparse macrophyte coverage the opposite was true for all other ensembles in all ensembles in the growth period may to september the macrophyte growth rate was rarely nutrient limited all ensemble medians 5 reduction in growth rate except in 1996 with ensemble 3 max 12 reduction instead macrophytes were light limited with reduced growth rates that range between no growth to 100 growth increasing with depth and varying between ensembles and time there were clear correlations between mean chl a concentrations in surface waters and macrophyte light limitation results not shown the observed phytoplankton community varied markedly with several different genera being dominant during the modelling period fig s7 the ensembles overall simulated similar phytoplankton dynamics in the years with spring diatom blooms followed by late summer cyanobacteria blooms encapsulating between 40 and 70 of the measured chl a concentrations table s2 during periods with low water column mixing in late summer cyanobacteria was modelled to actively seek surface waters simulating surface blooms fig 4c in summers with observed late cyanobacteria blooms some models in all ensembles except ensemble 3 reproduced the levels of total chl a concentrations with maximum concentrations between 400 and 800 Œºg chl a l albeit these dynamics were also reproduced in years where cyanobacteria blooms were not recorded in the bi weekly to monthly sampling campaign fig 3e the observed dynamics of the zooplankton groups daphnia and other zooplankton were overall well reproduced by several ensembles ensemble median r 0 03 to 0 36 with 67 88 and 50 70 of the observations captured respectively fig 6b and c in winter both simulated zooplankton groups generally died out although the observations showed that zooplankton species other than daphnia were present fig 6b to simulate biomanipulation and by catches of bream and roach the ensemble models were forced with a fish removal of 5 60 dependent upon year and fish group the mean of total fish wet weight biomasses removed per year for all ensembles varied between 0 3 and 22 2 mg with clear inter annual differences fig s7 the total amount of simulated fish removed from the lake ranged between 31 and 100 mg ww fish mean 56 mg ww fish in the years with biomanipulation the simulated fish removal significantly decreased the benthivorous fish biomass at the time of removal fig 6d whereas low removal of planktivorous fish exhibited no clear impact fig s8 to compare the effects of fish removal included in the simulations we executed all models in ensemble 1 3 and 6 without fish manipulation fish removal negatively impacted the concentrations of benthivorous fish for 4 5 months following the removal in mid april with changes in the mean monthly biomass from 146 to 111 in may to 21 to 26 in august compared to simulations without manipulation in the winter months in manipulation years and the subsequent years the differences in fish concentrations with and without manipulation varied between 10 no difference in ensemble median water quality concentrations was observed with or without fish removal in the manipulation years and the simulated fish removal did not clearly impact the simulation of chl a concentrations or the biomass of piscivorous fish figs 3e and 6e 3 4 nutrient load scenarios all ensembles responded to the 75 reduction or increase in nutrient load with changes in water quality and macrophyte coverage however with clear differences between the ensembles fig 7 for five out of the six ensembles the proportion of model years in the last four years of the model period with an ecological state with abundant macrophytes decreased with increased nutrient loading fig 7a the largest change in the proportion of ecological states occurred between the baseline and the 75 reduction scenario with 0 58 point increase the ensemble distributions of summer maximum macrophyte coverage also decreased along the scenario nutrient load gradient fig 7b responses in water quality also occurred as the ensembles ranges of summer mean tp tn and chl a overall shifted to higher concentrations with increased nutrient loading and all or five out of six ensemble median values increased for tp and tn or chl a and macrophyte coverage respectively fig 7b 4 discussion coupling the 1d hydrodynamic model gotm to the lake ecosystem model wet allowed for simulating physical and biogeochemical depth gradients in the water column and sediment the lake arreskov multiple model ensembles with the 1d lake ecosystem model gotm wet captured 8 80 of the observed water quality and 33 91 of the food web variables table s2 however the timing of shifts between the ecological states were not well reflected figs 3 5 and 6 still several model developments enabled a more accurate representation and simulation of the lake ecosystem compared to a previous modelling study using the 0d pclake model nielsen et al 2014 besides including a vertically resolved water column our study encompassed two simulated zooplankton groups and simulated fish harvest an expanded water quality dataset inorganic nutrients and new developments in auto calibration i e application of parsac and ensemble modelling the focus of this study was to test a 1d state of the art lake ecosystem model against a previously conducted case study with model difficulties and therefore each model development was not tested and compared directly to previous efforts nielsen et al 2014 we conducted an in depth analysis of model behaviour based on several ensemble simulations and placed our results in the context of simulating restored shallow lakes and compared them with the previous model study when relevant 4 1 improved water quality simulation by including the 1d hydrodynamic water column model gotm in the simulation water temperature across the depth was directly simulated with feedback from the biogeochemical processes in wet fig 3a which contrasts the 0d pclake study where water temperature was forced nielsen et al 2014 the observed water temperature dynamics were excellently reproduced with high agreement between ensembles fig 2 and performance was comparable to the best 10 of previous aquatic biogeochemical models arhonditsis and brett 2004 moreover although some ensembles simulated large ensembles spans the general seasonal do dynamics and magnitude were well reproduced and the performance was comparable to the best 10 20 of previous aquatic biogeochemical models with regard to relative error arhonditsis and brett 2004 the fluctuating summer do concentrations were not always captured likely due to inability of the ensembles to reproduce the timing of phytoplankton blooms and resuspension events several short term stratification events with clear vertical gradients in water temperature and dissolved oxygen were simulated by gotm wet e g fig 4 but were not always reflected in the water quality sampling of lake arreskov due to the bi weekly measurements explicit simulation of a vertical water column proved beneficial for modelling the phytoplankton dynamics in lake arreskov as it allowed simulated phytoplankton groups to be configured to float or swim upwards for increased light and or downwards for increased nutrients depending on the phytoplankton species being modelled in the newly developed wet model schnedler meyer et al 2022 in all the model ensembles cyanobacteria was modelled to move upwards for increased light in order to simulate the increased concentrations of the two cyanobacteria species aphanizonemenon sp and gloeotrichia echinulate at the surface during short term stratification events observed in most years in late summer fig s4 the ensemble models were capable of simulating clear differences in chl a concentrations between surface and deeper water layers fig 4c and reproduced the observed maximum levels of chl a concentrations during blooms fig 3e in comparison the previous model simulated maximum chl a concentrations of approx 200 Œºg l in most years and was therefore not able to reproduce the observed chl a peaks nielsen et al 2014 consequently the combination of simulating stratification events in a vertical water column model and phytoplankton swimming abilities proved central to simulate the dynamics of buoyant cyanobacteria during stratification in lake arreskov such combination will also be important when simulating water bodies with the potential to be or being impacted by harmful cyanobacterial blooms deteriorating the water quality huisman et al 2018 as many cyanobacterial species for instance microcystis sp wilkinson et al 2020 and planktothrix rubescens walsby et al 2006 can migrate vertically through the water column in periods with stagnant waters the simulated nutrient dynamics were also impacted by the inclusion of a vertical water column model as each water layer was connected to a sediment layer in gotm wet the previous 0d model study of lake arreskov did not report on the performance for inorganic nutrients no3 nh4 po4 and we therefore compared the model ensemble performance for inorganic nutrients to the total nutrient concentrations when reporting model performance it should be mentioned also that the previous study disregarded tp and tn concentrations measured in water shallower than 0 6 m although the concentrations here were often peak values nielsen et al 2014 despite this the ensemble simulations often reproduced the previous model performance ensemble medians max vs the nielsen et al 2014 r2 values tn 0 0 0 03 0 13 vs 0 03 po4 0 0 0 06 0 22 vs 0 06 or improved it no3 0 15 0 5 0 6 vs 0 03 depending on the specific model in the ensemble and performance metrics although nh4 simulations were likely not improved negative median r values for all ensembles overall for example the simulated summer nutrient retention in lake arreskov of 0 26 mg p m 2 day 1 and 12 mg n m 2 day 1 in 1994 for models in ensemble 1 was within the previously calculated ranges 0 24 to 2 2 mg p m 2 day 1 and 15 to 29 mg n m 2 day 1 based on the measured in and outflow discharge and nutrient concentrations fyns amt 1996 the simulated phosphate release from the sediment was also within the range of the previously measured tp sediment fluxes from lake arreskov and other meso trophic shallow lakes jensen and andersen 1992 before the model period organic p constituted a major fraction of the sediment p pool in the lake and the phosphate release likely originated from organic matter degradation jensen and andersen 1992 which was reproduced by the model yet most ensembles did not capture the late summer po4 concentration peaks likely due to underestimation of the redox dependent p release and overestimation of the cyanobacteria po4 uptake simulating p adsorption to fe and al is dependent upon the amounts of fe and al that are set by parameter values as a fixed fraction of the inorganic matter the external fe and al input is therefore not taken into account by the model although the fe input varies across the simulated period also in the wet model phytoplankton groups can only take up nutrients in the pelagic and it therefore did not fully simulate the behaviour of the observed dominant cyanobacteria species fig s4 that may take up p during a benthic growth period and later transfer it to the water by buoyancy cottingham et al 2020 reynolds 2006 as observed in lake arreskov from 1989 to 1991 aavad 1994 4 2 simulating shifts between ecological states the gotm wet model reproduced the expected difference between turbid phytoplankton dominated and clear water macrophyte dominated states across a nutrient load gradient as reflected by the nutrient load scenario results fig 7 and for some models in ensemble 3 however most model parameterisations were unable to simulate the fluctuating ecosystem states in the model period the model structure and or the model parameterisations were likely not adequately sensitive to the changes in forcing weather discharge and external nutrient load and the plankti benthivorous fish removal to reproduce the observed shifts below we discuss likely explanations of the model shortcomings related to the modelling of macrophytes zooplankton and fish the availability of light is central in the competition between phytoplankton and submerged macrophytes sand jensen and borum 1991 scheffer et al 1993 between the turbid and clear water states an intermediate state has been found to occur which is often characterised by a few macrophyte species that can complete their life cycle during the clear water phase in spring and early summer while later in summer cyanobacteria blooms often occur sayer et al 2010 this was observed in lake arreskov in the 2000s therefore the spring clear water phase most likely provides a window of opportunity for submerged macrophyte establishment its length timing and strength being crucial for the macrophyte development which may depend on among others weather and planktivorous fish hilt et al 2018 phillips et al 2016 the model was able to simulate the feedback between phytoplankton and macrophytes to light for instance the shading of macrophytes by phytoplankton as demonstrated by the macrophyte growth rate reductions due to light limitation recorded by all ensembles across the growth season however variations in the clear water phase were not adequate to reproduce the timing of the macrophyte establishment by the ensembles competition between periphyton and macrophytes also impacts the light availability for macrophytes and is considered an important mechanism in the shift between turbid and clear water states hilt et al 2013 sayer et al 2010 with the inclusion of periphyton through an empirical relationship with nutrients in a modified version of 0d pclake the modelling results showed a weakening of macrophyte growth conditions at higher nutrient loads due to periphyton shading hilt et al 2018 inclusion of periphyton would provide an additional mechanism weakening the established growth conditions for macrophytes thereby potentially improving the model s ability to simulate macrophyte loss 4 3 two zooplankton groups two zooplankton groups daphnia sp and other meso zooplankton were included in the lake arreskov food web fig 6b and c which reproduced or improved the zooplankton performance depending upon the specific ensemble ensemble medians max vs nielsen et al 2014 r2 values daphnia 0 02 0 1 0 28 vs 0 05 other zoopl 0 0 0 13 0 2 vs 0 05 large bodied zooplankton such as daphnia sp are more vulnerable to fish predation burks et al 2002 so following removal of planktivorous fish a predation release on large bodied zooplankton should occur increasing the grazing pressure on phytoplankton and thereby increasing water clarity jeppesen et al 2012 the mean size of cladocerans did increase during or after the biomanipulation in lake arreskov but large blooms of cyanobacteria still occurred in late summer hansen and hansen 2007 with the application of several zooplankton groups now common in water quality and lake ecosystem modelling e g bruce et al 2006 kong et al 2017 andersen et al 2020 this shift from small to large bodied zooplankton after for instance fish removal can be included in the model and thereby refine the description of the pelagic trophic cascade after biomanipulation depending on the choice of zooplankton groups jeppesen et al 2011 4 4 fish and biomanipulation both the lower external nutrient load in 1996 1997 and the fish removal between 1990 and 1997 are likely explanations of the reestablishment of submerged macrophytes and the shift in ecological states in lake arreskov hansen and hansen 2007 nielsen et al 2014 therefore both drivers were included in this study the total number of simulated fish removed from the lake varied between 31 and 100 mg ww ensemble median between 49 and 62 mg ww fish which was comparable to the 52 57 mg ww fish removed during the six years of reported bycatch and biomanipulation table 1 the fish removal was implemented as percentage removal rates so the ensembles overall reproduced the magnitude of the fish concentrations and harvest however after the fish removal no long term reduction in benthivorous fish or an increase in piscivorous fish was simulated as otherwise observed in lake arreskov hansen and hansen 2007 and expected from other biomanipulation studies jeppesen et al 2012 older and larger benthivorous fish are usually targeted in biomanipulations as these have a proportionally higher importance for whether the lake stays in a turbid phytoplankton dominated state by for instance increasing sediment resuspension negative impacts on macrophyte shoots and the fact that they are too large to be controlled by piscivorous fish e g jeppesen et al 2012 our wet model was configured with three fish groups connected juvenile zooplankton fish and adult benthivorous fish and a piscivorous fish group which did not allow for inclusion of size or more advanced life history traits however the wet food web configuration in wet is highly flexible schnedler meyer et al 2022 permitting investigation of different fish group configurations for instance with several groups of adult benthivorous fish to mimic different ecosystem impacts with life history changes besides wet 0d pclake is one of the few lake ecosystem models simulating fish and an effect of macrophytes on fish foraging efficiency janse 1997 janse et al 2008 benthic macroinvertebrate densities are higher in clear water lakes with high coverage of submerged macrophytes than in turbid lakes boll et al 2012 as many macroinvertebrates live on and of associated epiphyton and constitute an important food source for for instance juvenile perch vadeboncoeur et al 2002 however vegetation associated macroinvertebrates were not included in the lake arreskov model or in various versions of pclake either which might explain the model s underestimation of piscivorous fish concentrations as an important food source under clear water conditions could be missing 4 5 model performance in the context of the current state of the art overall the performance of the ensemble modelling of lake arreskov was comparable to that of previous model studies on water quality variables arhonditsis and brett 2004 many of the former model studies have covered shorter time scales 1 6 years e g janse and aldenberg 1990 elliott et al 2005 and systems that have not undergone restoration which somewhat eases the ability to capture ecosystem dynamics some model studies have simulated general effects of biomanipulation on lake food webs h√•kanson et al 2003 or ecological states with hypothetical lakes janssen et al 2019b and only few case studies have attempted to reproduce lake ecosystem dynamics before and after in lake restoration reflecting the need for comprehensive datasets for use in modelling however such datasets are often lacking for lake restoration projects jeppesen et al 2005 for example 0d pclake reproduced to a certain extent the observed short term effects of lake restoration in lake zwemlust after removal and restocking of fish and zooplankton transplantation of macrophytes and refilling the lake janse et al 1995 a modified version of 0d pclake was applied to a subtropical large shallow lake in china lake chaohu kong et al 2017 and reproduced the observed patterns of decadal impacts of hydrological regulation although only performance metrics after the intervention were available for fish with the gotm wet model applied in this paper several advantages appear that limit the difficulties of simulating the impacts of restoration in shallow lakes this encourages more case studies that will further test the model s ability to simulate restoration attempts our approach with multiple single model ensembles with different model performance criteria expanded the incorporated model uncertainty to include both model parameter and performance metric uncertainty we used this approach as our focus was on the ability of the individual model ensemble to investigate in depth the behaviour of the model and the variations in simulation behaviour due to variations in model parameters however model uncertainty also stems from model structure initial and boundary conditions and inputs khatami et al 2019 and the simulated ecosystem dynamics are dependent upon the food web configuration for example the configuration of the fish groups could be important for simulating the impacts of fish removal as discussed above so a natural next step would be to incorporate and investigate the structural uncertainty of the model thereby moving from single model to multi model ensembles when modelling shallow lakes which is facilitated by the modularisation of organism groups in wet 4 6 models in the context of management before evaluating a potential restoration measure through simulations any lake model study should incorporate the most important processes and dynamics in the lake ecosystem short term stratification can be an important descriptor as illustrated here with lake arreskov which highlights the benefits of coupling a fully dynamic water column model to a lake ecosystem model when simulating shallow lakes with the availability of worldwide high frequency meteorological data e g the ecmwf era5 dataset previous challenges arising from data limitation for running hydrodynamic models are now overcome accordingly simulations of mixing and stratification can rely on physical descriptions lake morphology and local weather as well as feedback from biogeochemical processes instead of empirical relations user data on stratification depth and timing janssen et al 2019a or an assumption of non stratification with the inclusion of ice cover in several water column models e g gotm mylake glm dyresm simulating vertical mixing and thermal regimes should now be possible for most shallow lakes worldwide for a successful lake restoration planning should be lake specific abell et al 2020 s√∏ndergaard et al 2007 as it requires an understanding of a range of limnological characteristics such as lake type nutrient sources loading history and food web structure hamilton et al 2016 process based lake models incorporate relevant limnological characteristics and can be made lake specific through configuration and model calibration thus making lake models a powerful planning tool e g √∂zkundakci et al 2011 andersen et al 2020 that can also provide predictions of impacts of climate warming e g trolle et al 2020 or fish removal on the ecological state in this study we applied wet a state of the art lake ecosystem model coupled to gotm a fully dynamic water column model and illustrated the improvements in simulating stratification and water quality extremes e g cyanobacteria surface blooms compared to previous model efforts with a traditional box model approach inclusion of an additional zooplankton group in the modelled food web configuration for lake arreskov also improved the ability of the model ensemble to reproduce the observed zooplankton dynamics however further improvements of the state of the art lake ecosystem models such as wet are needed to more accurately simulate shifts in ecological state produced by internal restoration measures as the timing of these shifts could not be reproduced despite adding more details to the model and transition to a 1d version software availability name of software water ecosystems tool wet developers nicolas aza√±a schnedler meyer tobias kuhlmann andersen karsten bolding fenjuan rose schmidt hu anders nielsen dennis trolle contact address department of ecoscience aarhus university vejls√∏vej 25 8600 silkeborg denmark email wet info wet au dk availability https gitlab com wet wet releases v0 1 0 license freely available under gnu general public license gpl version 2 the model configuration required to run the gotm wet model for lake arreskov is included in the wet repository as a test case name of software general ocean turbulence model gotm developers lars umlauf hans burchard and karsten bolding availability https gitlab com wet gotm releases v5 2 2 au documentation www gotm net license freely available under gnu general public license gpl version 2 name of software framework for aquatic biogeochemical models fabm developers jorn bruggeman and karsten bolding availability www gitlab com wet fabm and https github com fabm model fabm documentation bruggeman and bolding 2014 license freely available under gnu general public license gpl version 2 name of software parallel sensitivity analysis and calibration parsac developers jorn bruggeman and karsten bolding availability www doi org 10 5281 zenodo 4276111 and http www github com boldingbruggeman parsac license freely available under gnu general public license gpl version 2 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported partly by a research project on process based lake ecosystem models supported by the danish environmental protection agency ph d funding by the sino danish centre for education and research and the poul due jensen foundation e j was supported by the t√ºbitak bideb2232 program 118c250 a previous manuscript version of this article was included in the ph d dissertation by tobias kuhlmann andersen and we thank ian jones christian skov and peter borgen s√∏rensen for valuable comments and an insightful discussion we also highly appreciate the thoughtful comments and suggestions made by two anonymous reviewers finally we thank anne mette poulsen and tinna christensen for valuable editorial and layout assistance and the python community for asking and answering pressing questions in our efforts to process and visualise our model results appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105501 
25543,we investigated the ability of the process based 1 dimensional hydrodynamic ecosystem lake model gotm wet to reproduce the fluctuating dynamics and shifts between turbid and clear water states following restoration in temperate shallow lake arreskov denmark the lake model was calibrated on a comprehensive 12 year dataset with a multiple single model ensemble approach to address model parameter and performance metric uncertainty compared to earlier modelling attempts on this lake the gotm wet model which enables simulation of water column hydrodynamics such as stratification events and two zooplankton groups improved the simulation of the maximum chlorophyll a concentrations during cyanobacteria blooms and zooplankton dynamics however the timing of shifts between phytoplankton and submerged macrophytes dominance following fish removal was not well reproduced although both states were simulated we discuss potential improvements of the model to enhance the ability to simulate the effects of restoration on the food web and ecological states in this and similar lakes keywords aquatic ecosystem modelling water ecosystems tool shallow lake lake restoration general ocean turbulence model water quality 1 introduction shallow lakes are the most abundant lake type in the world meerhoff and jeppesen 2009 and provide essential ecosystem services such as drinking water supply habitats for aquatic flora and fauna and recreational services hilt et al 2017 however shallow lake ecosystems have undergone a continuing deterioration worldwide caused particularly by eutrophication smith and schindler 2009 with the symptoms being further exacerbated by the global changes in climate jeppesen et al 2020 to mitigate these negative impacts restoration measures are implemented for instance by decreasing the external nutrient loads through improved waste water treatment or broader catchment management jeppesen et al 2005 occasionally followed by in lake restoration methods s√∏ndergaard et al 2007 however management interventions can be costly and lead to uncertain outcomes abell et al 2020 therefore before implementing restoration actions it is important to make an assessment of how a specific lake ecosystem is likely to respond to available restoration measures and take into account the characteristics of a given system and its history to inform lake management and stewardship of the best practices in restoration planning process based lake ecosystem models can be powerful predictive tools but accurate simulation of specific restoration effects is required the in lake restoration method biomanipulation which can include removal of plankti and benthivorous fish and or stocking of piscivorous fish has been extensively used in northern temperate lakes in europe s√∏ndergaard et al 2007 its efficiency relies on the trophic cascade hypothesis according to which manipulation of the fish stock leads to increased zooplankton grazing which again results in decreased phytoplankton concentrations and increased water clarity carpenter and kitchell 1996 jeppesen et al 2012 some widely used process based lake ecosystem models include fish groups e g caedym and pclake mooij et al 2010 or have been coupled to an individual based fish model makler pick et al 2011 allowing simulation of the effects of fisheries and stocking however fish groups are rarely included in case studies soares and calijuri 2021 and if included the model results on fish are rarely presented e g ula≈Ñczyk et al 2021 zhang et al 2022 of the widely used models capable of fish simulation only a few e g pclake by janse 2005 are able to simulate shifts in ecological state between turbid and clear water for shallow lakes the models that do typically assume a homogenous water column by simulating biogeochemical processes in a box model 0 dimensional approach or they rely on forcing functions or simple empirical relationships that impose stratification but have not yet been quantitatively validated janssen et al 2019a however many shallow lakes undergo multiple short term thermal stratification events lewis jr 1983 that routine monitoring campaigns are not likely to capture even under short term stratification the distribution of biota and rates of biogeochemical processes may vary substantially across the depth thus impacting the overall assessment of the ecosystem e g bartosiewicz et al 2019 kangur et al 2018 several phytoplankton species can actively seek optimal placement in a stratified water column resulting in specific depth chlorophyll a chl a maxima hamilton et al 2010 and phosphorus sediment release rates depend on the redox potential and dissolved oxygen concentrations that can vary with depth jensen and andersen 1992 furthermore to increase confidence and trust in the capability of the lake models to provide reliable insights into the effects of restoration measures the models should be able to accurately reproduce the observed impacts of restoration efforts a few case studies exist where shallow lake models have been applied in an attempt to reproduce shifts in ecological state after in lake restoration often with mixed success janse et al 1995 kong et al 2017 nielsen et al 2014 for example the 0d pclake model could not adequately reproduce water quality dynamics during short term stratification events and shifts between clear water and turbid conditions before and after an external nutrient load reduction and fish harvests in shallow lake arreskov denmark nielsen et al 2014 nielsen et al 2014 suggested that a key reason for the model s underestimation of phytoplankton bloom concentrations was inability to simulate short term stratification events and the effect that this could have via the buoyancy of phytoplankton due to conceptual shortcomings in this study our aim was to reproduce the effects of external and in lake restoration measures on water quality dynamics and shifts in ecological state in a shallow lake with an improved state of the art lake ecosystem model we applied the new 1 dimensional 1d coupled hydrodynamic ecosystem model gotm wet general ocean turbulence model gotm coupled to the water ecosystems tool wet which includes water quality and food web dynamics ranging from nutrients to fish with updated meteorological forcing and nutrient load boundary conditions on shallow lake arreskov denmark relative to nielsen et al 2014 wet is redesigned from the original 0d pclake model janse 2005 and the 1d fabm pclake model hu et al 2016 and now features fish harvest it has a flexible food web configuration and when coupled to gotm a vertically resolved water column enabling simulation of stratification and phytoplankton movement the latter previously being identified as a key shortcoming of the 0d pclake model nielsen et al 2014 for an in depth analysis of the model capabilities we applied several single model ensembles based on different model performance evaluation strategies the term ensembles is used to represent a set of models with identical gotm wet configuration but varying parameter values considering the results of the updated lake ecosystem model for lake arreskov we highlight the benefits of applying a 1d water column model and discuss the model s ability to capture shifting ecological states and the potential for further improving the model for use in shallow lake management and planning 2 methods and materials 2 1 study site and sampling lake arreskov is a shallow lake located on the island of funen denmark mean depth 1 9 m maximum depth 3 6 m surface area 3 17 km2 the climate is temperate with an annual mean precipitation of 753 mm 1990 2010 and a mean winter october april and summer may september temperature of 4 8 c and 15 7 c respectively dee et al 2011 riddersholm and scharling 2010 the lake receives water from 13 tributaries of which half is artificially constructed to drain lowland areas and it has one outlet towards east fig s1 the lake has a hydraulic retention time of 1 1 1 6 yr the catchment area 28 km2 comprises mainly agriculture 58 and forest 36 with some urban areas as well fig s1 nielsen et al 2000 monitoring of the ecological state of lake arreskov and the external nutrient load from its catchment has been undertaken since 1989 as part of the national monitoring and assessment program for the aquatic and terrestrial environment in denmark novana kronvang et al 1993 lauridsen et al 2007 catchment monitoring encompasses the daily flow and bimonthly nutrient total nitrogen tn nitrate no3 ammonium nh4 total phosphorus tp and phosphate po4 concentrations in the three major tributaries draining approx 56 of the catchment area fig s1 thodsen et al 2019 estimates from the ungauged catchment area are based on the dk qnp model adapted to danish catchments windolf et al 2011 and estimates of inlet inorganic nutrient concentration fractions figs s2 and s3 the in lake water quality monitoring comprises 19 samples year of nutrients tn no3 tp po4 and chl a as a pooled sample from the surface to twice the measured secchi depth between 0 1 and 3 4 m as well as temperature and dissolved oxygen do measurements at 4 7 points evenly spaced from the surface to the bottom of the lake kristensen et al 1992 lauridsen et al 2007 phyto and zooplankton are sampled every 2 4 weeks from the surface to twice the secchi depth or from the entire water column respectively and usually identified to genus or species level kristensen et al 1992 lauridsen et al 2007 plankton dry weight biomasses are estimated based on phytoplankton taxa specific volume formula johansson and s√∏ndergaard 2017 or zooplankton taxa specific length to biomass relations hansen et al 1992 johansson 2011 and aggregated to diatoms cyanobacteria and other algae or to daphnia and other zooplankton respectively in the comparison for further details on phytoplankton sampling and handling see appendix a the mean depth of the pooled water quality and plankton samples represent the sampled depth of observations for model comparison additionally submerged macrophyte coverage was monitored once a year from 1993 to 1994 in mid august and from 1995 to 2005 in july to estimate uncertainty in in lake observations results from historical comparison projects of the participants samplers in the novana program were compiled and percentage coefficients of variation cv were calculated for nutrient concentrations submerged macrophytes phytoplankton biovolumes and zooplankton biomasses appendix a lake arreskov has been impacted by anthropogenic activity for the majority of the last hundred years in the 1920 1990s the lake was eutrophic with low secchi depth and frequent cyanobacteria blooms but a relatively rich aquatic vegetation was observed on some shores of the lake fyns amt 1990 petersen 1920 1950 in the 1950 1980s the lake received untreated and later mechanically treated sewage water from the nearby town of korinth 970 person equivalents the sewage was diverted in 1983 but the lake still received untreated wastewater from 124 rural households as well as runoff during storm flows through the 1990s fyns amt 1996 commercial eel fishery was established in the lake in 1963 which resulted in varied amounts of by catches mainly of cyprinid fish hansen and hansen 2007 from 1990 to 1997 a combination of by catches from commercial fishing and active harvest of cyprinids removed 52 57 mg wet weight ww fish mainly roach rutilus rutilus and bream abramis brama partly with the aim of restoring the ecological state of the lake sandby 1998 fyns amt 2006 s√∏ndergaard et al 2007 table 1 additionally stocking of 15 000 to 50 000 piscivore fingerlings pike esox lucius was undertaken each year in may in 1993 and 1995 1997 hansen and hansen 2007 two wetlands receiving water from some of the cultivated areas in the catchment were established adjacent to the largest tributaries in 2003 wetland area 0 39 km2 geddeb√¶kken and 2005 wetland area 0 1 km2 hammerdam to further reduce the external nutrient load hansen et al 2011 hoffmann et al 2004 also in 2005 to sustain discharges throughout summer when phosphorus concentrations were typically higher water level regulation at the outlet was initiated since the monitoring and restoration measures were implemented the ecological state of lake arreskov has shifted between turbid and clear water conditions several times johansson et al 2019 thus the lake has had three periods with high macrophyte coverage above 30 in 1997 1998 2005 2006 and 2011 usually with corresponding changes in secchi depth and chl a concentrations from 1989 to 2018 the lake experienced an overall improvement in water quality with significant decreases in summer mean tp and tn concentrations whereas no improvement of water quality has occurred after 2008 johansson et al 2019 additionally lake arreskov and its surroundings are an important site for a range of natural habitat types and is a resting ground for several protected bird species fyns amt 2006 2 2 model description and configuration the 1d process based lake hydrodynamic and ecosystem model complex gotm wet was applied and configured specifically to lake arreskov the framework for aquatic biogeochemical models fabm bruggeman and bolding 2014 provides the coupling between the gotm and wet models thereby ensuring feedback between the two models the lake version of the water column model gotm https gitlab com wet gotm tree au was applied to simulate a pelagic and a sediment domain in each model layer for lake arreskov gotm was configured with an initial maximum depth of 4 m and 16 layers in the water column with variable water level height throughout the simulation i e grid heights varied between 21 and 26 cm each water layer included a sediment layer of 10 cm and layer volumes and areas between layers and at the sediment water interface were derived from a lake specific hypsograph i e the relation between depth and horizontal area from the novana monitoring database we applied the positive definite and conservative extended modified patankar ordinary differential equation scheme for source and sink dynamics bruggeman et al 2007 to simulate lake ice thickness and cover implementation in gotm of the ice module from mylake saloranta and andersen 2007 was enabled water ecosystems tool wet is a process based lake ecosystem model capable of simulating biogeochemical and ecosystem interactions as well as dynamics from nutrients to multiple higher trophic levels within the water column and the top sediment schnedler meyer et al 2022 the model is a redesign of the aquatic ecosystem model fabm pclake hu et al 2016 which is based on the widely applied 0d aquatic ecosystem model pclake originally by janse van liere 1995 the model accounts for mass balances represented by total dry weight dw nitrogen n phosphorus p and silicon cycling between the various components of the ecosystem the lake arreskov model was configured with organic dissolved and particulate organic matter for dw n and p and inorganic nutrients no3 nh4 po4 and sio2 three phytoplankton groups diatoms cyanobacteria and other algae and one submerged rooted macrophyte group as well as with a food web consisting of two zooplankton groups daphnia and other zooplankton detritivorous macrozoobenthos juvenile zooplanktivorous and adult zoobenthivorous fish and piscivorous fish as well as the dynamics between these components for instance grazing predation nutrient uptake and shading fig 1 in the model primary producers may be limited by nitrogen phosphorus light and or temperature and additionally diatoms may be limited by silicate the three phytoplankton groups were chosen to represent the dominant phytoplankton genera observed in lake arreskov fig s4 the cyanobacteria group were configured to represent anabaena sp and aphanizomenon sp with active phototaxis i e cyanobacteria will swim upwards with a fixed speed whenever ambient light levels surpass a light detection threshold schnedler meyer et al 2022 whereas diatoms and the other algae group settle out of the water column with a fixed sinking rate the model assumes a fixed plant height of submerged macrophytes with a linear biomass dependent function from modelled biomass densities in dw to percentage coverage of the bottom sediment janse 2005 resuspension of particulate organic matter pom inorganic matter im and phytoplankton from the bottom sediment is reduced by a linear scale factor based on macrophyte biomass densities both phytoplankton and macrophytes influence the simulated light attenuation via a linear biomass dependent function zooplankton grazing efficiency is dependent on the specific zooplankton group and also varies between the phytoplankton groups by a phytoplankton specific preference factor hu et al 2016 janse 2005 which is often lowest for cyanobacteria light attenuation is affected by macrophyte biomass and pelagic concentrations of phytoplankton pom and im resuspension of particulates is dependent on wind speed and bioturbation by zoobenthos and adult zoobenthivorous fish for all three fish groups predation was modelled as a holling type iii response modified by water temperature and a macrophyte dependence assuming a linear decrease in feeding with increasing macrophytes biomass for plankti and benthivorous fish spawning of juvenile zooplanktivorous fish was simulated as a transfer of a fixed fraction of adult benthivorous fish biomass to juvenile zooplanktivorous fish every may likewise a biomass transfer from juvenile zooplanktivorous fish to adult benthivorous fish occurred at the end of every year to simulate aging for more details on the fish model description see hu et al 2016 and janse 2005 we also accounted for the fish harvest of juvenile and adult zooplanktivorous and zoobenthivorous fish 2 3 model forcing meteorological data on atmospheric forcing of gotm were obtained from the european ecmwf era5 dataset hersbach et al 2018 which provides data at an hourly resolution on air temperature o c air pressure hpa dew point temperature o c cloud cover and wind speed components m s in the north south and west east direction monthly averages of water inflow m3 s and nutrient concentrations mg l were used as boundary conditions see study site for more information initial values for the calibration of sediment inorganic matter particulate organic matter humus absorbed phosphate and the iron percentage of inorganic matter were estimated from sediment samples collected in 1992 by flindt et al 2015 with an estimated bulk density fish removal was applied as percentage removal of simulated fish dw p and n densities of juvenile planktivores and adult benthivores at specific dates dependent on cause of removal table 1 2 4 model calibration and performance the gotm wet model for lake arreskov was calibrated with two spin up years to initialise biogeochemical nutrient pools and a calibration period of 12 years 1992 2005 with a dataset including observed water temperatures in lake water quality and ecological variables we applied the entire dataset for calibration i e not including a validation period in an attempt to maximise the fit between observations and model simulations across periods both with and without macrophyte coverage this was also done in previous modelling efforts where challenges remained with reproducing the observed lake ecosystem dynamics e g nielsen et al 2014 the auto calibration program parsac version 0 5 7 bruggeman and bolding 2020 was used to perform an automatic global optimisation of a selected subset of model parameters parsac applies the parallel direct search method differential evolution storn and price 1997 to estimate the most optimal choice of model parameter values within predefined parameter specific ranges based on optimisation of a maximum likelihood multi objective function this allows a more thorough search for optimal parameter values in the global parameter space than previously achieved by manual trial and error calibration andersen et al 2020 the calibration procedure included 210 parameters selected from relevant sensitivity analyses andersen et al 2021 janse 2005 other temperate model studies andersen et al 2020 chen et al 2020 and user experience with the model during calibration initially the calibration procedure was based on a bottom up approach following seven consecutive steps with focus on calibrating specific state variable dynamics and model processes in each step 1 temperature and turbulence 2 do 3 nitrogen nitrification and denitrification 4 hypolimnetic phosphorus 5 all nutrients and plankton 6 macrophytes and 7 higher trophic levels and other relevant processes within each step several auto calibration iterations were undertaken with each iteration resulting in specific parameter value ranges being shifted and or narrowed this being determined by the modeller based on model performance evaluated by visual inspection of dotty plots relation between model parameter value and model performance metric performance metrics and the maximum likelihood multi objective function most of the calibration effort was spent on the last step with all relevant parameters and observations included in the auto calibration iterations to guide auto calibration of surface phytoplankton dynamics chl a concentrations with a mean depth of the pooled samples above 0 6 m were corrected to 0 2 m lastly the final auto calibration iteration including all selected model parameters against all state variables and their corresponding observations was conducted resulting in a model database of approx 350 000 model performances and their parameter values all auto calibration iterations were executed on a thinksystem sr850 with four intel xeon gold 6130 processors with a total of 64 cores the duration of one model run was often between 40 and 60 s and the final iteration ran for 10 days long term monitoring data from 600 danish shallow lakes mean depth 3 m showed an effect of increasing macrophytes coverage on chl a concentrations of up to 30 40 coverage s√∏ndergaard et al 2021 based on these findings we defined five ecological states based on macrophytes coverage no 10 sparse 10 30 and abundant macrophyte coverage 30 as well as between year shifts increase or reduction if coverage changed more than five percentage points in the following year for 10 30 macrophyte coverage each year in the observational data set and in model simulations was classified with an ecological state for an example see fig 3 model performance for the daily output of each state variable was evaluated by calculating pearson correlation r coefficient of determination r2 mean absolute relative error mare and percent bias pbias to describe both the correlation between observed and modelled values and also the bias or offset in the modelled values as recommended by bennett et al 2013 additionally we reported the percentage of observations and observations with estimated uncertainty within the simulated model ensemble span hit 2 5 model ensemble selection and uncertainty visualisation for the ensemble visualisation and evaluation a subset of model parameterisations from the final auto calibration iteration was selected from the model database results to explore the impacts of model performance criteria for model ensemble selection we tested several selection strategies based on bias reduction and or correlation maximisation on either a full water quality dataset water temperature tp po4 tn no3 nh4 chl a and zooplankton biomass concentrations or a limited water quality dataset water temperature tn tp and chl a concentrations in addition we tested a selection strategy focused only on simulating the observed shifts between phytoplankton and macrophyte coverage with a correlation maximisation of these this resulted in six different selection strategies 1 bias reduction of full water quality dataset 2 correlation maximisation of full water quality and macrophyte coverage dataset 3 correlation maximisation of chl a concentrations and macrophyte coverage 4 bias reduction of limited dataset 5 correlation maximisation of limited dataset and 6 combined bias reduction and correlation maximisation of limited dataset for each selection strategy the selection criteria for model performance were incrementally optimised until a subset sample of around 100 150 model parameterisations matched the criteria which required 5 10 trials of manually adjusting performance criteria to reach the desired sample size see table s1 for criteria for specific strategies for parameter value ranges for the calibrated parameters for ensembles see appendix b to visualise the range and centre of each single model ensemble set daily minimum maximum and volume or sediment area weighted means were extracted from each model in each ensemble from either the entire water column zooplankton macrophytes and fish variables or the surface layers 1 5 m temperature do nutrients and chl a for each ensemble set the ensemble span i e the extracted daily min and max values was set with a transparency of 90 to highlight agreement between ensemble sets and daily median values and daily model means from each ensemble were derived to represent the centre of the ensemble simulation range all model output was processed and visualised in python 3 6 with the packages xarray 0 15 1 hoyer and hamman 2017 pandas 0 1 5 mckinney 2011 matplotlib 3 1 2 hunter 2007 and seaborn 0 11 1 waskom 2021 to visualise in lake observation uncertainty we applied a percentage error to each state variable observation ranging between 25 phyto and zooplankton and 40 tp and tn based on a compilation of intercalibration exercises for the danish novana monitoring program and laboratory analysis uncertainty see appendix a for further details 2 6 model ensemble scenario simulations to investigate how the model ensembles captured shifting states at changes in nutrient load we performed two nutrient load change scenarios this also served as a test of some of the emergent properties to be expected in the lake arreskov model we defined a baseline period of the last four years of the model period 2000 2004 which closely resembled the mean external n and p load for the entire model period fig s5 to encompass the yearly minimum and maximum n and p loads of the model period for which the ecological state was observed to be either clear or turbid we defined increased and decreased external load scenarios as 75 or 75 respectively of the yearly external load in baseline baseline and nutrient load scenarios were looped five times creating a model period of 1992 2004 followed by baseline scenario period of 2004 2024 this scenario design allowed yearly variation between years in weather and inflow forcing and an adequate time period for most state variables to stabilise the two scenarios and baseline were run for all models in each ensemble in the last four years of each scenario the ecological state was classified as described in section 2 4 and volume weighted surface mean concentrations in summer were calculated for the water quality variables 3 results 3 1 model performance between evaluation strategies the six ensemble strategies included between 130 and 141 gotm wet model parameterisations in each ensemble based on adjusted selection criteria with a low e g po4 and tn to high e g temperature performance depending on the particular ensemble and state variable in general higher performance for ensemble elements was achieved for the state variables included in the selection strategy for the specific ensemble fig 2 for instance ensembles selected based on a limited water quality set temp tn tp and chl a with correlation as criterion ensembles 4 and 6 had the best correlation results for tn and ensemble 3 selected for improved correlation of primary producers performed best for chl a and macrophyte coverage however some ensembles had higher performance than ensembles that specifically included a state variable in their selection criteria for instance ensemble 5 and 6 had the highest quantiles and median for correlation performance for both daphnia and other zooplankton even though ensemble 1 and 2 included zooplankton as criteria the pbias reduction strategy clearly limited the ensemble span for targeted state variables compared to correlation maximisation not all selection criteria necessarily limited the amount of models in the ensemble for instance the lowest model performance for correlation of macrophyte coverage in ensemble 2 was r 0 18 but the correlation criteria used was r 0 05 3 2 temperature do and nutrients all ensembles reproduced the dynamics of observed water temperatures ranging between 1 and 29 c with high accuracy r2 0 98 to 0 99 and pbias 2 and agreement between ensemble strategies fig 3a with the inclusion of the water column model gotm several short term stratification events in temperature and do were simulated by the ensemble models in both summer and winter e g fig 4 a in most winters the simulated ice cover corresponded with actual lake observations fig s6 the model ensembles generally reproduced the observed seasonal patterns of do with agreement between median ensemble values fig 3b however the high variability in the observations was not fully captured ensemble median r2 between 0 02 and 0 22 but overall encapsulated by the ensemble spans 29 80 table s2 the overall seasonal dynamics of tp concentrations were captured by the ensembles albeit the seasonal dynamics of po4 concentrations were not reproduced well figs 3c and 5 c some models in all ensembles could reproduce the 1 2 fold increase in late summer tp concentrations summer peak concentrations between 0 3 and 0 6 mg p l in years with low to sparse macrophyte coverage but at the expense of underestimated po4 concentrations and overestimated summer tp concentrations in the years with macrophytes fig 3c the ensembles varied markedly in their ability to simulate po4 concentrations with ensemble 2 reproducing increased autumn concentrations and other ensembles e g ensemble 5 and 6 reproducing the magnitude of lower concentrations for the rest of the year fig 5c although decreased do concentrations in the deepest layers were simulated during short term stratification events this did not markedly impact the tp sediment release as the major part of the tp release was controlled by organic p pool mineralisation from the sediment the model included fe and al as a fixed fraction of the inorganic matter in the sediment set as parameter values to simulate p adsorption to fe and al dependent upon the anoxic to oxic fraction of the sediment for example in the model with highest r performance for tp in ensemble 1 the summer mean tp sediment release varied between 3 5 and 5 5 mg p m 2 day 1 with the majority of the p being released as pdom and the highest release occurring in august in all years data not shown the phytoplankton p uptake removed most of the available po4 summer mean uptake between 5 7 and 7 9 p m 2 day 1 compared to other fluxes while pelagic pdom mineralisation predominantly contributed to the po4 pool for nitrogen the ensembles encapsulated between 50 and 86 of the tn concentrations when accounting for observation errors table s2 with a general overestimation of tn ensemble median pbias between 2 and 75 especially in the years with high macrophyte coverage fig 3d all ensembles reproduced the seasonal and inter annual dynamics of no3 ensemble median r2 between 0 15 and 0 50 but most ensembles missed the timing with 1 3 months and overestimated the concentrations fig 5a the magnitude of nh4 concentrations was generally captured in spring and early summer which is also reflected in the summer mean concentrations across years in different ecological states but the variations observed in late summer concentrations of up to 25 90 fold within two months were not simulated by any of the ensembles fig 5b 3 3 primary producers and higher trophic levels none of the ensembles reproduced the timing and magnitude of the observed shifts in ecological state from no to high macrophyte coverage following the fish removal fig 6 a ensemble 1 and 3 simulated most accurately the mixed distribution of different ecological states fig 6a and ensemble 3 was the only ensemble with 39 out of 138 models simulating shifts between no and high macrophyte coverage in the model period the additional ensembles either simulated no ensemble 4 5 and 6 or high coverage ensemble 2 for the majority of models 82 and 92 99 respectively throughout the model period also the observed shift between submerged macrophytes and phytoplankton was not captured by any single ensemble as none of the ensembles reproduced the observed magnitudes of summer chl a for all periods with different macrophyte coverage fig 3e only ensemble 3 reproduced the observed chl a concentration ranges in model years with high coverage but also underestimated summer chl a concentrations at no to sparse macrophyte coverage the opposite was true for all other ensembles in all ensembles in the growth period may to september the macrophyte growth rate was rarely nutrient limited all ensemble medians 5 reduction in growth rate except in 1996 with ensemble 3 max 12 reduction instead macrophytes were light limited with reduced growth rates that range between no growth to 100 growth increasing with depth and varying between ensembles and time there were clear correlations between mean chl a concentrations in surface waters and macrophyte light limitation results not shown the observed phytoplankton community varied markedly with several different genera being dominant during the modelling period fig s7 the ensembles overall simulated similar phytoplankton dynamics in the years with spring diatom blooms followed by late summer cyanobacteria blooms encapsulating between 40 and 70 of the measured chl a concentrations table s2 during periods with low water column mixing in late summer cyanobacteria was modelled to actively seek surface waters simulating surface blooms fig 4c in summers with observed late cyanobacteria blooms some models in all ensembles except ensemble 3 reproduced the levels of total chl a concentrations with maximum concentrations between 400 and 800 Œºg chl a l albeit these dynamics were also reproduced in years where cyanobacteria blooms were not recorded in the bi weekly to monthly sampling campaign fig 3e the observed dynamics of the zooplankton groups daphnia and other zooplankton were overall well reproduced by several ensembles ensemble median r 0 03 to 0 36 with 67 88 and 50 70 of the observations captured respectively fig 6b and c in winter both simulated zooplankton groups generally died out although the observations showed that zooplankton species other than daphnia were present fig 6b to simulate biomanipulation and by catches of bream and roach the ensemble models were forced with a fish removal of 5 60 dependent upon year and fish group the mean of total fish wet weight biomasses removed per year for all ensembles varied between 0 3 and 22 2 mg with clear inter annual differences fig s7 the total amount of simulated fish removed from the lake ranged between 31 and 100 mg ww fish mean 56 mg ww fish in the years with biomanipulation the simulated fish removal significantly decreased the benthivorous fish biomass at the time of removal fig 6d whereas low removal of planktivorous fish exhibited no clear impact fig s8 to compare the effects of fish removal included in the simulations we executed all models in ensemble 1 3 and 6 without fish manipulation fish removal negatively impacted the concentrations of benthivorous fish for 4 5 months following the removal in mid april with changes in the mean monthly biomass from 146 to 111 in may to 21 to 26 in august compared to simulations without manipulation in the winter months in manipulation years and the subsequent years the differences in fish concentrations with and without manipulation varied between 10 no difference in ensemble median water quality concentrations was observed with or without fish removal in the manipulation years and the simulated fish removal did not clearly impact the simulation of chl a concentrations or the biomass of piscivorous fish figs 3e and 6e 3 4 nutrient load scenarios all ensembles responded to the 75 reduction or increase in nutrient load with changes in water quality and macrophyte coverage however with clear differences between the ensembles fig 7 for five out of the six ensembles the proportion of model years in the last four years of the model period with an ecological state with abundant macrophytes decreased with increased nutrient loading fig 7a the largest change in the proportion of ecological states occurred between the baseline and the 75 reduction scenario with 0 58 point increase the ensemble distributions of summer maximum macrophyte coverage also decreased along the scenario nutrient load gradient fig 7b responses in water quality also occurred as the ensembles ranges of summer mean tp tn and chl a overall shifted to higher concentrations with increased nutrient loading and all or five out of six ensemble median values increased for tp and tn or chl a and macrophyte coverage respectively fig 7b 4 discussion coupling the 1d hydrodynamic model gotm to the lake ecosystem model wet allowed for simulating physical and biogeochemical depth gradients in the water column and sediment the lake arreskov multiple model ensembles with the 1d lake ecosystem model gotm wet captured 8 80 of the observed water quality and 33 91 of the food web variables table s2 however the timing of shifts between the ecological states were not well reflected figs 3 5 and 6 still several model developments enabled a more accurate representation and simulation of the lake ecosystem compared to a previous modelling study using the 0d pclake model nielsen et al 2014 besides including a vertically resolved water column our study encompassed two simulated zooplankton groups and simulated fish harvest an expanded water quality dataset inorganic nutrients and new developments in auto calibration i e application of parsac and ensemble modelling the focus of this study was to test a 1d state of the art lake ecosystem model against a previously conducted case study with model difficulties and therefore each model development was not tested and compared directly to previous efforts nielsen et al 2014 we conducted an in depth analysis of model behaviour based on several ensemble simulations and placed our results in the context of simulating restored shallow lakes and compared them with the previous model study when relevant 4 1 improved water quality simulation by including the 1d hydrodynamic water column model gotm in the simulation water temperature across the depth was directly simulated with feedback from the biogeochemical processes in wet fig 3a which contrasts the 0d pclake study where water temperature was forced nielsen et al 2014 the observed water temperature dynamics were excellently reproduced with high agreement between ensembles fig 2 and performance was comparable to the best 10 of previous aquatic biogeochemical models arhonditsis and brett 2004 moreover although some ensembles simulated large ensembles spans the general seasonal do dynamics and magnitude were well reproduced and the performance was comparable to the best 10 20 of previous aquatic biogeochemical models with regard to relative error arhonditsis and brett 2004 the fluctuating summer do concentrations were not always captured likely due to inability of the ensembles to reproduce the timing of phytoplankton blooms and resuspension events several short term stratification events with clear vertical gradients in water temperature and dissolved oxygen were simulated by gotm wet e g fig 4 but were not always reflected in the water quality sampling of lake arreskov due to the bi weekly measurements explicit simulation of a vertical water column proved beneficial for modelling the phytoplankton dynamics in lake arreskov as it allowed simulated phytoplankton groups to be configured to float or swim upwards for increased light and or downwards for increased nutrients depending on the phytoplankton species being modelled in the newly developed wet model schnedler meyer et al 2022 in all the model ensembles cyanobacteria was modelled to move upwards for increased light in order to simulate the increased concentrations of the two cyanobacteria species aphanizonemenon sp and gloeotrichia echinulate at the surface during short term stratification events observed in most years in late summer fig s4 the ensemble models were capable of simulating clear differences in chl a concentrations between surface and deeper water layers fig 4c and reproduced the observed maximum levels of chl a concentrations during blooms fig 3e in comparison the previous model simulated maximum chl a concentrations of approx 200 Œºg l in most years and was therefore not able to reproduce the observed chl a peaks nielsen et al 2014 consequently the combination of simulating stratification events in a vertical water column model and phytoplankton swimming abilities proved central to simulate the dynamics of buoyant cyanobacteria during stratification in lake arreskov such combination will also be important when simulating water bodies with the potential to be or being impacted by harmful cyanobacterial blooms deteriorating the water quality huisman et al 2018 as many cyanobacterial species for instance microcystis sp wilkinson et al 2020 and planktothrix rubescens walsby et al 2006 can migrate vertically through the water column in periods with stagnant waters the simulated nutrient dynamics were also impacted by the inclusion of a vertical water column model as each water layer was connected to a sediment layer in gotm wet the previous 0d model study of lake arreskov did not report on the performance for inorganic nutrients no3 nh4 po4 and we therefore compared the model ensemble performance for inorganic nutrients to the total nutrient concentrations when reporting model performance it should be mentioned also that the previous study disregarded tp and tn concentrations measured in water shallower than 0 6 m although the concentrations here were often peak values nielsen et al 2014 despite this the ensemble simulations often reproduced the previous model performance ensemble medians max vs the nielsen et al 2014 r2 values tn 0 0 0 03 0 13 vs 0 03 po4 0 0 0 06 0 22 vs 0 06 or improved it no3 0 15 0 5 0 6 vs 0 03 depending on the specific model in the ensemble and performance metrics although nh4 simulations were likely not improved negative median r values for all ensembles overall for example the simulated summer nutrient retention in lake arreskov of 0 26 mg p m 2 day 1 and 12 mg n m 2 day 1 in 1994 for models in ensemble 1 was within the previously calculated ranges 0 24 to 2 2 mg p m 2 day 1 and 15 to 29 mg n m 2 day 1 based on the measured in and outflow discharge and nutrient concentrations fyns amt 1996 the simulated phosphate release from the sediment was also within the range of the previously measured tp sediment fluxes from lake arreskov and other meso trophic shallow lakes jensen and andersen 1992 before the model period organic p constituted a major fraction of the sediment p pool in the lake and the phosphate release likely originated from organic matter degradation jensen and andersen 1992 which was reproduced by the model yet most ensembles did not capture the late summer po4 concentration peaks likely due to underestimation of the redox dependent p release and overestimation of the cyanobacteria po4 uptake simulating p adsorption to fe and al is dependent upon the amounts of fe and al that are set by parameter values as a fixed fraction of the inorganic matter the external fe and al input is therefore not taken into account by the model although the fe input varies across the simulated period also in the wet model phytoplankton groups can only take up nutrients in the pelagic and it therefore did not fully simulate the behaviour of the observed dominant cyanobacteria species fig s4 that may take up p during a benthic growth period and later transfer it to the water by buoyancy cottingham et al 2020 reynolds 2006 as observed in lake arreskov from 1989 to 1991 aavad 1994 4 2 simulating shifts between ecological states the gotm wet model reproduced the expected difference between turbid phytoplankton dominated and clear water macrophyte dominated states across a nutrient load gradient as reflected by the nutrient load scenario results fig 7 and for some models in ensemble 3 however most model parameterisations were unable to simulate the fluctuating ecosystem states in the model period the model structure and or the model parameterisations were likely not adequately sensitive to the changes in forcing weather discharge and external nutrient load and the plankti benthivorous fish removal to reproduce the observed shifts below we discuss likely explanations of the model shortcomings related to the modelling of macrophytes zooplankton and fish the availability of light is central in the competition between phytoplankton and submerged macrophytes sand jensen and borum 1991 scheffer et al 1993 between the turbid and clear water states an intermediate state has been found to occur which is often characterised by a few macrophyte species that can complete their life cycle during the clear water phase in spring and early summer while later in summer cyanobacteria blooms often occur sayer et al 2010 this was observed in lake arreskov in the 2000s therefore the spring clear water phase most likely provides a window of opportunity for submerged macrophyte establishment its length timing and strength being crucial for the macrophyte development which may depend on among others weather and planktivorous fish hilt et al 2018 phillips et al 2016 the model was able to simulate the feedback between phytoplankton and macrophytes to light for instance the shading of macrophytes by phytoplankton as demonstrated by the macrophyte growth rate reductions due to light limitation recorded by all ensembles across the growth season however variations in the clear water phase were not adequate to reproduce the timing of the macrophyte establishment by the ensembles competition between periphyton and macrophytes also impacts the light availability for macrophytes and is considered an important mechanism in the shift between turbid and clear water states hilt et al 2013 sayer et al 2010 with the inclusion of periphyton through an empirical relationship with nutrients in a modified version of 0d pclake the modelling results showed a weakening of macrophyte growth conditions at higher nutrient loads due to periphyton shading hilt et al 2018 inclusion of periphyton would provide an additional mechanism weakening the established growth conditions for macrophytes thereby potentially improving the model s ability to simulate macrophyte loss 4 3 two zooplankton groups two zooplankton groups daphnia sp and other meso zooplankton were included in the lake arreskov food web fig 6b and c which reproduced or improved the zooplankton performance depending upon the specific ensemble ensemble medians max vs nielsen et al 2014 r2 values daphnia 0 02 0 1 0 28 vs 0 05 other zoopl 0 0 0 13 0 2 vs 0 05 large bodied zooplankton such as daphnia sp are more vulnerable to fish predation burks et al 2002 so following removal of planktivorous fish a predation release on large bodied zooplankton should occur increasing the grazing pressure on phytoplankton and thereby increasing water clarity jeppesen et al 2012 the mean size of cladocerans did increase during or after the biomanipulation in lake arreskov but large blooms of cyanobacteria still occurred in late summer hansen and hansen 2007 with the application of several zooplankton groups now common in water quality and lake ecosystem modelling e g bruce et al 2006 kong et al 2017 andersen et al 2020 this shift from small to large bodied zooplankton after for instance fish removal can be included in the model and thereby refine the description of the pelagic trophic cascade after biomanipulation depending on the choice of zooplankton groups jeppesen et al 2011 4 4 fish and biomanipulation both the lower external nutrient load in 1996 1997 and the fish removal between 1990 and 1997 are likely explanations of the reestablishment of submerged macrophytes and the shift in ecological states in lake arreskov hansen and hansen 2007 nielsen et al 2014 therefore both drivers were included in this study the total number of simulated fish removed from the lake varied between 31 and 100 mg ww ensemble median between 49 and 62 mg ww fish which was comparable to the 52 57 mg ww fish removed during the six years of reported bycatch and biomanipulation table 1 the fish removal was implemented as percentage removal rates so the ensembles overall reproduced the magnitude of the fish concentrations and harvest however after the fish removal no long term reduction in benthivorous fish or an increase in piscivorous fish was simulated as otherwise observed in lake arreskov hansen and hansen 2007 and expected from other biomanipulation studies jeppesen et al 2012 older and larger benthivorous fish are usually targeted in biomanipulations as these have a proportionally higher importance for whether the lake stays in a turbid phytoplankton dominated state by for instance increasing sediment resuspension negative impacts on macrophyte shoots and the fact that they are too large to be controlled by piscivorous fish e g jeppesen et al 2012 our wet model was configured with three fish groups connected juvenile zooplankton fish and adult benthivorous fish and a piscivorous fish group which did not allow for inclusion of size or more advanced life history traits however the wet food web configuration in wet is highly flexible schnedler meyer et al 2022 permitting investigation of different fish group configurations for instance with several groups of adult benthivorous fish to mimic different ecosystem impacts with life history changes besides wet 0d pclake is one of the few lake ecosystem models simulating fish and an effect of macrophytes on fish foraging efficiency janse 1997 janse et al 2008 benthic macroinvertebrate densities are higher in clear water lakes with high coverage of submerged macrophytes than in turbid lakes boll et al 2012 as many macroinvertebrates live on and of associated epiphyton and constitute an important food source for for instance juvenile perch vadeboncoeur et al 2002 however vegetation associated macroinvertebrates were not included in the lake arreskov model or in various versions of pclake either which might explain the model s underestimation of piscivorous fish concentrations as an important food source under clear water conditions could be missing 4 5 model performance in the context of the current state of the art overall the performance of the ensemble modelling of lake arreskov was comparable to that of previous model studies on water quality variables arhonditsis and brett 2004 many of the former model studies have covered shorter time scales 1 6 years e g janse and aldenberg 1990 elliott et al 2005 and systems that have not undergone restoration which somewhat eases the ability to capture ecosystem dynamics some model studies have simulated general effects of biomanipulation on lake food webs h√•kanson et al 2003 or ecological states with hypothetical lakes janssen et al 2019b and only few case studies have attempted to reproduce lake ecosystem dynamics before and after in lake restoration reflecting the need for comprehensive datasets for use in modelling however such datasets are often lacking for lake restoration projects jeppesen et al 2005 for example 0d pclake reproduced to a certain extent the observed short term effects of lake restoration in lake zwemlust after removal and restocking of fish and zooplankton transplantation of macrophytes and refilling the lake janse et al 1995 a modified version of 0d pclake was applied to a subtropical large shallow lake in china lake chaohu kong et al 2017 and reproduced the observed patterns of decadal impacts of hydrological regulation although only performance metrics after the intervention were available for fish with the gotm wet model applied in this paper several advantages appear that limit the difficulties of simulating the impacts of restoration in shallow lakes this encourages more case studies that will further test the model s ability to simulate restoration attempts our approach with multiple single model ensembles with different model performance criteria expanded the incorporated model uncertainty to include both model parameter and performance metric uncertainty we used this approach as our focus was on the ability of the individual model ensemble to investigate in depth the behaviour of the model and the variations in simulation behaviour due to variations in model parameters however model uncertainty also stems from model structure initial and boundary conditions and inputs khatami et al 2019 and the simulated ecosystem dynamics are dependent upon the food web configuration for example the configuration of the fish groups could be important for simulating the impacts of fish removal as discussed above so a natural next step would be to incorporate and investigate the structural uncertainty of the model thereby moving from single model to multi model ensembles when modelling shallow lakes which is facilitated by the modularisation of organism groups in wet 4 6 models in the context of management before evaluating a potential restoration measure through simulations any lake model study should incorporate the most important processes and dynamics in the lake ecosystem short term stratification can be an important descriptor as illustrated here with lake arreskov which highlights the benefits of coupling a fully dynamic water column model to a lake ecosystem model when simulating shallow lakes with the availability of worldwide high frequency meteorological data e g the ecmwf era5 dataset previous challenges arising from data limitation for running hydrodynamic models are now overcome accordingly simulations of mixing and stratification can rely on physical descriptions lake morphology and local weather as well as feedback from biogeochemical processes instead of empirical relations user data on stratification depth and timing janssen et al 2019a or an assumption of non stratification with the inclusion of ice cover in several water column models e g gotm mylake glm dyresm simulating vertical mixing and thermal regimes should now be possible for most shallow lakes worldwide for a successful lake restoration planning should be lake specific abell et al 2020 s√∏ndergaard et al 2007 as it requires an understanding of a range of limnological characteristics such as lake type nutrient sources loading history and food web structure hamilton et al 2016 process based lake models incorporate relevant limnological characteristics and can be made lake specific through configuration and model calibration thus making lake models a powerful planning tool e g √∂zkundakci et al 2011 andersen et al 2020 that can also provide predictions of impacts of climate warming e g trolle et al 2020 or fish removal on the ecological state in this study we applied wet a state of the art lake ecosystem model coupled to gotm a fully dynamic water column model and illustrated the improvements in simulating stratification and water quality extremes e g cyanobacteria surface blooms compared to previous model efforts with a traditional box model approach inclusion of an additional zooplankton group in the modelled food web configuration for lake arreskov also improved the ability of the model ensemble to reproduce the observed zooplankton dynamics however further improvements of the state of the art lake ecosystem models such as wet are needed to more accurately simulate shifts in ecological state produced by internal restoration measures as the timing of these shifts could not be reproduced despite adding more details to the model and transition to a 1d version software availability name of software water ecosystems tool wet developers nicolas aza√±a schnedler meyer tobias kuhlmann andersen karsten bolding fenjuan rose schmidt hu anders nielsen dennis trolle contact address department of ecoscience aarhus university vejls√∏vej 25 8600 silkeborg denmark email wet info wet au dk availability https gitlab com wet wet releases v0 1 0 license freely available under gnu general public license gpl version 2 the model configuration required to run the gotm wet model for lake arreskov is included in the wet repository as a test case name of software general ocean turbulence model gotm developers lars umlauf hans burchard and karsten bolding availability https gitlab com wet gotm releases v5 2 2 au documentation www gotm net license freely available under gnu general public license gpl version 2 name of software framework for aquatic biogeochemical models fabm developers jorn bruggeman and karsten bolding availability www gitlab com wet fabm and https github com fabm model fabm documentation bruggeman and bolding 2014 license freely available under gnu general public license gpl version 2 name of software parallel sensitivity analysis and calibration parsac developers jorn bruggeman and karsten bolding availability www doi org 10 5281 zenodo 4276111 and http www github com boldingbruggeman parsac license freely available under gnu general public license gpl version 2 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported partly by a research project on process based lake ecosystem models supported by the danish environmental protection agency ph d funding by the sino danish centre for education and research and the poul due jensen foundation e j was supported by the t√ºbitak bideb2232 program 118c250 a previous manuscript version of this article was included in the ph d dissertation by tobias kuhlmann andersen and we thank ian jones christian skov and peter borgen s√∏rensen for valuable comments and an insightful discussion we also highly appreciate the thoughtful comments and suggestions made by two anonymous reviewers finally we thank anne mette poulsen and tinna christensen for valuable editorial and layout assistance and the python community for asking and answering pressing questions in our efforts to process and visualise our model results appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2022 105501 
25544,live fuel moisture content lfmc is an important environmental indicator used to measure vegetation conditions and monitor for high fire risk conditions however lfmc is challenging to measure on a wide scale thus reliable models for estimating lfmc are needed therefore this paper proposes a new deep learning architecture for lfmc estimation the architecture comprises an ensemble of temporal convolutional neural networks that learn from year long time series of meteorological and reflectance data and a few auxiliary inputs including the climate zone lfmc estimation models are designed for two training and evaluation scenarios one for sites where historical lfmc measurements are available within site the other for sites without historical lfmc measurements out of site the models were trained and evaluated using a large database of lfmc samples measured in the field from 2001 to 2017 and achieved an rmse of 20 87 for the within site scenario and 25 36 for the out of site scenario keywords live fuel moisture content modis convolutional neural network time series analysis fire risk deep learning ensembles data availability software the software developed for this study is available from https github com lynn miller lfmc estimation tree multi modal lfmc it is written in python version 3 8 using the anaconda 3 distribution version 2020 11 and can be run on both 64 bit windows and linux a cuda enabled gpu is recommended the main non standard python packages used are gdal version 2 3 3 tensorflow version 2 3 0 cuda cuda toolkit version 10 1 243 with cudnn version 7 6 5 and the google earth engine python api earthengine api version 0 1 283 a full list of packages and versions is provided in the github repository all maps were created using qgis desktop version 3 20 3 datasets all datasets used in this study are listed in table s1 table s1 summary of datasets used in the study table s1 product description source licence reference globe lfmc global database of lfmc field measurements https doi org 10 6084 m9 figshare c 4526810 v2 cc0 yebra et al 2019 beck kg v1 present 0p0083 present day 1980 2016 k√∂ppen geiger climate classification https figshare com articles dataset present and future k ppen geiger climate classification maps at 1 km resolution 6396959 2 cc by 4 0 beck et al 2018 nasa mcd43a4 006 modis daily reflectance data google earth engine gee product modis 006 mcd43a4 open access schaaf and wang 2015 nasa mod10a1 006 modis daily snow cover data gee product modis 006 mod10a1 open access hall and riggs 2016 nasa mod44w 006 modis water mask gee product modis 006 mod44w open access carroll et al 2017 prism an81d prism gridded daily climate dataset gee product oregonstate prism an81d free for non commercial use prism climate group 2004 nasa srtm digital elevation nasa srtm digital elevation 30 m gee product usgs srtmgl1 003 open access nasa jpl 2013 1 introduction historically wildfires form part of the ecological cycle kilgore 1973 sharples et al 2016 however they can cause widespread ecological damage and pose a threat to human life and economic activity gill et al 2013 wang et al 2021 in recent times wildfires have become larger and more frequent and intense a trend that is likely to continue due to anthropogenic climate change abatzoglou and williams 2016 abram et al 2021 jolly et al 2015 effective methods of preparing for and managing wildfire risk are therefore critical to mitigating the associated socio economic risks an important indicator of how a wildfire will burn and spread is the amount of water contained in living vegetation wet vegetation burns less easily as more energy is needed to evaporate the water xanthopoulos and wakimoto 1993 therefore acting as a heat sink dimitrakopoulos and papaioannou 2001 a common measure of this amount of water is the ratio between the weight of the water in the vegetation and its dry weight or live fuel moisture content lfmc dasgupta et al 2007 yebra et al 2013 however measuring lfmc is both time consuming and costly as vegetation samples must be collected from field sites and then weighed and dried under controlled conditions while this method is effective for monitoring small sites it is infeasible for monitoring large areas at risk from wildfires yebra et al 2013 consequently other methods of estimating lfmc are required an approach used by several studies is to estimate lfmc from remote sensing data collected by optical sensors on board low earth orbiting satellites marino et al 2020 yebra et al 2018 optical data can help to monitor lfmc due to the strong water absorption features in the near infrared and shortwave infrared frequencies danson and bowyer 2004 yebra et al 2013 a widely used source of optical data is the moderate resolution imaging spectrometer modis on board the nasa terra and aqua satellites nasa n d these satellites have been operational for over 20 years providing satellite image time series at high daily temporal resolution that help with the understanding of both seasonal and long term changes to the environment ban 2016 methods of modelling lfmc from remote sensing data fall into two broad categories physical modelling based on the inversion of radiative transfer models rtms and empirical models based on statistical or machine learning techniques quan et al 2021 yebra et al 2013 physical modelling utilises the known physical relationships between vegetation conditions and optical reflectance quan et al 2017 while these relationships are site independent and physical models potentially generalise well yebra et al 2008 they are dependent on accurate assessment of the biophysical parameters required to calibrate and parameterise the models quan et al 2016 physical models also suffer from the ill posed inversion problem yebra and chuvieco 2009 where different vegetation conditions result in similar reflectance levels due to the mixing from different properties quan et al 2021 which means that inferring vegetation conditions solely from reflectance levels can lead to a non unique solution hence prior knowledge is necessary to resolve this problem quan et al 2016 zhu et al 2021 as a result only a few studies quan et al 2021 yebra et al 2018 have developed continental and global scale lfmc maps using these physical models many studies have used empirical or statistical models to estimate lfmc from spectral bands or vegetation indices yebra et al 2013 these yield good results when used over local areas and for known land cover caccamo et al 2012 marino et al 2020 peterson et al 2008 however the relationships between biological and physical conditions and remote sensing data are complex and non linear leading to simple models not being able to generalise well quan et al 2017 which limits their usefulness in operational situations while optical remote sensing data captures the current state of the vegetation the drivers of changes to vegetation state are the climate and meteorological conditions however the relationship between meteorological conditions and the vegetation response is complex as time lags between precipitation and increases in vegetation water content vary between species jia et al 2019 pellizzaro et al 2007 vinodkumar et al 2021 cross seasonal dependencies have also been identified for example wet season spring precipitation levels have a direct effect on vegetation dryness during california s wildfire season in late summer and autumn swain 2021 although previous studies have used drought indices to incorporate seasonal meteorological conditions into remote sensing models for lfmc estimation costa saura et al 2021 garc√≠a et al 2008 pellizzaro et al 2007 these methods do not fully utilise time series of both remote sensing and meteorological data simultaneously and so may not adequately capture the relationships to vegetation state furthermore the climate has a major effect on vegetation distribution woodward and williams 1987 influencing both vegetation types and growth patterns beck et al 2018 data identifying locations with similar climates is therefore useful to help identify vegetation related features common to these regions recent research has explored the potential use of deep learning for lfmc estimation from time series of remote sensing data rao et al 2020 zhu et al 2021 deep learning models incorporate multiple levels of learning which can model complex and non linear relationships yuan et al 2020 temporal convolutional neural networks cnns are deep learning models that extract complex temporal features using stacked convolutional layers a convolutional layer consists of a sliding window filter that is applied along a time series identifying locations in the time series where the feature occurs pelletier et al 2019 a pooling layer where adjacent time steps are aggregated using a max pooling or average pooling operation may be applied after each convolutional layer ismail fawaz et al 2019a pooling layers reduce the resolution of the time series and provide a multi scale analysis pelletier et al 2019 zhu et al 2021 trained a temporal cnn hereinafter referred to as modis tempcnn to estimate lfmc from a one year time series of modis data which was used to produce lfmc maps for the contiguous usa conus the explanatory data included a set of auxiliary variables providing spatial temporal and topographical information about each pixel the modis tempcnn model requires no prior knowledge of the vegetation type thus eliminating land cover misclassifications as a potential source of error however modis tempcnn is a simple re implementation of a temporal cnn originally designed for land cover classification and mapping pelletier et al 2019 and was not optimised for numerical lfmc estimation nor did it take into account climate and meteorological conditions which are key drivers of lfmc a further enhancement investigated in this study is to create an ensemble of models then during the inference phase aggregate the individual model estimates to produce the final estimate ensembling machine learning models is a widely accepted method to improve the results of individual models by reducing model variance oza and tumer 2008 and providing a method of estimating model uncertainty gawlikowski et al 2021 ensembling has been proven to be useful in a wide range of remote sensing applications bigdeli et al 2021 huang and zhang 2013 kussul et al 2017 pal 2005 2008 rodriguez galiano et al 2012 while not designed specifically for remote sensing tasks inceptiontime ismail fawaz et al 2020 showed cnn ensembles for time series models have the potential to be both accurate and scalable in summary this study proposes a new deep learning architecture for large scale daily lfmc estimation hereinafter referred to as multi tempcnn multi tempcnn is an extension of modis tempcnn zhu et al 2021 that improves the accuracy and stability of the lfmc estimates by 1 including a year long time series of meteorological data and remotely sensed optical data 2 including the k√∂ppen geiger climate classification 3 using an ensemble of multi tempcnn models 4 providing a measure of uncertainty for the lfmc estimate and 5 tailoring the architecture for lfmc estimation and computational efficiency 2 methods 2 1 data sources 2 1 1 lfmc sample dataset the globe lfmc database yebra et al 2019 is a large historic archive of destructively sampled lfmc measurements collected between 1977 and 2018 each sample includes the sampling date location and land cover type while this database contains data from 1383 sites located in eleven countries 69 of the data collection sites and 84 of the samples are from the contiguous united states conus yebra et al 2013 table 4 accordingly this study is focused on the conus 2 1 2 optical remote sensing data many satellite missions provide optical reflectance data and selecting an appropriate source is often a trade off between the characteristics of each source the sources of the most commonly used reflectance data are modis the multispectral imagers on board the landsat satellites u s geological survey n d and the multispectral instrument msi on board the sentinel 2 satellites european space agency 2019 this study in line with zhu et al 2021 used modis data due to its high temporal resolution daily and historical data dating back to late february 2000 despite the comparatively low spatial resolution of 500 m and the approaching end of life of the terra and aqua satellites although both landsat and sentinel 2 provide higher resolution data thus potentially allowing lfmc to be estimated at higher resolution and have planned mission continuity they have drawbacks that make them unsuited to this study before the commissioning of landsat 9 in january 2022 landsat had low temporal resolution 16 days which after the removal of cloud covered images would yield a sparse time series sentinel 2 data is only fully available since the commissioning of the sentinel 2b satellite in 2017 providing little overlap with the date range in the globe lfmc database the modis data used was from the combined modis terra and aqua product mcd43a4 collection 6 mcd43a4 schaaf and wang 2015 this product contains daily reflectance measurements at 500 m resolution across seven spectral bands covering the visible near infra red nir and shortwave infra red swir frequencies it is an analysis ready product with a bidirectional reflectance distribution function brdf fitted using 16 days data from d 8 to d 7 is used to correct reflectance at date d of terra and aqua modis data this removes view angle effects to produce a nadir brdf adjusted reflectance nbar product modis nbar reflectance data is available from late february 2000 and was obtained from google earth engine gee gorelick et al 2017 product modis 006 mcd43a4 2 1 3 meteorological data the meteorological data is the an81d product from the oregon state university prism parameter elevation relationships on independent slopes model collection prism climate group 2004 prism products are widely used sources of spatial climate data for the conus lundquist et al 2015 and are the official spatial climate data sets of the usda us department of agriculture daly et al 2008 studies have found the products have good agreement with measurements from a network of 114 weather stations across the conus buban et al 2020 and perform well in complex terrain walton and hall 2018 data for the prism models is collected from up to 16 000 weather stations and interpolated into a gridded format using dem data and a climate elevation regression algorithm daly et al 2015 the prism an81d product contains daily estimates of total precipitation mean temperature minimum temperature maximum temperature mean dew point temperature minimum vapor pressure deficit and maximum vapor pressure deficit the data has a 4 km resolution making it one of the higher resolution gridded climate datasets available walton and hall 2018 prism data was obtained from gee product oregonstate prism an81d and was automatically resampled and reprojected to the modis resolution and projection on extraction from gee which uses nearest neighbour resampling 2 1 4 climate zone data in the 19th century k√∂ppen 1884 developed a method of classifying areas of the earth by climate to help identify regions of similar vegetation it was later modified by rudolf geiger kottek et al 2006 resulting in the k√∂ppen geiger classification system which is to date still the most widely used climate classification system peel et al 2007 the k√∂ppen geiger system classifies the earth s land surface into 30 climate zones based on precipitation and temperature peel et al 2007 and is organised into a 3 tier hierarchy with the first tier indicating the general climate the second the precipitation and the final tier the temperature recent revisions of the k√∂ppen geiger world map based on updated climate data include peel et al 2007 and beck et al 2018 while the beck dataset is not as widely used as the peel dataset it was selected for this study as it has several characteristics that make it a suitable source at approximately 1 km 0 0083 resolution it is one of the higher resolution k√∂ppen geiger maps available capturing the land surface s variability at a resolution close to the mcd43a4 data furthermore it is based on more recent climate data from more climate stations and multiple independent climate data sources beck et al 2018 which have been corrected to account for topographic effects on temperature and precipitation mcvicar et al 2007 roe 2005 improving classification accuracy in mountainous regions karger et al 2017 the conus sites in the globe lfmc dataset are located across fifteen climate zones table 1 fig 1 however over half the samples are from sites in climate zones bsk and csa while six climate zones bwh cfb dfa dsa dsc and dwb have ten or fewer sampling sites and less than four hundred samples additionally the globe lfmc dataset has no conus samples for seven climate zones the major climate zones with no samples are the three tropical zones af am and aw in southern florida fl and the dwa zone found mainly in nebraska ne and south dakota sd and coloured lilac in fig 1 the other three climate zones with no samples csc cfc and et cover small regions and are not visible at the scale used in fig 1 2 1 5 topographical data topographical data was obtained from gee product usgs srtmgl1 003 which is based on data from the shuttle radar topography mission srtm nasa jpl 2013 this dataset provides elevation data at 30 m resolution slope and aspect the direction in degrees the slope faces so a north facing slope has aspect 0 and an east facing slope has aspect 90 were calculated from the elevation data using the gee terrain products gorelick et al 2017 as recommended by grohmann 2015 the slope and aspect were calculated at 30 m resolution then all three topographical variables were resampled to modis resolution using the weighted means of the input pixels google earth engine 2021 2 2 data preparation this study used the conus samples from the globe lfmc dataset collected on or after march 1 2001 this date is one year after the start of the modis data thus a one year time series of modis data can be obtained for each sample there are 125 049 samples which were collected from 932 unique sites some pre processing of the globe lfmc samples was required as the multi tempcnn models estimate lfmc at the pixel level not point level site and sample locations were adjusted to be the centroid of the mcd43a4 pixel in which they are located samples located in the same mcd43a4 pixel and collected on the same day were merged with the merged lfmc value set to the mean of the merged samples this reduced the number of samples from 125 049 to 67 770 and the number of sites from 932 to 924 a further 824 samples that were collected under snow conditions according to the modis snow cover product mod10a1 hall and riggs 2016 obtained from gee product modis 006 mod10a1 were excluded so the final dataset of lfmc samples contained 66 946 samples from 924 sites subsequent references in this paper to the globe lfmc dataset and or lfmc samples are to this prepared dataset rather than the raw samples analysis of the prepared globe lfmc dataset shows the field samples were collected from mainly forest shrubland and grassland locations table 2 the dataset also contains a few samples from croplands agriculture samples collected from land cover classes that do not fit into any of these four categories mainly urban areas wetlands and water bodies are included in the other category for each sample in globe lfmc a one year 365 days time series of the seven spectral bands in the mcd43a4 data was extracted from gee the time series starts 365 days before the sampling date and ends the day before the sampling date as the tempcnn architecture does not handle missing data any missing mcd43a4 data was filled using linear interpolation in the temporal dimension gap filling was done using the python pandas package finally each band of each sample was normalised using the formula b i n p 2 i p 98 i p 2 i where b i n is the i t h band time series for sample n and p 2 i and p 98 i are the global 2nd and 98th percentiles for band b i pelletier et al 2019 zhu et al 2021 pre processing of the mcd43a4 data resulted in a 365 day 7 spectral band matrix for each sample the prism meteorological data was extracted from gee and normalised similarly six auxiliary variables were prepared for each sample table 3 the latitude and longitude of the mcd43a4 pixel centroid which provided location information the elevation slope and aspect that were extracted from the srtm data and the k√∂ppen climate zone obtained from the 1 km resolution beck et al 2018 climate zone dataset using nearest neighbour sampling this study does not use the day of the year which was included in the modis tempcnn model as shown in the results fig 10 including this variable was found to have a slightly detrimental effect on model accuracy following zhu et al 2021 the slope latitude and elevation were normalised to a 0 1 range a sine and cosine transformation were applied to both the longitude and aspect to obtain two normalised variables for each modelling the cyclic nature of these variables the categorical climate zone data was transformed using one hot encoding table 3 note 2 to obtain a binary variable for each of the fifteen climate zones represented in the samples in total there are twenty two normalised auxiliary variables 2 3 modelling scenarios two modelling scenarios are used in this study in the first scenario referred to as the within site model the model is trained to estimate lfmc at sites where historical field measurements exist the within site model is designed to meet an operational requirement where up to date lfmc estimates are needed at or near sites with historical measurements for example estimating the current lfmc in a fire prone area the second scenario referred to as the out of site model is designed to meet an operational requirement for lfmc estimates for regions for which historical lfmc field measurements are not available which is currently the majority of the earth s land surface in an operational context out of site models would be trained using sample data from all field sites however for evaluation purposes some sites are withheld from the training data and the samples from these withheld sites are used to evaluate the models this process is explained in more detail in the following paragraphs the out of site model was used to produce the lfmc map in fig 19 ideally the model built for each scenario would be trained using all the available data and that model used to produce estimates for unseen dates within site scenario or unseen locations out of site scenario however if all the data were used to train the models there would be no data to test the performance of the models therefore to estimate the performance of a model trained on all the data the sample data is split into several disjoint test sets each with a corresponding training set formed from the remaining samples models are trained on one training set and used to estimate the lfmc for each sample in the corresponding test set fig 2 a set of models using each pair of training and test sets once is referred to a model set in any model set each evaluation sample is in one model s test set therefore combining the test estimates from all the models in the model set yields one estimate for each sample from any test set accuracy statistics section 2 5 1 are calculated using all lfmc estimates from the model set and thus provide an estimate of the performance of the ideal model such as that used to produce the lfmc map in fig 19 the methods used to create the evaluation model sets and the corresponding test and training sets for each scenario are described in the next paragraphs each evaluation model set for the within site scenario was created as follows one model was built for each of the last four years for which the globe lfmc dataset contains data 2014 2017 the evaluation years the models were trained using the samples collected during all years before the evaluation year the trained model was then used to estimate lfmc for all samples collected during the evaluation year fig 2a for example the model for 2014 was trained with data from 2001 to 2013 and used to estimate lfmc for all samples from 2014 while the model for 2017 was trained with data from 2001 to 2016 and used to estimate lfmc for all samples from 2017 most results reported for the within site scenario were evaluated using collated lfmc estimates from all four years full results fig 2c however the annual results are also reported for the overall performance each evaluation model set for the out of site scenario was created using a 10 fold cross validation strategy the sites in the globe lfmc dataset were split into ten groups or folds using stratified random sampling the random sampling stratification was done by land cover class and used to ensure an even distribution of land cover classes across the folds ten models were built each of the ten models was trained using samples collected from sites in nine of the ten folds and then used to estimate lfmc for the remaining samples fig 2b each sample is therefore used for training in nine models and evaluation in one model results reported for this scenario were evaluated using the collated lfmc estimates for the evaluation samples from all ten folds fig 2d 2 4 multi tempcnn architecture 2 4 1 convolutional neural networks deep learning architectures or neural networks consist of a network of simple computational units or neurons arranged in layers lecun et al 2015 the basic architecture consists of fully connected fc layers where the inputs to each unit are the outputs from all the units in the previous layer each unit applies a linear transformation to the inputs then a non linear transformation or activation function to produce an output the final or output layer is either composed of multiple units providing either class membership probabilities for classification tasks or a single unit to estimate the required quantity for regression tasks training the network consists of learning the weights of the linear transformations that best fit the model to a training data set for a regression problem this is usually the mean squared error defined as i 1 n y ÀÜ i y i 2 n where y ÀÜ i and y i are respectively the estimated and measured values for the i t h training sample and n is the number of training samples convolutional layers lecun et al 1989 are useful when the inputs have dimensionality and so are suited to both spatial 2 dimensional and temporal 1 dimensional data here the neuron is replaced with a fixed width convolutional filter this filter is slid across the input and applied at each position to produce a linear combination of the surrounding points for example if a filter of size five is used with temporal data as in the current study the output is another time series where each point is the result of convolving the corresponding input point with the preceding two and succeeding two points a layer can have multiple convolutional filters resulting in a multivariate output convolutional neural networks cnns often contain both convolutional and fully connected layers 2 4 2 deep learning ensembles deep learning networks are commonly initialised by randomly assigning values to weights the model is then trained by iteratively updating the weights to improve the fit to the training data until a stopping condition is reached if multiple models that have been randomly assigned different initial weights are trained they will often find different solutions which may well be similar in overall accuracy choromanska et al 2015 but during inference show variation in the estimated target values for a single instance neal et al 2018 this study exploited this inherent randomness by training multiple models each with a different random weight initialisation and ensembled them by averaging the estimations at inference time ismail fawaz et al 2019b furthermore ensembled models allow estimating uncertainty for the lfmc map by using the standard deviation of the individual ensemble member predictions several factors were considered when setting the ensemble size 1 the training and inference time which increase with the ensemble size 2 the increase in accuracy as the ensemble size increases and 3 the increase in confidence of the uncertainty as the ensemble size increases while there are only small improvements in accuracy with an ensemble size larger than ten section 3 2 an ensemble size of twenty was chosen as being the minimum necessary to ensure a reasonable degree of confidence in the uncertainty 2 4 3 proposed architecture the proposed multi tempcnn deep learning architecture is based on the tempcnn architecture pelletier et al 2019 developed for land cover classification from optical data tempcnn consists of three temporal convolutional layers with average pooling to extract temporal features these features are passed through a fully connected layer then the final classification layer the activation function used for all layers is the rectified linear unit relu krizhevsky et al 2012 nair and hinton 2010 models are trained using the adam optimiser kingma and ba 2014 with default parameters Œ≤ 1 0 9 Œ≤ 2 0 999 and —î 10 8 and a learning rate of 0 01 deep learning models have a large number of parameters to train which can greatly exceed the number of training samples this leads to models that can overfit the training data to control for overfitting batch normalisation ioffe and szegedy 2015 and a small l2 weight regularisation of 10 6 are applied to each layer of both the within site and out of site models additionally dropout srivastava et al 2014 is used in the within site models the lower model complexity of the out of site models and ensembling also help to control overfitting modis tempcnn is a modification of tempcnn to the lfmc estimation context zhu et al 2021 the primary modifications were to include non temporal or auxiliary variables as additional inputs to the fully connected layer add a second fully connected layer and replace the final classification layer with a linear regression unit here modis tempcnn was further refined motivated by the need to expand the architecture to include time series of meteorological data and auxiliary climate zone data as inputs the most significant change was to add another set of convolutional layers to extract temporal features from the meteorological data furthermore to understand whether other architecture changes would benefit the new models a range of model architectures was explored table 4 as a result of this exploration changes were made to the number of layers and filters used in the convolutional layers the pooling size of each convolutional layer the number of layers and units used in the fully connected fc layers and the model wide hyper parameters of the drop out rate batch size and number of times to iterate through the training data epochs during this phase of the study care was taken to avoid overfitting the architecture to the evaluation data by using samples from only half the sites and collected before 2017 fig 3 after the architectures for each scenario were designed they were evaluated using the entire dataset with the data split into training and evaluation sets as described in section 2 3 consequently both the training and evaluation sets used for model evaluation contained a mix of samples used and not used during architecture design note that while all results presented are from models trained and evaluated using the full datasets both the model architectures and inputs were decided based on the architecture design results and locked in place before the experiments producing these results were performed to verify that no overfitting occurred model evaluation table 5 included a break down of performance by the samples used during architecture design architecture design samples and those held out from architecture design held out samples the architecture exploration showed that the two scenarios benefit from different model architectures fig 4 for the out of site scenario a small shallow architecture about one fifth the size of modis tempcnn with fewer convolutional filters and a single fully connected layer and training the model for fewer epochs gave the best results the simpler architecture likely extracted fewer but more generalisable features and increased the variation between models which was exploited by ensembling for the within site scenario a larger deeper architecture about three times the size of modis tempcnn with more convolutional layers and an extra fully connected layer gave superior results the additional complexity may have allowed the extraction of site specific features which were useful when making estimates for known sites but did not help when estimating for unknown sites changes that benefitted both scenarios were reducing the dropout rate and increasing the batch size used during model training 2 5 evaluation methods the multi tempcnn models were developed and implemented in python version 3 8 using tensorflow v2 3 abadi et al 2015 with keras chollet et al 2015 the code to construct train and evaluate the models is available at https github com lynn miller lfmc estimation tree multi modal lfmc two computing environments were used for model training and evaluation the first computer was a dell precision 5530 laptop with six intel core i7 8850h cpu 2 60 ghz processors 32 gb ram and one nvidia quadro p1000 gpu with 4 gb memory all reported timings are from tests run on this first computer as it was a dedicated environment the second environment was a high performance cluster with four nodes each node consisted of 20 intel xeon silver 4214 r cpu 2 40 ghz processors 180 gb ram and four nvidia quadro rtx 6000 gpus 2 5 1 evaluation metrics the results were analysed using the following standard metrics root mean squared error rmse r m s e i 1 n y ÀÜ i y i 2 n where n is the number of evaluation samples and y i and y ÀÜ i are the measured and estimated lfmc for the i t h sample respectively coefficient of determination r2 r 2 1 i 1 n y ÀÜ i y i 2 i 1 n y i y 2 where y is the mean measured lfmc of the evaluation samples r2 measures the performance of a model compared to a baseline model which simply estimates the sample mean values close to zero indicate the model only performs as well as the baseline model while values close to one indicate the model fits the data well when r2 is calculated for a subset of samples y is the mean of the measured lfmc for the full sample set not the mean of the subset by using this value for the mean all r2 calculations are compared to the same baseline thus allowing comparisons between r2 for different subsets of samples the model bias bias or the mean estimation error b i a s i 1 n y ÀÜ i y i n the sign of the bias indicates if the model over estimates b i a s 0 or under estimates b i a s 0 lfmc the absolute value of the bias indicates the mean over or under estimation the metrics were calculated for each model set to produce fifty metrics for each scenario the results presented are the mean of these fifty values and where relevant the standard deviation or 95 confidence interval ci 2 5 2 overall performance evaluation as deep learning models have some inherent randomness multiple instantiations of the models are required to ensure the robustness and generalisability of the results therefore to evaluate the performance of the proposed multi tempcnn within site and out of site models fifty different instantiations of the model sets for each scenario see section 2 3 were created however it was computationally infeasible to do this in a na√Øve way for an ensemble of size twenty this would mean creating 4000 individual within site models 4 evaluation years 20 models per ensemble 50 ensembles and 10 000 individual out of site modes 10 folds 20 models per ensemble 50 ensembles therefore for each evaluation year for the within site models or fold for the out of site models a pool of fifty individual models was created fig 5 ensembles were assembled by randomly selecting without replacement m models from this pool where m is the ensemble size and the ensemble estimations for each sample generated by averaging the estimations made for the sample by the m selected models this process was repeated for each of the evaluation years 2014 2017 for the within site models and each of the ten folds for the out of site models the model sets were then formed using one ensemble for each year or fold for example the first ensemble for each of the four evaluation years formed the first within site model set the second ensemble for each of the evaluation years formed the second within site model set and so on to create fifty model sets the fifty within site model sets are referred to as the within site models in the results and discussion sections similarly the fifty out of site model sets are referred to as the out of site models unless otherwise stated the ensemble size of all models in each model set is twenty to verify that no hyper parameter overfitting occurred model evaluation included a break down of performance by the samples used during architecture design architecture design samples and those held out from architecture design held out samples two sets of comparison models were evaluated the first to show the benefit of tailoring architectures to each scenario and the second to show the improvement of multi tempcnn over modis tempcnn for the first comparison fifty model sets with ensemble size twenty for each scenario were created by using the scenario to train and evaluate the architecture optimised for the other scenario for the second comparison fifty model sets of modis tempcnn models were trained and evaluated for each scenario as modis tempcnn is not an ensemble model each modis tempcnn model set is comprised of individual models in other words the ensemble size is one the computational efficiency of the multi tempcnn models were evaluated in terms of the training times and compared to the training time of the baseline modis tempcnn model while it is trivially easy to train ensemble members in parallel the reported training times assume sequential processing of ensemble members timings are for running the model fitting only and exclude reading and pre processing normalising the data 2 5 3 evaluation of ensembling the impact on the performance when varying the ensemble size was assessed by creating fifty ensembles of each of 5 10 15 20 and 25 model sets the ensemble size in the manner described in section 2 5 2 the mean and 95 ci for the rmse and r2 were calculated for each ensemble size and the individual models changes to bias are not evaluated as bias is not affected by ensembling ueda and nakano 1996 the performance improvement when ensembling is analysed by examining the mean estimation variance for the individual model sets and for the model sets of each ensemble size the mean estimation variance v a r s is defined as follows let e s be the set of model sets with ensemble size s then v a r s i 1 n j 1 m y i j y i 2 m n where y i j is the estimation made for the i th sample by ensemble j e s y i is the mean estimation for the i th sample m e s is the number of model sets and n is the number of evaluation samples the mean estimation variance is a component of the model error friedman 1997 and is expected to drop as the ensemble size increases thus improving the model performance ueda and nakano 1996 2 5 4 input and architecture ablation tests the contributions of the new inputs and the architecture changes to both the within site and out of site multi tempcnn models from those used in the baseline modis tempcnn model were evaluated via ablation tests for each test fifty model sets with an ensemble size of twenty were created following the method described in section 2 5 2 and the mean and 95 cis for the rmse bias and r2 calculated using these ensembles during the input ablation tests models were constructed withholding the meteorological and climate zone data in turn in addition each group of auxiliary variables table 3 was withheld in turn to confirm which auxiliaries are required in the multi tempcnn models the final input ablation test added to the models the day of year auxiliary variable prepared as described by zhu et al 2021 which is used in modis tempcnn but is not used in the multi tempcnn models the purpose of this test is to show that multi tempcnn does not require the day of year the architecture ablation tests reverted each hyper parameter changed for the multi tempcnn models back to the setting used in the modis tempcnn model the architecture changes were assessed by both model training time and model accuracy 2 5 5 evaluation by land cover and lfmc range as discussed by zhu et al 2021 models based on the tempcnn architecture are not land cover type specific nor do they need prior knowledge of the vegetation type because this information is embedded in the reflectance data however to assess how the proposed multi tempcnn models perform across these categories the results are evaluated by land cover type using the classifications shown in table 2 as vegetation types and conditions change at different elevations this includes an analysis of performance by elevation further analysis is done to understand how well the lfmc estimates can be used to identify whether or not vegetation is dry enough to sustain a fire should one ignite this is done by establishing fire danger thresholds for grassland forest and shrubland land cover and identifying the proportion of samples correctly identified as being high or low fire risk below or above the threshold respectively or mis classified as high fire risk or low fire risk while a range of fire danger thresholds have been proposed in various studies chuvieco et al 2004 dennison et al 2008 jurdao et al 2012 pimont et al 2019 this study uses the high fire hazard thresholds established by arga√±araz et al 2018 as indicative thresholds these thresholds are 67 for grasslands 105 for forests and 121 for shrublands when using lfmc estimates for fire risk evaluation good estimates of lfmc in dry conditions are critical as small differences in lfmc result in large changes to perceived fire risk jurdao et al 2012 while a larger error can be tolerated when vegetation is wet zhu et al 2021 therefore performance metrics are provided firstly for all high fire risk samples then by ranges of the ground truth lfmc grouped into 5 intervals from 30 to 250 to assess the potential of the models to be used to accurately identify the higher fire risk situations 2 5 6 spatial evaluation and lfmc maps most samples in the globe lfmc dataset were collected from sites located in the western states of the conus and from climate zones bsk arid cold steppes and csa temperate regions with dry hot summers if models are to be useful for operational applications they need the capability to generalise to locations and climate zones with fewer samples therefore a spatial analysis of the study results to identify differences in performance across locations and climate zones was also carried out firstly performance was evaluated across the fifteen climate zones represented in the globe lfmc dataset then performance was evaluated by site location across the conus and results in two key areas compared with the modis tempcnn baseline results here the lfmc sample sites have been grouped into 0 5 grid cells and the rmse and bias for the samples in each cell were computed for both models finally an lfmc map for the conus was produced for october 1 2017 using the out of site multi tempcnn model the model used to produce the map is an ensemble of twenty individual out of site models which were trained using all the samples in the globe lfmc dataset fig 6 thus is an instance of the model that the cross validation process described in section 2 3 was used to evaluate the map is accompanied by three other maps firstly an uncertainty map that uses the standard deviation of the estimates made by each ensemble member as the measure of uncertainty secondly an lfmc map produced using the modis tempcnn model the last map compares the multi tempcnn lfmc map with the modis tempcnn lfmc map by showing the differences in lfmc estimation for each pixel the map date of october 1 2017 was chosen as it is near the peak of the western conus wildfire season swain 2021 water bodies as identified by the modis mod44w 006 water mask product carroll et al 2017 have been masked 3 results 3 1 overall performance of the multi tempcnn models 3 1 1 within site models the multi tempcnn models proposed for the within site scenario achieved a mean rmse of 20 87 fig 7 a with a standard deviation of 0 03 table 5 across the models for the four estimation years 2014 2017 the mean r2 was 0 70 which means that the models collectively explain over two thirds of the variance in the sample lfmc the mean bias of the models is small at 0 22 so the models under estimate lfmc about as often as they over estimate it considering the results for each year separately table 5 the model for 2014 performed the best with an rmse of 20 07 models for 2016 and 2017 obtained rmses of about 20 64 and 20 37 respectively while the 2015 model performed worse with an rmse of 22 33 r2 ranged from 0 66 2015 to 0 72 2016 some variation in performance between years as shown in 2014 2016 and 2017 is to be expected the outlying 2015 result is likely due to the unusually high precipitation and el ni√±o conditions noaa national centers for environmental information 2016 occurring that year 3 1 2 out of site models the multi tempcnn models proposed for the out of site models achieved a mean rmse of 25 36 fig 7b with a standard deviation of 0 02 table 5 the models have a mean r2 of 0 54 so explain a little over half of the variance in sample lfmc and show little bias with a mean bias of 0 59 3 1 3 assessment of hyper parameter overfitting slightly better results were obtained when the assessment of the models was done with the samples held out from the architecture design compared to the samples used by the architecture design process this is most evident for the within site models where the rmse for the held out samples were 0 89 lower than the rmse for the architecture design samples the r2 value for the held out samples is 0 03 higher than for the architecture design samples the results for the out of site models are closer the rmse and r2 are 0 13 and 0 01 lower respectively for the held out samples than for the architecture design samples these results indicate that hyper parameters have not been overfitted during architecture design 3 1 4 model architecture comparisons for both the within site and out of site scenarios models trained using the architecture designed for the other scenario performed worse than models trained using the architecture designed for the scenario fig 8 the out of site architecture only achieved an rmse of 24 08 and r2 of 0 60 when trained and evaluated using the within site scenario similarly the within site architecture only achieved an rmse of 26 68 and r2 of 0 49 when trained and evaluated using the out of site scenario the within site comparison model showed a little more bias than the proposed models with a difference in absolute bias of 1 00 fig 8b the absolute bias of the out of site comparison model was similar to that of the proposed model but the comparison model tended to under estimate lfmc fig 8e for each evaluation metric differences between the models blue stars and orange circles in fig 8 are much larger than the confidence intervals of the means modis tempcnn models trained using the within site and out of site scenarios performed worse than the multi tempcnn models fig 8 the modis tempcnn architecture only achieved an rmse of 24 40 and r2 of 0 58 for the within site scenario and 27 62 and r2 of 0 45 for the out of site scenario both modis tempcnn models were also more biased than the multi tempcnn models with differences in absolute bias of 3 62 within site scenario and 3 24 out of site scenario and both modis tempcnn models tend to under estimate lfmc for each evaluation metric differences between the models blue stars and green hexagons in fig 8 are much larger than the confidence intervals of the means 3 1 5 computational efficiency the individual component models of the multi tempcnn models train much faster than modis tempcnn taking half and one seventh the time for the within site and out of site models respectively table 6 this is mainly due to the increased batch size and reduced number of convolutional filters see also table 7 in section 3 3 2 however training the full ensemble is slower than training the single modis tempcnn model as shown in section 3 2 ensembling contributes significantly to the improved lfmc estimates produced by multi tempcnn so compensating for the extra training time 3 2 the effect of ensembling on model performance a substantial improvement in rmse of about 1 5 from 22 62 for individual models to 21 13 for an ensemble of five for the within site models is gained by using even a small ensemble of five models and smaller improvements continue to be made as the ensemble size increases with an rmse of 20 84 for an ensemble size of 25 fig 9 a likewise r2 improves from 0 64 for individual models to 0 69 for ensembles of five models to 0 70 for ensembles of 15 25 models fig 9b the out of site models follow a similar pattern achieving an overall improvement in rmse of about 1 4 from 26 75 for individual models to 25 34 for models with an ensemble size of 25 r2 for the out of site models ranges from 0 48 for individual models to 0 54 for ensembles of size 20 and 25 confidence intervals for both metrics are too small to be plotted in fig 9 cis for the rmses range from 0 01 for the larger ensembles to 0 08 for individual models and cis for r2 are all less than 0 01 the mean estimation variance of the individual models is 80 9 for the within site models and 76 2 for the out of site models fig 9c the mean estimation variance of the ensembled models is significantly lower at only 2 of the individual model variance with ensemble size twenty five the performance gains due to ensembling are offset by increases in both training and estimation time which increase in proportion to the ensemble size therefore the optimal ensemble size is a trade off between the desired accuracy and computational requirements as discussed in section 2 4 2 the results reported in other sections of this paper use ensembles of twenty models 3 3 input and architecture ablation tests 3 3 1 input ablation tests the two new data sources used in the multi tempcnn models each primarily benefit the models for one scenario fig 10 the meteorological data primarily benefits the out of site models omitting this data increased the rmse by 1 37 and decreased r2 by 0 06 omitting the meteorological data from the within site models is less significant providing only a marginal degradation of 0 11 in rmse and 0 01 in r2 the climate zone primarily benefits the within site models omitting this data increased the rmse by 0 47 and decreased r2 by 0 02 omitting the climate zone data made almost no difference to the out of site models the contribution of the auxiliary variables also changes between the two scenarios fig 10 the topographic data benefits the within site models omitting this data increased the rmse by 0 69 and decreased r2 by 0 02 however the topographic data has a slightly detrimental effect on the out of site models the location variables provide a small benefit to both models the with day of year test shows that adding the day of year auxiliary variable which is used in modis tempcnn leads to slightly worse performance in both scenarios thus confirming the day of year is not required in the multi tempcnn models none of the tests showed any significant bias with the maximum bias being 0 67 for the within site models and 0 74 for the out of site models confidence intervals for the mean rmse ranged from 0 00 to 0 02 and ranged from 0 000 to 0 001 for the mean r2 3 3 2 architecture ablation tests the architecture improvements fall into two categories some reduced the model training time table 7 and others increased model accuracy fig 11 the changes that reduced the model training time were reducing the number of convolutional filters increasing the batch size and for the out of site models reducing the number of epochs while these changes substantially reduced the model training time they had only a minor effect on model accuracy the architectural change that resulted in the largest improvement to the accuracy of the lfmc estimations was to reduce the dropout rate used in model training fig 11 the out of site models did not require any dropout removing the dropout improved the rmse by 0 73 and r2 by 0 03 a low dropout rate of 0 1 was found to be beneficial to the within site models and the reduced dropout rate improved the rmse by 1 46 and r2 by 0 05 over the original dropout rate of 0 5 fewer convolutional filters in both architectures and extra pooling in the within site architecture reduced the number of features extracted by the convolutional layers reducing the need to control overfitting using dropout other changes that improved estimation accuracy were optimising the number of fully connected fc layers in both models and convolutional layers in the within site models the changes improved model rmse by between 0 26 and 0 45 and r2 by between 0 009 and 0 018 optimising the number of fully connected units only yielded a marginal improvement in estimation accuracy the number of fully connected layers appears to have a negative correlation to the model bias fig 11b and e the within site scenario test using two fully connected layers showed higher bias with a bias of 2 43 compared to 0 22 for the proposed model which has three fc layers the out of site scenario test using two fully connected layers has a bias of 0 09 which is less than the bias for the proposed model 0 58 the other test that showed significant bias was the out of site dropout test where adding a dropout of 0 5 caused the model to under estimate rather than over estimate lfmc and doubled the bias from 0 59 to 1 16 3 4 evaluation of multi tempcnn performance by land cover and lfmc range 3 4 1 evaluation of the models performance by land cover as shown in table 2 the predominant land cover classes in the globe lfmc dataset are grassland forest and shrubland the other classes of agriculture and other mainly wetlands water bodies and urban areas are included in the land cover analysis for completeness however as they are less critical for wildfire risk monitoring and have few samples they are excluded from both the land cover by elevation analysis later in this section and the fire risk analysis section 3 4 2 an outcome found repeatedly throughout this section is that a small number of samples for either a land cover class or a combination of land cover and elevation leads to less reliable model performances the within site models estimate lfmc in forest shrubland and other well fig 12 with rmse less than 21 so showing consistent results across the forest and shrubland classes which have the largest number of samples however there is a large difference between the r2 values for forest 0 62 and shrubland 0 77 due to low and high variation respectively in the measured lfmc across both time and locations table 2 the smaller number of agriculture and grassland samples result in poorer model performance on these landcover classes with rmses of 22 54 and 23 57 respectively the models tend to over estimate lfmc for grassland bias is 4 96 and under estimate it for agriculture bias is 3 97 due to these samples having lower and higher than average lfmc respectively fig 12 the out of site models estimate lfmc in grassland well with an rmse of 23 9 fig 12 which is encouraging as grassland lfmc is highly variable and difficult to estimate accurately yebra et al 2013 the results for forest shrubland and other are all close to the mean rmse for the proposed out of site model of 25 36 again showing consistency between the forest and shrubland classes however the r2 value for forest is low at 0 37 due to the low variation in the measured lfmc of forest samples across space and time the small number of samples in some land cover classes again results in poor model performance for example the landcover class agriculture for which there are just 1817 samples has the highest rmse 27 33 across all evaluated land cover classes the models under estimate lfmc for the class other the bias is 6 96 but for the other classes the bias is low 1 40 1 34 indicating the models over estimate lfmc about as often as they under estimate it the land cover classes are unevenly distributed across the elevation groups fig 13 with about 40 of forest samples collected from sites above 2000 m a third of shrubland samples from sites between 1500 m and 2000 m and 50 of grassland samples from sites below 500 m less than 4 of grassland samples are from sites above 2000 m so these results may not be reliable due to limited number of samples the sampling rate across the elevations is due to the distribution of vegetation types at different altitudes both the within site and out of site models performed well for grassland at lower elevations while a drop in the performance at elevations of over 500 m is observed due to the smaller number of grassland samples collected at higher elevations similarly both models perform well on both forest and shrubland up to 1000 m but show a decline in performance above that height an exception is the forest class which performs well again above 2000 m likely due to the substantial number of forest samples collected at high altitude the within site models show significant bias for grassland estimations at all elevations apart from 1500 to 2000 m and generally over estimate lfmc while the out of site models show less grassland bias particularly at lower elevations they significantly under estimate lfmc at altitudes above 2000 m grasslands respond differently to environmental drivers at high altitudes than at lower altitudes brut et al 2009 and the models are not able to adequately capture these differences due to the limited number of samples both models tend to over estimate forest lfmc and under estimate shrubland lfmc significant exceptions are observed in the within site models for forest between 1000 and 1500 m and in the out of site models for shrubland between 500 and 1000 m 3 4 2 evaluation of the potential for the models to detect fire risk the forest evaluation samples for the within site models are split roughly evenly between those above and below the forest fire danger threshold of 105 fig 14 a the models correctly identified high or low fire risk in 82 within site models and 73 out of site models of the forest samples however the within site models failed to detect high fire risk in 12 of these samples and falsely estimated high fire risk in 7 of samples similarly the out of site models failed to detect high fire risk in 19 of these samples fig 14d and falsely estimated high fire risk in 8 of samples the rmses for the high fire risk samples 17 14 for the within site models and 20 99 for the out of site models are well below the respective rmses for the full evaluation set 20 87 and 25 36 however the r2 values 0 64 and 0 40 respectively are also below those for the full evaluation sets indicating that homogeneity of these samples is a contributing factor to the low rmse the models have high positive bias for the high fire risk samples 8 16 and 12 70 respectively thus they tend to over estimate the lfmc of these samples the lfmc for most of the grassland samples is above the grassland fire danger threshold as this threshold is relatively low at 67 the models correctly estimated most of these samples as being low fire risk 82 4 for the within site models figs 14b and 85 7 for the out of site models fig 14e and falsely estimated high fire risk in 0 8 1 0 of samples due to the small number of high fire risk grassland training samples neither set of models performed well on the grassland samples below the fire danger threshold they failed to detect high fire risk in 7 8 within site models and 7 5 out of site models of samples which are 46 and 56 of the high fire risk grassland samples respectively the within site rmse for the grassland samples below the fire danger threshold 24 72 is well above the rmse for the full evaluation set 20 87 and the models have high positive bias for these samples 15 25 however the out of site rmse for these samples is 23 87 substantially lower than the rmse for the full evaluation set 25 36 and r2 is 0 80 well above the r2 for the full evaluation set 0 54 the out of site models show a high positive bias of 16 65 for the grassland high fire risk samples the lfmc for most of the shrubland samples are below the shrubland fire danger threshold 121 the models correctly estimated most of these samples as being high fire risk 69 9 for within site models figs 14c and 65 0 for out of site models fig 14f but failed to detect high fire risk in 3 9 and 7 2 of shrubland samples respectively the models performed worse on the low fire risk samples falsely estimating high fire risk on 6 2 and 7 0 of the shrubland samples about a quarter of the low fire risk shrubland samples respectively the rmses for the high fire risk shrubland samples 14 5 for the within site models and 19 36 for the out of site models are well below the rmses for the full evaluation sets the r2 values 0 75 and 0 55 respectively are close to the r2 values for the full evaluation sets while the models show a positive bias when estimating high fire risk shrubland samples 3 32 and 7 71 respectively this bias is lower than for the forest and grassland high fire risk categories 3 4 3 evaluation of the models performance by lfmc ranges the measured lfmc values in the globe lfmc dataset range from 1 0 to 477 0 table 8 the mean value is 109 1 and the median is 102 8 indicating the dataset has a small right skew the globe lfmc samples used to evaluate the within site models those collected between 2014 and 2017 inclusive has a smaller range from 1 0 to 434 5 and lower mean 108 3 the values estimated by the within site models range from 35 7 to 298 8 while the out of site model estimated a slightly narrower range from 48 4 to 263 6 the median and mean of the lfmc estimates from both models are close to the measured lfmc values neither the within site nor out of site models estimate low lfmc values well due to the small number of globe lfmc samples with lfmc below 50 it should also be noted that the models estimate lfmc at pixel not sample level and samples with extremely low lfmc values are generally dead fuels chuvieco et al 2004 yebra et al 2019 and so are unlikely to be representative of the vegetation across the entire pixel although neither model can accurately estimate the highest lfmc values they are able to distinguish between vegetation conditions that are and are not of concern for fire danger assessment model accuracy varies depending on the range of measured lfmc to be estimated fig 15 a and b as shown by the bias the mean estimation error of the samples in the bin both within site and out of site models tend to over estimate low lfmc values and under estimate high lfmc values switching from over estimating to under estimating at about 115 120 the standard deviation of the estimation error for samples in the bin stays below 20 for the range 40 140 then increases as lfmc increases the rmse at both extremes is high probably due to the small number of samples fig 15c and d but stays below the rmse for the full evaluation set in the critical 50 120 range one possible reason poor results are achieved at the extremities is the small number of samples in these lfmc ranges to understand how sample size affects the results a comparison was made between the number of samples in each 1 lfmc range and the accuracy of the estimations fig 16 the plots show an inverse log log relationship between the two variables which may indicate the minimum number of samples needed at each lfmc level to obtain a particular accuracy 3 5 spatial evaluation of the multi tempcnn models 3 5 1 multi tempcnn performance across the climate zones the within site and out of site models both show significant differences in performance across the climate zones fig 17 however the climate zones for which both the best bwh and worst dsc results were obtained have very few samples fig 17a and d consequently these results should be treated with caution of the climate zones with over one thousand samples both models did well for csa achieving rmses of 15 74 and 19 60 and r2 values of 0 83 and 0 74 for the within site and out of site models respectively rmse for bwk arid cold deserts was well above the model averages with respective rmses of 24 75 and 29 86 probably due to the sparsity of vegetation in desert regions however the r2 values were also high at 0 76 and 0 61 respectively the within site models also did well for csb temperate regions with dry warm summers with an rmse of 17 97 and r2 of 0 77 while the rmses for the remaining climate zones were about the model average of 20 87 the out of site models performed well for the cfa rmse is 22 23 and r2 is 0 49 and dfb rmse is 22 68 and r2 is 0 36 climate zones but not so well for the csb dfc cold with no dry season and cold summers and dsb cold with dry warm summers climate zones with rmses of 30 86 30 27 and 28 52 and r2 values of 0 37 0 24 and 0 35 respectively the good result for csa one of the predominant climate zones on the west coast including wildfire prone california is particularly pleasing as the vegetation samples from this climate zone are drier than those from other climate zones fig 17a and d and therefore are of major bushfire concern generally the models show greater error on the wetter climate zones in line with the finding that the estimation error increases with higher lfmc values the poor out of site performance on csb is of some concern given that it is one of the drier climate zones and is the other predominant one in california 3 5 2 multi tempcnn performance by site locations results for both models show consistent performance across the conus with just a few grid cells standing out as performing poorly fig 18 the median grid cell rmse for the within site models is 18 40 with 57 of the cells having an rmse less than 20 fig 18a as expected from the overall results the out of site model performance was slightly worse with the median rmse being 22 94 and only achieving an rmse of less than 20 on 38 of the cells fig 18b overall performance in the eastern conus where there are few sites and samples appears similar to that in the western conus with many sites and samples however better results were obtained for the cluster of sites in michigan mi than the cluster in virginia va climate zones are a likely contributing factor here as mi is under the dfb climate zone which is well represented in the globe lfmc database whereas the database contains very few samples for the dfa climate of virginia when comparing the spatial distribution results of the multi tempcnn models to modis tempcnn some changes were observed the modis tempcnn models under perform in the rocky mountain region in northern idaho and montana and the coastal plains region in southern texas zhu et al 2021 while both the multi tempcnn models still under performed in these areas marked with circles in fig 18 there was a marked increase in performance in southern texas with the within site model rmse being 51 63 compared to 89 33 for modis tempcnn the within site models showed less performance improvement in the rocky mountain location with an rmse of 38 13 for the within site model compared to 43 99 for modis tempcnn 3 5 3 lfmc maps the map for the out of site multi tempcnn model fig 19 a shows low lfmc the mean estimated lfmc is 93 across the western conus except for the pacific northwest and the northern rocky mountains where the mean lfmc is estimated to be 116 very dry vegetation can be seen in the deserts of southern california ca and arizona az and western texas with a mean estimated lfmc of 63 the eastern conus has wetter vegetation than in the west where the mean lfmc is 119 although dry vegetation can be seen in the midwestern corn belt the region from kansas ks and nebraska ne through to ohio oh which has a mean lfmc of 103 there is a clear transition to high lfmc values mean is 132 in the large cfa climate zone that covers most of the south east conus which is consistent with the high average precipitation in this region konrad and fuhrmann 2013 the uncertainty of the lfmc estimations measured as the standard deviation of the individual estimations made by the ensemble members of the multi tempcnn out of site model ranges from 2 3 to 89 8 and is below 10 for 68 of the conus fig 19c the regions of greatest certainty are the states of colorado co wyoming wy and montana these regions are predominantly the bsk climate zone which is the climate zone with the largest number of samples in the globe lfmc dataset greatest uncertainty can be seen in southern florida which has a tropical climate that was unrepresented in the globe lfmc dataset and some of the desert regions of california utah ut and arizona the multi tempcnn out of site model generally estimates similar or higher lfmc than modis tempcnn fig 19b and d with a mean estimated lfmc of 111 an increase of 3 over the mean estimate of 108 for modis tempcnn the multi tempcnn lfmc estimates are much higher than modis tempcnn estimates in the coastal regions of oregon and washington increasing by 40 from a mean estimated lmfc of 99 for modis tempcnn to 139 for multi tempcnn southern texas an increase of 43 from 115 to 158 and florida an increase of 39 from 126 to 165 the main regions where multi tempcnn estimates lower lfmc are the afore mentioned deserts a decrease in mean estimated lfmc of 20 from 83 for modis tempcnn to 63 for multi tempcnn and midwestern corn belt a decrease of 13 from 116 to 103 4 discussion this study has proposed multi tempcnn a new deep learning architecture for large scale lfmc estimation multi tempcnn is based on the modis tempcnn architecture zhu et al 2021 but includes new sources of explanatory data and architectural changes that improve the accuracy of the lfmc estimates the two new data sources are meteorological data in the form of a year long time series and the k√∂ppen geiger climate zone auxiliary variable architectural refinements improve both computational performance and estimation accuracy and ensembling is used to further improve estimation accuracy and provide a measure of the estimation uncertainty the models built using the multi tempcnn architecture were designed for use in two scenarios in the first scenario the within site scenario the model is trained using historical field lfmc samples for a set of sites and applied to estimating lfmc at these sites at later dates in the second scenario the out of site scenario the model is trained using contemporaneous and historical lfmc samples for a set of sites and applied to estimating lfmc at other sites the study results showed the benefits of tailoring the architecture for each scenario the out of site scenario benefitted from a small shallow architecture which encourages the training process to extract simple generalisable features the out of site models achieved an rmse of 25 36 and r2 of 0 54 better results for the within site scenario were obtained using a larger deeper architecture which allowed the discovery of more complex site specific features the within site models achieved an rmse of 20 87 and r2 of 0 70 the architecture on which multi tempcnn is based modis tempcnn is a light weight architecture and with the globe lfmc data can be trained on readily available hardware as specified in section 2 5 the individual component models used to form the multi tempcnn models take three to 10 min to train compared to the 20 min required to train the modis tempcnn model taking ensembling into account the overall models are computationally more expensive than modis tempcnn which is not an ensemble requiring a three to ten fold increase in the training time for an ensemble of twenty models this is offset by the improvements in results leading to improvements in rmse of between 1 4 and 1 8 and an improvement in r2 of 0 06 additionally the variations in the individual estimations made by the ensemble members can be used to estimate uncertainty windows for the lfmc estimations providing fire management agencies with additional information critical for assessing fire risk yebra et al 2013 large ensembles provide increased confidence in the estimations and more accurate assessment of the uncertainty windows however the study results show only minor improvements in both rmse and r2 once the ensemble size is larger than 10 15 component models adjusting the ensemble size therefore allows trade offs to be made between model training time and the required level of confidence in the quality of the estimations considering the new data sources the meteorological data was important in the out of site models accounting for about half the improvement in this scenario it was less important in the within site models providing only a small benefit here the climate zone data had the opposite effect it was of most benefit to the within site models and made almost no difference to the out of site models this study used the same auxiliary inputs for each scenario for consistency allowing analysis of the effects of tailoring the architecture to the scenarios however the results presented in fig 10 show there is a case for also tailoring the inputs to each scenario if the main use of a model is to estimate lfmc at sites without any historical lfmc measurements then based on our results we suggest omitting at least the topographical variables the architectural changes and additional inputs resulted in an improvement in rmse of 2 3 3 5 and of 0 09 0 12 in r2 over modis tempcnn when trained and tested on a dataset spanning the conus the multi tempcnn models are also less biased with biases of 0 22 0 59 compared to 3 84 3 83 for modis tempcnn considering the two problematic areas identified by zhu et al 2021 the models showed improved results for the southern texas coastal plains but only a slight improvement for the rocky mountain regions of idaho and montana while both the overall results and the result for southern texas show that climatic differences are a contributing factor there is scope to further improve the models identifying additional data sources to incorporate into the models such as information about climate anomalies wind speed anomalies and root zone soil moisture is therefore a promising future direction the multi tempcnn models do not require prior knowledge of the land cover as this information is incorporated into features extracted by the trained models from the reflectance data zhu et al 2021 this is an advantage given misclassifications in land cover classification used in other lfmc models can be propagated to model estimates yebra et al 2013 evaluation across the main land cover types showed both the within site and out of site models achieved similar rmses for the forest and shrubland regions the within site rmse for grassland was slightly worse than for the other land cover types but the out of site models performed better achieving similar rmse and r2 to the easier within site scenario this result is especially pleasing as 1 there are fewer grassland samples in the training set and 2 grassland lfmc shows high temporal variability yebra et al 2013 thus models typically present a higher rmse for grasslands than for forests or shrublands garc√≠a et al 2008 yebra et al 2018 the models performances vary with the measured level of lfmc but show better than average results for the lfmc values that have been associated with increased wildfire risk in previous studies outside the range of 50 200 model performance deteriorates rapidly the models over estimate lfmc in extremely dry conditions and under estimate it in wet conditions this is consistent with the findings from other studies danson and bowyer 2004 zhu et al 2021 while other studies have shown optical data has limited sensitivity to wet vegetation the analysis in this study shows an inverse relationship between the sampling density across lfmc ranges and model performance this relationship can potentially be used to provide useful information about how sampling size affects model accuracy however further investigation is needed to control for possible confounding effects the proposed architecture is flexible allowing easy incorporation of other time series datasets by expansion of the convolutional layers while the study has only considered time series of the same length and intervals this is not a requirement of the architecture as the inputs are processed individually in the convolutional layers and only combined for the final fully connected layers the architecture therefore facilitates the integration of remote sensing products with differing temporal resolution such as the higher resolution sentinel 1 microwave synthetic aperture radar data with a 6 day revisit time and sentinel 2 optical data with a 5 day revisit time data currently the main obstacle to using these products is the lack of sufficiently recent samples in the globe lfmc dataset however once sufficient data is available using these data sources as additional or alternative inputs would allow generation of higher spatial resolution lfmc maps allowing more precise modelling of vegetation conditions 5 conclusions this study has presented a multi modal temporal cnn for lfmc estimation this new model learns from year long time series of meteorological and reflectance data plus a few auxiliary inputs including the climate zone ensembles of twenty models are used to achieve state of the art accuracy and assess the uncertainty of the lfmc estimations providing additional information useful for fire risk assessment in an operational scenario while the models performed well when estimating lfmc at fire danger levels further improvements in both the model accuracy and uncertainty estimates are needed to improve fire management applications the curators of the globe lfmc database plan to publish updates as new lfmc sample data becomes available yebra et al 2019 therefore future work includes incorporating new lfmc samples in the multi tempcnn models as they become available to keep the models up to date which should further improve accuracy and facilitate using the more recent and higher resolution remote sensing data sources previously discussed the models were trained and evaluated using data collected mainly from the western conus however the consistent results from the smaller number of samples from the eastern states showed models can make reasonable estimations here especially for regions that have a similar climate to well represented western regions more challenging is the application of models to fire prone regions outside the united states where the availability of lfmc field measurements is limited and environmental drivers may differ future work will investigate applying spatial domain adaptation techniques for remote sensing data such as sourcerer lucas et al 2021 to the multi tempcnn models and assess their use for estimating lfmc in other regions of the world funding this work was supported by an australian government research training program rtp scholarship and the australian research council under award dp210100072 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the diagrams of the multi tempcnn models shown in fig 4 were created using alexander lenail s nn svg webpage https alexlenail me nn svg index html 
25544,live fuel moisture content lfmc is an important environmental indicator used to measure vegetation conditions and monitor for high fire risk conditions however lfmc is challenging to measure on a wide scale thus reliable models for estimating lfmc are needed therefore this paper proposes a new deep learning architecture for lfmc estimation the architecture comprises an ensemble of temporal convolutional neural networks that learn from year long time series of meteorological and reflectance data and a few auxiliary inputs including the climate zone lfmc estimation models are designed for two training and evaluation scenarios one for sites where historical lfmc measurements are available within site the other for sites without historical lfmc measurements out of site the models were trained and evaluated using a large database of lfmc samples measured in the field from 2001 to 2017 and achieved an rmse of 20 87 for the within site scenario and 25 36 for the out of site scenario keywords live fuel moisture content modis convolutional neural network time series analysis fire risk deep learning ensembles data availability software the software developed for this study is available from https github com lynn miller lfmc estimation tree multi modal lfmc it is written in python version 3 8 using the anaconda 3 distribution version 2020 11 and can be run on both 64 bit windows and linux a cuda enabled gpu is recommended the main non standard python packages used are gdal version 2 3 3 tensorflow version 2 3 0 cuda cuda toolkit version 10 1 243 with cudnn version 7 6 5 and the google earth engine python api earthengine api version 0 1 283 a full list of packages and versions is provided in the github repository all maps were created using qgis desktop version 3 20 3 datasets all datasets used in this study are listed in table s1 table s1 summary of datasets used in the study table s1 product description source licence reference globe lfmc global database of lfmc field measurements https doi org 10 6084 m9 figshare c 4526810 v2 cc0 yebra et al 2019 beck kg v1 present 0p0083 present day 1980 2016 k√∂ppen geiger climate classification https figshare com articles dataset present and future k ppen geiger climate classification maps at 1 km resolution 6396959 2 cc by 4 0 beck et al 2018 nasa mcd43a4 006 modis daily reflectance data google earth engine gee product modis 006 mcd43a4 open access schaaf and wang 2015 nasa mod10a1 006 modis daily snow cover data gee product modis 006 mod10a1 open access hall and riggs 2016 nasa mod44w 006 modis water mask gee product modis 006 mod44w open access carroll et al 2017 prism an81d prism gridded daily climate dataset gee product oregonstate prism an81d free for non commercial use prism climate group 2004 nasa srtm digital elevation nasa srtm digital elevation 30 m gee product usgs srtmgl1 003 open access nasa jpl 2013 1 introduction historically wildfires form part of the ecological cycle kilgore 1973 sharples et al 2016 however they can cause widespread ecological damage and pose a threat to human life and economic activity gill et al 2013 wang et al 2021 in recent times wildfires have become larger and more frequent and intense a trend that is likely to continue due to anthropogenic climate change abatzoglou and williams 2016 abram et al 2021 jolly et al 2015 effective methods of preparing for and managing wildfire risk are therefore critical to mitigating the associated socio economic risks an important indicator of how a wildfire will burn and spread is the amount of water contained in living vegetation wet vegetation burns less easily as more energy is needed to evaporate the water xanthopoulos and wakimoto 1993 therefore acting as a heat sink dimitrakopoulos and papaioannou 2001 a common measure of this amount of water is the ratio between the weight of the water in the vegetation and its dry weight or live fuel moisture content lfmc dasgupta et al 2007 yebra et al 2013 however measuring lfmc is both time consuming and costly as vegetation samples must be collected from field sites and then weighed and dried under controlled conditions while this method is effective for monitoring small sites it is infeasible for monitoring large areas at risk from wildfires yebra et al 2013 consequently other methods of estimating lfmc are required an approach used by several studies is to estimate lfmc from remote sensing data collected by optical sensors on board low earth orbiting satellites marino et al 2020 yebra et al 2018 optical data can help to monitor lfmc due to the strong water absorption features in the near infrared and shortwave infrared frequencies danson and bowyer 2004 yebra et al 2013 a widely used source of optical data is the moderate resolution imaging spectrometer modis on board the nasa terra and aqua satellites nasa n d these satellites have been operational for over 20 years providing satellite image time series at high daily temporal resolution that help with the understanding of both seasonal and long term changes to the environment ban 2016 methods of modelling lfmc from remote sensing data fall into two broad categories physical modelling based on the inversion of radiative transfer models rtms and empirical models based on statistical or machine learning techniques quan et al 2021 yebra et al 2013 physical modelling utilises the known physical relationships between vegetation conditions and optical reflectance quan et al 2017 while these relationships are site independent and physical models potentially generalise well yebra et al 2008 they are dependent on accurate assessment of the biophysical parameters required to calibrate and parameterise the models quan et al 2016 physical models also suffer from the ill posed inversion problem yebra and chuvieco 2009 where different vegetation conditions result in similar reflectance levels due to the mixing from different properties quan et al 2021 which means that inferring vegetation conditions solely from reflectance levels can lead to a non unique solution hence prior knowledge is necessary to resolve this problem quan et al 2016 zhu et al 2021 as a result only a few studies quan et al 2021 yebra et al 2018 have developed continental and global scale lfmc maps using these physical models many studies have used empirical or statistical models to estimate lfmc from spectral bands or vegetation indices yebra et al 2013 these yield good results when used over local areas and for known land cover caccamo et al 2012 marino et al 2020 peterson et al 2008 however the relationships between biological and physical conditions and remote sensing data are complex and non linear leading to simple models not being able to generalise well quan et al 2017 which limits their usefulness in operational situations while optical remote sensing data captures the current state of the vegetation the drivers of changes to vegetation state are the climate and meteorological conditions however the relationship between meteorological conditions and the vegetation response is complex as time lags between precipitation and increases in vegetation water content vary between species jia et al 2019 pellizzaro et al 2007 vinodkumar et al 2021 cross seasonal dependencies have also been identified for example wet season spring precipitation levels have a direct effect on vegetation dryness during california s wildfire season in late summer and autumn swain 2021 although previous studies have used drought indices to incorporate seasonal meteorological conditions into remote sensing models for lfmc estimation costa saura et al 2021 garc√≠a et al 2008 pellizzaro et al 2007 these methods do not fully utilise time series of both remote sensing and meteorological data simultaneously and so may not adequately capture the relationships to vegetation state furthermore the climate has a major effect on vegetation distribution woodward and williams 1987 influencing both vegetation types and growth patterns beck et al 2018 data identifying locations with similar climates is therefore useful to help identify vegetation related features common to these regions recent research has explored the potential use of deep learning for lfmc estimation from time series of remote sensing data rao et al 2020 zhu et al 2021 deep learning models incorporate multiple levels of learning which can model complex and non linear relationships yuan et al 2020 temporal convolutional neural networks cnns are deep learning models that extract complex temporal features using stacked convolutional layers a convolutional layer consists of a sliding window filter that is applied along a time series identifying locations in the time series where the feature occurs pelletier et al 2019 a pooling layer where adjacent time steps are aggregated using a max pooling or average pooling operation may be applied after each convolutional layer ismail fawaz et al 2019a pooling layers reduce the resolution of the time series and provide a multi scale analysis pelletier et al 2019 zhu et al 2021 trained a temporal cnn hereinafter referred to as modis tempcnn to estimate lfmc from a one year time series of modis data which was used to produce lfmc maps for the contiguous usa conus the explanatory data included a set of auxiliary variables providing spatial temporal and topographical information about each pixel the modis tempcnn model requires no prior knowledge of the vegetation type thus eliminating land cover misclassifications as a potential source of error however modis tempcnn is a simple re implementation of a temporal cnn originally designed for land cover classification and mapping pelletier et al 2019 and was not optimised for numerical lfmc estimation nor did it take into account climate and meteorological conditions which are key drivers of lfmc a further enhancement investigated in this study is to create an ensemble of models then during the inference phase aggregate the individual model estimates to produce the final estimate ensembling machine learning models is a widely accepted method to improve the results of individual models by reducing model variance oza and tumer 2008 and providing a method of estimating model uncertainty gawlikowski et al 2021 ensembling has been proven to be useful in a wide range of remote sensing applications bigdeli et al 2021 huang and zhang 2013 kussul et al 2017 pal 2005 2008 rodriguez galiano et al 2012 while not designed specifically for remote sensing tasks inceptiontime ismail fawaz et al 2020 showed cnn ensembles for time series models have the potential to be both accurate and scalable in summary this study proposes a new deep learning architecture for large scale daily lfmc estimation hereinafter referred to as multi tempcnn multi tempcnn is an extension of modis tempcnn zhu et al 2021 that improves the accuracy and stability of the lfmc estimates by 1 including a year long time series of meteorological data and remotely sensed optical data 2 including the k√∂ppen geiger climate classification 3 using an ensemble of multi tempcnn models 4 providing a measure of uncertainty for the lfmc estimate and 5 tailoring the architecture for lfmc estimation and computational efficiency 2 methods 2 1 data sources 2 1 1 lfmc sample dataset the globe lfmc database yebra et al 2019 is a large historic archive of destructively sampled lfmc measurements collected between 1977 and 2018 each sample includes the sampling date location and land cover type while this database contains data from 1383 sites located in eleven countries 69 of the data collection sites and 84 of the samples are from the contiguous united states conus yebra et al 2013 table 4 accordingly this study is focused on the conus 2 1 2 optical remote sensing data many satellite missions provide optical reflectance data and selecting an appropriate source is often a trade off between the characteristics of each source the sources of the most commonly used reflectance data are modis the multispectral imagers on board the landsat satellites u s geological survey n d and the multispectral instrument msi on board the sentinel 2 satellites european space agency 2019 this study in line with zhu et al 2021 used modis data due to its high temporal resolution daily and historical data dating back to late february 2000 despite the comparatively low spatial resolution of 500 m and the approaching end of life of the terra and aqua satellites although both landsat and sentinel 2 provide higher resolution data thus potentially allowing lfmc to be estimated at higher resolution and have planned mission continuity they have drawbacks that make them unsuited to this study before the commissioning of landsat 9 in january 2022 landsat had low temporal resolution 16 days which after the removal of cloud covered images would yield a sparse time series sentinel 2 data is only fully available since the commissioning of the sentinel 2b satellite in 2017 providing little overlap with the date range in the globe lfmc database the modis data used was from the combined modis terra and aqua product mcd43a4 collection 6 mcd43a4 schaaf and wang 2015 this product contains daily reflectance measurements at 500 m resolution across seven spectral bands covering the visible near infra red nir and shortwave infra red swir frequencies it is an analysis ready product with a bidirectional reflectance distribution function brdf fitted using 16 days data from d 8 to d 7 is used to correct reflectance at date d of terra and aqua modis data this removes view angle effects to produce a nadir brdf adjusted reflectance nbar product modis nbar reflectance data is available from late february 2000 and was obtained from google earth engine gee gorelick et al 2017 product modis 006 mcd43a4 2 1 3 meteorological data the meteorological data is the an81d product from the oregon state university prism parameter elevation relationships on independent slopes model collection prism climate group 2004 prism products are widely used sources of spatial climate data for the conus lundquist et al 2015 and are the official spatial climate data sets of the usda us department of agriculture daly et al 2008 studies have found the products have good agreement with measurements from a network of 114 weather stations across the conus buban et al 2020 and perform well in complex terrain walton and hall 2018 data for the prism models is collected from up to 16 000 weather stations and interpolated into a gridded format using dem data and a climate elevation regression algorithm daly et al 2015 the prism an81d product contains daily estimates of total precipitation mean temperature minimum temperature maximum temperature mean dew point temperature minimum vapor pressure deficit and maximum vapor pressure deficit the data has a 4 km resolution making it one of the higher resolution gridded climate datasets available walton and hall 2018 prism data was obtained from gee product oregonstate prism an81d and was automatically resampled and reprojected to the modis resolution and projection on extraction from gee which uses nearest neighbour resampling 2 1 4 climate zone data in the 19th century k√∂ppen 1884 developed a method of classifying areas of the earth by climate to help identify regions of similar vegetation it was later modified by rudolf geiger kottek et al 2006 resulting in the k√∂ppen geiger classification system which is to date still the most widely used climate classification system peel et al 2007 the k√∂ppen geiger system classifies the earth s land surface into 30 climate zones based on precipitation and temperature peel et al 2007 and is organised into a 3 tier hierarchy with the first tier indicating the general climate the second the precipitation and the final tier the temperature recent revisions of the k√∂ppen geiger world map based on updated climate data include peel et al 2007 and beck et al 2018 while the beck dataset is not as widely used as the peel dataset it was selected for this study as it has several characteristics that make it a suitable source at approximately 1 km 0 0083 resolution it is one of the higher resolution k√∂ppen geiger maps available capturing the land surface s variability at a resolution close to the mcd43a4 data furthermore it is based on more recent climate data from more climate stations and multiple independent climate data sources beck et al 2018 which have been corrected to account for topographic effects on temperature and precipitation mcvicar et al 2007 roe 2005 improving classification accuracy in mountainous regions karger et al 2017 the conus sites in the globe lfmc dataset are located across fifteen climate zones table 1 fig 1 however over half the samples are from sites in climate zones bsk and csa while six climate zones bwh cfb dfa dsa dsc and dwb have ten or fewer sampling sites and less than four hundred samples additionally the globe lfmc dataset has no conus samples for seven climate zones the major climate zones with no samples are the three tropical zones af am and aw in southern florida fl and the dwa zone found mainly in nebraska ne and south dakota sd and coloured lilac in fig 1 the other three climate zones with no samples csc cfc and et cover small regions and are not visible at the scale used in fig 1 2 1 5 topographical data topographical data was obtained from gee product usgs srtmgl1 003 which is based on data from the shuttle radar topography mission srtm nasa jpl 2013 this dataset provides elevation data at 30 m resolution slope and aspect the direction in degrees the slope faces so a north facing slope has aspect 0 and an east facing slope has aspect 90 were calculated from the elevation data using the gee terrain products gorelick et al 2017 as recommended by grohmann 2015 the slope and aspect were calculated at 30 m resolution then all three topographical variables were resampled to modis resolution using the weighted means of the input pixels google earth engine 2021 2 2 data preparation this study used the conus samples from the globe lfmc dataset collected on or after march 1 2001 this date is one year after the start of the modis data thus a one year time series of modis data can be obtained for each sample there are 125 049 samples which were collected from 932 unique sites some pre processing of the globe lfmc samples was required as the multi tempcnn models estimate lfmc at the pixel level not point level site and sample locations were adjusted to be the centroid of the mcd43a4 pixel in which they are located samples located in the same mcd43a4 pixel and collected on the same day were merged with the merged lfmc value set to the mean of the merged samples this reduced the number of samples from 125 049 to 67 770 and the number of sites from 932 to 924 a further 824 samples that were collected under snow conditions according to the modis snow cover product mod10a1 hall and riggs 2016 obtained from gee product modis 006 mod10a1 were excluded so the final dataset of lfmc samples contained 66 946 samples from 924 sites subsequent references in this paper to the globe lfmc dataset and or lfmc samples are to this prepared dataset rather than the raw samples analysis of the prepared globe lfmc dataset shows the field samples were collected from mainly forest shrubland and grassland locations table 2 the dataset also contains a few samples from croplands agriculture samples collected from land cover classes that do not fit into any of these four categories mainly urban areas wetlands and water bodies are included in the other category for each sample in globe lfmc a one year 365 days time series of the seven spectral bands in the mcd43a4 data was extracted from gee the time series starts 365 days before the sampling date and ends the day before the sampling date as the tempcnn architecture does not handle missing data any missing mcd43a4 data was filled using linear interpolation in the temporal dimension gap filling was done using the python pandas package finally each band of each sample was normalised using the formula b i n p 2 i p 98 i p 2 i where b i n is the i t h band time series for sample n and p 2 i and p 98 i are the global 2nd and 98th percentiles for band b i pelletier et al 2019 zhu et al 2021 pre processing of the mcd43a4 data resulted in a 365 day 7 spectral band matrix for each sample the prism meteorological data was extracted from gee and normalised similarly six auxiliary variables were prepared for each sample table 3 the latitude and longitude of the mcd43a4 pixel centroid which provided location information the elevation slope and aspect that were extracted from the srtm data and the k√∂ppen climate zone obtained from the 1 km resolution beck et al 2018 climate zone dataset using nearest neighbour sampling this study does not use the day of the year which was included in the modis tempcnn model as shown in the results fig 10 including this variable was found to have a slightly detrimental effect on model accuracy following zhu et al 2021 the slope latitude and elevation were normalised to a 0 1 range a sine and cosine transformation were applied to both the longitude and aspect to obtain two normalised variables for each modelling the cyclic nature of these variables the categorical climate zone data was transformed using one hot encoding table 3 note 2 to obtain a binary variable for each of the fifteen climate zones represented in the samples in total there are twenty two normalised auxiliary variables 2 3 modelling scenarios two modelling scenarios are used in this study in the first scenario referred to as the within site model the model is trained to estimate lfmc at sites where historical field measurements exist the within site model is designed to meet an operational requirement where up to date lfmc estimates are needed at or near sites with historical measurements for example estimating the current lfmc in a fire prone area the second scenario referred to as the out of site model is designed to meet an operational requirement for lfmc estimates for regions for which historical lfmc field measurements are not available which is currently the majority of the earth s land surface in an operational context out of site models would be trained using sample data from all field sites however for evaluation purposes some sites are withheld from the training data and the samples from these withheld sites are used to evaluate the models this process is explained in more detail in the following paragraphs the out of site model was used to produce the lfmc map in fig 19 ideally the model built for each scenario would be trained using all the available data and that model used to produce estimates for unseen dates within site scenario or unseen locations out of site scenario however if all the data were used to train the models there would be no data to test the performance of the models therefore to estimate the performance of a model trained on all the data the sample data is split into several disjoint test sets each with a corresponding training set formed from the remaining samples models are trained on one training set and used to estimate the lfmc for each sample in the corresponding test set fig 2 a set of models using each pair of training and test sets once is referred to a model set in any model set each evaluation sample is in one model s test set therefore combining the test estimates from all the models in the model set yields one estimate for each sample from any test set accuracy statistics section 2 5 1 are calculated using all lfmc estimates from the model set and thus provide an estimate of the performance of the ideal model such as that used to produce the lfmc map in fig 19 the methods used to create the evaluation model sets and the corresponding test and training sets for each scenario are described in the next paragraphs each evaluation model set for the within site scenario was created as follows one model was built for each of the last four years for which the globe lfmc dataset contains data 2014 2017 the evaluation years the models were trained using the samples collected during all years before the evaluation year the trained model was then used to estimate lfmc for all samples collected during the evaluation year fig 2a for example the model for 2014 was trained with data from 2001 to 2013 and used to estimate lfmc for all samples from 2014 while the model for 2017 was trained with data from 2001 to 2016 and used to estimate lfmc for all samples from 2017 most results reported for the within site scenario were evaluated using collated lfmc estimates from all four years full results fig 2c however the annual results are also reported for the overall performance each evaluation model set for the out of site scenario was created using a 10 fold cross validation strategy the sites in the globe lfmc dataset were split into ten groups or folds using stratified random sampling the random sampling stratification was done by land cover class and used to ensure an even distribution of land cover classes across the folds ten models were built each of the ten models was trained using samples collected from sites in nine of the ten folds and then used to estimate lfmc for the remaining samples fig 2b each sample is therefore used for training in nine models and evaluation in one model results reported for this scenario were evaluated using the collated lfmc estimates for the evaluation samples from all ten folds fig 2d 2 4 multi tempcnn architecture 2 4 1 convolutional neural networks deep learning architectures or neural networks consist of a network of simple computational units or neurons arranged in layers lecun et al 2015 the basic architecture consists of fully connected fc layers where the inputs to each unit are the outputs from all the units in the previous layer each unit applies a linear transformation to the inputs then a non linear transformation or activation function to produce an output the final or output layer is either composed of multiple units providing either class membership probabilities for classification tasks or a single unit to estimate the required quantity for regression tasks training the network consists of learning the weights of the linear transformations that best fit the model to a training data set for a regression problem this is usually the mean squared error defined as i 1 n y ÀÜ i y i 2 n where y ÀÜ i and y i are respectively the estimated and measured values for the i t h training sample and n is the number of training samples convolutional layers lecun et al 1989 are useful when the inputs have dimensionality and so are suited to both spatial 2 dimensional and temporal 1 dimensional data here the neuron is replaced with a fixed width convolutional filter this filter is slid across the input and applied at each position to produce a linear combination of the surrounding points for example if a filter of size five is used with temporal data as in the current study the output is another time series where each point is the result of convolving the corresponding input point with the preceding two and succeeding two points a layer can have multiple convolutional filters resulting in a multivariate output convolutional neural networks cnns often contain both convolutional and fully connected layers 2 4 2 deep learning ensembles deep learning networks are commonly initialised by randomly assigning values to weights the model is then trained by iteratively updating the weights to improve the fit to the training data until a stopping condition is reached if multiple models that have been randomly assigned different initial weights are trained they will often find different solutions which may well be similar in overall accuracy choromanska et al 2015 but during inference show variation in the estimated target values for a single instance neal et al 2018 this study exploited this inherent randomness by training multiple models each with a different random weight initialisation and ensembled them by averaging the estimations at inference time ismail fawaz et al 2019b furthermore ensembled models allow estimating uncertainty for the lfmc map by using the standard deviation of the individual ensemble member predictions several factors were considered when setting the ensemble size 1 the training and inference time which increase with the ensemble size 2 the increase in accuracy as the ensemble size increases and 3 the increase in confidence of the uncertainty as the ensemble size increases while there are only small improvements in accuracy with an ensemble size larger than ten section 3 2 an ensemble size of twenty was chosen as being the minimum necessary to ensure a reasonable degree of confidence in the uncertainty 2 4 3 proposed architecture the proposed multi tempcnn deep learning architecture is based on the tempcnn architecture pelletier et al 2019 developed for land cover classification from optical data tempcnn consists of three temporal convolutional layers with average pooling to extract temporal features these features are passed through a fully connected layer then the final classification layer the activation function used for all layers is the rectified linear unit relu krizhevsky et al 2012 nair and hinton 2010 models are trained using the adam optimiser kingma and ba 2014 with default parameters Œ≤ 1 0 9 Œ≤ 2 0 999 and —î 10 8 and a learning rate of 0 01 deep learning models have a large number of parameters to train which can greatly exceed the number of training samples this leads to models that can overfit the training data to control for overfitting batch normalisation ioffe and szegedy 2015 and a small l2 weight regularisation of 10 6 are applied to each layer of both the within site and out of site models additionally dropout srivastava et al 2014 is used in the within site models the lower model complexity of the out of site models and ensembling also help to control overfitting modis tempcnn is a modification of tempcnn to the lfmc estimation context zhu et al 2021 the primary modifications were to include non temporal or auxiliary variables as additional inputs to the fully connected layer add a second fully connected layer and replace the final classification layer with a linear regression unit here modis tempcnn was further refined motivated by the need to expand the architecture to include time series of meteorological data and auxiliary climate zone data as inputs the most significant change was to add another set of convolutional layers to extract temporal features from the meteorological data furthermore to understand whether other architecture changes would benefit the new models a range of model architectures was explored table 4 as a result of this exploration changes were made to the number of layers and filters used in the convolutional layers the pooling size of each convolutional layer the number of layers and units used in the fully connected fc layers and the model wide hyper parameters of the drop out rate batch size and number of times to iterate through the training data epochs during this phase of the study care was taken to avoid overfitting the architecture to the evaluation data by using samples from only half the sites and collected before 2017 fig 3 after the architectures for each scenario were designed they were evaluated using the entire dataset with the data split into training and evaluation sets as described in section 2 3 consequently both the training and evaluation sets used for model evaluation contained a mix of samples used and not used during architecture design note that while all results presented are from models trained and evaluated using the full datasets both the model architectures and inputs were decided based on the architecture design results and locked in place before the experiments producing these results were performed to verify that no overfitting occurred model evaluation table 5 included a break down of performance by the samples used during architecture design architecture design samples and those held out from architecture design held out samples the architecture exploration showed that the two scenarios benefit from different model architectures fig 4 for the out of site scenario a small shallow architecture about one fifth the size of modis tempcnn with fewer convolutional filters and a single fully connected layer and training the model for fewer epochs gave the best results the simpler architecture likely extracted fewer but more generalisable features and increased the variation between models which was exploited by ensembling for the within site scenario a larger deeper architecture about three times the size of modis tempcnn with more convolutional layers and an extra fully connected layer gave superior results the additional complexity may have allowed the extraction of site specific features which were useful when making estimates for known sites but did not help when estimating for unknown sites changes that benefitted both scenarios were reducing the dropout rate and increasing the batch size used during model training 2 5 evaluation methods the multi tempcnn models were developed and implemented in python version 3 8 using tensorflow v2 3 abadi et al 2015 with keras chollet et al 2015 the code to construct train and evaluate the models is available at https github com lynn miller lfmc estimation tree multi modal lfmc two computing environments were used for model training and evaluation the first computer was a dell precision 5530 laptop with six intel core i7 8850h cpu 2 60 ghz processors 32 gb ram and one nvidia quadro p1000 gpu with 4 gb memory all reported timings are from tests run on this first computer as it was a dedicated environment the second environment was a high performance cluster with four nodes each node consisted of 20 intel xeon silver 4214 r cpu 2 40 ghz processors 180 gb ram and four nvidia quadro rtx 6000 gpus 2 5 1 evaluation metrics the results were analysed using the following standard metrics root mean squared error rmse r m s e i 1 n y ÀÜ i y i 2 n where n is the number of evaluation samples and y i and y ÀÜ i are the measured and estimated lfmc for the i t h sample respectively coefficient of determination r2 r 2 1 i 1 n y ÀÜ i y i 2 i 1 n y i y 2 where y is the mean measured lfmc of the evaluation samples r2 measures the performance of a model compared to a baseline model which simply estimates the sample mean values close to zero indicate the model only performs as well as the baseline model while values close to one indicate the model fits the data well when r2 is calculated for a subset of samples y is the mean of the measured lfmc for the full sample set not the mean of the subset by using this value for the mean all r2 calculations are compared to the same baseline thus allowing comparisons between r2 for different subsets of samples the model bias bias or the mean estimation error b i a s i 1 n y ÀÜ i y i n the sign of the bias indicates if the model over estimates b i a s 0 or under estimates b i a s 0 lfmc the absolute value of the bias indicates the mean over or under estimation the metrics were calculated for each model set to produce fifty metrics for each scenario the results presented are the mean of these fifty values and where relevant the standard deviation or 95 confidence interval ci 2 5 2 overall performance evaluation as deep learning models have some inherent randomness multiple instantiations of the models are required to ensure the robustness and generalisability of the results therefore to evaluate the performance of the proposed multi tempcnn within site and out of site models fifty different instantiations of the model sets for each scenario see section 2 3 were created however it was computationally infeasible to do this in a na√Øve way for an ensemble of size twenty this would mean creating 4000 individual within site models 4 evaluation years 20 models per ensemble 50 ensembles and 10 000 individual out of site modes 10 folds 20 models per ensemble 50 ensembles therefore for each evaluation year for the within site models or fold for the out of site models a pool of fifty individual models was created fig 5 ensembles were assembled by randomly selecting without replacement m models from this pool where m is the ensemble size and the ensemble estimations for each sample generated by averaging the estimations made for the sample by the m selected models this process was repeated for each of the evaluation years 2014 2017 for the within site models and each of the ten folds for the out of site models the model sets were then formed using one ensemble for each year or fold for example the first ensemble for each of the four evaluation years formed the first within site model set the second ensemble for each of the evaluation years formed the second within site model set and so on to create fifty model sets the fifty within site model sets are referred to as the within site models in the results and discussion sections similarly the fifty out of site model sets are referred to as the out of site models unless otherwise stated the ensemble size of all models in each model set is twenty to verify that no hyper parameter overfitting occurred model evaluation included a break down of performance by the samples used during architecture design architecture design samples and those held out from architecture design held out samples two sets of comparison models were evaluated the first to show the benefit of tailoring architectures to each scenario and the second to show the improvement of multi tempcnn over modis tempcnn for the first comparison fifty model sets with ensemble size twenty for each scenario were created by using the scenario to train and evaluate the architecture optimised for the other scenario for the second comparison fifty model sets of modis tempcnn models were trained and evaluated for each scenario as modis tempcnn is not an ensemble model each modis tempcnn model set is comprised of individual models in other words the ensemble size is one the computational efficiency of the multi tempcnn models were evaluated in terms of the training times and compared to the training time of the baseline modis tempcnn model while it is trivially easy to train ensemble members in parallel the reported training times assume sequential processing of ensemble members timings are for running the model fitting only and exclude reading and pre processing normalising the data 2 5 3 evaluation of ensembling the impact on the performance when varying the ensemble size was assessed by creating fifty ensembles of each of 5 10 15 20 and 25 model sets the ensemble size in the manner described in section 2 5 2 the mean and 95 ci for the rmse and r2 were calculated for each ensemble size and the individual models changes to bias are not evaluated as bias is not affected by ensembling ueda and nakano 1996 the performance improvement when ensembling is analysed by examining the mean estimation variance for the individual model sets and for the model sets of each ensemble size the mean estimation variance v a r s is defined as follows let e s be the set of model sets with ensemble size s then v a r s i 1 n j 1 m y i j y i 2 m n where y i j is the estimation made for the i th sample by ensemble j e s y i is the mean estimation for the i th sample m e s is the number of model sets and n is the number of evaluation samples the mean estimation variance is a component of the model error friedman 1997 and is expected to drop as the ensemble size increases thus improving the model performance ueda and nakano 1996 2 5 4 input and architecture ablation tests the contributions of the new inputs and the architecture changes to both the within site and out of site multi tempcnn models from those used in the baseline modis tempcnn model were evaluated via ablation tests for each test fifty model sets with an ensemble size of twenty were created following the method described in section 2 5 2 and the mean and 95 cis for the rmse bias and r2 calculated using these ensembles during the input ablation tests models were constructed withholding the meteorological and climate zone data in turn in addition each group of auxiliary variables table 3 was withheld in turn to confirm which auxiliaries are required in the multi tempcnn models the final input ablation test added to the models the day of year auxiliary variable prepared as described by zhu et al 2021 which is used in modis tempcnn but is not used in the multi tempcnn models the purpose of this test is to show that multi tempcnn does not require the day of year the architecture ablation tests reverted each hyper parameter changed for the multi tempcnn models back to the setting used in the modis tempcnn model the architecture changes were assessed by both model training time and model accuracy 2 5 5 evaluation by land cover and lfmc range as discussed by zhu et al 2021 models based on the tempcnn architecture are not land cover type specific nor do they need prior knowledge of the vegetation type because this information is embedded in the reflectance data however to assess how the proposed multi tempcnn models perform across these categories the results are evaluated by land cover type using the classifications shown in table 2 as vegetation types and conditions change at different elevations this includes an analysis of performance by elevation further analysis is done to understand how well the lfmc estimates can be used to identify whether or not vegetation is dry enough to sustain a fire should one ignite this is done by establishing fire danger thresholds for grassland forest and shrubland land cover and identifying the proportion of samples correctly identified as being high or low fire risk below or above the threshold respectively or mis classified as high fire risk or low fire risk while a range of fire danger thresholds have been proposed in various studies chuvieco et al 2004 dennison et al 2008 jurdao et al 2012 pimont et al 2019 this study uses the high fire hazard thresholds established by arga√±araz et al 2018 as indicative thresholds these thresholds are 67 for grasslands 105 for forests and 121 for shrublands when using lfmc estimates for fire risk evaluation good estimates of lfmc in dry conditions are critical as small differences in lfmc result in large changes to perceived fire risk jurdao et al 2012 while a larger error can be tolerated when vegetation is wet zhu et al 2021 therefore performance metrics are provided firstly for all high fire risk samples then by ranges of the ground truth lfmc grouped into 5 intervals from 30 to 250 to assess the potential of the models to be used to accurately identify the higher fire risk situations 2 5 6 spatial evaluation and lfmc maps most samples in the globe lfmc dataset were collected from sites located in the western states of the conus and from climate zones bsk arid cold steppes and csa temperate regions with dry hot summers if models are to be useful for operational applications they need the capability to generalise to locations and climate zones with fewer samples therefore a spatial analysis of the study results to identify differences in performance across locations and climate zones was also carried out firstly performance was evaluated across the fifteen climate zones represented in the globe lfmc dataset then performance was evaluated by site location across the conus and results in two key areas compared with the modis tempcnn baseline results here the lfmc sample sites have been grouped into 0 5 grid cells and the rmse and bias for the samples in each cell were computed for both models finally an lfmc map for the conus was produced for october 1 2017 using the out of site multi tempcnn model the model used to produce the map is an ensemble of twenty individual out of site models which were trained using all the samples in the globe lfmc dataset fig 6 thus is an instance of the model that the cross validation process described in section 2 3 was used to evaluate the map is accompanied by three other maps firstly an uncertainty map that uses the standard deviation of the estimates made by each ensemble member as the measure of uncertainty secondly an lfmc map produced using the modis tempcnn model the last map compares the multi tempcnn lfmc map with the modis tempcnn lfmc map by showing the differences in lfmc estimation for each pixel the map date of october 1 2017 was chosen as it is near the peak of the western conus wildfire season swain 2021 water bodies as identified by the modis mod44w 006 water mask product carroll et al 2017 have been masked 3 results 3 1 overall performance of the multi tempcnn models 3 1 1 within site models the multi tempcnn models proposed for the within site scenario achieved a mean rmse of 20 87 fig 7 a with a standard deviation of 0 03 table 5 across the models for the four estimation years 2014 2017 the mean r2 was 0 70 which means that the models collectively explain over two thirds of the variance in the sample lfmc the mean bias of the models is small at 0 22 so the models under estimate lfmc about as often as they over estimate it considering the results for each year separately table 5 the model for 2014 performed the best with an rmse of 20 07 models for 2016 and 2017 obtained rmses of about 20 64 and 20 37 respectively while the 2015 model performed worse with an rmse of 22 33 r2 ranged from 0 66 2015 to 0 72 2016 some variation in performance between years as shown in 2014 2016 and 2017 is to be expected the outlying 2015 result is likely due to the unusually high precipitation and el ni√±o conditions noaa national centers for environmental information 2016 occurring that year 3 1 2 out of site models the multi tempcnn models proposed for the out of site models achieved a mean rmse of 25 36 fig 7b with a standard deviation of 0 02 table 5 the models have a mean r2 of 0 54 so explain a little over half of the variance in sample lfmc and show little bias with a mean bias of 0 59 3 1 3 assessment of hyper parameter overfitting slightly better results were obtained when the assessment of the models was done with the samples held out from the architecture design compared to the samples used by the architecture design process this is most evident for the within site models where the rmse for the held out samples were 0 89 lower than the rmse for the architecture design samples the r2 value for the held out samples is 0 03 higher than for the architecture design samples the results for the out of site models are closer the rmse and r2 are 0 13 and 0 01 lower respectively for the held out samples than for the architecture design samples these results indicate that hyper parameters have not been overfitted during architecture design 3 1 4 model architecture comparisons for both the within site and out of site scenarios models trained using the architecture designed for the other scenario performed worse than models trained using the architecture designed for the scenario fig 8 the out of site architecture only achieved an rmse of 24 08 and r2 of 0 60 when trained and evaluated using the within site scenario similarly the within site architecture only achieved an rmse of 26 68 and r2 of 0 49 when trained and evaluated using the out of site scenario the within site comparison model showed a little more bias than the proposed models with a difference in absolute bias of 1 00 fig 8b the absolute bias of the out of site comparison model was similar to that of the proposed model but the comparison model tended to under estimate lfmc fig 8e for each evaluation metric differences between the models blue stars and orange circles in fig 8 are much larger than the confidence intervals of the means modis tempcnn models trained using the within site and out of site scenarios performed worse than the multi tempcnn models fig 8 the modis tempcnn architecture only achieved an rmse of 24 40 and r2 of 0 58 for the within site scenario and 27 62 and r2 of 0 45 for the out of site scenario both modis tempcnn models were also more biased than the multi tempcnn models with differences in absolute bias of 3 62 within site scenario and 3 24 out of site scenario and both modis tempcnn models tend to under estimate lfmc for each evaluation metric differences between the models blue stars and green hexagons in fig 8 are much larger than the confidence intervals of the means 3 1 5 computational efficiency the individual component models of the multi tempcnn models train much faster than modis tempcnn taking half and one seventh the time for the within site and out of site models respectively table 6 this is mainly due to the increased batch size and reduced number of convolutional filters see also table 7 in section 3 3 2 however training the full ensemble is slower than training the single modis tempcnn model as shown in section 3 2 ensembling contributes significantly to the improved lfmc estimates produced by multi tempcnn so compensating for the extra training time 3 2 the effect of ensembling on model performance a substantial improvement in rmse of about 1 5 from 22 62 for individual models to 21 13 for an ensemble of five for the within site models is gained by using even a small ensemble of five models and smaller improvements continue to be made as the ensemble size increases with an rmse of 20 84 for an ensemble size of 25 fig 9 a likewise r2 improves from 0 64 for individual models to 0 69 for ensembles of five models to 0 70 for ensembles of 15 25 models fig 9b the out of site models follow a similar pattern achieving an overall improvement in rmse of about 1 4 from 26 75 for individual models to 25 34 for models with an ensemble size of 25 r2 for the out of site models ranges from 0 48 for individual models to 0 54 for ensembles of size 20 and 25 confidence intervals for both metrics are too small to be plotted in fig 9 cis for the rmses range from 0 01 for the larger ensembles to 0 08 for individual models and cis for r2 are all less than 0 01 the mean estimation variance of the individual models is 80 9 for the within site models and 76 2 for the out of site models fig 9c the mean estimation variance of the ensembled models is significantly lower at only 2 of the individual model variance with ensemble size twenty five the performance gains due to ensembling are offset by increases in both training and estimation time which increase in proportion to the ensemble size therefore the optimal ensemble size is a trade off between the desired accuracy and computational requirements as discussed in section 2 4 2 the results reported in other sections of this paper use ensembles of twenty models 3 3 input and architecture ablation tests 3 3 1 input ablation tests the two new data sources used in the multi tempcnn models each primarily benefit the models for one scenario fig 10 the meteorological data primarily benefits the out of site models omitting this data increased the rmse by 1 37 and decreased r2 by 0 06 omitting the meteorological data from the within site models is less significant providing only a marginal degradation of 0 11 in rmse and 0 01 in r2 the climate zone primarily benefits the within site models omitting this data increased the rmse by 0 47 and decreased r2 by 0 02 omitting the climate zone data made almost no difference to the out of site models the contribution of the auxiliary variables also changes between the two scenarios fig 10 the topographic data benefits the within site models omitting this data increased the rmse by 0 69 and decreased r2 by 0 02 however the topographic data has a slightly detrimental effect on the out of site models the location variables provide a small benefit to both models the with day of year test shows that adding the day of year auxiliary variable which is used in modis tempcnn leads to slightly worse performance in both scenarios thus confirming the day of year is not required in the multi tempcnn models none of the tests showed any significant bias with the maximum bias being 0 67 for the within site models and 0 74 for the out of site models confidence intervals for the mean rmse ranged from 0 00 to 0 02 and ranged from 0 000 to 0 001 for the mean r2 3 3 2 architecture ablation tests the architecture improvements fall into two categories some reduced the model training time table 7 and others increased model accuracy fig 11 the changes that reduced the model training time were reducing the number of convolutional filters increasing the batch size and for the out of site models reducing the number of epochs while these changes substantially reduced the model training time they had only a minor effect on model accuracy the architectural change that resulted in the largest improvement to the accuracy of the lfmc estimations was to reduce the dropout rate used in model training fig 11 the out of site models did not require any dropout removing the dropout improved the rmse by 0 73 and r2 by 0 03 a low dropout rate of 0 1 was found to be beneficial to the within site models and the reduced dropout rate improved the rmse by 1 46 and r2 by 0 05 over the original dropout rate of 0 5 fewer convolutional filters in both architectures and extra pooling in the within site architecture reduced the number of features extracted by the convolutional layers reducing the need to control overfitting using dropout other changes that improved estimation accuracy were optimising the number of fully connected fc layers in both models and convolutional layers in the within site models the changes improved model rmse by between 0 26 and 0 45 and r2 by between 0 009 and 0 018 optimising the number of fully connected units only yielded a marginal improvement in estimation accuracy the number of fully connected layers appears to have a negative correlation to the model bias fig 11b and e the within site scenario test using two fully connected layers showed higher bias with a bias of 2 43 compared to 0 22 for the proposed model which has three fc layers the out of site scenario test using two fully connected layers has a bias of 0 09 which is less than the bias for the proposed model 0 58 the other test that showed significant bias was the out of site dropout test where adding a dropout of 0 5 caused the model to under estimate rather than over estimate lfmc and doubled the bias from 0 59 to 1 16 3 4 evaluation of multi tempcnn performance by land cover and lfmc range 3 4 1 evaluation of the models performance by land cover as shown in table 2 the predominant land cover classes in the globe lfmc dataset are grassland forest and shrubland the other classes of agriculture and other mainly wetlands water bodies and urban areas are included in the land cover analysis for completeness however as they are less critical for wildfire risk monitoring and have few samples they are excluded from both the land cover by elevation analysis later in this section and the fire risk analysis section 3 4 2 an outcome found repeatedly throughout this section is that a small number of samples for either a land cover class or a combination of land cover and elevation leads to less reliable model performances the within site models estimate lfmc in forest shrubland and other well fig 12 with rmse less than 21 so showing consistent results across the forest and shrubland classes which have the largest number of samples however there is a large difference between the r2 values for forest 0 62 and shrubland 0 77 due to low and high variation respectively in the measured lfmc across both time and locations table 2 the smaller number of agriculture and grassland samples result in poorer model performance on these landcover classes with rmses of 22 54 and 23 57 respectively the models tend to over estimate lfmc for grassland bias is 4 96 and under estimate it for agriculture bias is 3 97 due to these samples having lower and higher than average lfmc respectively fig 12 the out of site models estimate lfmc in grassland well with an rmse of 23 9 fig 12 which is encouraging as grassland lfmc is highly variable and difficult to estimate accurately yebra et al 2013 the results for forest shrubland and other are all close to the mean rmse for the proposed out of site model of 25 36 again showing consistency between the forest and shrubland classes however the r2 value for forest is low at 0 37 due to the low variation in the measured lfmc of forest samples across space and time the small number of samples in some land cover classes again results in poor model performance for example the landcover class agriculture for which there are just 1817 samples has the highest rmse 27 33 across all evaluated land cover classes the models under estimate lfmc for the class other the bias is 6 96 but for the other classes the bias is low 1 40 1 34 indicating the models over estimate lfmc about as often as they under estimate it the land cover classes are unevenly distributed across the elevation groups fig 13 with about 40 of forest samples collected from sites above 2000 m a third of shrubland samples from sites between 1500 m and 2000 m and 50 of grassland samples from sites below 500 m less than 4 of grassland samples are from sites above 2000 m so these results may not be reliable due to limited number of samples the sampling rate across the elevations is due to the distribution of vegetation types at different altitudes both the within site and out of site models performed well for grassland at lower elevations while a drop in the performance at elevations of over 500 m is observed due to the smaller number of grassland samples collected at higher elevations similarly both models perform well on both forest and shrubland up to 1000 m but show a decline in performance above that height an exception is the forest class which performs well again above 2000 m likely due to the substantial number of forest samples collected at high altitude the within site models show significant bias for grassland estimations at all elevations apart from 1500 to 2000 m and generally over estimate lfmc while the out of site models show less grassland bias particularly at lower elevations they significantly under estimate lfmc at altitudes above 2000 m grasslands respond differently to environmental drivers at high altitudes than at lower altitudes brut et al 2009 and the models are not able to adequately capture these differences due to the limited number of samples both models tend to over estimate forest lfmc and under estimate shrubland lfmc significant exceptions are observed in the within site models for forest between 1000 and 1500 m and in the out of site models for shrubland between 500 and 1000 m 3 4 2 evaluation of the potential for the models to detect fire risk the forest evaluation samples for the within site models are split roughly evenly between those above and below the forest fire danger threshold of 105 fig 14 a the models correctly identified high or low fire risk in 82 within site models and 73 out of site models of the forest samples however the within site models failed to detect high fire risk in 12 of these samples and falsely estimated high fire risk in 7 of samples similarly the out of site models failed to detect high fire risk in 19 of these samples fig 14d and falsely estimated high fire risk in 8 of samples the rmses for the high fire risk samples 17 14 for the within site models and 20 99 for the out of site models are well below the respective rmses for the full evaluation set 20 87 and 25 36 however the r2 values 0 64 and 0 40 respectively are also below those for the full evaluation sets indicating that homogeneity of these samples is a contributing factor to the low rmse the models have high positive bias for the high fire risk samples 8 16 and 12 70 respectively thus they tend to over estimate the lfmc of these samples the lfmc for most of the grassland samples is above the grassland fire danger threshold as this threshold is relatively low at 67 the models correctly estimated most of these samples as being low fire risk 82 4 for the within site models figs 14b and 85 7 for the out of site models fig 14e and falsely estimated high fire risk in 0 8 1 0 of samples due to the small number of high fire risk grassland training samples neither set of models performed well on the grassland samples below the fire danger threshold they failed to detect high fire risk in 7 8 within site models and 7 5 out of site models of samples which are 46 and 56 of the high fire risk grassland samples respectively the within site rmse for the grassland samples below the fire danger threshold 24 72 is well above the rmse for the full evaluation set 20 87 and the models have high positive bias for these samples 15 25 however the out of site rmse for these samples is 23 87 substantially lower than the rmse for the full evaluation set 25 36 and r2 is 0 80 well above the r2 for the full evaluation set 0 54 the out of site models show a high positive bias of 16 65 for the grassland high fire risk samples the lfmc for most of the shrubland samples are below the shrubland fire danger threshold 121 the models correctly estimated most of these samples as being high fire risk 69 9 for within site models figs 14c and 65 0 for out of site models fig 14f but failed to detect high fire risk in 3 9 and 7 2 of shrubland samples respectively the models performed worse on the low fire risk samples falsely estimating high fire risk on 6 2 and 7 0 of the shrubland samples about a quarter of the low fire risk shrubland samples respectively the rmses for the high fire risk shrubland samples 14 5 for the within site models and 19 36 for the out of site models are well below the rmses for the full evaluation sets the r2 values 0 75 and 0 55 respectively are close to the r2 values for the full evaluation sets while the models show a positive bias when estimating high fire risk shrubland samples 3 32 and 7 71 respectively this bias is lower than for the forest and grassland high fire risk categories 3 4 3 evaluation of the models performance by lfmc ranges the measured lfmc values in the globe lfmc dataset range from 1 0 to 477 0 table 8 the mean value is 109 1 and the median is 102 8 indicating the dataset has a small right skew the globe lfmc samples used to evaluate the within site models those collected between 2014 and 2017 inclusive has a smaller range from 1 0 to 434 5 and lower mean 108 3 the values estimated by the within site models range from 35 7 to 298 8 while the out of site model estimated a slightly narrower range from 48 4 to 263 6 the median and mean of the lfmc estimates from both models are close to the measured lfmc values neither the within site nor out of site models estimate low lfmc values well due to the small number of globe lfmc samples with lfmc below 50 it should also be noted that the models estimate lfmc at pixel not sample level and samples with extremely low lfmc values are generally dead fuels chuvieco et al 2004 yebra et al 2019 and so are unlikely to be representative of the vegetation across the entire pixel although neither model can accurately estimate the highest lfmc values they are able to distinguish between vegetation conditions that are and are not of concern for fire danger assessment model accuracy varies depending on the range of measured lfmc to be estimated fig 15 a and b as shown by the bias the mean estimation error of the samples in the bin both within site and out of site models tend to over estimate low lfmc values and under estimate high lfmc values switching from over estimating to under estimating at about 115 120 the standard deviation of the estimation error for samples in the bin stays below 20 for the range 40 140 then increases as lfmc increases the rmse at both extremes is high probably due to the small number of samples fig 15c and d but stays below the rmse for the full evaluation set in the critical 50 120 range one possible reason poor results are achieved at the extremities is the small number of samples in these lfmc ranges to understand how sample size affects the results a comparison was made between the number of samples in each 1 lfmc range and the accuracy of the estimations fig 16 the plots show an inverse log log relationship between the two variables which may indicate the minimum number of samples needed at each lfmc level to obtain a particular accuracy 3 5 spatial evaluation of the multi tempcnn models 3 5 1 multi tempcnn performance across the climate zones the within site and out of site models both show significant differences in performance across the climate zones fig 17 however the climate zones for which both the best bwh and worst dsc results were obtained have very few samples fig 17a and d consequently these results should be treated with caution of the climate zones with over one thousand samples both models did well for csa achieving rmses of 15 74 and 19 60 and r2 values of 0 83 and 0 74 for the within site and out of site models respectively rmse for bwk arid cold deserts was well above the model averages with respective rmses of 24 75 and 29 86 probably due to the sparsity of vegetation in desert regions however the r2 values were also high at 0 76 and 0 61 respectively the within site models also did well for csb temperate regions with dry warm summers with an rmse of 17 97 and r2 of 0 77 while the rmses for the remaining climate zones were about the model average of 20 87 the out of site models performed well for the cfa rmse is 22 23 and r2 is 0 49 and dfb rmse is 22 68 and r2 is 0 36 climate zones but not so well for the csb dfc cold with no dry season and cold summers and dsb cold with dry warm summers climate zones with rmses of 30 86 30 27 and 28 52 and r2 values of 0 37 0 24 and 0 35 respectively the good result for csa one of the predominant climate zones on the west coast including wildfire prone california is particularly pleasing as the vegetation samples from this climate zone are drier than those from other climate zones fig 17a and d and therefore are of major bushfire concern generally the models show greater error on the wetter climate zones in line with the finding that the estimation error increases with higher lfmc values the poor out of site performance on csb is of some concern given that it is one of the drier climate zones and is the other predominant one in california 3 5 2 multi tempcnn performance by site locations results for both models show consistent performance across the conus with just a few grid cells standing out as performing poorly fig 18 the median grid cell rmse for the within site models is 18 40 with 57 of the cells having an rmse less than 20 fig 18a as expected from the overall results the out of site model performance was slightly worse with the median rmse being 22 94 and only achieving an rmse of less than 20 on 38 of the cells fig 18b overall performance in the eastern conus where there are few sites and samples appears similar to that in the western conus with many sites and samples however better results were obtained for the cluster of sites in michigan mi than the cluster in virginia va climate zones are a likely contributing factor here as mi is under the dfb climate zone which is well represented in the globe lfmc database whereas the database contains very few samples for the dfa climate of virginia when comparing the spatial distribution results of the multi tempcnn models to modis tempcnn some changes were observed the modis tempcnn models under perform in the rocky mountain region in northern idaho and montana and the coastal plains region in southern texas zhu et al 2021 while both the multi tempcnn models still under performed in these areas marked with circles in fig 18 there was a marked increase in performance in southern texas with the within site model rmse being 51 63 compared to 89 33 for modis tempcnn the within site models showed less performance improvement in the rocky mountain location with an rmse of 38 13 for the within site model compared to 43 99 for modis tempcnn 3 5 3 lfmc maps the map for the out of site multi tempcnn model fig 19 a shows low lfmc the mean estimated lfmc is 93 across the western conus except for the pacific northwest and the northern rocky mountains where the mean lfmc is estimated to be 116 very dry vegetation can be seen in the deserts of southern california ca and arizona az and western texas with a mean estimated lfmc of 63 the eastern conus has wetter vegetation than in the west where the mean lfmc is 119 although dry vegetation can be seen in the midwestern corn belt the region from kansas ks and nebraska ne through to ohio oh which has a mean lfmc of 103 there is a clear transition to high lfmc values mean is 132 in the large cfa climate zone that covers most of the south east conus which is consistent with the high average precipitation in this region konrad and fuhrmann 2013 the uncertainty of the lfmc estimations measured as the standard deviation of the individual estimations made by the ensemble members of the multi tempcnn out of site model ranges from 2 3 to 89 8 and is below 10 for 68 of the conus fig 19c the regions of greatest certainty are the states of colorado co wyoming wy and montana these regions are predominantly the bsk climate zone which is the climate zone with the largest number of samples in the globe lfmc dataset greatest uncertainty can be seen in southern florida which has a tropical climate that was unrepresented in the globe lfmc dataset and some of the desert regions of california utah ut and arizona the multi tempcnn out of site model generally estimates similar or higher lfmc than modis tempcnn fig 19b and d with a mean estimated lfmc of 111 an increase of 3 over the mean estimate of 108 for modis tempcnn the multi tempcnn lfmc estimates are much higher than modis tempcnn estimates in the coastal regions of oregon and washington increasing by 40 from a mean estimated lmfc of 99 for modis tempcnn to 139 for multi tempcnn southern texas an increase of 43 from 115 to 158 and florida an increase of 39 from 126 to 165 the main regions where multi tempcnn estimates lower lfmc are the afore mentioned deserts a decrease in mean estimated lfmc of 20 from 83 for modis tempcnn to 63 for multi tempcnn and midwestern corn belt a decrease of 13 from 116 to 103 4 discussion this study has proposed multi tempcnn a new deep learning architecture for large scale lfmc estimation multi tempcnn is based on the modis tempcnn architecture zhu et al 2021 but includes new sources of explanatory data and architectural changes that improve the accuracy of the lfmc estimates the two new data sources are meteorological data in the form of a year long time series and the k√∂ppen geiger climate zone auxiliary variable architectural refinements improve both computational performance and estimation accuracy and ensembling is used to further improve estimation accuracy and provide a measure of the estimation uncertainty the models built using the multi tempcnn architecture were designed for use in two scenarios in the first scenario the within site scenario the model is trained using historical field lfmc samples for a set of sites and applied to estimating lfmc at these sites at later dates in the second scenario the out of site scenario the model is trained using contemporaneous and historical lfmc samples for a set of sites and applied to estimating lfmc at other sites the study results showed the benefits of tailoring the architecture for each scenario the out of site scenario benefitted from a small shallow architecture which encourages the training process to extract simple generalisable features the out of site models achieved an rmse of 25 36 and r2 of 0 54 better results for the within site scenario were obtained using a larger deeper architecture which allowed the discovery of more complex site specific features the within site models achieved an rmse of 20 87 and r2 of 0 70 the architecture on which multi tempcnn is based modis tempcnn is a light weight architecture and with the globe lfmc data can be trained on readily available hardware as specified in section 2 5 the individual component models used to form the multi tempcnn models take three to 10 min to train compared to the 20 min required to train the modis tempcnn model taking ensembling into account the overall models are computationally more expensive than modis tempcnn which is not an ensemble requiring a three to ten fold increase in the training time for an ensemble of twenty models this is offset by the improvements in results leading to improvements in rmse of between 1 4 and 1 8 and an improvement in r2 of 0 06 additionally the variations in the individual estimations made by the ensemble members can be used to estimate uncertainty windows for the lfmc estimations providing fire management agencies with additional information critical for assessing fire risk yebra et al 2013 large ensembles provide increased confidence in the estimations and more accurate assessment of the uncertainty windows however the study results show only minor improvements in both rmse and r2 once the ensemble size is larger than 10 15 component models adjusting the ensemble size therefore allows trade offs to be made between model training time and the required level of confidence in the quality of the estimations considering the new data sources the meteorological data was important in the out of site models accounting for about half the improvement in this scenario it was less important in the within site models providing only a small benefit here the climate zone data had the opposite effect it was of most benefit to the within site models and made almost no difference to the out of site models this study used the same auxiliary inputs for each scenario for consistency allowing analysis of the effects of tailoring the architecture to the scenarios however the results presented in fig 10 show there is a case for also tailoring the inputs to each scenario if the main use of a model is to estimate lfmc at sites without any historical lfmc measurements then based on our results we suggest omitting at least the topographical variables the architectural changes and additional inputs resulted in an improvement in rmse of 2 3 3 5 and of 0 09 0 12 in r2 over modis tempcnn when trained and tested on a dataset spanning the conus the multi tempcnn models are also less biased with biases of 0 22 0 59 compared to 3 84 3 83 for modis tempcnn considering the two problematic areas identified by zhu et al 2021 the models showed improved results for the southern texas coastal plains but only a slight improvement for the rocky mountain regions of idaho and montana while both the overall results and the result for southern texas show that climatic differences are a contributing factor there is scope to further improve the models identifying additional data sources to incorporate into the models such as information about climate anomalies wind speed anomalies and root zone soil moisture is therefore a promising future direction the multi tempcnn models do not require prior knowledge of the land cover as this information is incorporated into features extracted by the trained models from the reflectance data zhu et al 2021 this is an advantage given misclassifications in land cover classification used in other lfmc models can be propagated to model estimates yebra et al 2013 evaluation across the main land cover types showed both the within site and out of site models achieved similar rmses for the forest and shrubland regions the within site rmse for grassland was slightly worse than for the other land cover types but the out of site models performed better achieving similar rmse and r2 to the easier within site scenario this result is especially pleasing as 1 there are fewer grassland samples in the training set and 2 grassland lfmc shows high temporal variability yebra et al 2013 thus models typically present a higher rmse for grasslands than for forests or shrublands garc√≠a et al 2008 yebra et al 2018 the models performances vary with the measured level of lfmc but show better than average results for the lfmc values that have been associated with increased wildfire risk in previous studies outside the range of 50 200 model performance deteriorates rapidly the models over estimate lfmc in extremely dry conditions and under estimate it in wet conditions this is consistent with the findings from other studies danson and bowyer 2004 zhu et al 2021 while other studies have shown optical data has limited sensitivity to wet vegetation the analysis in this study shows an inverse relationship between the sampling density across lfmc ranges and model performance this relationship can potentially be used to provide useful information about how sampling size affects model accuracy however further investigation is needed to control for possible confounding effects the proposed architecture is flexible allowing easy incorporation of other time series datasets by expansion of the convolutional layers while the study has only considered time series of the same length and intervals this is not a requirement of the architecture as the inputs are processed individually in the convolutional layers and only combined for the final fully connected layers the architecture therefore facilitates the integration of remote sensing products with differing temporal resolution such as the higher resolution sentinel 1 microwave synthetic aperture radar data with a 6 day revisit time and sentinel 2 optical data with a 5 day revisit time data currently the main obstacle to using these products is the lack of sufficiently recent samples in the globe lfmc dataset however once sufficient data is available using these data sources as additional or alternative inputs would allow generation of higher spatial resolution lfmc maps allowing more precise modelling of vegetation conditions 5 conclusions this study has presented a multi modal temporal cnn for lfmc estimation this new model learns from year long time series of meteorological and reflectance data plus a few auxiliary inputs including the climate zone ensembles of twenty models are used to achieve state of the art accuracy and assess the uncertainty of the lfmc estimations providing additional information useful for fire risk assessment in an operational scenario while the models performed well when estimating lfmc at fire danger levels further improvements in both the model accuracy and uncertainty estimates are needed to improve fire management applications the curators of the globe lfmc database plan to publish updates as new lfmc sample data becomes available yebra et al 2019 therefore future work includes incorporating new lfmc samples in the multi tempcnn models as they become available to keep the models up to date which should further improve accuracy and facilitate using the more recent and higher resolution remote sensing data sources previously discussed the models were trained and evaluated using data collected mainly from the western conus however the consistent results from the smaller number of samples from the eastern states showed models can make reasonable estimations here especially for regions that have a similar climate to well represented western regions more challenging is the application of models to fire prone regions outside the united states where the availability of lfmc field measurements is limited and environmental drivers may differ future work will investigate applying spatial domain adaptation techniques for remote sensing data such as sourcerer lucas et al 2021 to the multi tempcnn models and assess their use for estimating lfmc in other regions of the world funding this work was supported by an australian government research training program rtp scholarship and the australian research council under award dp210100072 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the diagrams of the multi tempcnn models shown in fig 4 were created using alexander lenail s nn svg webpage https alexlenail me nn svg index html 
