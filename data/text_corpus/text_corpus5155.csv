index,text
25775,in free surface turbulent flows large amount of air may be entrapped and advected in the water current the resulting air water flows are frequently observed in natural water systems where they are also relevant to water quality ecological sustainability and integrated assessment within such systems herein a review of physical and numerical modelling of air water flows is developed providing some fundamentals towards a consistent modelling of such flows to graduate and ph d level students as well as young researchers in environmental sciences and engineering with pre requisite knowledge in basic fluid mechanics after some theoretical and metrology considerations the main criteria for the design of physical models and the current literature on the numerical studies are discussed two case studies the hydraulic jump and the dropshaft are used to show the application of such criteria and methods overall the paper presents current knowledges challenges on physical and numerical modelling of self aerated free surface flows keywords air water flows introductory overview physical modelling numerical modelling metrology model validation 1 introduction in high velocity free surface turbulent flows large quantities of air bubbles pockets move across the air water interface being entrapped air entrainment in the water current and then are carried away within the flowing fluid and eventually exchanged back to the air flowing above the free surface halbronn et al 1953 jevdjevich and levin 1953 the resulting air water flow or self aerated flow is a mixture of air and water consisting of both air packets within water and water droplets surrounded by air chanson 1997 aerated flows are encountered in a wide range of applications in chemical civil environmental mechanical mining nuclear and water engineering rao and kobus 1974 fig 1 they span from small scale to very large scale in water and environmental systems self aerated flows are often observed in mountain streams storm waterways culverts dropshafts spillway chutes tidal channels and stilling basins where aeration is largely un controlled wood 1991 and such flows are also relevant to the water quality sediment transport ecological sustainability and ultimately environmental integrated assessment within such systems depending upon the application air entrainment should be maximised minimised or prevented wood 1985 the exchange of air across the air water interface is driven by the turbulence next to the air water interface the free surface breakup and air entrainment occur when the turbulent shear stress is greater than the surface tension force per unit area resisting the interfacial breakup hino 1961 ervine and falvey 1987 once some air is entrained within the bulk of the flow the break up of air pockets occurs when the tangential shear stress is greater than the capillary force per unit area chanson 2009 as bubbles and droplets are advected by the flow particle collisions may lead to their coalescence while air detrainment due to buoyancy also takes place the complex interactions between the entrained air and turbulence may produce some bubble clustering a cluster of bubbles is defined as a group of two or more bubbles with a distinct separation from the other bubbles before and after the cluster past studies demonstrated that a clustering analysis may provide some relevant insights about the interaction between turbulence and bubbly flow gualtieri and chanson 2010 2013 wang et al 2015a traditionally aerated flows are experimentally investigated through physical modelling but more recently numerical studies have been carried out physical models for such experimental studies must be designed on a sound similitude otherwise scale effects may affect the extrapolation of experimental results to full scale prototype structures kobus 1984 aerated flows are commonly studied using a froude similitude but in a geometrically similar model the dynamic similitude involves other dimensionless parameters shear flows are dominated by viscous effects while the mechanisms of bubble breakup and coalescence are controlled by surface tension forces chanson and gualtieri 2008 hence dynamic similitude in aerated flow requires that froude reynolds and morton numbers should be identical in both the prototype and its model but this is impossible to achieve using geometrically similar models unless working at the full scale using the same fluids i e air and water in the prototype and in laboratory a froude and morton similitude can be implemented but the model reynolds number cannot be as large as in the prototype leading to viscous scale effects in small size models rao and kobus 1974 wood 1985 chanson 2009 more recently computational fluid dynamics cfd methods have been applied to improve the current knowledge about self aerated flows such methods were developed over 50 years ago by engineers and mathematicians to solve flow problems in the area of industrial engineering and their application was later extended to many areas of fluid dynamics such as environmental fluid mechanics and water engineering including aerated flows cushman roisin et al 2012 rodi 2017 the main advantages of such methods are that they allow full control over the boundary conditions that they provide data in every point of the computational domain simultaneously and they might be performed at full scale albeit at a computational cost cfd also allows efficient parametric analyses of different configurations and for different flows and environmental conditions on the other side cfd methods are generally affected by uncertainties about input parameters that should be carefully considered blocken and gualtieri 2012 in self aerated flows further sources of uncertainty are related to the stability and sharpness of interfaces in schemes where interfaces are explicitly modelled and in other schemes to the representation of forces between the phases such as the lift and drag exerted by particles and to mesh refinement being limited by the particle size bombardelli 2012 viti et al 2018 hence cfd studies must be carried out following strict guidelines rizzi and vos 1998 roache 1998 2008 asme 2009 otherwise their accuracy and reliability and the correct use of their results can easily be compromised furthermore the results of numerical studies require to be validated using high quality experimental data collected in physical models for aerated flows several cfd techniques were applied but in many cases the validation of cfd simulations were conducted in terms of flow depth possibly time averaged velocity rarely including any comparison of void fraction distributions and interfacial properties chanson and lubin 2010 viti et al 2018 while to get a comprehensive validation turbulent microscopic flow quantities should also be considered this paper presents a review at the introductory level of physical and numerical modelling of aerated flows the review is intended to provide fundamentals towards a consistent modelling of such flows to graduate and ph d level students as well as young researchers in environmental sciences and engineering with pre requisite knowledge in basic fluid mechanics first after some theoretical considerations including the metrology of air water flows the main criteria for the design of physical models are discussed then the most widely used methods for the numerical simulation of aerated flows are presented and the current literature is reviewed pointing out the need for a proper validation of any numerical study two case studies the hydraulic jump and the dropshaft which probably received the largest and the lower attention within the literature respectively are used to provide guidelines for the application of such criteria and methods finally current challenges and future outlook on self aerated free surface flows are discussed 2 basic considerations 2 1 presentation incompressible turbulent flows are governed by the equations of conservation of mass and momentum these laws are represented through the navier stokes equations which in their original form encompass all known internal and external effects of the motion of a fluid unlike single phase turbulence where even simple reynolds closure models have proven some usefulness simple models have failed by and large in the case of multiphase gas liquid flows elgobashi 1991 balachandar and eaton 2010 the complexity of the two phase flow motion may be illustrated by the equations of fluid motion governing the multiphase gas liquid flows at the micro scale combined with some interface tracking tryggvason et al 2011 bombardelli 2012 chanson 2013 1a ρ w t i x y z ρ w v w i x i 0 w a t e r 1b ρ a t i x y z ρ a v a i x i 0 a i r 2a ρ w v w i t j x y z ρ w v w i v w j x j p w x i ρ w g i j x y z τ w i j x j w a t e r 2b ρ a v a i t j x y z ρ a v a i v a j x j p a x i ρ a g i j x y z τ a i j x j a i r where the subscripts a and w refer to the air and water properties respectively v is the instantaneous velocity component p is the instantaneous pressure gi is the gravity acceleration in the direction i x y z and τij denotes an instantaneous shear stress tensor component equations 1a and 2a for the water and equations 1b and 2b for the air phase must be complemented by a mathematical representation of the moving interface and the associated conditions to couple the equations across the air water interfaces this formulation may be used for detailed direct numerical simulations although the application is very complicated prosperetti and tryggvason 2009 tryggvason et al 2011 mortazavi et al 2016 the interface tracking the coupling of equations at the air water interfaces and the correct implementation of the boundary conditions are not as easy as it may sound in engineering applications bombardelli 2012 water crashing in high velocity free surface flows can be an extremely violent motion with much spray and splashing and complicated interfacial processes fig 1 the generation of bubble clouds and water droplets has been proved to induce energy dissipation and turbulent mixing to contribute to heat exchange and to enhance gas transfer gualtieri and gualtieri 2004 wanninkhof et al 2009 veron 2015 most recent modelling attempts are struggling with the lack of physical knowledge and conceptual model of the finest details of the breaking processes which makes the task of parameterising the impact of self aeration in steady and unsteady flows very difficult since no universal scaling laws for physical variables have been found so far lubin and glockner 2015 lubin and chanson 2017 newer models still need validation and further improvements 2 2 onset of air entrainment the onset of self aeration may be defined as the threshold condition for air entrainment to take place despite some fine distinction between the first single bubble entrainment and the onset of continuous bubble entrainment e g cummings and chanson 1999 chanson and manasseh 2003 these two flow conditions usually cover a narrow range broadly called the onset of air entrainment many early studies of air water flows expressed the onset condition as a function of a time averaged inflow velocity v1 i e with air entrainment in turbulent water flows occurring for v1 0 5 2 m s rao and kobus 1974 ervine et al 1980 some seminal studies related self aeration to the inflow turbulence hino 1961 mckeogh and ervine 1981 more recently detailed experimental studies showed a characteristic trend with onset conditions about v1 2 m s in smooth turbulent flows and v1 0 8 m s in rough turbulent flows cummings and chanson 1999 kiger and duncan 2012 nowadays it is understood that air bubble entrainment occurs when the turbulent shear stress next to the free surface interface is large enough to overcome the surface tension ervine and falvey 1987 chanson 2009 that is the onset of air bubble entrainment is linked to a characteristic threshold in terms of reynolds stresses in the liquid phase next to the air water interface for a spherical bubble of radius r and isotropic turbulence chanson et al 2021 derived an analytical expression of the onset condition 3 μ w v σ μ w 2 2 ρ w σ π r v v v 2 where ρw is the water density μw is the water dynamic viscosity σ is the surface tension v is the longitudinal water velocity v is the instantaneous turbulent velocity fluctuation despite the simplified development spherical bubble isotropic turbulence eq 3 predicts entrained bubble sizes consistent with experimental data in vertical plunging jets chanson et al 2021 fig 2 2 3 advection and turbulent diffusion of air bubbles once entrapped the entrained air is carried away with the flowing fluid and its motion in within the water column is controlled by the complex interaction among advection turbulent diffusion and mixing and upward buoyancy furthermore as bubbles and droplets are transported with the flow turbulent shear may induce breakup and formation of smaller daughter particles while particles collisions may lead to their coalescence various diffusion models were developed and applied successfully to a range of air water flow typology and flow conditions chanson 1997 in the hydraulic jump the transport of air bubbles downstream the jump toe can be modelled using the classical advection diffusion equation wood 1984 where advection is along the horizontal direction and the diffusion process occurs only in the vertical direction chanson 1995 2010 gualtieri and chanson 2007 in the dropshaft the longitudinal distribution of air bubbles around the underwater jet trajectory follows the diffusion equation gualtieri and chanson 2004 the same type of equations can be applied even in other air water flows such as on steep chutes and in vertical plunging jets chanson 2012 2 4 air water flow measurements air water flows are usually investigated in the flow region where the air concentration is less than 95 and the two phases move with a nearly identical velocity chanson 1997 for their description in comparison to a single phase flow aerated flows require a number of additional parameters such as the air concentration or void fraction the bubble count rate the bubble and drop size distributions the properties of clusters of air bubbles further if void fraction exceeds 5 some classical parameters of a turbulent flow e g the instantaneous velocity cannot be measured with traditional instruments such as pitot tube acoustic doppler velocimetry adv laser doppler velocimetry lda because air bubbles and air water interfaces affect adversely their operation since the 1950s some specialised instrumentations such as back flushing pitot tubes needle phase detection probes conical hot film probes and fibre phase doppler anemometry were developed for the measurement of aerated flows however their application was limited by calibration issues and hence the most widely applied device in the last 40 years has been the phase detection needle shaped probe or conductivity probe which is designed to pierce the bubbles and droplets fig 3 later with the development of image based velocimetry particle image velocimetry piv due to the advancement in computational power bubbles were used as tracer particles under ordinary lighting conditions to identify velocity in aerated flows in a method called bubble image velocimetry biv more recently in the last decade two major developments in air water velocity measurements have been the total pressure probe and the optical flow of metrology the needle probe is an intrusive phase detection probe used to discriminate between air and water phases using the different conductivity of air and water the signal output quality of this probe is closely linked to the sensor size with the needle diameter the sampling rate f sampl and sampling duration t sampl sensor sizes in less than 0 1 mm are used at low flow velocities v 1 2 m s while for higher velocity flows more sturdy probes with diameters typically between 0 1 and 0 5 mm are required with a needle probe the selection of the sampling frequency is linked to the smallest detectable bubble size which is of the order of magnitude of the needle diameter generally the sampling rate should be greater than 10 khz and the sampling duration larger than 20 s to have negligible effect on the void fraction bubble count rate and air water velocity measurements while more advanced correlation analyses including the estimate of the turbulence intensity require a sampling duration of 45 s or larger the phase detection probe could have a single tip or a dual tip design fig 3b where the latter provides additional information on the interfacial velocity and turbulence level bubble image velocimetry biv relies upon interrogation of an image frame pair by computing the spatial cross correlation ryu et al 2005 however due to its discrete data nature for certain tracer size range the method may cause displacement vectors to be biased towards integer pixel values commonly referred to as pixel locking chen and katz 2005 direct computation of the correlation surface is expensive and any velocity or seeding gradient in the interrogation region especially a large region introduces a bias towards smaller displacement another major limitation is the bias of the sidewall flow conditions where boundary friction cannot be neglected biv velocity data typically underestimates the velocity field on the channel centreline which is significantly larger than the near wall velocities when measured by an intrusive probe zhang and chanson 2018 total pressure measurements with miniature diaphragm sensor can deliver a fine characterization of the velocity and turbulence in the water phase when accounting for the local void fraction wang et al 2015b zhang et al 2016 the optical flow approach is based upon the detection of changes in brightness due to reflectance difference associated with passages of air water interfaces bung and valero 2016a zhang and chanson 2018 fig 6 bung and valero 2016b compared biv and optical flow estimates in seeded and aerated flows they found comparable accuracies for both methods with the optical flow technique providing higher resolution data albeit requiring a much longer computation time some key limitations of all optical techniques are the requirements for two dimensional flows the use of high speed high resolution video camera and the adverse impact of sidewall effects bung and valero 2016a zhang and chanson 2018 3 similitude and physical modelling any modelling investigation is expected to deliver a sound prediction of the hydrodynamic characteristics of the flow motion in a full scale prototype operation with a few examples illustrated in fig 1 henderson 1966 hamill 1995 chanson 2004a the modelling approach must be developed based upon the basic principles of similitude to deliver reliable extrapolations rayleigh 1915 the presentation of any modelling data has to be relevant to the full scale prototype applications and dimensional analysis is the underlying method to deliver the most relevant design parameters bertrand 1878 rouse 1938 liggett 1994 in air water flows an early study highlighted that model tests provide little help due to our ignorance of the laws of hydrodynamic similarity of aerated flow jevdjevich and levin 1953 p 439 while a more recent review paper emphasized that the results of experimental investigations demonstrated unequivocally the limitations of dynamic similarity and physical modelling of aerated flows chanson 2013 p 229 yet physical modelling and laboratory experiments remain essential tools to validate phenomenological theoretical and numerical models hanratty et al 2003 in a study of self aerated free surface flows the relevant dimensional parameters include the air and water properties physical properties channel dimensions and inflow conditions for a chute flow e g figs 1c 2 a simple dimensional analysis yield 4 c v v f n c l t t t f 1 x y z ρ w μ w σ g q b k s θ with c the void fraction v the interfacial velocity v a characteristic velocity fluctuation f the level of flow fragmentation nc the cluster rate lt and tt some characteristics turbulence length and time scale x y z being respectively the longitudinal normal and transverse coordinates ρw the water density μw the dynamic viscosity of water σ the surface tension between air and water g the gravity acceleration q the unit discharge b the channel width ks is the equivalent sand roughness height of the invert surface and θ the chute slope the buckingham π theorem 1 1 the buckingham π theorem is also called vaschy buckingham theorem after the french engineer aimé vaschy 1857 1899 and american physicist edgar buckingham 1867 1940 states that any dimensional equation with n variables with units encompassing mass length and time mlt may be rewritten into an equation with n 3 dimensionless parameters vaschy 1892 buckingham 1914 rouse 1938 thus equation 4 may be transformed as 5 c v v c v v c f d c v c n c d c v c l t d c t t v c d c f 2 x d c y d c z d c d c k s ρ v d h μ g μ 4 ρ σ 3 b d c θ with dc and vc the critical flow depth and velocity respectively and dh the hydraulic diameter equation 5 expresses the local dimensionless air water flow properties at a location x y z as functions of a number of dimensionless parameters including the froude number 4th term on the right handside reynolds number 5th term and morton number 6th term a laboratory study is typically undertaken using geometrically similar models in the physical model the air water flow properties must display similarity of form of motion and of forces novak and cabelka 1984 chanson 2004a if this is not achieved scale effects occur in relation to the parameter s of interest and the extrapolation of the model data will not accurately predict the full scale prototype performances considering a high velocity self aerated flow in a rectangular channel figs 2 5 the present analysis illustrated the large number of relevant parameters any true similarity would require identical dimensionless variables including froude reynolds and morton numbers in both laboratory and full scale prototype this situation is physically impossible because of the large number of independent parameters eq 5 past experiences showed that small laboratory experiments drastically under represented the air entrainment kobus 1984 chanson 1997 2009 figs 4 and 5 illustrates the air entrainment in two types of self aerated flows each figure presents photographs of the flow at an identical froude and morton number but different reynolds numbers for scale the inflow depth was 0 097 m 0 045 m and 0 027 m in fig 4a b and 4c respectively in fig 5a the shaft was 0 755 m long and 0 763 m wide while the shaft was 0 243 m long and 0 246 m wide in fig 5b both examples emphasized the scale effects in small sized laboratory facilities operating at relatively small reynolds numbers in practice the laboratory experiments must be conducted in a large size facility operating at relatively large reynolds numbers typically re 2 105 to 5 105 4 numerical modelling numerical methods are increasingly applied to improve the current knowledge about self aerated flows despite their several advantages the accuracy and reliability of the numerical approach are still of concern and verification and validation studies are limited it is widely recognized that the results of cfd simulations can be very sensitive to the wide range of computational parameters that have to be set by the user the set up of any numerical study is associated to uncertainties about geometry and boundary conditions drag coefficients driving forces and the interactions among different processes and inputs this is of paramount importance in the numerical simulation of self aerated flows which involves fluctuating boundaries as well as a multiphase flow while cfd methods were previously applied to self aerated flows to supplement the use of physical models bombardelli 2012 and address the intrinsic limitations of experimental measurements the development of advanced theoretical models for turbulence and two phase flows led such numerical methods to be often applied together with experimental methods to tackle and interpret self aerated flows in the last decade a flow field may be described following two approaches eulerian methods and lagrangian methods the first approach studies flow properties in a number of fixed points this corresponds to a coordinate system fixed in space where fluid properties are studied as functions of time as the flow passes the latter follows the motion of each individual fluid parcel as it moves from some initial location this corresponds to a coordinate system on each fluid parcel past numerical simulations of aerated flows encompass both eulerian such as reynolds averaged navier stokes rans detached eddy simulation des large eddy simulation les and direct numerical simulation dns and lagrangian such as smoothed particle hydrodynamics sph methods bombardelli 2012 rodi et al 2013 violeau and rogers 2016 viti et al 2018 the most widely applied approach to simulate a turbulent flow is that based on the time averaging even termed reynolds averaging of the navier stokes equations where the instantaneous values of velocity and pressure is assumed to be the sum of a time averaged value and a fluctuating component this statistical approach leads to the reynolds averaged navier stokes rans equations kundu et al 2012 where the averaging of non linear advective terms results in unknown correlations between fluctuating velocities these additional unknowns introduce the need for a closure of the rans equations such correlations are usually seen as stresses termed reynolds stresses additional to those due to fluid viscosity following the boussinesq hypothesis reynolds stresses are treated using an eddy or turbulent viscosity and the spatial gradient of the time averaged velocities kundu et al 2012 such eddy viscosity ultimately means that the effect of turbulence is to act on the mean flow as an increased viscosity different estimations for the eddy viscosity have been proposed dimensional reasoning suggests that the eddy viscosity can be obtained as a product of a turbulent velocity scale by a turbulent length scale different approaches can be used to derive these scales at the simplest level of complexity one may expect that the eddy viscosity would be determined by large scale eddies the size of which is close to the characteristic dimension and velocity of the flow itself thus eddy viscosity would be linked to the overall velocity gradient as in the mixing length model kundu et al 2012 alternatively an obvious choice for defining a turbulent velocity scale is the turbulent kinetic energy k while many variants were proposed in the literature to define a turbulent length scale leading to different families of two equations turbulence models such as the k ε model and the k ω turbulence model where the rate of turbulent energy dissipation ε and the specific dissipation rate ω ε k respectively are used to get a turbulent length scale each of these families has different derived models characterised from different equations and numerical constants rans approach has been frequently applied in aerated flows such as the hydraulic jump chippada et al 1994 gonzalez and bombardelli 2005 bayon et al 2016 valero et al 2018 witt et al 2018 macián pérez et al 2020 smooth and stepped spillways meireles et al 2014 toro et al 2016 lopes et al 2017 valero et al 2018 chutes hohermuth et al 2020 dropshafts sousa et al 2009 and plunge pools carrillo et al 2020 often in such studies an additional method is required to track the free surface i e volume of fluid vof by hirt and nichols 2008 while the large scale of the turbulent spectrum produced by mean flow is long living energetic diffusive inhomogeneous anisotropic and depending on domain geometry and boundaries the small scale produced by large eddies is short living no energetic dissipative universal random homogeneous isotropic and can be modelled statistically rodi et al 2013 this fundamental difference has led to conceptualize the large eddy simulation les approach where the large scale of turbulence is resolved while the small scale is modelled rodi et al 2013 the main difference between rans and les approaches is that the navier stokes equations are in the former time averaged and in the latter space filtered furthermore the cut off below which a model is used is a frequency domain cut off in les whereas in rans it is a physical domain cut off in spite of that both sets of equations get a similar form because a stress tensor is created by the time averaging and filtering processes however in les differently from rans models these stresses called sub grid scale stresses pertain only to the turbulent spectrum that is not solved but modelled using different methods rodi et al 2013 les is more computationally demanding that rans but the continuous development of computational power has led les approach to be increasingly applied to water engineering and environmental hydraulics including to self aerated flows such as the hydraulic jump gonzalez and bombardelli 2005 lubin et al 2009 and tidal bores lubin et al 2010a 2010b leng et al 2018a b to combine the advantages of rans and les minimising their limitations hybrid les rans approaches such as detached eddy simulation des were proposed and even applied to aerated flows but to the hydraulic jump only ma et al 2011 jesudhas et al 2018 2020 basically in the des rans and les methods are used in the near wall region and in the free stream region respectively rodi 2017 in the direct numerical simulation dns the unsteady 3d navier stokes equations are solved directly using spatial and temporal resolutions sufficiently fine to resolve the dynamics of the entire spectrum of turbulent eddies in the flow from the energy producing largest eddies whose size is comparable to the flow domain to the smallest eddies of kolmogorov scale at which turbulence energy is dissipated into heat by molecular action it is easy to identify a critical issue for dns in the extremely large computational power in terms of both the processor s performances and the size of the memory for storing intermediate results required to achieve results for real world problems in a reasonable time it is noteworthy that the computational power needed by dns is proportional to the flow reynolds number re9 4 the contributions of dns to turbulence research in the last few decades have been impressive alfonsi 2011 but its application to water engineering and environmental hydraulics problems which are characterised by large reynolds numbers is still very limited and in the field of self aerated flows only a study on hydraulic jump mortazavi et al 2016 was published so far among the meshless lagrangian techniques smoothed particle hydrodynamics sph which solves flow equations for a set of moving particles with a certain mass has been recently applied to aerated flows such as the hydraulic jump lópez et al 2010 de padova et al 2013 2018 wan et al 2018 smooth and stepped spillways wan et al 2017 nobrega et al 2020 and tidal bores nikeghbali and omidvar 2017 while the use of a kernel function to interpolate flow variables is critical to sph some general advantages of the sph method over the mesh based methods are the effectiveness in solving complex fluid dynamic problems with highly nonlinear deformations and the natural tracking of free surfaces and moving boundaries the analysis of the above mentioned literature shows that the number of numerical studies capable to gain a complete validation of their results is still limited for the hydraulic jump which is the most frequently investigated self aerated flow most rans les sph studies focused on the free surface simulation and on the prediction of the main jump parameters conjugate depth ratio roller length hydraulic jump length and efficiency etc and time averaged velocity while only few studies considered also pressure fluctuations turbulence features and air entrainment quantities void fraction on the other side high fidelity methods such as des and dns were extended to a comprehensive characterization of turbulence including the identification of coherent structures and interface length scales and of air entrainment mortazavi et al 2016 jesudhas et al 2018 2020 fig 6 for smooth and stepped spillways rans studies gained mostly a characterization of average velocity and pressure distribution vorticity turbulent kinetic energy and its dissipation rate meireles et al 2014 toro et al 2016 while sph predicted flow depth and velocity nobrega et al 2020 and also the longitudinal distribution of dissolved oxygen do concentration wan et al 2017 numerical studies on tidal bores both with les and sph focused on the prediction of time variable free surface dynamics and velocity distribution lubin et al 2010a 2010b nikeghbali and omidvar 2017 leng et al 2020 finally while the numerical analysis of a dropshaft was limited to discharge and water depth sousa et al 2009 very recent rans studies on chutes hohermuth et al 2020 and plunge pools carrillo et al 2020 gained the prediction of both the velocity field and of air entrainment including void fraction bubble frequency and sauter bubble diameter in a highly transient flow one application showed that the instantaneous void fraction and bubble distribution data presented systematically a lesser aeration region in the physical model compared to the numerical data leng et al 2018b 5 discussion at the end it is advisable for numerical studies of self aerated flows to get a comprehensive validation across a broad range of relevant air water flow properties with relevant turbulent integral length and time scales in addition to the microscopic flow structure e g clustering interparticle distances in line with the cfd validation requirements set for monophase flows rizzi and vos 1998 roache 1998 2008 asme 2009 blocken and gualtieri 2012 the validation of cfd numerical models is anything but trivial a proper validation necessitates a combined and fundamental understanding of the numerical model and its limitations together with an expert knowledge of the physical model its characteristics and its instrumentation leng et al 2018a such a combined expertise and experience is critical to ensure the suitability of the experimental physical data set for cfd validation most often a proper cfd model validation required a team of experts with physical and numerical experience yet both physical and numerical models are developed to reproduce a full scale three dimensional flow phenomenon for which prototype data are rarely available for the ultimate validation chanson 2013 a recent development has been the hybrid modelling combining laboratory experiments and numerical computational fluid dynamics cfd modelling together fig 7 leng et al 2018b leng and chanson 2020 a major advantage in engineering design is the optimisation of resources combining the flexibility of cfd modelling e g to reduce the costs in building and testing several large size physical models and operating large size laboratory models to produce realistic boundary and initial conditions yielding high quality validation data sets for cfd numerical modelling in turn reducing the total simulation times such a composite approach may include interactions feedbacks and loops between the physical and cfd techniques providing new capabilities to the entire design process 5 2 case studies 5 2 1 the hydraulic jump a hydraulic jump is a sudden transition from a high speed open channel flow into a slow fluvial flow commonly experienced in streams and rivers as well as in man made canals industrial channels and downstream of dam spillways the jump is a seminal fluid flow with extreme turbulence linked to the development of large scale eddies surface waves and spray energy dissipation and air entrainment figs 4 and 8 it is the most largely investigated physically and numerically self aerated flow in a hydraulic jump some air is entrapped at the discontinuity between the impinging flow and roller called jump toe or roller toe rajaratnam 1962 chanson and brattberg 2000 murzyn et al 2005 further air is entrained through the roller free surface wang and chanson 2015 at the jump toe the impingement perimeter acts as a source of vorticity and the developing air water mixing layer is the locus of the advective diffusion of vorticity and entrained air the hydraulic jump is characterised a sudden rise in water levels figs 4 and 8 associated with some discontinuity in terms of the pressure and velocity fields it is a hydrodynamic shock lighthill 1978 liggett 1994 the application of the equations of conservation of mass and momentum in an integral form gives a system of equations linking the one dimensional flow properties upstream and downstream of the jump henderson 1966 chanson 2012a for hydraulic jumps in a smooth horizontal rectangular channel it yields 6 d 2 d 1 1 2 1 8 f r 1 2 1 7 f r 2 f r 1 2 1 8 f r 1 2 1 3 2 where d and v are the flow and depth averaged velocity respectively fig 9 a the subscripts 1 and 2 refer to the upstream and downstream conjugate properties and fr is the froude number defined as fr v g d 1 2 for a rectangular channel equations 6 and 7 highlight the importance of the inflow froude number fr1 and the selection of the froude similitude for any physical modelling derives implicitly from these fundamental theoretical considerations a key feature of the hydraulic jump is the developing shear layer with a recirculation region above figs 8 9a the turbulent shear flow is somehow analogous to a wall jet rajaratnam 1965 chanson and brattberg 2000 while the advection of air bubbles can be modelled by an advection diffusion equation chanson 1995 1997 typical vertical distributions of void fraction c bubble count rate f and longitudinal velocity vx are sketched in fig 9b the shear layer is typically characterised by a local maximum in void fraction cmax which decreases pseudo exponentially with increasing distance from the roller toe as the shear layer expands chanson and brattberg 2000 murzyn et al 2005 similarly some momentum consideration implies that the maximum velocity vmax decays quasi exponentially with longitudinal distance from the roller toe typical experimental observations of cmax and vmax are shown in fig 9c in which they are compared with two robust correlations wang and chanson 2016 chanson 2010 8 c max 0 5 exp 1 1 8 f r 1 1 x x 1 d 1 9 v max v 1 exp 0 028 x x 1 d 1 with x1 the roller toe position the development of large turbulent structures and vortex pairing is conducive of bubble clustering in the turbulent mixing layer chanson 2007a wang et al 2015a figs 3 and 8 while a cluster is a three dimensional air water structure the current metrology is restricted to the detection of longitudinal and transverse clusters experimental data showed a large proportion of entrained bubbles advected in clusters typically mostly encompassing between 2 and 5 bubbles wang et al 2015a during the past two decades hydraulic jumps have been also investigated using numerical methods both lagrangian and eulerian gonzalez and bombardelli 2005 viti et al 2018 most numerical studies have been carried out by rans approach to validate this tool for a set of flow conditions different two equations turbulence models mostly belonging to the k ε family were applied in a range of inflow froude number from 1 5 to 9 5 generally rans simulations yielded relatively accurate results with accuracies over 90 for average flow variables conjugate depth ratio roller length hydraulic jump length and efficiency mean free surface and even in some cases for air entrainment mainly investigated in terms of air concentration void fraction and not considering the distribution of bubbles sizes viti et al 2018 at the end any of the applied two equation models presented could be used for design purposes provided that the related uncertainties are considered in the analysis of the numerical results while meshless sph simulations showed a promising agreement in terms of free surface elevations and velocity profiles high fidelity methods such as les and mostly des and dns provided a comprehensive characterization of turbulence quantities indicating the future area of development of numerical studies on the hydraulic jump mortazavi et al 2016 jesudhas et al 2018 2020 5 2 2 air entrainment in a rectangular dropshaft a dropshaft is a vertical conduit connecting two channels located at different elevations figs 5 and 10 the loss in potential energy acts basically as some energy dissipation in practice there are two common types of dropshaft i e the plunge type and the vortex type the plunge dropshaft design herein investigated has been used for millennia lopez cuervo 1985 chanson 2002a and modern applications encompass sewers storm waterways and even large spillway shafts with morning glory intake however the literature on dropshaft is not large the dropshaft operation may cover several flow regimes depending upon the boundary conditions i e shaft geometry and inflow conditions most frequently flow and air entrainment in a dropshaft are investigated by physical modelling chanson 2002a 2004b 2007b camino et al 2015 ma et al 2016 ding and zhu 2018 while numerical studies are unfortunately still limited sousa et al 2009 let us consider a rectangular dropshaft as illustrated in figs 5a and 10 which is a near full scale facility at low discharges the free falling nappe impacts into the shaft pool regime r1 fig 10a chanson 2004b gualtieri and chanson 2004 while at large flow rates the nappe impacts on the opposite wall regime r3 figs 5a 10b chanson 2007b for a narrow range of intermediate flows the nappe impact may interfere with the shaft outflow conduit regime r2 fig 5b in the regime r1 the air entrainment is primarily a plunging jet action with bubble entrainment occurring at the plunge point at large discharges air entrainment is a combined effect of nappe impact and splashing on the opposite wall and plunging action in the shaft pool fig 10 air entrainment in such dropshaft was experimentally investigated using a sturdy single tip phase detection probe chanson 2002b 2007b gualtieri and chanson 2004 such laboratory experiments showed a strong aeration of the dropshaft pool for all discharges with entrained bubbles mean size between 10 mm and 20 mm chanson 2007b maximum void fraction was located close to the theoretical trajectory of underwater jet fig 11 a where turbulent shear was the largest but bubble coalescence and detrainment processes reduced the percentage of the smaller air bubbles along such trajectory furthermore a decreasing number of air bubbles could penetrate into the pool at increasing depths void fraction data were found to obey to an analytical solution of diffusion equation for air bubbles whose distribution was skewed and followed reasonably well a log normal probability distribution function chanson 2002b 2007b gualtieri and chanson 2004 bubble clustering was further investigated in such dropshaft aerated flow to better characterise the interactions between bubbles and large scale vortices bubble clusters can be identified by analysing the water chord between two adjacent air particles or the interparticle arrival times iats τ ia for the air bubbles while the former method provides only some general features of the clustering process such as the number of clusters of clustered bubbles and of bubbles belonging to cluster structures in each point of measurement the iat analysis allows also to identify the range of particle sizes affected by clustering and ultimately the structure of each cluster and of the bubbly flow both methods demonstrated the relevance of clustering process in the dropshaft flow clustering was the largest close to the plunge point in the pool and along the theoretical trajectory of underwater jet with some decaying pattern with the depth gualtieri and chanson 2011 2013 the iat data demonstrated that for a similar level of turbulence the bubbly flow structure in the dropshaft had a density of bubbles per unit flux larger than in the hydraulic jump flow fig 11b suggesting a stronger level of interaction between air bubbles and turbulent flow in the dropshaft gualtieri and chanson 2013 as already noted numerical studies of the above presented rectangular dropshaft are mostly limited to that of sousa et al 2009 sousa and co workers investigated the hydraulic operation in that dropshaft using both standard k ε and rng k ε turbulence models combined with the vof volume of fluid method for tracking and locating the free surface their numerical results were compared with the experimental data by chanson 2002b demonstrating a reasonable agreement in terms of discharge and water depth future numerical studies should gain a more comprehensive characterization of the dropshaft flow including air entrainment and bubble clustering 6 conclusion and outlooks high velocity free surface flows are characterised by a sizeable amount of entrapped air and the advection of air water structures interacts with both the flow turbulence and atmosphere these self aerated flows are complicated multiphase flow motions commonly observed in natural water systems including breaking waves torrents and bores as well as in hydraulic structures the white waters have direct implications onto the water quality ecological sustainability and environmental integrated assessment of the natural systems in this introductory overview of physical and numerical modelling of self aerated air water flows the authors aimed to deliver a fundamental understanding to assist graduate and phd level students as well as early career professionals with the modelling of self aerated flows two case studies of self aerated flows the hydraulic jump and the dropshaft were introduced to illustrate the challenges in modelling air water flows so what is so special about self aerated air water flows they are multiphase i e gas liquid the basic equations must be developed for both phases with some coupling equations at the air water interfaces the interactions between air water entities and turbulent structures are not trivial and the use of standard force laws and mass and momentum transfer correlations typically developed for single phase flows can result in significant errors simply the influence of turbulence on the entrained air and surrounding atmosphere cannot be ignored this is sometimes referred to as two way coupling or even four way coupling between the various phases the presence of air water interfaces has some direct implication in the measurement techniques and instrumentation used in laboratory and prototype traditional instruments e g pitot tube piv lda are adversely affected by the gas liquid interfaces despite recent progresses in optical techniques the sidewall boundary effects cannot be neglected and the most robust metrology in highly self aerated free surface flows is the needle phase detection probe another challenge in physical modelling is the well known scaling issue with small size laboratory experiments small laboratory models drastically underestimate the air entrainment and the physical results cannot be extrapolated to a full scale system without bias and errors i e scale effects the multiphase structure of the flow impacts directly on the selection of suitable computational models several approaches were tested including rans les des dns and sph such cfd studies were mostly applied to the hydraulic jump with a few applications to smooth and stepped spillways breaking bores dropshafts and plunge pools a seminal challenge is the validation of the cfd results because it requires detailed air water physical data sets the quality of the validation data sets must be scrutinised because the validation process at the highest stage relies on comparisons with experiments rizzi and vos 1998 p 669 what type of data e g void fraction interfacial velocity bubble size distributions are turbulent multiphase flow quantities at the millimetric and sub millimetric scales e g clustering interparticle distances needed for a proper validation of cfd studies with what accuracy such validation should be carried out how confidently can the physical data set be extrapolated to a full scale prototype how could the results of cfd studies be applied to the design of hydraulic structures in water engineering the authors want to stress in the strongest terms the uppermost importance of cfd validation because nature is the final jury roache 1998 p 697 what are the outlooks on one side the last decade has seen some major development in air water self aerated flows detailed physical modelling studies are more common with a substantial increase in the amount of advanced air water flow measurements with needle phase detection probes optical techniques and other multiphase flow instrumentation advanced cfd research showed promising results with early dns work and even des simulations although most studies are still based upon les and rans meshless lagrangian sph method was also recently applied to self aerated flows it should be expected that the continuous increase in the available computational resources and modelling techniques will promote a shift of the applied cfd methods towards high fidelity approaches such as les and dns a very recent and successful development has been the hybrid modelling combining parallel physical and cfd numerical modelling with two way interactions between the two modelling techniques altogether we believe beyond doubt that all these recent progresses have been tremendous on another side there still some major knowledge gaps three obvious issues are 1 a lack of full scale prototype data 2 the requirements for high quality detailed validation data sets for cfd model development and 3 the need to expand modelling to more complicated air water flow applications new field measurements performed in situ constitute a key requirement to corroborate physical laboratory data and substantiate current cfd validation approaches based upon laboratory validation sets let us remember that even the large dropshaft seen in figs 5a and 10 could be regarded as a scale model of the larger dropshafts built beneath the cities of tokyo and chicago for example in physical modelling it is extremely difficult if not impossible to access detailed information at all spatio temporal scales relevant to cfd modelling any physical modelling can only generate a limited number of variables in contrast to numerical simulations which offer a larger range of outputs hence any validation contains intrinsic limitations simply let us remember that the validation of cfd numerical models is not trivial finally many practical applications correspond to some complicated three dimensional multiphase flow fig 1 that current physical and numerical models are most often unable to predict accurately most detailed physical models are two dimensional and three dimensional validation data sets are an exception at least for now at the end all the above issues suggest that further studies and approaches are still needed in the future to achieve a comprehensive physical and numerical modelling of self aerated air water flows declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge the very constructive comments from the reviewer carlo gualtieri acknowledges past fruitful discussions on numerical simulation of hydraulic jump with dr daniel valero and ing nicolò viti hubert chanson acknowledges the helpful discussions and interactions during more than 35 years of research on self aerated flows with many colleagues former students and current students including prof colin apelt prof fabian bombardelli prof daniel bung dr stefan felder dr carlos gonzalez dr xinqian sophia leng dr youkai li prof pierre lubin prof jorge matos dr frédéric murzyn rui ray shi dr luke toombes dr hang wang dr davide wüthrich and dr gangfu zhang in alphabetical order 
25775,in free surface turbulent flows large amount of air may be entrapped and advected in the water current the resulting air water flows are frequently observed in natural water systems where they are also relevant to water quality ecological sustainability and integrated assessment within such systems herein a review of physical and numerical modelling of air water flows is developed providing some fundamentals towards a consistent modelling of such flows to graduate and ph d level students as well as young researchers in environmental sciences and engineering with pre requisite knowledge in basic fluid mechanics after some theoretical and metrology considerations the main criteria for the design of physical models and the current literature on the numerical studies are discussed two case studies the hydraulic jump and the dropshaft are used to show the application of such criteria and methods overall the paper presents current knowledges challenges on physical and numerical modelling of self aerated free surface flows keywords air water flows introductory overview physical modelling numerical modelling metrology model validation 1 introduction in high velocity free surface turbulent flows large quantities of air bubbles pockets move across the air water interface being entrapped air entrainment in the water current and then are carried away within the flowing fluid and eventually exchanged back to the air flowing above the free surface halbronn et al 1953 jevdjevich and levin 1953 the resulting air water flow or self aerated flow is a mixture of air and water consisting of both air packets within water and water droplets surrounded by air chanson 1997 aerated flows are encountered in a wide range of applications in chemical civil environmental mechanical mining nuclear and water engineering rao and kobus 1974 fig 1 they span from small scale to very large scale in water and environmental systems self aerated flows are often observed in mountain streams storm waterways culverts dropshafts spillway chutes tidal channels and stilling basins where aeration is largely un controlled wood 1991 and such flows are also relevant to the water quality sediment transport ecological sustainability and ultimately environmental integrated assessment within such systems depending upon the application air entrainment should be maximised minimised or prevented wood 1985 the exchange of air across the air water interface is driven by the turbulence next to the air water interface the free surface breakup and air entrainment occur when the turbulent shear stress is greater than the surface tension force per unit area resisting the interfacial breakup hino 1961 ervine and falvey 1987 once some air is entrained within the bulk of the flow the break up of air pockets occurs when the tangential shear stress is greater than the capillary force per unit area chanson 2009 as bubbles and droplets are advected by the flow particle collisions may lead to their coalescence while air detrainment due to buoyancy also takes place the complex interactions between the entrained air and turbulence may produce some bubble clustering a cluster of bubbles is defined as a group of two or more bubbles with a distinct separation from the other bubbles before and after the cluster past studies demonstrated that a clustering analysis may provide some relevant insights about the interaction between turbulence and bubbly flow gualtieri and chanson 2010 2013 wang et al 2015a traditionally aerated flows are experimentally investigated through physical modelling but more recently numerical studies have been carried out physical models for such experimental studies must be designed on a sound similitude otherwise scale effects may affect the extrapolation of experimental results to full scale prototype structures kobus 1984 aerated flows are commonly studied using a froude similitude but in a geometrically similar model the dynamic similitude involves other dimensionless parameters shear flows are dominated by viscous effects while the mechanisms of bubble breakup and coalescence are controlled by surface tension forces chanson and gualtieri 2008 hence dynamic similitude in aerated flow requires that froude reynolds and morton numbers should be identical in both the prototype and its model but this is impossible to achieve using geometrically similar models unless working at the full scale using the same fluids i e air and water in the prototype and in laboratory a froude and morton similitude can be implemented but the model reynolds number cannot be as large as in the prototype leading to viscous scale effects in small size models rao and kobus 1974 wood 1985 chanson 2009 more recently computational fluid dynamics cfd methods have been applied to improve the current knowledge about self aerated flows such methods were developed over 50 years ago by engineers and mathematicians to solve flow problems in the area of industrial engineering and their application was later extended to many areas of fluid dynamics such as environmental fluid mechanics and water engineering including aerated flows cushman roisin et al 2012 rodi 2017 the main advantages of such methods are that they allow full control over the boundary conditions that they provide data in every point of the computational domain simultaneously and they might be performed at full scale albeit at a computational cost cfd also allows efficient parametric analyses of different configurations and for different flows and environmental conditions on the other side cfd methods are generally affected by uncertainties about input parameters that should be carefully considered blocken and gualtieri 2012 in self aerated flows further sources of uncertainty are related to the stability and sharpness of interfaces in schemes where interfaces are explicitly modelled and in other schemes to the representation of forces between the phases such as the lift and drag exerted by particles and to mesh refinement being limited by the particle size bombardelli 2012 viti et al 2018 hence cfd studies must be carried out following strict guidelines rizzi and vos 1998 roache 1998 2008 asme 2009 otherwise their accuracy and reliability and the correct use of their results can easily be compromised furthermore the results of numerical studies require to be validated using high quality experimental data collected in physical models for aerated flows several cfd techniques were applied but in many cases the validation of cfd simulations were conducted in terms of flow depth possibly time averaged velocity rarely including any comparison of void fraction distributions and interfacial properties chanson and lubin 2010 viti et al 2018 while to get a comprehensive validation turbulent microscopic flow quantities should also be considered this paper presents a review at the introductory level of physical and numerical modelling of aerated flows the review is intended to provide fundamentals towards a consistent modelling of such flows to graduate and ph d level students as well as young researchers in environmental sciences and engineering with pre requisite knowledge in basic fluid mechanics first after some theoretical considerations including the metrology of air water flows the main criteria for the design of physical models are discussed then the most widely used methods for the numerical simulation of aerated flows are presented and the current literature is reviewed pointing out the need for a proper validation of any numerical study two case studies the hydraulic jump and the dropshaft which probably received the largest and the lower attention within the literature respectively are used to provide guidelines for the application of such criteria and methods finally current challenges and future outlook on self aerated free surface flows are discussed 2 basic considerations 2 1 presentation incompressible turbulent flows are governed by the equations of conservation of mass and momentum these laws are represented through the navier stokes equations which in their original form encompass all known internal and external effects of the motion of a fluid unlike single phase turbulence where even simple reynolds closure models have proven some usefulness simple models have failed by and large in the case of multiphase gas liquid flows elgobashi 1991 balachandar and eaton 2010 the complexity of the two phase flow motion may be illustrated by the equations of fluid motion governing the multiphase gas liquid flows at the micro scale combined with some interface tracking tryggvason et al 2011 bombardelli 2012 chanson 2013 1a ρ w t i x y z ρ w v w i x i 0 w a t e r 1b ρ a t i x y z ρ a v a i x i 0 a i r 2a ρ w v w i t j x y z ρ w v w i v w j x j p w x i ρ w g i j x y z τ w i j x j w a t e r 2b ρ a v a i t j x y z ρ a v a i v a j x j p a x i ρ a g i j x y z τ a i j x j a i r where the subscripts a and w refer to the air and water properties respectively v is the instantaneous velocity component p is the instantaneous pressure gi is the gravity acceleration in the direction i x y z and τij denotes an instantaneous shear stress tensor component equations 1a and 2a for the water and equations 1b and 2b for the air phase must be complemented by a mathematical representation of the moving interface and the associated conditions to couple the equations across the air water interfaces this formulation may be used for detailed direct numerical simulations although the application is very complicated prosperetti and tryggvason 2009 tryggvason et al 2011 mortazavi et al 2016 the interface tracking the coupling of equations at the air water interfaces and the correct implementation of the boundary conditions are not as easy as it may sound in engineering applications bombardelli 2012 water crashing in high velocity free surface flows can be an extremely violent motion with much spray and splashing and complicated interfacial processes fig 1 the generation of bubble clouds and water droplets has been proved to induce energy dissipation and turbulent mixing to contribute to heat exchange and to enhance gas transfer gualtieri and gualtieri 2004 wanninkhof et al 2009 veron 2015 most recent modelling attempts are struggling with the lack of physical knowledge and conceptual model of the finest details of the breaking processes which makes the task of parameterising the impact of self aeration in steady and unsteady flows very difficult since no universal scaling laws for physical variables have been found so far lubin and glockner 2015 lubin and chanson 2017 newer models still need validation and further improvements 2 2 onset of air entrainment the onset of self aeration may be defined as the threshold condition for air entrainment to take place despite some fine distinction between the first single bubble entrainment and the onset of continuous bubble entrainment e g cummings and chanson 1999 chanson and manasseh 2003 these two flow conditions usually cover a narrow range broadly called the onset of air entrainment many early studies of air water flows expressed the onset condition as a function of a time averaged inflow velocity v1 i e with air entrainment in turbulent water flows occurring for v1 0 5 2 m s rao and kobus 1974 ervine et al 1980 some seminal studies related self aeration to the inflow turbulence hino 1961 mckeogh and ervine 1981 more recently detailed experimental studies showed a characteristic trend with onset conditions about v1 2 m s in smooth turbulent flows and v1 0 8 m s in rough turbulent flows cummings and chanson 1999 kiger and duncan 2012 nowadays it is understood that air bubble entrainment occurs when the turbulent shear stress next to the free surface interface is large enough to overcome the surface tension ervine and falvey 1987 chanson 2009 that is the onset of air bubble entrainment is linked to a characteristic threshold in terms of reynolds stresses in the liquid phase next to the air water interface for a spherical bubble of radius r and isotropic turbulence chanson et al 2021 derived an analytical expression of the onset condition 3 μ w v σ μ w 2 2 ρ w σ π r v v v 2 where ρw is the water density μw is the water dynamic viscosity σ is the surface tension v is the longitudinal water velocity v is the instantaneous turbulent velocity fluctuation despite the simplified development spherical bubble isotropic turbulence eq 3 predicts entrained bubble sizes consistent with experimental data in vertical plunging jets chanson et al 2021 fig 2 2 3 advection and turbulent diffusion of air bubbles once entrapped the entrained air is carried away with the flowing fluid and its motion in within the water column is controlled by the complex interaction among advection turbulent diffusion and mixing and upward buoyancy furthermore as bubbles and droplets are transported with the flow turbulent shear may induce breakup and formation of smaller daughter particles while particles collisions may lead to their coalescence various diffusion models were developed and applied successfully to a range of air water flow typology and flow conditions chanson 1997 in the hydraulic jump the transport of air bubbles downstream the jump toe can be modelled using the classical advection diffusion equation wood 1984 where advection is along the horizontal direction and the diffusion process occurs only in the vertical direction chanson 1995 2010 gualtieri and chanson 2007 in the dropshaft the longitudinal distribution of air bubbles around the underwater jet trajectory follows the diffusion equation gualtieri and chanson 2004 the same type of equations can be applied even in other air water flows such as on steep chutes and in vertical plunging jets chanson 2012 2 4 air water flow measurements air water flows are usually investigated in the flow region where the air concentration is less than 95 and the two phases move with a nearly identical velocity chanson 1997 for their description in comparison to a single phase flow aerated flows require a number of additional parameters such as the air concentration or void fraction the bubble count rate the bubble and drop size distributions the properties of clusters of air bubbles further if void fraction exceeds 5 some classical parameters of a turbulent flow e g the instantaneous velocity cannot be measured with traditional instruments such as pitot tube acoustic doppler velocimetry adv laser doppler velocimetry lda because air bubbles and air water interfaces affect adversely their operation since the 1950s some specialised instrumentations such as back flushing pitot tubes needle phase detection probes conical hot film probes and fibre phase doppler anemometry were developed for the measurement of aerated flows however their application was limited by calibration issues and hence the most widely applied device in the last 40 years has been the phase detection needle shaped probe or conductivity probe which is designed to pierce the bubbles and droplets fig 3 later with the development of image based velocimetry particle image velocimetry piv due to the advancement in computational power bubbles were used as tracer particles under ordinary lighting conditions to identify velocity in aerated flows in a method called bubble image velocimetry biv more recently in the last decade two major developments in air water velocity measurements have been the total pressure probe and the optical flow of metrology the needle probe is an intrusive phase detection probe used to discriminate between air and water phases using the different conductivity of air and water the signal output quality of this probe is closely linked to the sensor size with the needle diameter the sampling rate f sampl and sampling duration t sampl sensor sizes in less than 0 1 mm are used at low flow velocities v 1 2 m s while for higher velocity flows more sturdy probes with diameters typically between 0 1 and 0 5 mm are required with a needle probe the selection of the sampling frequency is linked to the smallest detectable bubble size which is of the order of magnitude of the needle diameter generally the sampling rate should be greater than 10 khz and the sampling duration larger than 20 s to have negligible effect on the void fraction bubble count rate and air water velocity measurements while more advanced correlation analyses including the estimate of the turbulence intensity require a sampling duration of 45 s or larger the phase detection probe could have a single tip or a dual tip design fig 3b where the latter provides additional information on the interfacial velocity and turbulence level bubble image velocimetry biv relies upon interrogation of an image frame pair by computing the spatial cross correlation ryu et al 2005 however due to its discrete data nature for certain tracer size range the method may cause displacement vectors to be biased towards integer pixel values commonly referred to as pixel locking chen and katz 2005 direct computation of the correlation surface is expensive and any velocity or seeding gradient in the interrogation region especially a large region introduces a bias towards smaller displacement another major limitation is the bias of the sidewall flow conditions where boundary friction cannot be neglected biv velocity data typically underestimates the velocity field on the channel centreline which is significantly larger than the near wall velocities when measured by an intrusive probe zhang and chanson 2018 total pressure measurements with miniature diaphragm sensor can deliver a fine characterization of the velocity and turbulence in the water phase when accounting for the local void fraction wang et al 2015b zhang et al 2016 the optical flow approach is based upon the detection of changes in brightness due to reflectance difference associated with passages of air water interfaces bung and valero 2016a zhang and chanson 2018 fig 6 bung and valero 2016b compared biv and optical flow estimates in seeded and aerated flows they found comparable accuracies for both methods with the optical flow technique providing higher resolution data albeit requiring a much longer computation time some key limitations of all optical techniques are the requirements for two dimensional flows the use of high speed high resolution video camera and the adverse impact of sidewall effects bung and valero 2016a zhang and chanson 2018 3 similitude and physical modelling any modelling investigation is expected to deliver a sound prediction of the hydrodynamic characteristics of the flow motion in a full scale prototype operation with a few examples illustrated in fig 1 henderson 1966 hamill 1995 chanson 2004a the modelling approach must be developed based upon the basic principles of similitude to deliver reliable extrapolations rayleigh 1915 the presentation of any modelling data has to be relevant to the full scale prototype applications and dimensional analysis is the underlying method to deliver the most relevant design parameters bertrand 1878 rouse 1938 liggett 1994 in air water flows an early study highlighted that model tests provide little help due to our ignorance of the laws of hydrodynamic similarity of aerated flow jevdjevich and levin 1953 p 439 while a more recent review paper emphasized that the results of experimental investigations demonstrated unequivocally the limitations of dynamic similarity and physical modelling of aerated flows chanson 2013 p 229 yet physical modelling and laboratory experiments remain essential tools to validate phenomenological theoretical and numerical models hanratty et al 2003 in a study of self aerated free surface flows the relevant dimensional parameters include the air and water properties physical properties channel dimensions and inflow conditions for a chute flow e g figs 1c 2 a simple dimensional analysis yield 4 c v v f n c l t t t f 1 x y z ρ w μ w σ g q b k s θ with c the void fraction v the interfacial velocity v a characteristic velocity fluctuation f the level of flow fragmentation nc the cluster rate lt and tt some characteristics turbulence length and time scale x y z being respectively the longitudinal normal and transverse coordinates ρw the water density μw the dynamic viscosity of water σ the surface tension between air and water g the gravity acceleration q the unit discharge b the channel width ks is the equivalent sand roughness height of the invert surface and θ the chute slope the buckingham π theorem 1 1 the buckingham π theorem is also called vaschy buckingham theorem after the french engineer aimé vaschy 1857 1899 and american physicist edgar buckingham 1867 1940 states that any dimensional equation with n variables with units encompassing mass length and time mlt may be rewritten into an equation with n 3 dimensionless parameters vaschy 1892 buckingham 1914 rouse 1938 thus equation 4 may be transformed as 5 c v v c v v c f d c v c n c d c v c l t d c t t v c d c f 2 x d c y d c z d c d c k s ρ v d h μ g μ 4 ρ σ 3 b d c θ with dc and vc the critical flow depth and velocity respectively and dh the hydraulic diameter equation 5 expresses the local dimensionless air water flow properties at a location x y z as functions of a number of dimensionless parameters including the froude number 4th term on the right handside reynolds number 5th term and morton number 6th term a laboratory study is typically undertaken using geometrically similar models in the physical model the air water flow properties must display similarity of form of motion and of forces novak and cabelka 1984 chanson 2004a if this is not achieved scale effects occur in relation to the parameter s of interest and the extrapolation of the model data will not accurately predict the full scale prototype performances considering a high velocity self aerated flow in a rectangular channel figs 2 5 the present analysis illustrated the large number of relevant parameters any true similarity would require identical dimensionless variables including froude reynolds and morton numbers in both laboratory and full scale prototype this situation is physically impossible because of the large number of independent parameters eq 5 past experiences showed that small laboratory experiments drastically under represented the air entrainment kobus 1984 chanson 1997 2009 figs 4 and 5 illustrates the air entrainment in two types of self aerated flows each figure presents photographs of the flow at an identical froude and morton number but different reynolds numbers for scale the inflow depth was 0 097 m 0 045 m and 0 027 m in fig 4a b and 4c respectively in fig 5a the shaft was 0 755 m long and 0 763 m wide while the shaft was 0 243 m long and 0 246 m wide in fig 5b both examples emphasized the scale effects in small sized laboratory facilities operating at relatively small reynolds numbers in practice the laboratory experiments must be conducted in a large size facility operating at relatively large reynolds numbers typically re 2 105 to 5 105 4 numerical modelling numerical methods are increasingly applied to improve the current knowledge about self aerated flows despite their several advantages the accuracy and reliability of the numerical approach are still of concern and verification and validation studies are limited it is widely recognized that the results of cfd simulations can be very sensitive to the wide range of computational parameters that have to be set by the user the set up of any numerical study is associated to uncertainties about geometry and boundary conditions drag coefficients driving forces and the interactions among different processes and inputs this is of paramount importance in the numerical simulation of self aerated flows which involves fluctuating boundaries as well as a multiphase flow while cfd methods were previously applied to self aerated flows to supplement the use of physical models bombardelli 2012 and address the intrinsic limitations of experimental measurements the development of advanced theoretical models for turbulence and two phase flows led such numerical methods to be often applied together with experimental methods to tackle and interpret self aerated flows in the last decade a flow field may be described following two approaches eulerian methods and lagrangian methods the first approach studies flow properties in a number of fixed points this corresponds to a coordinate system fixed in space where fluid properties are studied as functions of time as the flow passes the latter follows the motion of each individual fluid parcel as it moves from some initial location this corresponds to a coordinate system on each fluid parcel past numerical simulations of aerated flows encompass both eulerian such as reynolds averaged navier stokes rans detached eddy simulation des large eddy simulation les and direct numerical simulation dns and lagrangian such as smoothed particle hydrodynamics sph methods bombardelli 2012 rodi et al 2013 violeau and rogers 2016 viti et al 2018 the most widely applied approach to simulate a turbulent flow is that based on the time averaging even termed reynolds averaging of the navier stokes equations where the instantaneous values of velocity and pressure is assumed to be the sum of a time averaged value and a fluctuating component this statistical approach leads to the reynolds averaged navier stokes rans equations kundu et al 2012 where the averaging of non linear advective terms results in unknown correlations between fluctuating velocities these additional unknowns introduce the need for a closure of the rans equations such correlations are usually seen as stresses termed reynolds stresses additional to those due to fluid viscosity following the boussinesq hypothesis reynolds stresses are treated using an eddy or turbulent viscosity and the spatial gradient of the time averaged velocities kundu et al 2012 such eddy viscosity ultimately means that the effect of turbulence is to act on the mean flow as an increased viscosity different estimations for the eddy viscosity have been proposed dimensional reasoning suggests that the eddy viscosity can be obtained as a product of a turbulent velocity scale by a turbulent length scale different approaches can be used to derive these scales at the simplest level of complexity one may expect that the eddy viscosity would be determined by large scale eddies the size of which is close to the characteristic dimension and velocity of the flow itself thus eddy viscosity would be linked to the overall velocity gradient as in the mixing length model kundu et al 2012 alternatively an obvious choice for defining a turbulent velocity scale is the turbulent kinetic energy k while many variants were proposed in the literature to define a turbulent length scale leading to different families of two equations turbulence models such as the k ε model and the k ω turbulence model where the rate of turbulent energy dissipation ε and the specific dissipation rate ω ε k respectively are used to get a turbulent length scale each of these families has different derived models characterised from different equations and numerical constants rans approach has been frequently applied in aerated flows such as the hydraulic jump chippada et al 1994 gonzalez and bombardelli 2005 bayon et al 2016 valero et al 2018 witt et al 2018 macián pérez et al 2020 smooth and stepped spillways meireles et al 2014 toro et al 2016 lopes et al 2017 valero et al 2018 chutes hohermuth et al 2020 dropshafts sousa et al 2009 and plunge pools carrillo et al 2020 often in such studies an additional method is required to track the free surface i e volume of fluid vof by hirt and nichols 2008 while the large scale of the turbulent spectrum produced by mean flow is long living energetic diffusive inhomogeneous anisotropic and depending on domain geometry and boundaries the small scale produced by large eddies is short living no energetic dissipative universal random homogeneous isotropic and can be modelled statistically rodi et al 2013 this fundamental difference has led to conceptualize the large eddy simulation les approach where the large scale of turbulence is resolved while the small scale is modelled rodi et al 2013 the main difference between rans and les approaches is that the navier stokes equations are in the former time averaged and in the latter space filtered furthermore the cut off below which a model is used is a frequency domain cut off in les whereas in rans it is a physical domain cut off in spite of that both sets of equations get a similar form because a stress tensor is created by the time averaging and filtering processes however in les differently from rans models these stresses called sub grid scale stresses pertain only to the turbulent spectrum that is not solved but modelled using different methods rodi et al 2013 les is more computationally demanding that rans but the continuous development of computational power has led les approach to be increasingly applied to water engineering and environmental hydraulics including to self aerated flows such as the hydraulic jump gonzalez and bombardelli 2005 lubin et al 2009 and tidal bores lubin et al 2010a 2010b leng et al 2018a b to combine the advantages of rans and les minimising their limitations hybrid les rans approaches such as detached eddy simulation des were proposed and even applied to aerated flows but to the hydraulic jump only ma et al 2011 jesudhas et al 2018 2020 basically in the des rans and les methods are used in the near wall region and in the free stream region respectively rodi 2017 in the direct numerical simulation dns the unsteady 3d navier stokes equations are solved directly using spatial and temporal resolutions sufficiently fine to resolve the dynamics of the entire spectrum of turbulent eddies in the flow from the energy producing largest eddies whose size is comparable to the flow domain to the smallest eddies of kolmogorov scale at which turbulence energy is dissipated into heat by molecular action it is easy to identify a critical issue for dns in the extremely large computational power in terms of both the processor s performances and the size of the memory for storing intermediate results required to achieve results for real world problems in a reasonable time it is noteworthy that the computational power needed by dns is proportional to the flow reynolds number re9 4 the contributions of dns to turbulence research in the last few decades have been impressive alfonsi 2011 but its application to water engineering and environmental hydraulics problems which are characterised by large reynolds numbers is still very limited and in the field of self aerated flows only a study on hydraulic jump mortazavi et al 2016 was published so far among the meshless lagrangian techniques smoothed particle hydrodynamics sph which solves flow equations for a set of moving particles with a certain mass has been recently applied to aerated flows such as the hydraulic jump lópez et al 2010 de padova et al 2013 2018 wan et al 2018 smooth and stepped spillways wan et al 2017 nobrega et al 2020 and tidal bores nikeghbali and omidvar 2017 while the use of a kernel function to interpolate flow variables is critical to sph some general advantages of the sph method over the mesh based methods are the effectiveness in solving complex fluid dynamic problems with highly nonlinear deformations and the natural tracking of free surfaces and moving boundaries the analysis of the above mentioned literature shows that the number of numerical studies capable to gain a complete validation of their results is still limited for the hydraulic jump which is the most frequently investigated self aerated flow most rans les sph studies focused on the free surface simulation and on the prediction of the main jump parameters conjugate depth ratio roller length hydraulic jump length and efficiency etc and time averaged velocity while only few studies considered also pressure fluctuations turbulence features and air entrainment quantities void fraction on the other side high fidelity methods such as des and dns were extended to a comprehensive characterization of turbulence including the identification of coherent structures and interface length scales and of air entrainment mortazavi et al 2016 jesudhas et al 2018 2020 fig 6 for smooth and stepped spillways rans studies gained mostly a characterization of average velocity and pressure distribution vorticity turbulent kinetic energy and its dissipation rate meireles et al 2014 toro et al 2016 while sph predicted flow depth and velocity nobrega et al 2020 and also the longitudinal distribution of dissolved oxygen do concentration wan et al 2017 numerical studies on tidal bores both with les and sph focused on the prediction of time variable free surface dynamics and velocity distribution lubin et al 2010a 2010b nikeghbali and omidvar 2017 leng et al 2020 finally while the numerical analysis of a dropshaft was limited to discharge and water depth sousa et al 2009 very recent rans studies on chutes hohermuth et al 2020 and plunge pools carrillo et al 2020 gained the prediction of both the velocity field and of air entrainment including void fraction bubble frequency and sauter bubble diameter in a highly transient flow one application showed that the instantaneous void fraction and bubble distribution data presented systematically a lesser aeration region in the physical model compared to the numerical data leng et al 2018b 5 discussion at the end it is advisable for numerical studies of self aerated flows to get a comprehensive validation across a broad range of relevant air water flow properties with relevant turbulent integral length and time scales in addition to the microscopic flow structure e g clustering interparticle distances in line with the cfd validation requirements set for monophase flows rizzi and vos 1998 roache 1998 2008 asme 2009 blocken and gualtieri 2012 the validation of cfd numerical models is anything but trivial a proper validation necessitates a combined and fundamental understanding of the numerical model and its limitations together with an expert knowledge of the physical model its characteristics and its instrumentation leng et al 2018a such a combined expertise and experience is critical to ensure the suitability of the experimental physical data set for cfd validation most often a proper cfd model validation required a team of experts with physical and numerical experience yet both physical and numerical models are developed to reproduce a full scale three dimensional flow phenomenon for which prototype data are rarely available for the ultimate validation chanson 2013 a recent development has been the hybrid modelling combining laboratory experiments and numerical computational fluid dynamics cfd modelling together fig 7 leng et al 2018b leng and chanson 2020 a major advantage in engineering design is the optimisation of resources combining the flexibility of cfd modelling e g to reduce the costs in building and testing several large size physical models and operating large size laboratory models to produce realistic boundary and initial conditions yielding high quality validation data sets for cfd numerical modelling in turn reducing the total simulation times such a composite approach may include interactions feedbacks and loops between the physical and cfd techniques providing new capabilities to the entire design process 5 2 case studies 5 2 1 the hydraulic jump a hydraulic jump is a sudden transition from a high speed open channel flow into a slow fluvial flow commonly experienced in streams and rivers as well as in man made canals industrial channels and downstream of dam spillways the jump is a seminal fluid flow with extreme turbulence linked to the development of large scale eddies surface waves and spray energy dissipation and air entrainment figs 4 and 8 it is the most largely investigated physically and numerically self aerated flow in a hydraulic jump some air is entrapped at the discontinuity between the impinging flow and roller called jump toe or roller toe rajaratnam 1962 chanson and brattberg 2000 murzyn et al 2005 further air is entrained through the roller free surface wang and chanson 2015 at the jump toe the impingement perimeter acts as a source of vorticity and the developing air water mixing layer is the locus of the advective diffusion of vorticity and entrained air the hydraulic jump is characterised a sudden rise in water levels figs 4 and 8 associated with some discontinuity in terms of the pressure and velocity fields it is a hydrodynamic shock lighthill 1978 liggett 1994 the application of the equations of conservation of mass and momentum in an integral form gives a system of equations linking the one dimensional flow properties upstream and downstream of the jump henderson 1966 chanson 2012a for hydraulic jumps in a smooth horizontal rectangular channel it yields 6 d 2 d 1 1 2 1 8 f r 1 2 1 7 f r 2 f r 1 2 1 8 f r 1 2 1 3 2 where d and v are the flow and depth averaged velocity respectively fig 9 a the subscripts 1 and 2 refer to the upstream and downstream conjugate properties and fr is the froude number defined as fr v g d 1 2 for a rectangular channel equations 6 and 7 highlight the importance of the inflow froude number fr1 and the selection of the froude similitude for any physical modelling derives implicitly from these fundamental theoretical considerations a key feature of the hydraulic jump is the developing shear layer with a recirculation region above figs 8 9a the turbulent shear flow is somehow analogous to a wall jet rajaratnam 1965 chanson and brattberg 2000 while the advection of air bubbles can be modelled by an advection diffusion equation chanson 1995 1997 typical vertical distributions of void fraction c bubble count rate f and longitudinal velocity vx are sketched in fig 9b the shear layer is typically characterised by a local maximum in void fraction cmax which decreases pseudo exponentially with increasing distance from the roller toe as the shear layer expands chanson and brattberg 2000 murzyn et al 2005 similarly some momentum consideration implies that the maximum velocity vmax decays quasi exponentially with longitudinal distance from the roller toe typical experimental observations of cmax and vmax are shown in fig 9c in which they are compared with two robust correlations wang and chanson 2016 chanson 2010 8 c max 0 5 exp 1 1 8 f r 1 1 x x 1 d 1 9 v max v 1 exp 0 028 x x 1 d 1 with x1 the roller toe position the development of large turbulent structures and vortex pairing is conducive of bubble clustering in the turbulent mixing layer chanson 2007a wang et al 2015a figs 3 and 8 while a cluster is a three dimensional air water structure the current metrology is restricted to the detection of longitudinal and transverse clusters experimental data showed a large proportion of entrained bubbles advected in clusters typically mostly encompassing between 2 and 5 bubbles wang et al 2015a during the past two decades hydraulic jumps have been also investigated using numerical methods both lagrangian and eulerian gonzalez and bombardelli 2005 viti et al 2018 most numerical studies have been carried out by rans approach to validate this tool for a set of flow conditions different two equations turbulence models mostly belonging to the k ε family were applied in a range of inflow froude number from 1 5 to 9 5 generally rans simulations yielded relatively accurate results with accuracies over 90 for average flow variables conjugate depth ratio roller length hydraulic jump length and efficiency mean free surface and even in some cases for air entrainment mainly investigated in terms of air concentration void fraction and not considering the distribution of bubbles sizes viti et al 2018 at the end any of the applied two equation models presented could be used for design purposes provided that the related uncertainties are considered in the analysis of the numerical results while meshless sph simulations showed a promising agreement in terms of free surface elevations and velocity profiles high fidelity methods such as les and mostly des and dns provided a comprehensive characterization of turbulence quantities indicating the future area of development of numerical studies on the hydraulic jump mortazavi et al 2016 jesudhas et al 2018 2020 5 2 2 air entrainment in a rectangular dropshaft a dropshaft is a vertical conduit connecting two channels located at different elevations figs 5 and 10 the loss in potential energy acts basically as some energy dissipation in practice there are two common types of dropshaft i e the plunge type and the vortex type the plunge dropshaft design herein investigated has been used for millennia lopez cuervo 1985 chanson 2002a and modern applications encompass sewers storm waterways and even large spillway shafts with morning glory intake however the literature on dropshaft is not large the dropshaft operation may cover several flow regimes depending upon the boundary conditions i e shaft geometry and inflow conditions most frequently flow and air entrainment in a dropshaft are investigated by physical modelling chanson 2002a 2004b 2007b camino et al 2015 ma et al 2016 ding and zhu 2018 while numerical studies are unfortunately still limited sousa et al 2009 let us consider a rectangular dropshaft as illustrated in figs 5a and 10 which is a near full scale facility at low discharges the free falling nappe impacts into the shaft pool regime r1 fig 10a chanson 2004b gualtieri and chanson 2004 while at large flow rates the nappe impacts on the opposite wall regime r3 figs 5a 10b chanson 2007b for a narrow range of intermediate flows the nappe impact may interfere with the shaft outflow conduit regime r2 fig 5b in the regime r1 the air entrainment is primarily a plunging jet action with bubble entrainment occurring at the plunge point at large discharges air entrainment is a combined effect of nappe impact and splashing on the opposite wall and plunging action in the shaft pool fig 10 air entrainment in such dropshaft was experimentally investigated using a sturdy single tip phase detection probe chanson 2002b 2007b gualtieri and chanson 2004 such laboratory experiments showed a strong aeration of the dropshaft pool for all discharges with entrained bubbles mean size between 10 mm and 20 mm chanson 2007b maximum void fraction was located close to the theoretical trajectory of underwater jet fig 11 a where turbulent shear was the largest but bubble coalescence and detrainment processes reduced the percentage of the smaller air bubbles along such trajectory furthermore a decreasing number of air bubbles could penetrate into the pool at increasing depths void fraction data were found to obey to an analytical solution of diffusion equation for air bubbles whose distribution was skewed and followed reasonably well a log normal probability distribution function chanson 2002b 2007b gualtieri and chanson 2004 bubble clustering was further investigated in such dropshaft aerated flow to better characterise the interactions between bubbles and large scale vortices bubble clusters can be identified by analysing the water chord between two adjacent air particles or the interparticle arrival times iats τ ia for the air bubbles while the former method provides only some general features of the clustering process such as the number of clusters of clustered bubbles and of bubbles belonging to cluster structures in each point of measurement the iat analysis allows also to identify the range of particle sizes affected by clustering and ultimately the structure of each cluster and of the bubbly flow both methods demonstrated the relevance of clustering process in the dropshaft flow clustering was the largest close to the plunge point in the pool and along the theoretical trajectory of underwater jet with some decaying pattern with the depth gualtieri and chanson 2011 2013 the iat data demonstrated that for a similar level of turbulence the bubbly flow structure in the dropshaft had a density of bubbles per unit flux larger than in the hydraulic jump flow fig 11b suggesting a stronger level of interaction between air bubbles and turbulent flow in the dropshaft gualtieri and chanson 2013 as already noted numerical studies of the above presented rectangular dropshaft are mostly limited to that of sousa et al 2009 sousa and co workers investigated the hydraulic operation in that dropshaft using both standard k ε and rng k ε turbulence models combined with the vof volume of fluid method for tracking and locating the free surface their numerical results were compared with the experimental data by chanson 2002b demonstrating a reasonable agreement in terms of discharge and water depth future numerical studies should gain a more comprehensive characterization of the dropshaft flow including air entrainment and bubble clustering 6 conclusion and outlooks high velocity free surface flows are characterised by a sizeable amount of entrapped air and the advection of air water structures interacts with both the flow turbulence and atmosphere these self aerated flows are complicated multiphase flow motions commonly observed in natural water systems including breaking waves torrents and bores as well as in hydraulic structures the white waters have direct implications onto the water quality ecological sustainability and environmental integrated assessment of the natural systems in this introductory overview of physical and numerical modelling of self aerated air water flows the authors aimed to deliver a fundamental understanding to assist graduate and phd level students as well as early career professionals with the modelling of self aerated flows two case studies of self aerated flows the hydraulic jump and the dropshaft were introduced to illustrate the challenges in modelling air water flows so what is so special about self aerated air water flows they are multiphase i e gas liquid the basic equations must be developed for both phases with some coupling equations at the air water interfaces the interactions between air water entities and turbulent structures are not trivial and the use of standard force laws and mass and momentum transfer correlations typically developed for single phase flows can result in significant errors simply the influence of turbulence on the entrained air and surrounding atmosphere cannot be ignored this is sometimes referred to as two way coupling or even four way coupling between the various phases the presence of air water interfaces has some direct implication in the measurement techniques and instrumentation used in laboratory and prototype traditional instruments e g pitot tube piv lda are adversely affected by the gas liquid interfaces despite recent progresses in optical techniques the sidewall boundary effects cannot be neglected and the most robust metrology in highly self aerated free surface flows is the needle phase detection probe another challenge in physical modelling is the well known scaling issue with small size laboratory experiments small laboratory models drastically underestimate the air entrainment and the physical results cannot be extrapolated to a full scale system without bias and errors i e scale effects the multiphase structure of the flow impacts directly on the selection of suitable computational models several approaches were tested including rans les des dns and sph such cfd studies were mostly applied to the hydraulic jump with a few applications to smooth and stepped spillways breaking bores dropshafts and plunge pools a seminal challenge is the validation of the cfd results because it requires detailed air water physical data sets the quality of the validation data sets must be scrutinised because the validation process at the highest stage relies on comparisons with experiments rizzi and vos 1998 p 669 what type of data e g void fraction interfacial velocity bubble size distributions are turbulent multiphase flow quantities at the millimetric and sub millimetric scales e g clustering interparticle distances needed for a proper validation of cfd studies with what accuracy such validation should be carried out how confidently can the physical data set be extrapolated to a full scale prototype how could the results of cfd studies be applied to the design of hydraulic structures in water engineering the authors want to stress in the strongest terms the uppermost importance of cfd validation because nature is the final jury roache 1998 p 697 what are the outlooks on one side the last decade has seen some major development in air water self aerated flows detailed physical modelling studies are more common with a substantial increase in the amount of advanced air water flow measurements with needle phase detection probes optical techniques and other multiphase flow instrumentation advanced cfd research showed promising results with early dns work and even des simulations although most studies are still based upon les and rans meshless lagrangian sph method was also recently applied to self aerated flows it should be expected that the continuous increase in the available computational resources and modelling techniques will promote a shift of the applied cfd methods towards high fidelity approaches such as les and dns a very recent and successful development has been the hybrid modelling combining parallel physical and cfd numerical modelling with two way interactions between the two modelling techniques altogether we believe beyond doubt that all these recent progresses have been tremendous on another side there still some major knowledge gaps three obvious issues are 1 a lack of full scale prototype data 2 the requirements for high quality detailed validation data sets for cfd model development and 3 the need to expand modelling to more complicated air water flow applications new field measurements performed in situ constitute a key requirement to corroborate physical laboratory data and substantiate current cfd validation approaches based upon laboratory validation sets let us remember that even the large dropshaft seen in figs 5a and 10 could be regarded as a scale model of the larger dropshafts built beneath the cities of tokyo and chicago for example in physical modelling it is extremely difficult if not impossible to access detailed information at all spatio temporal scales relevant to cfd modelling any physical modelling can only generate a limited number of variables in contrast to numerical simulations which offer a larger range of outputs hence any validation contains intrinsic limitations simply let us remember that the validation of cfd numerical models is not trivial finally many practical applications correspond to some complicated three dimensional multiphase flow fig 1 that current physical and numerical models are most often unable to predict accurately most detailed physical models are two dimensional and three dimensional validation data sets are an exception at least for now at the end all the above issues suggest that further studies and approaches are still needed in the future to achieve a comprehensive physical and numerical modelling of self aerated air water flows declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors acknowledge the very constructive comments from the reviewer carlo gualtieri acknowledges past fruitful discussions on numerical simulation of hydraulic jump with dr daniel valero and ing nicolò viti hubert chanson acknowledges the helpful discussions and interactions during more than 35 years of research on self aerated flows with many colleagues former students and current students including prof colin apelt prof fabian bombardelli prof daniel bung dr stefan felder dr carlos gonzalez dr xinqian sophia leng dr youkai li prof pierre lubin prof jorge matos dr frédéric murzyn rui ray shi dr luke toombes dr hang wang dr davide wüthrich and dr gangfu zhang in alphabetical order 
25776,flooding is the most common natural hazard leading to property damage injuries and death despite the potential for major consequences urban flooding remains difficult to forecast largely due to a lack of data availability at fine spatial scales and associated predictive capabilities crowdsourcing of public webcams social media and citizen science represent potentially important data sources for obtaining fine scale hydrological data but also raise novel challenges related to data reliability and consistency we provide a review of literature and analysis of existing databases regarding the availability and quality of these unconventional sources that then drives a discussion of their potential to support fine grained urban flood modelling and prediction our review and analysis suggest that crowdsourced data are increasingly available in urban contexts and have considerable potential integration of crowdsourced data could help ameliorate quality and completeness issues in any one source yet substantial weaknesses and challenges remain to be addressed keywords urban hydrology flooding crowdsourced data data integration 1 introduction flooding is one of the most common and amongst the most damaging natural hazards in the united states national academies 2019 the vast majority of flood risks to life and property are concentrated in cities ashley and ashley 2008 cities are particularly vulnerable to pluvial flooding which occurs when high volumes of stormwater runoff exceed drainage capacity in developed areas national academies 2019 another portion of this flood vulnerability arises when urban development expands into marshes and floodplains increasing fluvial and coastal flooding vidal 2017 further complexity is added by a non stationary climate meaning the future climate cannot be predicted by historical trends chester and allenby 2018 milly et al 2008 extreme precipitation events are increasing in magnitude and in some regions increasing in frequency larsen et al 2009 minnery and smith 1996 prein et al 2017 schreider et al 2000 additionally low magnitude high frequency events may have consequences that exceed that of extreme precipitation events as seen with nuisance flooding moftakhari et al 2017 this leads to questions regarding the adequacy of stormwater infrastructure design standards and development approaches used to address flood vulnerability markolf et al 2020 zevenbergen et al 2008 consequences of urban flooding include physical and non physical direct impacts indirect impacts socioeconomic ramifications and risks to human life and health highlighting the interdependencies of infrastructure systems in urban spaces kim et al 2017 könig et al 2002 markolf et al 2019 rosenzweig et al 2021 these compounding impacts place a higher pressure on decision makers and emphasize the need for urban flood monitoring we refer to this set of decision makers as flood managers or practitioners and researchers who monitor model and manage urban flooding despite the recognized impacts urban flood monitoring and forecasting remains extremely limited and unreliable largely due to a lack of water level and streamflow data availability at the very fine spatial scales at which urban flooding occurs rosenzweig et al 2021 while the majority of precipitation data are collected in cities national oceanic and atmospheric administration noaa 2017 it is coarse scale and mostly rural river flow observations that are used to predict flooding united states geological survey n d despite the 8 000 streams that are currently gauged and operated by the united states geological survey usgs or other local agencies these groups do not have the capacity to cover fine scale urban drainage systems for instance surface drainage e g roadways or subsurface drainage e g stormwater systems or the nearly 400 000 named streams and the hundreds of thousands of undocumented streams throughout the united states wagner and eberts 2020 fig 1 from the purely practical perspectives of size cost manpower and physical access the professional grade gauging used by the usgs and local flood agencies to monitor streamflow could never be deployed at scale in an urban landscape for these reasons existing tools for urban flood monitoring modelling and warning are widely recognized as incomplete national academies 2019 for example the national water model which simulates flooding in 2 7 million waterways at a 250 m resolution is unable to resolve the human scale urban streams wetlands floodplains and stormwater basins that mitigate or exacerbate flooding in cities viterbo et al 2020 much less monitor flooding directly within the cities themselves decision makers infrastructure managers and citizens are not connected to adequate flood monitoring and forecasting where they arguably need it most in their backyards and streets alternative technologies provide a practical source for hydrological data and may have added benefits that traditional stream gauging does not the continuing rapid evolution of sensor network and data fusion technologies may provide a solution what cities currently lack in formal scientific infrastructure for stream and flood monitoring they make up for in an abundance of people and a rapidly increasing density and variety of sensor and communication systems that exist for a multitude of reasons such as citizen cell phones traffic cameras private and municipal webcams social media streams and increasingly sophisticated on board sensor packages in vehicles the term crowdsourcing denotes the opportunistic collection of data from otherwise autonomous dispersed sources for the purposes of this discussion we will use the term to refer collectively to novel independent data sources with a focus on public webcams social media and citizen science citizen science an extension of crowdsourcing is a collaborative science technique where professional scientists can partner with amateurs to collect and in some cases process scientific observations assumpção et al 2018 crowdsourcing allows for near real time observational data to be collected in exactly the right place at the right time i e in the exact locations where localized flooding occurs and creates impacts on people s activities to build flood monitoring and forecasting capacity in urban areas we assert that crowdsourced data could be collected integrated and merged with formal hydrological observations like stream or pipe flow gauges manual field methods rain gauges and rain radars or satellite remote sensing numerous crowdsourced data sets exist and are readily accessible including street level traffic cameras public webcams social media posts and citizen science approaches huwald et al 2015 lowry and fienen 2013 sadler et al 2018 schnebele et al 2014 crowdsourced data have shown potential to help identify flood events and promote preparedness and adaptation in disaster risk management hultquist and cervone 2020 paul et al 2017 however the capacity of crowdsourced and citizen science data to specifically support urban flood monitoring and forecasting has not been synthesized to date a first step towards answering this question is a comprehensive survey of literature and investigation of existing databases in respect to what crowdsourced data are available the nature quality and completeness of data they each provide and their potential for helping support a viable data foundation for urban flood monitoring throughout this paper we report on the feasibility of crowdsourced data integration in the context of our larger effort to develop a smart city infrastructure for managing urban flooding the following section section 2 identifies twelve qualitative and quantitative metrics to assess the capabilities of crowdsourced data for urban flood monitoring and introduces the examined crowdsourced data public webcams and traffic monitoring infrastructure social media and citizen science we structure the subsequent analysis as two separate components a literature review that expands beyond urban flood monitoring to assess the feasibility of each data source section 3 followed by presentation of case studies that investigate the feasibility of existing databases ranging in spatial and temporal scales but unified through reasonable expectation to support urban flood monitoring section 4 for public webcam and social media databases emphasis is placed on directories based in arizona notably cities within arizona are representative of other modern u s cities in terms of infrastructure and technology indicating that these technologies can be found across the country in section 5 we examine a case study beyond arizona in norfolk virginia we extend our boundary to norfolk due to the emphasis of public safety and integration of flood managers within the highlighted citizen science project showcasing a key advantage of urban flood monitoring not seen in existing projects examined in arizona in section 6 the opportunities and challenges for each data source are evaluated in sections 7 and 8 we conclude with recommendations to improve integration of the diverse crowdsource data to provide a reliable basis for urban flood monitoring and forecasting 2 identifying metrics of capability of crowdsourced data for urban flood monitoring the data types relevant and useful for flood modelling and forecasting including stage i e water level streamflow precipitation snowpack soil moisture ground temperature topography vegetation dynamics soil type and land cover data united states geological survey n d are often too coarse to capture the level of heterogeneity appropriate for urban spaces where flooding is driven by high concentrations of impervious surfaces furthermore flood forecasting in urban environments is not only an ecological and technological problem but a social one where risks vulnerabilities and priorities must be considered rosenzweig et al 2021 uusitalo et al 2015 y wang et al 2018 identified four primary drivers of inaccuracies in urban flood modelling 1 the spatial resolution of the topographic representations of terrain and urban key features 2 the lack of calibration and validation data 3 the approach used to consider the effects of underground urban drainage infrastructure drainage capacity and 4 the uncertainty of accelerated land use changes in long term modelling crowdsourced data could potentially serve to address the uncertainty posed by localized topographic and land cover variations and insufficient calibration and validation data by providing context specific observations assumpção et al 2018 uusitalo et al 2015 y wang et al 2018 three data sources public webcams social media and citizen science were chosen for analysis due to their potential for providing a large volume of observational data points at a spatially relevant urban scale and offering accessibility for immediate implementation and integration into urban flood monitoring these data sources can be characterized as either secondary where data is being opportunistically harvested from a data source established for another purpose or directed where the data source was purposefully put in place to collect water level data thus the public webcams and social media data sources are commonly secondary sources while citizen science as explored here is a directed data source we identified twelve metrics to assess the practical value of novel data sources to augment urban flood modelling accessibility format data type quantity frequency relevance density urban location durability real time and nighttime each metric is defined in supplementary materials appendix a table 1 these metrics allow us to assess the novel data sources capacity to augment urban flood monitoring throughout the survey of literature and investigation of existing databases there are occasions in which the data source is not applicable to a metric in which the metric is omitted or does not provide enough information to calculate the metric in which the value is omitted 3 crowdsourced data assessment academic literature 3 1 public webcams for the purposes of our analysis public webcams refer to any sort of video camera accessible via the internet and able to provide still images or videos of flood prone areas or drainages these include personal or municipal webcams freely accessible to the public as well as restricted access networks of cameras managed by city state or federal agencies that are accessible through negotiated access agreements for instance transportation departments across the u s host traffic cameras to monitor traffic conditions which can be impacted by vehicular accidents and weather events nearly all of these cameras capture flood related imagery in general webcam imagery combined with sophisticated automated image processing has tremendous potential as a data source allowing extraction of a wide range of data such as sensing turbidity leeuw and boss 2018 or determining vegetation phenology richardson et al 2018 the technique has also been explored in the field of hydrology with studies quantifying the accuracy of water level image processing while water stage extraction from images is a proven technology the number of studies mobilizing pre established webcams and traffic cameras is limited five studies were examined that use publicly available webcams for image processing to assess the feasibility of deploying the technology to urban flood monitoring brief descriptions of each study are available in supplementary material appendix b table 1 six metrics of capability are evaluated quantity frequency relevance density urban and accuracy quantity is defined by the total number of cameras available to the researchers within their broader study scope frequency represents the refresh rate of the webcam if an exact refresh rate is unknown qualitative indicators in the study are listed continuous is representative of real time feedback varies indicates that the cameras are refreshed at unique intervals as specified by the listed range and consistent indicates there was a reported minimum common refresh rate for all cameras relevance is the total number of webcams utilized for the study analysis within the literature review a camera may be considered irrelevant if it was of low resolution low refresh rate or down for maintenance additionally a camera was irrelevant if it did not capture the subject of interest for example castelletti et al 2016 sought to capture images of mountains therefore the camera needed an unobstructed view of a mountain to be considered density is calculated by dividing the number of relevant webcams by the area of the study region urban spaces are defined as having a population greater than 50 000 people cromartie 2019 and the urban metric is calculated as a percentage of observations that were located in an area with a population greater than 50 000 the omitted metrics include format accessibility data type location durability real time and nighttime the format for all studies was video and all streams were publicly accessible due to constraining our study to public webcams the public webcams are a secondary data source that are not designed to capture data pertaining to urban flood monitoring all webcams provided point data the real time metric is captured in the frequency metric noted as continuous durability of the cameras and nighttime data were not confirmed in the majority of studies an assessment of the capability metrics across studies is presented in table 1 which emphasizes the importance of webcams being properly positioned to provide relevant data while the presented studies expand beyond hydrology initial research utilizing private cameras have shown an average error of 1 5 cm for reading water levels from image processing eltner et al 2018 jiang et al 2020 kim et al 2008 lin et al 2018 ran et al 2016 schoener 2018 shin et al 2008 zhang et al 2019 accuracy and precision values reported by each study may be found in supplementary materials appendix c table 1 the main limitations to improved accuracy and precision throughout these hydrological studies are attributed to camera movements vibration in mounting structures and abrupt movements of the water level as well as ambient noises such as vegetation weather and variances in lighting and reflection this introduces a prevalent challenge of webcam image processing nighttime data nighttime use of public webcams has been either deemed irrelevant bothmann et al 2017 or unusable due to poor low light performance murdock et al 2013 in the gilmore et al 2013 study an inexpensive wireless surveillance camera with infrared ir lighting for night vision was used to test an image based water level measurement system this study was completed in a laboratory setting and had lower error values than the outdoor based studies previously presented it does provide a comparison for root mean squared error rmse between day and nighttime lighting while the nighttime observation rmse hovered around 0 3 cm the daytime rmse was 0 2 cm the study attributed most of the nighttime error to ir glare which they deemed correctable though further research is needed preliminary use of ir capable cameras for nighttime water level appears to be plausible the challenge may be addressed by sufficient nighttime lighting which is available in many urban locations 3 2 social media social media are virtual platforms that allow individuals to share information with other platform members including text messages photos videos and links to other online information social media have two potential roles in urban flood monitoring 1 they can serve as a directed or secondary data source to inform flood managers of flooding and 2 the platforms can be used to notify people of potential flooding hazards to explore the feasibility of utilizing social media to collect flood related observations academic articles assessing the ability of various social media platforms to gather data from flooding events around the world were reviewed most of these articles sampled high volumes of data from flood events in cities such as newcastle upon tyne uk and dresden germany fohringer et al 2015 smith et al 2015 descriptions of each article can be found in supplementary material appendix b table 2 based on the information provided by the articles we established seven metrics of data source quality and utility as a framework for our evaluation quantity frequency relevance density urban location and durability quantity relevance density urban location and durability are calculated following the same procedure presented for webcams but using a unit of posts where applicable relevancy for these articles varied by author a few factors considered were context in which keywords e g flood were used general location and time to explore a specific precipitation event and georeferencing frequency is reported by the quantity of posts divided by the number of event days location is a categorical variable for spatial scale depicting if the webcams provided point neighborhood or city georeferencing durability indicates the number of years the webcam directory has been active the five omitted metrics included format accessibility data type real time and nighttime data while the analyzed platform gives indication of format e g flickr primarily consists of image posts the studies did not typically report a breakdown for mix media platforms such as twitter the analyzed social media platforms of each study are listed in the descriptions due to being a secondary source urban flood monitoring data was not intended to be captured by the platforms data accessibility of various social media platforms is explored in section 4 2 and the data accessibility of these particular case studies are irrelevant the use of nighttime data was not distinguished by the presented studies and therefore excluded all studies reported real time data availability through twitter except for rosser et al 2017 which used flickr data furthermore jongman et al 2015 paired twitter data with satellite data and fohringer et al 2015 paired twitter data with flickr data upon reviewing these metrics in academic literature table 2 we see social media provides an opportunity to collect large quantities of real time information but georeferencing yields a significant challenge to obtain relevant data the use of social media as a means of quantitative data collection can introduce problems as social media users may not collect the most accurate or most quantitatively viable data fohringer et al 2015 however de albuquerque et al 2015 found that a social media user s physical proximity to flood affected zones significantly increased the likelihood of flood related social media observations on twitter a further obstacle to extraction of real time flood stage data from social media of any sort is that geo location of posts on these platforms is highly limited although some of the platforms provide an option for users to geo located their posts with exact gps coordinates most users decline to do so meaning that posts are at best roughly located by the social media platform e g north phoenix in other cases no geo location information is accessible at all this in combination with the uncertain accuracy of user reported flooding mentioned earlier means that social media data sources may be best utilized to simply highlight areas where flooding is occurring rather than to drive numerical simulations 3 3 citizen science citizen science projects were reviewed for applicability and feasibility by asking 1 does the project address hydrological processes in an urban region 2 is the project sustainable in the sense that it is growing or maintaining participation over a significant span of time a growing number of citizen science projects focused on hydrological data collection have been reported in the academic literature reviewed a brief description of each of the reviewed articles can be found in supplementary material appendix b table 3 we defined eight metrics to characterize the capability of these citizen science efforts to aid urban flood monitoring format quantity frequency relevance density urban location durability and real time all metrics were measured by the same procedures used for the social media data sources format represents the media of data collected and is listed as either text photo or video the addition of real time classification is denoted as yes 5 min no 5 min or capable indicating the project has the capacity to report real time data but was not monitored in this manner middleton et al 2014 the omitted metrics included accessibility data type and nighttime data the challenges of accessibility and data type are further discussed later in the paper nighttime data was not explicitly explored by the authors through this review citizen science projects showed the capacity to collect highly relevant data in urban settings on a fine spatial scale table 3 though studies evaluating the accuracy and precision of water levels reported by citizen science projects are limited supplementary material appendix c table 2 they indicate promising results for flooding applications mazzoleni et al 2015 more specifically research reviewing pluvial flooding found that the success of citizen science posts to simply locate inundated areas ranged from 72 to 95 see 2019 the future involvement of citizen science in hydrologic studies is dependent on the further analysis and quantification of accuracy of these observations it is also dependent upon the participation of citizens and their ability to capture precipitation events 4 crowdsourced data assessment existing databases to examine the capability of crowdsourced data we identified catalogued and characterized multiple databases for each data source we focused our inquiry to databases that 1 collected data in urban settings and 2 provided accessible data for collection for public webcams we examined directories on a global national and local arizona scale for social media we examined major platforms and collected data for storm events in arizona for citizen science projects we were able to focus our collected directories toward projects reporting stage or streamflow data a constraint that was not feasible for the first two secondary data sources this allowed for expansion beyond arizona at a local level in instances where a large number of observations were present for a single source e g facebook twitter a random sample of observations was used with a 90 confidence level and 10 margin of error to evaluate metrics for all other data sets metrics were evaluated through all available observations on the date accessed the following sub sections assess the crowdsourced data by the same metrics presented in the literature review 4 1 public webcams to supplement the literature review several webcam directories were identified to assess relevancy and feasibility of public webcams to complement urban flood monitoring supplementary material appendix b table 4 the public webcam sources had to be readily accessible online and provide near real time observations furthermore the directories were assessed for either quantity providing a high volume of spatially diverse data or quality high quality data for specific regions to be considered priority was given to data sources in the region of interest to our project specifically arizona directories were excluded if the majority of cameras were either inoperable or redundant with other more accessible directories in total seven directories were evaluated assessing accessibility format quantity density urban location durability and lag time quantity density and urban were calculated in the same manner as presented in the literature review for public webcams data accessibility was assessed by whether the source is publicly and freely available a value of no in this column would not absolutely exclude a data source but indicates a negotiated data access agreement may be required a varies value indicates that accessible data is limited format is a categorical variable listing whether the webcam provided photo or video durability indicates the number of years the webcam directory has been active from the project start date through 2018 lastly lag time is a categorical variable for whether or not the directory provided real time data the associated numerical value indicates the time in minutes between the real time event and posting of the image the omitted variables include data type relevance and nighttime data type and relevance are omitted due to the secondary source nature of the directories all webcam directories had cameras filming at night the analysis of these metrics across directories is shown in table 4 which again emphasizes the importance of webcams being properly positioned to provide relevant data but also shows the capacity to capture real time data 4 2 social media to complement the literature review large social media platforms including facebook youtube instagram google images twitter flickr vimeo and snapchat were considered for additional analysis due to the high volume of user engagement on each platform and accessible high quality image data supplementary materials appendix b table 5 other sites including reddit yelp pinterest and tumblr were not included in the analysis due to perceived low proportions of relevant data per keyword search as well as minimal search tool efficiency for finding real time flood related posts snapchat was ultimately omitted from analysis because public posts are only available for 24 h and search tool capacities are minimal all selected sources were manually assessed through a sample of relevant posts using keywords or hashtags for flood and flooding for social media relevance was defined based on posts falling within the search region of phoenix az and able to provide stage data or at very least presence of flooding during the flood events on october 7th and 13th in 2018 if a search returned more than one hundred posts results were filtered to the past five years and sampled in total seven social media platforms were evaluated assessing quantity frequency relevance density and location these metrics were calculated by the previously presented methods accessibility data type urban durability real time and nighttime metrics were excluded section 6 2 discusses the constraints of accessibility for social media platforms with an exploration of twitter data type is omitted as social media is a secondary data source however we examined posts with stage data or presence of flooding as stated in the relevancy metric the urban metric was excluded as all analyses were conducted in phoenix az i e urban is 100 in all cases all social media platforms are well established and considered durable data sources most data were provided near real time or had the capacity to be real time where the user upload was the limiting factor nighttime data was not distinguished the inspection of social media platforms conveyed similar results of the academic literature social media provides a large quantity of posts but not fine scale data table 5 4 3 citizen science to further assess the potential availability of citizen science projects as data sources for large scale hydrological monitoring efforts we identified catalogued and characterized ongoing projects projects were identified by searching a variety of citizen science community directories including citizenscience gov scistarter org citsci org arizona state university and environmental protection agency citizen science directories selected projects were then evaluated to assess the availability of hydrological data types and feasibility to supplement real time urban flood monitoring this query resulted in 19 citizen science projects that had 1 stage or streamflow data readily accessible online and 2 more than two hundred observations posted supplementary material appendix b table 6 fig 2 displays the thirteen hydrological data types collected across these projects the 19 citizen science projects were evaluated on accessibility format quantity frequency urban location durability real time and nighttime metrics all metrics were calculated in the manners previously listed for citizen science projects density was omitted as the spatial scale varied significantly from cities to streams with unclear boundaries all posts were relevant due to the constraints placed on the identified projects the nighttime metric was calculated by the percentage of observations posted 30 min after sunset to 30 min before sunrise for the respective location seven of these projects appear iseechange waze crowdhydrology rifls az water watch and idah2o had every observation evaluated by a member of our team to assess the metrics a random sample was used for analysis of the remaining projects the projects are categorized by those providing real time data defined as within 5 min table 6 and those that do not table 7 however it should be noted that azt app and stream tracker likely have the capacity to provide real time data additionally of the 19 projects seven require training for citizens to participate these include rifls springs online idah2o texas stream team utah water watch vsmp and missouri stream team the analysis of these metrics across directories emphasizes the degree of relevancy citizen science projects can have for urban flood monitoring with potentially high resolution 5 example of crowdsourced data and public safety integration while we have focused on crowdsourced data available in arizona we now highlight norfolk virginia a city that has integrated urban flood monitoring and public safety norfolk is located in the hampton roads region with a metropolitan area of 250 km2 housing more than 1 5 million residents and the world s largest naval station this region experiences a humid subtropical climate and has been experiencing an increased frequency of flooding due to low relief regional land subsidence and sea level rise burgos et al 2018 sweet and park 2014 the city of norfolk has been actively adopting strategies to become more resilient and was selected as one of the rockefeller foundation s 100 resilient cities in the world 100 resilient cities 2019 in 2018 rain events on august 11th 9 4 cm of precipitation and august 20th 5 2 cm of precipitation were explored to assess the quantity and density of crowdsourced data through two citizen science projects city of norfolk s system to track organize record and map storm which is used by city employees and citizens to record incidents that occur during inclement weather and waze a crowdsourced navigation application owned by google where waze users can submit reports regarding real time street conditions including flood reports our analysis of the crowdsourced data returned the following results fig 3 storm was searched using keywords flooded flooded street and flooded underpass and yielded 2 5 observations per day equating to a density of 1 observation per 99 88 km2 for the waze dataset a keyword search of hazard weather flood produced 67 relevant observations per day corresponding to a density of 1 post per 3 73 km2 the storm citizen science project observations provide high quality information reported or reviewed by trained officials at relevant sites while storm data reporting is restricted waze allows a larger group of individuals i e their users to make instantaneous flood reports again beyond density the spatial accuracy and measurement error of the observations is critical for urban flood monitoring and waze users are not trained to report flood data however waze holds the potential to facilitate real time flood warnings praharaj et al 2021 and a partnership has been established with norfolk through the waze connected citizens program the city of norfolk 2018 storm does provide a unique characteristic that supports a core motivation behind this study the ability to report flood hazards and damages and alert people of potential dangers in their communities storm observations publicly report damages such flooded streets flooded underpasses debris blocked streets downed powerlines or telephone poles waste water issues or damaged trees sadler et al 2018 arcgis 6 assessing the tradeoffs of crowdsourced data 6 1 public webcams the literature and database analysis revealed that the use of public webcams combined with subsequent image processing to collect data provides a number of challenges and opportunities images from webcams may not provide reliable data due to poor image quality highly variable viewing angles framing and distance to target unannounced downtime or discontinuation of service bothmann et al 2017 castelletti et al 2016 guastella and smith 2014 morris et al 2013 murdock et al 2013 because flood managers are a secondary user of the webcam stream they do not have direct control over the positioning of the webcams for data collection bothmann et al 2017 guastella and smith 2014 morris et al 2013 for instance many weather underground cameras are pointed toward the sky which would not capture water levels but users could potentially reposition cameras during storm events traffic cameras such as found in the arizona department of transportation adot database are often pointed at interstates which are designed more robustly than local roads and may not experience the same flooding risk there is potential for flood managers to be granted access to control traffic cameras where the flood manager could reposition the camera during storm events to potentially monitor underpasses or redirect toward a water level gauge potential sources of data errors include processing errors due to noise bothmann et al 2017 castelletti et al 2016 murdock et al 2013 or incorrect geo location metadata or timestamps morris et al 2013 murdock et al 2013 in the database analysis many sources did not provide exact locations of webcams earthcam weather underground usgs adot traffic cameras mohave flood district while this information could be collected manually this would be a resource sink lastly flood managers should be wary of sparsely or unevenly distributed webcams which could lead to extrapolation errors murdock et al 2013 as seen with the low density numbers reported in the existing database analysis however the benefits may outweigh the challenges considering the large spatial and real time temporal scales in which webcams are capable of operating bothmann et al 2017 castelletti et al 2016 guastella and smith 2014 morris et al 2013 murdock et al 2013 and access to large datasets can minimize the impacts of such errors murdock et al 2013 every database examined operated in real time although there may be a slight posting delay furthermore there are now individual cities with millions of webcams so a large dataset is a realistic possibility the two sources highlighted prior for potentially high accessibility weather underground and adot traffic cameras showed higher density and urban applicably than other databases examined indicating strong contenders for pilot programs when relevant webcams are identified they provide an objective and consistent data source that relies on little human involvement guastella and smith 2014 morris et al 2013 and can be processed with high accuracy bothmann et al 2017 castelletti et al 2016 morris et al 2013 the analyzed databases also showed promising durability with the minimum active years reported as six in short public webcam directories provide low cost or free resources to supplement traditional monitoring networks which are currently strained castelletti et al 2016 guastella and smith 2014 morris et al 2013 6 2 social media social media platforms deliver both opportunities and challenges for urban flood monitoring while social media provides a high volume stream of low cost real time data that may complement other data sources social media also provides an opportunity to capture a flood of any size as well as provide a communication tool in emergency situations that is readily accessible to people jongman et al 2015 middleton et al 2014 smith et al 2015 this accessibility is a notable advantage compared to citizen science projects such as storm that may lack awareness or adequate mobile compatibility machine processed validation techniques provide an opportunity for scalability that is not possible through manual validation techniques de bruijn et al 2018 herfort et al 2014 but this requires posts to have quality data attached fohringer et al 2015 smith et al 2015 due to the individuality of social media posting the data quality also varies widely as seen in the database investigation which reported text photo and video posts across platforms therefore it may be worthwhile to weight the knowledge of a user or number of post shares jongman et al 2015 smith et al 2015 for instance a city water department s post may be more valuable than an average citizen sophisticated algorithms and queries are needed to filter posts according to language and identify spatial scale based on location names arthur et al 2018 de bruijn et al 2018 fohringer et al 2015 herfort et al 2014 jongman et al 2015 middleton et al 2014 rosser et al 2017 smith et al 2015 to elaborate many social media posts lack high resolution georeferencing in our existing database analysis none of the posts were able to be pinned to a point location examining the various social media platforms the twitter application programming interface api is perhaps the most useful and accessible allowing extensive searching and filtering of results using one of three access tiers offering differing levels of search functionality a standard access tier that is free and premium and enterprise levels of paid access free access is limited to searching the last seven days of activity at rates up to 180 queries per 15 min window premium level allows searching both 30 day and complete archives of tweets as well as higher query rates enterprise access adds more powerful search criteria specification in practice we found the standard free api to be adequate for our exploration of tracking flood related twitter activity in real time where very high query rates and searching of historical tweet archives are not relevant queries could be organized and spaced out temporally to avoid exceeding access rate limits while still providing a stream of near real time data twitter allows complex logical queries to be formulated to retrieve and filter posts from specified geographic regions for instance one could search for tweets mentioning flooding but also having other keywords like water and storm to eliminate metaphorical uses of the worded flooding or flooded thus some careful effort could likely lead to development of effective query probes that do indeed relate to flooding as noted for all social media earlier twitter users rarely allow their posts to be tagged with detailed geo locations for instance we found that less than 1 of tweets found containing the words flood flooding or inundation have location data attached thus tweets can be geo located only to the city level or for larger cities to a general region of the city even so tweets might be useful as a general indicator of flooding activity to complement sparse public webcams coverage of an area or to help understand overall flooding dynamics in planning placement of webcams a preliminary analysis examined twitter activity around sixteen cities between november 15 2019 through january 20 2020 to explore how tweets may or may not correspond with storm and flooding activity and potential correlation was indicated to better understand these initial findings we built a software tool to mine twitter for targeted cities and to graph the resulting tweets in a tweetograph a social media analogy to a hydrograph fig 4 the methods of this preliminary analysis may be viewed in more detail in supplementary materials appendix d as phase 1 and phase 2 respectively we present the tweetographs below specifically to 1 provide proof of concept of social media data mining algorithms and 2 illustrate potential temporal correlation between social media activity and flooding from the preliminary analysis this utility highlights a pathway to integrate social media data into urban flood monitoring 6 3 citizen science integrating citizen science data sources into hydrological monitoring and modelling efforts holds promise due to the nature of direct data sources but also presents significant challenges in terms of accuracy reliability and coverage if citizen scientists are not trained there may be issues of quality due to unidentifiable observations e g shorthand blurry images etc or poor technique in taking hydrologic measurements le coz et al 2016 shupe 2017 smith and rodriguez 2017 r q wang et al 2018 weeser et al 2018 yu et al 2016 less than half of the examined citizen science projects trained users on data collection techniques coverage and reliability are often problematic as citizen science projects typically lack large networks leading to a low volume of irregular participation le coz et al 2016 shupe 2017 smith and rodriguez 2017 additionally inclement weather reasonably causes citizen scientists to stay indoors and away from active hydrological situations at the precise time when they are needed most asking citizen scientists to venture out in potentially unsafe flood conditions raises ethical and liability concerns le coz et al 2016 as mentioned in the norfolk case study training requirements also create an entry barrier which may reduce observations awkward data formats constraints on mobile data access and a limited feedback of high value information to contributing citizen scientists can limit interest and long term community engagement le coz et al 2016 smith and rodriguez 2017 r q wang et al 2018 yu et al 2016 these barriers will need to be addressed to adopt citizen science data into urban flood monitoring despite these challenges citizen science can offer tremendous opportunity for citizen engagement le coz et al 2016 shupe 2017 weeser et al 2018 observations can be reported real time and supported with text photos and videos providing low cost data the database analysis also showed that observations were often detailed reporting fine spatial scale and occasionally nighttime data furthermore citizen science projects allow the community to co generate knowledge in high traffic areas or spaces that are a priority to the community le coz et al 2016 naik 2016 shupe 2017 smith and rodriguez 2017 r q wang et al 2018 weeser et al 2018 projects with fixed gauges placed at key points within the cityscape e g crowdhydrology can provide long term data for a fixed reference point le coz et al 2016 lowry et al 2019 lowry and fienen 2013 r q wang et al 2018 weeser et al 2018 a couple databases missouri stream team and tx stream team have existed for 30 and 28 years respectively emphasizing the long term data source potential the integration of social media with citizen science can increase participation and awareness while further supplementing data gaps le coz et al 2016 r q wang et al 2018 projects based around events whether sampling blitzes or coastal tides typically have higher number of observations loftis et al 2019 despite its limits the public engagement of citizen science provides a notable opportunity to further educate and empower residents while potentially increasing project observations 6 4 crowdsourced data a summary of qualitative and quantitative metrics across academic literature and established databases for the three established data sources table 8 reveals the potential for integrating observational data from crowdsourcing and citizen science projects into urban flood monitoring public webcam networks have been around the longest 1990s while citizen science networks and social media sources primarily started within the following decade 2000s in general all three sources are relatively well established and could be described as durable over time indicating they are viable for long term data collection furthermore all three sources provide a wide variety of data that could supplement ongoing monitoring efforts with the continuous improvement of video and image processing utilizing public webcam and social media sources is becoming more plausible for relevant flood data such as stage streamflow precipitation snowpack weather topography vegetation dynamics land cover land use water use and informing gis data it remains necessary to have specialized camera setups or citizen science projects for anything more difficult to extrapolate beyond the visuals of videos and images such as soil moisture ground temperature water quality and groundwater data lastly there is concern about a lack of useful nighttime data across the data sources though specialized camera setups with nighttime lighting or infrared capabilities can provide nighttime data most public webcam networks do not have these capabilities while some citizen science projects reported nighttime data most did not notably the iseechange project yielded roughly a quarter of observations overnight but these were typically not reported until the following day social media is the most promising avenue for crowdsourcing nighttime flooding data as individuals may report local flooding 7 discussion each of the three data sources have unique opportunities and challenges to support urban flood monitoring social media and public webcams produce a large quantity of observations across a widespread area providing a frequent real time snapshot of water levels during urban flooding that is not captured by citizen science projects citizen science projects have an advantage of nearly all observations being relevant for water level measurements leading citizen science projects to provide better baseline data than event data for urban flood monitoring social media and public webcams may serve better providing real time data for flood events however both data sources have a high number of non relevant observations to address this multiple query probes could be configured to explore different filtering criteria recalling the twitter example fig 4 these queries could be used to discover a to what extent twitter activity tracks precipitation and associated urban flooding and if so b which content based logical filters yielded tweetographs with the best correlations to precipitation and flooding as might be expected our initial trials suggest that success will hinge on development of finely tuned query probes that accurately return tweets truly related to flooding activity at that point further analysis can determine how reliably tweeting does or does not fundamentally signal flooding events despite existing weaknesses across data sources these crowdsourced data provide a chance to augment urban flood monitoring using data fusion to exploit their combined and relative strengths the integration of primary observations into traditional urban flood modelling allows data to be collected where it is needed most at flooded locations in cities we propose an integrated flood stage observation network that fuses crowdsourced and professional hydrology data in real time to feed that data to modelers and communicate flood status and risk to community members and decision makers 8 conclusion opportunities and challenges exist for integration of each data source public webcams social media and citizen science into urban flood monitoring public webcams have the highest potential when located at frequently flooded or high risk locations webcams provide an opportunity for low cost reliable real time data and house potential for reasonably accurate nighttime observations through ir capable cameras public webcams are limited by difficult directory navigation and fixed locations camera views may be impaired by poor angles long distances obstructions and network problems when aided by a streetlight the webcam may be able to provide nighttime data next the greatest asset of social media is the high quantity of observations that provide free real time data that are typically not constrained to a specific location the drawbacks are the low number of relevant posts low quality media and lack of georeferenced observations social media provides ample opportunity for easy integration with other data collection methods to supplement observations lastly citizen science delivers detailed quality data through community engagement with low cost technologies such as smartphones however engagement is generally low outside of campaigned events and participation falls further during inclement weather such as flooding events there are barriers to data collection as not all projects publicly post data or the data may be in difficult formats citizen science s largest strength is the detail of information provided for each observation crowdsourced data introduces a layer of uncertainty of measurement errors it remains difficult to organize data gathered via crowdsourcing into real time monitoring systems due to inconsistent metrics uncoordinated efforts and to get enough georeferenced ground based observations to calibrate models despite the presented challenges each data source holds potential to enhance urban flood monitoring by providing access to real time localized flood reports especially if fused together to exploit their relative and combined strengths and to create a more complete and up to date data source for urban flooding declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests mikhail chester reports financial support was provided by national science foundation award no 1831475 jonathan l goodall reports financial support was provided by national science foundation award no 1735587 all authors are part of the nsf award no 1831475 but mikhail is the lead pi jonathan and faria are associated with nsf award no 1735587 with jon having the seniority of the two acknowledgments this project is funded by the national science foundation s smart and connected communities program 1831475 and critical resilient interdependent infrastructure systems and processes program 1735587 appendix a table 1 crowdsourcing and citizen science data source assessment metrics 12 metrics are identified to assess the capability of the novel data sources to augment urban flood modelling table 1 metric description unit accessibility can the observation be publicly downloaded for free binary classified as either yes or no format in what format is the observation communicated classified as either text photo or video data type what data types do sources provide percent of the data source with stage streamflow precipitation snowpack weather soil moisture ground temperature topography vegetation dynamics land cover land use water quality water use groundwater quality level and gis data quantity how many total observations total number of posts frequency how many observations per a temporal boundary posts day or event relevance what percentage of sources or posts are relevant for informing flood modelling and decision making number of posts that depict flooding within the spatial and temporal boundaries density how many observations per a spatial boundary posts km2 urban are urban observations supported percent of observations in locations that have a population 50 000 people cromartie 2019 location1 is the location of the observation provided if so are accuracy and precision maintained classified as point has latitude and longitude or the precise building location name neighborhood tagged by neighborhood name or to an area approximately the size of a neighborhood or city durability how long have observations been hosted total number of active years rounded up if less than one year input one year real time are real time observations supported if so are accuracy and precision is precision maintained binary classified as either yes or no by whether the posting delay exceeds 5 min if available lag time is noted nighttime are nighttime observations supported if so are accuracy and precision maintained percent of observations that provided stage data between 30 min after sunset and 30 min before sunrise 1the locations of observations are either directly geotagged visualized displayed on a map or described with text appendix b data sources table 1 brief summaries of academic literature regarding public webcams table 1 article description bothmann et al 2017 webcams from amos were used for vegetation greenness levels usa europe castelletti et al 2016 touristic webcams complemented with data from flickr used for flood prediction of a snow catchment lake como italy guastella and smith 2014 touristic webcams used for assessing changes in coastal morphology kwazulu natal south africa morris et al 2013 traffic webcams used for vegetation greenness levels england uk murdock et al 2013 outdoor webcams from the amos collection used to create cloud maps for weather forecasting usa table 2 brief summaries of academic literature regarding social media table 2 article description herfort et al 2014 observational flooding data recorded between june 8th 1 30 p m to june 10th 2013 midnight from geo referenced twitter data from river elbe flood germany 2013 de bruijn et al 2018 used an event based algorithm to sort through location ambiguity of tweets for flooding disasters between july 29 2014 and july 18 2017 global rosser et al 2017 collected geotagged social media flickr flood data for a series of storms between january 5th to 11th in 2014 to create a flooding model uk jongman et al 2015 analyzed twitter and satellite source data to determine how they are used to support flood responses to a tropical depression and typhoon respectively pakistan single day event and the philippines multiple day event middleton et al 2014 review and case study of social media s use in response data hurricane sandy monitored for five days in october 2012 fohringer et al 2015 case study of flood patterns and depths data collected through photos on twitter and flickr from may 5 to june 21 2013 flood dresden germany smith et al 2015 modeled and assessed two flooding events in 2012 on june 28th and august 5th newcastle upon tyne uk arthur et al 2018 examined twitter data from floods in 2015 from october 22 to november 25 uk table 3 brief summaries of academic literature regarding citizen science projects table 3 article description le coz et al 2016 flood photo project riskscape by the national institute of water and atmospheric research niwa christchurch new zealand shupe 2017 water quality data and hydrologic observations collected via a smartphone app vancouver canada smith and rodriguez 2017 flood observations recorded by new york city 311 online or with the non emergency city telephone line new york city new york usa r q wang et al 2018 study of the web and mobile crowd sourcing platform mycoast that collects photos of coastal urban flooding charleston sc usa weeser et al 2018 analysis of numerical text message water level data collected from thirteen sites kenya yu et al 2016 comparison of crowdsourced and model predicted flood inundation for an extreme one day storm event shanghai china sadler et al 2018 developed flood severity model using crowdsourced flood reports for coastal city norfolk virginia usa loftis et al 2019 crowdsourced flood mapping was used to identify the inundation extent during king tide events and validate flood forecast model hampton roads virginia usa naik 2016 precipitation event paralyzed a city resulting in the creation of an impromptu crowdsourced flood map chennai india table 4 public webcam resources a summary of each public webcam directory examined table 4 resource description global earthcam a global commercial network of live stream webcams for online tourism phenocam a database of landscape images in north america for phenological research weather underground a global commercial service that provides real time weather data derived from the national weather service and personal weather stations weatherbug a global web and mobile application that provides hyper local weather data from private weather stations and sensors national united states geological survey usgs a national multimedia gallery including webcams which provides near real time conditions to monitor extreme weather events local arizona department of transportation a state wide database of traffic cameras that provide near real time road and traffic conditions arizona mohave flood district a county specific database of real time weather cameras to provide flood warnings table 5 social media resources a summary of each social media platform examined table 5 resource description facebook a global social networking site where users may post text photos and video to a story i e content posted up to 24 h or to their feed e g content posted until deleted users may also interact with content through reactions sharing or comments flickr a global image and video hosting service where users may caption react and comment on uploads google images a global image searching service instagram a global social networking site where users may post images or videos to a story or feed with the ability to caption react share and comment on uploads twitter a global social networking site where users may post text photos and videos to a feed users may interact with content by replying sharing or reacting vimeo a global video hosting service where users may caption react and comment on uploads youtube a global video hosting service and social networking site where users may post text photos and video to a story or feed users may interact with content through reactions and comments to comply with permissions google images was limited to results labeled for noncommercial reuse table 6 citizen science resources a summary of each citizen science project examined table 6 resource description global appear a global mobile and web based application that crowdsources environmental data for freshwater aquatic environments for research and education crowdwater a global mobile application which crowdsources water level streamflow soil moisture plastic pollution data of water bodies to improve forecasting of hydrological events digital earth watch picture post a global mobile and web based database which crowdsources images of landscapes for environmental monitoring the fluker post project a global mobile application which crowdsources images of landscapes for long term natural resource management iseechange a global mobile and web based application that crowdsources examples of climate change mitigation and adaptation stream tracker project a global mobile application that crowdsources when and where water is flowing to monitor intermittent streams waze a global mobile application that crowdsources road and traffic conditions to provide trip navigation national crowdhydrology a national web based application which crowdsources water level data primarily in or near urban locations through text messaging to collect hydrologic data springs online a national web based database that crowdsources ecosystem characteristics and processes to promote healthy environments what s your water level a national web based application that crowdsources water level data focused on coastal flood management local arizona trail water report a state level web based database that crowdsources availability of water sources across arizona trails arizona water watch a state level mobile application which crowdsources water wildlife and pollution data of water bodies in arizona idah2o a state level web based database that crowdsources habitat biological chemical and physical data of idaho s streams michigan clean water corps volunteer stream monitoring program vsmp a state level web based database that crowdsources water quality data in michigan s wadeable streams and rivers for water resources management and protection missouri stream team a state level mobile and web based database that crowdsources water quality data in missouri for river conservation massachusetts river instream flow stewards rifls a state level web based database which crowdsources streamflow of ungauged rivers in massachusetts storm a city level web based application that crowdsources flooded streets in norfolk va texas stream team a state level web based database that crowdsources water and environmental quality of over 400 texas waterways for scientific research and environmental stewardship utah water watch a state level web based database which crowdsources water quality data of lakes and streams in utah for watershed management and education appendix c reported water level errors across data sources table 1 accuracy and precision of water level derived from webcam image and video studies of cameras indicate webcams hold potential to accurately measure water level data table 1 source error cm error errormethod precision cm jiang et al 2020 2 6 20 rmse zhang et al 2019 0 4 rmse 1 lin et al 2018 1 1 1 rmse eltner et al 2018 0 6 mae 1 5 schoener 2018 1 4 mae 3 ran et al 2016 2 rmse kim et al 2008 2 5 8 mae shin et al 2008 2 3 mae average 1 5 rmse root mean square error mae mean absolute error table 2 accuracy of water level derived from citizen science sources studies regarding citizen science projects are limited but indicate potential for retrieving accurate water level data table 2 source error cm error errormethod davids et al 2017 1 9 mae le coz et al 2016 15 20 mae lowry and fienen 2013 0 6 rmse average 0 6 9 7 rmse root mean square error mae mean absolute error appendix d detailed methods for twitter analysis our analysis of twitter as a potential near real time indicator of urban flooding activity was conducted in two phases first a simple initial pilot effort was manually conducted to get an initial assessment of potential utility second a more in depth analysis ongoing for which we constructed a specialized software twitter scraper tool with significant automation that could be targeted to monitor flood related twitter activity in any targeted locale over either specific timeframes or on an ongoing basis phase 1 pilot evaluation of twitter as an indicator of urban flooding using the public free twitter api searches were made using the keywords flood flooding and inundation these searches were made from november 15 2019 to january 20 2020 and were localized to several locations in the united states which were known to be at risk for flooding to reduce noise the following locations were used o pahrump nevada o jersey city new jersey o plano texas o peachtree corners georgia o miramar beach florida o raleigh north carolina o rogers city arkansas o carmel indiana o st augustine florida o the woodlands texas o round rock texas o denton texas o kissimmee florida o san marcos texas o sugar land texas o queens new york searches were made for tweets which were geo located within the boundaries of these locations or for tweets where the author states their location in their profile to be within one of these locations events were closely monitored then verified through local news outlets the following events were captured o pahrump nevada o december 12 2019 to december 15 2019 o plano texas o january 16 2020 o peachtree corners georgia o january 12 2020 to january 16 2020 o raleigh north carolina o december 5 2019 to december 6 2019 o december 13 2019 to december 15 2019 o december 18 2019 to december 20 2019 o rogers city arkansas o january 9 2020 to january 14 2020 o carmel indiana o january 9 2020 to january 12 2020 o denton texas o january 10 2020 to january 13 2020 o queens new york o january 2 2020 to january 5 2020 events analyzed in this way showed an increase of up to four times the number of tweets that matched search criteria during and shortly after flooding events these preliminary results were deemed promising enough to warrant initiating a deeper analysis of the reliability of twitter as a flooding indicator phase 2 software for automation large scale analysis of twitter as reliable indicator of urban flooding to provide a basis for a larger statistical analysis of the reliability of twitter activity as an indicator of urban flooding events we constructed of an automated twitter scraper software product to use as a research tool capable of monitoring and capturing twitter activity and precipitation in flood prone cities across the united states the tool is embodied in a web application that allows researchers to easily configure monitoring of any city either continuously or within specified time windows this allows continuous long term monitoring to provide a strong record of baseline twitter activity in various regions as well as targeted monitoring of specific events e g when a large storm is forecast for a given region some specific features include user can create and deploy an unlimited number of monitoring tasks targeted to specified geographic regions user can specify an unlimited set of search term sets which are essentially complex constructed logical queries aimed at filtering out flood related tweets from the stream of tweets o the probes used in the provided examples used flooding flood or flooded or flooding flood with flooded with flood of rainstorm monsoon or rain or rainstorm or thunderstorm for a given monitoring task user can attach one or more search term sets to use in monitoring that region monitoring tasks also query noaa in 15 min intervals to collect a timeline of current precipitation data for the targeted region a graphical interface is provided to visualize twitter activity in other words the number of tweets returned for each of the specified search term sets along with precipitation data can be generated screenshots of the tool in action are given in fig 4 in the main body of the paper we note that while the tool can technically support unlimited monitoring tasks and search term sets a practical limit is set by the api access level used to access twitter as outlined in the main paper thus for example the standard free access level is limited to 180 queries per 15 min rolling window with this quota spread across all active monitoring tasks and their respective search term sets the web application is based on the django v2 1 7 application framework this application and the various backend modules that do the actual web scraping are written in python v3 6 9 the tool is supported by a postgresql v10 12 database the webapp stores all monitoring tasks and logs status information to the database and the results of all twitter queries returned by all monitoring tasks are stored in the database as well this approach overcomes the limitations to archival tweets imposed by twitter we collect all relevant tweets in real time and essentially create our own historical archive in the database the script uses the tweepy v3 9 python package to access the twitter api and directly accesses the openweathermap v2 5 api to collect precipitation data at the time of this writing long term data collection is still running for a number of u s cities we are also setting targeted monitoring of select cities where heavy precipitation and or flood warnings are issued although detailed results await completion of data collection and more careful statistical analysis our early observations suggest that twitter may not be as reliable as our pilot effort led us to hope as an early indicator of urban flooding correlations between precipitation flooding and flood related tweets do often appear but are often obscured or confused by variations in baseline twitter activity in some locales more work is also required to discover more reliable filtering queries that separate tweets truly related to flooding from those that mention flooding and rain in unrelated mundane contexts a more complete report will be provided in an upcoming publication focused specifically on this topic 
25776,flooding is the most common natural hazard leading to property damage injuries and death despite the potential for major consequences urban flooding remains difficult to forecast largely due to a lack of data availability at fine spatial scales and associated predictive capabilities crowdsourcing of public webcams social media and citizen science represent potentially important data sources for obtaining fine scale hydrological data but also raise novel challenges related to data reliability and consistency we provide a review of literature and analysis of existing databases regarding the availability and quality of these unconventional sources that then drives a discussion of their potential to support fine grained urban flood modelling and prediction our review and analysis suggest that crowdsourced data are increasingly available in urban contexts and have considerable potential integration of crowdsourced data could help ameliorate quality and completeness issues in any one source yet substantial weaknesses and challenges remain to be addressed keywords urban hydrology flooding crowdsourced data data integration 1 introduction flooding is one of the most common and amongst the most damaging natural hazards in the united states national academies 2019 the vast majority of flood risks to life and property are concentrated in cities ashley and ashley 2008 cities are particularly vulnerable to pluvial flooding which occurs when high volumes of stormwater runoff exceed drainage capacity in developed areas national academies 2019 another portion of this flood vulnerability arises when urban development expands into marshes and floodplains increasing fluvial and coastal flooding vidal 2017 further complexity is added by a non stationary climate meaning the future climate cannot be predicted by historical trends chester and allenby 2018 milly et al 2008 extreme precipitation events are increasing in magnitude and in some regions increasing in frequency larsen et al 2009 minnery and smith 1996 prein et al 2017 schreider et al 2000 additionally low magnitude high frequency events may have consequences that exceed that of extreme precipitation events as seen with nuisance flooding moftakhari et al 2017 this leads to questions regarding the adequacy of stormwater infrastructure design standards and development approaches used to address flood vulnerability markolf et al 2020 zevenbergen et al 2008 consequences of urban flooding include physical and non physical direct impacts indirect impacts socioeconomic ramifications and risks to human life and health highlighting the interdependencies of infrastructure systems in urban spaces kim et al 2017 könig et al 2002 markolf et al 2019 rosenzweig et al 2021 these compounding impacts place a higher pressure on decision makers and emphasize the need for urban flood monitoring we refer to this set of decision makers as flood managers or practitioners and researchers who monitor model and manage urban flooding despite the recognized impacts urban flood monitoring and forecasting remains extremely limited and unreliable largely due to a lack of water level and streamflow data availability at the very fine spatial scales at which urban flooding occurs rosenzweig et al 2021 while the majority of precipitation data are collected in cities national oceanic and atmospheric administration noaa 2017 it is coarse scale and mostly rural river flow observations that are used to predict flooding united states geological survey n d despite the 8 000 streams that are currently gauged and operated by the united states geological survey usgs or other local agencies these groups do not have the capacity to cover fine scale urban drainage systems for instance surface drainage e g roadways or subsurface drainage e g stormwater systems or the nearly 400 000 named streams and the hundreds of thousands of undocumented streams throughout the united states wagner and eberts 2020 fig 1 from the purely practical perspectives of size cost manpower and physical access the professional grade gauging used by the usgs and local flood agencies to monitor streamflow could never be deployed at scale in an urban landscape for these reasons existing tools for urban flood monitoring modelling and warning are widely recognized as incomplete national academies 2019 for example the national water model which simulates flooding in 2 7 million waterways at a 250 m resolution is unable to resolve the human scale urban streams wetlands floodplains and stormwater basins that mitigate or exacerbate flooding in cities viterbo et al 2020 much less monitor flooding directly within the cities themselves decision makers infrastructure managers and citizens are not connected to adequate flood monitoring and forecasting where they arguably need it most in their backyards and streets alternative technologies provide a practical source for hydrological data and may have added benefits that traditional stream gauging does not the continuing rapid evolution of sensor network and data fusion technologies may provide a solution what cities currently lack in formal scientific infrastructure for stream and flood monitoring they make up for in an abundance of people and a rapidly increasing density and variety of sensor and communication systems that exist for a multitude of reasons such as citizen cell phones traffic cameras private and municipal webcams social media streams and increasingly sophisticated on board sensor packages in vehicles the term crowdsourcing denotes the opportunistic collection of data from otherwise autonomous dispersed sources for the purposes of this discussion we will use the term to refer collectively to novel independent data sources with a focus on public webcams social media and citizen science citizen science an extension of crowdsourcing is a collaborative science technique where professional scientists can partner with amateurs to collect and in some cases process scientific observations assumpção et al 2018 crowdsourcing allows for near real time observational data to be collected in exactly the right place at the right time i e in the exact locations where localized flooding occurs and creates impacts on people s activities to build flood monitoring and forecasting capacity in urban areas we assert that crowdsourced data could be collected integrated and merged with formal hydrological observations like stream or pipe flow gauges manual field methods rain gauges and rain radars or satellite remote sensing numerous crowdsourced data sets exist and are readily accessible including street level traffic cameras public webcams social media posts and citizen science approaches huwald et al 2015 lowry and fienen 2013 sadler et al 2018 schnebele et al 2014 crowdsourced data have shown potential to help identify flood events and promote preparedness and adaptation in disaster risk management hultquist and cervone 2020 paul et al 2017 however the capacity of crowdsourced and citizen science data to specifically support urban flood monitoring and forecasting has not been synthesized to date a first step towards answering this question is a comprehensive survey of literature and investigation of existing databases in respect to what crowdsourced data are available the nature quality and completeness of data they each provide and their potential for helping support a viable data foundation for urban flood monitoring throughout this paper we report on the feasibility of crowdsourced data integration in the context of our larger effort to develop a smart city infrastructure for managing urban flooding the following section section 2 identifies twelve qualitative and quantitative metrics to assess the capabilities of crowdsourced data for urban flood monitoring and introduces the examined crowdsourced data public webcams and traffic monitoring infrastructure social media and citizen science we structure the subsequent analysis as two separate components a literature review that expands beyond urban flood monitoring to assess the feasibility of each data source section 3 followed by presentation of case studies that investigate the feasibility of existing databases ranging in spatial and temporal scales but unified through reasonable expectation to support urban flood monitoring section 4 for public webcam and social media databases emphasis is placed on directories based in arizona notably cities within arizona are representative of other modern u s cities in terms of infrastructure and technology indicating that these technologies can be found across the country in section 5 we examine a case study beyond arizona in norfolk virginia we extend our boundary to norfolk due to the emphasis of public safety and integration of flood managers within the highlighted citizen science project showcasing a key advantage of urban flood monitoring not seen in existing projects examined in arizona in section 6 the opportunities and challenges for each data source are evaluated in sections 7 and 8 we conclude with recommendations to improve integration of the diverse crowdsource data to provide a reliable basis for urban flood monitoring and forecasting 2 identifying metrics of capability of crowdsourced data for urban flood monitoring the data types relevant and useful for flood modelling and forecasting including stage i e water level streamflow precipitation snowpack soil moisture ground temperature topography vegetation dynamics soil type and land cover data united states geological survey n d are often too coarse to capture the level of heterogeneity appropriate for urban spaces where flooding is driven by high concentrations of impervious surfaces furthermore flood forecasting in urban environments is not only an ecological and technological problem but a social one where risks vulnerabilities and priorities must be considered rosenzweig et al 2021 uusitalo et al 2015 y wang et al 2018 identified four primary drivers of inaccuracies in urban flood modelling 1 the spatial resolution of the topographic representations of terrain and urban key features 2 the lack of calibration and validation data 3 the approach used to consider the effects of underground urban drainage infrastructure drainage capacity and 4 the uncertainty of accelerated land use changes in long term modelling crowdsourced data could potentially serve to address the uncertainty posed by localized topographic and land cover variations and insufficient calibration and validation data by providing context specific observations assumpção et al 2018 uusitalo et al 2015 y wang et al 2018 three data sources public webcams social media and citizen science were chosen for analysis due to their potential for providing a large volume of observational data points at a spatially relevant urban scale and offering accessibility for immediate implementation and integration into urban flood monitoring these data sources can be characterized as either secondary where data is being opportunistically harvested from a data source established for another purpose or directed where the data source was purposefully put in place to collect water level data thus the public webcams and social media data sources are commonly secondary sources while citizen science as explored here is a directed data source we identified twelve metrics to assess the practical value of novel data sources to augment urban flood modelling accessibility format data type quantity frequency relevance density urban location durability real time and nighttime each metric is defined in supplementary materials appendix a table 1 these metrics allow us to assess the novel data sources capacity to augment urban flood monitoring throughout the survey of literature and investigation of existing databases there are occasions in which the data source is not applicable to a metric in which the metric is omitted or does not provide enough information to calculate the metric in which the value is omitted 3 crowdsourced data assessment academic literature 3 1 public webcams for the purposes of our analysis public webcams refer to any sort of video camera accessible via the internet and able to provide still images or videos of flood prone areas or drainages these include personal or municipal webcams freely accessible to the public as well as restricted access networks of cameras managed by city state or federal agencies that are accessible through negotiated access agreements for instance transportation departments across the u s host traffic cameras to monitor traffic conditions which can be impacted by vehicular accidents and weather events nearly all of these cameras capture flood related imagery in general webcam imagery combined with sophisticated automated image processing has tremendous potential as a data source allowing extraction of a wide range of data such as sensing turbidity leeuw and boss 2018 or determining vegetation phenology richardson et al 2018 the technique has also been explored in the field of hydrology with studies quantifying the accuracy of water level image processing while water stage extraction from images is a proven technology the number of studies mobilizing pre established webcams and traffic cameras is limited five studies were examined that use publicly available webcams for image processing to assess the feasibility of deploying the technology to urban flood monitoring brief descriptions of each study are available in supplementary material appendix b table 1 six metrics of capability are evaluated quantity frequency relevance density urban and accuracy quantity is defined by the total number of cameras available to the researchers within their broader study scope frequency represents the refresh rate of the webcam if an exact refresh rate is unknown qualitative indicators in the study are listed continuous is representative of real time feedback varies indicates that the cameras are refreshed at unique intervals as specified by the listed range and consistent indicates there was a reported minimum common refresh rate for all cameras relevance is the total number of webcams utilized for the study analysis within the literature review a camera may be considered irrelevant if it was of low resolution low refresh rate or down for maintenance additionally a camera was irrelevant if it did not capture the subject of interest for example castelletti et al 2016 sought to capture images of mountains therefore the camera needed an unobstructed view of a mountain to be considered density is calculated by dividing the number of relevant webcams by the area of the study region urban spaces are defined as having a population greater than 50 000 people cromartie 2019 and the urban metric is calculated as a percentage of observations that were located in an area with a population greater than 50 000 the omitted metrics include format accessibility data type location durability real time and nighttime the format for all studies was video and all streams were publicly accessible due to constraining our study to public webcams the public webcams are a secondary data source that are not designed to capture data pertaining to urban flood monitoring all webcams provided point data the real time metric is captured in the frequency metric noted as continuous durability of the cameras and nighttime data were not confirmed in the majority of studies an assessment of the capability metrics across studies is presented in table 1 which emphasizes the importance of webcams being properly positioned to provide relevant data while the presented studies expand beyond hydrology initial research utilizing private cameras have shown an average error of 1 5 cm for reading water levels from image processing eltner et al 2018 jiang et al 2020 kim et al 2008 lin et al 2018 ran et al 2016 schoener 2018 shin et al 2008 zhang et al 2019 accuracy and precision values reported by each study may be found in supplementary materials appendix c table 1 the main limitations to improved accuracy and precision throughout these hydrological studies are attributed to camera movements vibration in mounting structures and abrupt movements of the water level as well as ambient noises such as vegetation weather and variances in lighting and reflection this introduces a prevalent challenge of webcam image processing nighttime data nighttime use of public webcams has been either deemed irrelevant bothmann et al 2017 or unusable due to poor low light performance murdock et al 2013 in the gilmore et al 2013 study an inexpensive wireless surveillance camera with infrared ir lighting for night vision was used to test an image based water level measurement system this study was completed in a laboratory setting and had lower error values than the outdoor based studies previously presented it does provide a comparison for root mean squared error rmse between day and nighttime lighting while the nighttime observation rmse hovered around 0 3 cm the daytime rmse was 0 2 cm the study attributed most of the nighttime error to ir glare which they deemed correctable though further research is needed preliminary use of ir capable cameras for nighttime water level appears to be plausible the challenge may be addressed by sufficient nighttime lighting which is available in many urban locations 3 2 social media social media are virtual platforms that allow individuals to share information with other platform members including text messages photos videos and links to other online information social media have two potential roles in urban flood monitoring 1 they can serve as a directed or secondary data source to inform flood managers of flooding and 2 the platforms can be used to notify people of potential flooding hazards to explore the feasibility of utilizing social media to collect flood related observations academic articles assessing the ability of various social media platforms to gather data from flooding events around the world were reviewed most of these articles sampled high volumes of data from flood events in cities such as newcastle upon tyne uk and dresden germany fohringer et al 2015 smith et al 2015 descriptions of each article can be found in supplementary material appendix b table 2 based on the information provided by the articles we established seven metrics of data source quality and utility as a framework for our evaluation quantity frequency relevance density urban location and durability quantity relevance density urban location and durability are calculated following the same procedure presented for webcams but using a unit of posts where applicable relevancy for these articles varied by author a few factors considered were context in which keywords e g flood were used general location and time to explore a specific precipitation event and georeferencing frequency is reported by the quantity of posts divided by the number of event days location is a categorical variable for spatial scale depicting if the webcams provided point neighborhood or city georeferencing durability indicates the number of years the webcam directory has been active the five omitted metrics included format accessibility data type real time and nighttime data while the analyzed platform gives indication of format e g flickr primarily consists of image posts the studies did not typically report a breakdown for mix media platforms such as twitter the analyzed social media platforms of each study are listed in the descriptions due to being a secondary source urban flood monitoring data was not intended to be captured by the platforms data accessibility of various social media platforms is explored in section 4 2 and the data accessibility of these particular case studies are irrelevant the use of nighttime data was not distinguished by the presented studies and therefore excluded all studies reported real time data availability through twitter except for rosser et al 2017 which used flickr data furthermore jongman et al 2015 paired twitter data with satellite data and fohringer et al 2015 paired twitter data with flickr data upon reviewing these metrics in academic literature table 2 we see social media provides an opportunity to collect large quantities of real time information but georeferencing yields a significant challenge to obtain relevant data the use of social media as a means of quantitative data collection can introduce problems as social media users may not collect the most accurate or most quantitatively viable data fohringer et al 2015 however de albuquerque et al 2015 found that a social media user s physical proximity to flood affected zones significantly increased the likelihood of flood related social media observations on twitter a further obstacle to extraction of real time flood stage data from social media of any sort is that geo location of posts on these platforms is highly limited although some of the platforms provide an option for users to geo located their posts with exact gps coordinates most users decline to do so meaning that posts are at best roughly located by the social media platform e g north phoenix in other cases no geo location information is accessible at all this in combination with the uncertain accuracy of user reported flooding mentioned earlier means that social media data sources may be best utilized to simply highlight areas where flooding is occurring rather than to drive numerical simulations 3 3 citizen science citizen science projects were reviewed for applicability and feasibility by asking 1 does the project address hydrological processes in an urban region 2 is the project sustainable in the sense that it is growing or maintaining participation over a significant span of time a growing number of citizen science projects focused on hydrological data collection have been reported in the academic literature reviewed a brief description of each of the reviewed articles can be found in supplementary material appendix b table 3 we defined eight metrics to characterize the capability of these citizen science efforts to aid urban flood monitoring format quantity frequency relevance density urban location durability and real time all metrics were measured by the same procedures used for the social media data sources format represents the media of data collected and is listed as either text photo or video the addition of real time classification is denoted as yes 5 min no 5 min or capable indicating the project has the capacity to report real time data but was not monitored in this manner middleton et al 2014 the omitted metrics included accessibility data type and nighttime data the challenges of accessibility and data type are further discussed later in the paper nighttime data was not explicitly explored by the authors through this review citizen science projects showed the capacity to collect highly relevant data in urban settings on a fine spatial scale table 3 though studies evaluating the accuracy and precision of water levels reported by citizen science projects are limited supplementary material appendix c table 2 they indicate promising results for flooding applications mazzoleni et al 2015 more specifically research reviewing pluvial flooding found that the success of citizen science posts to simply locate inundated areas ranged from 72 to 95 see 2019 the future involvement of citizen science in hydrologic studies is dependent on the further analysis and quantification of accuracy of these observations it is also dependent upon the participation of citizens and their ability to capture precipitation events 4 crowdsourced data assessment existing databases to examine the capability of crowdsourced data we identified catalogued and characterized multiple databases for each data source we focused our inquiry to databases that 1 collected data in urban settings and 2 provided accessible data for collection for public webcams we examined directories on a global national and local arizona scale for social media we examined major platforms and collected data for storm events in arizona for citizen science projects we were able to focus our collected directories toward projects reporting stage or streamflow data a constraint that was not feasible for the first two secondary data sources this allowed for expansion beyond arizona at a local level in instances where a large number of observations were present for a single source e g facebook twitter a random sample of observations was used with a 90 confidence level and 10 margin of error to evaluate metrics for all other data sets metrics were evaluated through all available observations on the date accessed the following sub sections assess the crowdsourced data by the same metrics presented in the literature review 4 1 public webcams to supplement the literature review several webcam directories were identified to assess relevancy and feasibility of public webcams to complement urban flood monitoring supplementary material appendix b table 4 the public webcam sources had to be readily accessible online and provide near real time observations furthermore the directories were assessed for either quantity providing a high volume of spatially diverse data or quality high quality data for specific regions to be considered priority was given to data sources in the region of interest to our project specifically arizona directories were excluded if the majority of cameras were either inoperable or redundant with other more accessible directories in total seven directories were evaluated assessing accessibility format quantity density urban location durability and lag time quantity density and urban were calculated in the same manner as presented in the literature review for public webcams data accessibility was assessed by whether the source is publicly and freely available a value of no in this column would not absolutely exclude a data source but indicates a negotiated data access agreement may be required a varies value indicates that accessible data is limited format is a categorical variable listing whether the webcam provided photo or video durability indicates the number of years the webcam directory has been active from the project start date through 2018 lastly lag time is a categorical variable for whether or not the directory provided real time data the associated numerical value indicates the time in minutes between the real time event and posting of the image the omitted variables include data type relevance and nighttime data type and relevance are omitted due to the secondary source nature of the directories all webcam directories had cameras filming at night the analysis of these metrics across directories is shown in table 4 which again emphasizes the importance of webcams being properly positioned to provide relevant data but also shows the capacity to capture real time data 4 2 social media to complement the literature review large social media platforms including facebook youtube instagram google images twitter flickr vimeo and snapchat were considered for additional analysis due to the high volume of user engagement on each platform and accessible high quality image data supplementary materials appendix b table 5 other sites including reddit yelp pinterest and tumblr were not included in the analysis due to perceived low proportions of relevant data per keyword search as well as minimal search tool efficiency for finding real time flood related posts snapchat was ultimately omitted from analysis because public posts are only available for 24 h and search tool capacities are minimal all selected sources were manually assessed through a sample of relevant posts using keywords or hashtags for flood and flooding for social media relevance was defined based on posts falling within the search region of phoenix az and able to provide stage data or at very least presence of flooding during the flood events on october 7th and 13th in 2018 if a search returned more than one hundred posts results were filtered to the past five years and sampled in total seven social media platforms were evaluated assessing quantity frequency relevance density and location these metrics were calculated by the previously presented methods accessibility data type urban durability real time and nighttime metrics were excluded section 6 2 discusses the constraints of accessibility for social media platforms with an exploration of twitter data type is omitted as social media is a secondary data source however we examined posts with stage data or presence of flooding as stated in the relevancy metric the urban metric was excluded as all analyses were conducted in phoenix az i e urban is 100 in all cases all social media platforms are well established and considered durable data sources most data were provided near real time or had the capacity to be real time where the user upload was the limiting factor nighttime data was not distinguished the inspection of social media platforms conveyed similar results of the academic literature social media provides a large quantity of posts but not fine scale data table 5 4 3 citizen science to further assess the potential availability of citizen science projects as data sources for large scale hydrological monitoring efforts we identified catalogued and characterized ongoing projects projects were identified by searching a variety of citizen science community directories including citizenscience gov scistarter org citsci org arizona state university and environmental protection agency citizen science directories selected projects were then evaluated to assess the availability of hydrological data types and feasibility to supplement real time urban flood monitoring this query resulted in 19 citizen science projects that had 1 stage or streamflow data readily accessible online and 2 more than two hundred observations posted supplementary material appendix b table 6 fig 2 displays the thirteen hydrological data types collected across these projects the 19 citizen science projects were evaluated on accessibility format quantity frequency urban location durability real time and nighttime metrics all metrics were calculated in the manners previously listed for citizen science projects density was omitted as the spatial scale varied significantly from cities to streams with unclear boundaries all posts were relevant due to the constraints placed on the identified projects the nighttime metric was calculated by the percentage of observations posted 30 min after sunset to 30 min before sunrise for the respective location seven of these projects appear iseechange waze crowdhydrology rifls az water watch and idah2o had every observation evaluated by a member of our team to assess the metrics a random sample was used for analysis of the remaining projects the projects are categorized by those providing real time data defined as within 5 min table 6 and those that do not table 7 however it should be noted that azt app and stream tracker likely have the capacity to provide real time data additionally of the 19 projects seven require training for citizens to participate these include rifls springs online idah2o texas stream team utah water watch vsmp and missouri stream team the analysis of these metrics across directories emphasizes the degree of relevancy citizen science projects can have for urban flood monitoring with potentially high resolution 5 example of crowdsourced data and public safety integration while we have focused on crowdsourced data available in arizona we now highlight norfolk virginia a city that has integrated urban flood monitoring and public safety norfolk is located in the hampton roads region with a metropolitan area of 250 km2 housing more than 1 5 million residents and the world s largest naval station this region experiences a humid subtropical climate and has been experiencing an increased frequency of flooding due to low relief regional land subsidence and sea level rise burgos et al 2018 sweet and park 2014 the city of norfolk has been actively adopting strategies to become more resilient and was selected as one of the rockefeller foundation s 100 resilient cities in the world 100 resilient cities 2019 in 2018 rain events on august 11th 9 4 cm of precipitation and august 20th 5 2 cm of precipitation were explored to assess the quantity and density of crowdsourced data through two citizen science projects city of norfolk s system to track organize record and map storm which is used by city employees and citizens to record incidents that occur during inclement weather and waze a crowdsourced navigation application owned by google where waze users can submit reports regarding real time street conditions including flood reports our analysis of the crowdsourced data returned the following results fig 3 storm was searched using keywords flooded flooded street and flooded underpass and yielded 2 5 observations per day equating to a density of 1 observation per 99 88 km2 for the waze dataset a keyword search of hazard weather flood produced 67 relevant observations per day corresponding to a density of 1 post per 3 73 km2 the storm citizen science project observations provide high quality information reported or reviewed by trained officials at relevant sites while storm data reporting is restricted waze allows a larger group of individuals i e their users to make instantaneous flood reports again beyond density the spatial accuracy and measurement error of the observations is critical for urban flood monitoring and waze users are not trained to report flood data however waze holds the potential to facilitate real time flood warnings praharaj et al 2021 and a partnership has been established with norfolk through the waze connected citizens program the city of norfolk 2018 storm does provide a unique characteristic that supports a core motivation behind this study the ability to report flood hazards and damages and alert people of potential dangers in their communities storm observations publicly report damages such flooded streets flooded underpasses debris blocked streets downed powerlines or telephone poles waste water issues or damaged trees sadler et al 2018 arcgis 6 assessing the tradeoffs of crowdsourced data 6 1 public webcams the literature and database analysis revealed that the use of public webcams combined with subsequent image processing to collect data provides a number of challenges and opportunities images from webcams may not provide reliable data due to poor image quality highly variable viewing angles framing and distance to target unannounced downtime or discontinuation of service bothmann et al 2017 castelletti et al 2016 guastella and smith 2014 morris et al 2013 murdock et al 2013 because flood managers are a secondary user of the webcam stream they do not have direct control over the positioning of the webcams for data collection bothmann et al 2017 guastella and smith 2014 morris et al 2013 for instance many weather underground cameras are pointed toward the sky which would not capture water levels but users could potentially reposition cameras during storm events traffic cameras such as found in the arizona department of transportation adot database are often pointed at interstates which are designed more robustly than local roads and may not experience the same flooding risk there is potential for flood managers to be granted access to control traffic cameras where the flood manager could reposition the camera during storm events to potentially monitor underpasses or redirect toward a water level gauge potential sources of data errors include processing errors due to noise bothmann et al 2017 castelletti et al 2016 murdock et al 2013 or incorrect geo location metadata or timestamps morris et al 2013 murdock et al 2013 in the database analysis many sources did not provide exact locations of webcams earthcam weather underground usgs adot traffic cameras mohave flood district while this information could be collected manually this would be a resource sink lastly flood managers should be wary of sparsely or unevenly distributed webcams which could lead to extrapolation errors murdock et al 2013 as seen with the low density numbers reported in the existing database analysis however the benefits may outweigh the challenges considering the large spatial and real time temporal scales in which webcams are capable of operating bothmann et al 2017 castelletti et al 2016 guastella and smith 2014 morris et al 2013 murdock et al 2013 and access to large datasets can minimize the impacts of such errors murdock et al 2013 every database examined operated in real time although there may be a slight posting delay furthermore there are now individual cities with millions of webcams so a large dataset is a realistic possibility the two sources highlighted prior for potentially high accessibility weather underground and adot traffic cameras showed higher density and urban applicably than other databases examined indicating strong contenders for pilot programs when relevant webcams are identified they provide an objective and consistent data source that relies on little human involvement guastella and smith 2014 morris et al 2013 and can be processed with high accuracy bothmann et al 2017 castelletti et al 2016 morris et al 2013 the analyzed databases also showed promising durability with the minimum active years reported as six in short public webcam directories provide low cost or free resources to supplement traditional monitoring networks which are currently strained castelletti et al 2016 guastella and smith 2014 morris et al 2013 6 2 social media social media platforms deliver both opportunities and challenges for urban flood monitoring while social media provides a high volume stream of low cost real time data that may complement other data sources social media also provides an opportunity to capture a flood of any size as well as provide a communication tool in emergency situations that is readily accessible to people jongman et al 2015 middleton et al 2014 smith et al 2015 this accessibility is a notable advantage compared to citizen science projects such as storm that may lack awareness or adequate mobile compatibility machine processed validation techniques provide an opportunity for scalability that is not possible through manual validation techniques de bruijn et al 2018 herfort et al 2014 but this requires posts to have quality data attached fohringer et al 2015 smith et al 2015 due to the individuality of social media posting the data quality also varies widely as seen in the database investigation which reported text photo and video posts across platforms therefore it may be worthwhile to weight the knowledge of a user or number of post shares jongman et al 2015 smith et al 2015 for instance a city water department s post may be more valuable than an average citizen sophisticated algorithms and queries are needed to filter posts according to language and identify spatial scale based on location names arthur et al 2018 de bruijn et al 2018 fohringer et al 2015 herfort et al 2014 jongman et al 2015 middleton et al 2014 rosser et al 2017 smith et al 2015 to elaborate many social media posts lack high resolution georeferencing in our existing database analysis none of the posts were able to be pinned to a point location examining the various social media platforms the twitter application programming interface api is perhaps the most useful and accessible allowing extensive searching and filtering of results using one of three access tiers offering differing levels of search functionality a standard access tier that is free and premium and enterprise levels of paid access free access is limited to searching the last seven days of activity at rates up to 180 queries per 15 min window premium level allows searching both 30 day and complete archives of tweets as well as higher query rates enterprise access adds more powerful search criteria specification in practice we found the standard free api to be adequate for our exploration of tracking flood related twitter activity in real time where very high query rates and searching of historical tweet archives are not relevant queries could be organized and spaced out temporally to avoid exceeding access rate limits while still providing a stream of near real time data twitter allows complex logical queries to be formulated to retrieve and filter posts from specified geographic regions for instance one could search for tweets mentioning flooding but also having other keywords like water and storm to eliminate metaphorical uses of the worded flooding or flooded thus some careful effort could likely lead to development of effective query probes that do indeed relate to flooding as noted for all social media earlier twitter users rarely allow their posts to be tagged with detailed geo locations for instance we found that less than 1 of tweets found containing the words flood flooding or inundation have location data attached thus tweets can be geo located only to the city level or for larger cities to a general region of the city even so tweets might be useful as a general indicator of flooding activity to complement sparse public webcams coverage of an area or to help understand overall flooding dynamics in planning placement of webcams a preliminary analysis examined twitter activity around sixteen cities between november 15 2019 through january 20 2020 to explore how tweets may or may not correspond with storm and flooding activity and potential correlation was indicated to better understand these initial findings we built a software tool to mine twitter for targeted cities and to graph the resulting tweets in a tweetograph a social media analogy to a hydrograph fig 4 the methods of this preliminary analysis may be viewed in more detail in supplementary materials appendix d as phase 1 and phase 2 respectively we present the tweetographs below specifically to 1 provide proof of concept of social media data mining algorithms and 2 illustrate potential temporal correlation between social media activity and flooding from the preliminary analysis this utility highlights a pathway to integrate social media data into urban flood monitoring 6 3 citizen science integrating citizen science data sources into hydrological monitoring and modelling efforts holds promise due to the nature of direct data sources but also presents significant challenges in terms of accuracy reliability and coverage if citizen scientists are not trained there may be issues of quality due to unidentifiable observations e g shorthand blurry images etc or poor technique in taking hydrologic measurements le coz et al 2016 shupe 2017 smith and rodriguez 2017 r q wang et al 2018 weeser et al 2018 yu et al 2016 less than half of the examined citizen science projects trained users on data collection techniques coverage and reliability are often problematic as citizen science projects typically lack large networks leading to a low volume of irregular participation le coz et al 2016 shupe 2017 smith and rodriguez 2017 additionally inclement weather reasonably causes citizen scientists to stay indoors and away from active hydrological situations at the precise time when they are needed most asking citizen scientists to venture out in potentially unsafe flood conditions raises ethical and liability concerns le coz et al 2016 as mentioned in the norfolk case study training requirements also create an entry barrier which may reduce observations awkward data formats constraints on mobile data access and a limited feedback of high value information to contributing citizen scientists can limit interest and long term community engagement le coz et al 2016 smith and rodriguez 2017 r q wang et al 2018 yu et al 2016 these barriers will need to be addressed to adopt citizen science data into urban flood monitoring despite these challenges citizen science can offer tremendous opportunity for citizen engagement le coz et al 2016 shupe 2017 weeser et al 2018 observations can be reported real time and supported with text photos and videos providing low cost data the database analysis also showed that observations were often detailed reporting fine spatial scale and occasionally nighttime data furthermore citizen science projects allow the community to co generate knowledge in high traffic areas or spaces that are a priority to the community le coz et al 2016 naik 2016 shupe 2017 smith and rodriguez 2017 r q wang et al 2018 weeser et al 2018 projects with fixed gauges placed at key points within the cityscape e g crowdhydrology can provide long term data for a fixed reference point le coz et al 2016 lowry et al 2019 lowry and fienen 2013 r q wang et al 2018 weeser et al 2018 a couple databases missouri stream team and tx stream team have existed for 30 and 28 years respectively emphasizing the long term data source potential the integration of social media with citizen science can increase participation and awareness while further supplementing data gaps le coz et al 2016 r q wang et al 2018 projects based around events whether sampling blitzes or coastal tides typically have higher number of observations loftis et al 2019 despite its limits the public engagement of citizen science provides a notable opportunity to further educate and empower residents while potentially increasing project observations 6 4 crowdsourced data a summary of qualitative and quantitative metrics across academic literature and established databases for the three established data sources table 8 reveals the potential for integrating observational data from crowdsourcing and citizen science projects into urban flood monitoring public webcam networks have been around the longest 1990s while citizen science networks and social media sources primarily started within the following decade 2000s in general all three sources are relatively well established and could be described as durable over time indicating they are viable for long term data collection furthermore all three sources provide a wide variety of data that could supplement ongoing monitoring efforts with the continuous improvement of video and image processing utilizing public webcam and social media sources is becoming more plausible for relevant flood data such as stage streamflow precipitation snowpack weather topography vegetation dynamics land cover land use water use and informing gis data it remains necessary to have specialized camera setups or citizen science projects for anything more difficult to extrapolate beyond the visuals of videos and images such as soil moisture ground temperature water quality and groundwater data lastly there is concern about a lack of useful nighttime data across the data sources though specialized camera setups with nighttime lighting or infrared capabilities can provide nighttime data most public webcam networks do not have these capabilities while some citizen science projects reported nighttime data most did not notably the iseechange project yielded roughly a quarter of observations overnight but these were typically not reported until the following day social media is the most promising avenue for crowdsourcing nighttime flooding data as individuals may report local flooding 7 discussion each of the three data sources have unique opportunities and challenges to support urban flood monitoring social media and public webcams produce a large quantity of observations across a widespread area providing a frequent real time snapshot of water levels during urban flooding that is not captured by citizen science projects citizen science projects have an advantage of nearly all observations being relevant for water level measurements leading citizen science projects to provide better baseline data than event data for urban flood monitoring social media and public webcams may serve better providing real time data for flood events however both data sources have a high number of non relevant observations to address this multiple query probes could be configured to explore different filtering criteria recalling the twitter example fig 4 these queries could be used to discover a to what extent twitter activity tracks precipitation and associated urban flooding and if so b which content based logical filters yielded tweetographs with the best correlations to precipitation and flooding as might be expected our initial trials suggest that success will hinge on development of finely tuned query probes that accurately return tweets truly related to flooding activity at that point further analysis can determine how reliably tweeting does or does not fundamentally signal flooding events despite existing weaknesses across data sources these crowdsourced data provide a chance to augment urban flood monitoring using data fusion to exploit their combined and relative strengths the integration of primary observations into traditional urban flood modelling allows data to be collected where it is needed most at flooded locations in cities we propose an integrated flood stage observation network that fuses crowdsourced and professional hydrology data in real time to feed that data to modelers and communicate flood status and risk to community members and decision makers 8 conclusion opportunities and challenges exist for integration of each data source public webcams social media and citizen science into urban flood monitoring public webcams have the highest potential when located at frequently flooded or high risk locations webcams provide an opportunity for low cost reliable real time data and house potential for reasonably accurate nighttime observations through ir capable cameras public webcams are limited by difficult directory navigation and fixed locations camera views may be impaired by poor angles long distances obstructions and network problems when aided by a streetlight the webcam may be able to provide nighttime data next the greatest asset of social media is the high quantity of observations that provide free real time data that are typically not constrained to a specific location the drawbacks are the low number of relevant posts low quality media and lack of georeferenced observations social media provides ample opportunity for easy integration with other data collection methods to supplement observations lastly citizen science delivers detailed quality data through community engagement with low cost technologies such as smartphones however engagement is generally low outside of campaigned events and participation falls further during inclement weather such as flooding events there are barriers to data collection as not all projects publicly post data or the data may be in difficult formats citizen science s largest strength is the detail of information provided for each observation crowdsourced data introduces a layer of uncertainty of measurement errors it remains difficult to organize data gathered via crowdsourcing into real time monitoring systems due to inconsistent metrics uncoordinated efforts and to get enough georeferenced ground based observations to calibrate models despite the presented challenges each data source holds potential to enhance urban flood monitoring by providing access to real time localized flood reports especially if fused together to exploit their relative and combined strengths and to create a more complete and up to date data source for urban flooding declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests mikhail chester reports financial support was provided by national science foundation award no 1831475 jonathan l goodall reports financial support was provided by national science foundation award no 1735587 all authors are part of the nsf award no 1831475 but mikhail is the lead pi jonathan and faria are associated with nsf award no 1735587 with jon having the seniority of the two acknowledgments this project is funded by the national science foundation s smart and connected communities program 1831475 and critical resilient interdependent infrastructure systems and processes program 1735587 appendix a table 1 crowdsourcing and citizen science data source assessment metrics 12 metrics are identified to assess the capability of the novel data sources to augment urban flood modelling table 1 metric description unit accessibility can the observation be publicly downloaded for free binary classified as either yes or no format in what format is the observation communicated classified as either text photo or video data type what data types do sources provide percent of the data source with stage streamflow precipitation snowpack weather soil moisture ground temperature topography vegetation dynamics land cover land use water quality water use groundwater quality level and gis data quantity how many total observations total number of posts frequency how many observations per a temporal boundary posts day or event relevance what percentage of sources or posts are relevant for informing flood modelling and decision making number of posts that depict flooding within the spatial and temporal boundaries density how many observations per a spatial boundary posts km2 urban are urban observations supported percent of observations in locations that have a population 50 000 people cromartie 2019 location1 is the location of the observation provided if so are accuracy and precision maintained classified as point has latitude and longitude or the precise building location name neighborhood tagged by neighborhood name or to an area approximately the size of a neighborhood or city durability how long have observations been hosted total number of active years rounded up if less than one year input one year real time are real time observations supported if so are accuracy and precision is precision maintained binary classified as either yes or no by whether the posting delay exceeds 5 min if available lag time is noted nighttime are nighttime observations supported if so are accuracy and precision maintained percent of observations that provided stage data between 30 min after sunset and 30 min before sunrise 1the locations of observations are either directly geotagged visualized displayed on a map or described with text appendix b data sources table 1 brief summaries of academic literature regarding public webcams table 1 article description bothmann et al 2017 webcams from amos were used for vegetation greenness levels usa europe castelletti et al 2016 touristic webcams complemented with data from flickr used for flood prediction of a snow catchment lake como italy guastella and smith 2014 touristic webcams used for assessing changes in coastal morphology kwazulu natal south africa morris et al 2013 traffic webcams used for vegetation greenness levels england uk murdock et al 2013 outdoor webcams from the amos collection used to create cloud maps for weather forecasting usa table 2 brief summaries of academic literature regarding social media table 2 article description herfort et al 2014 observational flooding data recorded between june 8th 1 30 p m to june 10th 2013 midnight from geo referenced twitter data from river elbe flood germany 2013 de bruijn et al 2018 used an event based algorithm to sort through location ambiguity of tweets for flooding disasters between july 29 2014 and july 18 2017 global rosser et al 2017 collected geotagged social media flickr flood data for a series of storms between january 5th to 11th in 2014 to create a flooding model uk jongman et al 2015 analyzed twitter and satellite source data to determine how they are used to support flood responses to a tropical depression and typhoon respectively pakistan single day event and the philippines multiple day event middleton et al 2014 review and case study of social media s use in response data hurricane sandy monitored for five days in october 2012 fohringer et al 2015 case study of flood patterns and depths data collected through photos on twitter and flickr from may 5 to june 21 2013 flood dresden germany smith et al 2015 modeled and assessed two flooding events in 2012 on june 28th and august 5th newcastle upon tyne uk arthur et al 2018 examined twitter data from floods in 2015 from october 22 to november 25 uk table 3 brief summaries of academic literature regarding citizen science projects table 3 article description le coz et al 2016 flood photo project riskscape by the national institute of water and atmospheric research niwa christchurch new zealand shupe 2017 water quality data and hydrologic observations collected via a smartphone app vancouver canada smith and rodriguez 2017 flood observations recorded by new york city 311 online or with the non emergency city telephone line new york city new york usa r q wang et al 2018 study of the web and mobile crowd sourcing platform mycoast that collects photos of coastal urban flooding charleston sc usa weeser et al 2018 analysis of numerical text message water level data collected from thirteen sites kenya yu et al 2016 comparison of crowdsourced and model predicted flood inundation for an extreme one day storm event shanghai china sadler et al 2018 developed flood severity model using crowdsourced flood reports for coastal city norfolk virginia usa loftis et al 2019 crowdsourced flood mapping was used to identify the inundation extent during king tide events and validate flood forecast model hampton roads virginia usa naik 2016 precipitation event paralyzed a city resulting in the creation of an impromptu crowdsourced flood map chennai india table 4 public webcam resources a summary of each public webcam directory examined table 4 resource description global earthcam a global commercial network of live stream webcams for online tourism phenocam a database of landscape images in north america for phenological research weather underground a global commercial service that provides real time weather data derived from the national weather service and personal weather stations weatherbug a global web and mobile application that provides hyper local weather data from private weather stations and sensors national united states geological survey usgs a national multimedia gallery including webcams which provides near real time conditions to monitor extreme weather events local arizona department of transportation a state wide database of traffic cameras that provide near real time road and traffic conditions arizona mohave flood district a county specific database of real time weather cameras to provide flood warnings table 5 social media resources a summary of each social media platform examined table 5 resource description facebook a global social networking site where users may post text photos and video to a story i e content posted up to 24 h or to their feed e g content posted until deleted users may also interact with content through reactions sharing or comments flickr a global image and video hosting service where users may caption react and comment on uploads google images a global image searching service instagram a global social networking site where users may post images or videos to a story or feed with the ability to caption react share and comment on uploads twitter a global social networking site where users may post text photos and videos to a feed users may interact with content by replying sharing or reacting vimeo a global video hosting service where users may caption react and comment on uploads youtube a global video hosting service and social networking site where users may post text photos and video to a story or feed users may interact with content through reactions and comments to comply with permissions google images was limited to results labeled for noncommercial reuse table 6 citizen science resources a summary of each citizen science project examined table 6 resource description global appear a global mobile and web based application that crowdsources environmental data for freshwater aquatic environments for research and education crowdwater a global mobile application which crowdsources water level streamflow soil moisture plastic pollution data of water bodies to improve forecasting of hydrological events digital earth watch picture post a global mobile and web based database which crowdsources images of landscapes for environmental monitoring the fluker post project a global mobile application which crowdsources images of landscapes for long term natural resource management iseechange a global mobile and web based application that crowdsources examples of climate change mitigation and adaptation stream tracker project a global mobile application that crowdsources when and where water is flowing to monitor intermittent streams waze a global mobile application that crowdsources road and traffic conditions to provide trip navigation national crowdhydrology a national web based application which crowdsources water level data primarily in or near urban locations through text messaging to collect hydrologic data springs online a national web based database that crowdsources ecosystem characteristics and processes to promote healthy environments what s your water level a national web based application that crowdsources water level data focused on coastal flood management local arizona trail water report a state level web based database that crowdsources availability of water sources across arizona trails arizona water watch a state level mobile application which crowdsources water wildlife and pollution data of water bodies in arizona idah2o a state level web based database that crowdsources habitat biological chemical and physical data of idaho s streams michigan clean water corps volunteer stream monitoring program vsmp a state level web based database that crowdsources water quality data in michigan s wadeable streams and rivers for water resources management and protection missouri stream team a state level mobile and web based database that crowdsources water quality data in missouri for river conservation massachusetts river instream flow stewards rifls a state level web based database which crowdsources streamflow of ungauged rivers in massachusetts storm a city level web based application that crowdsources flooded streets in norfolk va texas stream team a state level web based database that crowdsources water and environmental quality of over 400 texas waterways for scientific research and environmental stewardship utah water watch a state level web based database which crowdsources water quality data of lakes and streams in utah for watershed management and education appendix c reported water level errors across data sources table 1 accuracy and precision of water level derived from webcam image and video studies of cameras indicate webcams hold potential to accurately measure water level data table 1 source error cm error errormethod precision cm jiang et al 2020 2 6 20 rmse zhang et al 2019 0 4 rmse 1 lin et al 2018 1 1 1 rmse eltner et al 2018 0 6 mae 1 5 schoener 2018 1 4 mae 3 ran et al 2016 2 rmse kim et al 2008 2 5 8 mae shin et al 2008 2 3 mae average 1 5 rmse root mean square error mae mean absolute error table 2 accuracy of water level derived from citizen science sources studies regarding citizen science projects are limited but indicate potential for retrieving accurate water level data table 2 source error cm error errormethod davids et al 2017 1 9 mae le coz et al 2016 15 20 mae lowry and fienen 2013 0 6 rmse average 0 6 9 7 rmse root mean square error mae mean absolute error appendix d detailed methods for twitter analysis our analysis of twitter as a potential near real time indicator of urban flooding activity was conducted in two phases first a simple initial pilot effort was manually conducted to get an initial assessment of potential utility second a more in depth analysis ongoing for which we constructed a specialized software twitter scraper tool with significant automation that could be targeted to monitor flood related twitter activity in any targeted locale over either specific timeframes or on an ongoing basis phase 1 pilot evaluation of twitter as an indicator of urban flooding using the public free twitter api searches were made using the keywords flood flooding and inundation these searches were made from november 15 2019 to january 20 2020 and were localized to several locations in the united states which were known to be at risk for flooding to reduce noise the following locations were used o pahrump nevada o jersey city new jersey o plano texas o peachtree corners georgia o miramar beach florida o raleigh north carolina o rogers city arkansas o carmel indiana o st augustine florida o the woodlands texas o round rock texas o denton texas o kissimmee florida o san marcos texas o sugar land texas o queens new york searches were made for tweets which were geo located within the boundaries of these locations or for tweets where the author states their location in their profile to be within one of these locations events were closely monitored then verified through local news outlets the following events were captured o pahrump nevada o december 12 2019 to december 15 2019 o plano texas o january 16 2020 o peachtree corners georgia o january 12 2020 to january 16 2020 o raleigh north carolina o december 5 2019 to december 6 2019 o december 13 2019 to december 15 2019 o december 18 2019 to december 20 2019 o rogers city arkansas o january 9 2020 to january 14 2020 o carmel indiana o january 9 2020 to january 12 2020 o denton texas o january 10 2020 to january 13 2020 o queens new york o january 2 2020 to january 5 2020 events analyzed in this way showed an increase of up to four times the number of tweets that matched search criteria during and shortly after flooding events these preliminary results were deemed promising enough to warrant initiating a deeper analysis of the reliability of twitter as a flooding indicator phase 2 software for automation large scale analysis of twitter as reliable indicator of urban flooding to provide a basis for a larger statistical analysis of the reliability of twitter activity as an indicator of urban flooding events we constructed of an automated twitter scraper software product to use as a research tool capable of monitoring and capturing twitter activity and precipitation in flood prone cities across the united states the tool is embodied in a web application that allows researchers to easily configure monitoring of any city either continuously or within specified time windows this allows continuous long term monitoring to provide a strong record of baseline twitter activity in various regions as well as targeted monitoring of specific events e g when a large storm is forecast for a given region some specific features include user can create and deploy an unlimited number of monitoring tasks targeted to specified geographic regions user can specify an unlimited set of search term sets which are essentially complex constructed logical queries aimed at filtering out flood related tweets from the stream of tweets o the probes used in the provided examples used flooding flood or flooded or flooding flood with flooded with flood of rainstorm monsoon or rain or rainstorm or thunderstorm for a given monitoring task user can attach one or more search term sets to use in monitoring that region monitoring tasks also query noaa in 15 min intervals to collect a timeline of current precipitation data for the targeted region a graphical interface is provided to visualize twitter activity in other words the number of tweets returned for each of the specified search term sets along with precipitation data can be generated screenshots of the tool in action are given in fig 4 in the main body of the paper we note that while the tool can technically support unlimited monitoring tasks and search term sets a practical limit is set by the api access level used to access twitter as outlined in the main paper thus for example the standard free access level is limited to 180 queries per 15 min rolling window with this quota spread across all active monitoring tasks and their respective search term sets the web application is based on the django v2 1 7 application framework this application and the various backend modules that do the actual web scraping are written in python v3 6 9 the tool is supported by a postgresql v10 12 database the webapp stores all monitoring tasks and logs status information to the database and the results of all twitter queries returned by all monitoring tasks are stored in the database as well this approach overcomes the limitations to archival tweets imposed by twitter we collect all relevant tweets in real time and essentially create our own historical archive in the database the script uses the tweepy v3 9 python package to access the twitter api and directly accesses the openweathermap v2 5 api to collect precipitation data at the time of this writing long term data collection is still running for a number of u s cities we are also setting targeted monitoring of select cities where heavy precipitation and or flood warnings are issued although detailed results await completion of data collection and more careful statistical analysis our early observations suggest that twitter may not be as reliable as our pilot effort led us to hope as an early indicator of urban flooding correlations between precipitation flooding and flood related tweets do often appear but are often obscured or confused by variations in baseline twitter activity in some locales more work is also required to discover more reliable filtering queries that separate tweets truly related to flooding from those that mention flooding and rain in unrelated mundane contexts a more complete report will be provided in an upcoming publication focused specifically on this topic 
25777,to facilitate understanding and decision making in the food energy water few nexus context we develop an integrated technology environment economics model iteem at a watershed scale iteem is built as an integration of various models including models for grain processing drinking water treatment and wastewater treatment technology a watershed model for hydrology water quality crop production and nutrient cycling environment an economics model assessing total benefit including non market valuation of environmental benefits different data techniques are applied to develop suitable surrogates for computer based models including a response matrix method artificial neural networks and lookup tables empirical equations are applied to develop models of economics and drinking water treatment the input output relationships between the models are formulated in a unified computational framework iteem a spatially semi distributed dynamic simulation model can be used to quantify the environmental and socioeconomic impacts of various management practices technologies and policy interventions on few systems in the corn belt keywords integrated modeling surrogate modeling machine learning nutrient recycling phosphorus recovery environmental benefits 1 introduction food energy water few systems in the us corn belt are highly interconnected and sensitive to stresses and threats grain production and subsequent utilization for animal feed human food and ethanol production have pervasive effects on water quantity and quality in downstream environments both locally e g lakes and rivers with elevated nitrogen and phosphorus and nationally e g hypoxic zone in the gulf of mexico us epa 2017 water stress associated with increased climatic variability is anticipated to increase muttiah and wurbs 2002 especially in many mid sized cities in the corn belt that interact with neighboring agricultural lands major industrial needs li et al 2018 and their shared watersheds energy demand and overall costs for wastewater and drinking water treatment have increased and this trend is expected to be exacerbated by continued expansion of food and bioethanol production simpson et al 2008 twomey et al 2010 to deal with these threats to and risks within few systems long term efforts have been made to resolve the conflicts between agriculture food industry water supply and environmental protection for example wastewater treatment and corn ethanol refinery facilities have begun extracting nutrients from waste and process byproducts which results in both the reuse of extracted materials as inorganic mineral fertilizers e g struvite and calcium phytate and the reduction of point source discharge of nutrients to the environment for example recovering phosphorus p can conserve a finite resource e g phosphate rock cordell et al 2009 juneja et al 2019 margenot et al 2019 cost effective water treatment technologies are adopted to conserve energy use bhatnagar and sillanpää 2011 agricultural best management practices bmps reduce nutrient and soil loss from farmland in upstream watersheds lemke et al 2011 rao et al 2009 researchers have called for holistic integrated modeling development and assessment for few systems at various scales to avoid fragmented status quo decision making leck et al 2015 little et al 2019 this paper presents an integrated technology environment economics model iteem which unites a set of surrogates and empirical models derived from the various primary models simulating key processes at a watershed scale the developed iteem is capable of analyzing complex systems and specific solutions to interconnected problems in few systems in corn belt watersheds there are several major challenges when integrating models from different disciplines first most physical models are developed using discipline specific computer programs or software packages e g swat for hydrologic processes gps x for wastewater treatment which causes a barrier for automatic information transfer recently some interfaces have been developed for simple automated data exchange between two models anderson et al 2018 xiang et al 2020 for a large interdisciplinary integrated model involving agricultural hydrologic and engineering components developed in various computer programs including commercial software as the case of our study the level of complexity can be overwhelming to modelers and it usually turns out to be infeasible to directly integrate different models due to incompatibilities among discipline specific computer programs little et al 2019 second some engineering design models e g gps x for wastewater treatment superpro designer for grain processing are proprietary which may impose costs and legal constraints on direct coupling third inputs and outputs from separate models are likely to have different temporal and spatial scales with distinct data formats which need to be harmonized at the points of interaction between models cai 2008 appropriately building the interactions between various models is a key step to enable information transfer endogenously within a consistent model fourth complex physical models can be highly computationally expensive an affordable computational burden is especially important if the research of interest will address stochastic problems little et al 2019 thus directly integrating many computationally heavy models is often computationally infeasible researchers have developed various integrated models cai 2008 carmichael et al 2004 gaddis et al 2010 housh et al 2014 cai 2008 shared reflective comments on the advantages and challenges of holistic modeling tight coupling of different components in one consistent model versus compound modeling approaches loose coupling of different components via external data exchanges holistic models embed different components into a single consistent optimization model such as hydrologic economic models cai 2008 cai et al 2003 harou et al 2009 hydro biogeochemical model wu et al 2016 and system of systems models e g a biofuel biomass and refinery infrastructure environment model housh et al 2014 holistic optimization models are usually composed of mathematical equations including the objective function s and constraint function s other system modeling approaches applied in few systems include agent based models ng et al 2011 life cycle assessment li et al 2020 system dynamics feng et al 2016 gaddis and voinov 2010 etc however these system modeling methods have less focus on integrating detailed physical process modeling but more focus on other perspectives for example agent based models focus on simulating the behavior and decision making of multiple stakeholders life cycle assessment focuses on quantifying environmental impacts from cradle to grave and system dynamics focuses on modeling the feedbacks among stock variables and drivers the degree of process details at which those system modeling approaches have may not lend themselves to coupling multiple complex process models in a system of systems little et al 2019 proposed a generic tiered system of systems gtsos to upscale physical models from the process level to the system level via integration while keeping computational tractability and minimizing the loss of fidelity little et al 2019 models that are developed at the process level in various computer programs or software packages with domain specific knowledge and data can be replaced by surrogates also termed reduced order models meta models or emulators if process models cannot be integrated directly due to complexity and incompatibilities among discipline specific computer programs various data techniques can be applied for emulating a process model such as polynomial response surfaces artificial neural networks and supporting vector machine using numerical samples of inputs and outputs of the primary model under a systematic sampling strategy leperi et al 2019 lu and ricciuto 2019 those surrogates typically build statistical relationships between inputs and outputs of a system modeled by a primary model another type of surrogate is based on hybrid theory and data also termed as lower fidelity physically based surrogates razavi et al 2012 by replacing complex process models with appropriate surrogates one can integrate them into a consistent model maintaining reasonable fidelity of the primary process models without causing a serious computational burden as most surrogates do not have a rich internal structure carmichael et al 2004 although the gtsos framework provides a promising direction on model integration for analyzing a system of systems the development of such a framework is challenging specific challenges include the selection of an appropriate mathematical form of a surrogate for a particular process model and the integration of the surrogates across multiple spatial and temporal scales cai 2008 in addition examples of real world problems are needed to demonstrate the effectiveness and applicability of gtsos to the various complex system modeling cases here we explain the methodology used to overcome these challenges in the construction and execution of iteem disciplinary specific process models are replaced by surrogates and these surrogates are integrated within a unified computational software framework to form a holistic model iteem is demonstrated in a watershed in the corn belt to analyze inter connected problems of crop production grain processing water and wastewater treatment and nutrient management with consideration of technologies management practices and policies for multiple sectors 2 research problem and few systems characterization in corn belt watersheds 2 1 research problem few systems are usually highly interconnected crossing multiple sectors in many regions for the corn belt watersheds few systems are sensitive to stresses and threats with respect to food production and increasing biomass production and energy supply and demand which pose impacts on water quality water supply energy demand and cost resources conservation and economic growth and financial stability these interconnected components of few systems are depicted in fig 1 managing phosphorus p within these systems has proven especially challenging over the last 40 years due to the so called phosphorus paradox on the one hand phosphorus is an essential nutrient for plant growth and correspondingly copious applications of phosphorus fertilizer have been critical to meeting demand for food livestock feed and biofuel jarvie et al 2015 however on the other hand phosphorus fertilizer applied in agricultural fields is at risk of being transported into water bodies where in excess it contributes to water quality degradation namely toxic algae blooms bennett et al 2001 carpenter 2008 efforts to navigate these conflicting objectives have been undermined by long lasting stores of p in fields and streams i e p legacy which create time lags between changed agricultural practices and their impact on water quality or crop yields jarvie et al 2017 powers et al 2016 sharpley et al 2013 developing technologies for p removal and recovery from waste streams with feasible costs are important given that these interventions play unique and under emphasized roles within the few nexus regarding water pollution resource recovery and agriculture production as shown in fig 1 traditionally various management options are evaluated and implemented within individual systems f e or w or processes at local scales nutrient pollution in a corn belt watershed typically comes from a combination of identifiable pollution discharges i e point sources such as wastewater treatment plants and diffuse pollution i e non point sources such as agricultural runoff grain processing also indirectly contributes to nonpoint source p pollution by concentrating p in coproducts corn gluten feed cgf and dried distillers grains with solubles ddgs to the extent that they exceed livestock dietary requirements leading to pollution by p enriched manure in livestock feedlots nahm 2002 studies have shown the p concentration in cgf and ddgs two commonly used ingredients for cattle and poultry diets can be reduced by recovering p from light steepwater wet milling plant and thin stillage dry grind plant for potential use as a fertilizer juneja et al 2019 2020 drinking water treatment is considered as a local scale process that takes raw water from lakes that could have upstream point and nonpoint sources of nitrate and sediment traditional approaches usually use separate disciplinary specific models and ignore or do not fully consider the impact of the few nexus relations that exist at certain spatial scales such approaches cannot capture the interconnected influence of measures taken across the interdependent systems to address this general deficit in the corn belt and other regions a seamless integrated technology environment economics model iteem is developed to assess the tradeoffs and synergies within few systems in the corn belt 2 2 primary models for different components of few systems components of few systems shown in fig 1 are modeled by various computer based programs and empirical relationships on data and knowledge in individual disciplinary domains specifically the soil and water assessment tool swat a semi distributed and physically based watershed management model jayakrishnan et al 2005 is used to simulate water quality quantity and crop yield based on different land uses and bmps in a corn belt watershed the wastewater treatment wwt is modeled in gps x hydromantis environmental software solutions inc with advanced mathematical modeling optimization and management of wastewater treatment processes grain processing gp is modeled in superpro designer intelligen inc to evaluate the potential of p recovery from corn coproducts with existing physical chemical and enzymatic technology swat wwt and gp models involve detailed processes characterized by biological chemical and physical principles the drinking water treatment dwt model is empirical and driven by historical plant data for energy requirement and cost according to influent nitrate and sediment concentrations in addition to the physical component of few systems we also develop an economic model that represents the human dimension of the few systems the economic model is semi theoretical and empirical based on choice experiments evaluating the relationships between water quality improvements and farmers or the public willingness to pay that is assessed using survey data details on the development of each primary model are provided in the supplementary information si section 1 3 development of iteem via multi disciplinary teamwork three process models swat wwt and gp and empirical dwt or theoretical empirical models economics are first established at the process level the lower part of fig 2 then the components of iteem are developed in the form of surrogates or empirical relationships which are coupled by integrating input and output relationships crossing temporal and spatial scales at the interaction points between the components at the system level the upper part of fig 2 such a hierarchical structure allows modelers to drill down to the process level and access details for better interpreting results simulated at the system level all components of the iteem are coded in the same programming platform python the interaction between the technology t environment en and economics ec at the system level of iteem are shown in the upper part of fig 2 the technology is composed of bmps simulated in swat and engineering technologies simulated in wwt and gp components the relationships between t ec include capital operation and maintenance costs for p extraction and the cost savings from a changing farm management practices to use p recovered from biorefineries as crop fertilizer b introducing cover crops with no till etc the relationships between en ec include a non market benefit as a measure of the value to the population of an improvement in water quality and a measure of people s preferences for alternative ways of achieving lower p pollution levels b water treatment and water supply cost due to extra nutrient discharge the relationships between t en include a p removal from grain coproducts and hence downstream reductions of p in manure and feedlot runoff b nonpoint source change of p nitrate and sediment loads in rivers and to lakes c point source nutrient discharge reduction d mined p offset with p recovered from biorefineries and wastewater treatment plant 3 1 selection of model forms for components of iteem as discussed before it is challenging to select appropriate model forms for different model components razavi et al 2012 as there are no well defined standards for selecting model forms we select an appropriate model form for each component based on the model availability i e if a process model is available for a specific component complexity and attributes of existing models and data availability a decision tree used for selecting model forms is presented in fig 3 in general we start by examining if a component either physical or economic can be represented by a set of empirical equations for the purpose of our study especially for those components for which primary computer based models are not available or do not have external sources to develop a computer based model in the current study the economics and drinking water treatment dwt components fall within this category empirical equations are directly modularized in python to represent such components for the components for which computer based models i e swat wwt and gp exist distinct surrogate forms are chosen according to the particularities of each case first if there are sufficient simulation data determined by the number of inputs and outputs of a computer based model a machine learning model e g artificial neural network ann can be used to surrogate a complex and nonlinear process model such as the case for the wwt component of iteem note that the sample size of the simulation data varies case by case and is dependent on model complexity davis et al 2018 sufficient simulation data support the development of a surrogate model with desirable and stable performance second if a computer based model has a large number of inputs and simulate copious spatial and temporal outputs it becomes more challenging to obtain sufficient simulation data for training traditional surrogate models e g anns svm the challenge arises from two perspectives 1 the process of generating sufficient swat simulations for such a spatially distributed dynamic model is computationally expensive itself 2 the process of training ann and svm with such high dimension inputs and outputs takes up computer memory and numerous calculations thus are prone to crash although machine learning has also been applied to approximate complex hydrological models cai et al 2015 zhang et al 2009 the number of inputs and outputs in their swat are usually relatively low only several or dozens of inputs and outputs at most thus requiring fewer simulation data only thousands of simulations at most for our case we aim at developing a surrogate model that can reasonably replicate swat simulations including temporal and spatial heterogeneity while varying a large number of inputs e g bmp applications at each of the subwatersheds since neither ann nor svm with limited model simulations e g less than 100 model runs could be effective in generating such a spatially distributed and dynamic surrogate for copious inputs and outputs we choose a response matrix rm method as a surrogate model to produce spatially distributed dynamic outputs of swat which we show is appropriate in this study the rm method estimates water sediment and nutrient yields from landscapes with partial adoption of management practices by interpolating between simulation results when those practices are applied to all or none of the landscape the loading of nitrate total p sediment and streamflow in each channel reach is then the sum of all upstream landscape loads due to its simplicity and effectiveness the rm method has been widely used in different areas such as groundwater management models and watershed models gorelick 1983 housh et al 2014 the details of the mathematical definitions adopted to implement the rm method are provided in section 3 1 1 third if advanced surrogates e g anns svm rm method are not appropriate due to data limitation and the characteristics of a computer based model as the case of the gp component we look for another surrogate form too the data limitation arises that although superpro designer can generate sufficient samples via its built in monte carlo simulator the simulator does not provide outputs with the degree of detail needed for calculating the amount of p flow recovered from process streams the characteristics of the gp model refers to the fact that unlike the swat that simulates spatial and temporal variables the gp model generates steady state operation outputs which are only determined by the plant capacity for this case lookup tables a most basic form of surrogates are created to determine gp simulation outputs under a set of plant capacities reflecting a common range of commercial grain processing plants all data in the lookup tables are directly derived from the high fidelity computer based model 3 2 interactions between components of the iteem after selecting a model form for each of the iteem components the next challenge is to couple the surrogates whose inputs and outputs have distinct spatial and temporal scales this is a common challenge for integrating multiple components within a consistent model while each case may have its unique complexity when coupling spatially and temporally varied data and processes across components cohesive spatial and temporal scales are chosen for iteem to couple point source nutrient loadings from wwt with nonpoint source loadings simulated with swat the temporal scale of swat outputs can be daily monthly or annual and the spatial scale can be by hydrological response units hrus subwatersheds or the entire watershed in contrast the steady state wwt model operates at the weekly bi weekly or monthly scale and discharges to a specific point in a watershed given such inconsistent spatial and temporal scales we couple swat and wwt models at the monthly scale to match the spatial scale we couple swat and wwt models at the subwatershed scale 12 digit hydrologic unit code this reduces hundreds of hrus to dozens of subwatersheds point source nutrient loadings are added as inflows to the channel reach in the subwatershed where wwt plants are located more details about coupling over temporal and spatial scales are provided in mathematical formulations section 3 3 1 1 fig 4 illustrates the multiple interacting feedback loops among the components in iteem swat has the most interactions with other components the inputs to swat include bmp e g tillage p fertilizer rates grassed waterways riparian buffers etc land allocations at the subwatershed level the simulated nitrate and sediment from swat are inputs to the dwt model to estimate the required treatment cost and energy consumption to purify drinking water swat also simulates corn and soybean yields which are inputs to the gp model that simulates the amount of recovered p rp the resulting amount of phytin based fertilizer produced from rp is then simulated as a substitute that displaces mined p fertilizer besides the p recovery from corn grain byproducts cgf ddgs causes a reduction of p content in livestock diets leading to reduced manure p content and ultimately reduced p runoff from feedlots the selection of a wastewater treatment technology also has implications for p recovery nutrient discharge and cost the four wwt alternatives are 1 activated sludge 2 activated sludge with chemical precipitation 3 a modified 5 stage bardenpho process with enhanced biological phosphorus removal ebpr and 4 a modified 5 stage bardenpho ebpr process with struvite a form of p recovery ebpr str replacing mined p with an rp product i e struvite from wwt or phytin from gp is an additional agricultural bmp within iteem via techno economic analysis the costs e g capital operation labor and maintenance costs for the wwt gp and dwt technologies and practices are first calculated as total present value and then converted into equivalent annualized cost eac expressed as per year and their associated energy requirements are also calculated the other two economic components of iteem include 1 linking water quality levels to public willingness to pay wtp and farmers willingness to accept wta payment to adopt new conservation practices and 2 calculating the total net benefit accounting for engineering technologies farm management practices and non market environmental benefit beyond the interactions between these components iteem as a whole is driven by the climate market price of crops and rp fertilizer products policy regulations on maximum contaminant level mcl for nitrate in drinking water and wastewater nutrient effluent limits and technology options proposed for wwt and gp components 3 3 overview of components in the iteem the basic overview of each surrogate and empirical model in iteem are provided in this section detailed mathematical formulations of each modeling component are provided in si section 2 3 3 1 response matrix rm for swat swat simulates water quality i e nitrate total phosphorus and sediment yield water quantity i e streamflow and crop yield i e corn soybean corn silage perennial grass for each hydrologic response unit the smallest spatially homogeneous unit in swat and aggregates to the subwatershed scale the computational time for running swat can be expensive from minutes to hours depending on temporal and spatial scales and the number of simulations the rm method has been used previously to approximate the impacts of different crop allocations on simulated water and nutrient yield from landscapes housh et al 2014 to apply the rm method swat simulated water sediment nutrient and crop yields under various scenarios of complete bmp adoption are stored in a set of response matrices this initial simulation may require large computational efforts however the resulting rm can efficiently handle a large set of decision variables i e the land area of bmp adoption in each subwatershed involved in watershed management it is worth noting that the traditional rm method only estimates the landscape loss e g nonpoint source nutrient and sediment contributing to rivers see detailed calculations in si section 2 1 1 in this work we extend the traditional rm to further account for point source pollution reservoir trapping and in stream processes so that in stream loading can be accurately estimated first the point source loading of nitrate and tp simulated from the wwt component see section 3 3 2 is added to the subwatershed where wwt plants are located second modifications of the traditional rm method are also required to account for the trapping of sediment and nutrients in reservoirs third in stream processes such as nutrient cycling e g settling and microbial uptake respiration and sediment deposition must be considered in order to estimate the final in stream loading for the special case of sediment swat simulated loads are strongly controlled by in stream deposition and degradation these in stream sediment processes cannot be effectively accommodated for by an affine function land area allocation therefore we instead assume that all streams carry their full flow limited sediment capacity calculated similarly to the simplified version of the bagnold sediment stream equation which is an option within swat though another better performing option is applied in our swat simulations where the simplified bagnold equation of swat determines sediment carrying capacity according to flow velocity we estimate the capacity according to volumetric flow rate bagnold 1977 that is whenever incoming sediment loads exceed the flow determined capacity for sediment sediment is deposited when capacity exceeds incoming load sediment is eroded from the streambed detailed calculations of those modifications can be found in si section 2 1 2 unlike streamflow sediment and nutrients that are dependent on its spatial reach network and in stream processes the total watershed crop production is simply the sum of that in each subwatershed si section 2 1 3 3 3 2 artificial neural networks for wwt wwt plants in the corn belt can contribute considerable point source nutrient loading nitrate n tp due to combined sewerage from stormwater domestic and high strength industrial especially from biorefineries wastewater the wwt component of iteem includes four wastewater treatment plant design alternatives to treat the combined influent the four alternatives include 1 activated sludge as 2 activated sludge with chemical precipitation ascp to reduce effluent p concentrations from the wwt plant 3 a modified 5 stage bardenpho process with enhanced biological phosphorus removal ebpr 4 and a modified 5 stage bardenpho ebpr process with struvite magnesium ammonium phosphate recovery ebpr str we include the impact of stormwater that causes highly variable treatment performance during the process development using gps x software detailed descriptions of process development for each treatment alternative are provided in si section 1 2 as the wastewater treatment involves complex and nonlinear physical and biological processes advanced data driven techniques can be applied to predict treatment performance under fluctuations of influents artificial neural networks anns have been widely applied in various fields to capture nonlinear complex relationships between inputs and outputs for a generic ann a vector of input data x can be mapped to a vector of output data y i e y f a n n x where f a n n x represents a function of neural networks in this study four feed forward back propagation anns are applied to surrogate the four wwt alternatives i e as ascp ebpr ebpr str once plant layouts of the four wwt alternatives are designed and optimized we simulate stochastic influent conditions and run process simulations for each wwt design alternative to account for wwt performance variability each treatment alternative is simulated 10 000 times in the original high fidelity model using the gps x software and the dataset is split 60 20 20 into training validation and test datasets details of anns training are provided in si section 2 2 after successfully training anns for the four treatment alternatives the next step is to predict the effluents under stochastic conditions of influent using anns to be consistent with the temporal scale of the swat component and reduce the computational time we simulated monthly loading from the wwt assuming each month is run as a steady state for each month the total inflow domestic and industrial wastewater rainwater is determined using historical data while the chemical oxygen demand cod total kjeldahl nitrogen tkn total phosphorus tp are randomly sampled 1000 times from their fitted historical distributions since swat simulations provide deterministic monthly values the monthly mean values of effluent loadings from the 1000 simulations are calculated and added into the subwatershed where the wwt is located this is a key step to integrate point source and nonpoint source pollutant loadings from the different components the techno economic analysis of the four treatment alternatives is conducted using a combination of modeling and calculations specifically the capital costs e g construction and fixed operational costs e g labor maintenance are calculated in capdetworks a proprietary software compatible with gps x for estimating fixed costs capital labor maintenance cost for wwt models operational costs that vary with influent characteristics are calculated with process design and cost estimate equations from the us environmental protection agency harris et al 1982 3 3 3 lookup tables for grain processing two grain processing gp models i e corn wet milling and corn dry grind are developed in superpro designer intelligen inc which contains rigorous reactor modules for mechanical and chemical engineering of corn grain processing juneja et al 2019 2020 and details of process development are provided in si section 1 3 since superpro designer is commercially programmed and cannot be directly connected with the other iteem components we develop lookup tables that store results simulated from superpro designer the lookup tables contain two plant layouts for each plant capacity the capital cost operational cost energy and water use and p content of cgf and ddgs are simulated for each plant capacity in both wet milling and dry grind corn processing models two plant layouts are simulated 1 status quo grain processing without p recovery and 2 alternative technology that processes grain and recovers p as p complex which can be further purified as phytin a calcium magnesium salt of phytic acid calculations of cost and energy use are provided in si section 2 3 3 3 4 empirical equations for dwt the dwt model is developed based on operational data from a drinking water treatment plant located in the corn belt the cost data include fixed and variable costs for the nitrate removal facility nrf the fixed cost includes management overhead labor cost for operation and maintenance depreciation cost and nrf energy cost note that the daily energy consumption in nrf is assumed constant as detailed data are unavailable the variable cost includes the use of sodium chloride as the regenerant chemical for ion exchange resins and alum and polymer for turbidity treatment the consumption of sodium chloride is dependent on the nitrate level in the untreated water entering the dwt plant the consumption of alum and polymer is dependent on the sediment concentration in the raw water entering the dwt plant the costs do not include the total cost in the main treatment facility as the purpose of the dwt component is to estimate costs and energy requirements associated with excess nitrate and sediment treatment only the nitrate n no3 n and sediment loadings and streamflow estimated from swat in the subwatershed where the dwt plant is located are inputs to the dwt plant component the decision to operate the nrf is based on daily no3 n concentration in the untreated water entering the dwt plant the nrf will operate on any day where the influent no3 n concentration exceeds the threshold of 8 0 no3 n mg l based on the current maximum contaminant level mcl of 10 mg l that is we assume the operation threshold nitrate concentration to operate the nrf is 80 of the mcl calculations of cost and energy use for nitrate and sediment treatment are provided in si 2 4 3 3 5 theoretical empirical equations for economics 3 3 5 1 non market valuation for water quality improvement choice experiments ces are conducted to elicit the general public s wtp for water quality improvements and farmers wta payment to change management practices ces are a widely used non market valuation method in which respondents are asked to select the most preferred alternative in a hypothetical decision making situation while varying the levels of different attributes of interest louviere et al 2000 we utilize fractional factorial designs to allocate attribute levels i e cost recreational options attainment of nutrient loss reduction goals to each alternative a single combination of attribute levels and choice sets a combination of different alternatives to choose from the responses to the ces are analyzed using statistical methods based on random utility theory rut the utility of an individual survey respondent n from alterative a u n a includes systematic v n a and stochastic ε n a components with u n a v n a ε n a β t x n a ε n a vector x n a contains attribute levels faced by individual n in alternative a and β is a vector of coefficients estimated corresponding to the attributes because the non market valuation study is not yet complete this study uses a published wtp estimate from a related study parthum and ando 2020 to demonstrate how the choice experiment results will be used in the iteem 3 3 5 2 evaluation of iteem total costs and benefits after estimating the costs of engineered technologies and agricultural management practices the total economic net benefits of the entire system are calculated for each option each option includes a combination of wwt and bioprocessing technology together with a spatially explicit configuration of agricultural practices total benefits include the sum of revenue from product sales and non market benefits associated with water quality changes and the total cost is the sum of those incurred for wastewater and drinking water treatment grain processing and agricultural management practice implementation the total net benefits are calculated as 1 δ b b w t p r r p r c r o p r g p c w w t c d w t c g p c a g where δ b yr is the economic total net benefits for a given option b w t p yr is the monetary measure of public wtp for water quality improvements r r p yr is the revenue generated by selling recovered p product r c r o p yr is crop revenue r g p yr is the revenue from grain processing products sold c w w t yr is the cost of wastewater treatment c d w t yr is the cost of drinking water treatment c g p yr is the cost of grain processing plants c a g yr is the total cost of all agricultural management practices applied a given scenario all the terms in eq 1 are annualized cash flow that takes factors of the time value of money and inflation into account detailed calculations of terms in eq 1 are provided in si section 2 3 4 sensitivity analysis of iteem as each of the components contributes uncertainty to iteem via bmps environmental engineering technologies and policies it is important to investigate how the uncertainties from different component models propagate and affect the overall outputs of iteem at the system level the multiple sources of uncertainties which can be correlated complicate the sensitivity analysis for a demonstration purpose we conduct a simple one at a time oat sensitivity analysis of key parameters and leave more complete global sensitivity analysis for future work which indeed can be standard alone study we use a sensitivity indicator calculated as below 2 s e n s i t i v i t y i j 1 k 1 k δ o u t p u t j k o u t p u t j k b a s e l i n e δ p a r a m e t e r i k p a r a m e t e r i k b a s e l i n e where s e n s i t i v i t y i j represents the averaged relative change of parameter i on output j across different scenarios k δ o u t p u t j k is the change of output j of scenario k from baseline output o u t p u t j k b a s e l i n e δ p a r a m e t e r i k is the change of parameter i of scenario k from baseline p a r a m e t e r j k b a s e l i n e the multi dimension outputs of iteem are aggregated into four categories 1 water quality and quantity 2 energy consumption of engineering systems 3 costs and benefits 4 production of crop and recovered p the key parameters investigated in iteem includes six parameters from swat e g runoff curve water capacity in the soil soil evaporation etc two parameters i e influent nutrient strength and inflow in wwt model and other parameters i e crop price chemical price utility cost willingness to pay per household and interest rate related to costs and revenues across various models detailed descriptions and the range of each of these parameters are provided in si table s4 4 computational implementation of iteem in object oriented programming platform a coherent computational framework is developed to link and execute models from individual knowledge domains in an orderly manner standards of integrated modeling have been promoted by researchers to produce a useable and low friction simulation environment such as the community surface dynamics modeling system csdms project by peckham et al in 2013 the design criteria include but not limited to support of multiple operating system use of open source tools rather than proprietary software ease of reusability and maintenance etc we develop iteem using the object oriented programming in python which fits the several standards promoted by csdms an object oriented framework connects models as inherited objects where some models are parent objects for others there are several advantages for using an object oriented framework first by inheriting attributes variables of an object and methods functions of an object new child objects can be easily built which meets csdms design criterion of code reusability second creating various methods within the same object allows distinguishing separate functionalities e g technology cost versus treatment performance thus exhibiting a clear structure for the ease of maintenance the iteem developed in the object oriented language can also be easily converted to a different language using language interoperability tool e g babel peckham et al 2013 there are some other features that need to be improved in the future such as the support of serial and parallel computation the five component models of iteem are modularized as five independent objects note that the five component models of iteem are represented either in not original primary model at the process level they are rather their surrogates derived from primary process models or simplified or empirical models which are all integrated at the system level for example class swat is simulated by its surrogate model the response matrix method variables are stored as attributes functions are partitioned into various methods within each object fig 5 shows the implementation and integration of iteem described in the unified modeling language uml routines of data exchange are specified either in the attributes or inputs of methods in objects specific outputs of interest are obtained by calling specific methods for example a method called get loading nitrate is defined within the swat object to obtain nitrate loading specific to a particular scenario instead of calling swat to produce all outputs simultaneously based on the five objects that represent the five components respectively an overall object that integrates the five components is created as an iteem object that incorporates attributes variables and methods functions from all components into a single entity such an entity provides a computationally efficient model based on a large set of interconnected technology environment and economic relationships the detailed descriptions of attributes and methods are provided in si table s3 5 demonstration of iteem in the upper sangamon river watershed illinois we demonstrate iteem via a testbed watershed the upper sangamon river watershed usrw different scenarios are tested to explore a portfolio of alternative engineering technologies policies and bmps the results of the scenarios are compared to a baseline scenario in terms of multiple few systems indicators 5 1 study area for a testbed watershed the usrw located in central illinois usa is selected as an illustrative testbed for its few nexus issues data availability and existing modeling studies for this watershed fig 6 water quality in the usrw is threatened by both agricultural runoff and municipal and industrial nutrient discharges the relatively flat prairie soils in the watershed are highly productive extensively underlain by subsurface drainage systems and cultivated for maize and soybean production within the watershed lake decatur created by a dam on the sangamon river is the source of municipal water supply for the city of decatur and the village of mount zion combined population of 79 000 and industrial water supply for grain processors the lake has been classified as impaired because of high nitrate and p concentrations and low dissolved oxygen periodic dredging of sediment has been necessary to maintain the lake s storage capacity the cost of nitrate p and sediment delivered to the lake from agricultural runoff has been born by the water and wastewater ratepayers in decatur and mount zion the sanitary district of decatur sdd treats stormwater industrial wastewater and domestic wastewater sdd discharges treated effluent to the sangamon river downstream of the lake decatur dam the total discharge of sdd is approximately 600 mg no3 n yr and 582 mg p yr the largest of any facility in the state of il at concentrations typically ranging from 6 to 10 mg no3 n l and 5 30 mg p l sdd is faced with the challenge of complying with an impending effluent standard of 1 mg p l this is a major challenge because influent concentrations from the biorefineries responsible for approximately 90 of sdd discharge are more than twice the typical high range for effective biological p removal other wwt plants discharge in the watershed but their contribution to nutrient pollution is relatively small 5 of total point source tn and tp the three corn grain processing facilities located in the usrw have combined processing capacity of 8 1 million tonnes of corn per year in addition one major dairy feedlot produces an estimated 9400 metric tonnes of manure per year from around 3100 milking cows the status quo p content of manure is assumed to be 9 5 g kg dry fecal matter if p can be recovered from grain processing plant waste streams the p concentrations in manure can be reduced to 6 3 g p kg dry manure see section 3 3 3 5 2 scenario analysis iteem enables us to quantify the impacts of various nutrient management strategies technologies and policies that could enhance the beneficial synergies of few systems in the corn belt to illustrate the use of iteem we simulate and compare four scenarios of agricultural management practices engineering technologies and drinking water standards the baseline scenario consists of the status quo agricultural management practices wwt technology wwt i e activated sludge without rp and gp technology wet milling and dry grind corn processing without rp and current mcl policy governing nitrate in drinking water i e 10 no3 n mg l the scenario definitions are provided in table 1 5 3 results and discussion 5 3 1 surrogate model performance reducing complex process models into simpler surrogates almost inevitably introduces new uncertainty therefore it is imperative to ensure acceptable performance for the various surrogates used in iteem compared to the original high fidelity models here we evaluate the performance of three surrogate models i e rm for swat anns for wwt and lookup tables for gp for dwt and economics components they are already developed as empirical or theoretical empirical equations at the process level and there are no additional surrogates applied to upscale them to the system level the primary component models e g swat wwt gp have been developed in different software packages juneja et al 2019 2020 which however is not the focus of this study in fact some software packages such as wwt have consensus models that have been validated and used in engineering design traditionally the rm method is used for landscape loss estimates housh et al 2014 in this work we extended the rm to account for reservoir trapping point source loading and some in stream processes authors who are interested the details of the modified rm please contact the corresponding author we chose two widely used goodness of fit measures to assess the performance of the rm method percent bias p bias and nash sutcliffe efficiency nse moriasi et al 2007 the ideal value of p bias is zero indicating no long term overestimation or underestimation with positive values indicating overestimation and negative values underestimation nse varies from negative infinity to one one indicating a perfect match between the rm results and swat results and with values less than zero indicating that model prediction is less accurate than using the mean of observed data to test the performance of the rm method we select a combination of five bmps with randomly assigned agricultural land area compare the results using the rm method versus swat and present detailed results of one realization in fig 7 details regarding the five selected bmps can be found in si table s5 and the spatial agricultural land area allocations for the realization demonstrated in fig 7 can be found in si table s6 the rm method almost perfectly predicts streamflow and nitrate loads as evidenced by both p bias nearly 0 and nse nearly 1 in fig 7a and c this is because nitrate loading and streamflow are minimally influenced by in stream processes in the study watershed streamflow and nitrate load at the watershed outlet are very similar to the sum of the loads from individual subbasins minus a constant percentage trapped by the reservoir the rm method also predicts the total phosphorus tp and sediment loading with high accuracy in general p bias 4 9 and nse 0 911 for tp p bias 3 35 and nse 0 968 for sediment for tp major discrepancies between the rm method and swat simulation are observed during the low streamflow periods e g 2012 2013 this discrepancy arises because the rm method accounts for in stream p settling and biological p uptake respiration by applying a constant percent reduction to tp loss from the landscape 11 percent the constant percent reduction corresponds to some representative travel time for water through the channel network when water spends more time within the local channel network the impact of the in stream processes grows and accordingly the swat simulation shows the stream acting as a stronger p sink than does the rm method as discussed above sediment is predicted by applying a simplified bagnold equation to the rm estimates for streamflow fig 7b we apply streamflow constrained equations to estimate the in stream sediment loading the flow constrained method performs better than simply summing up the landscape losses of sediment this is because in this tested watershed sediment transport in streams is controlled by streamflow not landscape sediment loss to test the robustness of the performance of the rm method we run 10 realizations the performance of the rm method remains stable as shown by the detailed results of 10 realizations in si table s7 overall the rm method in iteem performs satisfactorily as a surrogate for swat across an explicitly spatial and temporal scale however future efforts should be devoted to incorporating a more realistic depiction of sediment deposition and degradation processes the anns exhibit satisfactory performance in surrogating the complex process based wastewater treatment model wwt using the simulation data from each of the four wwt alternatives the anns shows high prediction accuracy for all outputs mse 0 001 and r squared 0 95 as shown in si fig s7 detailed data of mse and r squared for each output in each wwt alternative are provided in si table s8 note that among the outputs predicted by the anns the total nitrate and phosphorus loading from wwt is coupled with the rm method to account for total nutrient loading for the watershed for the gp model surrogated by lookup tables all data stored in lookup tables are directly from the simulation results in the high fidelity model we assume that each gp plant is operated at steady state and at its plant capacity and the simulation data are extracted directly from lookup tables therefore there is no additional uncertainty introduced to this surrogate model 5 3 2 suitability of surrogate modeling and applicability of surrogate based model coupling as introduced in section 3 1 we develop surrogate models for complex process models first and then couple the surrogates along empirical models for water supply and economic analysis to formulate iteem the surrogate based model coupling method is suitable only if the following conditions exist first if primary process models cannot be integrated directly with compatibilities among discipline specific computer programs as discussed by little et al 2019 second the coupled primary process models are not computationally tractable especially it is difficult if not possible to use the coupled models for decision analysis e g being coupled with an optimization algorithm e g genetic algorithm to find optimal solutions while surrogate modeling has also been applied to acting as emulators and approximate uncertainty quantification in many domains alemazkoor and meidani 2020 razavi et al 2012 wu et al 2014 the difference between surrogate modeling and surrogate based model coupling should be noted surrogate modeling assesses a surrogate to one single high fidelity simulation model and uses it for certain modeling purposes surrogate based model coupling assesses the joint application of multiple surrogate models derived from multiple process simulation models in a consistent modeling framework various other model coupling methods have been applied to couple complex process models together such as wrapper model scripting and model translation those methods couple high fidelity models directly and thus have higher accuracy for instance anderson et al 2018 integrated dssat a crop simulation model and greet a simulation model for energy use and emissions for various vehicles models via their application programming interfaces apis to simulate the inter relationships between crop production and environmental impacts of biofuel production anderson et al 2018 however not all software developers provide apis especially for commercial software packages xiang et al 2020 integrated dssat with modflow a groundwater simulation model by writing scripts for external controls on both models in a batch mode in python model translation constructs different individual models from scratch in a common platform however this method can only be feasible for simple simulation models as it requires rewriting all equations included in the primary simulation models malard et al 2017 there is not a single model coupling method that is deemed to be better than others under all cases users should consider their strengths and weaknesses for particular applications for few systems analysis present in this study with appropriate simplifications as described above our surrogate based model coupling approach can deliver a computational tractable integrated model at the system level 5 3 3 tradeoffs and limitations of surrogate based integration design choices various spatial and temporal scales exist in different component models for example the temporal scales can be daily monthly and annual the spatial scales can be a point watersheds small and large and river basin for different component models our integration design choices on selecting targeted spatial and temporal scales and their interaction points are driven by decision making requirements for the model as well as technical considerations one of our study purposes is to evaluate solutions based on bmps and environmental engineering technologies for combined point and non point source nutrient management we chose a monthly temporal scale for both point source and non point source simulation which is usually sufficient to maintain nutrient mass balance for water quality related decision making the daily raw nitrate level is important and thus the dwt model uses a daily time scaler correspondingly we use the results of swat at a daily scale to estimate the daily raw nitrate at the sub watershed where the dwt locates technically there is a typical tradeoff between the modeling accuracy and computational requirement in the choices of temporal and spatial scales and the aggregation level for the integrated model in particular the dwt is modeled at a point scale where a treatment plant takes raw water from a storage or a river segment within the study watershed it would be is ideal to have the nitrate concentration at a finer spatial scale because of the spatial variance of the nitrate concentration however swat only simulates the in stream loads of nutrients at a sub watershed level and the average nitrate concentration in the sub watershed is taken for the simulation for wwt the effluent of point source nutrients is dynamic and impacted by domestic influent fluctuations as well as wet weather however dynamic modeling of wwt over a long term period is challenging due to the lack of detailed knowledge of influent wastewater characteristics and of the rainfall translation to plant influent and operation changes eventually we use a steady state approach to simulate monthly point source nutrients such a temporal aggregation from nearly real time to monthly scale limits the capability of the wwt model in simulating peak stormwater demand caused by an extreme rainfall event to rigorously quantify the tradeoffs between accuracy and computational time due to different choices of spatial and temporal scales would be an interesting investigation we do not explicitly quantify such tradeoffs in this paper as we focus more on developing an integrated model that tightly couples process based and empirical models at the same platform that can be used to test hypotheses and generate insights for watershed nutrient management 5 3 4 tradeoffs among food energy water and economics to demonstrate how iteem can be applied to explore tradeoffs among multiple metrics of few systems outputs from the three alternatives and the baseline are simulated using the method run iteem to facilitate tradeoff evaluation we normalize the performance of each metric indicator from 1 to 3 with 1 indicating the worst and 3 indicating the best among all scenarios as shown in fig 8 a the minimum and maximum indicator values are provided in fig 8b compared to scenarios 1 3 the baseline scenario has the lowest overall performance for water quality and quantity indicators but the best overall performance on energy consumption cost of technologies gp and wwt and crop production the result arises because all three alternatives introduce best agricultural management practices upgrade the existing technology to recover p and advance point source p removal the non market benefits represent the estimated willingness to pay of general public living in upstream of decatur reservoir for increases in likelihood of achieving the nutrient reduction target 45 by 2045 in illinois based on the study of parthum and ando 2020 their study estimates that each household located upstream of the reservoir would be willing to pay 0 95 per year to increase the likelihood of meeting the nutrient target by one percentage point using the estimate we simplify the non market benefits under each scenario as 0 95 household year multiplied by 113 700 approximate number of households in the upstream of the reservoir and then multiplied by the extent to which a scenario attains the 45 target for nitrate n or tp e g if a scenario reduces tp by 45 then this value is 100 the values in fig 8 are the sum of benefits derived from reduced nitrate and tp loads according to this formulation the three alternatives generate environmental benefits ranging from 3 0 to 8 1 million dollars yr however those environmental benefits are not enough to offset the associated costs to upgrade treatment technologies and adopt new agricultural bmps as evidenced in fig 8a the baseline scenario provides greater total benefits than do the three alternative scenarios note that the non market benefits will be updated based on our choice experiment results and the results might change considerably among the three alternative scenarios scenario 3 has the best nitrate reduction and second best tp reduction due to the adoption of cover crops and the choice of modified bardenpho enhanced biological phosphorus removal with struvite recovery ebpr str for wwt the nitrate reduction in scenario 3 also results in decreased energy consumption and cost associated with nitrate removal at the dwt scenarios 1 3 reduce the dap fertilizer application rate by 15 15 and 30 respectively but the impact on corn yield is relatively negligible this could be because either the baseline is currently over applying dap fertilizer or there is enough p accumulated from prior years to make up the gap in crop demand for p overall the three alternatives illustrate the tradeoffs between reduced nutrient loading energy demand and cost for alternative technologies 5 3 5 sensitivity analysis of iteem we conduct the sensitivity analysis for key parameters including six parameters from swat two parameters from the wwt and the other eight parameters of benefits and costs see fig 9 for the list of the parameters note that the ww nutrient parameter in the wwt model varies the influent cod tkn and tp altogether details provided in si table s4 the heatmap fig 9 shows the sensitivity results with parameters listed horizontally and multi dimensional outputs listed vertically under the four scenarios baseline three alternatives in table 1 investigated in this study we consider a change of 20 as upper and lower bound for most key parameters except for the interest rate and willingness to pay per household as mentioned earlier the sensitivity results of the key parameters are scenario dependent values in the heatmap represent the change to an output responding to one unit change of a parameter for example the sensitivity of parameter swat runoff curve on the output nitrate load is 2 1 as shown in the top left corner of the heatmap meaning that a 1 change of the runoff curve value from the baseline can cause a 2 1 change of nitrate load at the outlet it is found that the six swat parameters have the most significant impacts on water quality i e nutrient and sediment load and quantity i e streamflow and crop production the uncertainty from those parameters further propagates into crop revenue the willingness to pay wtp and the total net benefit the total net benefit is also significantly impacted by the market prices of products e g starch and ethanol from grain processing plants and the cost of feedstock e g corn sold for grain processing as the profit from gp has a large contribution to the total net benefit in contrast the prices of chemicals rp and utility e g electricity and natural gas have negligible impacts across the outputs the two parameters from the wwt model are evaluated with the four treatment alternatives as for baseline ascp for s1 ebpr acetate for s2 and ebpr str for s3 corresponding to the four scenarios provided in table 1 the influent nutrient strength ww nutrient and inflow of wastewater ww inflow have a noticeable impact on the energy use of wwt the amount of recovered struvite and the wtp which demonstrates that the uncertainty of influent characteristics from the wwt model can have a significant impact at the system level outputs however we also have two interesting observations 1 parameters from the wwt model do not have a significant impact on tp load at the outlet fig 9 despite the fact the point source p is the leading contributor to the total tp load for the testbed watershed 2 parameters are sensitive on the outputs point source only of individual wwt treatment alternatives for example the sensitivity of ww influent for the four treatment alternatives is 1 38 for point source nitrate and 0 87 for point source tp details provided in si table s9 however the sensitivity of ww nutrient is decreased when integrated at the system level point nonpoint sources with sensitivity being 0 03 for nitrate and 0 25 for tp details provided in si table s9 both observations can be attributed to the fact that our local sensitivity analysis is scenario dependent for the cases of ascp ebpr and ebpr str the point source nutrients are significantly reduced and are not the leading tp contributor anymore therefore for the scenarios with ascp ebpr and ebpr str the parameter changes on wwt model will not have sensitive impact on total tp load which ultimately decreases its sensitivity on tp load at the system level 6 conclusions and future research addressing large scale environmental sustainability challenges requires integrated analysis of complex inter relationships within few systems this paper presents the development of an integrated technology environment economics model iteem for typical watersheds in the corn belt we use various data techniques to convert complex models simulating physical engineering processes and socioeconomic relationships into computationally tractable surrogates and link these surrogates via input output relationships within a consistent computer based modeling platform the procedures for developing iteem for a case study watershed upper sangamon river watershed usrw can be applied to other watersheds in the corn belt with required data and model preparation as shown for the usrw based on our experience developing iteem with a team including researchers from hydrology and water resources system analysis environmental engineering environmental economics and sociology we reflect on steps for selecting surrogate models i e which type of data driven surrogates is most suitable for a particular physical and process model based on data and model availability as well as the purpose of the integrated model in this study we applied the response matrix method and artificial neural networks respectively to create surrogates for swat and wwt a detailed process model for dwt is not available for our project therefore we adopted empirical equations for the dwt component the lookup table method used for the gp component is simple but sufficient since gp can be assumed to operate at steady state and is only impacted by the size of plant capacity the economics component is formulated with equations that include non market valuation estimates and overall economic net benefits in our one at a time sensitivity analysis we show to what extent the uncertainty of selected key parameters in the component models can impact the outputs of iteem we identify some critical parameters that are worthy of further investigation future work should adopt a global sensitivity analysis considering the correlation of the uncertainties from different component models as well as the uncertainty due to future climate change iteem enables testing hypotheses for few systems analysis and exploring solutions to resolve inter connected few problems for example one hypothesis to test is that the most economically efficient way to improve water quality in corn belt watersheds should be to jointly employ a combination of agricultural land management practices and p recovery from co products generated by grain biorefinery facilities or wastewater treatment it is noted that iteem is designed for evaluating long term strategic planning for few systems in the corn belt but not for evaluating short term events such as extreme rainfall events that can cause peak stormwater flow that affects both point and non point pollution future work will be conducted to evaluate few system resilience under a set of stress and disturbance scenarios last but not the least iteem will be coupled with a multi objective optimization algorithm to search for optimal technologies and policies data and code availability the data and codes are available from the corresponding author upon request declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the us national science foundation infews t1 award number 1739788 we are grateful to industrial governmental and agricultural stakeholders in decatur il for providing valuable data for developing components of the iteem we appreciate valuable comments and constructive suggestions from the editors daniel p ames and tatiana filatova and three anonymous reviewers which have considerably improved the quality of this work appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105083 
25777,to facilitate understanding and decision making in the food energy water few nexus context we develop an integrated technology environment economics model iteem at a watershed scale iteem is built as an integration of various models including models for grain processing drinking water treatment and wastewater treatment technology a watershed model for hydrology water quality crop production and nutrient cycling environment an economics model assessing total benefit including non market valuation of environmental benefits different data techniques are applied to develop suitable surrogates for computer based models including a response matrix method artificial neural networks and lookup tables empirical equations are applied to develop models of economics and drinking water treatment the input output relationships between the models are formulated in a unified computational framework iteem a spatially semi distributed dynamic simulation model can be used to quantify the environmental and socioeconomic impacts of various management practices technologies and policy interventions on few systems in the corn belt keywords integrated modeling surrogate modeling machine learning nutrient recycling phosphorus recovery environmental benefits 1 introduction food energy water few systems in the us corn belt are highly interconnected and sensitive to stresses and threats grain production and subsequent utilization for animal feed human food and ethanol production have pervasive effects on water quantity and quality in downstream environments both locally e g lakes and rivers with elevated nitrogen and phosphorus and nationally e g hypoxic zone in the gulf of mexico us epa 2017 water stress associated with increased climatic variability is anticipated to increase muttiah and wurbs 2002 especially in many mid sized cities in the corn belt that interact with neighboring agricultural lands major industrial needs li et al 2018 and their shared watersheds energy demand and overall costs for wastewater and drinking water treatment have increased and this trend is expected to be exacerbated by continued expansion of food and bioethanol production simpson et al 2008 twomey et al 2010 to deal with these threats to and risks within few systems long term efforts have been made to resolve the conflicts between agriculture food industry water supply and environmental protection for example wastewater treatment and corn ethanol refinery facilities have begun extracting nutrients from waste and process byproducts which results in both the reuse of extracted materials as inorganic mineral fertilizers e g struvite and calcium phytate and the reduction of point source discharge of nutrients to the environment for example recovering phosphorus p can conserve a finite resource e g phosphate rock cordell et al 2009 juneja et al 2019 margenot et al 2019 cost effective water treatment technologies are adopted to conserve energy use bhatnagar and sillanpää 2011 agricultural best management practices bmps reduce nutrient and soil loss from farmland in upstream watersheds lemke et al 2011 rao et al 2009 researchers have called for holistic integrated modeling development and assessment for few systems at various scales to avoid fragmented status quo decision making leck et al 2015 little et al 2019 this paper presents an integrated technology environment economics model iteem which unites a set of surrogates and empirical models derived from the various primary models simulating key processes at a watershed scale the developed iteem is capable of analyzing complex systems and specific solutions to interconnected problems in few systems in corn belt watersheds there are several major challenges when integrating models from different disciplines first most physical models are developed using discipline specific computer programs or software packages e g swat for hydrologic processes gps x for wastewater treatment which causes a barrier for automatic information transfer recently some interfaces have been developed for simple automated data exchange between two models anderson et al 2018 xiang et al 2020 for a large interdisciplinary integrated model involving agricultural hydrologic and engineering components developed in various computer programs including commercial software as the case of our study the level of complexity can be overwhelming to modelers and it usually turns out to be infeasible to directly integrate different models due to incompatibilities among discipline specific computer programs little et al 2019 second some engineering design models e g gps x for wastewater treatment superpro designer for grain processing are proprietary which may impose costs and legal constraints on direct coupling third inputs and outputs from separate models are likely to have different temporal and spatial scales with distinct data formats which need to be harmonized at the points of interaction between models cai 2008 appropriately building the interactions between various models is a key step to enable information transfer endogenously within a consistent model fourth complex physical models can be highly computationally expensive an affordable computational burden is especially important if the research of interest will address stochastic problems little et al 2019 thus directly integrating many computationally heavy models is often computationally infeasible researchers have developed various integrated models cai 2008 carmichael et al 2004 gaddis et al 2010 housh et al 2014 cai 2008 shared reflective comments on the advantages and challenges of holistic modeling tight coupling of different components in one consistent model versus compound modeling approaches loose coupling of different components via external data exchanges holistic models embed different components into a single consistent optimization model such as hydrologic economic models cai 2008 cai et al 2003 harou et al 2009 hydro biogeochemical model wu et al 2016 and system of systems models e g a biofuel biomass and refinery infrastructure environment model housh et al 2014 holistic optimization models are usually composed of mathematical equations including the objective function s and constraint function s other system modeling approaches applied in few systems include agent based models ng et al 2011 life cycle assessment li et al 2020 system dynamics feng et al 2016 gaddis and voinov 2010 etc however these system modeling methods have less focus on integrating detailed physical process modeling but more focus on other perspectives for example agent based models focus on simulating the behavior and decision making of multiple stakeholders life cycle assessment focuses on quantifying environmental impacts from cradle to grave and system dynamics focuses on modeling the feedbacks among stock variables and drivers the degree of process details at which those system modeling approaches have may not lend themselves to coupling multiple complex process models in a system of systems little et al 2019 proposed a generic tiered system of systems gtsos to upscale physical models from the process level to the system level via integration while keeping computational tractability and minimizing the loss of fidelity little et al 2019 models that are developed at the process level in various computer programs or software packages with domain specific knowledge and data can be replaced by surrogates also termed reduced order models meta models or emulators if process models cannot be integrated directly due to complexity and incompatibilities among discipline specific computer programs various data techniques can be applied for emulating a process model such as polynomial response surfaces artificial neural networks and supporting vector machine using numerical samples of inputs and outputs of the primary model under a systematic sampling strategy leperi et al 2019 lu and ricciuto 2019 those surrogates typically build statistical relationships between inputs and outputs of a system modeled by a primary model another type of surrogate is based on hybrid theory and data also termed as lower fidelity physically based surrogates razavi et al 2012 by replacing complex process models with appropriate surrogates one can integrate them into a consistent model maintaining reasonable fidelity of the primary process models without causing a serious computational burden as most surrogates do not have a rich internal structure carmichael et al 2004 although the gtsos framework provides a promising direction on model integration for analyzing a system of systems the development of such a framework is challenging specific challenges include the selection of an appropriate mathematical form of a surrogate for a particular process model and the integration of the surrogates across multiple spatial and temporal scales cai 2008 in addition examples of real world problems are needed to demonstrate the effectiveness and applicability of gtsos to the various complex system modeling cases here we explain the methodology used to overcome these challenges in the construction and execution of iteem disciplinary specific process models are replaced by surrogates and these surrogates are integrated within a unified computational software framework to form a holistic model iteem is demonstrated in a watershed in the corn belt to analyze inter connected problems of crop production grain processing water and wastewater treatment and nutrient management with consideration of technologies management practices and policies for multiple sectors 2 research problem and few systems characterization in corn belt watersheds 2 1 research problem few systems are usually highly interconnected crossing multiple sectors in many regions for the corn belt watersheds few systems are sensitive to stresses and threats with respect to food production and increasing biomass production and energy supply and demand which pose impacts on water quality water supply energy demand and cost resources conservation and economic growth and financial stability these interconnected components of few systems are depicted in fig 1 managing phosphorus p within these systems has proven especially challenging over the last 40 years due to the so called phosphorus paradox on the one hand phosphorus is an essential nutrient for plant growth and correspondingly copious applications of phosphorus fertilizer have been critical to meeting demand for food livestock feed and biofuel jarvie et al 2015 however on the other hand phosphorus fertilizer applied in agricultural fields is at risk of being transported into water bodies where in excess it contributes to water quality degradation namely toxic algae blooms bennett et al 2001 carpenter 2008 efforts to navigate these conflicting objectives have been undermined by long lasting stores of p in fields and streams i e p legacy which create time lags between changed agricultural practices and their impact on water quality or crop yields jarvie et al 2017 powers et al 2016 sharpley et al 2013 developing technologies for p removal and recovery from waste streams with feasible costs are important given that these interventions play unique and under emphasized roles within the few nexus regarding water pollution resource recovery and agriculture production as shown in fig 1 traditionally various management options are evaluated and implemented within individual systems f e or w or processes at local scales nutrient pollution in a corn belt watershed typically comes from a combination of identifiable pollution discharges i e point sources such as wastewater treatment plants and diffuse pollution i e non point sources such as agricultural runoff grain processing also indirectly contributes to nonpoint source p pollution by concentrating p in coproducts corn gluten feed cgf and dried distillers grains with solubles ddgs to the extent that they exceed livestock dietary requirements leading to pollution by p enriched manure in livestock feedlots nahm 2002 studies have shown the p concentration in cgf and ddgs two commonly used ingredients for cattle and poultry diets can be reduced by recovering p from light steepwater wet milling plant and thin stillage dry grind plant for potential use as a fertilizer juneja et al 2019 2020 drinking water treatment is considered as a local scale process that takes raw water from lakes that could have upstream point and nonpoint sources of nitrate and sediment traditional approaches usually use separate disciplinary specific models and ignore or do not fully consider the impact of the few nexus relations that exist at certain spatial scales such approaches cannot capture the interconnected influence of measures taken across the interdependent systems to address this general deficit in the corn belt and other regions a seamless integrated technology environment economics model iteem is developed to assess the tradeoffs and synergies within few systems in the corn belt 2 2 primary models for different components of few systems components of few systems shown in fig 1 are modeled by various computer based programs and empirical relationships on data and knowledge in individual disciplinary domains specifically the soil and water assessment tool swat a semi distributed and physically based watershed management model jayakrishnan et al 2005 is used to simulate water quality quantity and crop yield based on different land uses and bmps in a corn belt watershed the wastewater treatment wwt is modeled in gps x hydromantis environmental software solutions inc with advanced mathematical modeling optimization and management of wastewater treatment processes grain processing gp is modeled in superpro designer intelligen inc to evaluate the potential of p recovery from corn coproducts with existing physical chemical and enzymatic technology swat wwt and gp models involve detailed processes characterized by biological chemical and physical principles the drinking water treatment dwt model is empirical and driven by historical plant data for energy requirement and cost according to influent nitrate and sediment concentrations in addition to the physical component of few systems we also develop an economic model that represents the human dimension of the few systems the economic model is semi theoretical and empirical based on choice experiments evaluating the relationships between water quality improvements and farmers or the public willingness to pay that is assessed using survey data details on the development of each primary model are provided in the supplementary information si section 1 3 development of iteem via multi disciplinary teamwork three process models swat wwt and gp and empirical dwt or theoretical empirical models economics are first established at the process level the lower part of fig 2 then the components of iteem are developed in the form of surrogates or empirical relationships which are coupled by integrating input and output relationships crossing temporal and spatial scales at the interaction points between the components at the system level the upper part of fig 2 such a hierarchical structure allows modelers to drill down to the process level and access details for better interpreting results simulated at the system level all components of the iteem are coded in the same programming platform python the interaction between the technology t environment en and economics ec at the system level of iteem are shown in the upper part of fig 2 the technology is composed of bmps simulated in swat and engineering technologies simulated in wwt and gp components the relationships between t ec include capital operation and maintenance costs for p extraction and the cost savings from a changing farm management practices to use p recovered from biorefineries as crop fertilizer b introducing cover crops with no till etc the relationships between en ec include a non market benefit as a measure of the value to the population of an improvement in water quality and a measure of people s preferences for alternative ways of achieving lower p pollution levels b water treatment and water supply cost due to extra nutrient discharge the relationships between t en include a p removal from grain coproducts and hence downstream reductions of p in manure and feedlot runoff b nonpoint source change of p nitrate and sediment loads in rivers and to lakes c point source nutrient discharge reduction d mined p offset with p recovered from biorefineries and wastewater treatment plant 3 1 selection of model forms for components of iteem as discussed before it is challenging to select appropriate model forms for different model components razavi et al 2012 as there are no well defined standards for selecting model forms we select an appropriate model form for each component based on the model availability i e if a process model is available for a specific component complexity and attributes of existing models and data availability a decision tree used for selecting model forms is presented in fig 3 in general we start by examining if a component either physical or economic can be represented by a set of empirical equations for the purpose of our study especially for those components for which primary computer based models are not available or do not have external sources to develop a computer based model in the current study the economics and drinking water treatment dwt components fall within this category empirical equations are directly modularized in python to represent such components for the components for which computer based models i e swat wwt and gp exist distinct surrogate forms are chosen according to the particularities of each case first if there are sufficient simulation data determined by the number of inputs and outputs of a computer based model a machine learning model e g artificial neural network ann can be used to surrogate a complex and nonlinear process model such as the case for the wwt component of iteem note that the sample size of the simulation data varies case by case and is dependent on model complexity davis et al 2018 sufficient simulation data support the development of a surrogate model with desirable and stable performance second if a computer based model has a large number of inputs and simulate copious spatial and temporal outputs it becomes more challenging to obtain sufficient simulation data for training traditional surrogate models e g anns svm the challenge arises from two perspectives 1 the process of generating sufficient swat simulations for such a spatially distributed dynamic model is computationally expensive itself 2 the process of training ann and svm with such high dimension inputs and outputs takes up computer memory and numerous calculations thus are prone to crash although machine learning has also been applied to approximate complex hydrological models cai et al 2015 zhang et al 2009 the number of inputs and outputs in their swat are usually relatively low only several or dozens of inputs and outputs at most thus requiring fewer simulation data only thousands of simulations at most for our case we aim at developing a surrogate model that can reasonably replicate swat simulations including temporal and spatial heterogeneity while varying a large number of inputs e g bmp applications at each of the subwatersheds since neither ann nor svm with limited model simulations e g less than 100 model runs could be effective in generating such a spatially distributed and dynamic surrogate for copious inputs and outputs we choose a response matrix rm method as a surrogate model to produce spatially distributed dynamic outputs of swat which we show is appropriate in this study the rm method estimates water sediment and nutrient yields from landscapes with partial adoption of management practices by interpolating between simulation results when those practices are applied to all or none of the landscape the loading of nitrate total p sediment and streamflow in each channel reach is then the sum of all upstream landscape loads due to its simplicity and effectiveness the rm method has been widely used in different areas such as groundwater management models and watershed models gorelick 1983 housh et al 2014 the details of the mathematical definitions adopted to implement the rm method are provided in section 3 1 1 third if advanced surrogates e g anns svm rm method are not appropriate due to data limitation and the characteristics of a computer based model as the case of the gp component we look for another surrogate form too the data limitation arises that although superpro designer can generate sufficient samples via its built in monte carlo simulator the simulator does not provide outputs with the degree of detail needed for calculating the amount of p flow recovered from process streams the characteristics of the gp model refers to the fact that unlike the swat that simulates spatial and temporal variables the gp model generates steady state operation outputs which are only determined by the plant capacity for this case lookup tables a most basic form of surrogates are created to determine gp simulation outputs under a set of plant capacities reflecting a common range of commercial grain processing plants all data in the lookup tables are directly derived from the high fidelity computer based model 3 2 interactions between components of the iteem after selecting a model form for each of the iteem components the next challenge is to couple the surrogates whose inputs and outputs have distinct spatial and temporal scales this is a common challenge for integrating multiple components within a consistent model while each case may have its unique complexity when coupling spatially and temporally varied data and processes across components cohesive spatial and temporal scales are chosen for iteem to couple point source nutrient loadings from wwt with nonpoint source loadings simulated with swat the temporal scale of swat outputs can be daily monthly or annual and the spatial scale can be by hydrological response units hrus subwatersheds or the entire watershed in contrast the steady state wwt model operates at the weekly bi weekly or monthly scale and discharges to a specific point in a watershed given such inconsistent spatial and temporal scales we couple swat and wwt models at the monthly scale to match the spatial scale we couple swat and wwt models at the subwatershed scale 12 digit hydrologic unit code this reduces hundreds of hrus to dozens of subwatersheds point source nutrient loadings are added as inflows to the channel reach in the subwatershed where wwt plants are located more details about coupling over temporal and spatial scales are provided in mathematical formulations section 3 3 1 1 fig 4 illustrates the multiple interacting feedback loops among the components in iteem swat has the most interactions with other components the inputs to swat include bmp e g tillage p fertilizer rates grassed waterways riparian buffers etc land allocations at the subwatershed level the simulated nitrate and sediment from swat are inputs to the dwt model to estimate the required treatment cost and energy consumption to purify drinking water swat also simulates corn and soybean yields which are inputs to the gp model that simulates the amount of recovered p rp the resulting amount of phytin based fertilizer produced from rp is then simulated as a substitute that displaces mined p fertilizer besides the p recovery from corn grain byproducts cgf ddgs causes a reduction of p content in livestock diets leading to reduced manure p content and ultimately reduced p runoff from feedlots the selection of a wastewater treatment technology also has implications for p recovery nutrient discharge and cost the four wwt alternatives are 1 activated sludge 2 activated sludge with chemical precipitation 3 a modified 5 stage bardenpho process with enhanced biological phosphorus removal ebpr and 4 a modified 5 stage bardenpho ebpr process with struvite a form of p recovery ebpr str replacing mined p with an rp product i e struvite from wwt or phytin from gp is an additional agricultural bmp within iteem via techno economic analysis the costs e g capital operation labor and maintenance costs for the wwt gp and dwt technologies and practices are first calculated as total present value and then converted into equivalent annualized cost eac expressed as per year and their associated energy requirements are also calculated the other two economic components of iteem include 1 linking water quality levels to public willingness to pay wtp and farmers willingness to accept wta payment to adopt new conservation practices and 2 calculating the total net benefit accounting for engineering technologies farm management practices and non market environmental benefit beyond the interactions between these components iteem as a whole is driven by the climate market price of crops and rp fertilizer products policy regulations on maximum contaminant level mcl for nitrate in drinking water and wastewater nutrient effluent limits and technology options proposed for wwt and gp components 3 3 overview of components in the iteem the basic overview of each surrogate and empirical model in iteem are provided in this section detailed mathematical formulations of each modeling component are provided in si section 2 3 3 1 response matrix rm for swat swat simulates water quality i e nitrate total phosphorus and sediment yield water quantity i e streamflow and crop yield i e corn soybean corn silage perennial grass for each hydrologic response unit the smallest spatially homogeneous unit in swat and aggregates to the subwatershed scale the computational time for running swat can be expensive from minutes to hours depending on temporal and spatial scales and the number of simulations the rm method has been used previously to approximate the impacts of different crop allocations on simulated water and nutrient yield from landscapes housh et al 2014 to apply the rm method swat simulated water sediment nutrient and crop yields under various scenarios of complete bmp adoption are stored in a set of response matrices this initial simulation may require large computational efforts however the resulting rm can efficiently handle a large set of decision variables i e the land area of bmp adoption in each subwatershed involved in watershed management it is worth noting that the traditional rm method only estimates the landscape loss e g nonpoint source nutrient and sediment contributing to rivers see detailed calculations in si section 2 1 1 in this work we extend the traditional rm to further account for point source pollution reservoir trapping and in stream processes so that in stream loading can be accurately estimated first the point source loading of nitrate and tp simulated from the wwt component see section 3 3 2 is added to the subwatershed where wwt plants are located second modifications of the traditional rm method are also required to account for the trapping of sediment and nutrients in reservoirs third in stream processes such as nutrient cycling e g settling and microbial uptake respiration and sediment deposition must be considered in order to estimate the final in stream loading for the special case of sediment swat simulated loads are strongly controlled by in stream deposition and degradation these in stream sediment processes cannot be effectively accommodated for by an affine function land area allocation therefore we instead assume that all streams carry their full flow limited sediment capacity calculated similarly to the simplified version of the bagnold sediment stream equation which is an option within swat though another better performing option is applied in our swat simulations where the simplified bagnold equation of swat determines sediment carrying capacity according to flow velocity we estimate the capacity according to volumetric flow rate bagnold 1977 that is whenever incoming sediment loads exceed the flow determined capacity for sediment sediment is deposited when capacity exceeds incoming load sediment is eroded from the streambed detailed calculations of those modifications can be found in si section 2 1 2 unlike streamflow sediment and nutrients that are dependent on its spatial reach network and in stream processes the total watershed crop production is simply the sum of that in each subwatershed si section 2 1 3 3 3 2 artificial neural networks for wwt wwt plants in the corn belt can contribute considerable point source nutrient loading nitrate n tp due to combined sewerage from stormwater domestic and high strength industrial especially from biorefineries wastewater the wwt component of iteem includes four wastewater treatment plant design alternatives to treat the combined influent the four alternatives include 1 activated sludge as 2 activated sludge with chemical precipitation ascp to reduce effluent p concentrations from the wwt plant 3 a modified 5 stage bardenpho process with enhanced biological phosphorus removal ebpr 4 and a modified 5 stage bardenpho ebpr process with struvite magnesium ammonium phosphate recovery ebpr str we include the impact of stormwater that causes highly variable treatment performance during the process development using gps x software detailed descriptions of process development for each treatment alternative are provided in si section 1 2 as the wastewater treatment involves complex and nonlinear physical and biological processes advanced data driven techniques can be applied to predict treatment performance under fluctuations of influents artificial neural networks anns have been widely applied in various fields to capture nonlinear complex relationships between inputs and outputs for a generic ann a vector of input data x can be mapped to a vector of output data y i e y f a n n x where f a n n x represents a function of neural networks in this study four feed forward back propagation anns are applied to surrogate the four wwt alternatives i e as ascp ebpr ebpr str once plant layouts of the four wwt alternatives are designed and optimized we simulate stochastic influent conditions and run process simulations for each wwt design alternative to account for wwt performance variability each treatment alternative is simulated 10 000 times in the original high fidelity model using the gps x software and the dataset is split 60 20 20 into training validation and test datasets details of anns training are provided in si section 2 2 after successfully training anns for the four treatment alternatives the next step is to predict the effluents under stochastic conditions of influent using anns to be consistent with the temporal scale of the swat component and reduce the computational time we simulated monthly loading from the wwt assuming each month is run as a steady state for each month the total inflow domestic and industrial wastewater rainwater is determined using historical data while the chemical oxygen demand cod total kjeldahl nitrogen tkn total phosphorus tp are randomly sampled 1000 times from their fitted historical distributions since swat simulations provide deterministic monthly values the monthly mean values of effluent loadings from the 1000 simulations are calculated and added into the subwatershed where the wwt is located this is a key step to integrate point source and nonpoint source pollutant loadings from the different components the techno economic analysis of the four treatment alternatives is conducted using a combination of modeling and calculations specifically the capital costs e g construction and fixed operational costs e g labor maintenance are calculated in capdetworks a proprietary software compatible with gps x for estimating fixed costs capital labor maintenance cost for wwt models operational costs that vary with influent characteristics are calculated with process design and cost estimate equations from the us environmental protection agency harris et al 1982 3 3 3 lookup tables for grain processing two grain processing gp models i e corn wet milling and corn dry grind are developed in superpro designer intelligen inc which contains rigorous reactor modules for mechanical and chemical engineering of corn grain processing juneja et al 2019 2020 and details of process development are provided in si section 1 3 since superpro designer is commercially programmed and cannot be directly connected with the other iteem components we develop lookup tables that store results simulated from superpro designer the lookup tables contain two plant layouts for each plant capacity the capital cost operational cost energy and water use and p content of cgf and ddgs are simulated for each plant capacity in both wet milling and dry grind corn processing models two plant layouts are simulated 1 status quo grain processing without p recovery and 2 alternative technology that processes grain and recovers p as p complex which can be further purified as phytin a calcium magnesium salt of phytic acid calculations of cost and energy use are provided in si section 2 3 3 3 4 empirical equations for dwt the dwt model is developed based on operational data from a drinking water treatment plant located in the corn belt the cost data include fixed and variable costs for the nitrate removal facility nrf the fixed cost includes management overhead labor cost for operation and maintenance depreciation cost and nrf energy cost note that the daily energy consumption in nrf is assumed constant as detailed data are unavailable the variable cost includes the use of sodium chloride as the regenerant chemical for ion exchange resins and alum and polymer for turbidity treatment the consumption of sodium chloride is dependent on the nitrate level in the untreated water entering the dwt plant the consumption of alum and polymer is dependent on the sediment concentration in the raw water entering the dwt plant the costs do not include the total cost in the main treatment facility as the purpose of the dwt component is to estimate costs and energy requirements associated with excess nitrate and sediment treatment only the nitrate n no3 n and sediment loadings and streamflow estimated from swat in the subwatershed where the dwt plant is located are inputs to the dwt plant component the decision to operate the nrf is based on daily no3 n concentration in the untreated water entering the dwt plant the nrf will operate on any day where the influent no3 n concentration exceeds the threshold of 8 0 no3 n mg l based on the current maximum contaminant level mcl of 10 mg l that is we assume the operation threshold nitrate concentration to operate the nrf is 80 of the mcl calculations of cost and energy use for nitrate and sediment treatment are provided in si 2 4 3 3 5 theoretical empirical equations for economics 3 3 5 1 non market valuation for water quality improvement choice experiments ces are conducted to elicit the general public s wtp for water quality improvements and farmers wta payment to change management practices ces are a widely used non market valuation method in which respondents are asked to select the most preferred alternative in a hypothetical decision making situation while varying the levels of different attributes of interest louviere et al 2000 we utilize fractional factorial designs to allocate attribute levels i e cost recreational options attainment of nutrient loss reduction goals to each alternative a single combination of attribute levels and choice sets a combination of different alternatives to choose from the responses to the ces are analyzed using statistical methods based on random utility theory rut the utility of an individual survey respondent n from alterative a u n a includes systematic v n a and stochastic ε n a components with u n a v n a ε n a β t x n a ε n a vector x n a contains attribute levels faced by individual n in alternative a and β is a vector of coefficients estimated corresponding to the attributes because the non market valuation study is not yet complete this study uses a published wtp estimate from a related study parthum and ando 2020 to demonstrate how the choice experiment results will be used in the iteem 3 3 5 2 evaluation of iteem total costs and benefits after estimating the costs of engineered technologies and agricultural management practices the total economic net benefits of the entire system are calculated for each option each option includes a combination of wwt and bioprocessing technology together with a spatially explicit configuration of agricultural practices total benefits include the sum of revenue from product sales and non market benefits associated with water quality changes and the total cost is the sum of those incurred for wastewater and drinking water treatment grain processing and agricultural management practice implementation the total net benefits are calculated as 1 δ b b w t p r r p r c r o p r g p c w w t c d w t c g p c a g where δ b yr is the economic total net benefits for a given option b w t p yr is the monetary measure of public wtp for water quality improvements r r p yr is the revenue generated by selling recovered p product r c r o p yr is crop revenue r g p yr is the revenue from grain processing products sold c w w t yr is the cost of wastewater treatment c d w t yr is the cost of drinking water treatment c g p yr is the cost of grain processing plants c a g yr is the total cost of all agricultural management practices applied a given scenario all the terms in eq 1 are annualized cash flow that takes factors of the time value of money and inflation into account detailed calculations of terms in eq 1 are provided in si section 2 3 4 sensitivity analysis of iteem as each of the components contributes uncertainty to iteem via bmps environmental engineering technologies and policies it is important to investigate how the uncertainties from different component models propagate and affect the overall outputs of iteem at the system level the multiple sources of uncertainties which can be correlated complicate the sensitivity analysis for a demonstration purpose we conduct a simple one at a time oat sensitivity analysis of key parameters and leave more complete global sensitivity analysis for future work which indeed can be standard alone study we use a sensitivity indicator calculated as below 2 s e n s i t i v i t y i j 1 k 1 k δ o u t p u t j k o u t p u t j k b a s e l i n e δ p a r a m e t e r i k p a r a m e t e r i k b a s e l i n e where s e n s i t i v i t y i j represents the averaged relative change of parameter i on output j across different scenarios k δ o u t p u t j k is the change of output j of scenario k from baseline output o u t p u t j k b a s e l i n e δ p a r a m e t e r i k is the change of parameter i of scenario k from baseline p a r a m e t e r j k b a s e l i n e the multi dimension outputs of iteem are aggregated into four categories 1 water quality and quantity 2 energy consumption of engineering systems 3 costs and benefits 4 production of crop and recovered p the key parameters investigated in iteem includes six parameters from swat e g runoff curve water capacity in the soil soil evaporation etc two parameters i e influent nutrient strength and inflow in wwt model and other parameters i e crop price chemical price utility cost willingness to pay per household and interest rate related to costs and revenues across various models detailed descriptions and the range of each of these parameters are provided in si table s4 4 computational implementation of iteem in object oriented programming platform a coherent computational framework is developed to link and execute models from individual knowledge domains in an orderly manner standards of integrated modeling have been promoted by researchers to produce a useable and low friction simulation environment such as the community surface dynamics modeling system csdms project by peckham et al in 2013 the design criteria include but not limited to support of multiple operating system use of open source tools rather than proprietary software ease of reusability and maintenance etc we develop iteem using the object oriented programming in python which fits the several standards promoted by csdms an object oriented framework connects models as inherited objects where some models are parent objects for others there are several advantages for using an object oriented framework first by inheriting attributes variables of an object and methods functions of an object new child objects can be easily built which meets csdms design criterion of code reusability second creating various methods within the same object allows distinguishing separate functionalities e g technology cost versus treatment performance thus exhibiting a clear structure for the ease of maintenance the iteem developed in the object oriented language can also be easily converted to a different language using language interoperability tool e g babel peckham et al 2013 there are some other features that need to be improved in the future such as the support of serial and parallel computation the five component models of iteem are modularized as five independent objects note that the five component models of iteem are represented either in not original primary model at the process level they are rather their surrogates derived from primary process models or simplified or empirical models which are all integrated at the system level for example class swat is simulated by its surrogate model the response matrix method variables are stored as attributes functions are partitioned into various methods within each object fig 5 shows the implementation and integration of iteem described in the unified modeling language uml routines of data exchange are specified either in the attributes or inputs of methods in objects specific outputs of interest are obtained by calling specific methods for example a method called get loading nitrate is defined within the swat object to obtain nitrate loading specific to a particular scenario instead of calling swat to produce all outputs simultaneously based on the five objects that represent the five components respectively an overall object that integrates the five components is created as an iteem object that incorporates attributes variables and methods functions from all components into a single entity such an entity provides a computationally efficient model based on a large set of interconnected technology environment and economic relationships the detailed descriptions of attributes and methods are provided in si table s3 5 demonstration of iteem in the upper sangamon river watershed illinois we demonstrate iteem via a testbed watershed the upper sangamon river watershed usrw different scenarios are tested to explore a portfolio of alternative engineering technologies policies and bmps the results of the scenarios are compared to a baseline scenario in terms of multiple few systems indicators 5 1 study area for a testbed watershed the usrw located in central illinois usa is selected as an illustrative testbed for its few nexus issues data availability and existing modeling studies for this watershed fig 6 water quality in the usrw is threatened by both agricultural runoff and municipal and industrial nutrient discharges the relatively flat prairie soils in the watershed are highly productive extensively underlain by subsurface drainage systems and cultivated for maize and soybean production within the watershed lake decatur created by a dam on the sangamon river is the source of municipal water supply for the city of decatur and the village of mount zion combined population of 79 000 and industrial water supply for grain processors the lake has been classified as impaired because of high nitrate and p concentrations and low dissolved oxygen periodic dredging of sediment has been necessary to maintain the lake s storage capacity the cost of nitrate p and sediment delivered to the lake from agricultural runoff has been born by the water and wastewater ratepayers in decatur and mount zion the sanitary district of decatur sdd treats stormwater industrial wastewater and domestic wastewater sdd discharges treated effluent to the sangamon river downstream of the lake decatur dam the total discharge of sdd is approximately 600 mg no3 n yr and 582 mg p yr the largest of any facility in the state of il at concentrations typically ranging from 6 to 10 mg no3 n l and 5 30 mg p l sdd is faced with the challenge of complying with an impending effluent standard of 1 mg p l this is a major challenge because influent concentrations from the biorefineries responsible for approximately 90 of sdd discharge are more than twice the typical high range for effective biological p removal other wwt plants discharge in the watershed but their contribution to nutrient pollution is relatively small 5 of total point source tn and tp the three corn grain processing facilities located in the usrw have combined processing capacity of 8 1 million tonnes of corn per year in addition one major dairy feedlot produces an estimated 9400 metric tonnes of manure per year from around 3100 milking cows the status quo p content of manure is assumed to be 9 5 g kg dry fecal matter if p can be recovered from grain processing plant waste streams the p concentrations in manure can be reduced to 6 3 g p kg dry manure see section 3 3 3 5 2 scenario analysis iteem enables us to quantify the impacts of various nutrient management strategies technologies and policies that could enhance the beneficial synergies of few systems in the corn belt to illustrate the use of iteem we simulate and compare four scenarios of agricultural management practices engineering technologies and drinking water standards the baseline scenario consists of the status quo agricultural management practices wwt technology wwt i e activated sludge without rp and gp technology wet milling and dry grind corn processing without rp and current mcl policy governing nitrate in drinking water i e 10 no3 n mg l the scenario definitions are provided in table 1 5 3 results and discussion 5 3 1 surrogate model performance reducing complex process models into simpler surrogates almost inevitably introduces new uncertainty therefore it is imperative to ensure acceptable performance for the various surrogates used in iteem compared to the original high fidelity models here we evaluate the performance of three surrogate models i e rm for swat anns for wwt and lookup tables for gp for dwt and economics components they are already developed as empirical or theoretical empirical equations at the process level and there are no additional surrogates applied to upscale them to the system level the primary component models e g swat wwt gp have been developed in different software packages juneja et al 2019 2020 which however is not the focus of this study in fact some software packages such as wwt have consensus models that have been validated and used in engineering design traditionally the rm method is used for landscape loss estimates housh et al 2014 in this work we extended the rm to account for reservoir trapping point source loading and some in stream processes authors who are interested the details of the modified rm please contact the corresponding author we chose two widely used goodness of fit measures to assess the performance of the rm method percent bias p bias and nash sutcliffe efficiency nse moriasi et al 2007 the ideal value of p bias is zero indicating no long term overestimation or underestimation with positive values indicating overestimation and negative values underestimation nse varies from negative infinity to one one indicating a perfect match between the rm results and swat results and with values less than zero indicating that model prediction is less accurate than using the mean of observed data to test the performance of the rm method we select a combination of five bmps with randomly assigned agricultural land area compare the results using the rm method versus swat and present detailed results of one realization in fig 7 details regarding the five selected bmps can be found in si table s5 and the spatial agricultural land area allocations for the realization demonstrated in fig 7 can be found in si table s6 the rm method almost perfectly predicts streamflow and nitrate loads as evidenced by both p bias nearly 0 and nse nearly 1 in fig 7a and c this is because nitrate loading and streamflow are minimally influenced by in stream processes in the study watershed streamflow and nitrate load at the watershed outlet are very similar to the sum of the loads from individual subbasins minus a constant percentage trapped by the reservoir the rm method also predicts the total phosphorus tp and sediment loading with high accuracy in general p bias 4 9 and nse 0 911 for tp p bias 3 35 and nse 0 968 for sediment for tp major discrepancies between the rm method and swat simulation are observed during the low streamflow periods e g 2012 2013 this discrepancy arises because the rm method accounts for in stream p settling and biological p uptake respiration by applying a constant percent reduction to tp loss from the landscape 11 percent the constant percent reduction corresponds to some representative travel time for water through the channel network when water spends more time within the local channel network the impact of the in stream processes grows and accordingly the swat simulation shows the stream acting as a stronger p sink than does the rm method as discussed above sediment is predicted by applying a simplified bagnold equation to the rm estimates for streamflow fig 7b we apply streamflow constrained equations to estimate the in stream sediment loading the flow constrained method performs better than simply summing up the landscape losses of sediment this is because in this tested watershed sediment transport in streams is controlled by streamflow not landscape sediment loss to test the robustness of the performance of the rm method we run 10 realizations the performance of the rm method remains stable as shown by the detailed results of 10 realizations in si table s7 overall the rm method in iteem performs satisfactorily as a surrogate for swat across an explicitly spatial and temporal scale however future efforts should be devoted to incorporating a more realistic depiction of sediment deposition and degradation processes the anns exhibit satisfactory performance in surrogating the complex process based wastewater treatment model wwt using the simulation data from each of the four wwt alternatives the anns shows high prediction accuracy for all outputs mse 0 001 and r squared 0 95 as shown in si fig s7 detailed data of mse and r squared for each output in each wwt alternative are provided in si table s8 note that among the outputs predicted by the anns the total nitrate and phosphorus loading from wwt is coupled with the rm method to account for total nutrient loading for the watershed for the gp model surrogated by lookup tables all data stored in lookup tables are directly from the simulation results in the high fidelity model we assume that each gp plant is operated at steady state and at its plant capacity and the simulation data are extracted directly from lookup tables therefore there is no additional uncertainty introduced to this surrogate model 5 3 2 suitability of surrogate modeling and applicability of surrogate based model coupling as introduced in section 3 1 we develop surrogate models for complex process models first and then couple the surrogates along empirical models for water supply and economic analysis to formulate iteem the surrogate based model coupling method is suitable only if the following conditions exist first if primary process models cannot be integrated directly with compatibilities among discipline specific computer programs as discussed by little et al 2019 second the coupled primary process models are not computationally tractable especially it is difficult if not possible to use the coupled models for decision analysis e g being coupled with an optimization algorithm e g genetic algorithm to find optimal solutions while surrogate modeling has also been applied to acting as emulators and approximate uncertainty quantification in many domains alemazkoor and meidani 2020 razavi et al 2012 wu et al 2014 the difference between surrogate modeling and surrogate based model coupling should be noted surrogate modeling assesses a surrogate to one single high fidelity simulation model and uses it for certain modeling purposes surrogate based model coupling assesses the joint application of multiple surrogate models derived from multiple process simulation models in a consistent modeling framework various other model coupling methods have been applied to couple complex process models together such as wrapper model scripting and model translation those methods couple high fidelity models directly and thus have higher accuracy for instance anderson et al 2018 integrated dssat a crop simulation model and greet a simulation model for energy use and emissions for various vehicles models via their application programming interfaces apis to simulate the inter relationships between crop production and environmental impacts of biofuel production anderson et al 2018 however not all software developers provide apis especially for commercial software packages xiang et al 2020 integrated dssat with modflow a groundwater simulation model by writing scripts for external controls on both models in a batch mode in python model translation constructs different individual models from scratch in a common platform however this method can only be feasible for simple simulation models as it requires rewriting all equations included in the primary simulation models malard et al 2017 there is not a single model coupling method that is deemed to be better than others under all cases users should consider their strengths and weaknesses for particular applications for few systems analysis present in this study with appropriate simplifications as described above our surrogate based model coupling approach can deliver a computational tractable integrated model at the system level 5 3 3 tradeoffs and limitations of surrogate based integration design choices various spatial and temporal scales exist in different component models for example the temporal scales can be daily monthly and annual the spatial scales can be a point watersheds small and large and river basin for different component models our integration design choices on selecting targeted spatial and temporal scales and their interaction points are driven by decision making requirements for the model as well as technical considerations one of our study purposes is to evaluate solutions based on bmps and environmental engineering technologies for combined point and non point source nutrient management we chose a monthly temporal scale for both point source and non point source simulation which is usually sufficient to maintain nutrient mass balance for water quality related decision making the daily raw nitrate level is important and thus the dwt model uses a daily time scaler correspondingly we use the results of swat at a daily scale to estimate the daily raw nitrate at the sub watershed where the dwt locates technically there is a typical tradeoff between the modeling accuracy and computational requirement in the choices of temporal and spatial scales and the aggregation level for the integrated model in particular the dwt is modeled at a point scale where a treatment plant takes raw water from a storage or a river segment within the study watershed it would be is ideal to have the nitrate concentration at a finer spatial scale because of the spatial variance of the nitrate concentration however swat only simulates the in stream loads of nutrients at a sub watershed level and the average nitrate concentration in the sub watershed is taken for the simulation for wwt the effluent of point source nutrients is dynamic and impacted by domestic influent fluctuations as well as wet weather however dynamic modeling of wwt over a long term period is challenging due to the lack of detailed knowledge of influent wastewater characteristics and of the rainfall translation to plant influent and operation changes eventually we use a steady state approach to simulate monthly point source nutrients such a temporal aggregation from nearly real time to monthly scale limits the capability of the wwt model in simulating peak stormwater demand caused by an extreme rainfall event to rigorously quantify the tradeoffs between accuracy and computational time due to different choices of spatial and temporal scales would be an interesting investigation we do not explicitly quantify such tradeoffs in this paper as we focus more on developing an integrated model that tightly couples process based and empirical models at the same platform that can be used to test hypotheses and generate insights for watershed nutrient management 5 3 4 tradeoffs among food energy water and economics to demonstrate how iteem can be applied to explore tradeoffs among multiple metrics of few systems outputs from the three alternatives and the baseline are simulated using the method run iteem to facilitate tradeoff evaluation we normalize the performance of each metric indicator from 1 to 3 with 1 indicating the worst and 3 indicating the best among all scenarios as shown in fig 8 a the minimum and maximum indicator values are provided in fig 8b compared to scenarios 1 3 the baseline scenario has the lowest overall performance for water quality and quantity indicators but the best overall performance on energy consumption cost of technologies gp and wwt and crop production the result arises because all three alternatives introduce best agricultural management practices upgrade the existing technology to recover p and advance point source p removal the non market benefits represent the estimated willingness to pay of general public living in upstream of decatur reservoir for increases in likelihood of achieving the nutrient reduction target 45 by 2045 in illinois based on the study of parthum and ando 2020 their study estimates that each household located upstream of the reservoir would be willing to pay 0 95 per year to increase the likelihood of meeting the nutrient target by one percentage point using the estimate we simplify the non market benefits under each scenario as 0 95 household year multiplied by 113 700 approximate number of households in the upstream of the reservoir and then multiplied by the extent to which a scenario attains the 45 target for nitrate n or tp e g if a scenario reduces tp by 45 then this value is 100 the values in fig 8 are the sum of benefits derived from reduced nitrate and tp loads according to this formulation the three alternatives generate environmental benefits ranging from 3 0 to 8 1 million dollars yr however those environmental benefits are not enough to offset the associated costs to upgrade treatment technologies and adopt new agricultural bmps as evidenced in fig 8a the baseline scenario provides greater total benefits than do the three alternative scenarios note that the non market benefits will be updated based on our choice experiment results and the results might change considerably among the three alternative scenarios scenario 3 has the best nitrate reduction and second best tp reduction due to the adoption of cover crops and the choice of modified bardenpho enhanced biological phosphorus removal with struvite recovery ebpr str for wwt the nitrate reduction in scenario 3 also results in decreased energy consumption and cost associated with nitrate removal at the dwt scenarios 1 3 reduce the dap fertilizer application rate by 15 15 and 30 respectively but the impact on corn yield is relatively negligible this could be because either the baseline is currently over applying dap fertilizer or there is enough p accumulated from prior years to make up the gap in crop demand for p overall the three alternatives illustrate the tradeoffs between reduced nutrient loading energy demand and cost for alternative technologies 5 3 5 sensitivity analysis of iteem we conduct the sensitivity analysis for key parameters including six parameters from swat two parameters from the wwt and the other eight parameters of benefits and costs see fig 9 for the list of the parameters note that the ww nutrient parameter in the wwt model varies the influent cod tkn and tp altogether details provided in si table s4 the heatmap fig 9 shows the sensitivity results with parameters listed horizontally and multi dimensional outputs listed vertically under the four scenarios baseline three alternatives in table 1 investigated in this study we consider a change of 20 as upper and lower bound for most key parameters except for the interest rate and willingness to pay per household as mentioned earlier the sensitivity results of the key parameters are scenario dependent values in the heatmap represent the change to an output responding to one unit change of a parameter for example the sensitivity of parameter swat runoff curve on the output nitrate load is 2 1 as shown in the top left corner of the heatmap meaning that a 1 change of the runoff curve value from the baseline can cause a 2 1 change of nitrate load at the outlet it is found that the six swat parameters have the most significant impacts on water quality i e nutrient and sediment load and quantity i e streamflow and crop production the uncertainty from those parameters further propagates into crop revenue the willingness to pay wtp and the total net benefit the total net benefit is also significantly impacted by the market prices of products e g starch and ethanol from grain processing plants and the cost of feedstock e g corn sold for grain processing as the profit from gp has a large contribution to the total net benefit in contrast the prices of chemicals rp and utility e g electricity and natural gas have negligible impacts across the outputs the two parameters from the wwt model are evaluated with the four treatment alternatives as for baseline ascp for s1 ebpr acetate for s2 and ebpr str for s3 corresponding to the four scenarios provided in table 1 the influent nutrient strength ww nutrient and inflow of wastewater ww inflow have a noticeable impact on the energy use of wwt the amount of recovered struvite and the wtp which demonstrates that the uncertainty of influent characteristics from the wwt model can have a significant impact at the system level outputs however we also have two interesting observations 1 parameters from the wwt model do not have a significant impact on tp load at the outlet fig 9 despite the fact the point source p is the leading contributor to the total tp load for the testbed watershed 2 parameters are sensitive on the outputs point source only of individual wwt treatment alternatives for example the sensitivity of ww influent for the four treatment alternatives is 1 38 for point source nitrate and 0 87 for point source tp details provided in si table s9 however the sensitivity of ww nutrient is decreased when integrated at the system level point nonpoint sources with sensitivity being 0 03 for nitrate and 0 25 for tp details provided in si table s9 both observations can be attributed to the fact that our local sensitivity analysis is scenario dependent for the cases of ascp ebpr and ebpr str the point source nutrients are significantly reduced and are not the leading tp contributor anymore therefore for the scenarios with ascp ebpr and ebpr str the parameter changes on wwt model will not have sensitive impact on total tp load which ultimately decreases its sensitivity on tp load at the system level 6 conclusions and future research addressing large scale environmental sustainability challenges requires integrated analysis of complex inter relationships within few systems this paper presents the development of an integrated technology environment economics model iteem for typical watersheds in the corn belt we use various data techniques to convert complex models simulating physical engineering processes and socioeconomic relationships into computationally tractable surrogates and link these surrogates via input output relationships within a consistent computer based modeling platform the procedures for developing iteem for a case study watershed upper sangamon river watershed usrw can be applied to other watersheds in the corn belt with required data and model preparation as shown for the usrw based on our experience developing iteem with a team including researchers from hydrology and water resources system analysis environmental engineering environmental economics and sociology we reflect on steps for selecting surrogate models i e which type of data driven surrogates is most suitable for a particular physical and process model based on data and model availability as well as the purpose of the integrated model in this study we applied the response matrix method and artificial neural networks respectively to create surrogates for swat and wwt a detailed process model for dwt is not available for our project therefore we adopted empirical equations for the dwt component the lookup table method used for the gp component is simple but sufficient since gp can be assumed to operate at steady state and is only impacted by the size of plant capacity the economics component is formulated with equations that include non market valuation estimates and overall economic net benefits in our one at a time sensitivity analysis we show to what extent the uncertainty of selected key parameters in the component models can impact the outputs of iteem we identify some critical parameters that are worthy of further investigation future work should adopt a global sensitivity analysis considering the correlation of the uncertainties from different component models as well as the uncertainty due to future climate change iteem enables testing hypotheses for few systems analysis and exploring solutions to resolve inter connected few problems for example one hypothesis to test is that the most economically efficient way to improve water quality in corn belt watersheds should be to jointly employ a combination of agricultural land management practices and p recovery from co products generated by grain biorefinery facilities or wastewater treatment it is noted that iteem is designed for evaluating long term strategic planning for few systems in the corn belt but not for evaluating short term events such as extreme rainfall events that can cause peak stormwater flow that affects both point and non point pollution future work will be conducted to evaluate few system resilience under a set of stress and disturbance scenarios last but not the least iteem will be coupled with a multi objective optimization algorithm to search for optimal technologies and policies data and code availability the data and codes are available from the corresponding author upon request declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the us national science foundation infews t1 award number 1739788 we are grateful to industrial governmental and agricultural stakeholders in decatur il for providing valuable data for developing components of the iteem we appreciate valuable comments and constructive suggestions from the editors daniel p ames and tatiana filatova and three anonymous reviewers which have considerably improved the quality of this work appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105083 
25778,air quality has become a central issue in public health and urban planning management due to the proven adverse effects of airborne pollutants considering temporary mobility restriction measures used to face low air quality episodes the capability of foreseeing pollutant concentrations is crucial we thus present socaire spanish acronim for operational forecast system for air quality an operational tool based on a bayesian and spatiotemporal ensemble of neural and statistical nested models socaire integrates endogenous and exogenous information in order to predict and monitor future distributions of the concentration for the main pollutants it focuses on modeling available components which affect air quality past concentrations of pollutants human activity and numerical pollution and weather predictions this tool is currently in operation in madrid producing daily air quality predictions for the next 48 h and anticipating the probability of the activation of the measures included in the city s official air quality no2 protocols through probabilistic inferences about compound events graphical abstract image 1 keywords air quality spatio temporal series statistical modeling neural networks 1 introduction during the last decades an increasing number of studies point out that degraded air quality is a major problem in cities around the world martuzzi et al 2006 héroux et al 2015 while there is general consensus that it causes health problems kim et al 2015a özkaynak et al 2009 how dangerous it can be is still a matter of debate sellier et al 2014 even in the best case scenario this seemingly endemic issue affecting the life in big cities is already considered one of the main causes of both direct and indirect mortality badyda et al 2017 of all the tools and systems that help fight pollution the prediction of future pollutant concentrations or levels is of principal importance for air quality management and control bai et al 2018 air quality forecasting systems allow for sending out warnings of upcoming high pollution episodes to the population in the short term so that appropriate measures can be taken to minimize as far as possible the damage caused by these episodes as an example the city of madrid in order to comply with european regulations european union 2008 devised an air quality protocol which includes restrictions to the use of polluting vehicles when the concentrations of no2 reach certain thresholds consequently foreseeing the activation of such restrictions is critical both for the decision makers which need to announce them in advance and for the vehicle owners which need to plan their transport alternatives the use of data driven approaches to predict and control air quality is not new following the discussion started in breiman 2001 when approaching a modeling problem two families of methodologies or cultures in breiman terms coexist the data modeling culture based on the search for a stochastic data model for example time difference equations in the case of air quality that capture the inner behavior of the intervening physical processes and the algorithmic modeling culture based on the use of algorithms to directly learn the model from data given that pollutant concentrations can be seen as time series the stochastic data modeling usually deals with using arma based methods kumar and jain 2010 hassanzadeh et al 2009 however this kind of models have trouble handling high non linearities and high dimensional environments to solve this problem and thanks to the big amount of data that is gathered nowadays machine learning models have been applied to environmental modeling with some success grivas and chaloulakou 2006 navares and aznarte 2020 in this work however we advocate for a hybrid culture in which stochastic data models are combined with algorithmic ones in a way which permits harvesting the benefits of both approaches while reducing their disadvantages as we will show due to the increase in the available computing power and the advances in the field of neural network based models it is nowadays common to find real applications in which algorithmic modeling is put into practice to predict air quality for example in nebenzal and fishbain 2018 a system is deployed in which pollution levels based on a threshold are used to study transitions among states which makes possible to estimate high pollution episodes in the long term in thatcher and hurley 2010 authors have combined the tapm and ccam atmospheric models to form a customizable local scale meteorological and air pollution forecasting system showing that using macroscopic models in the local scale can provide positive points in the prediction macia g et al 2019 is an example of an ensemble model with a neural network and an arima in a similar vein to what will be our proposal applied successfully to a real urban environment in the city of london for this kind of real applications it is useful to produce instead of a forecast of the expected value of the magnitude under study an estimation of the full future distribution which in turn allows for decision making based on the probability of the surpassing of certain thresholds this idea which will ultimately be the main goal of the system described below is common in other fields and was introduced to air pollution forecasting by aznarte 2017 the integration of meteorological information and human activities have been addressed by multiple studies some relevant variables as temperature precipitation or wind speed have shown to be good indicators of pollution levels kalisa et al 2018 ouyang et al 2015 kim et al 2015b also the physical chemical mechanisms governing the relationship between these and air quality has been studied in vega garcía and aznarte 2020 interpretability techniques for deep learning are used to gain insight into feature importance in a highly similar environment and methodology to ours concluding that weather variables in general have a high impact when using machine learning methods for predicting pollution however while these issues have been widely studied from the univariate time series perspective the observed spatial interactions between nearby observation stations might be of importance too as air quality at different stations might be implicitly related spatial based approaches usually imply assuming or learning these interdependencies based for example on closeness but as it has been shown de medrano and aznarte 2020 this is not necessarily the most natural and optimal way to go in this paper we introduce socaire spanish acronim for operational forecast system for air quality the new official air quality monitoring system for the city of madrid this tool in operational use nowadays makes use of both external and internal variables related to air quality in order to forecast pollutant concentrations it is a complex modular mathematical system composed of an ensemble of data manipulation techniques and models that let us exploit different knowledge in each module from data cleaning and imputation through handling spatial and meteorological non linear features to integrating human behavior and its patterns by correctly treating all this information it is possible to avoid redundancies and to achieve very high performance as one of the biggest and most populated cities in europe madrid is a perfect setting for developing and testing these kind of systems the rest of the paper is organized as follows in section 2 the problem is stated and madrid s air quality protocol for no2 is described while section 3 presents an analysis and explanation of the different data sources and the data wrangling process section 4 presents the proposed approach for air quality forecasting then in section 5 we introduce the bayesian probabilistic framework that let us accommodate socaire to the no2 protocol section 6 shows the evaluation of the proposed architecture after appropriate experimentation and its comparison with other methodologies finally in section 7 we point out future research directions and conclusions 2 problem statement 2 1 study area and general information through this work we look for a system which is able to predict up to 48 h of four of the main existing pollutants nitrogen dioxide no2 ozone o3 and particulate matter pm10 and pm2 5 where 10 and 2 5 denote the maximum diameters in micrometers of the particles this estimation needs to be done in the 24 stations that compose the pollution measurement network each one with different pollutants since one of the main objectives of the system is to anticipate the activation of mobility restrictions in face of high pollution episodes we forecast the main quantiles of the distribution so it is easier to make decisions based on pollution level probabilities thanks to its bayesian estimation of compound events socaire becomes an ideal tool to foresee the scenarios of madrid s no2 protocol which will be explained later in this section socaire operates daily on a 48 h basis it produces forecasts from 10 00 of the present day to 09 00 two days later in the spatial dimension the measurement stations of the city council are used as reference points specifically there are 24 stations distributed throughout the city with sensors capable of recording different pollutants fig 1 a shows graphically the location of all the stations at the same time the city considers 5 different areas in the city that are related to the activation of the no2 protocol these areas are shown in fig 1b table 1 shows the correspondence between the different stations and their code their location and the pollutants measured at each one 2 2 the no2 protocol of the city of madrid in 2018 the city council of madrid approved an action protocol for no2 pollution episodes madrid protocol 2018 from this point referred to as the no2 protocol which defines a set of increasing alert levels thus classifying the situations of high concentrations of no2 as follows 1 prewarning when any two stations in the same area simultaneously exceed 180 μgm 3 for two consecutive hours or any three stations in the surveillance network simultaneously exceed the same level for three consecutive hours 2 warning when any two stations in the same area exceed 200 μgm 3 during two consecutive hours or any three stations in the surveillance network exceed the same level simultaneously during three consecutive hours 3 alert when in any three stations of the same zone or two if it is zone 4 is exceeded simultaneously 400 μgm 3 during three consecutive hours depending on the level and the meteorological prospect a set of increasingly restrictive mobility limitations will be imposed city wide by the council with the aim of mitigating and reducing the negative effects of contamination on the health and integrity of the population thus the main objective is to know when and how the conditions leading to the different alert levels will be met in order to enable the anticipation of the measures 2 3 framework overview fig 2 presents a summary of socaire s mathematical structure created to forecast and monitor pollution levels its operation is based on the compilation of several data sources which will be described in section 3 after a proper analysis and cleaning process the complete database will be used through an ensemble model composed of a cascade of nested models each one in charge of modeling different processes that alter air quality dynamics section 4 finally and thanks to the probabilistic nature of the predictions the system is able to estimate probabilities from compound events using a bayesian approach explained in section 5 that is adapted to the aforementioned no2 protocol 3 data analysis and wrangling as stated above in order to aim for the highest performance socaire makes use of all the available information related to the problem thus before introducing the actual modeling it is important to present and analyze the set of available data sources concretely as anticipated socaire uses the data of the concentrations of the different pollutants in the different stations in madrid as dependent variables output and as independent variables inputs past pollutant concentrations numerical pollution predictions coming from the european cams model numerical weather predictions served by aemet 2021 and anthropogenic information encoding different events such as holidays and school calendar the data used along this paper corresponds to the period july 2016 october 2020 both included the following subsections will detail the origin peculiarities and processing of these data 3 1 pollutants the temporal behavior of each pollutant series is shown in fig 3 the daily cycle of all pollutants is dominated in one way or another by the peak hours of road traffic except for ozone the other three pollutants to be analyzed have their daily peaks after peak traffic hours the no2 has the most intense traffic sensitive cycle followed by the 10 and 2 5 microparticles which show a delay of about an hour with respect to the no2 o3 presents a daily cycle that is practically inverted with respect to the rest everything said for the daily cycle applies to the weekly cycle with weekend being days with lower levels of traffic it can be assumed that holidays and long weekends will behave as public holidays so the forecast model would have to take this into account as expected the daily cycle is not independent of the weekly one but each day of the week has its own cycle especially different on weekends from working days in the annual cycles a greater variety of behaviors can be observed all pollutants especially ozone rebound in summer except no2 which has the opposite behavior in this case respect to the spatial dimension fig 4 represents the empirical distributions for each pollutant it can be seen that all stations report a similar behavior without clear relation patterns between closeness and distribution this fact will be of interest later when taking into account these spatial relationships in the modeling process as the distributions show a clear asymmetry logarithmic transformations are used pollutant data is publicly available at the open data portal of madrid madrid council 3 2 numerical weather predictions nwp as mentioned in section 1 meteorology has shown to be especially important for air quality hence having weather forecasts for the period in which the air quality forecasting is being made is expected to positively impact the precision of the forecasts in this work we use nwp from the integrated forecasting system ifs of the ecmfw blanchonnet 2015 for the following set of variables boundary layer height in meters this parameter is the depth of air next to the earth s surface which is most affected by the resistance to the transfer of momentum heat or moisture across the surface the boundary layer height can be as low as a few tens of meters such as in cooling air at night or as high as several kilometers over the desert in the middle of a hot sunny day when the boundary layer height is low higher concentrations of pollutants emitted from the earth s surface are found surface pressure in pa this parameter is the pressure force per unit area of the atmosphere on the surface of land sea and in land water it is a measure of the weight of all the air in a column vertically above the area of the earth s surface represented at a fixed point air pollution is especially prominent where high pressure dominates subsiding motions within an anticyclone suppress air trying to rise off the surface adiabatic warming of subsiding air creates a subsidence inversion which acts as a cap to upwardly moving air pollution problems dissipate when a low pressure system replaces a retreating anticyclone temperature in k this parameter is the temperature of air at 2 m above the surface of land sea or in land waters generally higher temperatures and hotwaves are directly related to episodes of higher pollution levels precipitation in mm this parameter is the accumulated liquid and frozen water including rain and snow that falls to the earth s surface it is the sum of large scale precipitation that precipitation which is generated by large scale weather patterns such as troughs and cold fronts and convective precipitation generated by convection which occurs when air at lower levels in the atmosphere is warmer and less dense than the air above so it rises precipitation parameters do not include fog dew or the precipitation that evaporates in the atmosphere before it lands at the surface of the earth air pollution is typically negatively correlated to the quantity of rainfall existing a so called washing effect of precipitation u wind component in ms 1 this parameter is the eastward component of the 10 m wind it is the horizontal speed of air moving towards the east at a height of 10 m above the surface of the earth pollutants tend to concentrate in calm conditions when wind speeds are not more than about 3 ms 1 speeds of 4 ms 1 or more favour dispersal of pollutants which literally clears the air v wind component in ms 1 this parameter is the northward component of the 10 m wind it is the vertical speed of air moving towards the north at a height of 10 m above the surface of the earth again wind is highly related to pollution dissemination nwp are interpolated to the location of each station of the air quality monitoring network as pointed out previously these forecasts are provided by aemet in an hourly basis the spatial resolution of these forecasts is 0 05 0 05 in a regular grid while the temporal resolution is hourly with up to 56 horizons 3 3 numerical pollution predictions npp cams c provides a four day horizon hourly pollution forecast which covers all europe on a synoptic scale the model takes into account global and regional numerical weather predictions from the ecmwf marécal et al 2015 as well as other types of forecasts about the production of certain chemicals of natural and human origin from models such as c ifs forecasts or cams 81 all these models always refer to a geodesic grid of between 10 and 20 km on each side so it is not very sensible to use them to directly forecast the concentrations with the resolution required inside a city which might well be below 1 km 3 4 anthropogenic features as we saw in fig 3 depending on the human activity the temporal patterns of the series are different similar to weekends and months public holidays and other designated days as well as the school calendar have a significant influence on road traffic giving rise to a very different daily cycle in special dates we usually find a lower intensity in the center but a punctual growth in other places particularly on the main access roads to the city related to holiday departures and returns also each type of calendar effect has different effects on each hour of the day in addition some of them can fall on saturday or even on sunday in the case of christmas eve and new year s eve and it is clear that the effect cannot be the same as when it falls during the week so all these issues must be taken into consideration in our particular case we will take into account the following aspects public holidays public holidays long weekends and special days such as christmas eve and new year s eve are characterized by significantly less road traffic than a normal working day apart from other departure and return operations that may occur on some of these days and which will be taken into account later it has been observed that public holidays have different effects both in terms of level and intraday evolution depending on their location within the year probably due to climate reasons hours of light and living patterns holiday departures and returns extraordinary periods such as bank holidays long weekends or even weekends cause a temporary exodus of citizens with large accumulations of vehicles in the so called departure and return operations departure operations can take place during the evening of the eve of the first non working day or during the morning of that day while return operations occur mostly during the evening of the last holiday sometimes reaching the early morning of the next working day as with other variables the effect varies with the hours within a relatively soft form school calendar in spain school calendar and schedule is highly related to usual hourly weekly and monthly patterns and so it can model with high precision the daily living the school day can be complete or normal average pre and post holidays or non existent either in isolation or for summer winter nor spring holidays each type of day other than the normal one is introduced as an effect with a different intraday cycle between 07 00 and 08 00 by combining all these variables we ensure that the information relating to human mobility in the city is covered both for normal situations and for special events these exogenous variables are defined for each station as not all parts of the city have the same dynamics 3 5 data wrangling when working with such diverse data sources is usual to deal with very heterogeneous formats and criteria which implies that pre processing and cleaning steps are of utmost importance some of the most important ones for this project are listed in this section firstly some sources use utc time and others use madrid s local time in addition the processes that transfer data between different programming environments r tol and python also have to take into account that each of these systems work differently with respect to winter and summer daily savings time changes secondly both nwp and npp distribute their forecasts in a different geodesic grid which in turn does not coincide with the coordinates of the pollution monitoring stations at first an attempt of interpolation was made by using the three closest grid points to each station as drivers but it soon became apparent that this was an excessive complication with very little added value as the forecasts were highly correlated therefore in the final version only the nearest reticular point to each station is used thirdly weather predictions are not always in the most appropriate metric so it is necessary to create derived variables that serve better as drivers of the models to begin with there are variables that change scale throughout history and it is necessary to unify the criterion for obtaining uniform series in time then there are other variables that are interesting to modify conceptually for example instead of the east west and north south coordinates of wind speed it is much better to use scalar speed which is the fundamental factor of diffusion and direction which is less important finally it is known that meteorological factors not only have an instantaneous effect but also a delayed effect that can be exercised up to a few hours later for this reason some variables delayed up to 4 h have been created and integrated with the rest of features finally since we are dealing with a cascade like ensemble of models in which the output of one is the input of another which may require a substantially different structure each level of modeling requires a series of steps to prepare the data to be as expected in the next phase let us note that the most laborious part of the data pre processing has been the imputation of missing values however given the importance of this part it has been decided to include imputation of data as part of the modeling strategy and is explained later in section 4 1 4 modeling strategy the concentration of a given pollutant in the air depends on at least two conceptually distinct groups of factors emission factors generally these are of a social order such as road traffic or heating which are predictable to some extent although there are also totally unpredictable events such as fires and others that could be anticipated to some extent such as strikes or sporting events with a multitudinous following dispersion factors basically these are consequences of the weather conditions on which there are quite precise forecasts on the horizon of 2 or 3 days ahead note that a certain factor such as rain can work in both directions at the same time on the one hand it can cause an increase in traffic on a normal working day which increases pollution but on the other hand it disperses especially the particles as they are carried to the ground which decreases pollution it is even possible that the effect is different depending on the day and time following the example of the rain that normally increases the traffic in a working day it can on the contrary contract the traffic in an exit operation when it will discourage people to leave the city this causal complexity added to the high degree of interaction between factors makes the phenomenon highly unstable and therefore very difficult to predict using any individual methodology for this reason an ensemble model composed of a cascade of nested models has been designed such that the output of each is used in the next to get the most out of each imputation techniques although this task is usually framed as part of the data wrangling process in this project it involves the development of models of some complexity due to the fact that the omitted elements are presented with a certain frequency and not always in a sporadic way but covering periods of time that can even be of several weeks these techniques are detailed in section 4 1 nned model a special flavor of convolutional neural networks called neural net encoder decoder which using as inputs the outputs of the imputation models allows to jointly forecast the concentrations of a pollutant in all the stations at the same time it takes into account the nwp and npp as well as the recent past of all stations for each input variable including the previous pollution itself and is capable of automatically detecting non linearities and interactions between different features however it does not allow for the natural treatment of irregularities in non cyclical anthropogenic factors related with traffic it is described in detail in section 4 2 pp fslr arfima qr model this is a chain of models by itself developed specifically to deal with anthropogenic factors in a bayesian way it will be explained in detail in section 4 3 4 1 imputation techniques in the different data sources it is relatively frequent to find missing data that can cause problems in the modeling process for this reason it is necessary to devise a sensible way to fill in these missing values replacing them with approximate or expected values by a series of auxiliary models when there are only very sporadic omissions of short duration it might be sufficient to apply some kind of approximation by interpolation but there might be up to consecutive weeks of data omitted in several or all variables from one or more sources at the same time thus in order to develop a robust operational system able to function even in the presence of missing data more complex and specialized techniques are required 4 1 1 trigonometric interpolation first a trigonometric interpolation is used as a univariate method to generate sensible values for those series with clear cyclical components such as temperature in our case these series present very few omissions so we consider this technique to be sufficient since the data are arranged in a regular grid this can be done by the discrete fourier transform 4 1 2 multiple imputation using additive regression bootstrapping and predictive mean matching hmisc multiple imputation using additive regression bootstrapping and predictive mean matching consists of drawing a sample with replacement from the real series where the target variable is observed i e not missing fitting a flexible additive model to predict this target variable while finding the optimum transformation of it using this fitted model to estimate the target variable in all of the original series and finally imputing missing values of the target with the observed value whose predicted transformed value is closest to the predicted transformed value of the missing value this methodology is implemented in the r package hmisc jr 2020 as the meteorological variables have already been imputed with the previous method which will be used as input here it is only applied to the npp and the pollutant concentrations themselves this method is actually used for safety in case the next one x arima fails as several parts of the framework can not handle missing data this step is required in order to assure proper functioning 4 1 3 x arima once the previous two standard imputation methods are applied it is turn for a univariate dynamic causal imputation method it analyses how both the present and the past of a group of variables including the target variable itself act on the future of this target variable these models are quite complex and to improve the imputation they are applied in two successive phases in the first one the npp are imputed as a function of the nwp in the second one the pollution observations are imputed as a function of the nwp and the npp mathematically speaking we have that being y t the time series of concentration of the pollutant in question and x t k the linearized inputs from the explanatory terms described above the general formula of the box jenkins x arima models box et al 1976 used is as follows where as usual b is the backward operator 1 δ b φ b y t k k 1 x t k α k θ b ε t the summation k 1 k x t k α k will be called the filter of exogenous effects while the equations in differences expressed by the delay polynomials will be called endogenous factors or the arma part of the model the difference between the output and the linear filter is called arima noise z t 2 z t y t k k 1 x t k α k the previously defined backward operator delays the time indicator of some element mathematically speaking 3 b k z t z t k in order to illustrate the backward notation we may show its behavior for some simple cases let us suppose that the process under study presents a regular difference δ b 1 b i e the difference between each pair of consecutive data is stationary 4 δ b z t 1 b z t z t z t 1 regarding to autoregressive polynomials ar φ b and moving average ma θ b they behave similarly suppose we have an ar of first grade φ b 1 φ 1 b and a ma of second grade θ b 1 θ 1 b θ 2 b thus equation 1 writes as follows 5 1 b 1 φ 1 b z t 1 θ 1 b θ 2 b ε t 6 1 1 φ 1 b φ 1 b 2 z t ε t θ 1 ε t 1 θ 2 ε t 2 7 z t 1 φ 1 z t 1 φ 1 z t 2 ε t θ 1 ε t 1 θ 2 ε t 2 note that this model is very different from the typical arima model with exogenous effects of the arima x class 8 δ b φ b y t k k 1 x t k α k θ b ε t which is easier to estimate but also is considered to be much less effective in explaining the phenomena that actually occur in real life see appendix a exogenous factors the nwp series has only very few isolated omitted data and in principle there is no reason to think that they will occur more frequently in the future for this reason it is more than sufficient to use an imputation system based on the fourier transform the imputation of the npp series will take as inputs the previously imputed nwp that shows quantitative relevance when imputing npp values specifically the boundary layer height blh wind speed ws and precipitation tp have been used applying different box jenkins time transfer functions box et al 1976 with different damping parameters in order to collect in a more synthetic way the time delayed transfers already discussed for the series of pollution observations both nwp and npp will be used after all of them have been already imputed endogenous factors the arima polynomials in this case are multi seasonal among the inertial factors of the stochastic process and besides the regular time hourly both the daily cycle of periodicity 24 h and the weekly cycle of periodicity 24 7 168 h are taken into account obviously there is also a pseudo annual cycle and a trend but they will be filtered by some of the explanatory drivers or exogenous factors indicated in the previous section on the one hand the annual cycle is not in harmony with the weekly or daily cycle that is its periodicity is not a whole number and on the other hand it is enormous 365 2425 24 8765 82 so it is practically intractable for the arima approach in an hourly series even in a daily series it presents serious difficulties and consumes a lot of resources a complete overview of the imputation process is shown in fig 5 4 2 neural network encoder decoder nned model given that interactions between pollution itself and other relevant features as nwp show a complex and highly non linear behavior in both time and space deep learning arises as a suitable mathematical solution no anthropogenic interactions are modeled at this point a step forward with respect to the usual deep learning architectures nned model is based on the idea of spatial agnosticism for solving spatio temporal regression problems presented in de medrano and aznarte 2020 it has been shown that when the spatial granularity of the series is low and its spatial autocorrelation is close to 0 traditional convolutional neural networks cnn fail to extract all the information from the series as the adjacency assumption for learning shared weights does not entirely hold that way it is possible to obtain better prediction performance by avoiding traditional cnns by using a spatially agnostic version of convolution by spatial agnostic network we refer to a neural network in which no spatial information is introduced and past temporal information can be handled and introduced in the calculation of each new state in order to do so the input sequence scheme relies upon a c t s images as shown in fig 6 where the number of channels c represents the number of input spatio temporal variables similar to the usual input scheme presented in graph neural networks this methodology let us treat both spatial and temporal dimension simultaneously for our concrete case the input series will be pollution nwp and npp for all stations during the past 48 h the model will output pollution forecasting for all stations for the next 48 h nned is composed of three different modules encoder it is in charge of coding the input information of the space time series in a space of superior dimension h that is it increases the expressiveness of the input by relating all the variables to each other as we expect this model to work without spatial information the encoder needs some modifications in its convolution scheme the convolution itself operator has the usual form for 2d images given an input x 9 x k i j m k 1 n k 2 x m n k i m j n where k is the learnable kernel however the kernel size is regularly used with equivalent values for its two dimensions k 1 k 2 k in this case not only this kernel uses different values for each component but kernel size for spatial dimension must be equal to the number of spatial zones k 2 s as a result the convolution operation is made over all locations at once the kernel size in the temporal dimension is defined as t past and needs to be fixed as part of the network architecture the temporal dimension is dominated by a causal convolution generally causal convolution ensures that the state created at time t derives only from inputs from time t to t t past in other words it shifts the filter in the right temporal direction thus t past can be interpreted as how many lags are been considered when processing a specific timestep given that previous temporal states are taken into account for each step and that parameters are shared all over the convolution this methodology might be seen as some kind of memory mechanism by itself unlike memory based rnn like lstm and gru where the memory mechanism is learned via the hidden state in this case t past acts as a variable that lets us take some control over this property in order to ensure that each input timestep has a corresponding new state when convolving a padding of p t past 1 at the top of the image is required to guarantee temporal integrity this padding must be done only at the top by using convolution in this form once the kernel has moved over the entire input image t s the output image will be t 1 now if we repeat this operation h s times we will create a new hidden state with h channels and an output image with h t s dimensions thus we have coded input information relating all variables among them without exploiting prior spatial information based on adjacency decoder its function is to decode the information contained in the hidden space of high dimensionality to do this it learns how to merge the h hidden states present for each input and location timestep into a single value because this information is expected to be similar throughout the image a kernel of size k 1 k 2 1 is used thus it changes from an image h t s to again a t s multilayer perceptron finally a multilayer perceptron of input t s and output t s is used relating each element obtained by the processes of coding and decoding with each of the zones and times to be predicted the output of this multilayer perceptron is the output reported by the nned model finally the complete procedure for this model is described graphically in fig 7 4 3 pp fslr arfima qr model the pp fslr arfima qr model is actually a chain of models itself which has been developed specifically to address the anthropogenic factors that in this case are of the non cyclical calendar type it is true that there is an underlying weekly cycle but due to holidays and long weekends and the interaction with the annual cycle a long weekend in spring is not the same as in winter it presents strong distortions that have to be dealt with ad hoc thus this model uses the different initial data sources and knowledge learned from previous modules to exploit all this information in order to return a probabilistic prediction for the next 48 h in this case a different model is adjusted for each station 4 3 1 pp daily classification into pseudo periodic sub dates principally the pp pseudo periodic module is responsible for dividing the time sequence according to the type of day depending on its position at weekends and holidays they are called pseudo periodic because they do not form perfect cycles like the days of the week as the existence of holidays and long weekends disturbs their periodicity post after a long weekend usually monday ext both the day before and the day after are working days usually tuesday thursday prev weekend or holiday eve usually friday first first day of a long weekend or weekend saturday mostly int internal to a long weekend excluding the first and last day last last day of a holiday or weekend sunday as a rule for each one of these 6 possibilities a time series is generated and a chain of models described below is developed 4 3 2 fslr fixed sign linear regression once the type of day has been determined we start with a linear regression whose coefficients are forced to be non negative based on the work of lawson and hanson 1995 if a driver should have a negative effect it is introduced with a change of sign this bayesian approach is not very common but it is very appropriate in many occasions since we often do not have a very detailed quantitative information about the form of the distribution of the typical prior conjugate fink 1997 but we do have a very clear qualitative knowledge for example with respect to the sign that it should take which can be expressed as a uniform distribution in the semimark x 0 or x 0 the effects considered in this regression are instantaneous nned forecast the main driver is the forecast made with the neural network model explained in section 4 2 in the case of the ext type of day it is diversified according to the day of the week which can be tuesday wednesday or thursday as it has been observed that a certain differences exist in the rest of sub dating the case of days of the week does not allow for such diversification daily inertia medium term the average of the already known observations with 23 24 and 25 h of delay on the one hand and with 47 48 and 49 on the other by forcing the positive sign the inertia is maintained if it is significant and positive in other cases the nned algorithm itself is in charge of canceling it it works approximately as a kind of autoregressive seasonal model of period 24 in the natural time dating as opposed to the artificial time division subdate just described in the previous section concretely tree hours have been chosen to smooth these components considering that the daily periodicity is not completely precise in these series because of their anthropogenic component in addition choosing several hours avoids potential issues with the two seasonal time changes throughout the year daily correction medium term the average of the errors made by the model itself with 23 24 and 25 h of delay on the one hand and with 47 48 and 49 on the other which are also known in this case they will be used with the opposite sign that is if an error is made in one direction it is corrected in the other provided that such effect has been estimated as significant and otherwise the nned cancels it out it works approximately like a kind of moving average seasonal model of period 24 in natural time dating concretely tree hours have been chosen to smooth these components considering that the daily periodicity is not completely precise in these series because of their anthropogenic component in addition choosing several hours avoids potential issues with the two seasonal time changes throughout the year inertia and time correction short term for the first hours of the morning of each forecast session the observations and errors of the last hours are also available so it is possible to build inertia and short term correction inputs similar to the two previous ones from midday of the same forecast day they are no longer useful they work as a kind of regular arma in natural time dating protocol activation when the mobility restrictions imposed by the no2 protocol described in section 2 2 are activated the pollutant concentrations might be reduced with greater or lesser success so that the nned forecasts become obsolete and must be intervened in a deterministic way they are entered with a negative sign because it would not make sense for the action to increase contamination workday indicator within a long weekend pollution is particularly reduced on the public holidays themselves so a slight upward correction is needed for the rest of the days of the long weekend it only affects the type of day int school calendar during school vacations and adaptation periods with reduced schedules at the beginning and end of the school year there is a certain reduction in pollution that suggests a downward correction concretely this regression is estimated in logarithmic terms of both the observations and the nned forecasts and errors since it has been experimentally observed that the multiplicative relationship predominates over the additive 4 3 3 dynamic regression arfima on the errors of the previous regression a regular dynamic model is developed without a seasonal part that is concerned with maintaining inertia and correcting errors produced by the anthropogenic features definition arfima these type of models are considered as an extension of traditional arima models letting the differencing parameter to take non integer values by doing so arfima models are more appropriate for modeling time series with long memory granger and joyeux 1980 through this work the arfima function from the r package forecast is used hyndman and khandakar 2008 4 3 4 qr probability regression at this point the forecasts generated represent the mathematical expectation of the output magnitudes with this one can aspire at most to asymptotically estimate a log normal distribution under the laws of regression but since the distribution will not always fit perfectly with a log normal it is preferable to use a method based solely on the data to do this a new probabilistic quantile regression qr is estimated in order to estimate the future concentrations with as single input the forecast of the previous fslr arfima model in original terms without applying the logarithmic transformation quantile regression koenker 2005 is an extension of linear regression used when the mean is considered insufficient to characterise the response variable while the method of least squares estimates the conditional mean of the response variable qr allows for the estimation of the median q50 or in fact any other quantile thus allowing for the characterisation of the full distribution of the forecasts in our concrete case we obtain all percentiles from 1 to 99 in this setting since there is not always enough contrast surface the data variables ratio is low it may happen that the estimated percentiles do not comply with the basic rules of non negative and non decreasing applicable to every probability distribution usually it is in the extremes where there are more problems to alleviate this inconsistency an i spline interpolation is applied to these percentiles to ensure that these properties are as close as possible to the estimated values a general schematic of the pp fslr arfima qr model is presented in fig 8 while fig 9 summarizes the complete model with the data sources that govern the system 4 4 training procedure and operation details from a methodological point of view the training and parameters setting of the complete system has to be adapted to the essence of each block or model separately since socaire presents modules of very different nature in general terms the data used for the training along this paper correspond to july 2016 october 2020 unless otherwise specified the operational behavior of each of the models in relation to training and parameter estimation can be summarized as follows x arima model section 4 1 3 the x arima parameters are estimated through bayesian methods with intellectual property reserved these methodologies use all data available for parameter estimation without need of hyperparameters search this procedure is repeated each three months with all available data to that moment nned model section 4 2 in this case the training follows the usual pattern of neural networks the estimation of hyperparameters is performed by random search with data belonging to the interval january 2013 july 2016 as validation set after this process the network is trained with all the remaining available data using the adam algorithm for neural parameter optimization and once operative the network is updated weekly by means of new optimizations that take the most recently trained network as a starting point every three months a complete retraining of the network is allowed some other minor details are that the network is trained using the mean squared error mse as objective function batch size is 256 learning rate decay is set to 10 3 the initial learning rate is 0 001 and both early stopping and learning rate decay are implemented in order to avoid overfitting and improve performance pp fslr arfima qr model section 4 3 as in the case of x arima this model parameter s are estimated through bayesian methods with intellectual property reserved again these methodologies can use all data available through this process however in this case this procedure is repeated each day with all available data as the computation require little computational power 5 probabilistic prediction of the alert levels as described in section 2 2 the activation of the no2 protocol depends on meeting a number of requirements defined in three alert levels from a probabilistic point of view these requirements can be seen as compound events and being able to compute the future probability for the activation of each level is of utmost importance for decision makers according to the no2 protocol the activation of the different levels depends on what happens in several stations at the same time and in a certain number of consecutive hours in order to compute the aggregated probability the evaluation of the probability of the intersection of several events is thus needed knowing only the marginal percentiles and the historical residues left by each of the models 5 1 empirical marginal distribution of the different stations as we have shown above the model for each station offers a probabilistic forecast condensed in a quantile vector specifically the 99 integer percentiles are taken that is those corresponding to the probabilities p k 1 2 98 99 in this section we will look for a way to calculate the marginal distribution function for the forecast of each pollutant concentration from these quantiles calculated by each station s model for this it will be necessary to calculate the inverse of this distribution function and some statistics such as the mode which in turn requires an analytical representation that allows us to obtain its first and second derivatives in summary we need a pair of easily computable continuous and doubly derivable functions that allow us to evaluate very efficiently and precisely approximations of the distribution function and its inverse at any point of their respective domains the selected method is in fact an empirical change of variable that transforms the concentration into a standardized normal we will first take into account the fact that by definition the estimated quantiles are evaluations of the change in a variable that transforms the forecasts into a uniform distribution although this is valid for any source distribution for reasons of numerical stability it is preferable to apply the process to the logarithms of the quantiles thus if we apply the inverse of the standard normal distribution function to these log quantiles then the values obtained will follow that distribution by construction note that the calculation of the mode and deviation becomes trivial in this context during the approximation process we will establish the restriction that the probability density of the concentration forecast is always unimodal which agrees perfectly with the analyzed observations and the type of models used let us think of the moment in which decision making takes place t 0 and let us call y s t 0 the real concentration not yet observed in station s at future instant t t 0 the model of the s station will give us the q s t k percentiles of the forecast such that p y s t q s t k p k the transformed values are thus defined as z s t log y s t and the standardized normal quantile is u k φ 0 1 1 p k where obviously φ0 1 is the normal distribution function with mean 0 and deviation 1 now we will interpolate the pairs u k z t s by means of a function f s t r r that passes through those points 10 f s t u k z t s and in an analogous way the inverse function g s t f s t 1 r r will be constructed as the interpolating function that passes through the points z t s u k that is to say g s t z k u t s this allows us to construct an approximation of the concentration distribution function as follows 11 φ 0 1 g s t log y p y s t y ψ s t y and similarly we will obtain the approximation of its inverse 12 exp f s t φ 0 1 1 p ψ s t 1 p although we could have directly interpolated these functions which are the true objective numerically speaking the interpolation with these transformations is more stable largely because both z s t and u k are not bounded to avoid problems in the tails of the distribution and taking into account that both functions are monotonous it is highly recommended to use an interpolation method that guarantees this monotonicity in particular a monotonic spline interpolation has been used in this work the monotony of the functions f s t and g s t together with the monotony of the logarithm and the exponential functions guarantees that the maximum probable value of the concentration will be y s t exp f s t 0 let the standardized residue of the forecast be 13 ϵ s t g s t log y s t n 0 1 and note that indeed if the probable maximum forecast is exact i e if y s t y s t then 14 ϵ s t g s t log exp f s t 0 g s t f s t 0 0 similarly if the standardized residue is zero then the forecast is exact 5 2 empirical joint distribution section 4 has described the models that marginally predict the concentration of each pollutant at each station for different time horizons these models thanks to their arima structure are able to adequately treat the internal temporal correlation of each station that is the autocorrelation of each of the series of pollutants of the different stations in fig 10 it can be seen that the autocorrelation function acf is never too big and that when it does exceed the 2 sigma limits so does the partial autocorrelation function pacf this fact suggests that these are spurious correlations or any other types of concurrent causes not linked to time however in view of fig 11 there is nothing that indicates that residuals from different stations will be independent of each other rather they appear to correlate on the one hand even if nned models the spatio temporal dynamics of the process it is expected that closer stations will be more similar amongst them giving rise to positive correlations between their residues on the other hand as shown in fig 12 the errors in each forecast horizon for a single station will also not be independent of other stations previous horizons in fact this occurs mostly mutually present errors of a station correlate with the past errors of another station and vice versa in the previous section we have seen how to obtain by means of an interpolative variable change standardized normal residues in a marginal way for each station s and for each future instant t at current time t 0 however if the independence hypothesis is not plausible it is clear that knowing the marginal distributions does not imply knowing the joint distribution a family of models which are naturally capable of dealing with this situation are the x vecarma a type of multivariate models sims 1980 that include exogenous inputs cointegration and vector arma they are considered very powerful for the representation of cross correlated vector processes that might include exogenous factors eventually shared by several of them however they are intractable in computational terms for this setting thus we propose an empirical multi normal copula nelsen 1999 to approximate the joint distribution for every station and horizon the aim is to obtain an estimate of such joint distribution function for all the forecasts obtained marginally both in time and space using the joint sample correlation matrix between each pair of stations among all the horizons and stations however since there are 48 horizons and 24 stations that gives us a square matrix of 1152 rows and we would need at least 10 years of forecasts to obtain a meager 3 to 1 response surface which is clearly unacceptable for this reason we have developed a boxed tridiagonal scheme in which correlations are only taken into account one period ahead with this scheme only one year of forecasting is sufficient to obtain a reasonable estimate we will assume that the joint distributions of these standardized residues only depend on the station and the forecast horizons h t t 0 and h 1 t 1 t 0 but not on the specific moment t since the forecasts will be made every day at the same time since the marginal distributions of all the ϵ 1 t are normal unbiased and with unit variance the joint distribution of all stations 15 ϵ h ϵ 1 t 0 h ϵ s t 0 h ϵ s t 0 h t r s will be an unbiased multinormal with an unknown but obligatory unitary covariance matrix that is equal to the correlation matrix in the same way we will suppose that the residuals ϵ h 1 ϵ h corresponding to each pair of consecutive horizons are also distributed the same way by the principle of causality for the previous horizon ϵ h 1 an independent distribution of the following ϵ h 1 will be postulated since future events cannot influence the past in this way we can define the joint distribution of the different stations in each horizon in a recursive way 16 ϵ 1 n 0 c 1 ϵ h 1 ϵ h n 0 c h h 2 3 h c h c h 1 h 1 c h 1 h c h 1 h t c h h r 2 s 2 s h 2 3 h c 1 c 1 1 c h 1 h c h h r s s c h 1 h 1 s s c h h s s 1 c h h s s ρ ϵ s t 0 h ϵ s t 0 h 1 1 c h 1 h s s ρ ϵ s t 0 h 1 ϵ s t 0 h 1 1 note that the joint distribution of all horizons would have a tridiagonal covariance matrix with partitions of order s 17 c c 1 1 c 1 2 0 c 1 2 t c h 1 h 0 c h 1 h t c h h if we calculate the forecasts for enough dates t 0 of the past at the same time of the day and with the same horizons h 1 2 h we can obtain many samples of the residues with which we can thus estimate the matrices c h 1 h and c h h in this way we would obtain the distributions for each horizon conditioned on the previous horizon using the formula known analytically for the conditional partitioned multivariate normal 18 ϵ h n μ h c h μ h c h 1 h t c h 1 h 1 1 ϵ h 1 r s c h c h h c h 1 h t c h 1 h 1 1 c h 1 h r s s h 2 3 h these matrices can be stored for later use in future joint forecasts along with their cholesky and inverse decompositions 19 c h h l h l h t h 1 2 3 h c h l h l h t h 2 3 h first we simulate n vectors of n standardized independent residuals for the first horizon 20 η 1 n n 0 i n 1 2 n and pre multiplying them by l 1 we will have the standardized residuals of all the stations for the first horizon 21 ϵ 1 n l 1 η 1 n n 0 c 1 from there also starting from independent residuals 22 η h n n 0 i n 1 2 n residuals of each horizon conditioned by the previous one can be simulated 23 ϵ h μ h l h η h n on the one hand this approach solves the problem of time correlation in consecutive hours which is what is required and on the other hand it is simple enough to be able to generate correct estimations finally applying the transformations detailed section 5 2 we obtain n realizations of the future forecasts of the concentrations of the different stations in each horizon 24 y s t 0 h n exp f s t ϵ s h n if this simulation is repeated a sufficient number of times we can calculate any joint statistic from the forecasts of the concentrations in the different stations in particular for example to calculate the probability of activation of the pre warning level of the no2 protocol defined as the probability of the concentration of no2 exceeding a certain threshold υ 180 in at least two stations during two consecutive hours it will simply be necessary to calculate what proportion of the simulated samples meet these criteria 6 operation and performance 6 1 operation in order to be used by decision makers in the department of the city council in charge of air quality socaire has been integrated with a web app that allows to simply and directly view the forecasts for pollutants and the probability of reaching the levels established within the no2 protocol as explained in section 2 2 this section will show the site structure and its basic operation principles the main overview of the web tool is shown in fig 13 on the one hand at the top you can choose the pollutant to display blue buttons the date on which you want to make a query calendar button and different submenus where you can see in more detail the probability that the protocol will be activated shown tab and both the system predictions and a summary of contrast measures on the other hand in the central part the information related to the submenu in which the user is at that moment is shown in this specific case the probability of the levels of the no2 protocol being activated the operation of the tool for monitoring the future probability of reaching the different levels of the protocol are presented in fig 14 after using the ensemble of nested models described in section 4 to forecast no2 quantiles the outer rings show the probability of each individual station exceeding the levels set in the no2 protocol 180 μg m3 200 μg m3 and 400 μg m3 for prewarning warning and alert respectively once the individual probabilities are computed it is possible to use the process explained through section 5 to estimate probabilities of compound events given that the protocol is defined over areas and not for individual station levels the intermediate ring shows the probability of exceeding the expected pollution levels for each of the 5 areas in which madrid is partitioned in the no2 protocol see fig 1b lastly the inner ring contains the aggregated probability of the different levels of the protocol being activated in the entire city it uses the probabilities over the five areas to estimate this final probability since the set of mobility measures defined in the no2 protocol depends on reaching extreme levels in various stations and for a pre set number of consecutive hours having such an overview is especially important however it is also interesting to visualize the individual forecast for each station over time the socaire website allows viewing the actual forecasts for each pollutant and each station as shown in fig 15 together with the predicted quantiles and real observed values these plots also show the probability of exceeding each level and the levels themselves 6 2 performance analysis usual error metrics as rmse refer to expected values which are found in the central part of the distribution but do not take into account any other information and are thus particularly unfit to evaluate probabilistic forecasts since the most usual models produce point forecasts and not the entire distribution these kinds of metrics are the only option however when dealing with the prediction of the complete distribution as in our case other metrics have been proposed in order to summarise model performance information in a more comprehensive and realistic way for example crps is a measure of the squared difference between the forecast cumulative distribution function cdf and the empirical cdf of the observation gneiting and katzfuss 2014 as we will show in terms of performance socaire compares favorably to benchmarks in order to get a clear and quick idea about the behavior of the model table 2 shows the rmse and bias averaged both in time and space of the proposed methodology and compares it with four other models that due to their characteristics make it easier to understand the real performance of socaire persistence linear regression nned output without any linear correction and the npp provided by cams the persistence model is a naive model in which the forecast value is taken to be the observed value at the previous timestep it is thus a good benchmark model and one can get a rough idea of how good a new model is by seeing how much improvement there is with respect to persistence in our specific case for contractual reasons we use a more elaborated version of persistence which includes the daily weekly and annual cyclical structure of the series and is thus a simple although powerful model linear regression is a well known methodology for all kind of regression problems characterized by its simplicity but performing reasonably well in a multitude of scenarios in its most basic version it is limited by its only linear response so it is a good candidate to be beaten as a sample of having a model of minimum guarantees as was the case with persistence in our particular case we use a multioutput scheme for each station we generate a model that will have as input the past timesteps and will return jointly all predicted timesteps regarding the nned model its inclusion has a dual purpose on the one hand to have a clear and direct comparison with a neural architecture on the other hand to be able to clearly and precisely visualize the improvement that the complete modeling explained in section 4 implies in terms of performance and the potential benefit that can be obtained from using both types of strategies similarly the npp provided by cams represent another good baseline to be improved upon by any new model since it is based on a synoptic scale it is expected that any model focused on a smaller and concrete terrain extension will improve its results if this is not the case it would make more sense to use cams npp as an approximation instead of the proposed new methodology for a more detailed view of error metrics refer to fig 16 as it can be seen socaire consistently outperforms all baselines in terms of rmse and bias for the four pollutants concretely socaire supposes an average rmse improving of 37 with respect to cams a 27 with respect to lr a 10 with respect to nned and 44 with respect to persistence reinforcing the idea that socaire shows good performance and behaves very well as a predictor also socaire demonstrates to be in general terms an unbiased predictor of pollution which emphasizes the fact that the proposed model is being able to correctly describe the aforementioned terms related to the system another issue that is of special importance in our problem is the behavior of the model depending on the prediction timestep hour as it was shown in fig 3 the series are highly hour dependent for example no2 presents peaks usually around 08 00 10 00 and 22 00 00 00 in the framework of air quality management and monitoring these peaks are extremely important as they represent the higher risk and consequently the moments when the maximum recommended and or permitted levels are usually exceeded thus and given that one of the main objectives of socaire framework is forecasting the probability of each level of the no2 protocol showing a good performance in peak hours is of crucial importance fig 17 presents the rmse error for each pollutant and for each prediction horizon averaged over all stations from this figure it becomes clear that socaire is especially efficient in peak hours where the gap with baseline models is even wider until now we have covered aggregated error over all stations as the activation of the no2 protocol depends on compound events of individual stations it is important to make sure all of them behave similarly as it was explained before the complete model has a module which is able to relate and exploit shared spatial information section 4 2 but it also models each station independently based on its own characteristics section 4 3 by taking into account both types of information we expect to avoid possible biases of predominance by some spatial areas over others but still be able to make use of the relations that exist among them the crps for the no2 predictions at each station is shown in the top row of fig 18 it is worth noting that stations with lower crps errors correspond to green areas of the city of madrid stations 24 49 and 58 scaling these crps values to a n 0 1 bottom row of fig 18 let us see how all error distributions have a very similar behavior hence it is possible to assure that our modeling strategy works as expected and results in an approximately unbiased prediction of the spatial component the evaluation of these models has been done using the data from january 2020 to october 2020 with the system already operational and therefore functioning as described in section 4 4 thus the estimated errors represent realistically the errors the model is recording in its daily operation the aggregated error metrics from all predicted timesteps over that period generate the error distributions analyzed in this section 7 conclusions and future work throughout this manuscript we have discussed the details of socaire the new operational system for air quality forecasting and monitoring in the city of madrid based on an ensemble of statistical and neural models socaire is built under the premise that it is possible to integrate the diverse information that correlates with air quality in order to model it this information includes historical values of the series itself numerical weather and pollution predictions and anthropogenic features concretely the proposed methodology tackles the prediction of the four main pollutants no2 o3 pm10 and pm2 5 for a 48 h horizon thanks to its probabilistic nature the system is able to combine the predictions of the full probability distribution for compound events using a bayesian estimation of the future distribution of the different stations over time thus the system outputs are a valuable tool for managing the no2 protocol enforced by the city council of madrid the tool presented in this paper is not only a theoretical proposal but it has been adopted as the official application to monitor analyze and make day to day decisions about air quality the last part of this work summarizes the structure and operation of socaire s web as well as the main highlights of the good results and performance of the system in the future it would be interesting to apply a cost effectiveness analysis focused on the no2 protocol activation probability also we are working towards the inclusion of a traffic forecasting system which might improve the performance of the models by enhancing the information that anthropogenic features provide finally socaire could be adapted to predict any kind of combined air quality index and not only those ones affecting the current protocol declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank josé amador fernández viejo and the team at the general directorate for sustainability of the municipality of madrid especially maría de los ángeles cristóbal lópez for her continuing support and enthousiasm towards this project this research has been partially funded by empresa municipal de transportes emt of madrid spain under the program cátedra emt uned de calidad del aire y movilidad sostenible a x arima vs arima x in order to illustrate the differences between both kinds of models let us introduce the simplest case ar 1 specifically δ b 1 φ b 1 φ 1 b and θ b 1 in this circumstances arima x would be reduced to a linear regression 25 y t α x t φ 1 y t 1 ε t whereas the x arima model would be bilinear which is much more complicated to estimate than a linear regression 26 y t α x t φ 1 y t 1 α x t 1 ε t note that when the absolute value of φ 1 is very small there will be almost no difference between the two models but otherwise they will be very different let us imagine for simplicity that there is a single exogenous driver consisting of a pulse the blue line in fig 19 i e its value is 1 at a given instant of time and 0 the rest of the time we have arbitrarily set the parameters σ 0 1 α 2 and φ 1 0 93 and simulated two processes each following one of the models in red with a thicker line the x arima and in orange the arima x both generated from the same series of residuals green line logically both series coincide perfectly until the pulse occurs but while in the first one the effect of the pulse vanishes instantaneously in the second one it lasts quite a long time because the ar root is very close to unity if we had set φ 1 0 70 the effect would have lasted not 12 h but almost two days fig 19 comparison between x arima and arima x models fig 19 while usually instantaneous transfers are much more common than damped transfers like the one we have shown even when they occur they do not usually present exactly the same shape and damping rate as the series noise itself although possible the probabilities of all transfer functions of all the inputs being coincident with each other and with the arima model are scarce strictly speaking using the appropriate transfer functions the two model classes are equivalent but x arima fits in a more natural way and without using complicated constraints 
25778,air quality has become a central issue in public health and urban planning management due to the proven adverse effects of airborne pollutants considering temporary mobility restriction measures used to face low air quality episodes the capability of foreseeing pollutant concentrations is crucial we thus present socaire spanish acronim for operational forecast system for air quality an operational tool based on a bayesian and spatiotemporal ensemble of neural and statistical nested models socaire integrates endogenous and exogenous information in order to predict and monitor future distributions of the concentration for the main pollutants it focuses on modeling available components which affect air quality past concentrations of pollutants human activity and numerical pollution and weather predictions this tool is currently in operation in madrid producing daily air quality predictions for the next 48 h and anticipating the probability of the activation of the measures included in the city s official air quality no2 protocols through probabilistic inferences about compound events graphical abstract image 1 keywords air quality spatio temporal series statistical modeling neural networks 1 introduction during the last decades an increasing number of studies point out that degraded air quality is a major problem in cities around the world martuzzi et al 2006 héroux et al 2015 while there is general consensus that it causes health problems kim et al 2015a özkaynak et al 2009 how dangerous it can be is still a matter of debate sellier et al 2014 even in the best case scenario this seemingly endemic issue affecting the life in big cities is already considered one of the main causes of both direct and indirect mortality badyda et al 2017 of all the tools and systems that help fight pollution the prediction of future pollutant concentrations or levels is of principal importance for air quality management and control bai et al 2018 air quality forecasting systems allow for sending out warnings of upcoming high pollution episodes to the population in the short term so that appropriate measures can be taken to minimize as far as possible the damage caused by these episodes as an example the city of madrid in order to comply with european regulations european union 2008 devised an air quality protocol which includes restrictions to the use of polluting vehicles when the concentrations of no2 reach certain thresholds consequently foreseeing the activation of such restrictions is critical both for the decision makers which need to announce them in advance and for the vehicle owners which need to plan their transport alternatives the use of data driven approaches to predict and control air quality is not new following the discussion started in breiman 2001 when approaching a modeling problem two families of methodologies or cultures in breiman terms coexist the data modeling culture based on the search for a stochastic data model for example time difference equations in the case of air quality that capture the inner behavior of the intervening physical processes and the algorithmic modeling culture based on the use of algorithms to directly learn the model from data given that pollutant concentrations can be seen as time series the stochastic data modeling usually deals with using arma based methods kumar and jain 2010 hassanzadeh et al 2009 however this kind of models have trouble handling high non linearities and high dimensional environments to solve this problem and thanks to the big amount of data that is gathered nowadays machine learning models have been applied to environmental modeling with some success grivas and chaloulakou 2006 navares and aznarte 2020 in this work however we advocate for a hybrid culture in which stochastic data models are combined with algorithmic ones in a way which permits harvesting the benefits of both approaches while reducing their disadvantages as we will show due to the increase in the available computing power and the advances in the field of neural network based models it is nowadays common to find real applications in which algorithmic modeling is put into practice to predict air quality for example in nebenzal and fishbain 2018 a system is deployed in which pollution levels based on a threshold are used to study transitions among states which makes possible to estimate high pollution episodes in the long term in thatcher and hurley 2010 authors have combined the tapm and ccam atmospheric models to form a customizable local scale meteorological and air pollution forecasting system showing that using macroscopic models in the local scale can provide positive points in the prediction macia g et al 2019 is an example of an ensemble model with a neural network and an arima in a similar vein to what will be our proposal applied successfully to a real urban environment in the city of london for this kind of real applications it is useful to produce instead of a forecast of the expected value of the magnitude under study an estimation of the full future distribution which in turn allows for decision making based on the probability of the surpassing of certain thresholds this idea which will ultimately be the main goal of the system described below is common in other fields and was introduced to air pollution forecasting by aznarte 2017 the integration of meteorological information and human activities have been addressed by multiple studies some relevant variables as temperature precipitation or wind speed have shown to be good indicators of pollution levels kalisa et al 2018 ouyang et al 2015 kim et al 2015b also the physical chemical mechanisms governing the relationship between these and air quality has been studied in vega garcía and aznarte 2020 interpretability techniques for deep learning are used to gain insight into feature importance in a highly similar environment and methodology to ours concluding that weather variables in general have a high impact when using machine learning methods for predicting pollution however while these issues have been widely studied from the univariate time series perspective the observed spatial interactions between nearby observation stations might be of importance too as air quality at different stations might be implicitly related spatial based approaches usually imply assuming or learning these interdependencies based for example on closeness but as it has been shown de medrano and aznarte 2020 this is not necessarily the most natural and optimal way to go in this paper we introduce socaire spanish acronim for operational forecast system for air quality the new official air quality monitoring system for the city of madrid this tool in operational use nowadays makes use of both external and internal variables related to air quality in order to forecast pollutant concentrations it is a complex modular mathematical system composed of an ensemble of data manipulation techniques and models that let us exploit different knowledge in each module from data cleaning and imputation through handling spatial and meteorological non linear features to integrating human behavior and its patterns by correctly treating all this information it is possible to avoid redundancies and to achieve very high performance as one of the biggest and most populated cities in europe madrid is a perfect setting for developing and testing these kind of systems the rest of the paper is organized as follows in section 2 the problem is stated and madrid s air quality protocol for no2 is described while section 3 presents an analysis and explanation of the different data sources and the data wrangling process section 4 presents the proposed approach for air quality forecasting then in section 5 we introduce the bayesian probabilistic framework that let us accommodate socaire to the no2 protocol section 6 shows the evaluation of the proposed architecture after appropriate experimentation and its comparison with other methodologies finally in section 7 we point out future research directions and conclusions 2 problem statement 2 1 study area and general information through this work we look for a system which is able to predict up to 48 h of four of the main existing pollutants nitrogen dioxide no2 ozone o3 and particulate matter pm10 and pm2 5 where 10 and 2 5 denote the maximum diameters in micrometers of the particles this estimation needs to be done in the 24 stations that compose the pollution measurement network each one with different pollutants since one of the main objectives of the system is to anticipate the activation of mobility restrictions in face of high pollution episodes we forecast the main quantiles of the distribution so it is easier to make decisions based on pollution level probabilities thanks to its bayesian estimation of compound events socaire becomes an ideal tool to foresee the scenarios of madrid s no2 protocol which will be explained later in this section socaire operates daily on a 48 h basis it produces forecasts from 10 00 of the present day to 09 00 two days later in the spatial dimension the measurement stations of the city council are used as reference points specifically there are 24 stations distributed throughout the city with sensors capable of recording different pollutants fig 1 a shows graphically the location of all the stations at the same time the city considers 5 different areas in the city that are related to the activation of the no2 protocol these areas are shown in fig 1b table 1 shows the correspondence between the different stations and their code their location and the pollutants measured at each one 2 2 the no2 protocol of the city of madrid in 2018 the city council of madrid approved an action protocol for no2 pollution episodes madrid protocol 2018 from this point referred to as the no2 protocol which defines a set of increasing alert levels thus classifying the situations of high concentrations of no2 as follows 1 prewarning when any two stations in the same area simultaneously exceed 180 μgm 3 for two consecutive hours or any three stations in the surveillance network simultaneously exceed the same level for three consecutive hours 2 warning when any two stations in the same area exceed 200 μgm 3 during two consecutive hours or any three stations in the surveillance network exceed the same level simultaneously during three consecutive hours 3 alert when in any three stations of the same zone or two if it is zone 4 is exceeded simultaneously 400 μgm 3 during three consecutive hours depending on the level and the meteorological prospect a set of increasingly restrictive mobility limitations will be imposed city wide by the council with the aim of mitigating and reducing the negative effects of contamination on the health and integrity of the population thus the main objective is to know when and how the conditions leading to the different alert levels will be met in order to enable the anticipation of the measures 2 3 framework overview fig 2 presents a summary of socaire s mathematical structure created to forecast and monitor pollution levels its operation is based on the compilation of several data sources which will be described in section 3 after a proper analysis and cleaning process the complete database will be used through an ensemble model composed of a cascade of nested models each one in charge of modeling different processes that alter air quality dynamics section 4 finally and thanks to the probabilistic nature of the predictions the system is able to estimate probabilities from compound events using a bayesian approach explained in section 5 that is adapted to the aforementioned no2 protocol 3 data analysis and wrangling as stated above in order to aim for the highest performance socaire makes use of all the available information related to the problem thus before introducing the actual modeling it is important to present and analyze the set of available data sources concretely as anticipated socaire uses the data of the concentrations of the different pollutants in the different stations in madrid as dependent variables output and as independent variables inputs past pollutant concentrations numerical pollution predictions coming from the european cams model numerical weather predictions served by aemet 2021 and anthropogenic information encoding different events such as holidays and school calendar the data used along this paper corresponds to the period july 2016 october 2020 both included the following subsections will detail the origin peculiarities and processing of these data 3 1 pollutants the temporal behavior of each pollutant series is shown in fig 3 the daily cycle of all pollutants is dominated in one way or another by the peak hours of road traffic except for ozone the other three pollutants to be analyzed have their daily peaks after peak traffic hours the no2 has the most intense traffic sensitive cycle followed by the 10 and 2 5 microparticles which show a delay of about an hour with respect to the no2 o3 presents a daily cycle that is practically inverted with respect to the rest everything said for the daily cycle applies to the weekly cycle with weekend being days with lower levels of traffic it can be assumed that holidays and long weekends will behave as public holidays so the forecast model would have to take this into account as expected the daily cycle is not independent of the weekly one but each day of the week has its own cycle especially different on weekends from working days in the annual cycles a greater variety of behaviors can be observed all pollutants especially ozone rebound in summer except no2 which has the opposite behavior in this case respect to the spatial dimension fig 4 represents the empirical distributions for each pollutant it can be seen that all stations report a similar behavior without clear relation patterns between closeness and distribution this fact will be of interest later when taking into account these spatial relationships in the modeling process as the distributions show a clear asymmetry logarithmic transformations are used pollutant data is publicly available at the open data portal of madrid madrid council 3 2 numerical weather predictions nwp as mentioned in section 1 meteorology has shown to be especially important for air quality hence having weather forecasts for the period in which the air quality forecasting is being made is expected to positively impact the precision of the forecasts in this work we use nwp from the integrated forecasting system ifs of the ecmfw blanchonnet 2015 for the following set of variables boundary layer height in meters this parameter is the depth of air next to the earth s surface which is most affected by the resistance to the transfer of momentum heat or moisture across the surface the boundary layer height can be as low as a few tens of meters such as in cooling air at night or as high as several kilometers over the desert in the middle of a hot sunny day when the boundary layer height is low higher concentrations of pollutants emitted from the earth s surface are found surface pressure in pa this parameter is the pressure force per unit area of the atmosphere on the surface of land sea and in land water it is a measure of the weight of all the air in a column vertically above the area of the earth s surface represented at a fixed point air pollution is especially prominent where high pressure dominates subsiding motions within an anticyclone suppress air trying to rise off the surface adiabatic warming of subsiding air creates a subsidence inversion which acts as a cap to upwardly moving air pollution problems dissipate when a low pressure system replaces a retreating anticyclone temperature in k this parameter is the temperature of air at 2 m above the surface of land sea or in land waters generally higher temperatures and hotwaves are directly related to episodes of higher pollution levels precipitation in mm this parameter is the accumulated liquid and frozen water including rain and snow that falls to the earth s surface it is the sum of large scale precipitation that precipitation which is generated by large scale weather patterns such as troughs and cold fronts and convective precipitation generated by convection which occurs when air at lower levels in the atmosphere is warmer and less dense than the air above so it rises precipitation parameters do not include fog dew or the precipitation that evaporates in the atmosphere before it lands at the surface of the earth air pollution is typically negatively correlated to the quantity of rainfall existing a so called washing effect of precipitation u wind component in ms 1 this parameter is the eastward component of the 10 m wind it is the horizontal speed of air moving towards the east at a height of 10 m above the surface of the earth pollutants tend to concentrate in calm conditions when wind speeds are not more than about 3 ms 1 speeds of 4 ms 1 or more favour dispersal of pollutants which literally clears the air v wind component in ms 1 this parameter is the northward component of the 10 m wind it is the vertical speed of air moving towards the north at a height of 10 m above the surface of the earth again wind is highly related to pollution dissemination nwp are interpolated to the location of each station of the air quality monitoring network as pointed out previously these forecasts are provided by aemet in an hourly basis the spatial resolution of these forecasts is 0 05 0 05 in a regular grid while the temporal resolution is hourly with up to 56 horizons 3 3 numerical pollution predictions npp cams c provides a four day horizon hourly pollution forecast which covers all europe on a synoptic scale the model takes into account global and regional numerical weather predictions from the ecmwf marécal et al 2015 as well as other types of forecasts about the production of certain chemicals of natural and human origin from models such as c ifs forecasts or cams 81 all these models always refer to a geodesic grid of between 10 and 20 km on each side so it is not very sensible to use them to directly forecast the concentrations with the resolution required inside a city which might well be below 1 km 3 4 anthropogenic features as we saw in fig 3 depending on the human activity the temporal patterns of the series are different similar to weekends and months public holidays and other designated days as well as the school calendar have a significant influence on road traffic giving rise to a very different daily cycle in special dates we usually find a lower intensity in the center but a punctual growth in other places particularly on the main access roads to the city related to holiday departures and returns also each type of calendar effect has different effects on each hour of the day in addition some of them can fall on saturday or even on sunday in the case of christmas eve and new year s eve and it is clear that the effect cannot be the same as when it falls during the week so all these issues must be taken into consideration in our particular case we will take into account the following aspects public holidays public holidays long weekends and special days such as christmas eve and new year s eve are characterized by significantly less road traffic than a normal working day apart from other departure and return operations that may occur on some of these days and which will be taken into account later it has been observed that public holidays have different effects both in terms of level and intraday evolution depending on their location within the year probably due to climate reasons hours of light and living patterns holiday departures and returns extraordinary periods such as bank holidays long weekends or even weekends cause a temporary exodus of citizens with large accumulations of vehicles in the so called departure and return operations departure operations can take place during the evening of the eve of the first non working day or during the morning of that day while return operations occur mostly during the evening of the last holiday sometimes reaching the early morning of the next working day as with other variables the effect varies with the hours within a relatively soft form school calendar in spain school calendar and schedule is highly related to usual hourly weekly and monthly patterns and so it can model with high precision the daily living the school day can be complete or normal average pre and post holidays or non existent either in isolation or for summer winter nor spring holidays each type of day other than the normal one is introduced as an effect with a different intraday cycle between 07 00 and 08 00 by combining all these variables we ensure that the information relating to human mobility in the city is covered both for normal situations and for special events these exogenous variables are defined for each station as not all parts of the city have the same dynamics 3 5 data wrangling when working with such diverse data sources is usual to deal with very heterogeneous formats and criteria which implies that pre processing and cleaning steps are of utmost importance some of the most important ones for this project are listed in this section firstly some sources use utc time and others use madrid s local time in addition the processes that transfer data between different programming environments r tol and python also have to take into account that each of these systems work differently with respect to winter and summer daily savings time changes secondly both nwp and npp distribute their forecasts in a different geodesic grid which in turn does not coincide with the coordinates of the pollution monitoring stations at first an attempt of interpolation was made by using the three closest grid points to each station as drivers but it soon became apparent that this was an excessive complication with very little added value as the forecasts were highly correlated therefore in the final version only the nearest reticular point to each station is used thirdly weather predictions are not always in the most appropriate metric so it is necessary to create derived variables that serve better as drivers of the models to begin with there are variables that change scale throughout history and it is necessary to unify the criterion for obtaining uniform series in time then there are other variables that are interesting to modify conceptually for example instead of the east west and north south coordinates of wind speed it is much better to use scalar speed which is the fundamental factor of diffusion and direction which is less important finally it is known that meteorological factors not only have an instantaneous effect but also a delayed effect that can be exercised up to a few hours later for this reason some variables delayed up to 4 h have been created and integrated with the rest of features finally since we are dealing with a cascade like ensemble of models in which the output of one is the input of another which may require a substantially different structure each level of modeling requires a series of steps to prepare the data to be as expected in the next phase let us note that the most laborious part of the data pre processing has been the imputation of missing values however given the importance of this part it has been decided to include imputation of data as part of the modeling strategy and is explained later in section 4 1 4 modeling strategy the concentration of a given pollutant in the air depends on at least two conceptually distinct groups of factors emission factors generally these are of a social order such as road traffic or heating which are predictable to some extent although there are also totally unpredictable events such as fires and others that could be anticipated to some extent such as strikes or sporting events with a multitudinous following dispersion factors basically these are consequences of the weather conditions on which there are quite precise forecasts on the horizon of 2 or 3 days ahead note that a certain factor such as rain can work in both directions at the same time on the one hand it can cause an increase in traffic on a normal working day which increases pollution but on the other hand it disperses especially the particles as they are carried to the ground which decreases pollution it is even possible that the effect is different depending on the day and time following the example of the rain that normally increases the traffic in a working day it can on the contrary contract the traffic in an exit operation when it will discourage people to leave the city this causal complexity added to the high degree of interaction between factors makes the phenomenon highly unstable and therefore very difficult to predict using any individual methodology for this reason an ensemble model composed of a cascade of nested models has been designed such that the output of each is used in the next to get the most out of each imputation techniques although this task is usually framed as part of the data wrangling process in this project it involves the development of models of some complexity due to the fact that the omitted elements are presented with a certain frequency and not always in a sporadic way but covering periods of time that can even be of several weeks these techniques are detailed in section 4 1 nned model a special flavor of convolutional neural networks called neural net encoder decoder which using as inputs the outputs of the imputation models allows to jointly forecast the concentrations of a pollutant in all the stations at the same time it takes into account the nwp and npp as well as the recent past of all stations for each input variable including the previous pollution itself and is capable of automatically detecting non linearities and interactions between different features however it does not allow for the natural treatment of irregularities in non cyclical anthropogenic factors related with traffic it is described in detail in section 4 2 pp fslr arfima qr model this is a chain of models by itself developed specifically to deal with anthropogenic factors in a bayesian way it will be explained in detail in section 4 3 4 1 imputation techniques in the different data sources it is relatively frequent to find missing data that can cause problems in the modeling process for this reason it is necessary to devise a sensible way to fill in these missing values replacing them with approximate or expected values by a series of auxiliary models when there are only very sporadic omissions of short duration it might be sufficient to apply some kind of approximation by interpolation but there might be up to consecutive weeks of data omitted in several or all variables from one or more sources at the same time thus in order to develop a robust operational system able to function even in the presence of missing data more complex and specialized techniques are required 4 1 1 trigonometric interpolation first a trigonometric interpolation is used as a univariate method to generate sensible values for those series with clear cyclical components such as temperature in our case these series present very few omissions so we consider this technique to be sufficient since the data are arranged in a regular grid this can be done by the discrete fourier transform 4 1 2 multiple imputation using additive regression bootstrapping and predictive mean matching hmisc multiple imputation using additive regression bootstrapping and predictive mean matching consists of drawing a sample with replacement from the real series where the target variable is observed i e not missing fitting a flexible additive model to predict this target variable while finding the optimum transformation of it using this fitted model to estimate the target variable in all of the original series and finally imputing missing values of the target with the observed value whose predicted transformed value is closest to the predicted transformed value of the missing value this methodology is implemented in the r package hmisc jr 2020 as the meteorological variables have already been imputed with the previous method which will be used as input here it is only applied to the npp and the pollutant concentrations themselves this method is actually used for safety in case the next one x arima fails as several parts of the framework can not handle missing data this step is required in order to assure proper functioning 4 1 3 x arima once the previous two standard imputation methods are applied it is turn for a univariate dynamic causal imputation method it analyses how both the present and the past of a group of variables including the target variable itself act on the future of this target variable these models are quite complex and to improve the imputation they are applied in two successive phases in the first one the npp are imputed as a function of the nwp in the second one the pollution observations are imputed as a function of the nwp and the npp mathematically speaking we have that being y t the time series of concentration of the pollutant in question and x t k the linearized inputs from the explanatory terms described above the general formula of the box jenkins x arima models box et al 1976 used is as follows where as usual b is the backward operator 1 δ b φ b y t k k 1 x t k α k θ b ε t the summation k 1 k x t k α k will be called the filter of exogenous effects while the equations in differences expressed by the delay polynomials will be called endogenous factors or the arma part of the model the difference between the output and the linear filter is called arima noise z t 2 z t y t k k 1 x t k α k the previously defined backward operator delays the time indicator of some element mathematically speaking 3 b k z t z t k in order to illustrate the backward notation we may show its behavior for some simple cases let us suppose that the process under study presents a regular difference δ b 1 b i e the difference between each pair of consecutive data is stationary 4 δ b z t 1 b z t z t z t 1 regarding to autoregressive polynomials ar φ b and moving average ma θ b they behave similarly suppose we have an ar of first grade φ b 1 φ 1 b and a ma of second grade θ b 1 θ 1 b θ 2 b thus equation 1 writes as follows 5 1 b 1 φ 1 b z t 1 θ 1 b θ 2 b ε t 6 1 1 φ 1 b φ 1 b 2 z t ε t θ 1 ε t 1 θ 2 ε t 2 7 z t 1 φ 1 z t 1 φ 1 z t 2 ε t θ 1 ε t 1 θ 2 ε t 2 note that this model is very different from the typical arima model with exogenous effects of the arima x class 8 δ b φ b y t k k 1 x t k α k θ b ε t which is easier to estimate but also is considered to be much less effective in explaining the phenomena that actually occur in real life see appendix a exogenous factors the nwp series has only very few isolated omitted data and in principle there is no reason to think that they will occur more frequently in the future for this reason it is more than sufficient to use an imputation system based on the fourier transform the imputation of the npp series will take as inputs the previously imputed nwp that shows quantitative relevance when imputing npp values specifically the boundary layer height blh wind speed ws and precipitation tp have been used applying different box jenkins time transfer functions box et al 1976 with different damping parameters in order to collect in a more synthetic way the time delayed transfers already discussed for the series of pollution observations both nwp and npp will be used after all of them have been already imputed endogenous factors the arima polynomials in this case are multi seasonal among the inertial factors of the stochastic process and besides the regular time hourly both the daily cycle of periodicity 24 h and the weekly cycle of periodicity 24 7 168 h are taken into account obviously there is also a pseudo annual cycle and a trend but they will be filtered by some of the explanatory drivers or exogenous factors indicated in the previous section on the one hand the annual cycle is not in harmony with the weekly or daily cycle that is its periodicity is not a whole number and on the other hand it is enormous 365 2425 24 8765 82 so it is practically intractable for the arima approach in an hourly series even in a daily series it presents serious difficulties and consumes a lot of resources a complete overview of the imputation process is shown in fig 5 4 2 neural network encoder decoder nned model given that interactions between pollution itself and other relevant features as nwp show a complex and highly non linear behavior in both time and space deep learning arises as a suitable mathematical solution no anthropogenic interactions are modeled at this point a step forward with respect to the usual deep learning architectures nned model is based on the idea of spatial agnosticism for solving spatio temporal regression problems presented in de medrano and aznarte 2020 it has been shown that when the spatial granularity of the series is low and its spatial autocorrelation is close to 0 traditional convolutional neural networks cnn fail to extract all the information from the series as the adjacency assumption for learning shared weights does not entirely hold that way it is possible to obtain better prediction performance by avoiding traditional cnns by using a spatially agnostic version of convolution by spatial agnostic network we refer to a neural network in which no spatial information is introduced and past temporal information can be handled and introduced in the calculation of each new state in order to do so the input sequence scheme relies upon a c t s images as shown in fig 6 where the number of channels c represents the number of input spatio temporal variables similar to the usual input scheme presented in graph neural networks this methodology let us treat both spatial and temporal dimension simultaneously for our concrete case the input series will be pollution nwp and npp for all stations during the past 48 h the model will output pollution forecasting for all stations for the next 48 h nned is composed of three different modules encoder it is in charge of coding the input information of the space time series in a space of superior dimension h that is it increases the expressiveness of the input by relating all the variables to each other as we expect this model to work without spatial information the encoder needs some modifications in its convolution scheme the convolution itself operator has the usual form for 2d images given an input x 9 x k i j m k 1 n k 2 x m n k i m j n where k is the learnable kernel however the kernel size is regularly used with equivalent values for its two dimensions k 1 k 2 k in this case not only this kernel uses different values for each component but kernel size for spatial dimension must be equal to the number of spatial zones k 2 s as a result the convolution operation is made over all locations at once the kernel size in the temporal dimension is defined as t past and needs to be fixed as part of the network architecture the temporal dimension is dominated by a causal convolution generally causal convolution ensures that the state created at time t derives only from inputs from time t to t t past in other words it shifts the filter in the right temporal direction thus t past can be interpreted as how many lags are been considered when processing a specific timestep given that previous temporal states are taken into account for each step and that parameters are shared all over the convolution this methodology might be seen as some kind of memory mechanism by itself unlike memory based rnn like lstm and gru where the memory mechanism is learned via the hidden state in this case t past acts as a variable that lets us take some control over this property in order to ensure that each input timestep has a corresponding new state when convolving a padding of p t past 1 at the top of the image is required to guarantee temporal integrity this padding must be done only at the top by using convolution in this form once the kernel has moved over the entire input image t s the output image will be t 1 now if we repeat this operation h s times we will create a new hidden state with h channels and an output image with h t s dimensions thus we have coded input information relating all variables among them without exploiting prior spatial information based on adjacency decoder its function is to decode the information contained in the hidden space of high dimensionality to do this it learns how to merge the h hidden states present for each input and location timestep into a single value because this information is expected to be similar throughout the image a kernel of size k 1 k 2 1 is used thus it changes from an image h t s to again a t s multilayer perceptron finally a multilayer perceptron of input t s and output t s is used relating each element obtained by the processes of coding and decoding with each of the zones and times to be predicted the output of this multilayer perceptron is the output reported by the nned model finally the complete procedure for this model is described graphically in fig 7 4 3 pp fslr arfima qr model the pp fslr arfima qr model is actually a chain of models itself which has been developed specifically to address the anthropogenic factors that in this case are of the non cyclical calendar type it is true that there is an underlying weekly cycle but due to holidays and long weekends and the interaction with the annual cycle a long weekend in spring is not the same as in winter it presents strong distortions that have to be dealt with ad hoc thus this model uses the different initial data sources and knowledge learned from previous modules to exploit all this information in order to return a probabilistic prediction for the next 48 h in this case a different model is adjusted for each station 4 3 1 pp daily classification into pseudo periodic sub dates principally the pp pseudo periodic module is responsible for dividing the time sequence according to the type of day depending on its position at weekends and holidays they are called pseudo periodic because they do not form perfect cycles like the days of the week as the existence of holidays and long weekends disturbs their periodicity post after a long weekend usually monday ext both the day before and the day after are working days usually tuesday thursday prev weekend or holiday eve usually friday first first day of a long weekend or weekend saturday mostly int internal to a long weekend excluding the first and last day last last day of a holiday or weekend sunday as a rule for each one of these 6 possibilities a time series is generated and a chain of models described below is developed 4 3 2 fslr fixed sign linear regression once the type of day has been determined we start with a linear regression whose coefficients are forced to be non negative based on the work of lawson and hanson 1995 if a driver should have a negative effect it is introduced with a change of sign this bayesian approach is not very common but it is very appropriate in many occasions since we often do not have a very detailed quantitative information about the form of the distribution of the typical prior conjugate fink 1997 but we do have a very clear qualitative knowledge for example with respect to the sign that it should take which can be expressed as a uniform distribution in the semimark x 0 or x 0 the effects considered in this regression are instantaneous nned forecast the main driver is the forecast made with the neural network model explained in section 4 2 in the case of the ext type of day it is diversified according to the day of the week which can be tuesday wednesday or thursday as it has been observed that a certain differences exist in the rest of sub dating the case of days of the week does not allow for such diversification daily inertia medium term the average of the already known observations with 23 24 and 25 h of delay on the one hand and with 47 48 and 49 on the other by forcing the positive sign the inertia is maintained if it is significant and positive in other cases the nned algorithm itself is in charge of canceling it it works approximately as a kind of autoregressive seasonal model of period 24 in the natural time dating as opposed to the artificial time division subdate just described in the previous section concretely tree hours have been chosen to smooth these components considering that the daily periodicity is not completely precise in these series because of their anthropogenic component in addition choosing several hours avoids potential issues with the two seasonal time changes throughout the year daily correction medium term the average of the errors made by the model itself with 23 24 and 25 h of delay on the one hand and with 47 48 and 49 on the other which are also known in this case they will be used with the opposite sign that is if an error is made in one direction it is corrected in the other provided that such effect has been estimated as significant and otherwise the nned cancels it out it works approximately like a kind of moving average seasonal model of period 24 in natural time dating concretely tree hours have been chosen to smooth these components considering that the daily periodicity is not completely precise in these series because of their anthropogenic component in addition choosing several hours avoids potential issues with the two seasonal time changes throughout the year inertia and time correction short term for the first hours of the morning of each forecast session the observations and errors of the last hours are also available so it is possible to build inertia and short term correction inputs similar to the two previous ones from midday of the same forecast day they are no longer useful they work as a kind of regular arma in natural time dating protocol activation when the mobility restrictions imposed by the no2 protocol described in section 2 2 are activated the pollutant concentrations might be reduced with greater or lesser success so that the nned forecasts become obsolete and must be intervened in a deterministic way they are entered with a negative sign because it would not make sense for the action to increase contamination workday indicator within a long weekend pollution is particularly reduced on the public holidays themselves so a slight upward correction is needed for the rest of the days of the long weekend it only affects the type of day int school calendar during school vacations and adaptation periods with reduced schedules at the beginning and end of the school year there is a certain reduction in pollution that suggests a downward correction concretely this regression is estimated in logarithmic terms of both the observations and the nned forecasts and errors since it has been experimentally observed that the multiplicative relationship predominates over the additive 4 3 3 dynamic regression arfima on the errors of the previous regression a regular dynamic model is developed without a seasonal part that is concerned with maintaining inertia and correcting errors produced by the anthropogenic features definition arfima these type of models are considered as an extension of traditional arima models letting the differencing parameter to take non integer values by doing so arfima models are more appropriate for modeling time series with long memory granger and joyeux 1980 through this work the arfima function from the r package forecast is used hyndman and khandakar 2008 4 3 4 qr probability regression at this point the forecasts generated represent the mathematical expectation of the output magnitudes with this one can aspire at most to asymptotically estimate a log normal distribution under the laws of regression but since the distribution will not always fit perfectly with a log normal it is preferable to use a method based solely on the data to do this a new probabilistic quantile regression qr is estimated in order to estimate the future concentrations with as single input the forecast of the previous fslr arfima model in original terms without applying the logarithmic transformation quantile regression koenker 2005 is an extension of linear regression used when the mean is considered insufficient to characterise the response variable while the method of least squares estimates the conditional mean of the response variable qr allows for the estimation of the median q50 or in fact any other quantile thus allowing for the characterisation of the full distribution of the forecasts in our concrete case we obtain all percentiles from 1 to 99 in this setting since there is not always enough contrast surface the data variables ratio is low it may happen that the estimated percentiles do not comply with the basic rules of non negative and non decreasing applicable to every probability distribution usually it is in the extremes where there are more problems to alleviate this inconsistency an i spline interpolation is applied to these percentiles to ensure that these properties are as close as possible to the estimated values a general schematic of the pp fslr arfima qr model is presented in fig 8 while fig 9 summarizes the complete model with the data sources that govern the system 4 4 training procedure and operation details from a methodological point of view the training and parameters setting of the complete system has to be adapted to the essence of each block or model separately since socaire presents modules of very different nature in general terms the data used for the training along this paper correspond to july 2016 october 2020 unless otherwise specified the operational behavior of each of the models in relation to training and parameter estimation can be summarized as follows x arima model section 4 1 3 the x arima parameters are estimated through bayesian methods with intellectual property reserved these methodologies use all data available for parameter estimation without need of hyperparameters search this procedure is repeated each three months with all available data to that moment nned model section 4 2 in this case the training follows the usual pattern of neural networks the estimation of hyperparameters is performed by random search with data belonging to the interval january 2013 july 2016 as validation set after this process the network is trained with all the remaining available data using the adam algorithm for neural parameter optimization and once operative the network is updated weekly by means of new optimizations that take the most recently trained network as a starting point every three months a complete retraining of the network is allowed some other minor details are that the network is trained using the mean squared error mse as objective function batch size is 256 learning rate decay is set to 10 3 the initial learning rate is 0 001 and both early stopping and learning rate decay are implemented in order to avoid overfitting and improve performance pp fslr arfima qr model section 4 3 as in the case of x arima this model parameter s are estimated through bayesian methods with intellectual property reserved again these methodologies can use all data available through this process however in this case this procedure is repeated each day with all available data as the computation require little computational power 5 probabilistic prediction of the alert levels as described in section 2 2 the activation of the no2 protocol depends on meeting a number of requirements defined in three alert levels from a probabilistic point of view these requirements can be seen as compound events and being able to compute the future probability for the activation of each level is of utmost importance for decision makers according to the no2 protocol the activation of the different levels depends on what happens in several stations at the same time and in a certain number of consecutive hours in order to compute the aggregated probability the evaluation of the probability of the intersection of several events is thus needed knowing only the marginal percentiles and the historical residues left by each of the models 5 1 empirical marginal distribution of the different stations as we have shown above the model for each station offers a probabilistic forecast condensed in a quantile vector specifically the 99 integer percentiles are taken that is those corresponding to the probabilities p k 1 2 98 99 in this section we will look for a way to calculate the marginal distribution function for the forecast of each pollutant concentration from these quantiles calculated by each station s model for this it will be necessary to calculate the inverse of this distribution function and some statistics such as the mode which in turn requires an analytical representation that allows us to obtain its first and second derivatives in summary we need a pair of easily computable continuous and doubly derivable functions that allow us to evaluate very efficiently and precisely approximations of the distribution function and its inverse at any point of their respective domains the selected method is in fact an empirical change of variable that transforms the concentration into a standardized normal we will first take into account the fact that by definition the estimated quantiles are evaluations of the change in a variable that transforms the forecasts into a uniform distribution although this is valid for any source distribution for reasons of numerical stability it is preferable to apply the process to the logarithms of the quantiles thus if we apply the inverse of the standard normal distribution function to these log quantiles then the values obtained will follow that distribution by construction note that the calculation of the mode and deviation becomes trivial in this context during the approximation process we will establish the restriction that the probability density of the concentration forecast is always unimodal which agrees perfectly with the analyzed observations and the type of models used let us think of the moment in which decision making takes place t 0 and let us call y s t 0 the real concentration not yet observed in station s at future instant t t 0 the model of the s station will give us the q s t k percentiles of the forecast such that p y s t q s t k p k the transformed values are thus defined as z s t log y s t and the standardized normal quantile is u k φ 0 1 1 p k where obviously φ0 1 is the normal distribution function with mean 0 and deviation 1 now we will interpolate the pairs u k z t s by means of a function f s t r r that passes through those points 10 f s t u k z t s and in an analogous way the inverse function g s t f s t 1 r r will be constructed as the interpolating function that passes through the points z t s u k that is to say g s t z k u t s this allows us to construct an approximation of the concentration distribution function as follows 11 φ 0 1 g s t log y p y s t y ψ s t y and similarly we will obtain the approximation of its inverse 12 exp f s t φ 0 1 1 p ψ s t 1 p although we could have directly interpolated these functions which are the true objective numerically speaking the interpolation with these transformations is more stable largely because both z s t and u k are not bounded to avoid problems in the tails of the distribution and taking into account that both functions are monotonous it is highly recommended to use an interpolation method that guarantees this monotonicity in particular a monotonic spline interpolation has been used in this work the monotony of the functions f s t and g s t together with the monotony of the logarithm and the exponential functions guarantees that the maximum probable value of the concentration will be y s t exp f s t 0 let the standardized residue of the forecast be 13 ϵ s t g s t log y s t n 0 1 and note that indeed if the probable maximum forecast is exact i e if y s t y s t then 14 ϵ s t g s t log exp f s t 0 g s t f s t 0 0 similarly if the standardized residue is zero then the forecast is exact 5 2 empirical joint distribution section 4 has described the models that marginally predict the concentration of each pollutant at each station for different time horizons these models thanks to their arima structure are able to adequately treat the internal temporal correlation of each station that is the autocorrelation of each of the series of pollutants of the different stations in fig 10 it can be seen that the autocorrelation function acf is never too big and that when it does exceed the 2 sigma limits so does the partial autocorrelation function pacf this fact suggests that these are spurious correlations or any other types of concurrent causes not linked to time however in view of fig 11 there is nothing that indicates that residuals from different stations will be independent of each other rather they appear to correlate on the one hand even if nned models the spatio temporal dynamics of the process it is expected that closer stations will be more similar amongst them giving rise to positive correlations between their residues on the other hand as shown in fig 12 the errors in each forecast horizon for a single station will also not be independent of other stations previous horizons in fact this occurs mostly mutually present errors of a station correlate with the past errors of another station and vice versa in the previous section we have seen how to obtain by means of an interpolative variable change standardized normal residues in a marginal way for each station s and for each future instant t at current time t 0 however if the independence hypothesis is not plausible it is clear that knowing the marginal distributions does not imply knowing the joint distribution a family of models which are naturally capable of dealing with this situation are the x vecarma a type of multivariate models sims 1980 that include exogenous inputs cointegration and vector arma they are considered very powerful for the representation of cross correlated vector processes that might include exogenous factors eventually shared by several of them however they are intractable in computational terms for this setting thus we propose an empirical multi normal copula nelsen 1999 to approximate the joint distribution for every station and horizon the aim is to obtain an estimate of such joint distribution function for all the forecasts obtained marginally both in time and space using the joint sample correlation matrix between each pair of stations among all the horizons and stations however since there are 48 horizons and 24 stations that gives us a square matrix of 1152 rows and we would need at least 10 years of forecasts to obtain a meager 3 to 1 response surface which is clearly unacceptable for this reason we have developed a boxed tridiagonal scheme in which correlations are only taken into account one period ahead with this scheme only one year of forecasting is sufficient to obtain a reasonable estimate we will assume that the joint distributions of these standardized residues only depend on the station and the forecast horizons h t t 0 and h 1 t 1 t 0 but not on the specific moment t since the forecasts will be made every day at the same time since the marginal distributions of all the ϵ 1 t are normal unbiased and with unit variance the joint distribution of all stations 15 ϵ h ϵ 1 t 0 h ϵ s t 0 h ϵ s t 0 h t r s will be an unbiased multinormal with an unknown but obligatory unitary covariance matrix that is equal to the correlation matrix in the same way we will suppose that the residuals ϵ h 1 ϵ h corresponding to each pair of consecutive horizons are also distributed the same way by the principle of causality for the previous horizon ϵ h 1 an independent distribution of the following ϵ h 1 will be postulated since future events cannot influence the past in this way we can define the joint distribution of the different stations in each horizon in a recursive way 16 ϵ 1 n 0 c 1 ϵ h 1 ϵ h n 0 c h h 2 3 h c h c h 1 h 1 c h 1 h c h 1 h t c h h r 2 s 2 s h 2 3 h c 1 c 1 1 c h 1 h c h h r s s c h 1 h 1 s s c h h s s 1 c h h s s ρ ϵ s t 0 h ϵ s t 0 h 1 1 c h 1 h s s ρ ϵ s t 0 h 1 ϵ s t 0 h 1 1 note that the joint distribution of all horizons would have a tridiagonal covariance matrix with partitions of order s 17 c c 1 1 c 1 2 0 c 1 2 t c h 1 h 0 c h 1 h t c h h if we calculate the forecasts for enough dates t 0 of the past at the same time of the day and with the same horizons h 1 2 h we can obtain many samples of the residues with which we can thus estimate the matrices c h 1 h and c h h in this way we would obtain the distributions for each horizon conditioned on the previous horizon using the formula known analytically for the conditional partitioned multivariate normal 18 ϵ h n μ h c h μ h c h 1 h t c h 1 h 1 1 ϵ h 1 r s c h c h h c h 1 h t c h 1 h 1 1 c h 1 h r s s h 2 3 h these matrices can be stored for later use in future joint forecasts along with their cholesky and inverse decompositions 19 c h h l h l h t h 1 2 3 h c h l h l h t h 2 3 h first we simulate n vectors of n standardized independent residuals for the first horizon 20 η 1 n n 0 i n 1 2 n and pre multiplying them by l 1 we will have the standardized residuals of all the stations for the first horizon 21 ϵ 1 n l 1 η 1 n n 0 c 1 from there also starting from independent residuals 22 η h n n 0 i n 1 2 n residuals of each horizon conditioned by the previous one can be simulated 23 ϵ h μ h l h η h n on the one hand this approach solves the problem of time correlation in consecutive hours which is what is required and on the other hand it is simple enough to be able to generate correct estimations finally applying the transformations detailed section 5 2 we obtain n realizations of the future forecasts of the concentrations of the different stations in each horizon 24 y s t 0 h n exp f s t ϵ s h n if this simulation is repeated a sufficient number of times we can calculate any joint statistic from the forecasts of the concentrations in the different stations in particular for example to calculate the probability of activation of the pre warning level of the no2 protocol defined as the probability of the concentration of no2 exceeding a certain threshold υ 180 in at least two stations during two consecutive hours it will simply be necessary to calculate what proportion of the simulated samples meet these criteria 6 operation and performance 6 1 operation in order to be used by decision makers in the department of the city council in charge of air quality socaire has been integrated with a web app that allows to simply and directly view the forecasts for pollutants and the probability of reaching the levels established within the no2 protocol as explained in section 2 2 this section will show the site structure and its basic operation principles the main overview of the web tool is shown in fig 13 on the one hand at the top you can choose the pollutant to display blue buttons the date on which you want to make a query calendar button and different submenus where you can see in more detail the probability that the protocol will be activated shown tab and both the system predictions and a summary of contrast measures on the other hand in the central part the information related to the submenu in which the user is at that moment is shown in this specific case the probability of the levels of the no2 protocol being activated the operation of the tool for monitoring the future probability of reaching the different levels of the protocol are presented in fig 14 after using the ensemble of nested models described in section 4 to forecast no2 quantiles the outer rings show the probability of each individual station exceeding the levels set in the no2 protocol 180 μg m3 200 μg m3 and 400 μg m3 for prewarning warning and alert respectively once the individual probabilities are computed it is possible to use the process explained through section 5 to estimate probabilities of compound events given that the protocol is defined over areas and not for individual station levels the intermediate ring shows the probability of exceeding the expected pollution levels for each of the 5 areas in which madrid is partitioned in the no2 protocol see fig 1b lastly the inner ring contains the aggregated probability of the different levels of the protocol being activated in the entire city it uses the probabilities over the five areas to estimate this final probability since the set of mobility measures defined in the no2 protocol depends on reaching extreme levels in various stations and for a pre set number of consecutive hours having such an overview is especially important however it is also interesting to visualize the individual forecast for each station over time the socaire website allows viewing the actual forecasts for each pollutant and each station as shown in fig 15 together with the predicted quantiles and real observed values these plots also show the probability of exceeding each level and the levels themselves 6 2 performance analysis usual error metrics as rmse refer to expected values which are found in the central part of the distribution but do not take into account any other information and are thus particularly unfit to evaluate probabilistic forecasts since the most usual models produce point forecasts and not the entire distribution these kinds of metrics are the only option however when dealing with the prediction of the complete distribution as in our case other metrics have been proposed in order to summarise model performance information in a more comprehensive and realistic way for example crps is a measure of the squared difference between the forecast cumulative distribution function cdf and the empirical cdf of the observation gneiting and katzfuss 2014 as we will show in terms of performance socaire compares favorably to benchmarks in order to get a clear and quick idea about the behavior of the model table 2 shows the rmse and bias averaged both in time and space of the proposed methodology and compares it with four other models that due to their characteristics make it easier to understand the real performance of socaire persistence linear regression nned output without any linear correction and the npp provided by cams the persistence model is a naive model in which the forecast value is taken to be the observed value at the previous timestep it is thus a good benchmark model and one can get a rough idea of how good a new model is by seeing how much improvement there is with respect to persistence in our specific case for contractual reasons we use a more elaborated version of persistence which includes the daily weekly and annual cyclical structure of the series and is thus a simple although powerful model linear regression is a well known methodology for all kind of regression problems characterized by its simplicity but performing reasonably well in a multitude of scenarios in its most basic version it is limited by its only linear response so it is a good candidate to be beaten as a sample of having a model of minimum guarantees as was the case with persistence in our particular case we use a multioutput scheme for each station we generate a model that will have as input the past timesteps and will return jointly all predicted timesteps regarding the nned model its inclusion has a dual purpose on the one hand to have a clear and direct comparison with a neural architecture on the other hand to be able to clearly and precisely visualize the improvement that the complete modeling explained in section 4 implies in terms of performance and the potential benefit that can be obtained from using both types of strategies similarly the npp provided by cams represent another good baseline to be improved upon by any new model since it is based on a synoptic scale it is expected that any model focused on a smaller and concrete terrain extension will improve its results if this is not the case it would make more sense to use cams npp as an approximation instead of the proposed new methodology for a more detailed view of error metrics refer to fig 16 as it can be seen socaire consistently outperforms all baselines in terms of rmse and bias for the four pollutants concretely socaire supposes an average rmse improving of 37 with respect to cams a 27 with respect to lr a 10 with respect to nned and 44 with respect to persistence reinforcing the idea that socaire shows good performance and behaves very well as a predictor also socaire demonstrates to be in general terms an unbiased predictor of pollution which emphasizes the fact that the proposed model is being able to correctly describe the aforementioned terms related to the system another issue that is of special importance in our problem is the behavior of the model depending on the prediction timestep hour as it was shown in fig 3 the series are highly hour dependent for example no2 presents peaks usually around 08 00 10 00 and 22 00 00 00 in the framework of air quality management and monitoring these peaks are extremely important as they represent the higher risk and consequently the moments when the maximum recommended and or permitted levels are usually exceeded thus and given that one of the main objectives of socaire framework is forecasting the probability of each level of the no2 protocol showing a good performance in peak hours is of crucial importance fig 17 presents the rmse error for each pollutant and for each prediction horizon averaged over all stations from this figure it becomes clear that socaire is especially efficient in peak hours where the gap with baseline models is even wider until now we have covered aggregated error over all stations as the activation of the no2 protocol depends on compound events of individual stations it is important to make sure all of them behave similarly as it was explained before the complete model has a module which is able to relate and exploit shared spatial information section 4 2 but it also models each station independently based on its own characteristics section 4 3 by taking into account both types of information we expect to avoid possible biases of predominance by some spatial areas over others but still be able to make use of the relations that exist among them the crps for the no2 predictions at each station is shown in the top row of fig 18 it is worth noting that stations with lower crps errors correspond to green areas of the city of madrid stations 24 49 and 58 scaling these crps values to a n 0 1 bottom row of fig 18 let us see how all error distributions have a very similar behavior hence it is possible to assure that our modeling strategy works as expected and results in an approximately unbiased prediction of the spatial component the evaluation of these models has been done using the data from january 2020 to october 2020 with the system already operational and therefore functioning as described in section 4 4 thus the estimated errors represent realistically the errors the model is recording in its daily operation the aggregated error metrics from all predicted timesteps over that period generate the error distributions analyzed in this section 7 conclusions and future work throughout this manuscript we have discussed the details of socaire the new operational system for air quality forecasting and monitoring in the city of madrid based on an ensemble of statistical and neural models socaire is built under the premise that it is possible to integrate the diverse information that correlates with air quality in order to model it this information includes historical values of the series itself numerical weather and pollution predictions and anthropogenic features concretely the proposed methodology tackles the prediction of the four main pollutants no2 o3 pm10 and pm2 5 for a 48 h horizon thanks to its probabilistic nature the system is able to combine the predictions of the full probability distribution for compound events using a bayesian estimation of the future distribution of the different stations over time thus the system outputs are a valuable tool for managing the no2 protocol enforced by the city council of madrid the tool presented in this paper is not only a theoretical proposal but it has been adopted as the official application to monitor analyze and make day to day decisions about air quality the last part of this work summarizes the structure and operation of socaire s web as well as the main highlights of the good results and performance of the system in the future it would be interesting to apply a cost effectiveness analysis focused on the no2 protocol activation probability also we are working towards the inclusion of a traffic forecasting system which might improve the performance of the models by enhancing the information that anthropogenic features provide finally socaire could be adapted to predict any kind of combined air quality index and not only those ones affecting the current protocol declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to thank josé amador fernández viejo and the team at the general directorate for sustainability of the municipality of madrid especially maría de los ángeles cristóbal lópez for her continuing support and enthousiasm towards this project this research has been partially funded by empresa municipal de transportes emt of madrid spain under the program cátedra emt uned de calidad del aire y movilidad sostenible a x arima vs arima x in order to illustrate the differences between both kinds of models let us introduce the simplest case ar 1 specifically δ b 1 φ b 1 φ 1 b and θ b 1 in this circumstances arima x would be reduced to a linear regression 25 y t α x t φ 1 y t 1 ε t whereas the x arima model would be bilinear which is much more complicated to estimate than a linear regression 26 y t α x t φ 1 y t 1 α x t 1 ε t note that when the absolute value of φ 1 is very small there will be almost no difference between the two models but otherwise they will be very different let us imagine for simplicity that there is a single exogenous driver consisting of a pulse the blue line in fig 19 i e its value is 1 at a given instant of time and 0 the rest of the time we have arbitrarily set the parameters σ 0 1 α 2 and φ 1 0 93 and simulated two processes each following one of the models in red with a thicker line the x arima and in orange the arima x both generated from the same series of residuals green line logically both series coincide perfectly until the pulse occurs but while in the first one the effect of the pulse vanishes instantaneously in the second one it lasts quite a long time because the ar root is very close to unity if we had set φ 1 0 70 the effect would have lasted not 12 h but almost two days fig 19 comparison between x arima and arima x models fig 19 while usually instantaneous transfers are much more common than damped transfers like the one we have shown even when they occur they do not usually present exactly the same shape and damping rate as the series noise itself although possible the probabilities of all transfer functions of all the inputs being coincident with each other and with the arima model are scarce strictly speaking using the appropriate transfer functions the two model classes are equivalent but x arima fits in a more natural way and without using complicated constraints 
25779,strategic decision making on long term drought risk management can be supported by integrated assessment models to explore uncertain future conditions and potential policy actions such models have to meet many sometimes conflicting requirements posed by policy makers model developers and stakeholders this paper discusses the case of the national water model nwm that is applied for national policy making on drought risk management in the netherlands the case demonstrates that the chosen assembled model set up in which several existing models are combined is cost effective and increases stakeholder acceptance but also leads to high model complexity and computation time to be effective for policy making integrated assessment models need to produce relevant model outcomes that are accepted by stakeholders within acceptable time and cost limits for this the model set up must support simulations at different aggregation levels allowing both detailed analysis and exploratory analysis of many scenario strategy combinations while maintaining internal consistency keywords national water model large scale integrated modelling national policy making regional stakeholder support risk based decision making drought risk management dutch delta programme climate change model requirements meta modelling cpu time 1 introduction strategic decision making on long term drought risk management is a complex process in which a large range of relevant system processes and impacted sectors is involved and uncertainty about future changes in climate and in the environmental and socio economic system is large integrated assessment models are indispensable tools in support of a decision making process as they provide a framework to systematically and transparently understand the system including linkages and feedbacks between system components explore scenarios and policy actions and communicate with stakeholders hamilton et al 2019 loucks and van beek 2005 the trade off between model complexity and computing time needs to balance the need to cover a required number of calculations and scenarios while keeping sufficient mechanistic and spatial detail to represent the elementary functioning of the system haasnoot et al 2014 booij et al 2003 many requirements play a role in the development of integrated assessment models common requirements for an effective decision support model reviewed and clustered by hamilton et al 2019 include credibility relevance legitimacy model accessibility end user satisfaction timeliness and costs for maintenance and computing the prioritization of these criteria may be subject of debate among the model developers decision makers and other stakeholders from a decision maker s perspective model outcomes should be relevant for the decision at hand accepted by the stakeholders and produced in time to be useful in the decision process from a modeller s perspective the model should be scientifically and technically valid sufficiently representing the system dynamics and accessible including availability of user friendly and well documented software and data dilemmas in the model development may arise from conflicting requirements for example the desire to include more detailed processes at the cost of computational efficiency there is a growing interest in quantitative risk analysis to inform drought risk management policies similar to the approach taken for other natural hazards such as floods and earthquakes hall and leng 2019 drought risk is understood as the combination of the probability of drought occurrence and the associated impact on society appreciating different meteorological hydrological soil moisture and or groundwater drought typologies van loon 2015 risk analysis involves considering the full range of drought conditions to which a system might be exposed and forms the basis for cost benefit analysis of investments by governments and water users such as drinking water companies the agricultural sector and industries because droughts develop slowly over time and various hydrological processes soil moisture groundwater river discharge impact a variety of sectors the construction of the full range of plausible and relevant conditions with statistical or empirical methods is far from straightforward mishra and singh 2011 therefore simulation of long time series of meteorological conditions with a coupled hydrological and hydrodynamic model representing natural variability at different time scales is considered to be a necessary prerequisite for a reliable mapping of relevant drought conditions and their frequency furthermore sufficient spatial and temporal detail is necessary to be able to assess and compare a variety of risk management options for example the implementation of more efficient irrigation techniques may significantly reduce dependency on the water supply system when implemented on a large scale multiple scales thus need to be integrated in the model to better understand drought propagation through the socio hydrological system to improve our understanding of the dilemmas in the development and application of integrated assessment models and ways to overcome these this paper discusses the role of the national water model nwm in support of drought risk management in the netherlands over the past decades the nwm has been frequently updated to meet gradually changing requirements despite its importance in national policy making the model is increasingly perceived by stakeholders as being too slow and too complex in other words policy makers model experts and stakeholders question is the model still fit for purpose we analyse how the nwm has evolved in response to the requirements and how dilemmas arise based on the authors collective experience and in depth interviews with scientists and policy advisors for the delta programme we focus on three aspects that determine whether a model is fit for purpose from a decision maker s perspective relevance of the model outcome stakeholder acceptance and timeliness in the decision making process relevance of the model outcome relates to the processes that are modelled and their level of detail as well as the number of strategy scenario combinations that can be simulated the resulting model requirements in terms of spatial and temporal resolution may vary throughout the policy process stakeholder acceptance relates to the agreement between model output results and observed or perceived status of the water system at the regional scale addressed by the stakeholder timeliness refers to whether the desired model outcomes can be published at a convenient and opportune time for the policy process these aspects are a condensation of the criteria reviewed by hamilton et al 2019 and play an important role in the fit for purpose discussion that is taking place in practice insights from this paper will be used as input for the ongoing debate about the future of the nwm instrument in support of the next policy cycle of the delta programme 2022 2027 these insights can provide inspiration for other practitioners who design maintain and apply integrated assessment models in support of long term policy making for drought risk management 2 description of the national water model 2 1 role and history the national water model nwm plays an important role in the netherlands national delta programme on fresh water supply glas 2019 addressing the national strategy to manage the risk of water shortage due to droughts under climate change and socio economic change in support of this decision making process the national water model integrates hydrological hydrodynamic and water allocation models for all physical processes that are relevant for drought risk analysis the model has a long history some of the submodels have been first developed in the 80s as part of the policy analysis for the water management of the netherlands pawn study see wegner 1981 abrahamse et al 1982 goeller et al 1983 goeller et al 1985 which provided model based decision support for the 2nd national policy on water management rijkswaterstaat 1984 in this pawn project more than 50 models were developed to quantitatively describe all relevant aspects of drought risk management including impacts on agriculture shipping industry and drinking water this pioneered the application of cost benefit analysis for the netherlands policy interventions in water management various model concepts from pawn have been integrated in the current national hydrological model lhm which now forms one of the submodels of the nwm since the 1990s the lhm has been further developed see vermulst et al 1998 currently by a consortium of dutch research institutes 1 1 http nhi nu nl index php organisatie in different forms lhm has supported strategic decision making on water management in the netherlands over the past decades for example the delta programme strategy for the period 2016 2021 kuijken 2014 and currently for the upcoming delta programme strategy for the period 2022 2027 2 2 physical processes in the netherlands drought is defined as a period of precipitation deficit a period of low river discharge or a combination of both drought impacts include soil moisture deficit decline in groundwater levels shortage of surface water supply for irrigation water level management navigability and flushing of polders salt water intrusion in coastal areas reduced water levels in the main transportation waterways and water temperature increase in rivers and canals the national water model assembles several submodels that each simulate a subset of these relevant physical processes for a full understanding of drought risk economic and societal impacts of droughts are quantified with impact modules that are available as post processing tools in the nwm various processes are considered at different spatial scales for example surface water allocation of rhine river water in periods of drought takes place on a national scale the groundwater system is of crucial importance primarily in the southeast and east of the netherlands and water shortage in the north of the netherlands depends on water allocation schemes and reservoir management of lakes ijssel and marker furthermore drought impacts in the west are caused by salt water intrusion from sea and salinization of groundwater determines water availability for agriculture in all coastal areas these processes are all integrated in one national model in order to be able to understand water shortage issues at a national scale and compare various types of measures with feedback effects between processes and regions the input data of the nwm include time series of precipitation evapotranspiration and discharge from the main rivers rhine and meuse at national entry points output of the nwm includes groundwater levels on 250 250 m resolution daily and 10 daily time series of river discharge within the national domain and water demand and supply for various users irrigation water level control flushing of polders drinking water industry on several inlet points of the distribution network fig 1 2 3 submodels the following submodels are incorporated in the nwm fig 2 lhm a national hydrological model of the netherlands lsm light a national one dimensional hydrodynamic model sobek ndb a regional one dimensional hydrodynamic model of the south west of the netherlands including salt water simulations ltm light a national surface water temperature model based on lsm light a detailed technical description of each underlying model can be found in prinsen et al 2014 and de lange et al 2014 the groundwater model modflow is derived from modflow 2005 harbaugh 2005 and coupled with the unsaturated zone model metaswap van walsum and groenendijk 2008 which in turn is based on the physics of the swap model kroes et al 2008 the regional surface water distribution is modelled using mozart bos et al 1997 and the distribution model dm is used to describe the water distribution in the large rivers and distribution canals lsm light prinsen and wesselius 2015 is a one dimensional 1d hydrodynamic surface water model of the major rivers and canals of the netherlands sobek ndb van der linden and van zetten 2002 is a 1d hydrodynamic model for calculating salt water intrusion from the sea in the coastal rivers and canals in the mid western parts of the netherlands finally ltm light meijers and boderie 2004 simulates the depth average 1d surface water temperature in the major rivers and canals the computational framework delft fews werner et al 2013 takes care of the model coupling data exchange and feedbacks between the submodels 2 4 nwm governance and stakeholder groups the nwm project is guided by a steering committee an advisory board and a scientific committee the steering committee consists of national and regional water managers and policy advisors who decide on which model developments should be prioritised the advisory board includes experts from national research institutes who advise on long term developments furthermore the scientific committee advises on the scientific quality of the models and model structure of the nwm the underlying submodels of the nwm are developed validated and maintained in separate projects each with its own governance structure the stakeholders in the steering committees of these submodels partly overlap with those in the nwm committees the development of lhm the largest submodel of nwm is a continuous process in interaction with stakeholders including hydrologists and policy advisors from all regional water authorities and the national water management authority rijkswaterstaat in the netherlands the lhm model has its own set of requirements with respect to the model concept model accuracy parameterization of physical processes simulation time data etc 2 5 the nwm project nwm is an example of a modular assemblage approach to integrated modelling in contrast to a single integral model representing the whole system see voinov and shugart 2013 in an assemblage approach existing models are reused that are developed and tested by specialists in that particular area integrated particularly refers to the consistent treatment of input data and the time synchronization across all processes and regions for example the exploration of the effect of long term changes in climate and land use requires model input data for the future scenario to be developed consistently for all regions and submodels a major advantage of an assemblage approach is that it reduces model development cost time and effort on the other hand investments are needed in the interfacing interoperability information exchange and governance of the development of the components rizzoli et al 2008 the nwm project started in 2010 then labelled as the delta model with key objectives to provide a consistent and accepted set of models in a robust and flexible modelling environment to support policy analysis within the delta programme prinsen et al 2014 ruijgh et al 2015 to ensure consistency and reproducibility a selection of the most appropriate subset of models from a large ensemble of existing candidate models with varying spatial resolutions and scopes was made the delft fews framework enabled the utilization of standardized boundary conditions and data exchange between the models furthermore the submodel developers are required to provide proper documentation and support and maintenance to enhance uptake of the models in the nwm framework choices regarding the level of detail and upscaling of regional data to the national scale were negotiated between the scientific community regional water managers and policy makers from 2010 onwards in view of the first phase of the delta programme additional submodels were coupled first sobek ndb and later lsm light see fig 2 and the exchange of data between these submodels was improved the salt water intrusion model sobek ndb was coupled to dynamically calculate the salt dependent closure of one of the main fresh water inlet points in the west of the netherlands because feedbacks exist between salt concentration and water distribution an iteration loop of both lhm and lsm light was required furthermore in 2015 the choice was made to start simulating 100 year time series of historical and future precipitation evaporation river discharge and sea level based on historical measurements kroon et al 2015 with this approach the observed temporal and spatial correlations between the various variables are included thereby allowing a proper estimation of the probability distribution of drought impacts in parallel the submodel lhm was updated with regional data provided by regional stakeholders such as measured time series of surface and groundwater levels detailed data on the surface water elements and on the vertical discretization of the subsurface for example the number of soil types was tripled thereby refining the unsaturated zone parameterization to increase computational efficiency parallelization of the computational cores has been developed see e g verkaik et al 2021 the current activities of the nwm project mainly consist of integrating updated versions of the underlying submodels and data streams in combination with hardware upgrades improving model coupling and data exchange and improving the consistency of boundary conditions between the submodels for example related to future scenarios 2 6 nwm quick scan tool over the past years a quick scan tool qwast gijsbers et al 2017 has been developed to allow the quick and rough exploration of measures that are aimed at water demand or water allocation on the national scale the temporal and spatial resolutions of the water allocation network are similar to that of the distribution model one of the nwm submodels in the policy process qwast serves as a first order evaluation of the effectiveness of measures with the aim of developing a water allocation strategy qwast simulates water allocation given pre calculated and time dependent water demands taken from nwm of various users in all regions that are connected to the main water distribution system considering prioritization over users and regions the quick scan model does not cover the full range of potential risk management actions since it only includes surface water processes and water allocation excluding rainfed agriculture salt water intrusion and groundwater processes 3 discussing the key requirements of the national water model the nwm aims to support a decision making process for which questions about system behaviour effects of external developments climate change socio economic change and effectiveness of drought risk reduction measures need to be answered in a timely manner this section first describes the modelling goals and model complexity in relation to the policy process and then discusses the extent to which the nwm is able to meet the three key requirements relevance timeliness and stakeholder acceptance 3 1 modelling goals in relation to the policy process in the context of water resources management modelling activities are designed to provide useful and timely information to all stakeholders involved in the decision making process which typically consists of the following phases including feedback loops loucks and van beek 2005 i inception ii situation analysis iii strategy building and iv action planning implementation monitoring and evaluation modelling takes place in support of the situation analysis phase ii and strategy building phase iii in order to analyse the problem under current and future conditions through scenario analysis and to assess a collection of alternative policy actions within phase ii and iii four modelling goals can be distinguished fig 3 the first goal is to understand the natural and human system with all relevant processes and feedbacks and identify the main characteristics of the problem for drought risk management this involves the identification of the probability of water shortage due to drought as well as the impacts on a range of water users major drought characteristics may vary across the different regions the second goal is to explore potential emergence or development of future problems explore future scenarios such scenario analysis includes mapping the uncertainty of future climate change and potential response of water users to this for example changing time varying water demands the third goal is to explore various policy actions to reduce drought risk and help the stakeholders develop a preferred strategy from a long list of potential measures these include operational tactical and strategic measures focusing on water demand reduction and or water supply increase e g buurman et al 2017 finally the fourth goal is to assess several strategies on their effectiveness i e the ability to reduce the impact of water shortage due to drought against acceptable societal costs integrated assessment with the purpose of supporting long term policy making 50 100 years ahead involves taking into account many types of deep uncertainty arising from multiple plausible future developments multiple views on system evaluation various responses to events and trends natural variability and limited knowledge of the system processes and functioning marchau et al 2019 walker and van daalen 2013 hallegatte et al 2012 haasnoot et al 2012 lempert et al 2003 to ensure consistency and optimize model management one integrated assessment model is preferably used to analyse the whole system and to explore the interaction between future scenarios and policy actions while additional detailed models may be used subsequently to perform in depth analyses for a certain region sector or measure in the current policy cycle leading to an updated national strategy to deal with climate change delta programme 2022 2027 nwm is used for three of the four described goals fig 3 to explore water shortage in the future four scenarios were used for two future time periods 2050 and 2100 these so called deltascenarios wolters et al 2018 combine two climate change scenarios knmi 2015 with two socio economic scenarios manders and kool 2015 resulting in four different storylines for climate change and the response of the human system for different socio economic configurations land use irrigation and drinking water demand are examples of scenario specific human responses additionally one scenario variant was developed to explore the effect on water demand of implementing drainage systems in peat areas to reduce co2 emissions according to the paris agreement all scenarios were translated into consistent model input and boundary conditions by hunink et al 2018 a total of 11 model experiments 1 x reference 5 scenarios for 2050 5 scenarios for 2100 were thus carried out to understand the drought risk system and explore drought risks in the future the meta model qwast has been used to quickly explore a large set of policy actions third goal the choice for a separate quick scan tool was made because at least currently the nwm is not fast enough to support the iterative process of moving from a long list of potential measures to a short list of promising measures about 60 model experiments were conducted with qwast 20 individual measures and 10 strategies combinations of measures were simulated for 2 scenarios encompassing a 100 year times series each 100 year qwast run takes a few hours to simulate nwm was finally used again to compare priority strategies in order to choose the preferred strategy fourth goal however because of the simulation time and cost constraints both following from model complexity only 4 scenario strategy combinations were explored 2 strategies for 2 scenarios in 2050 each for the 100 year time series the subsequent interviews with policy advisors revealed that this collection was not sufficient to address all relevant policy questions for example the relative impact of drinking water and agricultural water extraction on declining groundwater levels under future conditions remained unresolved 3 2 model complexity in terms of model complexity it is generally accepted that for the purpose of decision support and exploration of future developments spatially and temporally less detailed models are required than for instance operational models to forecast system behaviour in response to weather predictions fig 4 displays a conceptual overview of model applications with respect to three model dimensions the required conceptual detail of the processes that are modelled model complexity the level of acceptable model uncertainty and the time range of the analysis three types of model application can be identified 1 prediction estimating the value of a system variable given a change in system inputs or boundary conditions 2 forecasting predicting the value of system variables in the near future on the basis of varying system forcings and 3 exploration estimating model variables in the future given changes in a combination of model inputs parameters and boundary conditions while for prediction and forecasting historical accuracy is an important characteristic of model approaches exploring long term future 30 100 years ahead requires model outcomes to be plausible given assumed conditions see kelly et al 2013 many scenarios are needed to explore the interactions between processes and impacts of interrelated changing conditions including feedbacks and how this can be adapted by policy actions for this type of application models used should be fast enough to allow large numbers of calculations for long time series while keeping sufficient detail to represent all relevant processes and their interactions haasnoot 2013 booij et al 2003 a comparison of models with different levels of complexity is needed to determine the configuration that would be sufficient for answering policy questions guillaume and jakeman 2012 determination of an optimal model configuration is difficult when the model will be used for different applications simultaneously in a so called unified modelling concept see e g clark et al 2015 also the lhm is used for purposes other than long term assessments e g as a predictive model for regional hydrological studies as a component in the national operational drought management system and for simulating and forecasting the real time drought situation and expected water shortage in the short term these multiple purposes imply that choices in temporal and spatial resolution require a trade off between high accuracy versus high computational efficiency interestingly many particularly regional policy advisors consider the current version of lhm not detailed enough for regional groundwater studies since the multi purpose model lhm is a major component of nwm the nwm is placed in the middle of the vertical axis in fig 4 reflecting a compromise between exploring the future and forecasting the steering committee considered it necessary that nwm contains groundwater and surface water interaction because 1 large parts of the subsurface of the netherlands consists of highly permeable river sediments causing the surface water and groundwater systems to act as one system winter et al 1999 with mutual feedback mechanisms especially when simulating dry conditions and 2 the netherlands partly consists of large areas with deeper groundwater levels that react slowly 20 years to changes in hydrological boundary conditions causing adjacent connected areas to react also slower than other areas and 3 the central veluwe area with deep groundwater levels directly connects with the lake ijssel via the deeper subsurface which is relevant when performing simulations for long time periods gehrels 1999 3 3 relevance of the model outcomes the required level of detail in an integrated assessment model directly follows from its purpose in the policy process in this case to understand the system explore scenarios and assess strategies to deal with potential future problems see fig 3 relevant system processes must be included at the right temporal and spatial scale and their response to changing climate and land use conditions need to be assessed as well for the exploration of drought risk and mitigation strategies the nwm should provide a realistic representation of natural variability and extremes of main drivers and internal dynamics of droughts which determines the frequency and intensity of major drought impacts van loon 2015 this in turn gives guidance to the policies addressing the balance between water demand and supply under current and potential future climate and socio economic conditions the relevant performance indicators for drought risk assessment in the netherlands include frequency and severity of soil moisture deficit which may change due to climate change land use change and water management actions average summer groundwater levels which may change due to the combination of climate change and extractions for drinking water production industrial use and irrigation frequency level and duration of salt water intrusion which may change due to the combination of changing river flows and sea level rise and affecting fresh water inlets level and frequency of water shortage from the main rivers canals and lakes which may change due to a combination of temperature change change in river flows land use change and farmers response to climate change duration and frequency of low water depths along the main waterways that impact inland shipping this set of variables reflect the many physical processes related to drought furthermore the computational framework allows long time series to represent climate variability many combinations of measures and scenarios can be simulated and output is easily connected to economic impact models the model outcome is thus considered relevant for the decision making process 3 4 timeliness to be useful in the decision making process model results should be published at a convenient and opportune time hamilton et al 2019 timeliness is not only related to the net simulation time computation time of the model but also to the time it takes to prepare model inputs and schematisations e g derived from scenarios and proposed policy actions and the analysis of the outcomes complex models are more difficult to schematize and interpret thereby increasing the duration of the modelling exercise which poses a risk for the timeliness of the outcomes available budget to carry out the computations clearly play a role as well which may limit the efficiency of the simulations for example the more model runs that are carried out in parallel the higher the use cost of processing units over the past decade the simulation time of nwm has significantly increased due to the addition of submodels and the simulation of long time series despite developments in computation architecture that allow parallelization of model runs see section 3 2 this increased the simulation time of the full model train from a few weeks to 2 3 months for one 100 year run making it increasingly challenging to synchronize with the policy making process and significantly increasing the project costs in practice the deadlines of the delta programme policy making process are met by limiting the number of nwm runs and the use of qwast to explore potential policy actions this shows that the nwm s ability to simulate a sufficient number of scenario strategy combinations to answer relevant policy questions is limited 3 5 stakeholder acceptance to increase acceptance of model outcomes the decisions that are supported by these outcomes and commitment to its implementation stakeholders must be engaged in the model design process hamilton et al 2019 voinov and bousquet 2010 furthermore model outputs must be scientifically justified and developed without a bias towards a desirable outcome or interpretation hamilton et al 2019 cash et al 2003 a range of techniques is available to evaluate the scientific performance of an environmental model see bennett et al 2013 including quantitative comparison with observed data model validation since model behaviour under future conditions cannot be directly derived from model validation additional qualitative evaluation methods are often used based on e g theoretical reasoning extrapolation to future conditions or finding analogues in different locations or time ranges jakeman et al 2006 furthermore trust in model outcomes is also gained by discussions on their plausibility with stakeholders with high expertise in the functioning of the system they manage in practice quantitative and qualitative model validation is not carried out as part of the nwm project but is carried out as part of the projects that maintain and develop the submodels although the submodel lhm has been significantly improved over the past 10 years by including more processes and updating underlying data full model validation is not applied routinely but undertaken in irregular dedicated projects such a project is running in 2021 the former validation took place in 2013 similarly the submodel lsm has been improved by merging several regional models but has hardly been validated with observed low flow data besides model validation other activities have contributed to the trust in model outcomes stakeholders in the lhm project not only advise about model developments but also share local and regional data about their water system for example the groundwater model of lhm is based on upscaled data from the underlying regional high resolution 25 25 m groundwater models e g hoogewoud et al 2013 acceptance of the results of national analysis by regional stakeholders is promoted by the ability to compare results with their own regional model outcomes also the periodic discussion of model outcomes with an independent expert group facilitates this acceptance such discussions were facilitated in the previous delta programme policy cycle as well as in the pawn study during which the model based analysis was reviewed by a group of independent model experts from national and regional authorities goeller et al 1985 summarizing because the submodels are developed and validated in a continuous process in close interaction with regional stakeholders the nwm is accepted as state of the art 4 the main nwm dilemma over the past 10 years the national water model nwm has developed into a well documented and structured integrated model instrument with interconnected models that are accepted by stakeholders its outcomes are used in the drought risk management process because it describes all relevant drought related physical processes it allows long time series to represent climate variability and it connects to economic impact models its computational framework fews allows transparency and reproducibility of model simulations because continuous investments in development and validation of the submodels occur in a parallel process and in close interaction with regional water managers and model experts these submodels are considered state of the art for national scale analyses the nwm thus meets many of the key requirements however the multi purpose design of some components particularly the lhm submodel inevitably lead to trade offs in the configuration that is not fully optimized for its specific use as integrated assessment model for national policy making the subsequent heavy computational burden of the nwm limits its usefulness for the national delta programme because the number of scenario strategy combinations that can be afforded to be simulated in time for the policy process is too limited this demonstrates the main dilemma an assemblage approach limits development costs and increases stakeholder acceptance but it also increases the model complexity and computation time compromising timeliness in and knowledge for the policy making process the nwm case shows that policy analysts come up with practical solutions to overcome the dilemma for example by limiting the number of nwm simulations which increased timeliness but reduced the model outcome relevance because not all policy questions were answered another example is that the length of the time series for some of the scenario runs with nwm was reduced from 100 to 30 years at the expense of a reduced insight into natural variability and extreme drought events another choice that was made to overcome the dilemma was to develop a quick scan model qwast for water demands and water allocation the qwast served the purpose of quickly providing insight into the performance of several combinations of policy options under a range of scenarios however its relevance to the policy process is still limited because several relevant processes are left out and it heavily depends on the nwm for input on climate change and land use scenarios also a systematic comparison between the performance characteristics of qwast and nwm is not executed limiting the consistency between the two models clearly the choice of nwm as an integrated assessment model was well justified for its status as state of the art repository of accepted submodels but it is currently not fit for purpose to explore the desired scenario uncertainty range and the range of policy options in a timely manner its computational costs are too large and its complexity requires considerable time and effort to translate storylines about future development policy actions and model uncertainty into model schematizations and boundary conditions and to analyse the model output to provide valuable model outcomes for future development and application of the nwm the following is recommended 1 increase the flexibility of the nwm modelling framework and the submodel software allowing to switch between spatial scales resolution and degree of conceptual complexity for example in regions where groundwater is less important the lhm model could be used to simulate surface water allocation with a semi static groundwater boundary condition flexibility may also be introduced by adding the ability to alternate between the national scale and a regional scale using multi resolution modelling concepts davis and bigelow 1998 rabelo et al 2015 specific to the nwm case an upgrade to modflow6 langevin et al 2017 would allow for more flexibility in groundwater modelling facilitating detailed analysis for a certain region while maintaining connection and consistency with the rest of the model domain 2 derive and periodically update a meta model version of nwm that includes all relevant processes in a less detailed way than currently done in the nwm submodels a meta model or fast and simple model mimics the behaviour of complex models by using simplified cause effect relationships davis and bigelow 1998 van grol et al 2006 walker and van daalen 2013 haasnoot et al 2014 such a meta model can be used to explore many combinations of scenarios and policy options and to assess adaptation strategies the more detailed nwm and or its submodels may be used subsequently to perform in depth analyses for a certain region sector and or measure to ensure the acceptance and uptake of meta model outcomes by all stakeholders a standardized procedure is required to preserve consistency between the simple meta model and the institutionalized nwm furthermore stakeholders must be engaged in the model development process hamilton et al 2019 van delden et al 2011 voinov and bousquet 2010 in view of the varying model requirements in terms of spatial and temporal resolution throughout the policy process the future nwm should not be considered a single integrated model but instead a collection of detailed connected submodels and a less detailed meta model depending on the policy phase and resulting requirements many scenario strategy combinations can be explored with the meta model or more detailed insights can be obtained with submodels for specific regions processes and or measures see also haasnoot et al 2014 to move forward with the nwm project it is recommended to organize a continuous conversation about fit for purpose between policy advisors model experts and stakeholders the literature provides useful frameworks to structure and formalize such a conversation bennett et al 2013 guillaume and jakeman 2012 furthermore a comparison of models with different levels of complexity is needed to determine the configuration that would be sufficient for answering upcoming drought risk management policy questions 5 conclusion the netherlands national water model nwm is an integrated assessment model that allows exploring and assessing various adaptation measures long term strategies and future scenarios on a national scale in support of policy making on drought risk management the model s outcomes are considered relevant and are accepted by stakeholders but its simulation time is considered too long to respond quickly to policy questions also its complexity requires considerable effort in schematization and analysing the model simulations which reduces timeliness and increases project costs the nwm case has illustrated a clear dilemma that occurs when submodels are used as a basis for developing an integrated assessment model in support of policy making such an assemblage approach limits development costs and increases stakeholder acceptance but also implies trade offs when multi purpose submodels are used this poses a risk of increased model complexity and computation time compromising timeliness in and knowledge for the policy making process to deal with this dilemma two recommendations were made 1 increase the flexibility of the nwm modelling framework and the submodel software allowing to switch between spatial scales resolution and degree of conceptual complexity this is known as multi resolution modelling and 2 derive and periodically update from this institutionalized complex model a meta model i e a fast simple model fsm that includes all relevant processes to quickly explore many scenario strategy combinations the insights from the nwm case can be valuable for others that are involved with developing and maintaining integrated assessment models in support of long term policy making for water resources management the fit for purpose conversation between policy advisors and model developers deserves formalization and should be continuous in view of changing requirements and ongoing submodel developments when integrated assessment is supported by a meta model a standardized procedure is required to assure consistency between the meta model and the institutionalized complex model declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we greatly acknowledge the following people who contributed to this paper by sharing their valuable experience as policy advisor in the first and or second phase of the delta program or as expert in the field of integrated assessment modelling bas de jong eelco van beek erik ruijgh jeroen ligtenberg judith ter maat marjolijn haasnoot mark bruinsma neeltje kielen sharon muurling van geffen timo kroon and wim de lange we also thank the three anonymous reviewers for their valuable feedback on previous versions of this paper the research was partly funded by the dutch ministry of infrastructure and water management 
25779,strategic decision making on long term drought risk management can be supported by integrated assessment models to explore uncertain future conditions and potential policy actions such models have to meet many sometimes conflicting requirements posed by policy makers model developers and stakeholders this paper discusses the case of the national water model nwm that is applied for national policy making on drought risk management in the netherlands the case demonstrates that the chosen assembled model set up in which several existing models are combined is cost effective and increases stakeholder acceptance but also leads to high model complexity and computation time to be effective for policy making integrated assessment models need to produce relevant model outcomes that are accepted by stakeholders within acceptable time and cost limits for this the model set up must support simulations at different aggregation levels allowing both detailed analysis and exploratory analysis of many scenario strategy combinations while maintaining internal consistency keywords national water model large scale integrated modelling national policy making regional stakeholder support risk based decision making drought risk management dutch delta programme climate change model requirements meta modelling cpu time 1 introduction strategic decision making on long term drought risk management is a complex process in which a large range of relevant system processes and impacted sectors is involved and uncertainty about future changes in climate and in the environmental and socio economic system is large integrated assessment models are indispensable tools in support of a decision making process as they provide a framework to systematically and transparently understand the system including linkages and feedbacks between system components explore scenarios and policy actions and communicate with stakeholders hamilton et al 2019 loucks and van beek 2005 the trade off between model complexity and computing time needs to balance the need to cover a required number of calculations and scenarios while keeping sufficient mechanistic and spatial detail to represent the elementary functioning of the system haasnoot et al 2014 booij et al 2003 many requirements play a role in the development of integrated assessment models common requirements for an effective decision support model reviewed and clustered by hamilton et al 2019 include credibility relevance legitimacy model accessibility end user satisfaction timeliness and costs for maintenance and computing the prioritization of these criteria may be subject of debate among the model developers decision makers and other stakeholders from a decision maker s perspective model outcomes should be relevant for the decision at hand accepted by the stakeholders and produced in time to be useful in the decision process from a modeller s perspective the model should be scientifically and technically valid sufficiently representing the system dynamics and accessible including availability of user friendly and well documented software and data dilemmas in the model development may arise from conflicting requirements for example the desire to include more detailed processes at the cost of computational efficiency there is a growing interest in quantitative risk analysis to inform drought risk management policies similar to the approach taken for other natural hazards such as floods and earthquakes hall and leng 2019 drought risk is understood as the combination of the probability of drought occurrence and the associated impact on society appreciating different meteorological hydrological soil moisture and or groundwater drought typologies van loon 2015 risk analysis involves considering the full range of drought conditions to which a system might be exposed and forms the basis for cost benefit analysis of investments by governments and water users such as drinking water companies the agricultural sector and industries because droughts develop slowly over time and various hydrological processes soil moisture groundwater river discharge impact a variety of sectors the construction of the full range of plausible and relevant conditions with statistical or empirical methods is far from straightforward mishra and singh 2011 therefore simulation of long time series of meteorological conditions with a coupled hydrological and hydrodynamic model representing natural variability at different time scales is considered to be a necessary prerequisite for a reliable mapping of relevant drought conditions and their frequency furthermore sufficient spatial and temporal detail is necessary to be able to assess and compare a variety of risk management options for example the implementation of more efficient irrigation techniques may significantly reduce dependency on the water supply system when implemented on a large scale multiple scales thus need to be integrated in the model to better understand drought propagation through the socio hydrological system to improve our understanding of the dilemmas in the development and application of integrated assessment models and ways to overcome these this paper discusses the role of the national water model nwm in support of drought risk management in the netherlands over the past decades the nwm has been frequently updated to meet gradually changing requirements despite its importance in national policy making the model is increasingly perceived by stakeholders as being too slow and too complex in other words policy makers model experts and stakeholders question is the model still fit for purpose we analyse how the nwm has evolved in response to the requirements and how dilemmas arise based on the authors collective experience and in depth interviews with scientists and policy advisors for the delta programme we focus on three aspects that determine whether a model is fit for purpose from a decision maker s perspective relevance of the model outcome stakeholder acceptance and timeliness in the decision making process relevance of the model outcome relates to the processes that are modelled and their level of detail as well as the number of strategy scenario combinations that can be simulated the resulting model requirements in terms of spatial and temporal resolution may vary throughout the policy process stakeholder acceptance relates to the agreement between model output results and observed or perceived status of the water system at the regional scale addressed by the stakeholder timeliness refers to whether the desired model outcomes can be published at a convenient and opportune time for the policy process these aspects are a condensation of the criteria reviewed by hamilton et al 2019 and play an important role in the fit for purpose discussion that is taking place in practice insights from this paper will be used as input for the ongoing debate about the future of the nwm instrument in support of the next policy cycle of the delta programme 2022 2027 these insights can provide inspiration for other practitioners who design maintain and apply integrated assessment models in support of long term policy making for drought risk management 2 description of the national water model 2 1 role and history the national water model nwm plays an important role in the netherlands national delta programme on fresh water supply glas 2019 addressing the national strategy to manage the risk of water shortage due to droughts under climate change and socio economic change in support of this decision making process the national water model integrates hydrological hydrodynamic and water allocation models for all physical processes that are relevant for drought risk analysis the model has a long history some of the submodels have been first developed in the 80s as part of the policy analysis for the water management of the netherlands pawn study see wegner 1981 abrahamse et al 1982 goeller et al 1983 goeller et al 1985 which provided model based decision support for the 2nd national policy on water management rijkswaterstaat 1984 in this pawn project more than 50 models were developed to quantitatively describe all relevant aspects of drought risk management including impacts on agriculture shipping industry and drinking water this pioneered the application of cost benefit analysis for the netherlands policy interventions in water management various model concepts from pawn have been integrated in the current national hydrological model lhm which now forms one of the submodels of the nwm since the 1990s the lhm has been further developed see vermulst et al 1998 currently by a consortium of dutch research institutes 1 1 http nhi nu nl index php organisatie in different forms lhm has supported strategic decision making on water management in the netherlands over the past decades for example the delta programme strategy for the period 2016 2021 kuijken 2014 and currently for the upcoming delta programme strategy for the period 2022 2027 2 2 physical processes in the netherlands drought is defined as a period of precipitation deficit a period of low river discharge or a combination of both drought impacts include soil moisture deficit decline in groundwater levels shortage of surface water supply for irrigation water level management navigability and flushing of polders salt water intrusion in coastal areas reduced water levels in the main transportation waterways and water temperature increase in rivers and canals the national water model assembles several submodels that each simulate a subset of these relevant physical processes for a full understanding of drought risk economic and societal impacts of droughts are quantified with impact modules that are available as post processing tools in the nwm various processes are considered at different spatial scales for example surface water allocation of rhine river water in periods of drought takes place on a national scale the groundwater system is of crucial importance primarily in the southeast and east of the netherlands and water shortage in the north of the netherlands depends on water allocation schemes and reservoir management of lakes ijssel and marker furthermore drought impacts in the west are caused by salt water intrusion from sea and salinization of groundwater determines water availability for agriculture in all coastal areas these processes are all integrated in one national model in order to be able to understand water shortage issues at a national scale and compare various types of measures with feedback effects between processes and regions the input data of the nwm include time series of precipitation evapotranspiration and discharge from the main rivers rhine and meuse at national entry points output of the nwm includes groundwater levels on 250 250 m resolution daily and 10 daily time series of river discharge within the national domain and water demand and supply for various users irrigation water level control flushing of polders drinking water industry on several inlet points of the distribution network fig 1 2 3 submodels the following submodels are incorporated in the nwm fig 2 lhm a national hydrological model of the netherlands lsm light a national one dimensional hydrodynamic model sobek ndb a regional one dimensional hydrodynamic model of the south west of the netherlands including salt water simulations ltm light a national surface water temperature model based on lsm light a detailed technical description of each underlying model can be found in prinsen et al 2014 and de lange et al 2014 the groundwater model modflow is derived from modflow 2005 harbaugh 2005 and coupled with the unsaturated zone model metaswap van walsum and groenendijk 2008 which in turn is based on the physics of the swap model kroes et al 2008 the regional surface water distribution is modelled using mozart bos et al 1997 and the distribution model dm is used to describe the water distribution in the large rivers and distribution canals lsm light prinsen and wesselius 2015 is a one dimensional 1d hydrodynamic surface water model of the major rivers and canals of the netherlands sobek ndb van der linden and van zetten 2002 is a 1d hydrodynamic model for calculating salt water intrusion from the sea in the coastal rivers and canals in the mid western parts of the netherlands finally ltm light meijers and boderie 2004 simulates the depth average 1d surface water temperature in the major rivers and canals the computational framework delft fews werner et al 2013 takes care of the model coupling data exchange and feedbacks between the submodels 2 4 nwm governance and stakeholder groups the nwm project is guided by a steering committee an advisory board and a scientific committee the steering committee consists of national and regional water managers and policy advisors who decide on which model developments should be prioritised the advisory board includes experts from national research institutes who advise on long term developments furthermore the scientific committee advises on the scientific quality of the models and model structure of the nwm the underlying submodels of the nwm are developed validated and maintained in separate projects each with its own governance structure the stakeholders in the steering committees of these submodels partly overlap with those in the nwm committees the development of lhm the largest submodel of nwm is a continuous process in interaction with stakeholders including hydrologists and policy advisors from all regional water authorities and the national water management authority rijkswaterstaat in the netherlands the lhm model has its own set of requirements with respect to the model concept model accuracy parameterization of physical processes simulation time data etc 2 5 the nwm project nwm is an example of a modular assemblage approach to integrated modelling in contrast to a single integral model representing the whole system see voinov and shugart 2013 in an assemblage approach existing models are reused that are developed and tested by specialists in that particular area integrated particularly refers to the consistent treatment of input data and the time synchronization across all processes and regions for example the exploration of the effect of long term changes in climate and land use requires model input data for the future scenario to be developed consistently for all regions and submodels a major advantage of an assemblage approach is that it reduces model development cost time and effort on the other hand investments are needed in the interfacing interoperability information exchange and governance of the development of the components rizzoli et al 2008 the nwm project started in 2010 then labelled as the delta model with key objectives to provide a consistent and accepted set of models in a robust and flexible modelling environment to support policy analysis within the delta programme prinsen et al 2014 ruijgh et al 2015 to ensure consistency and reproducibility a selection of the most appropriate subset of models from a large ensemble of existing candidate models with varying spatial resolutions and scopes was made the delft fews framework enabled the utilization of standardized boundary conditions and data exchange between the models furthermore the submodel developers are required to provide proper documentation and support and maintenance to enhance uptake of the models in the nwm framework choices regarding the level of detail and upscaling of regional data to the national scale were negotiated between the scientific community regional water managers and policy makers from 2010 onwards in view of the first phase of the delta programme additional submodels were coupled first sobek ndb and later lsm light see fig 2 and the exchange of data between these submodels was improved the salt water intrusion model sobek ndb was coupled to dynamically calculate the salt dependent closure of one of the main fresh water inlet points in the west of the netherlands because feedbacks exist between salt concentration and water distribution an iteration loop of both lhm and lsm light was required furthermore in 2015 the choice was made to start simulating 100 year time series of historical and future precipitation evaporation river discharge and sea level based on historical measurements kroon et al 2015 with this approach the observed temporal and spatial correlations between the various variables are included thereby allowing a proper estimation of the probability distribution of drought impacts in parallel the submodel lhm was updated with regional data provided by regional stakeholders such as measured time series of surface and groundwater levels detailed data on the surface water elements and on the vertical discretization of the subsurface for example the number of soil types was tripled thereby refining the unsaturated zone parameterization to increase computational efficiency parallelization of the computational cores has been developed see e g verkaik et al 2021 the current activities of the nwm project mainly consist of integrating updated versions of the underlying submodels and data streams in combination with hardware upgrades improving model coupling and data exchange and improving the consistency of boundary conditions between the submodels for example related to future scenarios 2 6 nwm quick scan tool over the past years a quick scan tool qwast gijsbers et al 2017 has been developed to allow the quick and rough exploration of measures that are aimed at water demand or water allocation on the national scale the temporal and spatial resolutions of the water allocation network are similar to that of the distribution model one of the nwm submodels in the policy process qwast serves as a first order evaluation of the effectiveness of measures with the aim of developing a water allocation strategy qwast simulates water allocation given pre calculated and time dependent water demands taken from nwm of various users in all regions that are connected to the main water distribution system considering prioritization over users and regions the quick scan model does not cover the full range of potential risk management actions since it only includes surface water processes and water allocation excluding rainfed agriculture salt water intrusion and groundwater processes 3 discussing the key requirements of the national water model the nwm aims to support a decision making process for which questions about system behaviour effects of external developments climate change socio economic change and effectiveness of drought risk reduction measures need to be answered in a timely manner this section first describes the modelling goals and model complexity in relation to the policy process and then discusses the extent to which the nwm is able to meet the three key requirements relevance timeliness and stakeholder acceptance 3 1 modelling goals in relation to the policy process in the context of water resources management modelling activities are designed to provide useful and timely information to all stakeholders involved in the decision making process which typically consists of the following phases including feedback loops loucks and van beek 2005 i inception ii situation analysis iii strategy building and iv action planning implementation monitoring and evaluation modelling takes place in support of the situation analysis phase ii and strategy building phase iii in order to analyse the problem under current and future conditions through scenario analysis and to assess a collection of alternative policy actions within phase ii and iii four modelling goals can be distinguished fig 3 the first goal is to understand the natural and human system with all relevant processes and feedbacks and identify the main characteristics of the problem for drought risk management this involves the identification of the probability of water shortage due to drought as well as the impacts on a range of water users major drought characteristics may vary across the different regions the second goal is to explore potential emergence or development of future problems explore future scenarios such scenario analysis includes mapping the uncertainty of future climate change and potential response of water users to this for example changing time varying water demands the third goal is to explore various policy actions to reduce drought risk and help the stakeholders develop a preferred strategy from a long list of potential measures these include operational tactical and strategic measures focusing on water demand reduction and or water supply increase e g buurman et al 2017 finally the fourth goal is to assess several strategies on their effectiveness i e the ability to reduce the impact of water shortage due to drought against acceptable societal costs integrated assessment with the purpose of supporting long term policy making 50 100 years ahead involves taking into account many types of deep uncertainty arising from multiple plausible future developments multiple views on system evaluation various responses to events and trends natural variability and limited knowledge of the system processes and functioning marchau et al 2019 walker and van daalen 2013 hallegatte et al 2012 haasnoot et al 2012 lempert et al 2003 to ensure consistency and optimize model management one integrated assessment model is preferably used to analyse the whole system and to explore the interaction between future scenarios and policy actions while additional detailed models may be used subsequently to perform in depth analyses for a certain region sector or measure in the current policy cycle leading to an updated national strategy to deal with climate change delta programme 2022 2027 nwm is used for three of the four described goals fig 3 to explore water shortage in the future four scenarios were used for two future time periods 2050 and 2100 these so called deltascenarios wolters et al 2018 combine two climate change scenarios knmi 2015 with two socio economic scenarios manders and kool 2015 resulting in four different storylines for climate change and the response of the human system for different socio economic configurations land use irrigation and drinking water demand are examples of scenario specific human responses additionally one scenario variant was developed to explore the effect on water demand of implementing drainage systems in peat areas to reduce co2 emissions according to the paris agreement all scenarios were translated into consistent model input and boundary conditions by hunink et al 2018 a total of 11 model experiments 1 x reference 5 scenarios for 2050 5 scenarios for 2100 were thus carried out to understand the drought risk system and explore drought risks in the future the meta model qwast has been used to quickly explore a large set of policy actions third goal the choice for a separate quick scan tool was made because at least currently the nwm is not fast enough to support the iterative process of moving from a long list of potential measures to a short list of promising measures about 60 model experiments were conducted with qwast 20 individual measures and 10 strategies combinations of measures were simulated for 2 scenarios encompassing a 100 year times series each 100 year qwast run takes a few hours to simulate nwm was finally used again to compare priority strategies in order to choose the preferred strategy fourth goal however because of the simulation time and cost constraints both following from model complexity only 4 scenario strategy combinations were explored 2 strategies for 2 scenarios in 2050 each for the 100 year time series the subsequent interviews with policy advisors revealed that this collection was not sufficient to address all relevant policy questions for example the relative impact of drinking water and agricultural water extraction on declining groundwater levels under future conditions remained unresolved 3 2 model complexity in terms of model complexity it is generally accepted that for the purpose of decision support and exploration of future developments spatially and temporally less detailed models are required than for instance operational models to forecast system behaviour in response to weather predictions fig 4 displays a conceptual overview of model applications with respect to three model dimensions the required conceptual detail of the processes that are modelled model complexity the level of acceptable model uncertainty and the time range of the analysis three types of model application can be identified 1 prediction estimating the value of a system variable given a change in system inputs or boundary conditions 2 forecasting predicting the value of system variables in the near future on the basis of varying system forcings and 3 exploration estimating model variables in the future given changes in a combination of model inputs parameters and boundary conditions while for prediction and forecasting historical accuracy is an important characteristic of model approaches exploring long term future 30 100 years ahead requires model outcomes to be plausible given assumed conditions see kelly et al 2013 many scenarios are needed to explore the interactions between processes and impacts of interrelated changing conditions including feedbacks and how this can be adapted by policy actions for this type of application models used should be fast enough to allow large numbers of calculations for long time series while keeping sufficient detail to represent all relevant processes and their interactions haasnoot 2013 booij et al 2003 a comparison of models with different levels of complexity is needed to determine the configuration that would be sufficient for answering policy questions guillaume and jakeman 2012 determination of an optimal model configuration is difficult when the model will be used for different applications simultaneously in a so called unified modelling concept see e g clark et al 2015 also the lhm is used for purposes other than long term assessments e g as a predictive model for regional hydrological studies as a component in the national operational drought management system and for simulating and forecasting the real time drought situation and expected water shortage in the short term these multiple purposes imply that choices in temporal and spatial resolution require a trade off between high accuracy versus high computational efficiency interestingly many particularly regional policy advisors consider the current version of lhm not detailed enough for regional groundwater studies since the multi purpose model lhm is a major component of nwm the nwm is placed in the middle of the vertical axis in fig 4 reflecting a compromise between exploring the future and forecasting the steering committee considered it necessary that nwm contains groundwater and surface water interaction because 1 large parts of the subsurface of the netherlands consists of highly permeable river sediments causing the surface water and groundwater systems to act as one system winter et al 1999 with mutual feedback mechanisms especially when simulating dry conditions and 2 the netherlands partly consists of large areas with deeper groundwater levels that react slowly 20 years to changes in hydrological boundary conditions causing adjacent connected areas to react also slower than other areas and 3 the central veluwe area with deep groundwater levels directly connects with the lake ijssel via the deeper subsurface which is relevant when performing simulations for long time periods gehrels 1999 3 3 relevance of the model outcomes the required level of detail in an integrated assessment model directly follows from its purpose in the policy process in this case to understand the system explore scenarios and assess strategies to deal with potential future problems see fig 3 relevant system processes must be included at the right temporal and spatial scale and their response to changing climate and land use conditions need to be assessed as well for the exploration of drought risk and mitigation strategies the nwm should provide a realistic representation of natural variability and extremes of main drivers and internal dynamics of droughts which determines the frequency and intensity of major drought impacts van loon 2015 this in turn gives guidance to the policies addressing the balance between water demand and supply under current and potential future climate and socio economic conditions the relevant performance indicators for drought risk assessment in the netherlands include frequency and severity of soil moisture deficit which may change due to climate change land use change and water management actions average summer groundwater levels which may change due to the combination of climate change and extractions for drinking water production industrial use and irrigation frequency level and duration of salt water intrusion which may change due to the combination of changing river flows and sea level rise and affecting fresh water inlets level and frequency of water shortage from the main rivers canals and lakes which may change due to a combination of temperature change change in river flows land use change and farmers response to climate change duration and frequency of low water depths along the main waterways that impact inland shipping this set of variables reflect the many physical processes related to drought furthermore the computational framework allows long time series to represent climate variability many combinations of measures and scenarios can be simulated and output is easily connected to economic impact models the model outcome is thus considered relevant for the decision making process 3 4 timeliness to be useful in the decision making process model results should be published at a convenient and opportune time hamilton et al 2019 timeliness is not only related to the net simulation time computation time of the model but also to the time it takes to prepare model inputs and schematisations e g derived from scenarios and proposed policy actions and the analysis of the outcomes complex models are more difficult to schematize and interpret thereby increasing the duration of the modelling exercise which poses a risk for the timeliness of the outcomes available budget to carry out the computations clearly play a role as well which may limit the efficiency of the simulations for example the more model runs that are carried out in parallel the higher the use cost of processing units over the past decade the simulation time of nwm has significantly increased due to the addition of submodels and the simulation of long time series despite developments in computation architecture that allow parallelization of model runs see section 3 2 this increased the simulation time of the full model train from a few weeks to 2 3 months for one 100 year run making it increasingly challenging to synchronize with the policy making process and significantly increasing the project costs in practice the deadlines of the delta programme policy making process are met by limiting the number of nwm runs and the use of qwast to explore potential policy actions this shows that the nwm s ability to simulate a sufficient number of scenario strategy combinations to answer relevant policy questions is limited 3 5 stakeholder acceptance to increase acceptance of model outcomes the decisions that are supported by these outcomes and commitment to its implementation stakeholders must be engaged in the model design process hamilton et al 2019 voinov and bousquet 2010 furthermore model outputs must be scientifically justified and developed without a bias towards a desirable outcome or interpretation hamilton et al 2019 cash et al 2003 a range of techniques is available to evaluate the scientific performance of an environmental model see bennett et al 2013 including quantitative comparison with observed data model validation since model behaviour under future conditions cannot be directly derived from model validation additional qualitative evaluation methods are often used based on e g theoretical reasoning extrapolation to future conditions or finding analogues in different locations or time ranges jakeman et al 2006 furthermore trust in model outcomes is also gained by discussions on their plausibility with stakeholders with high expertise in the functioning of the system they manage in practice quantitative and qualitative model validation is not carried out as part of the nwm project but is carried out as part of the projects that maintain and develop the submodels although the submodel lhm has been significantly improved over the past 10 years by including more processes and updating underlying data full model validation is not applied routinely but undertaken in irregular dedicated projects such a project is running in 2021 the former validation took place in 2013 similarly the submodel lsm has been improved by merging several regional models but has hardly been validated with observed low flow data besides model validation other activities have contributed to the trust in model outcomes stakeholders in the lhm project not only advise about model developments but also share local and regional data about their water system for example the groundwater model of lhm is based on upscaled data from the underlying regional high resolution 25 25 m groundwater models e g hoogewoud et al 2013 acceptance of the results of national analysis by regional stakeholders is promoted by the ability to compare results with their own regional model outcomes also the periodic discussion of model outcomes with an independent expert group facilitates this acceptance such discussions were facilitated in the previous delta programme policy cycle as well as in the pawn study during which the model based analysis was reviewed by a group of independent model experts from national and regional authorities goeller et al 1985 summarizing because the submodels are developed and validated in a continuous process in close interaction with regional stakeholders the nwm is accepted as state of the art 4 the main nwm dilemma over the past 10 years the national water model nwm has developed into a well documented and structured integrated model instrument with interconnected models that are accepted by stakeholders its outcomes are used in the drought risk management process because it describes all relevant drought related physical processes it allows long time series to represent climate variability and it connects to economic impact models its computational framework fews allows transparency and reproducibility of model simulations because continuous investments in development and validation of the submodels occur in a parallel process and in close interaction with regional water managers and model experts these submodels are considered state of the art for national scale analyses the nwm thus meets many of the key requirements however the multi purpose design of some components particularly the lhm submodel inevitably lead to trade offs in the configuration that is not fully optimized for its specific use as integrated assessment model for national policy making the subsequent heavy computational burden of the nwm limits its usefulness for the national delta programme because the number of scenario strategy combinations that can be afforded to be simulated in time for the policy process is too limited this demonstrates the main dilemma an assemblage approach limits development costs and increases stakeholder acceptance but it also increases the model complexity and computation time compromising timeliness in and knowledge for the policy making process the nwm case shows that policy analysts come up with practical solutions to overcome the dilemma for example by limiting the number of nwm simulations which increased timeliness but reduced the model outcome relevance because not all policy questions were answered another example is that the length of the time series for some of the scenario runs with nwm was reduced from 100 to 30 years at the expense of a reduced insight into natural variability and extreme drought events another choice that was made to overcome the dilemma was to develop a quick scan model qwast for water demands and water allocation the qwast served the purpose of quickly providing insight into the performance of several combinations of policy options under a range of scenarios however its relevance to the policy process is still limited because several relevant processes are left out and it heavily depends on the nwm for input on climate change and land use scenarios also a systematic comparison between the performance characteristics of qwast and nwm is not executed limiting the consistency between the two models clearly the choice of nwm as an integrated assessment model was well justified for its status as state of the art repository of accepted submodels but it is currently not fit for purpose to explore the desired scenario uncertainty range and the range of policy options in a timely manner its computational costs are too large and its complexity requires considerable time and effort to translate storylines about future development policy actions and model uncertainty into model schematizations and boundary conditions and to analyse the model output to provide valuable model outcomes for future development and application of the nwm the following is recommended 1 increase the flexibility of the nwm modelling framework and the submodel software allowing to switch between spatial scales resolution and degree of conceptual complexity for example in regions where groundwater is less important the lhm model could be used to simulate surface water allocation with a semi static groundwater boundary condition flexibility may also be introduced by adding the ability to alternate between the national scale and a regional scale using multi resolution modelling concepts davis and bigelow 1998 rabelo et al 2015 specific to the nwm case an upgrade to modflow6 langevin et al 2017 would allow for more flexibility in groundwater modelling facilitating detailed analysis for a certain region while maintaining connection and consistency with the rest of the model domain 2 derive and periodically update a meta model version of nwm that includes all relevant processes in a less detailed way than currently done in the nwm submodels a meta model or fast and simple model mimics the behaviour of complex models by using simplified cause effect relationships davis and bigelow 1998 van grol et al 2006 walker and van daalen 2013 haasnoot et al 2014 such a meta model can be used to explore many combinations of scenarios and policy options and to assess adaptation strategies the more detailed nwm and or its submodels may be used subsequently to perform in depth analyses for a certain region sector and or measure to ensure the acceptance and uptake of meta model outcomes by all stakeholders a standardized procedure is required to preserve consistency between the simple meta model and the institutionalized nwm furthermore stakeholders must be engaged in the model development process hamilton et al 2019 van delden et al 2011 voinov and bousquet 2010 in view of the varying model requirements in terms of spatial and temporal resolution throughout the policy process the future nwm should not be considered a single integrated model but instead a collection of detailed connected submodels and a less detailed meta model depending on the policy phase and resulting requirements many scenario strategy combinations can be explored with the meta model or more detailed insights can be obtained with submodels for specific regions processes and or measures see also haasnoot et al 2014 to move forward with the nwm project it is recommended to organize a continuous conversation about fit for purpose between policy advisors model experts and stakeholders the literature provides useful frameworks to structure and formalize such a conversation bennett et al 2013 guillaume and jakeman 2012 furthermore a comparison of models with different levels of complexity is needed to determine the configuration that would be sufficient for answering upcoming drought risk management policy questions 5 conclusion the netherlands national water model nwm is an integrated assessment model that allows exploring and assessing various adaptation measures long term strategies and future scenarios on a national scale in support of policy making on drought risk management the model s outcomes are considered relevant and are accepted by stakeholders but its simulation time is considered too long to respond quickly to policy questions also its complexity requires considerable effort in schematization and analysing the model simulations which reduces timeliness and increases project costs the nwm case has illustrated a clear dilemma that occurs when submodels are used as a basis for developing an integrated assessment model in support of policy making such an assemblage approach limits development costs and increases stakeholder acceptance but also implies trade offs when multi purpose submodels are used this poses a risk of increased model complexity and computation time compromising timeliness in and knowledge for the policy making process to deal with this dilemma two recommendations were made 1 increase the flexibility of the nwm modelling framework and the submodel software allowing to switch between spatial scales resolution and degree of conceptual complexity this is known as multi resolution modelling and 2 derive and periodically update from this institutionalized complex model a meta model i e a fast simple model fsm that includes all relevant processes to quickly explore many scenario strategy combinations the insights from the nwm case can be valuable for others that are involved with developing and maintaining integrated assessment models in support of long term policy making for water resources management the fit for purpose conversation between policy advisors and model developers deserves formalization and should be continuous in view of changing requirements and ongoing submodel developments when integrated assessment is supported by a meta model a standardized procedure is required to assure consistency between the meta model and the institutionalized complex model declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we greatly acknowledge the following people who contributed to this paper by sharing their valuable experience as policy advisor in the first and or second phase of the delta program or as expert in the field of integrated assessment modelling bas de jong eelco van beek erik ruijgh jeroen ligtenberg judith ter maat marjolijn haasnoot mark bruinsma neeltje kielen sharon muurling van geffen timo kroon and wim de lange we also thank the three anonymous reviewers for their valuable feedback on previous versions of this paper the research was partly funded by the dutch ministry of infrastructure and water management 
