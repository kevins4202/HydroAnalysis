index,text
165,underground hydrogen storage uhs has been launched as a catalyst to the low carbon energy transitions the limited understanding of the subsurface processes is a major obstacle for rapid and widespread uhs implementation we use microfluidics to experimentally describe pore scale multiphase hydrogen flow in an aquifer storage scenario in a series of drainage imbibition experiments we report the effect of capillary number on hydrogen saturations displacement trapping mechanisms dissolution kinetics and contact angle hysteresis we find that the hydrogen saturation after injection drainage increases with increasing capillary number during hydrogen withdrawal imbibition two distinct mechanisms control the displacement and residual trapping i1 and i2 imbibition mechanisms respectively local hydrogen dissolution kinetics show dependency on injection rate and hydrogen cluster size dissolved global hydrogen concentration corresponds up to 28 of reported hydrogen solubility indicating pore scale non equilibrium dissolution contact angles show hysteresis and vary between 17 and 56 our results provide key uhs experimental data to improve understanding of hydrogen multiphase flow behaviour graphical abstract image graphical abstract keywords underground hydrogen storage microfluidics residual trapping dissolution rates contact angle hysteresis 1 introduction as a no carbon energy carrier hydrogen may play a significant role in the energy transition needed to reach net zero societies hydrogen implementation in transport heating and power generation will require large scale seasonal storage and underground hydrogen storage uhs in aquifers has been proposed as one option carden and paterson 1979 lord et al 2014 technical aspects of uhs are similar to natural gas storge ugs where gas is injected in the subsurface cushion gas and is then withdrawn at peak demand working gas although knowledge transfer from ugs is possible high hydrogen mobility and its potential biogeochemical activity panfilov 2010 calls for caution and revision of conventional storage practices experience with commercial underground storage of pure hydrogen is limited to salt caverns ozarslan 2012 underground aquifers have been used for town gas storage only with hydrogen content up to 50 60 smigan et al 1990 panfilov 2016 hydrogen injection and withdrawal in underground porous formations involve complex displacement and trapping mechanisms controlled by hydrogen flow properties and interactions with reservoir fluids and rocks the understanding of hydrogen flow physics and trapping in porous media is therefore essential to establishing reliable storage models for lab scale tests feasibility studies and piloting most porous media research on hydrogen is mainly focused on biogeochemical interactions berta et al 2018 flesch et al 2018 bo et al 2021 but there are fewer fundamental studies reporting multiphase flow data with the dominance of the numerical modelling approaches lubon and tarkowski 2021 lysyy et al 2021 mahdi kanaani 2022 most numerical studies use extrapolated flow functions not specifically measured for hydrogen thus experimental efforts are needed to improve the hydrogen flow modelling a single reported experimental core scale study found that hydrogen water relative permeability is independent of pressure and temperature conditions yekta et al 2018 a major concern is that the displacement is prone to front instabilities and viscous fingering due to an unfavourable hydrogen water mobility ratio microscopic viscous fingers were confirmed with laboratory models paterson 1983 in addition hydrogen withdrawal will be associated with loss caused by residual and dissolution trapping unlike co2 sequestration residual and dissolution trapping are not desirable in uhs as it leads to unrecoverable hydrogen thus representing a permanent loss carden and paterson 1979 uhs involves many injection withdrawal cycles and the residually trapped hydrogen may reconnect during subsequent hydrogen injections known as hysteresis microfluidic experiments are perfectly suited for visualization of porous media hydrogen flow thereby providing direct evidence of the proposed displacement and trapping mechanisms to corroborate core scale measurements due to the 2d nature of the micromodels and their limited volume microfluidic experiments should mainly focus on the qualitative rather than quantitative results extrapolation of quantitative 2d data to 3d natural environment should be done with caution best achieved through pore scale modelling in particular hydrogen contact angle measurements assist the pore scale models in estimating upscaled relative permeability and capillary pressure functions which can be used as input for numerical studies at field scale hashemi et al 2021 classical pore scale displacement theory defines four displacement mechanisms which may result in residual trapping piston like snap off i1 imbibition and i2 imbibition lenormand et al 1983 dissolution trapping occurs when the residually trapped phase dissolves in water controlled by the trapped phase diffusivity and solubility hydrogen solubility studies relevant for uhs demonstrated inconsistencies due to missing experimental support and or different measurement approaches de lucia pilz et al 2015 li et al 2018 lopez lazaro et al 2019 chabab et al 2020 contact angle measurements are commonly used in multiphase transport research to understand the effects of wettability and capillary pressure and relative permeability hysteresis on fluid systems the hydrogen water system is still not adequately investigated and lack consistent and systematic approaches however hydrogen contact angles have been derived for basalt al yaseri and jha 2021 and measured for quartz iglauer et al 2021 and sandstone hashemi et al 2021 rocks and the results showed discrepancies in terms of pressure temperature and salinity effects overall pore scale displacement and trapping mechanisms are well described for co2 sequestration buchgraber et al 2012 cao et al 2016 chang et al 2016 hu et al 2017 chang et al 2020 but remains to our knowledge unaddressed for hydrogen our work examines hydrogen flow behaviour in an initially water filled micromodel relevant for uhs in aquifers we perform a series of injection drainage and withdrawal imbibition experiments to qualitatively describe pore scale hydrogen displacement and trapping mechanisms with image analysis we quantify hydrogen dissolution kinetics and measure contact angles this study is relevant for readers seeking to understanding of hydrogen flow physics in porous media and adds new data to experimental dataset 2 materials and methods 2 1 micromodel all drainage and imbibition experiments were conducted in a silicon wafer micromodel based on natural sandstone pore patterns with a large variation in grain and pore sizes and shapes extracted from the scanning electron microscope image of a representative sandstone thin section and slightly modified to enable flow the 2d pore network was etched into silicon wafer with deep reactive ion etching drie hornbrook et al 1991 buchgraber et al 2012 the drie realistically reproduces topological features such as high pore body to pore throat ratio coordination number 4 8 sharp pore walls and surface roughness 100 nm the exact reproduction of pore and pore throat sizes generate capillary forces at the magnitude relevant for real porous rocks the heterogeneous mineralogy is however not reproduced i e no clay and or calcite minerals present the silicon dioxide layer on the micromodel surfaces prevents hydrogen adsorption four ports etched through the micromodel bottom facilitate external access to the porous network whereas two high permeable fracture channels between the ports allow to easily distribute the injected fluids the micromodel bottom silicon wafer and top borosilicate glass surfaces were connected through anodic bonding resulting in the hydrophilic pore network the micromodel hydrophilic nature allowed us to distinguish between the injected fluids under microscopic view fig 2 hydrogen light blue develops a convex curvature towards water blue and the grains the etched porous network has the length x width x depth dimensions of 2 8 cm x 2 2 cm x 0 0030 cm respectively and porosity of 60 yielding the pore volume of 11 µl the average pore diameter is in the order of 100 µm with the grain size and pore throat length distributions of 100 7900 µm2 and 10 200 µm respectively alcorn et al 2020 the pore network extraction tends to increase the total micromodel porosity compared with the representative rock and the micromodel porosities up to 46 55 can be found in the literature buchgraber et al 2012 chang et al 2019 wu et al 2021 our pore network was based on the thin rock section containing both small and large pore clusters the pore network was repeated 36 4 9 times and stitched together on the micromodel surface resulting in relatively high total porosity of 60 note however that our microscope provided the observations of the micromodel field of view fov only which is approximately 1 of the whole micromodel area the local fov porosity is in the order of 30 which is closer to the natural rocks 2 2 experimental set up and procedure the micromodel was mounted in the peek holder and connected to two quizix pumps through 1 16 peek and stainless steel tubing fig 1 a quizix sp 5200 pump cylinder c5000 10k ss at was filled with pure hydrogen 99 999 whereas quizix qx pump contained distilled water the micromodel was illuminated by a light source photonic led f1 cold light 5500 k a microscope nikon smz1500 connected to a camera nikon d7100 and computer provided direct real time observations of the fov experimental data was acquired through live view video recordings with the frame rate 29 97 fps and the resolution of 0 5 pixels µm prior to every run consisting of hydrogen and water injections the pore space was initially 100 saturated with distilled water every experiment consisted of one hydrogen injection drainage and one water injection imbibition from two opposite inlets creating a diagonal flow through the pore network all injections used a pore pressure of p 5 bar and room temperature constant pressure was maintained with a hydrogen filled pump whereas a water filled pump performed water withdrawal injection at constant flow rates hydrogen injections drainage were initiated by water withdrawal and lasted until between 50 and 500 water pore volumes pv were withdrawn after hydrogen invasion enabling quasi steady state subsequently water injection imbibition started with the same flow rate and the injection was maintained until hydrogen was completely dissolved thereby running a single cycle of hydrogen injection withdrawal only the micromodel was then cleaned with distilled water to remove any residual hydrogen and to re saturate the pore space with 100 distilled water making the system ready for the next experiment four different flow rates were applied 0 1 1 10 and 50 ml h with corresponding capillary numbers nca calculated from the equation nca u µ σ where u is flow velocity m s µ is the wetting phase water viscosity pa s and σ is the interfacial tension n m flow velocity u was calculated as u q l d ф with q rate m3 s l near inlet length inside the micromodel m d porous network depth m ф porosity fraction hydrogen interfacial tension σ at experimental pressure was calculated to 0 072 n m based on the empirical formulation massoudi and king 1974 the calculated capillary numbers allowed to locate our experiments on the log nca log m stability diagram fig 1b where m is defined as the hydrogen water viscosity ratio 2 3 relevance of experimental conditions our experiments were run under low pressure and with distilled water the reservoir brine salinity may affect the gas surface properties the gas brine interfacial tension duchateau and broseta 2012 and contact angles jafari and jung 2019 increase with increasing salinity these correlations are yet to be confirmed for hydrogen brine systems and should be addressed in future studies low pressure was selected in our work due to the safety risks associated with hydrogen flammability and equipment compatibility under high pressure hydrogen properties affecting 2d porous media multiphase flow viscosity and h2 h2o interfacial tension do not vary significantly with increasing storage depths increasing pressure and temperature unlike many other gases like co2 n2 and ch2 beckingham and winningham 2020 al 2022 pressure dependant variations in hydrogen density play an important role in gravity dominated 3d problems and thus are assumed to be insignificant in our 2d microfluidic study moreover one of the hydrogen storage projects was operated at pressures down to 5 10 bars in the argentinian depleted gas field pérez et al 2016 our low pressure study is therefore relevant for the real storage conditions 2 4 image analysis experimental data was quantified with image analysis in the open source imagej software hydrogen saturations were estimated based on colour thresholding permitting to calculate local fov porosity and distinguish hydrogen from water dissolution data was obtained by measuring the areal decrease of the hydrogen phase with time contact angles were measured using an angle tool static contact angles were measured when the hydrogen water interface did not move whereas paused videos allowed to measure dynamic contact angles when the hydrogen water interface moved during water withdrawal injection receding contact angles were measured when hydrogen displaced water drainage advancing contact angles were measured when water displaced hydrogen imbibition note that the image analysis is dependant on the image resolution segmentation and user adjustments the image resolution was high enough to distinguish between hydrogen and water in some cases the light source limitations resulting in the image gradients required pre processing of the images with the manual segmentation of fluids 3 results and discussion 3 1 hydrogen saturation establishment during drainage hydrogen invaded the pores immediately after entering the fov indicating that non wetting phase invasion occurred on millisecond scale independent of capillary number nca this is consistent with previous micromodel studies in oil air water systems mohanty et al 1987 moebius and or 2012 armstrong and berg 2013 rapid hydrogen pore invasion serves as indirect evidence of hydrogen non wetting nature andrew et al 2015 which was directly confirmed by contact angle measurements detailed in section 3 4 3 1 1 initial hydrogen saturation the fov hydrogen saturation sg after drainage increased with increasing nca fig 2 as expected from classical pore scale displacement theory lenormand et al 1983 hydrogen invasion into neighbouring pore clusters was restricted by narrow pore throats with higher capillary entry pressures pores invaded by hydrogen were predominantly saturated with hydrogen with some visible water accumulations droplet forms on the pore bottom due to surface roughness white arrows in fig 2 number of water droplets were largest at upper medium and high nca whereas they were absent at low nca at the end of drainage at the hydrogen breakthrough however water droplets formed even at low nca fig 3 with continued hydrogen injection the droplets were displaced likely due to 1 the hydrophilic micromodel surfaces with high water phase connectivity through connected wetting films and or 2 water evaporation in hydrogen the water droplet displacement evaporation demonstrated the pore scale efficiency of low nca drainage when multiple pore occupancies establish in hydrophilic systems from this we could expect that hydrogen injection strategies that result in pore occupancy by hydrogen phase only no pore water will decrease water cut upon hydrogen withdrawal in aquifer storage improving overall storage performance low nca hydrogen injection may therefore be preferred from the perspective of the pore occupancies 3 1 2 hydrogen connectivity the non wetting phase connectivity defines if the phase is connected through the pore clusters the hydrogen connectivity was high at low and medium nca with observations of several connected gas paths fig 2a c at high nca both connected and disconnected hydrogen phases established fig 2d with disconnected hydrogen phase accounting for 11 of the total fov hydrogen saturation the predominant mechanism for disconnected hydrogen was roof snap off roof 1970 due to the front interface destabilization after entering the neighbouring pore to maintain capillary equilibrium water thickens in the pore throat leading to the non wetting phase hydrogen in our case disconnection for roof snap off to occur high water availability is required near the pore throat and the non wetting phase must pass the pore throat for a distance of at least seven times the throat radius snap off controlled by capillary pressure is expected in smaller pore throats in our work roof snap off occurred in small 15 µm and large 25 µm pore throats without occurring in neighbouring pore throats of similar sizes fig 4 this implies that roof snap off was a local phenomenon likely controlled by water mobility and availability as suggested for drainage snap off in a co2 brine system andrew et al 2015 the pore throat water thickening which was believed to cause hydrogen snap off was not possible to confirm visually because of sub second snap off and insufficient microscope resolution to detect thin wetting films before snap off occurred in small and large pores fig 4 the distance propagated by hydrogen corresponded to 35 and 15 times of the pore throat radius respectively fulfilling the condition for roof snap off snap off during drainage is less common than in imbibition and is still not appropriately investigated in the seminal work of roof 1970 the criteria for drainage snap off were linked to local conditions water availability pore throat and interface size wettability the drainage snap off dependency on global dynamic conditions viscosity ratio compressibility capillary number were proposed deng et al 2015 herring et al 2018 our results suggest that drainage snap off was triggered by both local and global factors the snap off independence on pore throat sizes fig 4 showed local features of snap off events whereas the snap off occurrence at high nca only suggests that snap off drainage was correlated to global dynamic parameter nca snap off during and after drainage is undesired in seasonal hydrogen storage as this may lead to permanent hydrogen entrapment in our work most of disconnected hydrogen bubbles fig 2d did not reconnect during imbibition resulting in a complete hydrogen dissolution although the highest hydrogen saturation fig 2 was achieved at high nca 3 84 10 4 high injection rates will not necessarily yield the maximum injection efficiency in aquifer storage projects considering the possibility for snap off 3 1 3 flow regime according to the lognc a logm phase diagram fig 1b unstable viscous dominated displacement was expected to prevail over capillary dominated flow the lognc a logm phase diagram boundaries are however system dependant zhang et al 2011 and are not necessarily applicable for our micromodel neither viscous nor capillary fingering were possible to observe due to the limited fov nevertheless some indirect evidence of viscous and capillary flow regimes was observed locally the establishment of the connected hydrogen phase stopped by narrow pore throats may show the importance of capillary fingering at low medium nca roof snap off caused by hydrogen penetration through narrow pore throats indicate the dominance of viscous forces at high nca zhang et al 2011 micromodel studies enabling to observe the entire micromodel at a wider nca range will be beneficial for a direct determination of the dominating flow regime in hydrogen water systems 3 2 displacement and residual trapping during imbibition hydrogen displacement and disconnection residual trapping was observed during imbibition that started with water injection into the same fluid system which established after drainage imbibition proceeded in three main steps common for all nca 1 displacement 2 disconnection 3 dissolution an additional step between steps 1 and 2 was observed at upper medium and high nca hydrogen redistribution caused by fluid displacement from outside the fov this section describes the first two steps displacement and disconnection in addition to redistribution dissolution will be described in section 3 3 3 2 1 displacement mechanisms hydrogen displacement was mainly governed by i1 imbibition mechanism fig 5 a initially occupying three pores hydrogen was forced into a single pore because of the curvature instability resulting from the curvature detachment from the pore walls two other displacement mechanisms were common at specific nca piston like displacement and redistribution piston like displacement was observed at low nca where a stable displacement front moved through a single pore channel fig 5b the pore channel was surrounded by narrow pore throats forcing water to displace hydrogen from one direction only at upper medium and high nca hydrogen redistribution occurred fig 5c where the original hydrogen phase was first displaced and trapped followed by a partial reconnection with surrounding hydrogen this mechanism occurred because of high hydrogen saturation after upper medium high nca drainage permitting hydrogen movement through the entire micromodel during imbibition note that most of the hydrogen bubbles disconnected due to roof snap off during drainage remained disconnected during redistribution only a single hydrogen bubble in the fov reconnected with the continuous hydrogen phase the inability to reconnect resulted in a complete hydrogen bubble dissolution demonstrating the disadvantages of the drainage snap off 3 2 2 residual trapping hydrogen disconnection leading to residual trapping occurred mainly by i2 imbibition mechanism fig 6 a displacement from the pore centre towards the pore wall resulted in hydrogen disconnection when the hydrogen water interface reached the pore wall the disconnected hydrogen occupied two pores trapping by bypass was observed at upper medium and high nca fig 6b the water flow paths did not manage to invade the large hydrogen saturated pore clusters with narrow pore throats resulting in a significant hydrogen fraction being bypassed the dominance of the i2 mechanism over bypass was likely due to topological reasons high coordination number permitting the transverse to hydrogen water flow paths bypass is expected in large pore clusters with narrow pore throats chatzis et al 1983 consistent with our observations however at low and lower medium nca hydrogen did not occupy the large pore clusters fig 2a b where bypass was observed at upper medium and high nca for a more general conclusion on the relative importance of i2 and bypass mechanisms hydrogen must occupy the same pore clusters in all experiments which is challenging to control in the heterogeneous pore space with the micromodels used in this study trapping by snap off was not identified despite high micromodel aspect ratio and roughness likely due to experimental conditions snap off is expected to dominate at nca 10 7 hu et al 2017 whereas our experiments were conducted at nca 7 68 10 7 the fov hydrogen saturation profiles were estimated for imbibition to construct the imbibition cdc and gas trapping curves fig s2 and fig s1 in the supplementary materials respectively 3 3 dissolution 3 3 1 dissolution mechanisms dissolution of disconnected and trapped hydrogen was observed during prolonged water injection three dissolution mechanisms were identified fig 7 one end dissolution two end dissolution and displacement dissolution the one end dissolution fig 7a was frequently observed at upper medium and high nca where hydrogen bubbles dissolved from one end only reflecting the water flow direction the rapidly developed waterfront propagating through the micromodel in one main direction was not able to enter narrow pore throats counter currently against the main flow direction hence dissolution initiated only from one end of the trapped hydrogen bubble residing in the pore corners surrounded by narrow pore throats the one end dissolution was also observed in supercritical co2 dissolution in micromodel chang et al 2016 the two end dissolution mechanism fig 7b prevailed at lower medium nca where the hydrogen bubbles were dissolved at both sides simultaneously this mechanism was attributed to a more stable waterfront and greater water availability originating from lower hydrogen saturation developed after drainage the displacement dissolution mechanism fig 7c was characterized by mobilization of smaller hydrogen bubbles that were able to penetrate narrow pore throats this mechanism was observed at upper medium and high nca due to faster and more directed water flow overall observed dissolution mechanisms suggest that hydrogen dissolution was governed by the waterfront velocity and direction which in turn was controlled by nca two dissolution processes were detected independent of nca homogeneous and heterogeneous dissolution they differed in terms of the microbubble final state at the end of dissolution in homogenous dissolution microbubbles dissolved completely whereas the residual microbubbles accumulated at the surface roughness in heterogeneous dissolution homogenous heterogeneous dissolution as well as displacement dissolution fig 7c were also reported for co2 dissolution in micromodel buchgraber et al 2012 3 3 2 dissolution kinetics local and global dissolution kinetics were estimated based on the image analysis local dissolution kinetics was quantified by calculating the temporal change in the individual hydrogen bubble size and the depletion rate fig 8 as expected time required for compete dissolution decreased with increasing nca because of the faster water supply fig 8a the total dissolution time in every experiment was nearly equal and independent of the initial bubble area implying simultaneous dissolution in the entire fov the depletion rate fig 8b was calculated as the depleted hydrogen mass per time interval between two sequential images qd a d ρh2 t where a decrease in individual hydrogen bubble area between two sequential images d porous network depth ρh2 hydrogen density under experimental conditions 4 12 10 4 g ml t time interval between two sequential images the depletion rate changed with time shifting from nearly constant values to increasing or decreasing trends similar discrepancies in depletion rate trends were also observed in pore scale supercritical co2 dissolution explained by the number of water flow paths their direction transverse or longitudinal and the co2 water interface area chang et al 2016 the average hydrogen depletion rate ranged from 2 3 10 12 to 22 10 12 g sec with the lowest rate observed at lower medium nca expected and the highest rate observed at upper medium nca unexpected high nca did not yield the highest depletion rate due to the smaller initial bubble size compared with upper medium nca cases when comparing bubbles of similar size high nca depletion rate was higher than upper medium nca table 2 global dissolution kinetics was analysed based on the fov hydrogen saturation profiles during dissolution fig 9 the global depletion rate fig 9b was calculated as follows qd global sg vp ρh2 t where sg decrease in the fov hydrogen saturation between two sequential images vp micromodel pore volume ρh2 hydrogen density under experimental conditions t time interval between two sequential images the global depletion rate calculations assume that fov hydrogen saturation profiles are representative for the entire micromodel the global depletion rates showed non constant trends and on average varied between 3 6 10 10 to 277 10 10 g sec two orders of magnitude higher compared with local depletion rate of individual bubbles similar to observations of co2 dissolution chang et al 2016 the global depletion rate was the lowest at lower medium nca and highest at high nca table 2 3 3 2 1 dissolved hydrogen concentration and solubility the averaged dissolved hydrogen concentration table 2 was calculated as the dissolved hydrogen amount per injected water mass between two sequential images according to the formula c qd mh2 q ρh2o where qd hydrogen depletion rate mh2 hydrogen molar mass q water injection rate ρh2o water density these calculations were based on the mass balance principle assuming that hydrogen depletion is solely controlled by dissolution and water advection chang et al 2016 chang et al 2019 hydrogen depletion will be controlled by diffusion when the water phase is immobile which is not the case in our work where water is continuously injected during imbibition in local dissolution the dissolved individual hydrogen bubble concentration ranged between 0 4 10 6 and 5 9 10 6 mol kg in global dissolution the dissolved hydrogen concentration varied from 6 4 10 4 to 11 10 4 mol kg corresponding to 16 0 and 28 3 of the hydrogen solubility under the applied experimental conditions chabab et al 2020 lower than solubility hydrogen concentrations indicate non equilibrium hydrogen dissolution in our work conflicting with classic equilibrium dissolution theories applied in numerical modelling pruess and spycher 2007 non equilibrium slow dissolution has also been reported for co2 both in experimental core scale akbarabadi and piri 2013 chang et al 2013 and pore scale chang et al 2016 chang et al 2019 studies as well as numerical studies chen et al 2018 for instance co2 dissolution measurements in micromodels showed that the average co2 concentration varied between 0 25 13 of co2 solubility chang et al 2016 chang et al 2019 they explained non equilibrium dissolution by insufficient co2 water interface area and non uniform co2 mobile water distribution they argued that at reservoir scale where dissolution occurs at slower timescales co2 dissolution after the injection stop will approach equilibrium unless strong heterogeneity is present in hydrogen aquifer storage hydrogen is cyclically injected and withdrawn at high rates and we therefore speculate that non equilibrium dissolution may play an important role 3 4 contact angles 3 4 1 static and dynamic contact angles static θs and dynamic θd contact angles were measured during drainage and imbibition each measurement was performed five times at the same measuring point and then averaged with the uncertainty represented as standard deviation the measured contact angles fig 10 varied from 17 to 56 similar to contact angles of 22 45 for hydrogen water sandstone systems yekta et al 2018 hashemi et al 2021 our results confirmed that the micromodel is hydrophilic when exposed to hydrogen no clear relationship between contact angles and pore diameter emerged although the contact angle range appeared to narrow with increasing pore diameter majority of measurements performed in pores with diameter between 50 and 125 µm four contact angle types receding θr advancing θa static in drainage θs dr and static in imbibition θs im were averaged for each experiment table 3 and plotted as a function of nca fig 11 a the θa were significantly higher than the θr consistent with classic theories johnson and dettre 1964 as expected θa θs im but θr and θs dr were surprisingly similar θr θs dr the similarity between θr and θs dr could be linked to the experimental procedure after hydrogen breakthrough under drainage hydrogen injection continued through the connected hydrogen phase and the θs dr were measured when the interface movement terminated visually in this state despite being motionless the interfaces did not reach the equilibrium due to continuous hydrogen injection the measured θs dr approached more dynamic than static states resulting in θr θs dr and were thus believed to be underestimated and less reproducible on the other hand the water breakthrough under imbibition resulted in hydrogen residual trapping with several hydrogen clusters being bypassed by water in these regions the interface was believed to be surrounded by immobile water thus approaching equilibrium and yielding θa θs im the lower θs reproducibility was also reported for co2 contact angle measurements in micromodels using similar experimental methods jafari and jung 2017 note that θd are more important for hydrogen storge than less reproducible θs because θd represent dynamic hydrogen injection withdrawal 3 4 1 1 equilibrium contact angles the equilibrium contact angles θe were estimated based on the following equation tadmor 2004 θ e c o s 1 r a c o s θ a r r c o s θ r r a r r where r a sin 3 θ a 2 3 cos θ a cos 3 θ a 1 3 and r r sin 3 θ r 2 3 cos θ r cos 3 θ r 1 3 the calculated θe were plotted together with the measured θa and θr fig 11b and compared with the original relationship based on the measurements through the capillary rise in polytetrafluoroethylene ptfe tubes morrow 1975 in the seminal work of morrow three different classes were defined depending on the degree of surface roughness where the contact angles were measured class i on smooth surfaces θa θr class ii on slightly roughened surfaces and class iii on well roughened surfaces our results showed that hydrogen water fluid pair in a natural sandstone based micromodel fit class ii behaviour although the θr were slightly overestimated this overestimation could be attributed to the difference in the measurement procedures in our work the θe were calculated based on the equation and the measured θa and θr in the heterogeneous micromodel with realistic pore geometries whereas in morrow 1975 both θe and θa θr were experimentally measured in ptfe tubes which cannot account for the effect of the pore geometries the relationship between θe and θa θr is essential for pore scaling modelling where class iii behaviour is widely implemented generally valvatne and blunt 2004 and for hydrogen in particular hashemi et al 2021 with known contact angles relative permeability and capillary pressure functions can be predicted using pore scale modelling and then upscaled for field scale applications discrepancies between the pore scale modelling approaches which use class iii behaviour well roughened surfaces and our results showing class ii behaviour slightly roughened surfaces has a direct impact on understanding hydrogen pore scale flow physics although the micromodel used in this study was reproduced with 100 nm surface roughness buchgraber et al 2012 the micromodel may not be sufficiently rough to accurately reproduce the experimental results with pore scale modelling mismatch between the experimental and simulated data may be expected when using class iii behaviour as the modelling input 3 4 2 contact angle hysteresis hysteresis was estimated for dynamic θd θa θr and static contact angles θs θs im θs dr fig 12 a as expected static contact angle hysteresis θs was lower than the dynamic one θd no clear relationship was noted between θs and nca likely due to lower θs reproducibility in the micromodels as discussed earlier on the other hand θd seemed to depend on nca with nearly constant value until a slight increase at nca 7 68 10 5 the θd is expected to increase with increasing nca due to increasing θa and decreasing θr eral et al 2013 a theoretical model for liquid gas systems hoffman 1983 showed that the increase in θa becomes more pronounced at nca 10 4 10 3 comparable with our threshold nca 7 68 10 5 the θs is mainly attributed to surface roughness heterogeneity joanny and degennes 1984 or disjoining conjoining pressure isotherm in the three phase contact line kuchin and starov 2016 whereas the θd is due to local surface blemishes which pin the three phase contact line tadmor 2004 the θd dependency on nca originates from competition between capillary and viscous forces friedman 1999 hysteresis may also be described by comparing the static and dynamic contact angles in each injection process where drainage hysteresis describes the difference between θs dr and θr and imbibition hysteresis refers to θs im and θa several models have been proposed to characterize imbibition hysteresis through the following equation jiang et al 1979 seebergh and berg 1992 li et al 2013 cos θ s i m cos θ a cos θ s i m 1 b 1 n c a b 2 this model was recently adapted for drainage hysteresis in liquid bridges as follows shi et al 2018 cos θ r cos θ s d r cos θ s d r 1 b 1 n c a b 2 we applied these models to estimate drainage and imbibition hysteresis fig 12b our contact angles were fitted with b1 18 8 and b2 1 0 for drainage and b1 0 29 and b2 0 16 for imbibition but the correlation was poor with r2 0 38 and r2 0 47 respectively note however that for field scale implications drainage imbibition hysteresis is less important than the dynamic one θd as discussed above the θd can be used to estimate drainage imbibition hydrogen water relative permeability curves 3 4 3 discussion on measurement techniques literature data on hydrogen contact angles is scare despite being highly relevant for wettability and relative permeability estimations our contact angle measurements matched well with θs reported for hydrogen in berea sandstone hashemi et al 2021 and with θr in vosges sandstone yekta et al 2018 but were higher than θe in basaltic al yaseri and jha 2021 and θd in quartz iglauer et al 2021 rocks inconsistency between literature results may be related to different experimental conditions measurement techniques and cleaning procedures iglauer et al 2015 contact angle dependency on pressure and temperature was showed with tilted plate method al yaseri and jha 2021 iglauer et al 2021 and core scale steady state drainage yekta et al 2018 whereas the captive bubble method did not identify any pressure temperature and salinity effects hashemi et al 2021 micromodel based measurement technique used in this study is unconventional and relatively novel previously applied by a few co2 studies only hu et al 2017 jafari and jung 2017 chang et al 2020 however this measurement method is valuable as it provides direct static and dynamic contact angle measurements in micromodel pores thus representing multiphase flow in porous media more accurately than indirect measurements our results can enhance understanding of wettability as well as relative permeability and capillary pressure hysteresis which are well recognized in natural gas storage colonna et al 1972 but not appropriately studied for hydrogen despite being valuable for wettability and hysteresis determination in real pore structures our measurement technique possessed several sources of uncertainties the first source was related to the identification of the hydrogen grain contact line caused by non planar grain surfaces and shadows due to non vertical pore walls to minimize this error sufficiently long contact lines were selected on nearly flat grains the second source of uncertainties was linked to the tangent line drawn along the hydrogen water curvature the third source was caused by random hydrogen distribution which did not allow to measure all four contact angle types in the same pore enhancing local effect on the measurements to suppress the effect of uncertainties the contact angles measurements were repeated five times in each measurement and average values were reported together with uncertainties thus adding reliability to our measurements 3 5 discussion on implications and methodologies our results have several field scale implications hydrogen saturation after drainage increased with increasing injection rate suggesting the storage site development is the most efficient at high injection rates nevertheless the roof snap off was observed at the highest injection rate nca 3 84 10 4 this resulted in hydrogen disconnection and trapping potentially leading to lower storage efficiencies the observations of the drainage snap off show that current pore scale modelling approaches based on invasion percolation and static snap off criteria should be revisited roman et al 2017 herring et al 2018 note that uhs field scale projects will use intermittent hydrogen injections with various frequencies and loads potentially resulting in reconnection of the residually trapped hydrogen if the drainage snap off occurs at nca 3 84 10 4 the corresponding field scale injection rate would be approximately 1 7 million sm3 d standard cubic metre per day based on the perforation length of 30 m and experimental injection velocity lower hydrogen injection rates 1 7 million sm3 d may therefore be preferred to avoid the hydrogen disconnection note however that the laboratory nca should be applied for field scale implications with caution because they do not account for important reservoir parameters such as gravity heterogeneity and wettability despite low solubility in water direct pore scale hydrogen dissolution was observed in our work hydrogen dissolution is undesired in storage projects due to loss of recoverable hydrogen carden and paterson 1979 the observed dissolution emphasizes the importance of the cushion gas composition where other than hydrogen cushion gases with low solubility in water are preferred moreover hydrogen dissolution may be enhanced by water encroachment during withdrawal as well as the buoyancy driven hydrogen injection from the reservoir bottom non equilibrium dissolution if valid at the reservoir scale is in turn more favourable compared with the equilibrium one leading to slower hydrogen dissolution under subsurface conditions hydrogen dissolution kinetics is expected to change according to the literature solubility data hydrogen solubility increases with increasing pressure and decreases with increasing salinity chabab et al 2020 the numerical approach dominates the uhs literature relying on the parameter approximations without exact knowledge since the uhs is an emerging field the models need to be validated with hydrogen laboratory data microfluidics offers systematic investigation of the parameter space to collaborate model development however upscaling of microfluidic experiments to field scale should be implemented with caution due to their 2d nature with lack of gravitational effects and heterogeneity furthermore the interactions between viscous capillary and gravitational forces become more pronounced at field scale the most suitable way to upscale the microfluidic experiments is through pore scale modelling for future work we recommend coupling microfluidic experiments and pore scale modelling to support the proposed models of hydrogen behaviour in porous media 4 conclusions microfluidic drainage and imbibition experiments were performed to examine the hydrogen water flow in a natural sandstone geometry in situ live camera monitoring provided qualitative data describing hydrogen displacement and trapping mechanisms hydrogen dissolution kinetics and contact angles were quantified using image processing we summarize our main findings as follows hydrogen saturation after drainage increased with increasing capillary number nca hydrogen phase connectivity was generally high except for high nca 3 84 10 4 drainage where disconnected hydrogen phase established due to roof snap off hydrogen displacement during imbibition was mainly governed by i1 imbibition mechanism whereas hydrogen disconnection with subsequent residual trapping was generally triggered by i2 imbibition mechanism hydrogen dissolution occurred at one end of the bubble mainly hydrogen dissolution kinetics was quantified showing that average depletion rate of individual hydrogen bubbles ranged between 2 3 10 12 to 22 10 12 g sec and appeared to depend on nca and initial bubble size the average global hydrogen depletion rate varied between 3 6 10 10 to 277 10 10 g sec the average dissolved hydrogen concentration in injected water mass was within the range of 6 4 10 4 to 11 10 4 mol kg which was only 16 0 28 3 of the literature solubility demonstrating the non equilibrium dissolution static and dynamic contact angles ranged from 17 to 56 confirming the non wetting hydrogen nature the equilibrium angle calculations fit class ii behaviour hysteresis was quantified showing that dynamic contact angle hysteresis was higher than the static credit authorship contribution statement maksim lysyy conceptualization methodology formal analysis investigation writing original draft visualization geir ersland conceptualization formal analysis writing review editing supervision project administration martin fernø conceptualization formal analysis writing review editing visualization supervision project administration declaration of competing interest none acknowledgements the authors would like to thank the department of physics and technology the university of bergen for financial support supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2022 104167 appendix supplementary materials image application 1 image video 2 image video 3 image video 4 image video 5 image video 6 image video 7 image video 8 
165,underground hydrogen storage uhs has been launched as a catalyst to the low carbon energy transitions the limited understanding of the subsurface processes is a major obstacle for rapid and widespread uhs implementation we use microfluidics to experimentally describe pore scale multiphase hydrogen flow in an aquifer storage scenario in a series of drainage imbibition experiments we report the effect of capillary number on hydrogen saturations displacement trapping mechanisms dissolution kinetics and contact angle hysteresis we find that the hydrogen saturation after injection drainage increases with increasing capillary number during hydrogen withdrawal imbibition two distinct mechanisms control the displacement and residual trapping i1 and i2 imbibition mechanisms respectively local hydrogen dissolution kinetics show dependency on injection rate and hydrogen cluster size dissolved global hydrogen concentration corresponds up to 28 of reported hydrogen solubility indicating pore scale non equilibrium dissolution contact angles show hysteresis and vary between 17 and 56 our results provide key uhs experimental data to improve understanding of hydrogen multiphase flow behaviour graphical abstract image graphical abstract keywords underground hydrogen storage microfluidics residual trapping dissolution rates contact angle hysteresis 1 introduction as a no carbon energy carrier hydrogen may play a significant role in the energy transition needed to reach net zero societies hydrogen implementation in transport heating and power generation will require large scale seasonal storage and underground hydrogen storage uhs in aquifers has been proposed as one option carden and paterson 1979 lord et al 2014 technical aspects of uhs are similar to natural gas storge ugs where gas is injected in the subsurface cushion gas and is then withdrawn at peak demand working gas although knowledge transfer from ugs is possible high hydrogen mobility and its potential biogeochemical activity panfilov 2010 calls for caution and revision of conventional storage practices experience with commercial underground storage of pure hydrogen is limited to salt caverns ozarslan 2012 underground aquifers have been used for town gas storage only with hydrogen content up to 50 60 smigan et al 1990 panfilov 2016 hydrogen injection and withdrawal in underground porous formations involve complex displacement and trapping mechanisms controlled by hydrogen flow properties and interactions with reservoir fluids and rocks the understanding of hydrogen flow physics and trapping in porous media is therefore essential to establishing reliable storage models for lab scale tests feasibility studies and piloting most porous media research on hydrogen is mainly focused on biogeochemical interactions berta et al 2018 flesch et al 2018 bo et al 2021 but there are fewer fundamental studies reporting multiphase flow data with the dominance of the numerical modelling approaches lubon and tarkowski 2021 lysyy et al 2021 mahdi kanaani 2022 most numerical studies use extrapolated flow functions not specifically measured for hydrogen thus experimental efforts are needed to improve the hydrogen flow modelling a single reported experimental core scale study found that hydrogen water relative permeability is independent of pressure and temperature conditions yekta et al 2018 a major concern is that the displacement is prone to front instabilities and viscous fingering due to an unfavourable hydrogen water mobility ratio microscopic viscous fingers were confirmed with laboratory models paterson 1983 in addition hydrogen withdrawal will be associated with loss caused by residual and dissolution trapping unlike co2 sequestration residual and dissolution trapping are not desirable in uhs as it leads to unrecoverable hydrogen thus representing a permanent loss carden and paterson 1979 uhs involves many injection withdrawal cycles and the residually trapped hydrogen may reconnect during subsequent hydrogen injections known as hysteresis microfluidic experiments are perfectly suited for visualization of porous media hydrogen flow thereby providing direct evidence of the proposed displacement and trapping mechanisms to corroborate core scale measurements due to the 2d nature of the micromodels and their limited volume microfluidic experiments should mainly focus on the qualitative rather than quantitative results extrapolation of quantitative 2d data to 3d natural environment should be done with caution best achieved through pore scale modelling in particular hydrogen contact angle measurements assist the pore scale models in estimating upscaled relative permeability and capillary pressure functions which can be used as input for numerical studies at field scale hashemi et al 2021 classical pore scale displacement theory defines four displacement mechanisms which may result in residual trapping piston like snap off i1 imbibition and i2 imbibition lenormand et al 1983 dissolution trapping occurs when the residually trapped phase dissolves in water controlled by the trapped phase diffusivity and solubility hydrogen solubility studies relevant for uhs demonstrated inconsistencies due to missing experimental support and or different measurement approaches de lucia pilz et al 2015 li et al 2018 lopez lazaro et al 2019 chabab et al 2020 contact angle measurements are commonly used in multiphase transport research to understand the effects of wettability and capillary pressure and relative permeability hysteresis on fluid systems the hydrogen water system is still not adequately investigated and lack consistent and systematic approaches however hydrogen contact angles have been derived for basalt al yaseri and jha 2021 and measured for quartz iglauer et al 2021 and sandstone hashemi et al 2021 rocks and the results showed discrepancies in terms of pressure temperature and salinity effects overall pore scale displacement and trapping mechanisms are well described for co2 sequestration buchgraber et al 2012 cao et al 2016 chang et al 2016 hu et al 2017 chang et al 2020 but remains to our knowledge unaddressed for hydrogen our work examines hydrogen flow behaviour in an initially water filled micromodel relevant for uhs in aquifers we perform a series of injection drainage and withdrawal imbibition experiments to qualitatively describe pore scale hydrogen displacement and trapping mechanisms with image analysis we quantify hydrogen dissolution kinetics and measure contact angles this study is relevant for readers seeking to understanding of hydrogen flow physics in porous media and adds new data to experimental dataset 2 materials and methods 2 1 micromodel all drainage and imbibition experiments were conducted in a silicon wafer micromodel based on natural sandstone pore patterns with a large variation in grain and pore sizes and shapes extracted from the scanning electron microscope image of a representative sandstone thin section and slightly modified to enable flow the 2d pore network was etched into silicon wafer with deep reactive ion etching drie hornbrook et al 1991 buchgraber et al 2012 the drie realistically reproduces topological features such as high pore body to pore throat ratio coordination number 4 8 sharp pore walls and surface roughness 100 nm the exact reproduction of pore and pore throat sizes generate capillary forces at the magnitude relevant for real porous rocks the heterogeneous mineralogy is however not reproduced i e no clay and or calcite minerals present the silicon dioxide layer on the micromodel surfaces prevents hydrogen adsorption four ports etched through the micromodel bottom facilitate external access to the porous network whereas two high permeable fracture channels between the ports allow to easily distribute the injected fluids the micromodel bottom silicon wafer and top borosilicate glass surfaces were connected through anodic bonding resulting in the hydrophilic pore network the micromodel hydrophilic nature allowed us to distinguish between the injected fluids under microscopic view fig 2 hydrogen light blue develops a convex curvature towards water blue and the grains the etched porous network has the length x width x depth dimensions of 2 8 cm x 2 2 cm x 0 0030 cm respectively and porosity of 60 yielding the pore volume of 11 µl the average pore diameter is in the order of 100 µm with the grain size and pore throat length distributions of 100 7900 µm2 and 10 200 µm respectively alcorn et al 2020 the pore network extraction tends to increase the total micromodel porosity compared with the representative rock and the micromodel porosities up to 46 55 can be found in the literature buchgraber et al 2012 chang et al 2019 wu et al 2021 our pore network was based on the thin rock section containing both small and large pore clusters the pore network was repeated 36 4 9 times and stitched together on the micromodel surface resulting in relatively high total porosity of 60 note however that our microscope provided the observations of the micromodel field of view fov only which is approximately 1 of the whole micromodel area the local fov porosity is in the order of 30 which is closer to the natural rocks 2 2 experimental set up and procedure the micromodel was mounted in the peek holder and connected to two quizix pumps through 1 16 peek and stainless steel tubing fig 1 a quizix sp 5200 pump cylinder c5000 10k ss at was filled with pure hydrogen 99 999 whereas quizix qx pump contained distilled water the micromodel was illuminated by a light source photonic led f1 cold light 5500 k a microscope nikon smz1500 connected to a camera nikon d7100 and computer provided direct real time observations of the fov experimental data was acquired through live view video recordings with the frame rate 29 97 fps and the resolution of 0 5 pixels µm prior to every run consisting of hydrogen and water injections the pore space was initially 100 saturated with distilled water every experiment consisted of one hydrogen injection drainage and one water injection imbibition from two opposite inlets creating a diagonal flow through the pore network all injections used a pore pressure of p 5 bar and room temperature constant pressure was maintained with a hydrogen filled pump whereas a water filled pump performed water withdrawal injection at constant flow rates hydrogen injections drainage were initiated by water withdrawal and lasted until between 50 and 500 water pore volumes pv were withdrawn after hydrogen invasion enabling quasi steady state subsequently water injection imbibition started with the same flow rate and the injection was maintained until hydrogen was completely dissolved thereby running a single cycle of hydrogen injection withdrawal only the micromodel was then cleaned with distilled water to remove any residual hydrogen and to re saturate the pore space with 100 distilled water making the system ready for the next experiment four different flow rates were applied 0 1 1 10 and 50 ml h with corresponding capillary numbers nca calculated from the equation nca u µ σ where u is flow velocity m s µ is the wetting phase water viscosity pa s and σ is the interfacial tension n m flow velocity u was calculated as u q l d ф with q rate m3 s l near inlet length inside the micromodel m d porous network depth m ф porosity fraction hydrogen interfacial tension σ at experimental pressure was calculated to 0 072 n m based on the empirical formulation massoudi and king 1974 the calculated capillary numbers allowed to locate our experiments on the log nca log m stability diagram fig 1b where m is defined as the hydrogen water viscosity ratio 2 3 relevance of experimental conditions our experiments were run under low pressure and with distilled water the reservoir brine salinity may affect the gas surface properties the gas brine interfacial tension duchateau and broseta 2012 and contact angles jafari and jung 2019 increase with increasing salinity these correlations are yet to be confirmed for hydrogen brine systems and should be addressed in future studies low pressure was selected in our work due to the safety risks associated with hydrogen flammability and equipment compatibility under high pressure hydrogen properties affecting 2d porous media multiphase flow viscosity and h2 h2o interfacial tension do not vary significantly with increasing storage depths increasing pressure and temperature unlike many other gases like co2 n2 and ch2 beckingham and winningham 2020 al 2022 pressure dependant variations in hydrogen density play an important role in gravity dominated 3d problems and thus are assumed to be insignificant in our 2d microfluidic study moreover one of the hydrogen storage projects was operated at pressures down to 5 10 bars in the argentinian depleted gas field pérez et al 2016 our low pressure study is therefore relevant for the real storage conditions 2 4 image analysis experimental data was quantified with image analysis in the open source imagej software hydrogen saturations were estimated based on colour thresholding permitting to calculate local fov porosity and distinguish hydrogen from water dissolution data was obtained by measuring the areal decrease of the hydrogen phase with time contact angles were measured using an angle tool static contact angles were measured when the hydrogen water interface did not move whereas paused videos allowed to measure dynamic contact angles when the hydrogen water interface moved during water withdrawal injection receding contact angles were measured when hydrogen displaced water drainage advancing contact angles were measured when water displaced hydrogen imbibition note that the image analysis is dependant on the image resolution segmentation and user adjustments the image resolution was high enough to distinguish between hydrogen and water in some cases the light source limitations resulting in the image gradients required pre processing of the images with the manual segmentation of fluids 3 results and discussion 3 1 hydrogen saturation establishment during drainage hydrogen invaded the pores immediately after entering the fov indicating that non wetting phase invasion occurred on millisecond scale independent of capillary number nca this is consistent with previous micromodel studies in oil air water systems mohanty et al 1987 moebius and or 2012 armstrong and berg 2013 rapid hydrogen pore invasion serves as indirect evidence of hydrogen non wetting nature andrew et al 2015 which was directly confirmed by contact angle measurements detailed in section 3 4 3 1 1 initial hydrogen saturation the fov hydrogen saturation sg after drainage increased with increasing nca fig 2 as expected from classical pore scale displacement theory lenormand et al 1983 hydrogen invasion into neighbouring pore clusters was restricted by narrow pore throats with higher capillary entry pressures pores invaded by hydrogen were predominantly saturated with hydrogen with some visible water accumulations droplet forms on the pore bottom due to surface roughness white arrows in fig 2 number of water droplets were largest at upper medium and high nca whereas they were absent at low nca at the end of drainage at the hydrogen breakthrough however water droplets formed even at low nca fig 3 with continued hydrogen injection the droplets were displaced likely due to 1 the hydrophilic micromodel surfaces with high water phase connectivity through connected wetting films and or 2 water evaporation in hydrogen the water droplet displacement evaporation demonstrated the pore scale efficiency of low nca drainage when multiple pore occupancies establish in hydrophilic systems from this we could expect that hydrogen injection strategies that result in pore occupancy by hydrogen phase only no pore water will decrease water cut upon hydrogen withdrawal in aquifer storage improving overall storage performance low nca hydrogen injection may therefore be preferred from the perspective of the pore occupancies 3 1 2 hydrogen connectivity the non wetting phase connectivity defines if the phase is connected through the pore clusters the hydrogen connectivity was high at low and medium nca with observations of several connected gas paths fig 2a c at high nca both connected and disconnected hydrogen phases established fig 2d with disconnected hydrogen phase accounting for 11 of the total fov hydrogen saturation the predominant mechanism for disconnected hydrogen was roof snap off roof 1970 due to the front interface destabilization after entering the neighbouring pore to maintain capillary equilibrium water thickens in the pore throat leading to the non wetting phase hydrogen in our case disconnection for roof snap off to occur high water availability is required near the pore throat and the non wetting phase must pass the pore throat for a distance of at least seven times the throat radius snap off controlled by capillary pressure is expected in smaller pore throats in our work roof snap off occurred in small 15 µm and large 25 µm pore throats without occurring in neighbouring pore throats of similar sizes fig 4 this implies that roof snap off was a local phenomenon likely controlled by water mobility and availability as suggested for drainage snap off in a co2 brine system andrew et al 2015 the pore throat water thickening which was believed to cause hydrogen snap off was not possible to confirm visually because of sub second snap off and insufficient microscope resolution to detect thin wetting films before snap off occurred in small and large pores fig 4 the distance propagated by hydrogen corresponded to 35 and 15 times of the pore throat radius respectively fulfilling the condition for roof snap off snap off during drainage is less common than in imbibition and is still not appropriately investigated in the seminal work of roof 1970 the criteria for drainage snap off were linked to local conditions water availability pore throat and interface size wettability the drainage snap off dependency on global dynamic conditions viscosity ratio compressibility capillary number were proposed deng et al 2015 herring et al 2018 our results suggest that drainage snap off was triggered by both local and global factors the snap off independence on pore throat sizes fig 4 showed local features of snap off events whereas the snap off occurrence at high nca only suggests that snap off drainage was correlated to global dynamic parameter nca snap off during and after drainage is undesired in seasonal hydrogen storage as this may lead to permanent hydrogen entrapment in our work most of disconnected hydrogen bubbles fig 2d did not reconnect during imbibition resulting in a complete hydrogen dissolution although the highest hydrogen saturation fig 2 was achieved at high nca 3 84 10 4 high injection rates will not necessarily yield the maximum injection efficiency in aquifer storage projects considering the possibility for snap off 3 1 3 flow regime according to the lognc a logm phase diagram fig 1b unstable viscous dominated displacement was expected to prevail over capillary dominated flow the lognc a logm phase diagram boundaries are however system dependant zhang et al 2011 and are not necessarily applicable for our micromodel neither viscous nor capillary fingering were possible to observe due to the limited fov nevertheless some indirect evidence of viscous and capillary flow regimes was observed locally the establishment of the connected hydrogen phase stopped by narrow pore throats may show the importance of capillary fingering at low medium nca roof snap off caused by hydrogen penetration through narrow pore throats indicate the dominance of viscous forces at high nca zhang et al 2011 micromodel studies enabling to observe the entire micromodel at a wider nca range will be beneficial for a direct determination of the dominating flow regime in hydrogen water systems 3 2 displacement and residual trapping during imbibition hydrogen displacement and disconnection residual trapping was observed during imbibition that started with water injection into the same fluid system which established after drainage imbibition proceeded in three main steps common for all nca 1 displacement 2 disconnection 3 dissolution an additional step between steps 1 and 2 was observed at upper medium and high nca hydrogen redistribution caused by fluid displacement from outside the fov this section describes the first two steps displacement and disconnection in addition to redistribution dissolution will be described in section 3 3 3 2 1 displacement mechanisms hydrogen displacement was mainly governed by i1 imbibition mechanism fig 5 a initially occupying three pores hydrogen was forced into a single pore because of the curvature instability resulting from the curvature detachment from the pore walls two other displacement mechanisms were common at specific nca piston like displacement and redistribution piston like displacement was observed at low nca where a stable displacement front moved through a single pore channel fig 5b the pore channel was surrounded by narrow pore throats forcing water to displace hydrogen from one direction only at upper medium and high nca hydrogen redistribution occurred fig 5c where the original hydrogen phase was first displaced and trapped followed by a partial reconnection with surrounding hydrogen this mechanism occurred because of high hydrogen saturation after upper medium high nca drainage permitting hydrogen movement through the entire micromodel during imbibition note that most of the hydrogen bubbles disconnected due to roof snap off during drainage remained disconnected during redistribution only a single hydrogen bubble in the fov reconnected with the continuous hydrogen phase the inability to reconnect resulted in a complete hydrogen bubble dissolution demonstrating the disadvantages of the drainage snap off 3 2 2 residual trapping hydrogen disconnection leading to residual trapping occurred mainly by i2 imbibition mechanism fig 6 a displacement from the pore centre towards the pore wall resulted in hydrogen disconnection when the hydrogen water interface reached the pore wall the disconnected hydrogen occupied two pores trapping by bypass was observed at upper medium and high nca fig 6b the water flow paths did not manage to invade the large hydrogen saturated pore clusters with narrow pore throats resulting in a significant hydrogen fraction being bypassed the dominance of the i2 mechanism over bypass was likely due to topological reasons high coordination number permitting the transverse to hydrogen water flow paths bypass is expected in large pore clusters with narrow pore throats chatzis et al 1983 consistent with our observations however at low and lower medium nca hydrogen did not occupy the large pore clusters fig 2a b where bypass was observed at upper medium and high nca for a more general conclusion on the relative importance of i2 and bypass mechanisms hydrogen must occupy the same pore clusters in all experiments which is challenging to control in the heterogeneous pore space with the micromodels used in this study trapping by snap off was not identified despite high micromodel aspect ratio and roughness likely due to experimental conditions snap off is expected to dominate at nca 10 7 hu et al 2017 whereas our experiments were conducted at nca 7 68 10 7 the fov hydrogen saturation profiles were estimated for imbibition to construct the imbibition cdc and gas trapping curves fig s2 and fig s1 in the supplementary materials respectively 3 3 dissolution 3 3 1 dissolution mechanisms dissolution of disconnected and trapped hydrogen was observed during prolonged water injection three dissolution mechanisms were identified fig 7 one end dissolution two end dissolution and displacement dissolution the one end dissolution fig 7a was frequently observed at upper medium and high nca where hydrogen bubbles dissolved from one end only reflecting the water flow direction the rapidly developed waterfront propagating through the micromodel in one main direction was not able to enter narrow pore throats counter currently against the main flow direction hence dissolution initiated only from one end of the trapped hydrogen bubble residing in the pore corners surrounded by narrow pore throats the one end dissolution was also observed in supercritical co2 dissolution in micromodel chang et al 2016 the two end dissolution mechanism fig 7b prevailed at lower medium nca where the hydrogen bubbles were dissolved at both sides simultaneously this mechanism was attributed to a more stable waterfront and greater water availability originating from lower hydrogen saturation developed after drainage the displacement dissolution mechanism fig 7c was characterized by mobilization of smaller hydrogen bubbles that were able to penetrate narrow pore throats this mechanism was observed at upper medium and high nca due to faster and more directed water flow overall observed dissolution mechanisms suggest that hydrogen dissolution was governed by the waterfront velocity and direction which in turn was controlled by nca two dissolution processes were detected independent of nca homogeneous and heterogeneous dissolution they differed in terms of the microbubble final state at the end of dissolution in homogenous dissolution microbubbles dissolved completely whereas the residual microbubbles accumulated at the surface roughness in heterogeneous dissolution homogenous heterogeneous dissolution as well as displacement dissolution fig 7c were also reported for co2 dissolution in micromodel buchgraber et al 2012 3 3 2 dissolution kinetics local and global dissolution kinetics were estimated based on the image analysis local dissolution kinetics was quantified by calculating the temporal change in the individual hydrogen bubble size and the depletion rate fig 8 as expected time required for compete dissolution decreased with increasing nca because of the faster water supply fig 8a the total dissolution time in every experiment was nearly equal and independent of the initial bubble area implying simultaneous dissolution in the entire fov the depletion rate fig 8b was calculated as the depleted hydrogen mass per time interval between two sequential images qd a d ρh2 t where a decrease in individual hydrogen bubble area between two sequential images d porous network depth ρh2 hydrogen density under experimental conditions 4 12 10 4 g ml t time interval between two sequential images the depletion rate changed with time shifting from nearly constant values to increasing or decreasing trends similar discrepancies in depletion rate trends were also observed in pore scale supercritical co2 dissolution explained by the number of water flow paths their direction transverse or longitudinal and the co2 water interface area chang et al 2016 the average hydrogen depletion rate ranged from 2 3 10 12 to 22 10 12 g sec with the lowest rate observed at lower medium nca expected and the highest rate observed at upper medium nca unexpected high nca did not yield the highest depletion rate due to the smaller initial bubble size compared with upper medium nca cases when comparing bubbles of similar size high nca depletion rate was higher than upper medium nca table 2 global dissolution kinetics was analysed based on the fov hydrogen saturation profiles during dissolution fig 9 the global depletion rate fig 9b was calculated as follows qd global sg vp ρh2 t where sg decrease in the fov hydrogen saturation between two sequential images vp micromodel pore volume ρh2 hydrogen density under experimental conditions t time interval between two sequential images the global depletion rate calculations assume that fov hydrogen saturation profiles are representative for the entire micromodel the global depletion rates showed non constant trends and on average varied between 3 6 10 10 to 277 10 10 g sec two orders of magnitude higher compared with local depletion rate of individual bubbles similar to observations of co2 dissolution chang et al 2016 the global depletion rate was the lowest at lower medium nca and highest at high nca table 2 3 3 2 1 dissolved hydrogen concentration and solubility the averaged dissolved hydrogen concentration table 2 was calculated as the dissolved hydrogen amount per injected water mass between two sequential images according to the formula c qd mh2 q ρh2o where qd hydrogen depletion rate mh2 hydrogen molar mass q water injection rate ρh2o water density these calculations were based on the mass balance principle assuming that hydrogen depletion is solely controlled by dissolution and water advection chang et al 2016 chang et al 2019 hydrogen depletion will be controlled by diffusion when the water phase is immobile which is not the case in our work where water is continuously injected during imbibition in local dissolution the dissolved individual hydrogen bubble concentration ranged between 0 4 10 6 and 5 9 10 6 mol kg in global dissolution the dissolved hydrogen concentration varied from 6 4 10 4 to 11 10 4 mol kg corresponding to 16 0 and 28 3 of the hydrogen solubility under the applied experimental conditions chabab et al 2020 lower than solubility hydrogen concentrations indicate non equilibrium hydrogen dissolution in our work conflicting with classic equilibrium dissolution theories applied in numerical modelling pruess and spycher 2007 non equilibrium slow dissolution has also been reported for co2 both in experimental core scale akbarabadi and piri 2013 chang et al 2013 and pore scale chang et al 2016 chang et al 2019 studies as well as numerical studies chen et al 2018 for instance co2 dissolution measurements in micromodels showed that the average co2 concentration varied between 0 25 13 of co2 solubility chang et al 2016 chang et al 2019 they explained non equilibrium dissolution by insufficient co2 water interface area and non uniform co2 mobile water distribution they argued that at reservoir scale where dissolution occurs at slower timescales co2 dissolution after the injection stop will approach equilibrium unless strong heterogeneity is present in hydrogen aquifer storage hydrogen is cyclically injected and withdrawn at high rates and we therefore speculate that non equilibrium dissolution may play an important role 3 4 contact angles 3 4 1 static and dynamic contact angles static θs and dynamic θd contact angles were measured during drainage and imbibition each measurement was performed five times at the same measuring point and then averaged with the uncertainty represented as standard deviation the measured contact angles fig 10 varied from 17 to 56 similar to contact angles of 22 45 for hydrogen water sandstone systems yekta et al 2018 hashemi et al 2021 our results confirmed that the micromodel is hydrophilic when exposed to hydrogen no clear relationship between contact angles and pore diameter emerged although the contact angle range appeared to narrow with increasing pore diameter majority of measurements performed in pores with diameter between 50 and 125 µm four contact angle types receding θr advancing θa static in drainage θs dr and static in imbibition θs im were averaged for each experiment table 3 and plotted as a function of nca fig 11 a the θa were significantly higher than the θr consistent with classic theories johnson and dettre 1964 as expected θa θs im but θr and θs dr were surprisingly similar θr θs dr the similarity between θr and θs dr could be linked to the experimental procedure after hydrogen breakthrough under drainage hydrogen injection continued through the connected hydrogen phase and the θs dr were measured when the interface movement terminated visually in this state despite being motionless the interfaces did not reach the equilibrium due to continuous hydrogen injection the measured θs dr approached more dynamic than static states resulting in θr θs dr and were thus believed to be underestimated and less reproducible on the other hand the water breakthrough under imbibition resulted in hydrogen residual trapping with several hydrogen clusters being bypassed by water in these regions the interface was believed to be surrounded by immobile water thus approaching equilibrium and yielding θa θs im the lower θs reproducibility was also reported for co2 contact angle measurements in micromodels using similar experimental methods jafari and jung 2017 note that θd are more important for hydrogen storge than less reproducible θs because θd represent dynamic hydrogen injection withdrawal 3 4 1 1 equilibrium contact angles the equilibrium contact angles θe were estimated based on the following equation tadmor 2004 θ e c o s 1 r a c o s θ a r r c o s θ r r a r r where r a sin 3 θ a 2 3 cos θ a cos 3 θ a 1 3 and r r sin 3 θ r 2 3 cos θ r cos 3 θ r 1 3 the calculated θe were plotted together with the measured θa and θr fig 11b and compared with the original relationship based on the measurements through the capillary rise in polytetrafluoroethylene ptfe tubes morrow 1975 in the seminal work of morrow three different classes were defined depending on the degree of surface roughness where the contact angles were measured class i on smooth surfaces θa θr class ii on slightly roughened surfaces and class iii on well roughened surfaces our results showed that hydrogen water fluid pair in a natural sandstone based micromodel fit class ii behaviour although the θr were slightly overestimated this overestimation could be attributed to the difference in the measurement procedures in our work the θe were calculated based on the equation and the measured θa and θr in the heterogeneous micromodel with realistic pore geometries whereas in morrow 1975 both θe and θa θr were experimentally measured in ptfe tubes which cannot account for the effect of the pore geometries the relationship between θe and θa θr is essential for pore scaling modelling where class iii behaviour is widely implemented generally valvatne and blunt 2004 and for hydrogen in particular hashemi et al 2021 with known contact angles relative permeability and capillary pressure functions can be predicted using pore scale modelling and then upscaled for field scale applications discrepancies between the pore scale modelling approaches which use class iii behaviour well roughened surfaces and our results showing class ii behaviour slightly roughened surfaces has a direct impact on understanding hydrogen pore scale flow physics although the micromodel used in this study was reproduced with 100 nm surface roughness buchgraber et al 2012 the micromodel may not be sufficiently rough to accurately reproduce the experimental results with pore scale modelling mismatch between the experimental and simulated data may be expected when using class iii behaviour as the modelling input 3 4 2 contact angle hysteresis hysteresis was estimated for dynamic θd θa θr and static contact angles θs θs im θs dr fig 12 a as expected static contact angle hysteresis θs was lower than the dynamic one θd no clear relationship was noted between θs and nca likely due to lower θs reproducibility in the micromodels as discussed earlier on the other hand θd seemed to depend on nca with nearly constant value until a slight increase at nca 7 68 10 5 the θd is expected to increase with increasing nca due to increasing θa and decreasing θr eral et al 2013 a theoretical model for liquid gas systems hoffman 1983 showed that the increase in θa becomes more pronounced at nca 10 4 10 3 comparable with our threshold nca 7 68 10 5 the θs is mainly attributed to surface roughness heterogeneity joanny and degennes 1984 or disjoining conjoining pressure isotherm in the three phase contact line kuchin and starov 2016 whereas the θd is due to local surface blemishes which pin the three phase contact line tadmor 2004 the θd dependency on nca originates from competition between capillary and viscous forces friedman 1999 hysteresis may also be described by comparing the static and dynamic contact angles in each injection process where drainage hysteresis describes the difference between θs dr and θr and imbibition hysteresis refers to θs im and θa several models have been proposed to characterize imbibition hysteresis through the following equation jiang et al 1979 seebergh and berg 1992 li et al 2013 cos θ s i m cos θ a cos θ s i m 1 b 1 n c a b 2 this model was recently adapted for drainage hysteresis in liquid bridges as follows shi et al 2018 cos θ r cos θ s d r cos θ s d r 1 b 1 n c a b 2 we applied these models to estimate drainage and imbibition hysteresis fig 12b our contact angles were fitted with b1 18 8 and b2 1 0 for drainage and b1 0 29 and b2 0 16 for imbibition but the correlation was poor with r2 0 38 and r2 0 47 respectively note however that for field scale implications drainage imbibition hysteresis is less important than the dynamic one θd as discussed above the θd can be used to estimate drainage imbibition hydrogen water relative permeability curves 3 4 3 discussion on measurement techniques literature data on hydrogen contact angles is scare despite being highly relevant for wettability and relative permeability estimations our contact angle measurements matched well with θs reported for hydrogen in berea sandstone hashemi et al 2021 and with θr in vosges sandstone yekta et al 2018 but were higher than θe in basaltic al yaseri and jha 2021 and θd in quartz iglauer et al 2021 rocks inconsistency between literature results may be related to different experimental conditions measurement techniques and cleaning procedures iglauer et al 2015 contact angle dependency on pressure and temperature was showed with tilted plate method al yaseri and jha 2021 iglauer et al 2021 and core scale steady state drainage yekta et al 2018 whereas the captive bubble method did not identify any pressure temperature and salinity effects hashemi et al 2021 micromodel based measurement technique used in this study is unconventional and relatively novel previously applied by a few co2 studies only hu et al 2017 jafari and jung 2017 chang et al 2020 however this measurement method is valuable as it provides direct static and dynamic contact angle measurements in micromodel pores thus representing multiphase flow in porous media more accurately than indirect measurements our results can enhance understanding of wettability as well as relative permeability and capillary pressure hysteresis which are well recognized in natural gas storage colonna et al 1972 but not appropriately studied for hydrogen despite being valuable for wettability and hysteresis determination in real pore structures our measurement technique possessed several sources of uncertainties the first source was related to the identification of the hydrogen grain contact line caused by non planar grain surfaces and shadows due to non vertical pore walls to minimize this error sufficiently long contact lines were selected on nearly flat grains the second source of uncertainties was linked to the tangent line drawn along the hydrogen water curvature the third source was caused by random hydrogen distribution which did not allow to measure all four contact angle types in the same pore enhancing local effect on the measurements to suppress the effect of uncertainties the contact angles measurements were repeated five times in each measurement and average values were reported together with uncertainties thus adding reliability to our measurements 3 5 discussion on implications and methodologies our results have several field scale implications hydrogen saturation after drainage increased with increasing injection rate suggesting the storage site development is the most efficient at high injection rates nevertheless the roof snap off was observed at the highest injection rate nca 3 84 10 4 this resulted in hydrogen disconnection and trapping potentially leading to lower storage efficiencies the observations of the drainage snap off show that current pore scale modelling approaches based on invasion percolation and static snap off criteria should be revisited roman et al 2017 herring et al 2018 note that uhs field scale projects will use intermittent hydrogen injections with various frequencies and loads potentially resulting in reconnection of the residually trapped hydrogen if the drainage snap off occurs at nca 3 84 10 4 the corresponding field scale injection rate would be approximately 1 7 million sm3 d standard cubic metre per day based on the perforation length of 30 m and experimental injection velocity lower hydrogen injection rates 1 7 million sm3 d may therefore be preferred to avoid the hydrogen disconnection note however that the laboratory nca should be applied for field scale implications with caution because they do not account for important reservoir parameters such as gravity heterogeneity and wettability despite low solubility in water direct pore scale hydrogen dissolution was observed in our work hydrogen dissolution is undesired in storage projects due to loss of recoverable hydrogen carden and paterson 1979 the observed dissolution emphasizes the importance of the cushion gas composition where other than hydrogen cushion gases with low solubility in water are preferred moreover hydrogen dissolution may be enhanced by water encroachment during withdrawal as well as the buoyancy driven hydrogen injection from the reservoir bottom non equilibrium dissolution if valid at the reservoir scale is in turn more favourable compared with the equilibrium one leading to slower hydrogen dissolution under subsurface conditions hydrogen dissolution kinetics is expected to change according to the literature solubility data hydrogen solubility increases with increasing pressure and decreases with increasing salinity chabab et al 2020 the numerical approach dominates the uhs literature relying on the parameter approximations without exact knowledge since the uhs is an emerging field the models need to be validated with hydrogen laboratory data microfluidics offers systematic investigation of the parameter space to collaborate model development however upscaling of microfluidic experiments to field scale should be implemented with caution due to their 2d nature with lack of gravitational effects and heterogeneity furthermore the interactions between viscous capillary and gravitational forces become more pronounced at field scale the most suitable way to upscale the microfluidic experiments is through pore scale modelling for future work we recommend coupling microfluidic experiments and pore scale modelling to support the proposed models of hydrogen behaviour in porous media 4 conclusions microfluidic drainage and imbibition experiments were performed to examine the hydrogen water flow in a natural sandstone geometry in situ live camera monitoring provided qualitative data describing hydrogen displacement and trapping mechanisms hydrogen dissolution kinetics and contact angles were quantified using image processing we summarize our main findings as follows hydrogen saturation after drainage increased with increasing capillary number nca hydrogen phase connectivity was generally high except for high nca 3 84 10 4 drainage where disconnected hydrogen phase established due to roof snap off hydrogen displacement during imbibition was mainly governed by i1 imbibition mechanism whereas hydrogen disconnection with subsequent residual trapping was generally triggered by i2 imbibition mechanism hydrogen dissolution occurred at one end of the bubble mainly hydrogen dissolution kinetics was quantified showing that average depletion rate of individual hydrogen bubbles ranged between 2 3 10 12 to 22 10 12 g sec and appeared to depend on nca and initial bubble size the average global hydrogen depletion rate varied between 3 6 10 10 to 277 10 10 g sec the average dissolved hydrogen concentration in injected water mass was within the range of 6 4 10 4 to 11 10 4 mol kg which was only 16 0 28 3 of the literature solubility demonstrating the non equilibrium dissolution static and dynamic contact angles ranged from 17 to 56 confirming the non wetting hydrogen nature the equilibrium angle calculations fit class ii behaviour hysteresis was quantified showing that dynamic contact angle hysteresis was higher than the static credit authorship contribution statement maksim lysyy conceptualization methodology formal analysis investigation writing original draft visualization geir ersland conceptualization formal analysis writing review editing supervision project administration martin fernø conceptualization formal analysis writing review editing visualization supervision project administration declaration of competing interest none acknowledgements the authors would like to thank the department of physics and technology the university of bergen for financial support supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2022 104167 appendix supplementary materials image application 1 image video 2 image video 3 image video 4 image video 5 image video 6 image video 7 image video 8 
166,in multiscale modelling multiple models are used simultaneously to describe scale dependent phenomena in a system of interest here we introduce a machine learning ml based multiscale modelling framework for modelling hierarchical multiscale problems in these problems closure relations are required for the macroscopic problem in the form of constitutive relations however forming explicit closures for nonlinear and hysteretic processes remains challenging instead we provide a framework for learning constitutive mappings given microscale data generated according to micro and macro transitions governed by two scale homogenisation rules the resulting data driven model is then coupled to a macroscale simulator leading to a hybrid ml physics based modelling approach accordingly we apply the multiscale framework within the context of transient phenomena in dual porosity geomaterials in these materials the inter porosity flow is a complex time dependent function making its adoption within flow simulators challenging we explore nonlinear feedforward autoregressive ml strategies for the constitutive modelling of this sequential problem we demonstrate how to inject the resulting surrogate constitutive model into a simulator we then compare the resulting hybrid approach to traditional dual porosity and microscale models on a variety of tests we show the hybrid approach to give high quality results with respect to explicit microscale simulations without the computational burden of the latter lastly the steps provided by the multiscale framework herein are sufficiently general to be applied to a variety of multiscale settings using different data generation and learning techniques accordingly keywords multiscale constitutive modelling homogenisation dual porosity machine learning for transient phenomena hybrid machine learning physics based modelling 1 introduction when modelling we are often posed with the dilemma of choosing between models of varying levels of accuracy complexity and efficiency such problems are exemplified in the subsurface in these settings strong heterogeneities exist within the rock fabric leading to multiscale phenomena one solution is to model at the heterogeneity level microscale capturing the full range of scale dependent phenomena with appropriate post processing however at large scales computational and information requirements preclude the use of such detailed simulation models instead in situations where there is weak coupling between scales a so called separation of scales exists and we can use coarse representations accordingly we have a hierarchy of models where microscale information is incorporated implicitly within macroscale models in the form of constitutive relations however often these constitutive models are based on empiricism using simple considerations such as linearity weinan 2011 whilst practical use of macroscopic models with simplified constitutive relations can lead to undesirable inaccuracies this deficiency motivates the use of multiscale modelling approaches that is to combine the detail of microscale models with the efficiency of macroscale models here we are concerned with the use of multiscale methods with application to hierarchical multiscale problems generally multiscale methods can be classified as analytical or computational weinan 2011 although significant overlap may exist between the two in situations where analytical methods are impractical or intractable it is desirable instead to use computational approaches one such computational approach informed by developments in the latter part of the twentieth century is computational homogenisation feyel 1999 kouznetsova et al 2001 miehe and koch 2002 in this approach microscopic boundary value problems are nested within points in the macroscopic domain with the solution of the former providing information for the latter notably computational homogenisation has emerged as a powerful tool for modelling multiscale problems as diverse coupled multiphysics özdemir et al 2008b su et al 2011 kaessmair and steinmann 2018 and transient diffusion problems larsson et al 2010 ramos et al 2017 waseem et al 2020 to name but a few nonetheless whilst accurate these approaches still remain expensive to use in practice wang and sun 2018 this expense motivates the use of so called sequential methods here calculations are typically made offline to create a look up table the look up table is then called online using an appropriate interpolater where necessary the downsides of the look up table approach are we acquire little knowledge about the function the quality of our dataset and how well we may generalise to samples not included in the table alternatively we can learn a surrogate model offline given our input output data and other possible information learning such mappings is one of the fundamental goals of machine learning ml more generally ml encompasses a range of algorithms to learn models from data and as a discipline has seen widespread interest within the geoscience community to address a variety of modelling challenges razavi et al 2012 lary et al 2016 tahmasebi et al 2020 machine learning approaches to generating surrogate constitutive models can be traced back several decades for example in the pioneering work of ghaboussi et al 1991 the authors used so called neural networks to represent the constitutive behaviour of concrete under various loading conditions from this and other similar early works goh 1995 ghaboussi et al 1998 users have incorporated data driven surrogate models within multiscale modelling strategies for a variety of applications unger and könke 2009 hambli et al 2011 asproulis and drikakis 2013 more recently work has been done for modelling time dependent constitutive information using so called recurrent neural networks wang and sun 2018 ghavamian and simone 2019 chen 2021 despite the success of these recurrent algorithms for sequential modelling they can be challenging to implement and to understand pascanu et al 2013 bai et al 2018 further bai et al 2018 showed more recently autoregressive feedforward neural network architectures that give superior performance compared to recurrent structures for a wide variety of sequence modelling tasks given the variety of multiscale problems and machine learning methods it would be useful to have a framework detailing the key questions and considerations for the generation and use of surrogate constitutive models within sequential multiscale approaches the aim and novelty of this work is to introduce and apply a machine learning based multiscale framework for modelling complex constitutive behaviours such as those due to nonlinearity and hysteresis both of these phenomena are commonly observed in subsurface resource modelling our framework is broken down into four key components with the starting point being a homogenisation procedure here we use a two scale homogenisation approach such as those forming the basis for computational homogenisation methodologies in this framework the homogenisation procedure plays a fundamental role in generating consistent data to learn from further the procedure also allows insight into the quantities involved in the constitutive mapping with the scale transition rules from the homogenisation procedure in place the remaining parts of the framework involve generating the data learning the model and coupling the result to a physics based simulator for our application we consider the diffusive mass transfer involved in dual porosity or dual continuum more generally flow modelling in the dual porosity dp setting a strong contrast in conductivities exists between the different porosity levels a classical example of such a material is naturally fractured rock here the matrix has significantly lower permeability than the fractures as a result the matrix introduces a history dependent memory effect associated with the mass exchange between the two porosity levels accordingly in the presence of fracture dynamics the inter porosity exchange is non local in time requiring a convolutional product for its description further even for cases involving static fracture pressures challenges still remain in describing inter porosity exchange for these latter cases and for simple geometries analytical solutions for the inter porosity exchange term are possible however solutions for these static fracture pressure cases are still explicit in time and are formed from a complex infinite series accordingly describing inter porosity flow for both static and dynamic fracture behaviours remains difficult with traditional simulation methods these challenges necessitate simpler descriptions for inter porosity exchange leading to the commonly used linear description involving the difference between averaged pressures warren and root 1963 however this simplification is known to lead to measurable errors in flow behaviour particularly at early time zimmerman et al 1993 ashworth and doster 2020 instead of making any such assumptions we use a data driven constitutive model learnt on time series data coming directly from the microscale in line with the aims of this work to introduce and apply our framework we consider only cases of static fracture pressure accordingly we can give an overview of the framework on a problem without significant complexity using easy to generate data and established learning algorithms we subsequently compare various nonlinear feedforward autoregressive algorithms approaches to model the time dependent problem the successful model is then coupled to a dual porosity simulator in the matlab reservoir simulation toolbox mrst lie et al 2012 lie 2019 lastly in the context of this work it is worth mentioning the interesting work done by andrianov and nick 2021 for this the authors use ml to learn the mapping between images of pixelated fracture geometries and dp constitutive parameters in contrast taking inspiration from computational homogenisation approaches our goal here is to provide a framework to learn and apply a constitutive mapping itself we structure the remainder of this paper as follows in section 2 we introduce the framework detailing the key elements and considerations therein in section 3 we start the application of the framework through the two scale homogenisation approach to derive the macroscopic dual porosity model in section 4 we introduce a dual porosity model problem that will serve as the testbed for the remaining parts of the multiscale framework further given this model problem we derive an often used linear transfer model in section 5 we generate the dataset for learning to do so we use analytical solutions for the given model problem whilst also considering the end use of the ml model in a numerical simulator in section 6 we learn our surrogate constitutive model comparing a variety of learning algorithms for modelling the time dependent problem in section 7 we couple the resulting ml model to a physics based simulator leading to a hybrid ml physics approach we then test the resulting hybrid approach against a dual porosity model equipped with the traditionally used linear transfer model and microscale simulations in section 8 we finish with conclusions and recommendations for future work lastly for reproducibility the codes used in this work are available as open source 1 1 https github com mashworth11 ml mm 2 machine learning based constitutive modelling framework here we introduce the ml based multiscale constitutive modelling framework and the key concepts therein the framework itself can be broken into four key components homogenisation data generation surrogate constitutive model learning and ml and physics model coupling fig 1 however we stress that these components are interrelated and should be understood in parallel as opposed to isolated procedures 2 1 homogenisation homogenisation is concerned with the question how do we link between micro and macroscales providing constitutive information from the former to the latter various approaches exist for linking between scales such as asymptotic homogenisation sánchez palencia 1980 auriault et al 2010 the method of volume averaging quintard and whitaker 1993 whitaker 1999 and computational homogenisation approaches feyel 1999 kouznetsova et al 2001 depending on underlying requirements e g a priori knowledge for scaling parameters in the case of asymptotic approaches and resources some upscaling strategies may be more suitable than others regardless within the context of this framework the homogenisation procedure provides the guide for generating consistent offline data for training and validating a machine learning model we note one could consider the homogenisation step as part of the data generation process however we choose to distinguish these two steps within the current framework to highlight their respective importance and considerations therein 2 2 data generation data generation is concerned with the question how will we generate sample data for our learning algorithm microscale results can be expensive when numerically derived due to limited computational resources and or high problem dimensionality managing this trade off to ensure the dataset sample set is as informative as possible is referred to as experimental design or design of experiments razavi et al 2012 traditional approaches to experimental design involve the use of various sampling strategies e g monte carlo latin hypercube quasi monte carlo methods to build the dataset prior to learning however these so called one shot strategies can either contain too few samples for an accurate surrogate undersampling or more samples than we need oversampling thus wasting precious resources xiao et al 2018 an alternative to one shot approaches is to use feedback from the ml training process to find samples that provide the maximum information gain when labelled and used to update the model these so called active learning or sequential sampling approaches can address the under oversampling trade off described previously crombecq 2011 in addition to data driven ml approaches recent advances have seen the development of theory driven ml approaches here the idea is to use physics such as partial differential equations initial and boundary condition information symmetry or invariance properties and thermodynamical considerations ling et al 2016 wang and sun 2018 raissi et al 2019 zhu et al 2019 masi et al 2021 to constrain the learning process accordingly we can supplement missing or difficult to obtain data further theory driven approaches also act to regularise the learning process and ensure the physical consistency of our model however in this work we consider purely data driven methods and save the incorporation of theory into the learning process for future studies 2 3 surrogate constitutive model learning surrogate constitutive model learning is concerned with the question given the properties of our data the type of prediction problem and the end use for the resulting ml model what is the most appropriate learning strategy to use training ml algorithms can be expensive from both data and computational points of view if our sample set is poor complex algorithms run the risk of failing to generalise when given new data overfitting conversely if our model is too simple we risk failing to extract meaningful relationships from our data underfitting besides addressing underfitting and overfitting we may also want algorithms that are intrinsically interpretable and quick to retrain in light of new data e g linear regression tree based methods give us an insight into aleatoric data and or epistemic model uncertainties e g gaussian processes bayesian learning and provide an embedded approach to feature input engineering in light of complex and or high dimensional data e g deep learning ultimately within the context of multiscale modelling the choice of learning method should be guided by considering the interdependence between data quality and availability ml model accuracy and practicality and the model s end use in physics based simulation 2 4 model coupling model coupling is concerned with the question how will we use our learnt constitutive model in our macroscale physics simulator ideally the ml model can be implemented within a numerical solver without significant intrusion accordingly important considerations at this stage may be the accessibility of data during simulation to pass to the ml model the ml model inference time how predictions will be fed into the numerical model and in the case of nonlinear materials how we extract state dependent material properties such as permeability and stiffness from our surrogate constitutive model these latter points will depend on whether the constitutive model is to be used in an explicit or implicit manner we describe the former as those predictions made using previously observed and converged quantities subsequently in a nonlinear solver the resulting surrogate constitutive model will only appear in the residual alternatively if the prediction also depends on quantities at the current iteration level parameters from the surrogate associated with these quantities will be needed in the jacobian for example in hashash et al 2004 the authors consider nonlinear deformation problems and derive a closed form expression for calculating the tangent stiffness tensor from a neural network accordingly the authors provide consistent jacobian entries for use in a newton raphson scheme for this work our surrogate will be explicitly coupled due to the nature of the constitutive relation being injected nonetheless the example above highlights the type of considerations we need to make in order to embed ml models within traditional physics based simulations 3 application of the framework two scale homogenisation for dual porosity flow problems with the details of the framework in hand we now progress to its application for our application we consider problems associated with dual porosity flow such as those observed in multiscale geomaterials accordingly in this section we derive the dual porosity flow model by way of a two scale homogenisation approach developed for transient diffusion problems larsson et al 2010 ramos et al 2017 brassart and stainier 2019 waseem et al 2020 3 1 preliminaries the homogenisation procedure relies on the definition of representative elementary volumes ω rev taken from a macroscopic domain ω accordingly we are interested in quantities varying at two distinct scales those variations at points within an rev x ω and those variations at material points within the macroscopic body x ω note quantities written with x denote macroscopic quantities the rev is defined according to a separation of scales in terms of geometrical information and the wavelengths of physical processes involved formally the separation of scales is defined as 1 ℓ l l where ℓ l and l denote the characteristic lengths of the heterogeneities rev and macroscopic body respectively our reference material is naturally fractured rock accordingly the material is composed of a low permeability matrix denoted by subscript 1 and a high permeability fracture material denoted by subscript 2 we assume homogeneity and isotropy of each material as well as isotropic geometrical distributions at the microscale next we introduce the following volume averaging relations 2 z 1 ω ω z d v z α 1 ω α ω α z d v where z is an arbitrary field quantity accordingly denotes the volume average over the whole rev domain whilst α denotes the volume average over the constituent volume ω α ω the two volume averages defined in eq 2 are related such that 3 z v 1 z 1 v 2 z 2 where v α ω α ω is the volume fraction of phase α lastly we define the following relations between rates of change and their averages such that 4 z z t t z z 3 2 mass balance at the macro and microscales the dual porosity model is a multi continuum model at the macroscale described by the continuum wise mass balance as 5 m 1 w 1 r 1 6 m 2 w 2 r 2 where m α w α and r α denote the rate of macroscopic mass content change mass flux vector and inter continuum rate of mass transfer for continuum α respectively further mass conservation requires r 2 r 1 in addition to appropriate boundary and initial conditions we require constitutive relations for the macroscopic quantities m α w α and r α the simplest approach to specify these closures is through phenomenological arguments alternatively we can derive closure relations through homogenisation in this regard asymptotic homogenisation has been successful in deriving a closed dual porosity model starting at the microscale royer and auriault 1994 these results thus serve as a useful reference to the approach introduced herein at the microscale we have the following mass balance 7 m w 0 where m and w denote the rate of microscopic mass content change and mass flux vector respectively additionally at this scale we assume both materials are saturated by the same slightly compressible fluid of density ρ under isothermal flows we define the fluid compressibility relation 8 c l 1 ρ ρ p where p denotes the microscopic pressure a similar compressibility relation is defined for the local porosity ϕ ω p ω where ω p is the pore volume such that 9 c ϕ 1 ϕ ϕ p we assume small perturbations in fluid density and porosity allowing linearised evolutions of these quantities further the small perturbation hypothesis permits the replacement of ρ and ϕ with ρ 0 and ϕ 0 respectively where necessary notations with superscript 0 denote reference conditions from m ρ ϕ and w ρ q where q is the microscopic volume flux eq 7 becomes 10 ρ 0 ϕ 0 c p ρ 0 q 0 where we have made use of the small perturbation hypotheses and where c c l c ϕ lastly the microscopic volume flux is related to the microscopic pressure gradient through darcy s law neglecting gravitational effects 11 q k μ p where μ is the fluid viscosity for the isotropic case k k i in which i denotes the identity matrix to link the macroscopic description in eqs 5 to 6 to the microscale description in eq 7 we make use of a two scale homogenisation approach this approach is split into two steps downscaling involves translation of macroscopic loading parameters macroscopic pressures and gradients to consistent constraints and boundary conditions at the microscale upscaling involves transformation of microscopic rates of change and fluxes to their macroscopic counterparts through averaging we address the downscaling step in the section to follow 3 3 downscaling for a dual porosity material associated to a macroscopic point x we have two macroscopic pressures p 1 p 2 and gradients p 1 p 2 for each constituent we then define microscopic pressure fields according to taylor series expansions of the first order about the macroscopic point 12 p x x t p 1 x t p 1 x t x x p x x t in ω 1 13 p x x t p 2 x t p 2 x t x x p x x t in ω 2 where p denote higher order expansion terms corresponding to pressure fluctuations at the microscale we note the microscale is positioned relative to the macroscopic point such that x x 0 which also holds for constituent averages from eqs 12 to 13 we require the following compatibilities to hold 14 p 1 p 1 15 p 2 p 2 corresponding to the requirement p α 0 from özdemir et al 2008a eqs 14 to 15 are related to requiring consistency between the microscopic and macroscopic mass contents such that 16 ρ ϕ α c α p α ρ ϕ c p α where ϕ α ω α p ω v α ϕ α is the effective continuum porosity from eq 16 we propose the following consistency conditions for the mass storage 17 ρ ϕ α c α ρ ϕ c which is inline with results obtained through asymptotic homogenisation auriault 1983 royer and auriault 1994 in addition to constraints provide by eqs 14 to 15 we require averages of microscopic gradients to equal their macroscopic counterparts accordingly taking the gradients of eqs 12 to 13 and averaging over each constituent leads to 18 p 1 p 1 p 1 in ω 1 19 p 2 p 2 p 2 in ω 2 where we use the following relation x x i to fulfil the compatibility between gradients we require p α 0 for both constituents application of gauss divergence theorem on averages of the fluctuation gradients leads to 20 ω α p n d s 0 where ω α is the boundary of ω α and n is the outward unit normal vector condition eq 20 is satisfied by setting certain restrictions boundary conditions on the fluctuations at the boundaries examples of such boundary conditions include periodic boundary conditions and prescribed fluctuation boundary conditions in this work we use the latter for demonstrative purposes although the former is often used for flow based upscaling procedures lie 2019 accordingly prescribed fluctuation boundary conditions correspond to setting p 0 leading to 21 p x x t p α x t p α x t x x on ω α from either eq 12 or eq 13 in the dual porosity formulation we assume the matrix material to be completely permeated by the fracture material additionally we assume continuity of the microscopic pressure field at the interface γ ω 1 between the two materials accordingly boundary conditions for both materials can be completely defined by considering the prescribed fluctuation boundary conditions for the fracture material 22 p x x t p 2 x t p 2 x t x x on ω 2 we note eq 22 is somewhat similar to the boundary conditions specified by the asymptotic homogenisation approach on γ where p p 2 on γ royer and auriault 1994 however here the explicit size of the rev and macroscopic fracture pressure gradient is taken into account nonetheless for small macroscopic gradients and vanishingly small rev sizes p p 2 on γ is a reasonable approximation with the required downscaling relations we can now proceed to upscaling 3 4 upscaling upscaling for the macroscopic quantities m α w α is achieved through a virtual power equivalence relation between the micro and macroscales this relation is referred to as the generalised hill mandel condition blanco et al 2016 and requires the macroscopic virtual power to equal its volume averaged microscopic counterpart at a given material point here we introduce a virtual power relation for the dual porosity material by summing the external powers coming from eqs 5 to 6 and using δ p 1 r 1 δ p 2 r 2 0 to give 23 δ p 1 m 1 δ p 1 w 1 δ p 2 m 2 δ p 2 w 2 δ p m δ p w where we use eq 7 at the microscale for compactness use of the decomposition in eq 3 on eq 23 followed by substitution of the appropriate expansions in eqs 12 to 13 leads to 24 α 1 2 δ p α m α δ p α w α v 1 δ p 1 δ p 1 x x δ p m δ p 1 δ p w 1 v 2 δ p 2 δ p 2 x x δ p m δ p 2 δ p w 2 rearranging eq 24 to isolate terms involving fluctuations gives 25 α 1 2 δ p α m α δ p α w α v 1 δ p 1 m δ p 1 w x x m 1 v 2 δ p 2 m δ p 2 w x x m 2 v 1 δ p m δ p w 1 v 2 δ p m δ p w 2 the terms in eq 25 involving the pressure fluctuations represents the weak form of the phase wise microfluctuation balance of mass these terms are subsequently addressed through the chain rule and the divergence theorem leading to 26 v 1 δ p m δ p w 1 v 2 δ p m δ p w 2 δ p m w ω 1 p n d s ω 2 p n d s accordingly the first term on the right hand side of eq 26 is satisfied due to the microscopic conservation of mass eq 7 the remaining terms then vanish with the appropriate boundary conditions in this case the prescribed fluctuation boundary conditions having dealt with the fluctuation terms we seek to obtain a dual porosity formulation from the remaining terms in eq 25 this goal is pursued in the following section 3 5 recovery of the dual porosity model to recover a dual porosity model from the virtual power equivalence we assume quasi steady state in the fracture material due to the high phase permeability accordingly 27 w ρ 0 q 0 in ω 2 whilst transient diffusion effects remain in the matrix material at the microscale however due to the percolating fractures and the low intrinsic matrix permeability we assume w 1 0 with the following assumptions at the microscale we recover from eq 25 28 δ p 1 m 1 δ p 1 w 1 δ p 2 m 2 δ p 2 w 2 δ p 1 m 1 v 1 δ p 1 x x m 1 v 1 δ p 2 w 2 v 2 comparing terms in eq 28 we identify the following relations 29 m 1 v 1 m 1 ρ 0 ϕ 1 0 c 1 p 1 30 w 1 v 1 x x m 1 31 w 2 v 2 w 2 ρ 0 v 2 q 2 where fluid density at reference conditions at the microscale corresponds to the equivalent quantity at the macroscale i e ρ 0 ρ 0 further for the small perturbation considerations used here c α c α looking first at the averaged quantity in eq 30 we identify this as the first moment of the rate of pressure change in the matrix interestingly this term is not present in the classical asymptotic homogenisation approach royer and auriault 1994 physically this term introduces a size effect and represents the microscale mass inertia due to the low permeability of the matrix material however in general this inertial term is shown to be negligible with respect to the macroscopic flux from the fractures brassart and stainier 2019 further for isotropically distributed materials at the microscale this term is shown to vanish brassart and stainier 2019 from eq 31 we can identify the macroscopic volumetric fracture flux as 32 q 2 v 2 q 2 q where the relation on the right hand side of eq 32 arises due to the assumption v 1 q 1 0 and from eq 3 next using eq 11 in eq 32 we write 33 q 2 k 2 μ p 2 where k 2 denotes the macroscopic effective fracture permeability tensor accordingly we can recover entries to this permeability tensor provided we can calculate the macroscopic fracture flux corresponding to the macroscopic pressure gradient such calculations are addressed in so called flow based upscaling procedures lie 2019 finally eq 29 shows the relation between the micro and macroscopic rate of mass content change from eq 4 we see 34 ρ 0 ϕ 1 0 c 1 p 1 ρ 0 ϕ 1 0 c 1 p 1 accordingly with eq 29 resp eq 34 and eq 31 in eqs 5 to 6 whilst neglecting the inertial effects from eq 30 we recover the dual porosity model from the two scale homogenisation procedure as 35 ρ 0 ϕ 1 0 c 1 p 1 r 1 36 ρ 0 ϕ 2 0 c 2 p 2 ρ 0 v 2 q 2 r 1 where we use the compatibility in eq 16 for the fracture rate of mass content change m 2 in this work our focus is in the modelling of the inter porosity transfer term r 1 from eq 35 we see r 1 ρ 0 ϕ 1 0 c 1 p 1 subsequently to compute r 1 all we need is the solution to the microscale boundary value problem eq 10 in the matrix given boundary conditions from eq 22 on γ and some initial condition p 1 x x t 0 p 1 0 however often we not have direct access to the microscale within a macroscopic simulation further even if the solution could be calculated directly a priori it is difficult to use in practice owing to its complex form specifically for evolving fracture pressures the matrix pressure solution is defined as a convolution product over the fracture pressure history royer and auriault 1994 even for static fracture pressures following a step increase the matrix pressure solution is still an infinite series defined in explicit time shown in the following section due to the described complexities it is common to adopt a simple linear relation to model r 1 such simplification leads to measurable errors in flow behaviour accordingly in the remainder of this work we aim to improve on linear transfer models through machine learnt constitutive models which incorporate temporal information using quantities directly available during simulation 4 a dual porosity model problem and recovery of a first order inter porosity mass transfer model with the homogenisation step in hand we now have a guideline for how to generate data to learn from accordingly we proceed to the remaining steps of the multiscale framework to do so we first consider a model problem for which data can be generated using analytical solutions further from these solutions we can obtain a first order mass transfer model that is commonly used in dual continuum flow modelling this simplified transfer model will then serve as a reference for testing any data driven model coupled to a numerical simulator we note that whilst the problem considered here is not too complex it serves as a useful demonstration for the remaining steps of the framework and to explore various established machine learning algorithms for modelling the time dependent mass transfer problem future directions therefore concern numerically derived data involving fracture dynamics more complicated microscale geometries and more advanced methods for surrogate modelling e g active learning physics informed models etc in this latter element work combining the data driven parameterisations from andrianov and nick 2021 with elements of the framework introduced herein could be an interesting path for further research 4 1 model problem for the model problem we consider a two dimensional matrix domain surrounded by fractures fig 2 a at a time t 0 the matrix and fractures are in hydrostatic equilibrium then following a jump in fracture pressure at a time t 0 the fracture pressure remains fixed leading to p 2 0 fig 2 b accordingly from eq 21 and the pressure continuity across the interface we have 37 p x x t p 2 x t on ω 1 with initial conditions 38 p x x t 0 p 1 0 x x in ω 1 due to the considered fracture pressure boundary conditions and simple geometry analytical solutions exist for calculating the solution these solutions are introduced subsequently 4 2 analytical solution and first order mass transfer model the analytical solution to the 2d matrix diffusion problem described above is found by taking the product of the dimensionless solution to a 1d diffusion problem such as those arising in slab geometries crank 1979 holman 1990 lim and aziz 1995 averaging the solution over the rev and noting the equivalence with macroscopic quantities then leads to 39 p 1 p 1 0 p 2 p 1 0 1 n 0 m 0 8 π 2 2 1 2 n 1 2 2 m 1 2 exp π 2 k 1 t ϕ 1 0 c 1 μ ℓ 2 2 n 1 2 2 m 1 2 where we use p 1 p 1 next taking the time derivative of eq 39 gives 40 p 1 p 2 p 1 0 n 0 m 0 8 π 2 2 π 2 k 1 ϕ 1 0 c 1 μ ℓ 2 1 2 n 1 2 1 2 m 1 2 exp π 2 k 1 t ϕ 1 0 c 1 μ ℓ 2 2 n 1 2 2 m 1 2 substitution of eq 40 into eq 35 gives r 1 however the resulting mass transfer expression is unsuitable for simulation purposes given the presence of explicit time and the infinite series to alleviate these dependencies we can recover a first order approximation to eq 40 accordingly taking the first term from eq 40 and eliminating t using the first term from eq 39 gives 41 p 1 2 π 2 k 1 ϕ 1 0 c 1 μ ℓ 2 p 2 p 1 substitution of eq 41 into eq 35 leads to the following first order inter porosity transfer model zimmerman et al 1993 lim and aziz 1995 42 r 1 ρ 0 v 1 η k 1 μ p 2 p 1 where η n f π 2 ℓ 2 denotes the so called shape factor in which n f is the number of characteristic fracture sets several investigations have shown use of the linear constitutive model in eq 42 to lead to measurable errors in flow behaviour zimmerman et al 1993 ashworth and doster 2020 one common way to alleviate these inaccuracies is to use multi rate transfer models haggerty and gorelick 1995 geiger et al 2013 however in this work we treat the problem differently instead we use machine learning to bridge the scales in the form of a surrogate constitutive model 5 application of the framework data generation our goal for the supervised learning problem is to learn a mapping from quantities available during simulation to an output aligned with our constitutive property to do so we consider a dataset d of n samples d x i y i i 1 n where x i r d i n is a vector of inputs and y i r d o u t is a vector quantity derived from the microscale by way of our upscaling procedure learning a mapping between inputs and outputs then equates to learning a function of the form 43 y i f x i f ˆ x i as a result we can incorporate local scale physics such as those coming from the analytical solution or numerical simulations within macroscopic models without having to formulate explicit phenomenological expressions we frame our learning problem considering the physics of the process inputs coming from simulation and an output that is aligned with the discrete structure of the numerical problem for the latter we specify our output following application of the backward euler method commonly used for time discretisation in numerical modelling specifically 44 p 1 t p 1 t 1 p 1 t δ t from eq 44 our goal is to predict the pressure at the next time step 45 y i t 1 p 1 t 1 at simulation time we incorporate the resulting prediction into the finite difference calculation in eq 44 at each time step accordingly our learning problem is formulated on the basis of time series calculated with eq 39 each individual time series corresponds to a different initial matrix pressure p 1 0 we take 120 different p 1 0 sampled with logarithmic spacing from the interval of 1 1 1 0 6 pa every series starts from 0 and finishes at a final time of t 100 s with a uniform stepping interval of 0 1 s the remaining parameters in eq 39 are fixed as k 1 0 5 md ϕ 1 0 0 2 c 1 1 4 gpa 1 μ 1 cp ℓ 1 m and following the step in p 2 at t 0 p 2 1 mpa a plot of each series s as time vs y s is shown in fig 3 a further fig 3 b shows how this time series data is split into training and testing data using a 2 3 1 3 split respectively accordingly every third sequence corresponds to a test series 6 application of the framework surrogate constitutive model learning in this section we review and compare the learning algorithms used to generate our surrogate constitutive model specifically we introduce the various approaches and considerations for modelling the time dependent problem to follow we train and test the different modelling approaches on our data the results from this section will then be carried on to the next stage of the framework i e coupling the data driven model to a numerical simulator 6 1 autoregressive approaches to start we introduce the modelling description used to account for time dependency that is we model the sequential problem using an autoregressive description in the following we present autoregressive descriptions in the context of feedforward models in feedforward approaches the information flow is unidirectional from the current inputs to outputs this modelling approach is contrary to recurrent models in which the information flow comes from both the current inputs and from calculations made on inputs at previous time steps for a multivariable feedforward autoregressive model f ˆ uses previous series outputs to predict the next output in the series accordingly for a two variable univariate feedforward autoregressive model 46 y ˆ i t 1 f ˆ x i t f ˆ y ˆ i t y ˆ i t d y 1 u i t u i t d u 1 where u i denotes an external series and d y and d u denote the number of sequence terms or dependency length for the internal and external series variables respectively in this work we set d y d u d our external series corresponds to the boundary conditions for the matrix provided by the homogenisation approach given the model problem described in section 4 specifically previous sequence values for p 2 corresponding to p 2 p 1 0 when t t 0 and p 2 1 mpa or any selected pressure step value when t t 0 finally we note that d y and d u or d are defined a priori as a result potentially important long range information is lost if short dependency lengths are chosen one option to alleviate the challenges posed by fixed dependency lengths would be to use recurrent neural networks such as the long short term memory network for feedforward autoregressive training we treat the learning task as a single step ahead prediction problem in this strategy the idea is to use lagged ground truths targets y i in place of the predicted values y ˆ i in eq 46 autoregressive testing is treated according to how the ml model will be used when coupled to the macroscopic simulator that is as a multi step ahead prediction problem accordingly the trained single step ahead models are used in a recursive manner with predictions y ˆ at previous time steps fed back into the inputs to predict the next time step as per eq 46 the single step ahead training and testing strategy can work provided the training error is sufficiently small such that accumulated errors do not explode when using the model as a multi step ahead predictor when this requirement on the accumulated errors cannot be satisfied which is difficult to determine a priori iterative approaches are unstable in such cases multi output strategies that predict the whole forecast horizon at once can be used however the stability of multi output regressors often comes at the cost of predictive accuracy particularly for long forecast horizons sangiorgio and dercole 2020 in the section to follow we consider two ml algorithms for feedforward autoregressive modelling specifically polynomial regression pr and a fully connected neural network fc nn 6 2 polynomial regression here we introduce the polynomial regression algorithm these algorithms are attractive because they are easy to understand and fast to train however a noteable downside to these methods is the curse of dimensionality for large input vectors nonetheless pr algorithms are a simple place to start for many regression tasks and serve as a good baseline model accordingly under pr f ˆ is formulated as 47 y ˆ i f ˆ x i w θ x i where θ θ 1 θ n p is the vector of polynomial basis functions acting on x i and w r n p is a vector of weights parameters of the model in this work we use a second order polynomial feature transformation which comes as a standard tool in many machine statistical learning packages whilst the inputs are polynomial following a transformation in eq 47 the weights remain linear finding w is simply then a linear regression problem and is the goal of training in supervised learning problems training corresponds to minimising a loss function l that represents the error in approximating f by f ˆ on our data for the regression problem we use a squared loss such that 48 l w 1 n y f ˆ θ w 2 2 where θ θ x 1 θ x 2 θ x n and y y 1 y 2 y n for the univariate target case finding the optimal set of weights that minimise eq 48 is defined formally as 49 arg min w l w arg min w 1 n y f ˆ θ w 2 2 the squared loss in linear regression is a convex function which admits a global minimum in w at the point l 0 accordingly 50 θ θ w θ y leading to 51 w θ y where θ θ θ 1 θ pr is well known to suffer from high degrees of multicollinearity dalal and zickar 2012 as a result θ θ is ill conditioned and the solution in eq 51 is non unique for prediction problems such ill conditioning is generally not a concern dormann et al 2013 since new data comes from the same population as the training data however for problems of extrapolation such as multi step ahead prediction collinearity between inputs can change due to accumulating errors such changes can affect predictions on models arising from ill conditioned matrices and should be addressed dormann et al 2013 we therefore address multicollinearity between inputs using regularisation which aims to restrict the space of viable models that fit our data e g in the face of ill conditioning there are various forms of regularisation however in this work we add the following penalisation γ w 2 2 term to eq 48 leading to so called ridge regression the parameter γ in the penalty then controls the degree of weight penalisation 6 3 fully connected neural networks here we introduce the fully connected neural network model aside from the fc nn considered in this work neural networks more generally have become a popular choice for surrogate modelling due to their great flexibility scalability and significant advancement within research communities further contrary to the polynomial regression where the feature transforms are defined a priori in neural networks these transformations are implicit within the algorithm and learnt from the data to proceed within an fc nn information is fed forward from an input layer through hidden layers l 1 l 1 finishing at an output layer l l accordingly under an fc nn f ˆ is formulated as 52 y ˆ i f ˆ x i s l s l 1 s 1 x i for a given layer l s l is given as an affine transformation followed by an element wise nonlinear function such that 53 s l x h l w l x b l where w l r n l 1 n l and b l r n l are the weight matrix and bias parameter vector associated with layer l respectively evidently for the univariate prediction considered herein the weight matrix for the last layer will be a weight vector w l w l notation h is the nonlinear function referred to as an activation function in this work we take h l as a linear activation and all others as so called exponential linear units in training the fc nn we minimise the customary squared loss of the form 54 l λ 1 n y f ˆ x λ 2 2 where λ w 1 b 1 w l b l and x x 1 x 2 x n however unlike eq 48 with neural networks the loss function is non convex as a result weights and biases are updated iteratively using a chosen optimiser together with the backpropagation algorithm see goodfellow et al 2016 for further details in this work we found stochastic gradient descent with nesterov momentum to work the best nesterov 1983 we experimented with various model depths and widths and found the best results came using a simple single hidden layer fc nn with 12 hidden units additionally with this architecture there was little overfitting for the single step problem and hence no need for regularisation our fc nn was implemented using the keras api chollet 2015 lastly we construct the datasets used by these models as d x y such that x x 1 1 x 2 1 x 1000 s and y y 1 1 y 2 1 y 1000 s superscripts on the input matrix and target vector entries denote the time series label with s 120 being the total number of series subscripts then denote the time step within a given series 6 4 training and testing in the following we present some additional considerations for model training and interpretation then we present the results and discussions for the model training single step ahead and testing multi step ahead procedures considerations for our models we consider a dependency length of d 3 for the inputs although not studied in detail here d is a hyperparameter that can be tuned according to the trade off between gain in accuracy and complexity the weight penalty term γ for the regularised polynomial regression rpr was found to give the best results for γ 1 1 0 7 we present results comparing the impact of different values for γ for the multi step ahead problem on both training and testing sets in the section to follow when training our fc nn model we used a batch size of 128 run over 500 epochs further we standardise our data to have a mean of zero and a standard deviation of one standardisation during testing is then done using the mean and standard deviation calculated on the training set to evaluate between the different models we consider one minus the normalised root mean square error given as 55 1 n r m s e 1 r m s e σ n 1 i 1 n y i y 2 n 1 1 i 1 n y i y 2 where σ and y represent the sample standard deviation and mean of the train or test target set respectively from eq 55 the closer the value to one the more accurate the algorithm lastly to highlight the effects of multicollinearity we present results for the polynomial regression model both with and without regularisation results fig 4 shows the train and test results using the unregularised polynomial regression algorithm from fig 4 a we can see there is a good match when training as a single step ahead prediction problem the quality of this match is corroborated by the high evaluation metric and low rmse respectively shown in table 1 however during testing the prediction quality deteriorates particularly for sequences associated with high initial pressures fig 4 b further for the series corresponding to the highest initial pressure we observe unbounded error growth with the prediction rapidly going negative accordingly this latter unbounded prediction leads to the missing entries for the error metrics for the test problem shown in table 1 we hypothesised errors in the multi step ahead case with this model to be caused by the multicollinearity existing between the transformed features as described in section 6 2 accordingly to address this problem we introduced a regularised pr by way of a penalisation term on the loss results for the regularised pr are shown in fig 5 and table 1 the training results shown in fig 5 a are virtually identical to those shown in fig 4 a for the unregularised pr case table 1 however we can observe measurable differences on the multi step ahead test set between the two polynomial regression models from fig 5 b we see the regularised pr predictions are qualitatively closer to the targets contrary to fig 4 b further we do not observe the unbounded predictions shown in the latter figure looking at the error metrics for the regularised pr model in table 1 we see the rmse is significantly higher for the testing set such differences are expected given the accumulation of errors with the higher test error likely due to the small discrepancies observed at late times in fig 5 b nonetheless the results using the simple regularised polynomial regression model are both qualitatively and quantitatively reasonable serving as a good benchmark for the more complex methods to follow fig 6 shows the penalisation parameter tuning for the regularised pr accordingly we observe as γ increases from γ 1 1 0 9 to γ 1 1 0 7 the rmse on both the training and testing sets treated as both multi step ahead problems for this demonstration decreases however from γ 1 1 0 7 to γ 1 1 0 6 we can see a sharp increase in rmse indeed past γ 1 1 0 6 our rmse becomes unbounded as the accumulating errors become significant the results from fig 6 support our use of γ 1 1 0 7 for the weight penalisation lastly fig 7 and table 1 show the train and test results using the fc nn from fig 7 a we see a good match between the training predictions and targets however comparison of the quantitative training results in table 1 between the fc nn and the rpr suggests underfitting with the former from fig 7 b we can observe good qualitative matches between the predictions and targets over early time however from middle to late times the predictions diverge from the targets suggesting the effects of accumulating errors on the multi step predictions over these time periods the resulting degradation in performance is reflected in the error metrics in table 1 for the fc nn on the testing case we anticipate this diverging prediction behaviour to be a consequence of the underfitting observed with the fc nn possible solutions to the underfitting problem include increasing the complexity of the model and or using more data for the former we found increasing the width and depth of the network to add little benefit alternatively whilst increasing the availability of data is trivial for this problem it may not be practical for expensive to generate data regimes instead we hypothesise the underfitting in this case to be down to the optimisation process used when learning the fc nn specifically even for convex loss landscapes solutions achieved by gradient based optimisers are unlikely to represent the optimum solution due to the sensitivity of their performance on hyperparameters such as the learning rate nonetheless when learning on more complex loss landscapes these solutions are often good enough for example in computer vision and natural language processing deep neural networks remain the state of the art however for current purposes in light of accumulating errors in the multi step ahead prediction problem we require a high degree of accuracy from our learnt surrogate constitutive model accordingly in contrast to the fc nn the weights in the rpr are typically obtained using highly accurate matrix factorisation methods that lead to globally optimal solutions to the convex optimisation problem it is worth noting however despite its performance compared to the pr model the fc nn does not suffer from the same multicollinearity problems as the latter such a feature may be useful for future investigations involving neural networks in such scientific problems in summary we have shown training and testing results for the time dependent problem using several data driven modelling strategies we have seen that complexity does not always result in superior performance with the simple regularised polynomial regression model outperforming the neural network based approach on both training and testing problems indeed as part of this work we also experimented with fitting even more complex neural networks so called encoder decoder long short term memory networks 2 2 see https github com mashworth11 ml mm for further details however we found these recurrent neural networks more involved to fit in practice with the results falling short of the performance of the polynomial regression models for more complex problems the additional effort required to fit such models may be warranted however when learning from expensive to generate data we envisage a key challenge for machine learning based multiscale modelling to be in the sample efficiency of learning given the results shown in this section we use the regularised pr model for the next stage of the framework application 7 application of the framework model coupling here we couple our trained surrogate constitutive model to a macroscopic physics based model we test the subsequent hybrid ml physics model on a variety of cases against a traditional physics based model that uses the linear transfer and a microscale model 7 1 hybrid ml physics model our physics based numerical model is the discretised counterpart to eqs 35 to 36 time discretisation is achieved using the backward euler method as per eq 44 discretisation for the pressure gradient is done using the finite volume method by way of the two point flux approximation the latter is implemented as standard within mrst given the propensity for nonlinear phenomena in subsurface flow e g due to compressibility it is common to use nonlinear solvers accordingly we give eqs 35 to 36 in discrete block matrix form following application of newton s method as 56 f 1 g 1 g 2 f 2 k δ p ˆ 1 δ p ˆ 2 t 1 k 1 r 1 r 2 t 1 k where δ and k now denote the change in solution and current iteration levels respectively the flow matrix in the jacobian is f α q α δ t t α where q α is the compressibility matrix and t α is the transmissibility matrix see lie 2019 for details for the matrix material in the dual porosity setting the transmissibility matrix has zero entries matrix g α is the coupling matrix arising from an inter porosity flow expression such as the linear approximation in eq 42 see ashworth and doster 2020 for details note terms arising from the linear transfer approximation will also appear in the flow matrices the solution vectors are p ˆ α p ˆ α 1 p ˆ α n c e l l where p ˆ α denotes the cell wise pressure solution for continuum α notation n c e l l is then the number of grid cells control volumes next r α t 1 k r α 1 r α n c e l l are the residual vectors for continuum α per grid cell intuitively r α t 1 k provides the difference between the rate of change of fluid mass within a cell volume to that generated by sources and or fluid movement through its boundaries at iteration level k finally we inject our machine learning model into eq 56 through an explicit approach as a result following a newton iteration the coupling matrices g α in eq 56 are zero entries whilst the machine learning based r 1 appears in the residuals 7 2 tests to test the hybrid ml physics model we consider several realisations of the 2d diffusion problem described in section 4 for these realisations we test our framework on initial conditions and time lengths not used for training our model accordingly we compare the hybrid approach against a microscale model and a physics based model that uses the first order mass transfer constitutive relation for brevity in the remainder of this section we refer to the latter method as the traditional approach to dual continuum modelling in addition to the 2d problem we also consider an application of the framework to a geological model for this case we only compare the hybrid and traditional dual porosity approaches for the 2d problem test cases we discretise the dp problem using a single control volume for the microscale model we resolve the matrix using 40 40 grid cells within the dp model we initialise the fracture pressure as p 2 0 1 mpa further to mimic the fracture boundary within the dp model we set fracture permeability to 10 d as a result the fracture pressure equilibrates almost instantaneously with the external boundary in response to changes induced by inter continuum flow the remaining properties are as described in section 5 specifically ϕ 2 0 1 1 0 3 c 2 c 1 1 4 gpa 1 v 1 0 999 ϕ 1 0 0 2 μ 1 cp n f 1 ℓ 1 m ρ 0 1000 kgm 3 and k 1 k 1 md with respect to ϕ 2 0 and c 2 these are chosen somewhat arbitrarily since we do not consider fracture dynamics for the problems herein lastly we make use of a discrete equivalent to eq 2 to compare the averaged microscopic matrix pressure field with its dual continuum counterpart for the geological model we use a grid taken from the open porous media initiative 2015 accordingly each control volume now corresponds to a 3d domain involving three orthogonal fracture sets leading to a 3d diffusion problem within the matrix as a result the analytical solution used to generate data is a higher dimensional version of eq 39 further details of this solution and the parameterisations used for this test are described as part of the test case descriptions introduced below case 1 p 1 0 1 1 1 0 6 pa for the first set of test cases we consider two initial pressures not used when constructing d but coming from 1 1 1 0 6 pa specifically we sample the initial pressures such that p 1 0 i 10 pa and 0 1 mpa p 1 0 ii where superscripts i and ii correspond to the two tests respectively the boundary pressure is fixed as p 2 1 mpa the parameters for this 2d problem are as described above case 2 t 1 10 s for the second set of tests we consider shorter sequences for training and use the resulting models for testing on the full sequence specifically we consider datasets up to t 1 s and t 10 s for training instead of a full dataset corresponding to t 100 s the motivation behind this test case is to see how the models perform when trained under limited data and used beyond the training data range accordingly extrapolation outside of the training conditions is now caused by the length of the testing sequence length relative to the training sequence length as well as due to the multi step ahead approach the parameters for this 2d problem are as described above with p 1 0 1 pa case 3 application to a geological model for the 3d problem eq 39 is extended as lim and aziz 1995 zhou et al 2017 57 p 1 p 1 0 p 2 p 1 0 1 n 0 m 0 o 0 8 π 2 3 1 2 n 1 2 2 m 1 2 2 o 1 2 exp π 2 k 1 t ϕ 1 0 c 1 μ ℓ 2 2 n 1 2 2 m 1 2 2 o 1 2 where we parameterise eq 57 using p 2 1 mpa c 1 3 gpa 1 ϕ 1 0 0 2 μ 5 cp ℓ 1 m and k 1 1 1 0 4 md as before we take 120 different p 1 0 sampled from the interval 1 1 1 0 6 pa further we generate series starting from t 0 and finishing at t 100 h with a uniform stepping interval of δ t 1 h finally we train and test on the resulting data using an autoregressive regularised polynomial regression model which is subsequently injected into our physics based simulator for our geological dp model the boundaries are specified as pressure boundaries set at 1 mpa next the macroscopic fracture permeability is set at 10 d accordingly the high fracture permeability ensures our numerical test problem is consistent with our learning problem in terms of pressure conditions for the fracture we initialise the matrix as p 1 0 5 pa to complete the dp model we assign ϕ 2 0 1 1 0 3 c 2 3 gpa 1 and ρ 0 850 kgm 3 we compare results over the geological model between the hybrid ml physics model and the physics based model using the linear transfer for the latter the transfer function is parameterised using n f 3 and k 1 1 0 4 md finally despite only training for a series corresponding to 100 h we run each simulation for a 1000 h thus extrapolating outside of the training conditions 7 3 results here we present the matrix pressure evolution results for the test cases described above case 1 p 1 0 1 1 1 0 6 pa results for the first test case are shown in fig 8 in both cases we see pressure increasing with time as the matrix equilibrates with the boundary pressure we can see for both tests there is a measurable discrepancy between the dp model using a linear transfer and the equivalent microscale description this discrepancy is particularly pronounced for test i fig 8 a for this test case the linear transfer is a particularly poor early time model when the difference between continuum pressures is several orders of magnitude in contrast the dp approach with the ml transfer produces high quality matches for both test cases without the computational expense taken to run the microscale models in fig 5 b we observed small errors in the middle to late time regularised pr predictions for test series corresponding to high initial pressures however fig 8 b shows despite these discrepancies the dp model using the data driven transfer still outperforms a linear transfer based dp model with respect to the microscale solution lastly for both tests we also provide the analytical solutions fig 8 the benefit of this addition is clearer in fig 8 a from fig 8 a we observe a small discrepancy between the microscale model and the analytical solution for the first few time steps this discrepancy arises due to the time step discretisation error in the microscale model remarkably we do not see this discrepancy between the ml based approach and the analytical solution since the learnt model comes from data unaffected by these discretisation errors this subtle result corroborates the aim of multiscale approaches specifically to combine the accuracy of microscopic representations with the practicality of macroscopic models case 2 t 1 10 s fig 9 shows the results for the different training and testing sequence length test cases fig 9 a shows the test case when using up to t 1 s of data as denoted by the grey vertical line in this figure we observe a good qualitative match relative to the microscale model using the hybrid approach over the first 1 s further even for time steps within a few seconds after the 1 s training limit the match between the hybrid and microscale models is good however from middle to late times fig 9 a shows we see the hybrid model diverging from the microscale model solution nonetheless at late times the model converges back to the microscale solution in fig 9 b the sequence limit is now at t 10 s as one would expect more data used for our surrogate model results in improved hybrid model performance across the sequence length further in fig 9 b there is a noticeably smaller divergence between the hybrid and microscale model results following the 10 s limit compared to that shown in fig 9 a following the 1 s limit this result is consistent with the general trend between surrogate model accuracy and data volume leading to a lower accumulation of errors nevertheless even with a surrogate constitutive model trained using t 1 s of sequence data the hybrid model still performs well over the whole simulation time from the first two test cases we have seen a traditional dual porosity model approach gives the worst results at early times at these times there is a significant different between matrix and boundary pressures however at later times the linear transfer relation is a reasonable approximation of inter continuum mass exchange accordingly it is wasteful to use a surrogate constitutive model in regions where a simple constitutive model performs well given the data requirements to train the former an interesting development therefore would be a hybrid model that can use different constitutive models in different regions e g nonlinear vs linear of a problem space march et al 2016 case 3 application to a geological model figs 10 to 11 show the results for the geological model test case using the traditional and ml physics model respectively in both figures we can see matrix pressure rising over time in response to diffusion driven equilibration with the fractures however we can see significant differences between the two modelling approaches following the initial conditions up until equilibrium conditions comparing figs 10 to 11 at t 1 h we can see the matrix pressures are noticeably lower when using the linear transfer compared to the hybrid ml physics approach this observation is consistent with the results in the previous tests which showed the matrix pressures to be significantly underestimated when using a linear approximation accordingly matrix fluxes are underpredicted when using this linear transfer approach such errors could be significant in geological settings such as groundwater remediation haggerty and gorelick 1995 similar differences between the two approaches to those just described are observed at t 50 100 h with the ml physics approach predicting higher pressures than the traditional approach when extrapolating outside of the training conditions the hybrid approach shows physically reasonable results consistent with the previous time steps at t 500 h accordingly at this time step matrix pressures coming from the hybrid approach are higher compared to the traditional approach however by t 1000 h the two models have converged to the equilibrated pressure state in summary the results presented for the described tests show the potential for the ml physics approach for multiscale modelling with this framework we can capture the accuracy of microscopic representations with the practicality of macroscopic models we envision a number of interesting extensions to be investigated using the basis of the framework introduced herein 8 conclusions and future work this paper introduces and applies a machine learning based multiscale constitutive modelling framework accordingly we detailed the key components of the framework as homogenisation data generation surrogate constitutive model learning and ml physics model coupling in doing we described various considerations that the user should make at each step to test the framework we considered a multiscale pressure diffusion problem specifically that of describing inter continuum mass transfer in dual porosity materials in practice it is common to model this process using an overly simple linear constitutive relation instead of making any such assumption we use our framework to build a surrogate constitutive model based on microscale data in applying the framework we introduced the various machine learning methods used to create this surrogate constitutive model we account for time dependence using autoregressive approaches considering polynomial and neural network based regressors we found the former to provide higher accuracy when used as a multi step ahead predictor provided regularisation is used to deal with multicollinearity effects accordingly we integrated the resulting data driven surrogate into a physics based dual porosity model creating a hybrid ml physics approach we showed the resulting hybrid approach to give high quality results for a number of test cases compared to a linear transfer based dual porosity model without the computational expense of explicit microscale models we hypothesise a number of exciting possibilities for further work when using data coming from more complex microscale scenarios e g dynamic boundary conditions than the ones considered here in such cases data is likely to be obtained numerically however numerical data can be expensive leading to sparse datasets accordingly in scientific computing domains we see sample efficiency as a key challenge for the uptake of machine learning based multiscale modelling in addressing this challenge future developments could involve exploring different sampling and learning strategies such as active learning theory driven ml and probabilistic approaches to improve learning efficiency generalisability and to understand uncertainty in multiscale settings credit authorship contribution statement mark ashworth conceptualisation methodology investigation writing review ahmed h elsheikh supervision review florian doster supervision review declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors are grateful for the funding provided to them by the natural environmental research council to carry out this work and to anne sophie ruget for advice on figures 
166,in multiscale modelling multiple models are used simultaneously to describe scale dependent phenomena in a system of interest here we introduce a machine learning ml based multiscale modelling framework for modelling hierarchical multiscale problems in these problems closure relations are required for the macroscopic problem in the form of constitutive relations however forming explicit closures for nonlinear and hysteretic processes remains challenging instead we provide a framework for learning constitutive mappings given microscale data generated according to micro and macro transitions governed by two scale homogenisation rules the resulting data driven model is then coupled to a macroscale simulator leading to a hybrid ml physics based modelling approach accordingly we apply the multiscale framework within the context of transient phenomena in dual porosity geomaterials in these materials the inter porosity flow is a complex time dependent function making its adoption within flow simulators challenging we explore nonlinear feedforward autoregressive ml strategies for the constitutive modelling of this sequential problem we demonstrate how to inject the resulting surrogate constitutive model into a simulator we then compare the resulting hybrid approach to traditional dual porosity and microscale models on a variety of tests we show the hybrid approach to give high quality results with respect to explicit microscale simulations without the computational burden of the latter lastly the steps provided by the multiscale framework herein are sufficiently general to be applied to a variety of multiscale settings using different data generation and learning techniques accordingly keywords multiscale constitutive modelling homogenisation dual porosity machine learning for transient phenomena hybrid machine learning physics based modelling 1 introduction when modelling we are often posed with the dilemma of choosing between models of varying levels of accuracy complexity and efficiency such problems are exemplified in the subsurface in these settings strong heterogeneities exist within the rock fabric leading to multiscale phenomena one solution is to model at the heterogeneity level microscale capturing the full range of scale dependent phenomena with appropriate post processing however at large scales computational and information requirements preclude the use of such detailed simulation models instead in situations where there is weak coupling between scales a so called separation of scales exists and we can use coarse representations accordingly we have a hierarchy of models where microscale information is incorporated implicitly within macroscale models in the form of constitutive relations however often these constitutive models are based on empiricism using simple considerations such as linearity weinan 2011 whilst practical use of macroscopic models with simplified constitutive relations can lead to undesirable inaccuracies this deficiency motivates the use of multiscale modelling approaches that is to combine the detail of microscale models with the efficiency of macroscale models here we are concerned with the use of multiscale methods with application to hierarchical multiscale problems generally multiscale methods can be classified as analytical or computational weinan 2011 although significant overlap may exist between the two in situations where analytical methods are impractical or intractable it is desirable instead to use computational approaches one such computational approach informed by developments in the latter part of the twentieth century is computational homogenisation feyel 1999 kouznetsova et al 2001 miehe and koch 2002 in this approach microscopic boundary value problems are nested within points in the macroscopic domain with the solution of the former providing information for the latter notably computational homogenisation has emerged as a powerful tool for modelling multiscale problems as diverse coupled multiphysics özdemir et al 2008b su et al 2011 kaessmair and steinmann 2018 and transient diffusion problems larsson et al 2010 ramos et al 2017 waseem et al 2020 to name but a few nonetheless whilst accurate these approaches still remain expensive to use in practice wang and sun 2018 this expense motivates the use of so called sequential methods here calculations are typically made offline to create a look up table the look up table is then called online using an appropriate interpolater where necessary the downsides of the look up table approach are we acquire little knowledge about the function the quality of our dataset and how well we may generalise to samples not included in the table alternatively we can learn a surrogate model offline given our input output data and other possible information learning such mappings is one of the fundamental goals of machine learning ml more generally ml encompasses a range of algorithms to learn models from data and as a discipline has seen widespread interest within the geoscience community to address a variety of modelling challenges razavi et al 2012 lary et al 2016 tahmasebi et al 2020 machine learning approaches to generating surrogate constitutive models can be traced back several decades for example in the pioneering work of ghaboussi et al 1991 the authors used so called neural networks to represent the constitutive behaviour of concrete under various loading conditions from this and other similar early works goh 1995 ghaboussi et al 1998 users have incorporated data driven surrogate models within multiscale modelling strategies for a variety of applications unger and könke 2009 hambli et al 2011 asproulis and drikakis 2013 more recently work has been done for modelling time dependent constitutive information using so called recurrent neural networks wang and sun 2018 ghavamian and simone 2019 chen 2021 despite the success of these recurrent algorithms for sequential modelling they can be challenging to implement and to understand pascanu et al 2013 bai et al 2018 further bai et al 2018 showed more recently autoregressive feedforward neural network architectures that give superior performance compared to recurrent structures for a wide variety of sequence modelling tasks given the variety of multiscale problems and machine learning methods it would be useful to have a framework detailing the key questions and considerations for the generation and use of surrogate constitutive models within sequential multiscale approaches the aim and novelty of this work is to introduce and apply a machine learning based multiscale framework for modelling complex constitutive behaviours such as those due to nonlinearity and hysteresis both of these phenomena are commonly observed in subsurface resource modelling our framework is broken down into four key components with the starting point being a homogenisation procedure here we use a two scale homogenisation approach such as those forming the basis for computational homogenisation methodologies in this framework the homogenisation procedure plays a fundamental role in generating consistent data to learn from further the procedure also allows insight into the quantities involved in the constitutive mapping with the scale transition rules from the homogenisation procedure in place the remaining parts of the framework involve generating the data learning the model and coupling the result to a physics based simulator for our application we consider the diffusive mass transfer involved in dual porosity or dual continuum more generally flow modelling in the dual porosity dp setting a strong contrast in conductivities exists between the different porosity levels a classical example of such a material is naturally fractured rock here the matrix has significantly lower permeability than the fractures as a result the matrix introduces a history dependent memory effect associated with the mass exchange between the two porosity levels accordingly in the presence of fracture dynamics the inter porosity exchange is non local in time requiring a convolutional product for its description further even for cases involving static fracture pressures challenges still remain in describing inter porosity exchange for these latter cases and for simple geometries analytical solutions for the inter porosity exchange term are possible however solutions for these static fracture pressure cases are still explicit in time and are formed from a complex infinite series accordingly describing inter porosity flow for both static and dynamic fracture behaviours remains difficult with traditional simulation methods these challenges necessitate simpler descriptions for inter porosity exchange leading to the commonly used linear description involving the difference between averaged pressures warren and root 1963 however this simplification is known to lead to measurable errors in flow behaviour particularly at early time zimmerman et al 1993 ashworth and doster 2020 instead of making any such assumptions we use a data driven constitutive model learnt on time series data coming directly from the microscale in line with the aims of this work to introduce and apply our framework we consider only cases of static fracture pressure accordingly we can give an overview of the framework on a problem without significant complexity using easy to generate data and established learning algorithms we subsequently compare various nonlinear feedforward autoregressive algorithms approaches to model the time dependent problem the successful model is then coupled to a dual porosity simulator in the matlab reservoir simulation toolbox mrst lie et al 2012 lie 2019 lastly in the context of this work it is worth mentioning the interesting work done by andrianov and nick 2021 for this the authors use ml to learn the mapping between images of pixelated fracture geometries and dp constitutive parameters in contrast taking inspiration from computational homogenisation approaches our goal here is to provide a framework to learn and apply a constitutive mapping itself we structure the remainder of this paper as follows in section 2 we introduce the framework detailing the key elements and considerations therein in section 3 we start the application of the framework through the two scale homogenisation approach to derive the macroscopic dual porosity model in section 4 we introduce a dual porosity model problem that will serve as the testbed for the remaining parts of the multiscale framework further given this model problem we derive an often used linear transfer model in section 5 we generate the dataset for learning to do so we use analytical solutions for the given model problem whilst also considering the end use of the ml model in a numerical simulator in section 6 we learn our surrogate constitutive model comparing a variety of learning algorithms for modelling the time dependent problem in section 7 we couple the resulting ml model to a physics based simulator leading to a hybrid ml physics approach we then test the resulting hybrid approach against a dual porosity model equipped with the traditionally used linear transfer model and microscale simulations in section 8 we finish with conclusions and recommendations for future work lastly for reproducibility the codes used in this work are available as open source 1 1 https github com mashworth11 ml mm 2 machine learning based constitutive modelling framework here we introduce the ml based multiscale constitutive modelling framework and the key concepts therein the framework itself can be broken into four key components homogenisation data generation surrogate constitutive model learning and ml and physics model coupling fig 1 however we stress that these components are interrelated and should be understood in parallel as opposed to isolated procedures 2 1 homogenisation homogenisation is concerned with the question how do we link between micro and macroscales providing constitutive information from the former to the latter various approaches exist for linking between scales such as asymptotic homogenisation sánchez palencia 1980 auriault et al 2010 the method of volume averaging quintard and whitaker 1993 whitaker 1999 and computational homogenisation approaches feyel 1999 kouznetsova et al 2001 depending on underlying requirements e g a priori knowledge for scaling parameters in the case of asymptotic approaches and resources some upscaling strategies may be more suitable than others regardless within the context of this framework the homogenisation procedure provides the guide for generating consistent offline data for training and validating a machine learning model we note one could consider the homogenisation step as part of the data generation process however we choose to distinguish these two steps within the current framework to highlight their respective importance and considerations therein 2 2 data generation data generation is concerned with the question how will we generate sample data for our learning algorithm microscale results can be expensive when numerically derived due to limited computational resources and or high problem dimensionality managing this trade off to ensure the dataset sample set is as informative as possible is referred to as experimental design or design of experiments razavi et al 2012 traditional approaches to experimental design involve the use of various sampling strategies e g monte carlo latin hypercube quasi monte carlo methods to build the dataset prior to learning however these so called one shot strategies can either contain too few samples for an accurate surrogate undersampling or more samples than we need oversampling thus wasting precious resources xiao et al 2018 an alternative to one shot approaches is to use feedback from the ml training process to find samples that provide the maximum information gain when labelled and used to update the model these so called active learning or sequential sampling approaches can address the under oversampling trade off described previously crombecq 2011 in addition to data driven ml approaches recent advances have seen the development of theory driven ml approaches here the idea is to use physics such as partial differential equations initial and boundary condition information symmetry or invariance properties and thermodynamical considerations ling et al 2016 wang and sun 2018 raissi et al 2019 zhu et al 2019 masi et al 2021 to constrain the learning process accordingly we can supplement missing or difficult to obtain data further theory driven approaches also act to regularise the learning process and ensure the physical consistency of our model however in this work we consider purely data driven methods and save the incorporation of theory into the learning process for future studies 2 3 surrogate constitutive model learning surrogate constitutive model learning is concerned with the question given the properties of our data the type of prediction problem and the end use for the resulting ml model what is the most appropriate learning strategy to use training ml algorithms can be expensive from both data and computational points of view if our sample set is poor complex algorithms run the risk of failing to generalise when given new data overfitting conversely if our model is too simple we risk failing to extract meaningful relationships from our data underfitting besides addressing underfitting and overfitting we may also want algorithms that are intrinsically interpretable and quick to retrain in light of new data e g linear regression tree based methods give us an insight into aleatoric data and or epistemic model uncertainties e g gaussian processes bayesian learning and provide an embedded approach to feature input engineering in light of complex and or high dimensional data e g deep learning ultimately within the context of multiscale modelling the choice of learning method should be guided by considering the interdependence between data quality and availability ml model accuracy and practicality and the model s end use in physics based simulation 2 4 model coupling model coupling is concerned with the question how will we use our learnt constitutive model in our macroscale physics simulator ideally the ml model can be implemented within a numerical solver without significant intrusion accordingly important considerations at this stage may be the accessibility of data during simulation to pass to the ml model the ml model inference time how predictions will be fed into the numerical model and in the case of nonlinear materials how we extract state dependent material properties such as permeability and stiffness from our surrogate constitutive model these latter points will depend on whether the constitutive model is to be used in an explicit or implicit manner we describe the former as those predictions made using previously observed and converged quantities subsequently in a nonlinear solver the resulting surrogate constitutive model will only appear in the residual alternatively if the prediction also depends on quantities at the current iteration level parameters from the surrogate associated with these quantities will be needed in the jacobian for example in hashash et al 2004 the authors consider nonlinear deformation problems and derive a closed form expression for calculating the tangent stiffness tensor from a neural network accordingly the authors provide consistent jacobian entries for use in a newton raphson scheme for this work our surrogate will be explicitly coupled due to the nature of the constitutive relation being injected nonetheless the example above highlights the type of considerations we need to make in order to embed ml models within traditional physics based simulations 3 application of the framework two scale homogenisation for dual porosity flow problems with the details of the framework in hand we now progress to its application for our application we consider problems associated with dual porosity flow such as those observed in multiscale geomaterials accordingly in this section we derive the dual porosity flow model by way of a two scale homogenisation approach developed for transient diffusion problems larsson et al 2010 ramos et al 2017 brassart and stainier 2019 waseem et al 2020 3 1 preliminaries the homogenisation procedure relies on the definition of representative elementary volumes ω rev taken from a macroscopic domain ω accordingly we are interested in quantities varying at two distinct scales those variations at points within an rev x ω and those variations at material points within the macroscopic body x ω note quantities written with x denote macroscopic quantities the rev is defined according to a separation of scales in terms of geometrical information and the wavelengths of physical processes involved formally the separation of scales is defined as 1 ℓ l l where ℓ l and l denote the characteristic lengths of the heterogeneities rev and macroscopic body respectively our reference material is naturally fractured rock accordingly the material is composed of a low permeability matrix denoted by subscript 1 and a high permeability fracture material denoted by subscript 2 we assume homogeneity and isotropy of each material as well as isotropic geometrical distributions at the microscale next we introduce the following volume averaging relations 2 z 1 ω ω z d v z α 1 ω α ω α z d v where z is an arbitrary field quantity accordingly denotes the volume average over the whole rev domain whilst α denotes the volume average over the constituent volume ω α ω the two volume averages defined in eq 2 are related such that 3 z v 1 z 1 v 2 z 2 where v α ω α ω is the volume fraction of phase α lastly we define the following relations between rates of change and their averages such that 4 z z t t z z 3 2 mass balance at the macro and microscales the dual porosity model is a multi continuum model at the macroscale described by the continuum wise mass balance as 5 m 1 w 1 r 1 6 m 2 w 2 r 2 where m α w α and r α denote the rate of macroscopic mass content change mass flux vector and inter continuum rate of mass transfer for continuum α respectively further mass conservation requires r 2 r 1 in addition to appropriate boundary and initial conditions we require constitutive relations for the macroscopic quantities m α w α and r α the simplest approach to specify these closures is through phenomenological arguments alternatively we can derive closure relations through homogenisation in this regard asymptotic homogenisation has been successful in deriving a closed dual porosity model starting at the microscale royer and auriault 1994 these results thus serve as a useful reference to the approach introduced herein at the microscale we have the following mass balance 7 m w 0 where m and w denote the rate of microscopic mass content change and mass flux vector respectively additionally at this scale we assume both materials are saturated by the same slightly compressible fluid of density ρ under isothermal flows we define the fluid compressibility relation 8 c l 1 ρ ρ p where p denotes the microscopic pressure a similar compressibility relation is defined for the local porosity ϕ ω p ω where ω p is the pore volume such that 9 c ϕ 1 ϕ ϕ p we assume small perturbations in fluid density and porosity allowing linearised evolutions of these quantities further the small perturbation hypothesis permits the replacement of ρ and ϕ with ρ 0 and ϕ 0 respectively where necessary notations with superscript 0 denote reference conditions from m ρ ϕ and w ρ q where q is the microscopic volume flux eq 7 becomes 10 ρ 0 ϕ 0 c p ρ 0 q 0 where we have made use of the small perturbation hypotheses and where c c l c ϕ lastly the microscopic volume flux is related to the microscopic pressure gradient through darcy s law neglecting gravitational effects 11 q k μ p where μ is the fluid viscosity for the isotropic case k k i in which i denotes the identity matrix to link the macroscopic description in eqs 5 to 6 to the microscale description in eq 7 we make use of a two scale homogenisation approach this approach is split into two steps downscaling involves translation of macroscopic loading parameters macroscopic pressures and gradients to consistent constraints and boundary conditions at the microscale upscaling involves transformation of microscopic rates of change and fluxes to their macroscopic counterparts through averaging we address the downscaling step in the section to follow 3 3 downscaling for a dual porosity material associated to a macroscopic point x we have two macroscopic pressures p 1 p 2 and gradients p 1 p 2 for each constituent we then define microscopic pressure fields according to taylor series expansions of the first order about the macroscopic point 12 p x x t p 1 x t p 1 x t x x p x x t in ω 1 13 p x x t p 2 x t p 2 x t x x p x x t in ω 2 where p denote higher order expansion terms corresponding to pressure fluctuations at the microscale we note the microscale is positioned relative to the macroscopic point such that x x 0 which also holds for constituent averages from eqs 12 to 13 we require the following compatibilities to hold 14 p 1 p 1 15 p 2 p 2 corresponding to the requirement p α 0 from özdemir et al 2008a eqs 14 to 15 are related to requiring consistency between the microscopic and macroscopic mass contents such that 16 ρ ϕ α c α p α ρ ϕ c p α where ϕ α ω α p ω v α ϕ α is the effective continuum porosity from eq 16 we propose the following consistency conditions for the mass storage 17 ρ ϕ α c α ρ ϕ c which is inline with results obtained through asymptotic homogenisation auriault 1983 royer and auriault 1994 in addition to constraints provide by eqs 14 to 15 we require averages of microscopic gradients to equal their macroscopic counterparts accordingly taking the gradients of eqs 12 to 13 and averaging over each constituent leads to 18 p 1 p 1 p 1 in ω 1 19 p 2 p 2 p 2 in ω 2 where we use the following relation x x i to fulfil the compatibility between gradients we require p α 0 for both constituents application of gauss divergence theorem on averages of the fluctuation gradients leads to 20 ω α p n d s 0 where ω α is the boundary of ω α and n is the outward unit normal vector condition eq 20 is satisfied by setting certain restrictions boundary conditions on the fluctuations at the boundaries examples of such boundary conditions include periodic boundary conditions and prescribed fluctuation boundary conditions in this work we use the latter for demonstrative purposes although the former is often used for flow based upscaling procedures lie 2019 accordingly prescribed fluctuation boundary conditions correspond to setting p 0 leading to 21 p x x t p α x t p α x t x x on ω α from either eq 12 or eq 13 in the dual porosity formulation we assume the matrix material to be completely permeated by the fracture material additionally we assume continuity of the microscopic pressure field at the interface γ ω 1 between the two materials accordingly boundary conditions for both materials can be completely defined by considering the prescribed fluctuation boundary conditions for the fracture material 22 p x x t p 2 x t p 2 x t x x on ω 2 we note eq 22 is somewhat similar to the boundary conditions specified by the asymptotic homogenisation approach on γ where p p 2 on γ royer and auriault 1994 however here the explicit size of the rev and macroscopic fracture pressure gradient is taken into account nonetheless for small macroscopic gradients and vanishingly small rev sizes p p 2 on γ is a reasonable approximation with the required downscaling relations we can now proceed to upscaling 3 4 upscaling upscaling for the macroscopic quantities m α w α is achieved through a virtual power equivalence relation between the micro and macroscales this relation is referred to as the generalised hill mandel condition blanco et al 2016 and requires the macroscopic virtual power to equal its volume averaged microscopic counterpart at a given material point here we introduce a virtual power relation for the dual porosity material by summing the external powers coming from eqs 5 to 6 and using δ p 1 r 1 δ p 2 r 2 0 to give 23 δ p 1 m 1 δ p 1 w 1 δ p 2 m 2 δ p 2 w 2 δ p m δ p w where we use eq 7 at the microscale for compactness use of the decomposition in eq 3 on eq 23 followed by substitution of the appropriate expansions in eqs 12 to 13 leads to 24 α 1 2 δ p α m α δ p α w α v 1 δ p 1 δ p 1 x x δ p m δ p 1 δ p w 1 v 2 δ p 2 δ p 2 x x δ p m δ p 2 δ p w 2 rearranging eq 24 to isolate terms involving fluctuations gives 25 α 1 2 δ p α m α δ p α w α v 1 δ p 1 m δ p 1 w x x m 1 v 2 δ p 2 m δ p 2 w x x m 2 v 1 δ p m δ p w 1 v 2 δ p m δ p w 2 the terms in eq 25 involving the pressure fluctuations represents the weak form of the phase wise microfluctuation balance of mass these terms are subsequently addressed through the chain rule and the divergence theorem leading to 26 v 1 δ p m δ p w 1 v 2 δ p m δ p w 2 δ p m w ω 1 p n d s ω 2 p n d s accordingly the first term on the right hand side of eq 26 is satisfied due to the microscopic conservation of mass eq 7 the remaining terms then vanish with the appropriate boundary conditions in this case the prescribed fluctuation boundary conditions having dealt with the fluctuation terms we seek to obtain a dual porosity formulation from the remaining terms in eq 25 this goal is pursued in the following section 3 5 recovery of the dual porosity model to recover a dual porosity model from the virtual power equivalence we assume quasi steady state in the fracture material due to the high phase permeability accordingly 27 w ρ 0 q 0 in ω 2 whilst transient diffusion effects remain in the matrix material at the microscale however due to the percolating fractures and the low intrinsic matrix permeability we assume w 1 0 with the following assumptions at the microscale we recover from eq 25 28 δ p 1 m 1 δ p 1 w 1 δ p 2 m 2 δ p 2 w 2 δ p 1 m 1 v 1 δ p 1 x x m 1 v 1 δ p 2 w 2 v 2 comparing terms in eq 28 we identify the following relations 29 m 1 v 1 m 1 ρ 0 ϕ 1 0 c 1 p 1 30 w 1 v 1 x x m 1 31 w 2 v 2 w 2 ρ 0 v 2 q 2 where fluid density at reference conditions at the microscale corresponds to the equivalent quantity at the macroscale i e ρ 0 ρ 0 further for the small perturbation considerations used here c α c α looking first at the averaged quantity in eq 30 we identify this as the first moment of the rate of pressure change in the matrix interestingly this term is not present in the classical asymptotic homogenisation approach royer and auriault 1994 physically this term introduces a size effect and represents the microscale mass inertia due to the low permeability of the matrix material however in general this inertial term is shown to be negligible with respect to the macroscopic flux from the fractures brassart and stainier 2019 further for isotropically distributed materials at the microscale this term is shown to vanish brassart and stainier 2019 from eq 31 we can identify the macroscopic volumetric fracture flux as 32 q 2 v 2 q 2 q where the relation on the right hand side of eq 32 arises due to the assumption v 1 q 1 0 and from eq 3 next using eq 11 in eq 32 we write 33 q 2 k 2 μ p 2 where k 2 denotes the macroscopic effective fracture permeability tensor accordingly we can recover entries to this permeability tensor provided we can calculate the macroscopic fracture flux corresponding to the macroscopic pressure gradient such calculations are addressed in so called flow based upscaling procedures lie 2019 finally eq 29 shows the relation between the micro and macroscopic rate of mass content change from eq 4 we see 34 ρ 0 ϕ 1 0 c 1 p 1 ρ 0 ϕ 1 0 c 1 p 1 accordingly with eq 29 resp eq 34 and eq 31 in eqs 5 to 6 whilst neglecting the inertial effects from eq 30 we recover the dual porosity model from the two scale homogenisation procedure as 35 ρ 0 ϕ 1 0 c 1 p 1 r 1 36 ρ 0 ϕ 2 0 c 2 p 2 ρ 0 v 2 q 2 r 1 where we use the compatibility in eq 16 for the fracture rate of mass content change m 2 in this work our focus is in the modelling of the inter porosity transfer term r 1 from eq 35 we see r 1 ρ 0 ϕ 1 0 c 1 p 1 subsequently to compute r 1 all we need is the solution to the microscale boundary value problem eq 10 in the matrix given boundary conditions from eq 22 on γ and some initial condition p 1 x x t 0 p 1 0 however often we not have direct access to the microscale within a macroscopic simulation further even if the solution could be calculated directly a priori it is difficult to use in practice owing to its complex form specifically for evolving fracture pressures the matrix pressure solution is defined as a convolution product over the fracture pressure history royer and auriault 1994 even for static fracture pressures following a step increase the matrix pressure solution is still an infinite series defined in explicit time shown in the following section due to the described complexities it is common to adopt a simple linear relation to model r 1 such simplification leads to measurable errors in flow behaviour accordingly in the remainder of this work we aim to improve on linear transfer models through machine learnt constitutive models which incorporate temporal information using quantities directly available during simulation 4 a dual porosity model problem and recovery of a first order inter porosity mass transfer model with the homogenisation step in hand we now have a guideline for how to generate data to learn from accordingly we proceed to the remaining steps of the multiscale framework to do so we first consider a model problem for which data can be generated using analytical solutions further from these solutions we can obtain a first order mass transfer model that is commonly used in dual continuum flow modelling this simplified transfer model will then serve as a reference for testing any data driven model coupled to a numerical simulator we note that whilst the problem considered here is not too complex it serves as a useful demonstration for the remaining steps of the framework and to explore various established machine learning algorithms for modelling the time dependent mass transfer problem future directions therefore concern numerically derived data involving fracture dynamics more complicated microscale geometries and more advanced methods for surrogate modelling e g active learning physics informed models etc in this latter element work combining the data driven parameterisations from andrianov and nick 2021 with elements of the framework introduced herein could be an interesting path for further research 4 1 model problem for the model problem we consider a two dimensional matrix domain surrounded by fractures fig 2 a at a time t 0 the matrix and fractures are in hydrostatic equilibrium then following a jump in fracture pressure at a time t 0 the fracture pressure remains fixed leading to p 2 0 fig 2 b accordingly from eq 21 and the pressure continuity across the interface we have 37 p x x t p 2 x t on ω 1 with initial conditions 38 p x x t 0 p 1 0 x x in ω 1 due to the considered fracture pressure boundary conditions and simple geometry analytical solutions exist for calculating the solution these solutions are introduced subsequently 4 2 analytical solution and first order mass transfer model the analytical solution to the 2d matrix diffusion problem described above is found by taking the product of the dimensionless solution to a 1d diffusion problem such as those arising in slab geometries crank 1979 holman 1990 lim and aziz 1995 averaging the solution over the rev and noting the equivalence with macroscopic quantities then leads to 39 p 1 p 1 0 p 2 p 1 0 1 n 0 m 0 8 π 2 2 1 2 n 1 2 2 m 1 2 exp π 2 k 1 t ϕ 1 0 c 1 μ ℓ 2 2 n 1 2 2 m 1 2 where we use p 1 p 1 next taking the time derivative of eq 39 gives 40 p 1 p 2 p 1 0 n 0 m 0 8 π 2 2 π 2 k 1 ϕ 1 0 c 1 μ ℓ 2 1 2 n 1 2 1 2 m 1 2 exp π 2 k 1 t ϕ 1 0 c 1 μ ℓ 2 2 n 1 2 2 m 1 2 substitution of eq 40 into eq 35 gives r 1 however the resulting mass transfer expression is unsuitable for simulation purposes given the presence of explicit time and the infinite series to alleviate these dependencies we can recover a first order approximation to eq 40 accordingly taking the first term from eq 40 and eliminating t using the first term from eq 39 gives 41 p 1 2 π 2 k 1 ϕ 1 0 c 1 μ ℓ 2 p 2 p 1 substitution of eq 41 into eq 35 leads to the following first order inter porosity transfer model zimmerman et al 1993 lim and aziz 1995 42 r 1 ρ 0 v 1 η k 1 μ p 2 p 1 where η n f π 2 ℓ 2 denotes the so called shape factor in which n f is the number of characteristic fracture sets several investigations have shown use of the linear constitutive model in eq 42 to lead to measurable errors in flow behaviour zimmerman et al 1993 ashworth and doster 2020 one common way to alleviate these inaccuracies is to use multi rate transfer models haggerty and gorelick 1995 geiger et al 2013 however in this work we treat the problem differently instead we use machine learning to bridge the scales in the form of a surrogate constitutive model 5 application of the framework data generation our goal for the supervised learning problem is to learn a mapping from quantities available during simulation to an output aligned with our constitutive property to do so we consider a dataset d of n samples d x i y i i 1 n where x i r d i n is a vector of inputs and y i r d o u t is a vector quantity derived from the microscale by way of our upscaling procedure learning a mapping between inputs and outputs then equates to learning a function of the form 43 y i f x i f ˆ x i as a result we can incorporate local scale physics such as those coming from the analytical solution or numerical simulations within macroscopic models without having to formulate explicit phenomenological expressions we frame our learning problem considering the physics of the process inputs coming from simulation and an output that is aligned with the discrete structure of the numerical problem for the latter we specify our output following application of the backward euler method commonly used for time discretisation in numerical modelling specifically 44 p 1 t p 1 t 1 p 1 t δ t from eq 44 our goal is to predict the pressure at the next time step 45 y i t 1 p 1 t 1 at simulation time we incorporate the resulting prediction into the finite difference calculation in eq 44 at each time step accordingly our learning problem is formulated on the basis of time series calculated with eq 39 each individual time series corresponds to a different initial matrix pressure p 1 0 we take 120 different p 1 0 sampled with logarithmic spacing from the interval of 1 1 1 0 6 pa every series starts from 0 and finishes at a final time of t 100 s with a uniform stepping interval of 0 1 s the remaining parameters in eq 39 are fixed as k 1 0 5 md ϕ 1 0 0 2 c 1 1 4 gpa 1 μ 1 cp ℓ 1 m and following the step in p 2 at t 0 p 2 1 mpa a plot of each series s as time vs y s is shown in fig 3 a further fig 3 b shows how this time series data is split into training and testing data using a 2 3 1 3 split respectively accordingly every third sequence corresponds to a test series 6 application of the framework surrogate constitutive model learning in this section we review and compare the learning algorithms used to generate our surrogate constitutive model specifically we introduce the various approaches and considerations for modelling the time dependent problem to follow we train and test the different modelling approaches on our data the results from this section will then be carried on to the next stage of the framework i e coupling the data driven model to a numerical simulator 6 1 autoregressive approaches to start we introduce the modelling description used to account for time dependency that is we model the sequential problem using an autoregressive description in the following we present autoregressive descriptions in the context of feedforward models in feedforward approaches the information flow is unidirectional from the current inputs to outputs this modelling approach is contrary to recurrent models in which the information flow comes from both the current inputs and from calculations made on inputs at previous time steps for a multivariable feedforward autoregressive model f ˆ uses previous series outputs to predict the next output in the series accordingly for a two variable univariate feedforward autoregressive model 46 y ˆ i t 1 f ˆ x i t f ˆ y ˆ i t y ˆ i t d y 1 u i t u i t d u 1 where u i denotes an external series and d y and d u denote the number of sequence terms or dependency length for the internal and external series variables respectively in this work we set d y d u d our external series corresponds to the boundary conditions for the matrix provided by the homogenisation approach given the model problem described in section 4 specifically previous sequence values for p 2 corresponding to p 2 p 1 0 when t t 0 and p 2 1 mpa or any selected pressure step value when t t 0 finally we note that d y and d u or d are defined a priori as a result potentially important long range information is lost if short dependency lengths are chosen one option to alleviate the challenges posed by fixed dependency lengths would be to use recurrent neural networks such as the long short term memory network for feedforward autoregressive training we treat the learning task as a single step ahead prediction problem in this strategy the idea is to use lagged ground truths targets y i in place of the predicted values y ˆ i in eq 46 autoregressive testing is treated according to how the ml model will be used when coupled to the macroscopic simulator that is as a multi step ahead prediction problem accordingly the trained single step ahead models are used in a recursive manner with predictions y ˆ at previous time steps fed back into the inputs to predict the next time step as per eq 46 the single step ahead training and testing strategy can work provided the training error is sufficiently small such that accumulated errors do not explode when using the model as a multi step ahead predictor when this requirement on the accumulated errors cannot be satisfied which is difficult to determine a priori iterative approaches are unstable in such cases multi output strategies that predict the whole forecast horizon at once can be used however the stability of multi output regressors often comes at the cost of predictive accuracy particularly for long forecast horizons sangiorgio and dercole 2020 in the section to follow we consider two ml algorithms for feedforward autoregressive modelling specifically polynomial regression pr and a fully connected neural network fc nn 6 2 polynomial regression here we introduce the polynomial regression algorithm these algorithms are attractive because they are easy to understand and fast to train however a noteable downside to these methods is the curse of dimensionality for large input vectors nonetheless pr algorithms are a simple place to start for many regression tasks and serve as a good baseline model accordingly under pr f ˆ is formulated as 47 y ˆ i f ˆ x i w θ x i where θ θ 1 θ n p is the vector of polynomial basis functions acting on x i and w r n p is a vector of weights parameters of the model in this work we use a second order polynomial feature transformation which comes as a standard tool in many machine statistical learning packages whilst the inputs are polynomial following a transformation in eq 47 the weights remain linear finding w is simply then a linear regression problem and is the goal of training in supervised learning problems training corresponds to minimising a loss function l that represents the error in approximating f by f ˆ on our data for the regression problem we use a squared loss such that 48 l w 1 n y f ˆ θ w 2 2 where θ θ x 1 θ x 2 θ x n and y y 1 y 2 y n for the univariate target case finding the optimal set of weights that minimise eq 48 is defined formally as 49 arg min w l w arg min w 1 n y f ˆ θ w 2 2 the squared loss in linear regression is a convex function which admits a global minimum in w at the point l 0 accordingly 50 θ θ w θ y leading to 51 w θ y where θ θ θ 1 θ pr is well known to suffer from high degrees of multicollinearity dalal and zickar 2012 as a result θ θ is ill conditioned and the solution in eq 51 is non unique for prediction problems such ill conditioning is generally not a concern dormann et al 2013 since new data comes from the same population as the training data however for problems of extrapolation such as multi step ahead prediction collinearity between inputs can change due to accumulating errors such changes can affect predictions on models arising from ill conditioned matrices and should be addressed dormann et al 2013 we therefore address multicollinearity between inputs using regularisation which aims to restrict the space of viable models that fit our data e g in the face of ill conditioning there are various forms of regularisation however in this work we add the following penalisation γ w 2 2 term to eq 48 leading to so called ridge regression the parameter γ in the penalty then controls the degree of weight penalisation 6 3 fully connected neural networks here we introduce the fully connected neural network model aside from the fc nn considered in this work neural networks more generally have become a popular choice for surrogate modelling due to their great flexibility scalability and significant advancement within research communities further contrary to the polynomial regression where the feature transforms are defined a priori in neural networks these transformations are implicit within the algorithm and learnt from the data to proceed within an fc nn information is fed forward from an input layer through hidden layers l 1 l 1 finishing at an output layer l l accordingly under an fc nn f ˆ is formulated as 52 y ˆ i f ˆ x i s l s l 1 s 1 x i for a given layer l s l is given as an affine transformation followed by an element wise nonlinear function such that 53 s l x h l w l x b l where w l r n l 1 n l and b l r n l are the weight matrix and bias parameter vector associated with layer l respectively evidently for the univariate prediction considered herein the weight matrix for the last layer will be a weight vector w l w l notation h is the nonlinear function referred to as an activation function in this work we take h l as a linear activation and all others as so called exponential linear units in training the fc nn we minimise the customary squared loss of the form 54 l λ 1 n y f ˆ x λ 2 2 where λ w 1 b 1 w l b l and x x 1 x 2 x n however unlike eq 48 with neural networks the loss function is non convex as a result weights and biases are updated iteratively using a chosen optimiser together with the backpropagation algorithm see goodfellow et al 2016 for further details in this work we found stochastic gradient descent with nesterov momentum to work the best nesterov 1983 we experimented with various model depths and widths and found the best results came using a simple single hidden layer fc nn with 12 hidden units additionally with this architecture there was little overfitting for the single step problem and hence no need for regularisation our fc nn was implemented using the keras api chollet 2015 lastly we construct the datasets used by these models as d x y such that x x 1 1 x 2 1 x 1000 s and y y 1 1 y 2 1 y 1000 s superscripts on the input matrix and target vector entries denote the time series label with s 120 being the total number of series subscripts then denote the time step within a given series 6 4 training and testing in the following we present some additional considerations for model training and interpretation then we present the results and discussions for the model training single step ahead and testing multi step ahead procedures considerations for our models we consider a dependency length of d 3 for the inputs although not studied in detail here d is a hyperparameter that can be tuned according to the trade off between gain in accuracy and complexity the weight penalty term γ for the regularised polynomial regression rpr was found to give the best results for γ 1 1 0 7 we present results comparing the impact of different values for γ for the multi step ahead problem on both training and testing sets in the section to follow when training our fc nn model we used a batch size of 128 run over 500 epochs further we standardise our data to have a mean of zero and a standard deviation of one standardisation during testing is then done using the mean and standard deviation calculated on the training set to evaluate between the different models we consider one minus the normalised root mean square error given as 55 1 n r m s e 1 r m s e σ n 1 i 1 n y i y 2 n 1 1 i 1 n y i y 2 where σ and y represent the sample standard deviation and mean of the train or test target set respectively from eq 55 the closer the value to one the more accurate the algorithm lastly to highlight the effects of multicollinearity we present results for the polynomial regression model both with and without regularisation results fig 4 shows the train and test results using the unregularised polynomial regression algorithm from fig 4 a we can see there is a good match when training as a single step ahead prediction problem the quality of this match is corroborated by the high evaluation metric and low rmse respectively shown in table 1 however during testing the prediction quality deteriorates particularly for sequences associated with high initial pressures fig 4 b further for the series corresponding to the highest initial pressure we observe unbounded error growth with the prediction rapidly going negative accordingly this latter unbounded prediction leads to the missing entries for the error metrics for the test problem shown in table 1 we hypothesised errors in the multi step ahead case with this model to be caused by the multicollinearity existing between the transformed features as described in section 6 2 accordingly to address this problem we introduced a regularised pr by way of a penalisation term on the loss results for the regularised pr are shown in fig 5 and table 1 the training results shown in fig 5 a are virtually identical to those shown in fig 4 a for the unregularised pr case table 1 however we can observe measurable differences on the multi step ahead test set between the two polynomial regression models from fig 5 b we see the regularised pr predictions are qualitatively closer to the targets contrary to fig 4 b further we do not observe the unbounded predictions shown in the latter figure looking at the error metrics for the regularised pr model in table 1 we see the rmse is significantly higher for the testing set such differences are expected given the accumulation of errors with the higher test error likely due to the small discrepancies observed at late times in fig 5 b nonetheless the results using the simple regularised polynomial regression model are both qualitatively and quantitatively reasonable serving as a good benchmark for the more complex methods to follow fig 6 shows the penalisation parameter tuning for the regularised pr accordingly we observe as γ increases from γ 1 1 0 9 to γ 1 1 0 7 the rmse on both the training and testing sets treated as both multi step ahead problems for this demonstration decreases however from γ 1 1 0 7 to γ 1 1 0 6 we can see a sharp increase in rmse indeed past γ 1 1 0 6 our rmse becomes unbounded as the accumulating errors become significant the results from fig 6 support our use of γ 1 1 0 7 for the weight penalisation lastly fig 7 and table 1 show the train and test results using the fc nn from fig 7 a we see a good match between the training predictions and targets however comparison of the quantitative training results in table 1 between the fc nn and the rpr suggests underfitting with the former from fig 7 b we can observe good qualitative matches between the predictions and targets over early time however from middle to late times the predictions diverge from the targets suggesting the effects of accumulating errors on the multi step predictions over these time periods the resulting degradation in performance is reflected in the error metrics in table 1 for the fc nn on the testing case we anticipate this diverging prediction behaviour to be a consequence of the underfitting observed with the fc nn possible solutions to the underfitting problem include increasing the complexity of the model and or using more data for the former we found increasing the width and depth of the network to add little benefit alternatively whilst increasing the availability of data is trivial for this problem it may not be practical for expensive to generate data regimes instead we hypothesise the underfitting in this case to be down to the optimisation process used when learning the fc nn specifically even for convex loss landscapes solutions achieved by gradient based optimisers are unlikely to represent the optimum solution due to the sensitivity of their performance on hyperparameters such as the learning rate nonetheless when learning on more complex loss landscapes these solutions are often good enough for example in computer vision and natural language processing deep neural networks remain the state of the art however for current purposes in light of accumulating errors in the multi step ahead prediction problem we require a high degree of accuracy from our learnt surrogate constitutive model accordingly in contrast to the fc nn the weights in the rpr are typically obtained using highly accurate matrix factorisation methods that lead to globally optimal solutions to the convex optimisation problem it is worth noting however despite its performance compared to the pr model the fc nn does not suffer from the same multicollinearity problems as the latter such a feature may be useful for future investigations involving neural networks in such scientific problems in summary we have shown training and testing results for the time dependent problem using several data driven modelling strategies we have seen that complexity does not always result in superior performance with the simple regularised polynomial regression model outperforming the neural network based approach on both training and testing problems indeed as part of this work we also experimented with fitting even more complex neural networks so called encoder decoder long short term memory networks 2 2 see https github com mashworth11 ml mm for further details however we found these recurrent neural networks more involved to fit in practice with the results falling short of the performance of the polynomial regression models for more complex problems the additional effort required to fit such models may be warranted however when learning from expensive to generate data we envisage a key challenge for machine learning based multiscale modelling to be in the sample efficiency of learning given the results shown in this section we use the regularised pr model for the next stage of the framework application 7 application of the framework model coupling here we couple our trained surrogate constitutive model to a macroscopic physics based model we test the subsequent hybrid ml physics model on a variety of cases against a traditional physics based model that uses the linear transfer and a microscale model 7 1 hybrid ml physics model our physics based numerical model is the discretised counterpart to eqs 35 to 36 time discretisation is achieved using the backward euler method as per eq 44 discretisation for the pressure gradient is done using the finite volume method by way of the two point flux approximation the latter is implemented as standard within mrst given the propensity for nonlinear phenomena in subsurface flow e g due to compressibility it is common to use nonlinear solvers accordingly we give eqs 35 to 36 in discrete block matrix form following application of newton s method as 56 f 1 g 1 g 2 f 2 k δ p ˆ 1 δ p ˆ 2 t 1 k 1 r 1 r 2 t 1 k where δ and k now denote the change in solution and current iteration levels respectively the flow matrix in the jacobian is f α q α δ t t α where q α is the compressibility matrix and t α is the transmissibility matrix see lie 2019 for details for the matrix material in the dual porosity setting the transmissibility matrix has zero entries matrix g α is the coupling matrix arising from an inter porosity flow expression such as the linear approximation in eq 42 see ashworth and doster 2020 for details note terms arising from the linear transfer approximation will also appear in the flow matrices the solution vectors are p ˆ α p ˆ α 1 p ˆ α n c e l l where p ˆ α denotes the cell wise pressure solution for continuum α notation n c e l l is then the number of grid cells control volumes next r α t 1 k r α 1 r α n c e l l are the residual vectors for continuum α per grid cell intuitively r α t 1 k provides the difference between the rate of change of fluid mass within a cell volume to that generated by sources and or fluid movement through its boundaries at iteration level k finally we inject our machine learning model into eq 56 through an explicit approach as a result following a newton iteration the coupling matrices g α in eq 56 are zero entries whilst the machine learning based r 1 appears in the residuals 7 2 tests to test the hybrid ml physics model we consider several realisations of the 2d diffusion problem described in section 4 for these realisations we test our framework on initial conditions and time lengths not used for training our model accordingly we compare the hybrid approach against a microscale model and a physics based model that uses the first order mass transfer constitutive relation for brevity in the remainder of this section we refer to the latter method as the traditional approach to dual continuum modelling in addition to the 2d problem we also consider an application of the framework to a geological model for this case we only compare the hybrid and traditional dual porosity approaches for the 2d problem test cases we discretise the dp problem using a single control volume for the microscale model we resolve the matrix using 40 40 grid cells within the dp model we initialise the fracture pressure as p 2 0 1 mpa further to mimic the fracture boundary within the dp model we set fracture permeability to 10 d as a result the fracture pressure equilibrates almost instantaneously with the external boundary in response to changes induced by inter continuum flow the remaining properties are as described in section 5 specifically ϕ 2 0 1 1 0 3 c 2 c 1 1 4 gpa 1 v 1 0 999 ϕ 1 0 0 2 μ 1 cp n f 1 ℓ 1 m ρ 0 1000 kgm 3 and k 1 k 1 md with respect to ϕ 2 0 and c 2 these are chosen somewhat arbitrarily since we do not consider fracture dynamics for the problems herein lastly we make use of a discrete equivalent to eq 2 to compare the averaged microscopic matrix pressure field with its dual continuum counterpart for the geological model we use a grid taken from the open porous media initiative 2015 accordingly each control volume now corresponds to a 3d domain involving three orthogonal fracture sets leading to a 3d diffusion problem within the matrix as a result the analytical solution used to generate data is a higher dimensional version of eq 39 further details of this solution and the parameterisations used for this test are described as part of the test case descriptions introduced below case 1 p 1 0 1 1 1 0 6 pa for the first set of test cases we consider two initial pressures not used when constructing d but coming from 1 1 1 0 6 pa specifically we sample the initial pressures such that p 1 0 i 10 pa and 0 1 mpa p 1 0 ii where superscripts i and ii correspond to the two tests respectively the boundary pressure is fixed as p 2 1 mpa the parameters for this 2d problem are as described above case 2 t 1 10 s for the second set of tests we consider shorter sequences for training and use the resulting models for testing on the full sequence specifically we consider datasets up to t 1 s and t 10 s for training instead of a full dataset corresponding to t 100 s the motivation behind this test case is to see how the models perform when trained under limited data and used beyond the training data range accordingly extrapolation outside of the training conditions is now caused by the length of the testing sequence length relative to the training sequence length as well as due to the multi step ahead approach the parameters for this 2d problem are as described above with p 1 0 1 pa case 3 application to a geological model for the 3d problem eq 39 is extended as lim and aziz 1995 zhou et al 2017 57 p 1 p 1 0 p 2 p 1 0 1 n 0 m 0 o 0 8 π 2 3 1 2 n 1 2 2 m 1 2 2 o 1 2 exp π 2 k 1 t ϕ 1 0 c 1 μ ℓ 2 2 n 1 2 2 m 1 2 2 o 1 2 where we parameterise eq 57 using p 2 1 mpa c 1 3 gpa 1 ϕ 1 0 0 2 μ 5 cp ℓ 1 m and k 1 1 1 0 4 md as before we take 120 different p 1 0 sampled from the interval 1 1 1 0 6 pa further we generate series starting from t 0 and finishing at t 100 h with a uniform stepping interval of δ t 1 h finally we train and test on the resulting data using an autoregressive regularised polynomial regression model which is subsequently injected into our physics based simulator for our geological dp model the boundaries are specified as pressure boundaries set at 1 mpa next the macroscopic fracture permeability is set at 10 d accordingly the high fracture permeability ensures our numerical test problem is consistent with our learning problem in terms of pressure conditions for the fracture we initialise the matrix as p 1 0 5 pa to complete the dp model we assign ϕ 2 0 1 1 0 3 c 2 3 gpa 1 and ρ 0 850 kgm 3 we compare results over the geological model between the hybrid ml physics model and the physics based model using the linear transfer for the latter the transfer function is parameterised using n f 3 and k 1 1 0 4 md finally despite only training for a series corresponding to 100 h we run each simulation for a 1000 h thus extrapolating outside of the training conditions 7 3 results here we present the matrix pressure evolution results for the test cases described above case 1 p 1 0 1 1 1 0 6 pa results for the first test case are shown in fig 8 in both cases we see pressure increasing with time as the matrix equilibrates with the boundary pressure we can see for both tests there is a measurable discrepancy between the dp model using a linear transfer and the equivalent microscale description this discrepancy is particularly pronounced for test i fig 8 a for this test case the linear transfer is a particularly poor early time model when the difference between continuum pressures is several orders of magnitude in contrast the dp approach with the ml transfer produces high quality matches for both test cases without the computational expense taken to run the microscale models in fig 5 b we observed small errors in the middle to late time regularised pr predictions for test series corresponding to high initial pressures however fig 8 b shows despite these discrepancies the dp model using the data driven transfer still outperforms a linear transfer based dp model with respect to the microscale solution lastly for both tests we also provide the analytical solutions fig 8 the benefit of this addition is clearer in fig 8 a from fig 8 a we observe a small discrepancy between the microscale model and the analytical solution for the first few time steps this discrepancy arises due to the time step discretisation error in the microscale model remarkably we do not see this discrepancy between the ml based approach and the analytical solution since the learnt model comes from data unaffected by these discretisation errors this subtle result corroborates the aim of multiscale approaches specifically to combine the accuracy of microscopic representations with the practicality of macroscopic models case 2 t 1 10 s fig 9 shows the results for the different training and testing sequence length test cases fig 9 a shows the test case when using up to t 1 s of data as denoted by the grey vertical line in this figure we observe a good qualitative match relative to the microscale model using the hybrid approach over the first 1 s further even for time steps within a few seconds after the 1 s training limit the match between the hybrid and microscale models is good however from middle to late times fig 9 a shows we see the hybrid model diverging from the microscale model solution nonetheless at late times the model converges back to the microscale solution in fig 9 b the sequence limit is now at t 10 s as one would expect more data used for our surrogate model results in improved hybrid model performance across the sequence length further in fig 9 b there is a noticeably smaller divergence between the hybrid and microscale model results following the 10 s limit compared to that shown in fig 9 a following the 1 s limit this result is consistent with the general trend between surrogate model accuracy and data volume leading to a lower accumulation of errors nevertheless even with a surrogate constitutive model trained using t 1 s of sequence data the hybrid model still performs well over the whole simulation time from the first two test cases we have seen a traditional dual porosity model approach gives the worst results at early times at these times there is a significant different between matrix and boundary pressures however at later times the linear transfer relation is a reasonable approximation of inter continuum mass exchange accordingly it is wasteful to use a surrogate constitutive model in regions where a simple constitutive model performs well given the data requirements to train the former an interesting development therefore would be a hybrid model that can use different constitutive models in different regions e g nonlinear vs linear of a problem space march et al 2016 case 3 application to a geological model figs 10 to 11 show the results for the geological model test case using the traditional and ml physics model respectively in both figures we can see matrix pressure rising over time in response to diffusion driven equilibration with the fractures however we can see significant differences between the two modelling approaches following the initial conditions up until equilibrium conditions comparing figs 10 to 11 at t 1 h we can see the matrix pressures are noticeably lower when using the linear transfer compared to the hybrid ml physics approach this observation is consistent with the results in the previous tests which showed the matrix pressures to be significantly underestimated when using a linear approximation accordingly matrix fluxes are underpredicted when using this linear transfer approach such errors could be significant in geological settings such as groundwater remediation haggerty and gorelick 1995 similar differences between the two approaches to those just described are observed at t 50 100 h with the ml physics approach predicting higher pressures than the traditional approach when extrapolating outside of the training conditions the hybrid approach shows physically reasonable results consistent with the previous time steps at t 500 h accordingly at this time step matrix pressures coming from the hybrid approach are higher compared to the traditional approach however by t 1000 h the two models have converged to the equilibrated pressure state in summary the results presented for the described tests show the potential for the ml physics approach for multiscale modelling with this framework we can capture the accuracy of microscopic representations with the practicality of macroscopic models we envision a number of interesting extensions to be investigated using the basis of the framework introduced herein 8 conclusions and future work this paper introduces and applies a machine learning based multiscale constitutive modelling framework accordingly we detailed the key components of the framework as homogenisation data generation surrogate constitutive model learning and ml physics model coupling in doing we described various considerations that the user should make at each step to test the framework we considered a multiscale pressure diffusion problem specifically that of describing inter continuum mass transfer in dual porosity materials in practice it is common to model this process using an overly simple linear constitutive relation instead of making any such assumption we use our framework to build a surrogate constitutive model based on microscale data in applying the framework we introduced the various machine learning methods used to create this surrogate constitutive model we account for time dependence using autoregressive approaches considering polynomial and neural network based regressors we found the former to provide higher accuracy when used as a multi step ahead predictor provided regularisation is used to deal with multicollinearity effects accordingly we integrated the resulting data driven surrogate into a physics based dual porosity model creating a hybrid ml physics approach we showed the resulting hybrid approach to give high quality results for a number of test cases compared to a linear transfer based dual porosity model without the computational expense of explicit microscale models we hypothesise a number of exciting possibilities for further work when using data coming from more complex microscale scenarios e g dynamic boundary conditions than the ones considered here in such cases data is likely to be obtained numerically however numerical data can be expensive leading to sparse datasets accordingly in scientific computing domains we see sample efficiency as a key challenge for the uptake of machine learning based multiscale modelling in addressing this challenge future developments could involve exploring different sampling and learning strategies such as active learning theory driven ml and probabilistic approaches to improve learning efficiency generalisability and to understand uncertainty in multiscale settings credit authorship contribution statement mark ashworth conceptualisation methodology investigation writing review ahmed h elsheikh supervision review florian doster supervision review declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors are grateful for the funding provided to them by the natural environmental research council to carry out this work and to anne sophie ruget for advice on figures 
167,a multi phase and multi component numerical simulator assessed the removal efficiencies of light non aqueous phase liquid lnapl consisting of benzene toluene ethylbenzene and xylene p scenarios of the lnapl spilling natural distribution and remediation stages were designed in the full physics numerical modeling for lnapls remediation a multi phase extraction mpe and a steam injection technique were employed the removal efficiencies of lnapls were computed by systematically varying 6 factors that determine the configuration of the remediation wells then surrogate polynomial chaos expansion pce models mathematically predicting the removal efficiencies were developed through 600 training datasets representing 4 scenario cases different permeability and the location of si well were considered in the scenario cases the pce models were utilized for sobol global sensitivity analysis and stochastic monte carlo prediction as a result the depth of the mpe well was identified as the most significant factor in determining the removal efficiency of the lnapls the removal efficiency was maximized when the mpe well was positioned 1 5 m below the groundwater table additionally the contributions of influencing factors were significantly changed by the field permeability this study proposed a general framework that efficiently predicts lnapls remediation efficiency and identifies key influencing factors by combining advanced numerical modeling pce based surrogate modeling and sensitivity analyses keywords lnapls remediation removal efficiency polynomial chaos expansion sobol sensitivity analysis multi phase modeling monte carlo prediction 1 introduction light non aqueous phase liquids lnapls are typical contaminants that originate from various anthropogenic sources such as oil storage tanks gas stations transportation and military camps essaid et al 2015 huntley and beckett 2002 oostrom et al 2006 lnapls lighter than water float above the groundwater table and extensively contaminate both soil and groundwater zones when they spill into the subsurface lnapls contain deleterious components including benzene toluene ethylbenzene and xylene p btex these components can be volatilized into the unsaturated zone and concurrently dissolved into the groundwater while lnapls drifting above the groundwater table therefore the thermodynamic properties of lnapls that elucidate comprehensive phase transfer i e volatilization condensation and dissolution should properly be assessed however the assessment is challenging due to discrepancies in thermodynamical behaviors of multiple components btex within lnapls adenekan et al 1993 additionally migration of lnapls reflects multi phase transport within porous media representing the coexistence of lnapls air and water in this complex transport system both relative permeability and capillary pressure must be evaluated brennen and brennen 2005 oliveira and demond 2003 the complex dynamics of lnapls transport in the subsurface can be elucidated by conducting numerical simulations over the past two decades many studies have attempted to predict the migration of spilled lnapls via numerical simulations gupta et al 2019 kim and corapcioglu 2003 qi et al 2020 schroth et al 1998 wipfler et al 2004 although such studies successfully investigated the multi phase behavior of lnapls their approach limited lnapls as a single component while lnapls are generally observed as a mixture of several components essaid et al 2015 recently yang et al 2017 simulated multi component lnapls transport to evaluate the removal ratio of btex through soil vapor extraction the multi phase multi component lnapls simulation could be further enhanced by accounting for hysteresis effects and 3 d heterogeneity in permeability sookhak lari et al 2018 the results aided in simulating lnapl transport realistically and improved the assessment of lnapls removal efficiency however those full physics numerical modelings and subsequent analyses are not always viable such complex numerical analyses require enormous cpu time and memory spaces suited to supercomputing systems which may not be available to all users data driven surrogate modeling techniques have been suggested as alternative options to compensate for the difficulties caused by complexity in numerical models and to manage consecutive post analyses with heavy iterative computations asher et al 2015 razavi et al 2012 the surrogate modeling technique generates an empirical formula from several pairs of given input and output datasets predicted from complex numerical models the application of the formula is limited to the designed model domain or site specific represented by the chosen numerical simulator furthermore complex numerical models which is a basis of surrogate models may not provide a perfect description of physics as well as subsurface heterogeneity despite limitations addressed above the surrogate modeling technique has been in the limelight as decision support in the field of hydrogeology due to its ability to represent the dominant features in the subsurface with cheaper computational costs asher et al 2015 since the verified surrogate models allow for the prediction of the outputs without requiring heavy numerical simulations the demanding time for the computation is significantly reduced a key advantage from the reduction in the computational time is the availability of post analyses using stochastic methods in recent years the stochastic post analyses based on the surrogate model have been used to analyze seawater intrusion rajabi et al 2015 geologic co2 sequestration guyant et al 2016 jia et al 2016 piao et al 2020 and shallow groundwater contamination ciriello and de barros 2020 xing et al 2019 zhao et al 2020 as seen although the applications for the surrogate modeling techniques have expanded limited surrogate modeling and post analyses have been attempted in the field of lnapl transport and its remediation in this study a state of the art technique of surrogate modeling the polynomial chaos expansion pce was employed to link between the full physics numerical lnapl model and stochastic post analyses after developing the surrogate pce model sobol global sensitivity analysis was conducted to rank important factors that affect multi phase and multi component lnapl remediation in addition monte carlo analysis was conducted to comprehensively investigate the effects of each factor on remediation efficiency eventually this study proposes an efficient framework that quantitatively investigates the influencing factors and can be easily extended for optimizing various lnapls remediation scenarios 2 conceptual model a 2d full physics numerical model was developed to investigate the dynamics of lnapls in unsaturated and saturated zones as shown in fig 1 a it was assumed that the lnapls a mixture of btex were spilled from a designated point source the spilled lnapl plume migrated downward through the unsaturated zone until reaching the groundwater table due to buoyancy force the lnapls floated above the groundwater table in the capillary fringe zone where lnapls air and groundwater coexist lnapl dynamics were governed by gravity and capillary pressure bear et al 1996 mayer 2005 fig 1b here lnapls contemporaneously volatilized to the unsaturated zone as highly volatile components hence btex components are also referred to as volatile organic compounds vocs fig 1c kim and corapcioglu 2003 russell 1995 the amounts and rates of volatilized btex components are determined based on the partitioning coefficient and saturated vapor pressures mendoza and frind 1990 in addition at the bottom of the lnapl plume where the groundwater is contacted some of the lnapls dissolve into groundwater fig 1d the amounts of partially dissolved lnapls are determined by the solubilities of each btex component such addressed interactions including chemical dissolution and thermophysical phase transfer affect the fate of lnapl migration in the subsurface kim and corapcioglu 2003 in this study three different fluids were accounted for including gas phase air water vapor and vocs lnapl phase and aqueous phase residually trapped water in soil and water dissolved in lnapls and air because air is a dominant component in the unsaturated zone the gas phase in the unsaturated zone is referred to the air phase in addition the aqueous phase in the saturated zone is referred to the water phase 2 1 model domain initial boundary conditions and sources a base case lnapls spill and remediation scenario which can be realized with four modeling stages was considered the 1st stage is for the development of a sloping groundwater flow the 2nd stage is the spill of lnapls 3rd stage is the migration of lnapls under the ambient condition and the 4th stage is the active remediation of the lnapls 2 1 1 groundwater flow 1st stage model the model domain shown in fig 2 a represents the dimensions of 100 m by 50 m consisting of horizontally discretized 66 grid blocks per each of 50 layers δz 1 m from the surface to the bedrock porosity and permeability kx were uniformly assigned to 0 3 and 4 0 10 13 m2 to the model domain respectively the anisotropy kz kx was assigned to 0 25 assuming horizontal arrangement of soil deposits in the sedimentary layers nielsen et al 1986 scholes et al 2007 the groundwater flow model was developed in two stages in the 1st stage model a sloping groundwater table was developed which was the initial condition of the 2nd stage model simulating lnapl migration in the 1st stage model the constant atmospheric temperature and pressure 25 c and 1 01 105 pa conditions were assigned to the top boundary fig 2a immediately below the top boundary grid blocks an infiltration rate of 4 64 10 6 kg s was assigned to represent infiltrating precipitation the assigned infiltration rate was estimated from the annual average precipitation in the republic of korea both lateral boundaries were assigned to the constant pressure boundary with pressure gradients left boundary 1 01 105 pa top to 3 72 105 pa bottom and right boundary 1 01 105 pa top to 3 52 105 pa bottom the pressure difference between left and right boundaries is attributed to the groundwater flow the bottom boundary was set to no flow owing to the presence of low permeability base rock initially the flat groundwater table was at a depth of 21 m by setting water saturation of 0 2 and 0 99 at unsaturated and saturated zones respectively after simulating the 1st stage model the equilibrated sloping groundwater table was achieved and the resulting average groundwater velocity was 0 022 m day 2 1 2 lnapls spilling and remediation 2nd 3rd and 4th stage models in the 2nd stage model lnapl spilling stage the lnapls composed of btex spilled for 5 years from the point source located 37 5 m away from the left boundary and 4 5 m below the ground surface fig 2b the spilling constantly occurred at a rate of 1 65 10 5 kg s per each btex component the primary direction of the lnapl plume migration is the gravitational direction at this stage while its migration is governed by the relative saturation of air and lnapls in the unsaturated zone at the end of the 2nd stage the lnapl spilling ceased during the 3rd stage natural redistribution stage that lasted for 2 years the spilled lnapls were naturally redistributed in the capillary fringe primarily spreading above the sloping groundwater table fig 2c finally in the 4th stage remediation stage that lasted for 0 5 years both the steam injection si and the multi phase extraction mpe wells were installed to remediate the lnapls fig 2d the si well was installed below the lnapl source at a depth between 20 m and 22 5 m where 100 c steam was injected at a rate of 6 10 4 kg s to aid phase transfer from high viscosity liquid lnapls to low viscosity vapor lnapls in addition the mpe well was installed horizontally 5 m away from the lnapl source at a depth between 16 m and 20 m to extract both vapor and liquid phases of lnapls at the mpe well a constant bottom hole pressure bhp of 8 104 pa was assigned which was approximately equivalent to the rate of 5 7 10 3 kg s finally four modeling stages representing groundwater flow lnapl spilling natural redistribution and remediation were simulated using tough2 tmvoc pruess and battistelli 2002 2 2 parameters for 3 phase flow systems and thermodynamic properties of lnapls two of the major aspects governing the dynamic behaviors of lnapls are capillary pressure and relative permeability these are parameters characterizing multi phase fluid systems in porous media such multi phase effects spatiotemporally vary depending on the saturation of the individual fluid phase and the characteristics of porous media lenhard and parker 1987 miller et al 1998 the capillary pressure caused by the relative difference in wettability and interfacial tension between two immiscible fluids is one of the major factors governing lnapl migration in unsaturated zones anderson 1987 hassanizadeh and gray 1993 lnapls are the intermediate wetting phases among the three phases air water and lnapls therefore when the lnapls come into contact with air the lnapls and air are treated as wetting and non wetting fluids respectively whereas at the lnapls contacting with water the lnapls and water are treated as non wetting and wetting fluids respectively in the present study the 3 phase capillary pressures were calculated from parker s 3 phase function which was derived to solve multi phase organic contaminant transport in the unsaturated zone kaluarachchi and parker 1990 wu and forsyth 2001 the capillary pressures were calculated at the interfaces between the air lnapls p c al lnapls water p c lw and the water air p c wa the mathematical function for 3 phase capillary pressures are presented in table 1 where ρ w is the water density dependent on subsurface pressure and temperature pruess and battistelli 2002 and g is the gravitational acceleration 9 81 m s2 the α al α lw and n are empirically determined parameters and sm is the residual wetting phase water saturation finally sw and sl represent the saturation of water and lnapls respectively while groundwater lnapls and air concurrently migrate through porous media their movements are restricted according to variability in relative permeability consequently the average velocities of the three competing fluids decrease alizadeh and piri 2014 bradford et al 1997 in this study the 3 phase relative permeabilities kr were calculated from the stone i model which is commonly used for water wet porous media baker 1988 blunt 2000 stone 1970 in the stone i model the relative permeabilities of air k r a water k r w and lnapls k r l are functions of the relative saturation of each fluid phase as shown in table 1 here sa sw and sl are the saturation of air water and lnapls respectively in addition sar swr and slr representing residual saturations of air water and lnapls respectively were obtained from erning et al 2009 juanes 2003 and pruess 2003 respectively finally n is an adjustable fitting parameter in addition to both capillary pressure and relative permeability accurate prediction of lnapl migration in the subsurface requires the computation of thermophysical properties in individual btex component and their mixtures hence phase transfers volatilization condensation and dissolution of btex which are dependent on vapor pressure water solubility and henry s constant were accounted for at the designated temperature pressure and chemical compositions table 2 corresponding computations are referred to pruess and battistelli 2002 and reid et al 1987 3 workflow the workflow followed the 4 steps 1 sampling 2 development of full physics numerical model based surrogate model 3 global sensitivity analysis and 4 monte carlo prediction 3 1 sampling and development of surrogate polynomial chaos expansion model latin hypercube lh sampling is a common space filling method that enables uniform sampling with reasonable computational costs viana 2013 in multi dimensions the sampling method searches the sampling points p within orthogonal grids where each non overlapped set of rows and columns satisfied the space filling property the sampled combination of n number of input factors x x 1 x 2 xn and corresponding response y computed from the full physics numerical lnapl model composes the training dataset used to build a surrogate model from x and y the surrogate polynomial chaos expansion pce model can be established following blatman and sudret 2008 1 y m x α n n λ α ψ α x where λα is the deterministic chaos expansion coefficient and ψα x is the multivariate orthonormal basis polynomial assuming the independence between the input factors in x the ψα x is established by the multiplication of univariate polynomials of each input factor x i 2 ψ α x i 1 n ψ α i x i where each ψ α i is the univariate polynomial of degree α i with respect to the i th input factor the type of the ψ α i is selected based on the type of probability density function pdf of x i considering the uniform and gaussian pdfs the ψ α i x i are selected to normalized legendre and hermitian polynomials respectively xiu 2010 because the input factors employed in this study were all assumed to be uniform distribution the type of the univariate polynomial ψ α i x i was regarded as the legendre polynomial for the practical computation of the targeted pce from the legendre polynomials of x which was linearly transformed into 1 1 n m x would be truncated up to the total degree of the ψα x ω 3 m x m t x α a n ω λ α ψ α x x u 1 1 n where the truncated pce m t x is then remained as the finite terms corresponding to subsets of a n ω which refers to the truncation set satisfying α i 1 n α i ω to compute the designated number of legendre polynomials ψα x and the corresponding chaos expansion coefficients λα in a n ω the cardinality of a n ω should be determined to be p n ω n ω here n and ω represent the number of input factors and the total degree of the ψα x respectively after determining multivariate basis polynomials ψα x λα can be calculated through either intrusive or non intrusive schemes song et al 2019 in the present study least squares minimization lsm in the non intrusive scheme is utilized to calculate λα here to achieve the λα accurately the number of sampling points p from the lh sampling should be greater than p blatman and sudret 2008 2010b 3 2 global sensitivity analysis the sobol global sensitivity analysis is a type of variance based sensitivity analysis which is applicable when the input variables are independent the sobol scheme practically results in quantified sensitivity indicators sobol indices from the monte carlo simulation blatman and sudret 2010a and sudret 2008 provided a method for the straightforward computation of sobol indices from the pce this approach significantly reduces the computational cost to calculate the sobol indices requiring 2 to 3 orders of magnitude fewer simulations than the monte carlo simulation for the pce of the multivariate legendre polynomials eq 3 can be expressed as 4 y f x m t x j 0 p 1 λ j ψ j x x u 1 1 n the mean and variance of the pce were computed based on the orthogonality of the basis polynomial as follows 5 y e f x λ o 6 v p c e v a r j 0 p 1 λ j ψ j x j 0 p 1 λ j 2 e ψ j 2 x eq 4 can be expanded by sobol decomposition after derivation sudret 2008 as shown below 7 f x m t x λ 0 i 1 n α h i λ α ψ α x i 1 i 1 i 2 n α h i 1 i 2 λ α ψ α x i 1 x i 2 1 i 1 i s n α h i 1 i 2 i s λ α ψ α x i 1 x i s α h i 1 i 2 n λ α ψ α x 1 x n where each summand α h i 1 i s λ α ψ α x i 1 x i s are orthogonal polynomials produced by x i 1 x i s and h i 1 i s is the set in α which indicates that only the x i 1 x i s are involved in the summand as a result of squaring both the left and right sides of eq 7 the sobol indices combined with the pce s i i 1 i s for any h i 1 i s can be obtained as the proportion of the partial variance to the total variance 8 s i i 1 i s α h i 1 i s λ α 2 e ψ α 2 v p c e typically two types of sobol indices are used the sobol 1st order indices s i i 1 1 s t α h i 1 λ α 2 e ψ α 2 v p c e capable of showing the only influence of each independent variable on output and the sobol total order indices s i i 1 t i 1 i 1 i s s i i 1 i s considering additional effects by the interactions of the independent variables 4 results 4 1 full physics numerical lnapls transport model base case 4 1 1 spatiotemporal distribution of air lnapls and water simulation results for phase saturation sl sa and sw at the end of 2nd t 5 years 3rd t 7 years and 4th t 7 5 years stages are shown in fig 3 the sources for spilling lnapls are marked as a red circle and the white line represents the groundwater table during the 2nd stage lnapl spilling stage lnapls migrated downward through the unsaturated zone until they reached the capillary fringe above the groundwater table fig 3a the lnapl plume in the unsaturated zone expanded symmetrically owing to the homogeneous permeability and dispersion the average sl in the unsaturated zone was 0 11 the corresponding sa and sw were concurrently changed to 0 65 and 0 24 respectively fig 3b and 3c since sa and sw were 0 75 and 0 25 before lnapls spill changes in sa during the 2nd stage indicated that the spilled lnapls primarily displaced the low viscosity air in the pore space immediately above the groundwater table the lnapls accumulated in the capillary fringe the capillary fringe served as a primary flow path that allowed the lnapls to spread horizontally mayer 2005 in the model before the lnapls spill the thickness of the capillary fringe was approximately 3 m with associated sa and sw of 0 45 and 0 55 respectively fig 3b and 3c once the lnapl plume reached the capillary fringe it primarily displaced low viscosity air the sl increased from 0 to 0 45 fig 3a while sa decreased from 0 45 to almost 0 0 fig 3b additionally due to the difference in both density and wettability between lnapls and groundwater the downward movement of lnapls below the groundwater table was hindered instead the lnapls migrated approximately 9 m along the sloping groundwater table finally owing to the counter buoyancy force acting on the lnapl plume the groundwater table was suppressed approximately 1 5 m and consequently the capillary fringe was thickened during the 3rd stage natural redistribution stage sl in the unsaturated zone uniformly dropped to 0 052 close to the assigned slr 0 05 table 1 the corresponding k r l calculated from the stone i model was almost 0 indicating that most lnapls remained as the residually trapped phase in the unsaturated zone fig 3d above the groundwater table the mobile lnapls continuously accumulated while the lnapl plume spread horizontally above the sloping groundwater table at the end of the 3rd stage the maximum sl of the floating lnapl plume increased to 0 462 in addition sa in the unsaturated zone nearly returned to the initial saturation because the air re filled pore spaces where movable lnapls were left fig 3e finally the sl sa and sw are depicted at the end of the 4th stage remediation stage where the remediation scheme was implemented fig 3g 3h and 3i as mentioned in section 2 1 2 two remediation wells including steam injection si and multi phase extraction mpe wells were operated the screen depths of each well are highlighted in red and blue boxes respectively at the end of the remediation stage the sl of the floating lnapl plume rapidly decreased to 0 22 mainly due to extraction of the movable lnapls and phase transfer from liquid lnapls to volatilized lnapls by high temperature steam injection fig 3g the pressure gradient induced by the mpe well aided to extract low viscosity volatilized lnapls additionally the pressure gradient induced by the two remediation wells distorted the capillary fringe for instance adjacent to the si well a hole where sa was increased to 0 69 due to both volatilized lnapls and injected steam was developed fig 3h in the unsaturated zone the simultaneous operation of the two remediation wells was able to remove trapped lnapls sl was decreased to below 0 05 fig 3g at the bottom of the mpe well both sl and sw increased indicating that both lnapls and groundwater were effectively extracted by the mpe well fig 3g and 3i 4 1 2 btex in air lnapls and water phases the distributions of btex benzene toluene ethylbenzene and xylene p at the end of the 3rd stage were delineated to elucidate the different behaviors of the spilled btex in the subsurface based on their thermophysical properties fig 4 xl xa and xw represent the mole fraction of btex existing in lnapls air and water respectively in the lnapl phase the btex components were evenly distributed although their average xl values were different due to the difference in molar mass fig 4a the xl of btex were 0 3 0 25 0 22 and 0 22 respectively the mole fractions of btex in the air phase are delineated in fig 4b which reveals a clear difference depending on the volatilization of btex the degree of btex volatilization was related to the saturated vapor pressure table 2 benzene characterizing the largest saturated vapor pressure 12 523 50 pa showed the maximum xa 0 0372 which was 8 5 times greater than that of xylene p 0 00436 having the lowest saturated vapor pressure 1169 47 pa additionally benzene which had the largest solubility 0 411 g l among the btex showed the maximum xw of 1 24 10 4 fig 4c in summary among btex benzene was the dominant contaminant due to its high saturated vapor pressure and water solubility 4 1 3 removal efficiency of lnapls the btex distribution at the end of the 4th stage remediation stage after 7 5 years is shown in fig 5 at the end of the remediation stage the xl of benzene was the lowest 0 301 among the btex components indicating that the largest amount of removed components was benzene fig 5a the largest removal of benzene was due to effective phase transfer of benzene from liquid lnapls to volatilized lnapls responding to steam injection by the si well the temperature of the injected steam was approximately 100 c and the boiling temperature of benzene was 80 c whereas those for other components were over 100 c table 2 the results of such active volatilization in benzene were shown as the largest xa among the btex components fig 5b the maximum xa of benzene after the remediation was 0 31 while the average xa of other components remained at low quantities t 0 11 e 0 08 and x 0 08 fig 5b the increased xa of benzene accelerated the benzene transport to the mpe well by reducing the viscosity and increasing the relative permeability in contrast to xa of benzene xw of benzene was not significantly reduced by the remediation fig 5c this was due to the relatively small viscosity and small relative permeability of water at the unsaturated zone water was residually trapped in the pores and btex components dissolved in such residual water could not be extracted by the mpe well to evaluate the removal efficiency re a mass of component z in phase p p z m before and after remediation at the end of the 3rd and 4th stage was calculated the re p z r e 1 p z m 4 t h p z m 3 n d was then defined and the res of individual components b t e and x in lnapls air and water phases were calculated bar graph in fig 6 additionally the res t z r e 1 t z m 4 t h t z m 3 n d of individual components b t e and x in the total lnapls air water phase were delineated red circle in fig 6 the res of btex components between the lnapl l b r e l t r e l e r e and l x r e and total t b r e t t r e t e r e and t x r e phases were nearly the same with values of approximately 0 23 0 13 0 06 and 0 06 respectively fig 6a this indicated that the btex primarily existed as the lnapl phase although phase transfer to air and water had occurred the removed masses of btex in the lnapl phase are proportional to their volatilities table 2 here benzene showed the largest volatility as more benzene changed to the air phase the mobility of benzene in the air phase was enhanced then the mpe well accelerated the removal rate of benzene in the air phase consequently the a b r e 0 4 exceeded t b r e 0 23 nevertheless a t r e 0 08 a e r e 0 84 and a x r e 0 97 showed negative values indicating that lower volatile components were not efficiently removed by the mpe well after they had been volatilized fig 6b the removal efficiencies of benzene toluene ethylbenzene and xylene p in the water phase w b r e w t r e w e r e and w x r e showed the smallest deviation from 0 fig 6c benzene and toluene with high solubility specifically showed positive w b r e 0 14 and w t r e 0 01 respectively but ethylbenzene and xylene p with low solubility showed negative w e r e 0 14 and w x r e 0 17 respectively in summary the components showing effective phase change represented high removal efficiency 4 2 surrogate polynomial chaos expansion lnapls models 4 2 1 4 cases permeability and the location of steam injection wells including base case case i three additional cases were designed to evaluate the effect of permeability k and the location of the steam injection si well table 3 in case ii kx 4 0 10 13 m2 and kz 1 0 10 13 m2 was equal to case i but the location of the si well was shifted to the right of the mpe well fig 7 a and 7b at both case i and ii represented by high k lnapl plumes spread widely above the groundwater table while residually trapped lnapls remained in the unsaturated zone the res of total phase btex t b r e t t r e t e r e and t x r e for case i were similar to those for case ii indicating that the relative location of si to the mpe well did not influence a degree of remediation efficiency much fig 7e and 7f the t b r e 0 25 and 0 20 t t r e 0 13 and 0 10 t e r e 0 04 and 0 04 and t x r e 0 04 and 0 04 were predicted at case i and ii respectively for case iii and iv both kx and kz decreased 10 times to 4 0 10 14 m2 and 1 0 10 14 while relative locations of the si well were equal to one for case i and ii respectively fig 7c and 7d a decrease in k caused local accumulation of the lnapl plume with preventing horizontal expansion the small and concentrated lnapl plume caused less contact with the surrounding air and thus the amount of volatilized lnapls decreased additionally decreased mobility of lnapls due to low k let more lnapls remain in the unsaturated zone and impeded lnapl migration to the mpe well causing a decrease in t b r e t t r e case iii and iv in fig 7g and 7h 4 2 2 input factors and responses for assessing removal efficiency of lnapls for each of the 4 cases the 6 input factors governing the effectiveness of the remediation wells were selected table 3 the screen depths of both the mpe and si wells x 1 and x 2 the lateral distance between the mpe and si wells x 3 the bhp of the mpe well x 4 and the injection rate x 5 and the temperature of the si well x 6 the 6 input factors were acknowledged as influencing factors governing the removal efficiency of lnapls mccray and falta 1997 robin and gillham 1987 rogers and ong 2000 these input factors were assumed to have a uniform distribution with minimum and maximum limits table 3 within these ranges the sampling points representing the different combinations of input factors were selected through lh sampling for example the x 1 and x 2 were selected within the vertical sky blue and red scale lines represented in fig 7 respectively finally a single sampling point determined a single configuration of wells in each case a total of 150 points were sampled 600 points in 4 cases based on lh sampling points a total of 600 full physics numerical lnapl transport models were conducted to obtain responses the responses were the removal efficiencies of the total sum of btex t b t e x r e 1 t b t e x m 4 t h t b t e x m 3 n d calculated at the end of 4th stage 7 5 years then both input factors and responses served as the training dataset in developing the surrogate polynomial chaos expansion pce models of 4 cases 4 2 3 development of surrogate pce model validation and test pce models predicting t b t e x r e were developed using the following procedure with a given training dataset e g a combination of the 6 input factors and the responses multi variate basis functions ψα for the pce models were developed preliminary pce models were then developed by evaluating the determination of the coefficients λα which were calculated from the least square minimization lsm method however the preliminary pce models based on lsm may preserve unnecessary complexities and overfitting problems blatman and sudret 2010b to overcome these problems the preliminary pce models were improved through the validation process both an adaptive sparse algorithm with the least angle regression and a leave one out loo cross validation algorithm were used for the validation the adaptive sparse algorithm is a step wise procedure that reduces the complexity of the pce model by eliminating the insignificant ψα among the total p number of ψα blatman and sudret 2011 the total p p n ω n ω number of ψα was 210 n 6 and ω 4 210 n 6 and ω 4 210 n 6 and ω 4 and 84 n 6 and ω 3 in case i ii iii and iv respectively after applying the advanced adaptive sparse method with the least angle regression the total p number of ψα in preliminary pce models decreased to 29 30 53 and 14 in case i ii iii and iv once the pce models were streamlined the deterministic chaos expansion coefficients λα were subsequently modified to solve the overfitting problem by minimizing the loo error εloo blatman 2009 9 ε loo i 1 n m x i m p c i x i 2 i 1 n m x i μ y 2 where n is the number of the training dataset n 150 m x i is the i th response calculated from a sparse pce model developed by n training data m p c i x i is the i th response of a sparse pce model but the model derived from n 1 training data by excluding the i th training data and μ y is the mean of the n responses finally the optimal εloo values of the sparse pce models were calculated to be 0 036 0 044 0 029 and 0 066 in case i ii iii and iv respectively once the optimal εloo was calculated the corresponding λα values were selected to determine validated pce models in fig 8 the t b t e x r e calculated from both the validated pce models and the full physics numerical lnapls transport models are plotted for cases i ii iii and iv here 150 yellow circles and 50 green triangles indicate the training and test data respectively in each case the validated pce models developed using 150 training data were tested with 50 test data which were randomly chosen by the lh sampling using 50 test data the determination coefficient r 2 and normalized root mean squared error nrmse were calculated to investigate the predictability of the validated pce models the predictability of the validated pce models was the highest in case iv fig 8d r t e s t 2 and nrmse test were 0 930 and 0 067 respectively even for case ii showing the lowest predictability fig 8b the predictability was still acceptable r t e s t 2 0 889 and nrmse test 0 11 ensuring that the validated pce models were capable of substituting the responses of the full physics numerical lnapls transport models interestingly both training and test datasets were distributed differently depending on the magnitude of permeability k indicating that even small differences in k significantly influenced t b t e x r e at high k values cases i and ii both training and test datasets were widely distributed between 0 and 0 55 fig 8a and b and the widely distributed t b t e x r e indicates that the t b t e x r e was sensitive to remediation conditions represented by 6 input factors dependent on suitable combinations of 6 input factors the t b t e x r e could be maximized in contrast at low k cases iii and iv most of the training and test data were leaned to the 0 small t b t e x r e denoting that t b t e x r e cannot be improved significantly by the choice of the 6 input factors in fig 7 the distribution of the lnapl plume at both high k and low k is shown at high k the lnapl plume spread widely over the groundwater table and even migrated far from both mpe and si well fig 7a and b nevertheless t b t e x r e could be maximized dependent on the choice of 6 input factors fig 8a and b this is because the lnapls can be more easily mobilized by remediation wells when k is high at low k however although the lnapl plume was located close to both the mpe and si well fig 7c and d the overall t b t e x r e was still small because the high pumping rate of the mpe well did not improve the mobility of the lnapls fig 8c and d such a difference in t b t e x r e implies that the influence of k on the remediation of the lnapl plume presumably prevails over any 6 input factors therefore at contamination sites with extremely small k e g clay dominant or fractured rock sites changes in 6 input factors may not be able to improve t b t e x r e significantly 4 3 analysis using surrogate pce models 4 3 1 sobol global sensitivity analysis using the validated pce models the influence of the input factors x 1 x 2 x 3 x 4 x 5 and x 6 on the t b t e x r e were assessed through sobol global sensitivity analysis case i in fig 9 a case ii in fig 9b case iii in fig 9c and case iv in fig 9d for each case the sobol total t and first 1st order indices were calculated with 150 realizations of the pce model whereas they would be yielded by more than 104 realizations with a common monte carlo simulation sudret 2008 sobol t indices blue bar which include the effect of intercorrelation between the input factors are always greater than sobol 1st indices purple bar similar to fig 8 the sobol sensitivity analyses revealed differences primarily dependent on k at high k sobol t and 1st indices for x 1 were dominant indicating that the t b t e x r e was largely influenced by the x 1 fig 9a and b the sobol t and 1st indices were 0 84 and 0 80 for case i and 0 86 and 0 81 for case ii respectively the thickness of the lnapl plume is thin and widely spread over the groundwater table at high k thus t b t e x r e was highly influenced by the choice of screen depth in the mpe well the 2nd influential factor was x 4 the sobol t and 1st indices were 0 14 and 0 11 for case i and 0 14 and 0 10 for case ii respectively even though the values for x 4 are significantly smaller than those for x 1 their influences are still superior to the other input factors this indicates that sufficient pressure gradient generated by the mpe well effectively enhances the t b t e x r e at high k finally the sobol indices of the other input factors x 2 x 3 x 5 x 6 related to the si well were nearly zero at low k the most sensitive factor was x 1 identical to high k although the influence of x 1 was decreased compared to cases i and ii the sobol t and 1st indices of the x 1 were 0 54 and 0 40 for case iii and 0 63 and 0 44 for case iv respectively fig 9c and d the difference between the sobol t and 1st indices increased indicating that the interaction between x 1 and the other factors e g steam injection rate became more important at low k the 2nd influential factor was x 3 the sobol t and 1st indices of the x 3 were 0 38 and 0 25 for case iii and 0 45 and 0 25 for case iv respectively at low k the spreading of the lnapl plume was constrained and consequently the horizontal location and vertical depth of the mpe well became influential additionally the influence of other input factors x 2 and x 5 related to the si wells increased these results suggest that steam injection would be more effective for lnapl remediation at the low k site finally the change in x 6 within the designated range i e 100 150 c did not affect the t b t e x r e indicating that steam temperature above 100 c is not cost effective for remediating the btex 4 3 2 predicting lnapls remediation efficiency the influences of 6 input factors on t b t e x r e was comprehensively quantified using the validated pce models a randomly generated 6 input factors determined a single prediction of t b t e x r e through the pce model and 105 monte carlo implementations revealed density plots of 6 input factors relating to empirical probability density functions of t b t e x r e fig 10 the 6 density plots showing the correlation between 6 input factors along the x axis and t b t e x r e on the y axis are depicted for the 4 cases the solid lines represent the linear fitting curves additionally histograms and box whisker diagrams were plotted to show the statistical distribution of t b t e x r e similar to sobol global sensitivity the pce based monte carlo prediction also differed significantly depending on the magnitude of k cases i and ii vs cases iii and iv at high k cases i and ii the slopes of the fitting curves for the screen depth of the mpe well x 1 were the steepest confirming that x 1 had a strong effect on t b t e x r e fig 10a and b interestingly the bimodal distribution for x 1 which is distinguished by the depth of the groundwater table was shown when the mpe well was installed above the groundwater table 21 5 m the average t b t e x r e was approximately 0 15 however the t b t e x r e dramatically increased to over 0 4 when x 1 was slightly below the groundwater table the t b t e x r e was maximized approximately 1 5 m below the groundwater table this is attributed to the characteristic of lnapls floating above the groundwater table when the mpe well was installed in the unsaturated zone gaseous lnapls volatilized by steam injection were only removed however when the mpe well was located below the groundwater table both liquid lnapls and lnapls dissolved in groundwater were removed effectively qi et al 2020 in addition the hydraulic head gradient developed by remediation wells accelerated the migration of the lnapls to the mpe well simon et al 1999 different from x 1 the screen depth of the si well x 2 did not influence t b t e x r e significantly the t b t e x r e slightly improved as x 2 was shallow the distance between the mpe well and si well x 3 showed opposite slopes in cases i and ii because of the opposite direction of the lnapl source fig 7a and b the t b t e x r e increased as x 3 was close to the lnapl source the 2nd largest input factor was the bhp of the mpe well x 4 as x 4 approached the low limit 7 104 pa the average t b t e x r e increased the effect of both the steam injection rate x 5 and steam temperature x 6 was insignificant for lnapl removal finally the histogram of t b t e x r e featured two peaks high t b t e x r e and low t b t e x r e primarily split by the depth of the groundwater table at low k the influence of x 1 was weaker than that of high k fig 10c and d but its influence was still the largest among the 6 input factors similar to high k t b t e x r e increased when x 1 was located below the groundwater table the x 3 was the 2nd important input factor as x 3 was far from the lnapls source the t b t e x r e approached zero this indicates that the distance between the lnapl source and remediation wells is critical at the low k field where lnapls migration is hindered while the influence of x 3 became significant the effect of x 4 decreased at low k the effect of both x 5 and x 6 was small similar to that for high k overall at low k the t b t e x r e were significantly smaller than high k which is also reflected in the histograms and the box whisker plots this implies that the optimum choice of 6 input factors is more important for the high k 5 implications and limitations of pce models in this study four surrogate pce models accounting for lnapls remediation efficiency were developed for different conceptual cases case i ii iii and iv for each case the number of training datasets to develop the pce models was 150 however the application of advanced adaptive sparse pce algorithm could substantially reduce the required number of the dataset by eliminating the non influential interaction terms blatman and sudret 2011 such computational advantages by reducing the number of training datasets would allow for developing surrogate pce models representing more complex 3d heterogeneity numerical models with realistic geostructures and a large number of input factors such as hydrogeologic properties e g permeability porosity water saturation soil properties e g particle size distribution and capillary pressure or geochemical properties e g biodegradation coefficient absorption coefficient additional to computational benefits the surrogate pce model can analytically link to the global sensitivity analysis and then be easily interconnected with other quantitative analyses fajraoui et al 2011 for example the global sensitivity analysis in this study revealed a crucial feature in lnapls remediation that the installation depth of the mpe well dominantly affected lnapls remediation efficiency subsequent monte carlo prediction further revealed the optimum depth of the mpe well that also supported the results obtained by numerical simulations in other previous studies qi et al 2020 despite the aforementioned advantages of surrogate pce models in lnapls remediation researchers should be aware that the surrogate pce model can not be a general or ultimate solution to interpret the lnapls transport behavior in the subsurface firstly the surrogate pce model developed is valid only in the conceptual model domain represented by numerical simulation in the study the numerical model was 2d and homogeneous thus the surrogate pce model should be reevaluated if researchers wish to investigate heterogeneous subsurface systems 3d lnapls transport or different target contaminants secondly the surrogate pce model solely depends on the internal computation results of the full physics numerical lnapls transport model due to this reason the choice of different numerical simulators may produce different pce results accordingly utilizing a more accurate and effective surrogate pce model should be accompanied by improvement of the underlying numerical model for example recent studies considered more realistic physical properties in simulating lnapls migration such as hysteresis effect of multi phase fluids and also verified the lnapls model from the implementation of lnapls tank laboratory experiments pasha et al 2014 sookhak lari et al 2016 6 conclusion full physics numerical simulations that honor multi phase multi component lnapl transport and its remediation are challenging primarily because of their extensive memory and cpu requirements in addition to the complexities involved in geologic characterization and lnapl transport numerical simulations must be repeatedly performed to characterize uncertainties involved in input factors or to search for the optimum lnapl removal efficiency in this study to overcome the challenges addressed in lnapl transport and remediation the surrogate pce modeling technique was implemented in conjunction with the quantitative sobol sensitivity analysis and monte carlo prediction the proposed workflow involves forward full physics numerical lnapls transport modeling that generates a training dataset development of a pce based surrogate model global sensitivity analysis and monte carlo prediction for the full physics numerical modeling the tough2 tmvoc was employed to simulate multi phase and multi component lnapl transport the conceptual model delineated the scenarios of the lnapl spilling natural distribution and remediation stages here lnapls were assumed to be a mixture of btex components allowing mass transfer e g vaporization condensation and dissolution among individual components finally the removal efficiency of lnapls was assessed by implementing mpe and si wells the 4 cases considering different permeabilities and locations of the si well were designed to develop pce based surrogate models within 4 cases 6 factors related to well configuration were varied to assess the removal efficiency of lnapls by combining forward numerical modeling global sensitivity analyses and monte carlo prediction the governing factors improving the efficiency of lnapl remediation were identified first the screen depth of the mpe well was the most important among 6 factors associated with well configuration regardless of field permeability or location of the si well because the lnapl plume floated within the narrow capillary fringe above the groundwater table the maximum remediation efficiency was predicted when the depth of the mpe well was slightly 1 5 m below the groundwater table second the permeability of contaminated sites evidently influenced the remediation conditions at high k both the screen depth and bhp of the mpe well are important but the influence of the bhp of the mpe well was diminished at low k instead the distance between the mpe and si wells became important finally at high k the mpe well itself was enough to remove the lnapls and thus the configuration of the si well was not as important however at low k where the mobility of lnapls was small the si well improved the remediation efficiency by volatilizing liquid lnapls to gaseous lnapls this study successfully demonstrated the capability of the pce based surrogate modeling for the quantitative analysis of lnapl remediation while overcoming the computational burden in this study the model was limited in 2d and homogeneous matrix to focus on the demonstration of the proposed research framework as a future direction however it is expected that one can easily extend the proposed methodology to 3d heterogeneous aquifers with a consideration of various input factors when extending the framework the following elements should be accounted for results of the surrogate pce model are site specific and are subject to the performance of the chosen numerical simulator in addition it is important to specify proper minimum and maximum limits and probability distribution of input factors based on target scenarios such as target contaminants or hydrogeologic characteristics of the area open research the numerical simulation data used for evaluation of lnapls remediation efficiencies for the base case case i in the study are available in zenodo via https doi org 10 5281 zenodo 5374335 kim and han 2021 with restricted access conditions credit authorship contribution statement taehoon kim conceptualization data curation resources software formal analysis validation visualization methodology writing original draft weon shik han conceptualization supervision resources writing review editing funding acquisition jize piao methodology writing review editing peter k kang writing review editing jehyun shin resources declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this research was supported by the subsurface environmental management sem project through the korea environmental industry and technology institute keiti funded by the ministry of environment grant number 2018002440003 and by energy mineral resources development association of korea emrd grant funded by the korea government motie data science based oil gas exploration consortium peter k kang acknowledges the support by the national science foundation under grant no ear 2046015 in addition the authors would like to thank the useful discussion with dowan koo from yonsei university republic of korea 
167,a multi phase and multi component numerical simulator assessed the removal efficiencies of light non aqueous phase liquid lnapl consisting of benzene toluene ethylbenzene and xylene p scenarios of the lnapl spilling natural distribution and remediation stages were designed in the full physics numerical modeling for lnapls remediation a multi phase extraction mpe and a steam injection technique were employed the removal efficiencies of lnapls were computed by systematically varying 6 factors that determine the configuration of the remediation wells then surrogate polynomial chaos expansion pce models mathematically predicting the removal efficiencies were developed through 600 training datasets representing 4 scenario cases different permeability and the location of si well were considered in the scenario cases the pce models were utilized for sobol global sensitivity analysis and stochastic monte carlo prediction as a result the depth of the mpe well was identified as the most significant factor in determining the removal efficiency of the lnapls the removal efficiency was maximized when the mpe well was positioned 1 5 m below the groundwater table additionally the contributions of influencing factors were significantly changed by the field permeability this study proposed a general framework that efficiently predicts lnapls remediation efficiency and identifies key influencing factors by combining advanced numerical modeling pce based surrogate modeling and sensitivity analyses keywords lnapls remediation removal efficiency polynomial chaos expansion sobol sensitivity analysis multi phase modeling monte carlo prediction 1 introduction light non aqueous phase liquids lnapls are typical contaminants that originate from various anthropogenic sources such as oil storage tanks gas stations transportation and military camps essaid et al 2015 huntley and beckett 2002 oostrom et al 2006 lnapls lighter than water float above the groundwater table and extensively contaminate both soil and groundwater zones when they spill into the subsurface lnapls contain deleterious components including benzene toluene ethylbenzene and xylene p btex these components can be volatilized into the unsaturated zone and concurrently dissolved into the groundwater while lnapls drifting above the groundwater table therefore the thermodynamic properties of lnapls that elucidate comprehensive phase transfer i e volatilization condensation and dissolution should properly be assessed however the assessment is challenging due to discrepancies in thermodynamical behaviors of multiple components btex within lnapls adenekan et al 1993 additionally migration of lnapls reflects multi phase transport within porous media representing the coexistence of lnapls air and water in this complex transport system both relative permeability and capillary pressure must be evaluated brennen and brennen 2005 oliveira and demond 2003 the complex dynamics of lnapls transport in the subsurface can be elucidated by conducting numerical simulations over the past two decades many studies have attempted to predict the migration of spilled lnapls via numerical simulations gupta et al 2019 kim and corapcioglu 2003 qi et al 2020 schroth et al 1998 wipfler et al 2004 although such studies successfully investigated the multi phase behavior of lnapls their approach limited lnapls as a single component while lnapls are generally observed as a mixture of several components essaid et al 2015 recently yang et al 2017 simulated multi component lnapls transport to evaluate the removal ratio of btex through soil vapor extraction the multi phase multi component lnapls simulation could be further enhanced by accounting for hysteresis effects and 3 d heterogeneity in permeability sookhak lari et al 2018 the results aided in simulating lnapl transport realistically and improved the assessment of lnapls removal efficiency however those full physics numerical modelings and subsequent analyses are not always viable such complex numerical analyses require enormous cpu time and memory spaces suited to supercomputing systems which may not be available to all users data driven surrogate modeling techniques have been suggested as alternative options to compensate for the difficulties caused by complexity in numerical models and to manage consecutive post analyses with heavy iterative computations asher et al 2015 razavi et al 2012 the surrogate modeling technique generates an empirical formula from several pairs of given input and output datasets predicted from complex numerical models the application of the formula is limited to the designed model domain or site specific represented by the chosen numerical simulator furthermore complex numerical models which is a basis of surrogate models may not provide a perfect description of physics as well as subsurface heterogeneity despite limitations addressed above the surrogate modeling technique has been in the limelight as decision support in the field of hydrogeology due to its ability to represent the dominant features in the subsurface with cheaper computational costs asher et al 2015 since the verified surrogate models allow for the prediction of the outputs without requiring heavy numerical simulations the demanding time for the computation is significantly reduced a key advantage from the reduction in the computational time is the availability of post analyses using stochastic methods in recent years the stochastic post analyses based on the surrogate model have been used to analyze seawater intrusion rajabi et al 2015 geologic co2 sequestration guyant et al 2016 jia et al 2016 piao et al 2020 and shallow groundwater contamination ciriello and de barros 2020 xing et al 2019 zhao et al 2020 as seen although the applications for the surrogate modeling techniques have expanded limited surrogate modeling and post analyses have been attempted in the field of lnapl transport and its remediation in this study a state of the art technique of surrogate modeling the polynomial chaos expansion pce was employed to link between the full physics numerical lnapl model and stochastic post analyses after developing the surrogate pce model sobol global sensitivity analysis was conducted to rank important factors that affect multi phase and multi component lnapl remediation in addition monte carlo analysis was conducted to comprehensively investigate the effects of each factor on remediation efficiency eventually this study proposes an efficient framework that quantitatively investigates the influencing factors and can be easily extended for optimizing various lnapls remediation scenarios 2 conceptual model a 2d full physics numerical model was developed to investigate the dynamics of lnapls in unsaturated and saturated zones as shown in fig 1 a it was assumed that the lnapls a mixture of btex were spilled from a designated point source the spilled lnapl plume migrated downward through the unsaturated zone until reaching the groundwater table due to buoyancy force the lnapls floated above the groundwater table in the capillary fringe zone where lnapls air and groundwater coexist lnapl dynamics were governed by gravity and capillary pressure bear et al 1996 mayer 2005 fig 1b here lnapls contemporaneously volatilized to the unsaturated zone as highly volatile components hence btex components are also referred to as volatile organic compounds vocs fig 1c kim and corapcioglu 2003 russell 1995 the amounts and rates of volatilized btex components are determined based on the partitioning coefficient and saturated vapor pressures mendoza and frind 1990 in addition at the bottom of the lnapl plume where the groundwater is contacted some of the lnapls dissolve into groundwater fig 1d the amounts of partially dissolved lnapls are determined by the solubilities of each btex component such addressed interactions including chemical dissolution and thermophysical phase transfer affect the fate of lnapl migration in the subsurface kim and corapcioglu 2003 in this study three different fluids were accounted for including gas phase air water vapor and vocs lnapl phase and aqueous phase residually trapped water in soil and water dissolved in lnapls and air because air is a dominant component in the unsaturated zone the gas phase in the unsaturated zone is referred to the air phase in addition the aqueous phase in the saturated zone is referred to the water phase 2 1 model domain initial boundary conditions and sources a base case lnapls spill and remediation scenario which can be realized with four modeling stages was considered the 1st stage is for the development of a sloping groundwater flow the 2nd stage is the spill of lnapls 3rd stage is the migration of lnapls under the ambient condition and the 4th stage is the active remediation of the lnapls 2 1 1 groundwater flow 1st stage model the model domain shown in fig 2 a represents the dimensions of 100 m by 50 m consisting of horizontally discretized 66 grid blocks per each of 50 layers δz 1 m from the surface to the bedrock porosity and permeability kx were uniformly assigned to 0 3 and 4 0 10 13 m2 to the model domain respectively the anisotropy kz kx was assigned to 0 25 assuming horizontal arrangement of soil deposits in the sedimentary layers nielsen et al 1986 scholes et al 2007 the groundwater flow model was developed in two stages in the 1st stage model a sloping groundwater table was developed which was the initial condition of the 2nd stage model simulating lnapl migration in the 1st stage model the constant atmospheric temperature and pressure 25 c and 1 01 105 pa conditions were assigned to the top boundary fig 2a immediately below the top boundary grid blocks an infiltration rate of 4 64 10 6 kg s was assigned to represent infiltrating precipitation the assigned infiltration rate was estimated from the annual average precipitation in the republic of korea both lateral boundaries were assigned to the constant pressure boundary with pressure gradients left boundary 1 01 105 pa top to 3 72 105 pa bottom and right boundary 1 01 105 pa top to 3 52 105 pa bottom the pressure difference between left and right boundaries is attributed to the groundwater flow the bottom boundary was set to no flow owing to the presence of low permeability base rock initially the flat groundwater table was at a depth of 21 m by setting water saturation of 0 2 and 0 99 at unsaturated and saturated zones respectively after simulating the 1st stage model the equilibrated sloping groundwater table was achieved and the resulting average groundwater velocity was 0 022 m day 2 1 2 lnapls spilling and remediation 2nd 3rd and 4th stage models in the 2nd stage model lnapl spilling stage the lnapls composed of btex spilled for 5 years from the point source located 37 5 m away from the left boundary and 4 5 m below the ground surface fig 2b the spilling constantly occurred at a rate of 1 65 10 5 kg s per each btex component the primary direction of the lnapl plume migration is the gravitational direction at this stage while its migration is governed by the relative saturation of air and lnapls in the unsaturated zone at the end of the 2nd stage the lnapl spilling ceased during the 3rd stage natural redistribution stage that lasted for 2 years the spilled lnapls were naturally redistributed in the capillary fringe primarily spreading above the sloping groundwater table fig 2c finally in the 4th stage remediation stage that lasted for 0 5 years both the steam injection si and the multi phase extraction mpe wells were installed to remediate the lnapls fig 2d the si well was installed below the lnapl source at a depth between 20 m and 22 5 m where 100 c steam was injected at a rate of 6 10 4 kg s to aid phase transfer from high viscosity liquid lnapls to low viscosity vapor lnapls in addition the mpe well was installed horizontally 5 m away from the lnapl source at a depth between 16 m and 20 m to extract both vapor and liquid phases of lnapls at the mpe well a constant bottom hole pressure bhp of 8 104 pa was assigned which was approximately equivalent to the rate of 5 7 10 3 kg s finally four modeling stages representing groundwater flow lnapl spilling natural redistribution and remediation were simulated using tough2 tmvoc pruess and battistelli 2002 2 2 parameters for 3 phase flow systems and thermodynamic properties of lnapls two of the major aspects governing the dynamic behaviors of lnapls are capillary pressure and relative permeability these are parameters characterizing multi phase fluid systems in porous media such multi phase effects spatiotemporally vary depending on the saturation of the individual fluid phase and the characteristics of porous media lenhard and parker 1987 miller et al 1998 the capillary pressure caused by the relative difference in wettability and interfacial tension between two immiscible fluids is one of the major factors governing lnapl migration in unsaturated zones anderson 1987 hassanizadeh and gray 1993 lnapls are the intermediate wetting phases among the three phases air water and lnapls therefore when the lnapls come into contact with air the lnapls and air are treated as wetting and non wetting fluids respectively whereas at the lnapls contacting with water the lnapls and water are treated as non wetting and wetting fluids respectively in the present study the 3 phase capillary pressures were calculated from parker s 3 phase function which was derived to solve multi phase organic contaminant transport in the unsaturated zone kaluarachchi and parker 1990 wu and forsyth 2001 the capillary pressures were calculated at the interfaces between the air lnapls p c al lnapls water p c lw and the water air p c wa the mathematical function for 3 phase capillary pressures are presented in table 1 where ρ w is the water density dependent on subsurface pressure and temperature pruess and battistelli 2002 and g is the gravitational acceleration 9 81 m s2 the α al α lw and n are empirically determined parameters and sm is the residual wetting phase water saturation finally sw and sl represent the saturation of water and lnapls respectively while groundwater lnapls and air concurrently migrate through porous media their movements are restricted according to variability in relative permeability consequently the average velocities of the three competing fluids decrease alizadeh and piri 2014 bradford et al 1997 in this study the 3 phase relative permeabilities kr were calculated from the stone i model which is commonly used for water wet porous media baker 1988 blunt 2000 stone 1970 in the stone i model the relative permeabilities of air k r a water k r w and lnapls k r l are functions of the relative saturation of each fluid phase as shown in table 1 here sa sw and sl are the saturation of air water and lnapls respectively in addition sar swr and slr representing residual saturations of air water and lnapls respectively were obtained from erning et al 2009 juanes 2003 and pruess 2003 respectively finally n is an adjustable fitting parameter in addition to both capillary pressure and relative permeability accurate prediction of lnapl migration in the subsurface requires the computation of thermophysical properties in individual btex component and their mixtures hence phase transfers volatilization condensation and dissolution of btex which are dependent on vapor pressure water solubility and henry s constant were accounted for at the designated temperature pressure and chemical compositions table 2 corresponding computations are referred to pruess and battistelli 2002 and reid et al 1987 3 workflow the workflow followed the 4 steps 1 sampling 2 development of full physics numerical model based surrogate model 3 global sensitivity analysis and 4 monte carlo prediction 3 1 sampling and development of surrogate polynomial chaos expansion model latin hypercube lh sampling is a common space filling method that enables uniform sampling with reasonable computational costs viana 2013 in multi dimensions the sampling method searches the sampling points p within orthogonal grids where each non overlapped set of rows and columns satisfied the space filling property the sampled combination of n number of input factors x x 1 x 2 xn and corresponding response y computed from the full physics numerical lnapl model composes the training dataset used to build a surrogate model from x and y the surrogate polynomial chaos expansion pce model can be established following blatman and sudret 2008 1 y m x α n n λ α ψ α x where λα is the deterministic chaos expansion coefficient and ψα x is the multivariate orthonormal basis polynomial assuming the independence between the input factors in x the ψα x is established by the multiplication of univariate polynomials of each input factor x i 2 ψ α x i 1 n ψ α i x i where each ψ α i is the univariate polynomial of degree α i with respect to the i th input factor the type of the ψ α i is selected based on the type of probability density function pdf of x i considering the uniform and gaussian pdfs the ψ α i x i are selected to normalized legendre and hermitian polynomials respectively xiu 2010 because the input factors employed in this study were all assumed to be uniform distribution the type of the univariate polynomial ψ α i x i was regarded as the legendre polynomial for the practical computation of the targeted pce from the legendre polynomials of x which was linearly transformed into 1 1 n m x would be truncated up to the total degree of the ψα x ω 3 m x m t x α a n ω λ α ψ α x x u 1 1 n where the truncated pce m t x is then remained as the finite terms corresponding to subsets of a n ω which refers to the truncation set satisfying α i 1 n α i ω to compute the designated number of legendre polynomials ψα x and the corresponding chaos expansion coefficients λα in a n ω the cardinality of a n ω should be determined to be p n ω n ω here n and ω represent the number of input factors and the total degree of the ψα x respectively after determining multivariate basis polynomials ψα x λα can be calculated through either intrusive or non intrusive schemes song et al 2019 in the present study least squares minimization lsm in the non intrusive scheme is utilized to calculate λα here to achieve the λα accurately the number of sampling points p from the lh sampling should be greater than p blatman and sudret 2008 2010b 3 2 global sensitivity analysis the sobol global sensitivity analysis is a type of variance based sensitivity analysis which is applicable when the input variables are independent the sobol scheme practically results in quantified sensitivity indicators sobol indices from the monte carlo simulation blatman and sudret 2010a and sudret 2008 provided a method for the straightforward computation of sobol indices from the pce this approach significantly reduces the computational cost to calculate the sobol indices requiring 2 to 3 orders of magnitude fewer simulations than the monte carlo simulation for the pce of the multivariate legendre polynomials eq 3 can be expressed as 4 y f x m t x j 0 p 1 λ j ψ j x x u 1 1 n the mean and variance of the pce were computed based on the orthogonality of the basis polynomial as follows 5 y e f x λ o 6 v p c e v a r j 0 p 1 λ j ψ j x j 0 p 1 λ j 2 e ψ j 2 x eq 4 can be expanded by sobol decomposition after derivation sudret 2008 as shown below 7 f x m t x λ 0 i 1 n α h i λ α ψ α x i 1 i 1 i 2 n α h i 1 i 2 λ α ψ α x i 1 x i 2 1 i 1 i s n α h i 1 i 2 i s λ α ψ α x i 1 x i s α h i 1 i 2 n λ α ψ α x 1 x n where each summand α h i 1 i s λ α ψ α x i 1 x i s are orthogonal polynomials produced by x i 1 x i s and h i 1 i s is the set in α which indicates that only the x i 1 x i s are involved in the summand as a result of squaring both the left and right sides of eq 7 the sobol indices combined with the pce s i i 1 i s for any h i 1 i s can be obtained as the proportion of the partial variance to the total variance 8 s i i 1 i s α h i 1 i s λ α 2 e ψ α 2 v p c e typically two types of sobol indices are used the sobol 1st order indices s i i 1 1 s t α h i 1 λ α 2 e ψ α 2 v p c e capable of showing the only influence of each independent variable on output and the sobol total order indices s i i 1 t i 1 i 1 i s s i i 1 i s considering additional effects by the interactions of the independent variables 4 results 4 1 full physics numerical lnapls transport model base case 4 1 1 spatiotemporal distribution of air lnapls and water simulation results for phase saturation sl sa and sw at the end of 2nd t 5 years 3rd t 7 years and 4th t 7 5 years stages are shown in fig 3 the sources for spilling lnapls are marked as a red circle and the white line represents the groundwater table during the 2nd stage lnapl spilling stage lnapls migrated downward through the unsaturated zone until they reached the capillary fringe above the groundwater table fig 3a the lnapl plume in the unsaturated zone expanded symmetrically owing to the homogeneous permeability and dispersion the average sl in the unsaturated zone was 0 11 the corresponding sa and sw were concurrently changed to 0 65 and 0 24 respectively fig 3b and 3c since sa and sw were 0 75 and 0 25 before lnapls spill changes in sa during the 2nd stage indicated that the spilled lnapls primarily displaced the low viscosity air in the pore space immediately above the groundwater table the lnapls accumulated in the capillary fringe the capillary fringe served as a primary flow path that allowed the lnapls to spread horizontally mayer 2005 in the model before the lnapls spill the thickness of the capillary fringe was approximately 3 m with associated sa and sw of 0 45 and 0 55 respectively fig 3b and 3c once the lnapl plume reached the capillary fringe it primarily displaced low viscosity air the sl increased from 0 to 0 45 fig 3a while sa decreased from 0 45 to almost 0 0 fig 3b additionally due to the difference in both density and wettability between lnapls and groundwater the downward movement of lnapls below the groundwater table was hindered instead the lnapls migrated approximately 9 m along the sloping groundwater table finally owing to the counter buoyancy force acting on the lnapl plume the groundwater table was suppressed approximately 1 5 m and consequently the capillary fringe was thickened during the 3rd stage natural redistribution stage sl in the unsaturated zone uniformly dropped to 0 052 close to the assigned slr 0 05 table 1 the corresponding k r l calculated from the stone i model was almost 0 indicating that most lnapls remained as the residually trapped phase in the unsaturated zone fig 3d above the groundwater table the mobile lnapls continuously accumulated while the lnapl plume spread horizontally above the sloping groundwater table at the end of the 3rd stage the maximum sl of the floating lnapl plume increased to 0 462 in addition sa in the unsaturated zone nearly returned to the initial saturation because the air re filled pore spaces where movable lnapls were left fig 3e finally the sl sa and sw are depicted at the end of the 4th stage remediation stage where the remediation scheme was implemented fig 3g 3h and 3i as mentioned in section 2 1 2 two remediation wells including steam injection si and multi phase extraction mpe wells were operated the screen depths of each well are highlighted in red and blue boxes respectively at the end of the remediation stage the sl of the floating lnapl plume rapidly decreased to 0 22 mainly due to extraction of the movable lnapls and phase transfer from liquid lnapls to volatilized lnapls by high temperature steam injection fig 3g the pressure gradient induced by the mpe well aided to extract low viscosity volatilized lnapls additionally the pressure gradient induced by the two remediation wells distorted the capillary fringe for instance adjacent to the si well a hole where sa was increased to 0 69 due to both volatilized lnapls and injected steam was developed fig 3h in the unsaturated zone the simultaneous operation of the two remediation wells was able to remove trapped lnapls sl was decreased to below 0 05 fig 3g at the bottom of the mpe well both sl and sw increased indicating that both lnapls and groundwater were effectively extracted by the mpe well fig 3g and 3i 4 1 2 btex in air lnapls and water phases the distributions of btex benzene toluene ethylbenzene and xylene p at the end of the 3rd stage were delineated to elucidate the different behaviors of the spilled btex in the subsurface based on their thermophysical properties fig 4 xl xa and xw represent the mole fraction of btex existing in lnapls air and water respectively in the lnapl phase the btex components were evenly distributed although their average xl values were different due to the difference in molar mass fig 4a the xl of btex were 0 3 0 25 0 22 and 0 22 respectively the mole fractions of btex in the air phase are delineated in fig 4b which reveals a clear difference depending on the volatilization of btex the degree of btex volatilization was related to the saturated vapor pressure table 2 benzene characterizing the largest saturated vapor pressure 12 523 50 pa showed the maximum xa 0 0372 which was 8 5 times greater than that of xylene p 0 00436 having the lowest saturated vapor pressure 1169 47 pa additionally benzene which had the largest solubility 0 411 g l among the btex showed the maximum xw of 1 24 10 4 fig 4c in summary among btex benzene was the dominant contaminant due to its high saturated vapor pressure and water solubility 4 1 3 removal efficiency of lnapls the btex distribution at the end of the 4th stage remediation stage after 7 5 years is shown in fig 5 at the end of the remediation stage the xl of benzene was the lowest 0 301 among the btex components indicating that the largest amount of removed components was benzene fig 5a the largest removal of benzene was due to effective phase transfer of benzene from liquid lnapls to volatilized lnapls responding to steam injection by the si well the temperature of the injected steam was approximately 100 c and the boiling temperature of benzene was 80 c whereas those for other components were over 100 c table 2 the results of such active volatilization in benzene were shown as the largest xa among the btex components fig 5b the maximum xa of benzene after the remediation was 0 31 while the average xa of other components remained at low quantities t 0 11 e 0 08 and x 0 08 fig 5b the increased xa of benzene accelerated the benzene transport to the mpe well by reducing the viscosity and increasing the relative permeability in contrast to xa of benzene xw of benzene was not significantly reduced by the remediation fig 5c this was due to the relatively small viscosity and small relative permeability of water at the unsaturated zone water was residually trapped in the pores and btex components dissolved in such residual water could not be extracted by the mpe well to evaluate the removal efficiency re a mass of component z in phase p p z m before and after remediation at the end of the 3rd and 4th stage was calculated the re p z r e 1 p z m 4 t h p z m 3 n d was then defined and the res of individual components b t e and x in lnapls air and water phases were calculated bar graph in fig 6 additionally the res t z r e 1 t z m 4 t h t z m 3 n d of individual components b t e and x in the total lnapls air water phase were delineated red circle in fig 6 the res of btex components between the lnapl l b r e l t r e l e r e and l x r e and total t b r e t t r e t e r e and t x r e phases were nearly the same with values of approximately 0 23 0 13 0 06 and 0 06 respectively fig 6a this indicated that the btex primarily existed as the lnapl phase although phase transfer to air and water had occurred the removed masses of btex in the lnapl phase are proportional to their volatilities table 2 here benzene showed the largest volatility as more benzene changed to the air phase the mobility of benzene in the air phase was enhanced then the mpe well accelerated the removal rate of benzene in the air phase consequently the a b r e 0 4 exceeded t b r e 0 23 nevertheless a t r e 0 08 a e r e 0 84 and a x r e 0 97 showed negative values indicating that lower volatile components were not efficiently removed by the mpe well after they had been volatilized fig 6b the removal efficiencies of benzene toluene ethylbenzene and xylene p in the water phase w b r e w t r e w e r e and w x r e showed the smallest deviation from 0 fig 6c benzene and toluene with high solubility specifically showed positive w b r e 0 14 and w t r e 0 01 respectively but ethylbenzene and xylene p with low solubility showed negative w e r e 0 14 and w x r e 0 17 respectively in summary the components showing effective phase change represented high removal efficiency 4 2 surrogate polynomial chaos expansion lnapls models 4 2 1 4 cases permeability and the location of steam injection wells including base case case i three additional cases were designed to evaluate the effect of permeability k and the location of the steam injection si well table 3 in case ii kx 4 0 10 13 m2 and kz 1 0 10 13 m2 was equal to case i but the location of the si well was shifted to the right of the mpe well fig 7 a and 7b at both case i and ii represented by high k lnapl plumes spread widely above the groundwater table while residually trapped lnapls remained in the unsaturated zone the res of total phase btex t b r e t t r e t e r e and t x r e for case i were similar to those for case ii indicating that the relative location of si to the mpe well did not influence a degree of remediation efficiency much fig 7e and 7f the t b r e 0 25 and 0 20 t t r e 0 13 and 0 10 t e r e 0 04 and 0 04 and t x r e 0 04 and 0 04 were predicted at case i and ii respectively for case iii and iv both kx and kz decreased 10 times to 4 0 10 14 m2 and 1 0 10 14 while relative locations of the si well were equal to one for case i and ii respectively fig 7c and 7d a decrease in k caused local accumulation of the lnapl plume with preventing horizontal expansion the small and concentrated lnapl plume caused less contact with the surrounding air and thus the amount of volatilized lnapls decreased additionally decreased mobility of lnapls due to low k let more lnapls remain in the unsaturated zone and impeded lnapl migration to the mpe well causing a decrease in t b r e t t r e case iii and iv in fig 7g and 7h 4 2 2 input factors and responses for assessing removal efficiency of lnapls for each of the 4 cases the 6 input factors governing the effectiveness of the remediation wells were selected table 3 the screen depths of both the mpe and si wells x 1 and x 2 the lateral distance between the mpe and si wells x 3 the bhp of the mpe well x 4 and the injection rate x 5 and the temperature of the si well x 6 the 6 input factors were acknowledged as influencing factors governing the removal efficiency of lnapls mccray and falta 1997 robin and gillham 1987 rogers and ong 2000 these input factors were assumed to have a uniform distribution with minimum and maximum limits table 3 within these ranges the sampling points representing the different combinations of input factors were selected through lh sampling for example the x 1 and x 2 were selected within the vertical sky blue and red scale lines represented in fig 7 respectively finally a single sampling point determined a single configuration of wells in each case a total of 150 points were sampled 600 points in 4 cases based on lh sampling points a total of 600 full physics numerical lnapl transport models were conducted to obtain responses the responses were the removal efficiencies of the total sum of btex t b t e x r e 1 t b t e x m 4 t h t b t e x m 3 n d calculated at the end of 4th stage 7 5 years then both input factors and responses served as the training dataset in developing the surrogate polynomial chaos expansion pce models of 4 cases 4 2 3 development of surrogate pce model validation and test pce models predicting t b t e x r e were developed using the following procedure with a given training dataset e g a combination of the 6 input factors and the responses multi variate basis functions ψα for the pce models were developed preliminary pce models were then developed by evaluating the determination of the coefficients λα which were calculated from the least square minimization lsm method however the preliminary pce models based on lsm may preserve unnecessary complexities and overfitting problems blatman and sudret 2010b to overcome these problems the preliminary pce models were improved through the validation process both an adaptive sparse algorithm with the least angle regression and a leave one out loo cross validation algorithm were used for the validation the adaptive sparse algorithm is a step wise procedure that reduces the complexity of the pce model by eliminating the insignificant ψα among the total p number of ψα blatman and sudret 2011 the total p p n ω n ω number of ψα was 210 n 6 and ω 4 210 n 6 and ω 4 210 n 6 and ω 4 and 84 n 6 and ω 3 in case i ii iii and iv respectively after applying the advanced adaptive sparse method with the least angle regression the total p number of ψα in preliminary pce models decreased to 29 30 53 and 14 in case i ii iii and iv once the pce models were streamlined the deterministic chaos expansion coefficients λα were subsequently modified to solve the overfitting problem by minimizing the loo error εloo blatman 2009 9 ε loo i 1 n m x i m p c i x i 2 i 1 n m x i μ y 2 where n is the number of the training dataset n 150 m x i is the i th response calculated from a sparse pce model developed by n training data m p c i x i is the i th response of a sparse pce model but the model derived from n 1 training data by excluding the i th training data and μ y is the mean of the n responses finally the optimal εloo values of the sparse pce models were calculated to be 0 036 0 044 0 029 and 0 066 in case i ii iii and iv respectively once the optimal εloo was calculated the corresponding λα values were selected to determine validated pce models in fig 8 the t b t e x r e calculated from both the validated pce models and the full physics numerical lnapls transport models are plotted for cases i ii iii and iv here 150 yellow circles and 50 green triangles indicate the training and test data respectively in each case the validated pce models developed using 150 training data were tested with 50 test data which were randomly chosen by the lh sampling using 50 test data the determination coefficient r 2 and normalized root mean squared error nrmse were calculated to investigate the predictability of the validated pce models the predictability of the validated pce models was the highest in case iv fig 8d r t e s t 2 and nrmse test were 0 930 and 0 067 respectively even for case ii showing the lowest predictability fig 8b the predictability was still acceptable r t e s t 2 0 889 and nrmse test 0 11 ensuring that the validated pce models were capable of substituting the responses of the full physics numerical lnapls transport models interestingly both training and test datasets were distributed differently depending on the magnitude of permeability k indicating that even small differences in k significantly influenced t b t e x r e at high k values cases i and ii both training and test datasets were widely distributed between 0 and 0 55 fig 8a and b and the widely distributed t b t e x r e indicates that the t b t e x r e was sensitive to remediation conditions represented by 6 input factors dependent on suitable combinations of 6 input factors the t b t e x r e could be maximized in contrast at low k cases iii and iv most of the training and test data were leaned to the 0 small t b t e x r e denoting that t b t e x r e cannot be improved significantly by the choice of the 6 input factors in fig 7 the distribution of the lnapl plume at both high k and low k is shown at high k the lnapl plume spread widely over the groundwater table and even migrated far from both mpe and si well fig 7a and b nevertheless t b t e x r e could be maximized dependent on the choice of 6 input factors fig 8a and b this is because the lnapls can be more easily mobilized by remediation wells when k is high at low k however although the lnapl plume was located close to both the mpe and si well fig 7c and d the overall t b t e x r e was still small because the high pumping rate of the mpe well did not improve the mobility of the lnapls fig 8c and d such a difference in t b t e x r e implies that the influence of k on the remediation of the lnapl plume presumably prevails over any 6 input factors therefore at contamination sites with extremely small k e g clay dominant or fractured rock sites changes in 6 input factors may not be able to improve t b t e x r e significantly 4 3 analysis using surrogate pce models 4 3 1 sobol global sensitivity analysis using the validated pce models the influence of the input factors x 1 x 2 x 3 x 4 x 5 and x 6 on the t b t e x r e were assessed through sobol global sensitivity analysis case i in fig 9 a case ii in fig 9b case iii in fig 9c and case iv in fig 9d for each case the sobol total t and first 1st order indices were calculated with 150 realizations of the pce model whereas they would be yielded by more than 104 realizations with a common monte carlo simulation sudret 2008 sobol t indices blue bar which include the effect of intercorrelation between the input factors are always greater than sobol 1st indices purple bar similar to fig 8 the sobol sensitivity analyses revealed differences primarily dependent on k at high k sobol t and 1st indices for x 1 were dominant indicating that the t b t e x r e was largely influenced by the x 1 fig 9a and b the sobol t and 1st indices were 0 84 and 0 80 for case i and 0 86 and 0 81 for case ii respectively the thickness of the lnapl plume is thin and widely spread over the groundwater table at high k thus t b t e x r e was highly influenced by the choice of screen depth in the mpe well the 2nd influential factor was x 4 the sobol t and 1st indices were 0 14 and 0 11 for case i and 0 14 and 0 10 for case ii respectively even though the values for x 4 are significantly smaller than those for x 1 their influences are still superior to the other input factors this indicates that sufficient pressure gradient generated by the mpe well effectively enhances the t b t e x r e at high k finally the sobol indices of the other input factors x 2 x 3 x 5 x 6 related to the si well were nearly zero at low k the most sensitive factor was x 1 identical to high k although the influence of x 1 was decreased compared to cases i and ii the sobol t and 1st indices of the x 1 were 0 54 and 0 40 for case iii and 0 63 and 0 44 for case iv respectively fig 9c and d the difference between the sobol t and 1st indices increased indicating that the interaction between x 1 and the other factors e g steam injection rate became more important at low k the 2nd influential factor was x 3 the sobol t and 1st indices of the x 3 were 0 38 and 0 25 for case iii and 0 45 and 0 25 for case iv respectively at low k the spreading of the lnapl plume was constrained and consequently the horizontal location and vertical depth of the mpe well became influential additionally the influence of other input factors x 2 and x 5 related to the si wells increased these results suggest that steam injection would be more effective for lnapl remediation at the low k site finally the change in x 6 within the designated range i e 100 150 c did not affect the t b t e x r e indicating that steam temperature above 100 c is not cost effective for remediating the btex 4 3 2 predicting lnapls remediation efficiency the influences of 6 input factors on t b t e x r e was comprehensively quantified using the validated pce models a randomly generated 6 input factors determined a single prediction of t b t e x r e through the pce model and 105 monte carlo implementations revealed density plots of 6 input factors relating to empirical probability density functions of t b t e x r e fig 10 the 6 density plots showing the correlation between 6 input factors along the x axis and t b t e x r e on the y axis are depicted for the 4 cases the solid lines represent the linear fitting curves additionally histograms and box whisker diagrams were plotted to show the statistical distribution of t b t e x r e similar to sobol global sensitivity the pce based monte carlo prediction also differed significantly depending on the magnitude of k cases i and ii vs cases iii and iv at high k cases i and ii the slopes of the fitting curves for the screen depth of the mpe well x 1 were the steepest confirming that x 1 had a strong effect on t b t e x r e fig 10a and b interestingly the bimodal distribution for x 1 which is distinguished by the depth of the groundwater table was shown when the mpe well was installed above the groundwater table 21 5 m the average t b t e x r e was approximately 0 15 however the t b t e x r e dramatically increased to over 0 4 when x 1 was slightly below the groundwater table the t b t e x r e was maximized approximately 1 5 m below the groundwater table this is attributed to the characteristic of lnapls floating above the groundwater table when the mpe well was installed in the unsaturated zone gaseous lnapls volatilized by steam injection were only removed however when the mpe well was located below the groundwater table both liquid lnapls and lnapls dissolved in groundwater were removed effectively qi et al 2020 in addition the hydraulic head gradient developed by remediation wells accelerated the migration of the lnapls to the mpe well simon et al 1999 different from x 1 the screen depth of the si well x 2 did not influence t b t e x r e significantly the t b t e x r e slightly improved as x 2 was shallow the distance between the mpe well and si well x 3 showed opposite slopes in cases i and ii because of the opposite direction of the lnapl source fig 7a and b the t b t e x r e increased as x 3 was close to the lnapl source the 2nd largest input factor was the bhp of the mpe well x 4 as x 4 approached the low limit 7 104 pa the average t b t e x r e increased the effect of both the steam injection rate x 5 and steam temperature x 6 was insignificant for lnapl removal finally the histogram of t b t e x r e featured two peaks high t b t e x r e and low t b t e x r e primarily split by the depth of the groundwater table at low k the influence of x 1 was weaker than that of high k fig 10c and d but its influence was still the largest among the 6 input factors similar to high k t b t e x r e increased when x 1 was located below the groundwater table the x 3 was the 2nd important input factor as x 3 was far from the lnapls source the t b t e x r e approached zero this indicates that the distance between the lnapl source and remediation wells is critical at the low k field where lnapls migration is hindered while the influence of x 3 became significant the effect of x 4 decreased at low k the effect of both x 5 and x 6 was small similar to that for high k overall at low k the t b t e x r e were significantly smaller than high k which is also reflected in the histograms and the box whisker plots this implies that the optimum choice of 6 input factors is more important for the high k 5 implications and limitations of pce models in this study four surrogate pce models accounting for lnapls remediation efficiency were developed for different conceptual cases case i ii iii and iv for each case the number of training datasets to develop the pce models was 150 however the application of advanced adaptive sparse pce algorithm could substantially reduce the required number of the dataset by eliminating the non influential interaction terms blatman and sudret 2011 such computational advantages by reducing the number of training datasets would allow for developing surrogate pce models representing more complex 3d heterogeneity numerical models with realistic geostructures and a large number of input factors such as hydrogeologic properties e g permeability porosity water saturation soil properties e g particle size distribution and capillary pressure or geochemical properties e g biodegradation coefficient absorption coefficient additional to computational benefits the surrogate pce model can analytically link to the global sensitivity analysis and then be easily interconnected with other quantitative analyses fajraoui et al 2011 for example the global sensitivity analysis in this study revealed a crucial feature in lnapls remediation that the installation depth of the mpe well dominantly affected lnapls remediation efficiency subsequent monte carlo prediction further revealed the optimum depth of the mpe well that also supported the results obtained by numerical simulations in other previous studies qi et al 2020 despite the aforementioned advantages of surrogate pce models in lnapls remediation researchers should be aware that the surrogate pce model can not be a general or ultimate solution to interpret the lnapls transport behavior in the subsurface firstly the surrogate pce model developed is valid only in the conceptual model domain represented by numerical simulation in the study the numerical model was 2d and homogeneous thus the surrogate pce model should be reevaluated if researchers wish to investigate heterogeneous subsurface systems 3d lnapls transport or different target contaminants secondly the surrogate pce model solely depends on the internal computation results of the full physics numerical lnapls transport model due to this reason the choice of different numerical simulators may produce different pce results accordingly utilizing a more accurate and effective surrogate pce model should be accompanied by improvement of the underlying numerical model for example recent studies considered more realistic physical properties in simulating lnapls migration such as hysteresis effect of multi phase fluids and also verified the lnapls model from the implementation of lnapls tank laboratory experiments pasha et al 2014 sookhak lari et al 2016 6 conclusion full physics numerical simulations that honor multi phase multi component lnapl transport and its remediation are challenging primarily because of their extensive memory and cpu requirements in addition to the complexities involved in geologic characterization and lnapl transport numerical simulations must be repeatedly performed to characterize uncertainties involved in input factors or to search for the optimum lnapl removal efficiency in this study to overcome the challenges addressed in lnapl transport and remediation the surrogate pce modeling technique was implemented in conjunction with the quantitative sobol sensitivity analysis and monte carlo prediction the proposed workflow involves forward full physics numerical lnapls transport modeling that generates a training dataset development of a pce based surrogate model global sensitivity analysis and monte carlo prediction for the full physics numerical modeling the tough2 tmvoc was employed to simulate multi phase and multi component lnapl transport the conceptual model delineated the scenarios of the lnapl spilling natural distribution and remediation stages here lnapls were assumed to be a mixture of btex components allowing mass transfer e g vaporization condensation and dissolution among individual components finally the removal efficiency of lnapls was assessed by implementing mpe and si wells the 4 cases considering different permeabilities and locations of the si well were designed to develop pce based surrogate models within 4 cases 6 factors related to well configuration were varied to assess the removal efficiency of lnapls by combining forward numerical modeling global sensitivity analyses and monte carlo prediction the governing factors improving the efficiency of lnapl remediation were identified first the screen depth of the mpe well was the most important among 6 factors associated with well configuration regardless of field permeability or location of the si well because the lnapl plume floated within the narrow capillary fringe above the groundwater table the maximum remediation efficiency was predicted when the depth of the mpe well was slightly 1 5 m below the groundwater table second the permeability of contaminated sites evidently influenced the remediation conditions at high k both the screen depth and bhp of the mpe well are important but the influence of the bhp of the mpe well was diminished at low k instead the distance between the mpe and si wells became important finally at high k the mpe well itself was enough to remove the lnapls and thus the configuration of the si well was not as important however at low k where the mobility of lnapls was small the si well improved the remediation efficiency by volatilizing liquid lnapls to gaseous lnapls this study successfully demonstrated the capability of the pce based surrogate modeling for the quantitative analysis of lnapl remediation while overcoming the computational burden in this study the model was limited in 2d and homogeneous matrix to focus on the demonstration of the proposed research framework as a future direction however it is expected that one can easily extend the proposed methodology to 3d heterogeneous aquifers with a consideration of various input factors when extending the framework the following elements should be accounted for results of the surrogate pce model are site specific and are subject to the performance of the chosen numerical simulator in addition it is important to specify proper minimum and maximum limits and probability distribution of input factors based on target scenarios such as target contaminants or hydrogeologic characteristics of the area open research the numerical simulation data used for evaluation of lnapls remediation efficiencies for the base case case i in the study are available in zenodo via https doi org 10 5281 zenodo 5374335 kim and han 2021 with restricted access conditions credit authorship contribution statement taehoon kim conceptualization data curation resources software formal analysis validation visualization methodology writing original draft weon shik han conceptualization supervision resources writing review editing funding acquisition jize piao methodology writing review editing peter k kang writing review editing jehyun shin resources declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this research was supported by the subsurface environmental management sem project through the korea environmental industry and technology institute keiti funded by the ministry of environment grant number 2018002440003 and by energy mineral resources development association of korea emrd grant funded by the korea government motie data science based oil gas exploration consortium peter k kang acknowledges the support by the national science foundation under grant no ear 2046015 in addition the authors would like to thank the useful discussion with dowan koo from yonsei university republic of korea 
168,due to the recent advances in computing and data acquisition technologies numerical models solving the 2d shallow water equations swes are now widely used in predicting overland flow and surface water flooding process in natural catchments in catchment scale flood modelling in addition to the calculation of convective terms that are usually achieved through solving local riemann problems to capture transient flow dynamics correct discretisation of source terms is also essential to handle complex domain topography and ensure accurate and stable numerical solutions the external forces induced by gravity and bed friction create the key source terms that drive the change of momentum in the swes and affect the stability and accuracy of the adopted numerical scheme especially for applications involving very shallow water depth e g wet and dry fronts overland flows herein new schemes are introduced to reconstruct the face values of flow variables and a fully implicit algorithm is improved to discretise the stiff friction terms in the context of developing a godunov type finite volume hydrodynamic model for accurate and stable simulation of overland flow and surface water flooding involving very shallow water depth five analytical test cases are considered to validate the resulting models and the numerical results are compared with those produed by two alternative numerical schemes the performance of the models are further demonstrated by reproducing a flood event in the 2500 km2 eden catchment uk keywords finite volume method face value reconstruction shallow water equations overland flows hydrostatic reconstruction source term discretisation data source links and availability the links to the data sources used in storm desmond on the eden catchment uk are given as follows dem the dem of the catchment is obtained by merging the bare earth digital terrain model dtm https digimap edina ac uk with the digital surface model dsm https data gov uk dataset fba12e80 2010519f 20104be2 2010806f 201041be9e26ab96 lidar 2010composite 2010dsm 20102m to include buildings and key infrastructures additionally the river channels are considered by adding river bathymetry to the dem https datahub admiralty co uk portal apps sites marine data portal items q river land use land cover map is provided by the centre for ecology hydrology https www ceh ac uk services land cover map 2015 rainfall gridded radar rainfall data is available from the uk met office http badc nerc ac uk measurement and flood extent river gauge observations https environment data gov uk hydrology landing and the surveyed flood maps can be found in https data gov uk dataset 76292bec 20107d8b 201043e8 20109c98 201002734fd89c81 historic 2010flood 2010map wherein the dtm data is freely available to all users via subscribing to the digimap service the dsm data river gauge observations and surveyed flood maps are open to public users under the uk open government licence https data gov uk the landcover data is available upon request from the ceh data licensing team datalicensing ceh ac uk and the radar rainfall data is available upon request from ceda archive http archive ceda ac uk 1 introduction flooding is the most widespread natural hazard that affects most if not all of the countries and regions across the world hirabayashi et al 2013 it is recognised that climate change and rapid urbanisation will lead to an increase of flood risk in the 21st century and beyond ipcc 2021 it is crucial for governments of different levels across the world to effectively manage flood risk and enhance societal resilience to flooding and climate change to sustain socio economic development numerical modelling has become an indispensable tool in supporting different stages of flood risk management in catchment scale flood modelling hydrological models or simplified hydraulic models have been predominantly used in the past decades govindaraju et al 1988 jakeman and hornberger 1993 however these models do not enforce strict momentum conservation and are not suited for the simulation of transient flood dynamics induced by intense rainfall bout et al 2018 caviedes voullième et al 2020 cea and bladé 2015 xia et al 2017 many researchers recognise the necessity of adopting fully hydrodynamic models that solve the shallow water equations swes using a shock capturing numerical method to better account for the underlying physical processes and more reliably predict the rapidly varying flood waves interacting with complex terrain topographies and engineering structures e g cao et al 2010 to use this type of sophisticated hydrodynamic models to support catchment scale surface water flood modelling a key numerical challenge is to ensure a godunov type shock capturing scheme to maintain the so called c property bermudez and vazquez 1994 or more general well balanced property greenberg and leroux 1996 and this involves the implementation of compatible numerical methods to discretise the flux and source terms including bed slope and friction terms to accurately and stably predict overland flows with very shallow depth as part of the catchment scale rainfall flooding process xia et al 2017 numerous well balanced schemes have been developed and reported in the literature in the past three decades george 2008 greenberg and leroux 1996 kesserwani 2013 leveque 2002 murillo and garcía navarro 2010 zhou et al 2001 by directly considering slope source terms in the flux calculation the augmented roe riemann solver introduced by george 2008 can be implemented to effectively ensure well balanced property for simulations over variable topography in a roe riemann solver the source terms may be also projected onto the jacobian matrix to develop a path conservative scheme to achieve the required well balanced property e g castro et al 2006 murillo and garcía navarro 2010 parés and castro 2004 on the other hand without modifying a riemann solver the hydrostatic reconstruction hr method was first introduced by audusse et al 2004 to develop a well balanced scheme being able to effectively constrain accentuated bed slopes to avoid the creation of unphysically large velocities and ensure the positivity of water depth near to wet dry fronts the hr method has since been widely applied and improved in developing swe models for practical applications duran et al 2015 liang and marche 2009 liang and smith 2015 for the simulation of overland flow with relatively coarse resolutions the difference of bed elevation between two neighbouring cells may become bigger than the water depth the original hr method of audusse et al 2004 which is first order accurate will treat this as a wet dry front and convert the continuous overland flow into a local dam break problem with a dry bed at the downstream side leading to ill estimated riemann states and underestimated bed slope and eventually incorrect prediction of flow dynamics and flood peak delestre et al 2012 these numerical issues may be resolved by extending the numerical scheme to second order accuracy e g hou et al 2018 2013 liang and marche 2009 wittmann et al 2017 however a second order scheme is computationally more expensive and in most of the cases the numerical accuracy as gained will not be reflected in the real world applications as the major source of uncertainties to the numerical solutions comes more from data rather than the numerical scheme itself therefore hydrodynamic models based on a computationally more efficient first order scheme is more favourable in modelling or forecasting real world overland flow and flood events ming et al 2020 xia et al 2019 to improve the original hr scheme for simulation of overland flows over steep slopes xia et al 2017 proposed a surface reconstruction method srm which involves using a second order reconstruction approach to more realistically represent the actual bed slopes and a water surface reconstruction step to derive the riemann states of water depth for flux calculation after also implementing a fully implicit discretisation scheme for friction terms srm can successfully reproduce the equilibrium state of an overland flow controlled by the balance between the slope gravity and friction effects demonstratinggreat potential for catchment scale full process flood modelling xia et al 2019 however lu et al 2018 pointed out the existence of unexplained wiggles in the srm numerical solution which may be a numerical flaw that could potentially lead to unexpected predictions more recently chen and noelle 2017 reported a new hr scheme hr cn that adopts the lower water surface as the bed elevation at the interface shared by the two neighbouring cells under partial wet condition providing another way to deal with the slope source term discretisation for the shallow downhill flows consistent with the downhill overland flow conditions the hr cn scheme has since been adopted by other researchers e g buttinger kreuzhuber et al 2019 dong and li 2020 to develop hydrodynamic models for fluvial flood and dam break simulations but its performance in modelling overland flows has not been tested and confirmed another numerical challenge for hydrodynamic simulation of overland flows is to handle the friction source terms which can become stiff and lead to numerical instability due to the nonlinear relationship between water depth and velocity when the water depth becomes very small in overland flows the discretisation of the stiff friction terms should relax the numerical solution toward a correct equilibrium state represented by the balance between the friction and slope gravity effects to capture the underlying physics and ensure stable simulation teyssier 2015 this balance can be effectively restored and maintained by a simplified hydraulic model e g a kinematic wave model based on this yu and duan 2014 introduced an overland flow model based on the combined numerical solutions to the swes and the kinematic wave approximation swe kwa in which the kwa solution is imposed when the water depth becomes smaller than a prescribed threshold value later on yu and duan 2017 presented a similar modelling strategy for surface runoff in which the overland flow in those cells with a sufficiently shallow depth is calculated using the diffusion wave approximation dwa instead of kwa both the swe dwa and swe dwa models can restore the balance between the bed and friction slopes and effectively remove the stiffness of the friction terms providing satisfactory results for overland flow simulations however the models must introduce a water depth threshold to switch between the different modelling approaches as indicated by yu and duan 2014 the simulation results are to a certain extent sensitive to the threshold values which inevitably introduces an extra model parameter for calibration and gives rise to additional uncertainty moreover the simplified kwa and dwa solution approaches are subject to the loss of rotational invariance when the friction decoupling method is applied to update the flow discharge which will make the simulation results highly dependent on the alignment of the flow direction with the adopted reference coordinate framework cozzolino et al 2021 this clearly limits the capability of such models to accurately predict complex overland flow and flood dynamics induced by intense rainfall a fully implicit discretisation scheme may also remove the stiffness of the friction terms and provide a numerical solution consistent with the correct equilibrium state of overland flows xia et al 2017 research effort has been made to derive fully implicit solutions to the friction terms that support explicit implementation in an overall explicit godunov type finite volume scheme hou et al 2018 xia and liang 2018 however the solution procedures and the resulting formulations as reported are complicated and may also create incorrect estimation under certain flow conditions in summary there have been some attempts to develop robust numerical schemes in the context of a godunov type framework to support accurate and stable simulation of overland flows but certain numerical issues still exist this work will revisit the fundamental aspects of the technical challenges and accordingly devise novel variable reconstruction and source term discretisation schemes to solve the swes to develop a robust godunov type model to provide accurate and stable predictions of overland flow and flooding process at a catchment scale the rest of the paper is organised as follows section 2 introduces the governing equations section 3 presents the numerical models implemented with the novel reconstruction schemes for fluxes calculation and source term discretisation section 4 introduces the alternative variable reconstructions used for comparison section 5 reviews the technical challenges and explains how they are addressed in this work the resulting numerical models are validated in section 6 with the results compared with those produced using alternative approaches finally brief conclusions are drawn in section 7 2 governing equations with the horizontal extents much bigger than water depth flood hydrodynamics including overland flows can be mathematically described by the swes which may be written in the matrix form of the 2d hyperbolic conservation laws as 1 q t f x g y s where t is the time x and y are the two cartesian coordinates q is the vector of conserved flow variables f and g are the vectors of fluxes in the x and y directions respectively s contains different source or sink terms the vector terms are given by 2 q h q x q y f q x q x 2 h g h 2 2 q y q x h g q y q y q x h q y 2 h g h 2 2 s r s b s f r i 0 0 0 gh z x gh z y 0 c f q x q x 2 q y 2 h 2 c f q y q x 2 q y 2 h 2 in which h is the water depth qx uh and qy vh are the unit width discharges in the x and y directions with u and v being the corresponding depth averaged velocity components g 9 81 m s2 is the acceleration due to gravity the source term vector s is further subdivided into the mass source terms r including rainfall rate r and infiltration rate i the slope source terms s b and the friction source terms s f in the source terms z is the bed elevation and c f g n f 2 h 1 3 is the roughness coefficient with nf being the manning coefficient 3 new godunov type finite volume model for overland flow and flood simulation herein a godunov type finite volume model is implemented with novel variable reconstruction and source term discretisation schemes to solve the above swes 1 and 2 for stable and accurate prediction of overland flow and flooding process in a first order scheme the cell averaged flow variables in an arbitrary cell i may be updated using the following discretised time marching formula 3 q i n 1 q i n δ t ω t k 1 m f k q i n l k δ t r i n s bi n s fi n where δt is the time step the superscript n denotes the time level ω i is the area of cell i k and m are respectively the index and the total number of cell edges m 4 for the uniform cartesian grid used in this work f k f k n k x g k n k y represents the flux vector normal to cell edge k of length lk with n k n k x n k y being the outward normal unit vector to update the flow variables to the next time step it is necessary to calculate the flux and source terms based on the values of the flow variables at the current time level which will be introduced in the following sub sections for the mass source terms r the rainfall rate r is provided from numerical weather predictions or rainfall monitoring records to drive a simulation and the infiltration rate i is estimated using a green ampt method as implemented in xia et al 2019 the numerical scheme is overall explicit and an appropriate time step δt can be estimated according to the courant friedrichs lewy cfl criterion to maintain numerical stability refer to liang and marche 2009 zhao et al 2018 and zhao et al 2019 for detailed implementation 3 1 a well balanced hydrostatic reconstruction hr scheme to update the flow variables in the context of a godunov type framework using eq 3 the interface fluxes are calculated by solving local riemann problems defined by the riemann states at cell interfaces for example at interface i 1 2 shared by cell i and its neighbour i 1 in the x direction the interface fluxes f 1 1 2 can be obtained by solving the riemann problem f defined as 4 f 1 1 2 f q l q r where q l and q r are the riemann states defined at the left l and right r sides of the cell interface the fluxes in other cell interfaces can be similarly defined to derive the riemann states the face values of flow variables must be first reconstructed from their values available at the cell centres in a first order numerical scheme the face values of the flow variables are assumed to be the same as those at the corresponding cell centres known as the piecewise constant approximation which gives ql qi and qr q i 1 with q representing one of the flow variables i e h η qx or qy herein η z h is the water surface elevation or water level based on these face values a well balanced hr scheme originally reported by audusse et al 2004 may be implemented to derive the required riemann states firstly a single bed elevation zf is defined at the cell interface under consideration by 5 z f max η l h l η r h r based on which the riemann states of water depth can be now derived as 6 h l max 0 η l z f and h r max 0 η r z f which automatically ensure non negativity of depth the riemann states of the x direction unit width discharge are then obtained by 7 q x l h l u l and q x r h r u r where ul q x l hl and ur q x r hr the riemann states of qy can be similarly derived the local riemann problem in eq 4 is now defined using these riemann states which can be solved using an approximate riemann solver to calculate the interface fluxes in this work the harten lax and van leer approximate riemann solver with contact wave restored hllc is adopted and the detailed implementation can be found in toro 1999 and liang and borthwick 2009 to update the flow variables using eq 3 it is also necessary to estimate the source terms whilst the friction source terms will be calculated using a fully implicit scheme as explained later in section 3 3 the bed slope source terms are considered here and may be expressed in a divergence form as 8 s b 1 ω i k 1 m s bk l k herein s bk is calculated based on the reconstructed face values at cell edge k as 9 s b k 0 g 2 n k x h i h k l z k f l z i g 2 n k y h i h k l z k f l z i where z k f l η k l h k l is introduced as the bed elevation at the interior side of the cell edge under consideration it has been proved that this slope source term discretisation approach is compatible with the above flux calculation scheme jointly delivering a well balanced hr scheme that also ensures non negativity of water depth for a simulation involving wetting and drying hou et al 2013 however as already mentioned previously the above classic first order accurate hr scheme may underestimate the actual bed slope and numerically create a spurious dry bed dam break problem when the water depth becomes smaller than the bed discontinuity caused by the numerical discretisation on a coarse resolution grid leading to incorrect prediction of overland flow dynamics figure 1 illustrates the two typical cases to consider in the hr reconstruction at cell interface i 1 2 depending on the water surfaces and bed elevations in the two neighbouring cells as involved chen and noelle 2017 case 1 defines a fully wet interface where the lower reconstructed water surface i e η l is above the higher bed elevation between the two cells i e zr in case 2 a partially wet interface is reconstructed which corresponds to the situation when the lower water surface i e η l is equal to or lower than the higher bed elevation between the two cells i e zr case 1 represents the common flow conditions and the hr scheme can perform normally to provide reliable predictions case 2 occurs when the continuous flow depth is smaller than the bed difference between two neighbouring cells and so the water bodies become disconnected in the two neighbouring cells after reconstruction herein we assume a constant bed slope and the water level at cell i is lower than that in cell i 1 without losing generality in this case the hr scheme will reconstruct the riemann stages and bed elevation to locally create an unrealistic dry bed dam break at the cell interface furthermore the corresponding slope source terms at cell i are calculated as s b i 1 2 0 g h i 2 2 0 t and subsequently underestimate the actual bed slope for a large scale rainfall induced overland flow simulation shallow water depth may cover a large part of the domain case 2 may therefore become predominant in the overall numerical calculation and introduce large numerical errors and uncertainties to the simulation results 3 2 a new corrected first order reconstruction scheme an hr scheme estimates the face values of bed elevation by calculating the difference between the face values of the corresponding water surface elevation η and water depth h as shown on the right hand side of eq 5 to address the numerical issues of a classic hr scheme as mentioned in the previous section that limits its capability in stable and accurate simulation of overland flows xia et al 2017 proposed the srm to reconstruct the face values of η rather than directly deriving them from their cell centred values however lu et al 2018 pointed out that srm might give rise to unphysical wiggles in the numerical solutions to overcome this and provide a corrected solution to address the aforementioned issues the following new reconstruction scheme is introduced to evaluate the face values of η to support the implementation of an hr scheme 10 η l max min z l h i max η i η i 1 min η i η i 1 η r max min z r h i 1 max η i η i 1 min η i η i 1 in which zl and zr are the face values of bed elevation obtained through linear slope limited reconstruction to better represent the actual bed slope and maintain model stability xia et al 2017 11 z l z i if h i 1 ε z i 0 5 φ r z i i 1 z i 1 z i else z r z i 1 if h i ε z i 1 0 5 φ r z i 1 i z i z i 1 else where φ r z i i 1 and φ r z i 1 i are the slope limiting functions the minmod slope limiter is adopted here for simplicity defined respectively at the left and right sides of cell interface i 1 2 with r z i i 1 zi z i 1 z i 1 zi and r z i 1 i z i 1 z i 2 zi z i 1 being the corresponding upwind slope ratios ε 10 10 m is a threshold of water depth to define a dry cell eq 10 is derived by reinforcing the maximum principle i e min η i η i 1 η l η r max η i η i 1 introduced by park et al 2010 which ensures that the reconstructed water levels fall in the range between the local maximum and minimum to preserve monotonicity for a stable simulation to further avoid the presence of inconsistent water surface gradient after reconstruction i e η i η i 1 η l η r 0 the following condition must be also further implemented 12 η l η r η l η r 2 if η i η i 1 η l η r 0 the new reconstruction scheme eqs 10 12 provides the required face values of water level for implementing an hr scheme to effectively avoid the nonphysical wiggles in the srm solutions and also address the numerical issues of underestimated bed slope and spurious dam break in large scale overland flow and flood simulations this will be further explained and discussed later in section 5 3 3 friction source term discretisation when simulating overland flows involving a small water depth another numerical challenge is to handle the stiff friction terms to allow accurate and stable simulations using normal time steps attempts have been made to develop fully implicit discretisation schemes to handle these stiff friction terms hou et al 2018 xia and liang 2018 the fully implicit discretisation strategy reported by hou et al 2018 allows explicit implementation and was validated against different overland flow problems however the scheme may potentially give rise to numerical errors and therefore an improved scheme is presented herein to support more reliable simulations and also simplify its implementation to implement the proposed fully implicit friction discretisation scheme in the godunov type finite volume framework the flow variables are first updated explicitly to produce the following intermediate flow variables q i n without considering friction terms 13 q i n q i n δ t ω t k 1 m f k q i n l k δ t r i n s bi n the friction source terms are then calculated through a fully implicit scheme as 14 q i n 1 q i n δ t s f i n 1 since the continuity mass equation does not involve an effective friction term we have h i n 1 h i n and the friction term discretisation is only relevant to the two momentum equations taking the x direction momentum equation as an example the following scheme may be used to derive the required solution for explicit implementation 15 q x i n 1 q x i n δ t c f q x i n 1 h i n 1 2 q x i n 2 q y i n 2 q x i n where cf is calculated based on the already known h i n 1 notably the term q x i n 2 q y i n 2 q x i n defines the flow direction and is calculated explicitly using the already available intermediate flow variables which will not affect the overall simulation accuracy this is different from the original approach presented in hou et al 2018 in which the term is defined using the flow variables from the previous time step i e q x i n 2 q y i n 2 q x i n the subtle change is essential to ensure a technically correct simulation it is straightforward to calculate the modulus of the friction source term vector as s f g n f 2 h n 1 7 3 q x n 1 q y n 1 2 and this scheme therefore ensures the rotational invariance discussed in cozzolino et al 2021 is maintained whilst the flow direction is impossible to change between the intermediate and friction updating steps because the friction effect can cause the flow to slow down and stop but cannot revert the flow it may occur between successive time steps therefore hou et al 2018 s approach may lead to incorrect results under certain flow conditions clearly eq 15 provides a quadratic equation for q x i n 1 as all other variables including h i n 1 are already known from the intermediate updating step in eq 13 the roots can then be analytically derived to be 16 q x i n 1 1 1 4 c f δ t h i n 1 2 q x i n 2 q y i n 2 2 c f h i n 1 2 q x i n 2 q y i n 2 q x i n since the friction effect must not change the direction of the flow only the positive root retains leading to 17 q x i n 1 0 if h i n 1 ε or q x i n ζ 1 1 4 c f δ t h i n 1 2 q x i n 2 q y i n 2 2 c f h i n 1 2 q x i n 2 q y i n 2 q x i n else where ζ 10 10 m2 s is a discharge threshold to define a cell with still water this updated scheme provides a corrected solution to handle the stiff friction terms the updating process for qy can be implemented in a similar way this completes the whole updating process of the momentum equations to predict the flow variables to a new time step 3 4 model implementation the above corrected hr scheme is used to improve the high performance integrated hydrodynamic modelling system hipims and implemented using a python and cuda c hybrid programming framework to achieve multi gpu and multi node high performance computing for large scale simulations the procedure of implementing the scheme at each time step is summarised as follows 1 reconstruct the required face values of flow variables i e h qx and qy through piecewise constant approximation bed elevation using eq 11 and water level using eqs 10 and 12 2 define the single bed elevation using eq 5 based on which the riemann states can be defined accordingly by following eqs 6 and 7 3 calculate the riemann fluxes using an hllc solver 4 evaluate the slope source terms using eq 8 and 9 5 update the flow variables to an intermediate step using eq 13 which directly calculates the water depth h to the new time step 6 update the discharge variables to the new time step using eq 17 for qx and similarly for qy to take into account friction effect 7 finally update the time level and move to the next time step 4 alternative variable reconstruction schemes to better demonstrate the performance of the model designed with the new variable reconstruction scheme referred to as the present model pm in the rest of the text to simulate overland flows and surface water flooding two alternative variable reconstruction schemes are also considered and used for comparison moreover a second order scheme pm 2nd is also introduced and implemented for comparison the implicit friction term discretisation scheme as presented in the previous section is implemented in all of these numerical schemes 4 1 surface reconstruction method srm by xia et al 2017 the srm was introduced to improve the original first order hr method to handle the numerical issues caused by a partially wet interface in the context of a first order godunov type finite volume scheme the face values of η are reconstructed to cell interface i 1 2 using 18 η l η i max 0 min z i 1 z i δ z η i 1 η i a n d η r η i 1 max 0 min z i z i 1 δ z η i η i 1 with 19 δ z z r z l in which zl and zr are the face values of bed elevation reconstructed using the slope limited approach as introduced in section 3 i e eq 11 these face values of water level are then used to define the riemann states of water depth after defining the single bed elevation at the cell interface as follows 20 z f srm max z i z i 1 it should be noted that the above definition of single bed elevation is different from eq 5 the riemann states can be then derived by following eqs 6 and 7 the bed slope source terms may be also calculated in srm using eqs 8 and 9 but different from the present model the bed elevation at the interior side is defined as again taking cell interface i 1 2 as an example 21 z f l srm z f srm max 0 z f srm η i if h i 1 ε z f srm max 0 min δ z z f srm η i if h i 1 ε 4 2 hydrostatic reconstruction by chen and noelle 2017 hr cn in hr cn the single bed elevation is defined at cell interface i 1 2 as 22 z f cn min max z i z i 1 min η i η i 1 the riemann states of water depth are then defined as 23 h l min h i η i z f cn and h r min h i 1 η i 1 z f cn and the riemann states of unit discharges are subsequently derived from eq 7 when evaluating the bed slope source terms using eq 9 the bed elevation at the interior side takes the value of the single bed elevation i e z f l cn z f cn 4 3 second order godunov type finite volume scheme as discussed in the introduction the spurious dam break problem may be resolved by implementing a second order hr scheme however second order hr schemes have not been sufficiently considered and evaluated for real world applications in this context a second order godunov type finite volume scheme may be achieved by implementing a second order runge kutta integration method in time and applying a slope limited muscl reconstruction method to reconstruct the face values of the flow variables in space refer to liang 2010 and zhao et al 2019 for detailed implementation herein an improved second order hr scheme is introduced for stable overland flow modelling when developing a second order scheme slope limited muscl reconstruction is essential to ensure the monotonicity of the numerical scheme and avoid the spurious oscillations that would otherwise appear in the numerical solutions close to shocks the monotonicity of the numerical scheme can be confirmed if it is total variation diminishing tvd and muscl reconstruction automatically satisfies this necessary condition when a minmod limiter is adopted however an hr scheme usually calculates the face values of bed elevation by finding the difference between the water level and depth as shown in eq 5 as a result the reconstructed face values of bed elevation may not satisfy the tvd condition since they are not directly derived through the muscl reconstruction therefore a new additional treatment is introduced in this work to restrict the reconstructed bed elevations to ensure the tvd property of the scheme for stable simulations i e 24 z l z r z l z r 2 if z i z i 1 z l z r 0 based on which the face values of water depth are recalculated as follows 25 h l η l z l and h r η r z r which can then be used to find the single value bed elevation and derive riemann states since eq 24 only forces zl zr when the specific condition is met the tvd property of water depth will be the same as the water surface i e the tvd property of the water depth modified through eq 25 will not be affected by the proposed treatment in implementation the corrected face value reconstruction procedure will be applied in the two successive steps of the runge kutta integration to achieve a second order scheme both in space and time 5 numerical challenges and resolutions this section discusses how the different numerical schemes introduced in sections 3 and 4 approach the numerical challenges as mentioned in section 1 5 1 the spurious dry bed dam break problem as shown in figure 1 the water surface submerges the beds of both neighbouring cells in the fully wet case all of the hr schemes introduced in this work can effectively handle this case without creating the numerical issues as mentioned therefore the following discussion will focus only on the partially wet case as shown in section 4 2 hr cn defines the single bed elevation as the lower water level between the two sides of the interface subsequently the riemann states of water depth at the cell with a lower bed is reconstructed to be zero this effectively turns any partially wet case as illustrated in figure 1 into a dry bed dam break similar to the original hr scheme obviously this does not reflect the reality and may become problematic for large scale overland flow simulations srm and pm are devised to improve the indistinctive approach of the original hr scheme and hr cn in handling the partially wet case to better represent the actual bed slope and support the reconstruction of the face values of flow variables srm and pm apply the slope limited linear reconstruction eq 11 to approximate the values of bed elevation i e zl zr at the cell interface in a first order scheme the face values of water depth is assumed to be the same as cell centre values depending on whether the water depth at the lower side i e hi is larger than the bed difference across the cell interface i e zr zl the partially wet case can be further divided into two sub cases as illustrated in figure 2 1 a local wet bed dam break wdb case when hi zr zl and 2 a dry bed dam break ddb case when hi zr zl in the figure the bed elevation in cell i 1 is assumed to be higher consistent with figure 1 to better reflect the actual flow condition and produce more accurate numerical predictions wdb should not be switched into a spurious dry bed dam break problem for an overland flow simulation based on zl and zr pm and srm reconstruct the face values of water surface i e η l η r using eq 10 and eq 18 and then define the single bed elevation zf using eq 20 and eq 5 respectively whilst zf is defined differently in srm and pm as indicated in figure 2 it is trivial to prove that the derived riemann states of water depth are actually identical for wdb both schemes return a positive riemann state of water depth at the left side of the cell interface effectively avoiding the creation of a spurious dry bed dam break for the ddb case with hi zr zl both srm and pm reconstruct a dry bed dam break with a modified slope as necessary to ensure numerical stability as shown in figure 2 to accurately predict the dynamics of overland flows and surface water floods it is also required a numerical scheme to more precisely represent the actual bed slope to reliably account for the gravity effect as well as properly limiting the local excessive bed slope caused by discontinuous topography to maintain the numerical stability to better explain how the different variable reconstruction schemes handle the slope source terms the calculation formulae for the x direction slope source term are summarised in table 1 assuming a similar partially wet case at cell interface i 1 2 shared by i and i 1 for a wdb case we consider a particular example of a uniform flow over a uniform slope to explain how the different numerical schemes discretise the x direction slope source term for this special case the actual slope source term for inclusion in the variable update formula 3 becomes sb ghi z i 1 zi δx ghi zi z i 1 δx as shown in table 1 the discretised formula of sb from the original hr scheme clearly underestimates the actual slope as we have hi z i 1 zi for a wdb under the uniform flow assumption the hr cn formula for sb becomes sb ghi zi z i 1 hi 2 δx which again leads to underestimation of the actual slope on the other hand for srm we have δz 0 from eq 19 for a uniform slope and h l h i for a uniform flow apparently reproducing the actual slope source term sb ghi z i 1 zi δx similarly for pm we have z i 1 2 f z i 1 zi 2 and z i 1 2 f z i 1 zi 2 from eqs 5 10 and 11 which will again give sb ghi z i 1 zi δx at the end therefore whilst both the original hr and hr cn schemes underestimate the actual bed slope srm and pm can effectively reconstruct the correct slope in their slope source term discretisation schemes as demonstrated by this idealised example since both the original hr and hr cn do not separate wdb and ddb in their variable reconstruction and source term discretisation schemes the following discussion for a ddb case is only relevant to srm and pm we first take an example of a uniform flow from cells i 1 and i falling over a steep slope into cell i 1 e g waterfall which create a wdb case at interface i 1 2 and ddb at interface i 1 2 in this case although hr cn does not separate wdb and ddb for a partially wet case it may calculate an excessive slope source term due to the use of eq 22 to define the single bed elevation at a cell interface which may potentially lead to numerical instability since the calculation formulae for the slope source term at interface i 1 2 are identical for wdb and ddb table 1 the slope source term discretisation process for srm and pm is exactly the same as that has been introduced previously for the wdb case to better compare the different ways in handling ddb by srm and pm we further assume a ddb case at cell interface i 1 2 but a horizontal bed between cells i and i 1 to give a normal fully wet case at interface i 1 2 and also s b i 1 2 0 for all hr schemes as a first order scheme pm assumes the face values of the water surface elevation equal to the corresponding cell centred value in cell i i e η i 1 2 l η i this gives s b pm g h i 2 2 δ x in this case as can be easily deduced from the pm formula provided in table 1 as the slope across cells i and i 1 is zero we have δz 1 0 5φ r z i 1 i z i 1 zi from eq 19 for srm the srm slope source term can then be calculated as s b srm ghi 2 z i 1 min δz z i 1 η i zi δx the pm and srm slope source terms are clearly different and the difference is 26 δ s b s b srm s b pm g h i 2 max φ r z i 1 i z i 1 z i 2 h i 0 δ x this difference in slope source term calculation may cause numerical issues which will be further explored in section 6 5 2 equilibrium state and stability criteria an swe model should also relax the flow dynamics toward an equilibrium state characterised the balance between gravity and friction effect during an overland flow simulation when the water depth becomes small under this condition the source terms can be represented as 27 s q e q q t f where q eq contains the flow variables at the equilibrium state and tf is the corresponding time step required to relax to the equilibrium state which can be decided by reciprocating the jacobian matrix s trace of the source terms under consideration teyssier 2015 as 28 t f 1 tr s q since the numerical fluxes and slope source terms are updated explicitly in this work tf is only considered here for the friction terms and should be therefore based on the intermediate state obtained after updating flux and slope source terms if the time step of the numerical scheme restricted by the cfl criterion δ t cfl δ x u g h is sufficiently small the flow variables should be asymptotic to the equilibrium state i e u 1 n h 2 3 z x u 1 n f h 2 3 z x and h h e q for simplification a 1d case is considered here and the timescale ratio tf δt can be written as 29 t f δ t u gh cfl δ x 2 g n f 2 hu gh z x h 7 3 when tf δt is smaller than 1 we have δt tf to create a stiff friction term if an explicit scheme is adopted to calculate the friction source terms the non stiff region tf δt 1 of the friction source terms requires a large cfl number e g if nf 0 02 sm 1 3 z x 1 and δx 10 m the non stiff region will be defined by cfl 1 025 and cfl 4 127 for heq 0 1 m and heq 0 5 m which will clearly violate the stability criteria for an explicit swe model i e cfl 1 the corrected fully implicit scheme as previously introduced is adopted to handle the stiff source terms in this work which will be tested and further discussed in section 6 1 2 6 results and discussion in this section the proposed new solution schemes i e the present first order scheme pm and its second order counterpart pm 2nd are validated against four test cases with the numerical results compared with those predicted by the two alternative schemes as introduced in section 3 i e srm and hr cn to demonstrate their advantages cfl 0 5 is used in all of the simulations to support quantitative comparison of the numerical results with analytical solutions or observations the relative bias rb root mean square error rmse and nashe sutcliffe efficiency nse are adopted which are defined as follows 30 rb q 100 q sim q ana q ana 31 rmse q i 1 m q i obs q i sim 2 m 32 nse q 1 i 1 m q i obs q i sim 2 i 1 m q i obs q obs 2 herein rb evaluates the relative error between the simulation results q sim and the analytical solutions q ana rmse quantifies the overall difference between the simulated results q i sim and observations q i obs where in the equation m represents the size of observation data nse is an indicator representing the goodness of fit in which q obs is the average of the observed values nse returns a value ranging from negative infinity to 1 with nse 1 indicating a perfect fit between the prediction q sim and observation q obs nse 0 75 1 classified as very good nse 0 65 0 75 as good nse 0 5 0 65 as satisfactory and nse 0 5 as unsatisfactory moriasi et al 2007 the hit rate h false alarm ratio f and critical success index c are further used to qualify how well the inundation extent is captured by the numerical model and the relevant formulae can be found in ming et al 2020 6 1 moving shorelines in a 2d parabolic bowl two analytical test cases involving moving shorelines in a frictionless or frictional parabolic bowl are first considered to test the numerical schemes in handling flux and slope source terms involving wetting and drying with and without considering friction 6 1 1 moving shorelines in a 2d frictionless parabolic bowl the analytical solution for this planar rotation test case was presented by thacker 1981 the 2d frictionless bottom profile i e parabolic bowl is defined as 33 z b x y h 0 1 x x 0 2 y y 0 2 a 2 where x 0 y 0 represents the centre of the parabolic bowl h 0 is the initial water depth at the centre a is the distance from the centre to the shoreline the analytical solutions of the water level η and velocities u v are given by 34 η t σ h 0 a 2 2 x x 0 cos ω t 2 y y 0 sin ω t u t ω sin ω t v t ω σ cos ω t where σ is a constant value and ω 2 g h 0 a is the angular velocity of the rotating water body following hou et al 2013 the parameters are set to be h 0 0 1 m a 1 0 m and σ 0 5 and the computational domain is 4 4 m2 centred at 2 m 2 m the domain is respectively discretised using a fine uniform grid of 2000 2000 cells δx 0 002 m and a coarse grid of 200 200 cells δx 0 02 m to support different simulations the analytical solutions at t 0 s are used to initialise the simulations and all simulations are run for four periods 4t with t 4 4857 s figure 3 compares the numerical results in terms of the diagonal profiles of η u and v with the analytical solutions at t 3 625t the first and second order schemes present different levels of sensitivity to mesh resolution at the fine resolution all four numerical schemes i e pm pm 2nd srm and hr cn capture well the water surface profile when predicting flow velocities all first order schemes clearly produce less accurate results compared to pm 2nd due to more excessive numerical diffusion pm and srm perform similarly and both outperform hr cn in velocity prediction at the coarse grid resolution hr cn fails to predict water level with acceptable accuracy whilst both pm and srm can predict η with reasonable accuracy the results are clearly numerically more diffusive for this test case involving periodically changing water surface profile compared with those produced by pm 2nd in terms of flow velocities pm 2nd slightly underestimates u and overestimates v at the wet and dry fronts but the overall prediction accuracy is high pm and srm perform similarly except that srm predicts spurious wiggles that will be explained later but the resulting velocity profiles are less accurate than the second order solutions hr cn largely fails to correctly predict flow velocities in this case srm solutions present unphysical oscillations as shown in the zoomed in view in the second row in figure 3 lu et al 2018 and xia et al 2018 argued that the possible reason for the appearance of these numerical wiggles may be because the scheme fails to properly handle the so called water gravitational effect caused by the difference in water level and the subsequent problematic riemann states of water depth however when a simulation starts from the same initial conditions srm and pm effectively reconstruct identical riemann states including the riemann states of water depth at all cell edges for flux calculations but pm does not produce similar numerical wiggles the two schemes return different estimations for the slope source terms srm only calculated the slope source term at the interface shared with the neighbouring cell with higher bed elevation therefore srm s inappropriate estimation of the slope source terms and the following influence on the flow velocity and dynamics may actually cause the spurious wiggles under certain flow conditions 6 1 2 moving shorelines in a 2d frictional parabolic bowl the test case of moving shorelines in a 2d frictional parabolic bowl sampson et al 2006 is further considered to validate the proposed friction discretisation scheme the bed topography is defined as 35 z x y h 0 x x 0 2 y y 0 2 a 2 where the relevant coefficient are similarly defined as in eq 33 the analytical solution for the water level is given by 36 η x y t h 0 b 2 e τ t 2 g b e τ t 2 g τ sin s t 2 s cos s t x x 0 τ cos s t 2 s sin s t y y 0 and the analytical solution of the velocities are 37 u t b e τ t 2 sin s t v t b e τ t 2 cos s t where s p 2 τ 2 2 with p being the peak amplitude parameter calculated by p 8 g h 0 a if the frictional parameter τ is smaller than p b v 0 is a constant the frictional parameter cf in eq 17 is re defined as c f τ q x i n 2 q y i n 2 to allow direct application of the friction discretisation scheme following hou et al 2013 a square computational domain of 8000 8000 m2 is used centred at 4000 4000 the relevant parameters are h 0 10m a 3000 b 5 m s and τ 0 002 s 1 simulations are run for four periods i e 4t with t 1377 68 s respectively on a fine resolution grid of 800 800 cells δx 10m and a coarse resolution grid s of 160 160 cells δx 50 m since pm 2nd has been demonstrated to outperform other schemes in the previous test case it is used herein to validate the proposed friction discretisation scheme figure 4 presents simulation results predicted on the fine and coarse grids at t 1 375t in terms of diagonal profiles of η u and v compared with the analytical solutions and also the numerical results obtained using the friction discretisation scheme introduced by hou et al 2018 referred to as hou s method for short whilst both friction discretisation schemes can accurately predict η only pm 2nd produces acceptable numerical predictions for velocities as shown in the zoom in view of the v velocity profiles noticeable wiggles close to the wet dry fronts are predicted by hou s method on both fine and coarse grids it appears that calculation on a fine mesh is not able to reduce the numerical error resulting from the use of the cell averaged values from the previous time step in the implicit friction discretisation scheme the results confirm that the corrected friction discretisation scheme presented in eq 17 is essential to ensure a correct simulation 6 2 steady flows over slopes steady flows over different bed slopes are further considered to evaluate the performance of the different variable reconstruction schemes all of the resulting models are implemented with the corrected friction discretisation scheme different boundary conditions are considered to generate different steady flows in a 1000 m long 40 m wide idealised flume with varied slope topographies the manning coefficient is set to nf 0 033 s m1 3 and the domain is assumed to be initially dry for all of the simulations 6 2 1 steady flows on different uniform slopes steady flows on three different uniform slopes i e s 0 02 0 06 0 1 are used to validate the current source term discretisation schemes in handling the steady equilibrium achieved by balanced gravity and friction effects as shown in xia et al 2017 the equilibrium flow field over a uniform slope may be analytically derived as h ana n f q in s 0 6 and u ana q in h ana a unit width discharge of q in 0 005 m2 s and the corresponding equilibrium water depth h ana 0 01739m 0 0125m and 0 01073m corresponding to the three bed slopes are imposed at the upstream entrance of the flume to generate steady flows the downstream end is assumed as a free outflow with the same equilibrium depth the computational domain flume is respectively discretised with a fine δx 1 m and coarse δx 20 m grids to support different simulations under these settings the water depths h ana become smaller than the bed difference between adjacent grid cells e g the bed difference is 0 02 m for the case of s 0 02 and δx 1m which is larger than h ana 0 01739 m creating the partially wet condition as previously mentioned all of the simulations are run for 10 000 seconds to ensure the equilibrium flow condition is reached figure 5 presents the normalised velocity u u ana profiles predicted by the different numerical schemes at different grid resolutions pm pm 2nd and srm are shown to predict accurate results however hr cn fails to produce acceptable results in four out of the six simulations i e all of the three simulations at the fine grid resolution and the case with s 0 02 gentle slope and δx 20 m coarse resolution considering the uniform steady flow with a water depth of h ana refer to figure 2 the actual slope source term in the x direction is sb gh ana zi z i 1 δx through hr cn s b cn g h ana z i z i 1 δ x g h ana 2 2 δ x which obviously underestimates the actual bed slope and subsequently the flow velocity meanwhile the creation of the spurious local dry bed dam break may further provide an inaccurate prediction of flow velocity the relative error re calculated for the hr cn predicted slope term is re s b cn sb sb nfqin 0 6 2s 1 6δx with nf and qin being constants the increase of mesh size and or bed slope can reduce re thus providing better results in terms of computational efficiency without spatial reconstruction hr cn is computationally most efficient therefore the computational times required by different schemes are normalised against hr cn for direct comparison in table 2 for all of the simulations pm and srm are shown to require similar computational time and pm 2nd is almost two times more expensive compared to the first order schemes 6 2 2 steady flow over a varying slope the test case of a steady flow over a varying slope delestre et al 2013 macdonald 1996 thi 2008 is further considered the dimensions of the flume are assumed to be the same as the one used in section 6 2 1 but the bed is now featured with a varying slope specified as 38 d z d x 1 4 g h x 3 h x 4 n 2 h x 10 3 the analytical solution of the water depth was provided by macdonald 1996 and thi 2008 as 39 h x 4 g 1 3 1 0 5 exp 16 x 1000 0 5 2 and accordingly 40 h x 2 125 4 g 1 3 x 1000 0 5 exp 16 x 1000 0 5 2 to create a steady flow subcritical inflow boundary conditions specified by q 2 m2 s and h 0 748409 m are imposed at the upstream end of the flume and the same water depth is used to control the downstream outflow boundary the computational domain is discretised using four uniform grids with δx 1 5 10 20 m the steady flow depth is larger than the bed difference between adjacent cells leading to a fully wet condition in the whole domain all of the simulations are again run for 10 000 seconds figure 6 presents the simulation results obtained on the four grids of different resolutions in comparison with analytical solutions it is apparent that all four numerical schemes can accurately predict the water level without noticeable oscillations close examination of the relative bias rb calculated for the flow velocity rb u in figure 6 shows that the prediction errors increase with the gird size pm and pm 2nd are found to perform similarly in predicting flow velocity across different grid resolutions this is as expected because the flux limiting function returns zero value due to the constant discharge and pm 2nd is effectively equivalent to the first order scheme when reconstructing the face values of discharges in this specific case from the calculated rb u srm outperforms hr cn but is still less accurate than the first and second order pm schemes 6 3 overland flow in a v shape catchment an idealised frictional v shaped catchment is also simulated to validate the capability of the numerical schemes in predicting rainfall runoff and overland flow process as shown in figure 7 the v shaped catchment is formed by two 1000 m 800 m sloping sides connecting to a 20 m wide channel in the middle the slopes of the inclined side and the central channel are 0 05 and 0 02 respectively the sidewall of the central channel is sufficiently high to avoid overtopping the manning coefficients are 0 015 s m1 3 and 0 15 s m1 3 for the sloping sides and channel respectively during the simulations uniform rainfall of a constant intensity of 10 8 mm h is applied for a duration of 1 5 h over the whole domain which is discretised using a uniform grid with δx 10 m all of the domain boundaries are assumed to be close except for the outlet of the channel figure 8 presents the simulation results predicted by different numerical schemes in terms of discharge hydrographs recorded at a hillsides and the channel outlet and water depth at gauges p1 and p2 their locations are illustrated in figure 7 a the analytical solutions for discharge hydrographs were provided by di giammarco et al 1996 and are also plotted in figure 8 a b for comparison from the results pm pm 2nd and srm perform similarly for this case and are able to predict the changes of discharge and flow depth during the rainfall event to a high level of accuracy however hr cn can only satisfactorily predict the discharge at the hillside but substantially underestimates the discharge at the channel outlet and water depth at both of the gauge points in this case the maximum depth at the hillsides is about 0 005 m figure 8 c which is much smaller than the bed difference between adjacent cells 0 5 m leading to partially wet conditions over most of the domain hr cn cannot effectively handle the partially wet condition to provide a reliable prediction of overland flows flow falls from the hillsides into the central channel creating a local ddb interface shared by the hillside and channel cells as shown in figure 7 b since the channel along the x direction is flat this creates a flow that falls from a slope onto a flat area as discussed in section 5 1 the difference between slope source terms estimated by srm and pm is δsb ghi 2 max 0 5φ r z i 1 i z i 1 zi hi 0 δx for the current case of the hillside with a constant slope if the minmod slope limiter is adopted the value of 0 5φ r z i 1 i z i 1 zi 0 5 z i 2 z i 1 is related to the bed difference between the two neighbouring cells on the hillside and is dependent on the grid size therefore an additional simulation is conducted with δx 5 m to further explore the performance of srm and pm in this test case figure 9 compares the flow variables h and qx predicted by different schemes along the two cross sections s1 and s2 as illustrated in figure 7 c apparently the updated values of flow discharge e g qx are related to the slope source term therefore the difference between the slope terms returned from the numerical schemes can directly link to a varied discharge as shown in table 1 the slope source term in cell i is calculated by g h i 2 2 δ x from all numerical schemes except srm as shown in figure 9 the absolute value of qx calculated by srm scheme is significantly larger than that in pm when hi 0 5φ r z i 1 i z i 1 zi and the location where the difference disappears is perfectly captured when hi 0 5φ r z i 1 i z i 1 zi which can be explained through the difference of the slope terms calculated by eq 26 moreover the phenomenon is sensitive to the cell size as revealed by comparison between the numerical results in figure 9 the numerical issue associated with srm may be linked to its unexpected calculation of the slope source terms leading to inaccurate prediction of discharge at any ddb interfaces however the discharges qx into the central channel from the two hillsides have the same magnitude but opposite signs and are mixed when reaching the central channel therefore the prediction of y direction discharge is not influenced in this symmetric case this explains why srm can provide an acceptable prediction of discharge in the central channel on the other hand the pm predictions do not seem to be adversely influenced by the water depth and bed difference in the hillsides 6 4 the 2015 storm desmond flood on the eden catchment uk with their performance in simulating overland flows involving small water depth and wetting and drying confirmed in the previous test cases the first and second order pm schemes are finally applied to reproduce a rainfall induced surface water flood event across the 2500km2 eden catchment in england to further confirm their capability in real world applications the introduction of the flood event and details of the case study site and relevant datasets can be found in ming et al 2020 and xia et al 2019 according to ming et al 2020 the simulation at a 10 m spatial resolution provides a good balance between solution accuracy and computational cost therefore the 10 m dem and land use data figure 10 a b resampled from the original 5 m datasets are used in this work to support the following simulations simulations are also run on 20 m and 40 m uniform grids to explore the influence of grid resolution on the simulation results the rainfall radar data at 1 km spatial resolution and 5 min temporal resolution are obtained from the uk met office to drive the models the mean rainfall intensity over the whole catchment is shown in figure 10 c following ming et al 2020 and xia et al 2019 the calibrated manning coefficient of 0 055 s m1 3is used for river channels and 0 075 s m1 3 for other parts of the catchment zero infiltration is assumed due to antecedent rainfall events and fully saturated soil the rainfall data between 3rd 4th december 2005 is used to drive the models to produce the initial conditions for the following simulations of the 96 hrs flood event between 4th 8th december figure 11 compares the predicted water level hydrographs with the measurements at 16 river gauges overall both pm and pm 2nd schemes reproduce reasonably well the measured hydrographs and correctly capture the rising and falling limbs as well as the peak water levels in most of the gauges when the simulation resolution is sufficiently high e g 10 m due to the poor representation of topographic features and river streams using the 20 m and 40 m dems the simulation results are clearly much less favourable this is consistent with the results and conclusions reported by ming et al 2020 and xia et al 2019 who simulated the same events using a model based on srm even at 10 m resolution noticeable differences can be detected but this is as expected for such large scale simulations of a real world event in particular at the sands centre gauge the predicted water level largely deviates from the observations in both of the rising and falling stages this may be because the gauge is located in a smaller stream which is not sufficiently resolved by the dems being used at a high resolution of 10 m pm and pm 2nd are found to perform similarly in reproducing the measured hydrographs making it difficult to conclude which scheme works better at the coarse resolutions especially at 40m the performance of the two pm schemes is somehow mixed with pm 2nd capturing much better the shape of the hydrographs at several gauges in particular great musgrave bridge melbourne park and greenholme but pm is found to visually performed much better at kirkby stephen to quantitively evaluate the accuracy of numerical results nse and rmse are calculated against measurements at different gauges and presented in table 3 sand centre is excluded from the statistic analysis as the models supported by the currently available data clearly cannot produce reasonable results both of the pm schemes return a mean nse 0 65 for the corresponding simulations at 10 m resolution with the mean nse for pm being slightly higher 0 69 vs 0 67 this may be because the adopted values of the manning coefficient were originally calibrated for a first order scheme ming et al 2020 xia et al 2019 on the other hand the mean rmse calculated for the pm 2nd results is slightly better 0 36 vs 0 37 however the difference is too small to draw any meaningful conclusions at the coarser resolutions the simulation results become much less accurate as indicated by the calculated nses and rmses which is as expected due to the poor representation of terrain features especially river streams but it is interesting to observe that pm 2nd overall consistently outperforms the first order scheme in the coarse resolution simulations figure 12 presents the maximum inundation extents at the city of carlisle predicted by the two pm schemes at different resolutions the results compare reasonably well with the surveyed extent which do not seem to be significantly sensitive to grid resolution again consistent with previous studies ming et al 2020 xia et al 2019 examining more closely the results in the selected area as indicated in figures 12 the pm based model appears to predict more areas with inundation depth larger than 2 m and the water depth in these areas are shown to be relatively sensitive to grid resolution on the other hand the results from pm 2nd in this focused area are observed to be more consistent across the simulations at different grid resolutions following the approach adopted by ming et al 2020 the hit rate h false alarm ratio f and critical success index c are calculated for part of the city to provide quantitative assessment of the results in terms of inundation extent the selected area is focused on the north side of the newcastle carlisle railway as indicated by the area shaded by light green in figure 12 because the surveyed area does not seem to cover the south side of newcastle and carlisle railway e g newman catholic school from the calculated results as presented in table 4 the influence of mesh resolution on the results as indicated by the three statistical metrics is small with the hit rate varying between 0 97 0 98 the false alarm ratio 0 09 0 11 and the critical success index 0 88 0 89 the results from the pm scheme return slightly higher values for hit rate and false alarm ratio indicating a larger flood extent is predicted the critical success indices calculated for the results predicted by pm and pm 2nd on meshes of different resolutions are also consistent with pm 2nd results on δx 20 m being slightly better in comparison with the pm simulation results the pm 2nd predictions consistently return high hit rates and critical success indices but low false alarm ratios across all of the simulations this indicates that the predictions from the two schemes are consistent and it is difficult to justify the absolute benefit of using a 2nd order scheme in this field scale application due to the high uncertainty associated with the surveyed flood extent further conclusions can only be drawn through more thorough systematic analysis when high quality observation data are available for comparison the simulations are run on a gpu server with 2 nvidia tesla v100 and the runtimes for reproducing the 120 h flood event on different resolutions are listed in table 5 the pm 2nd based model is approximately 2 2 2 8 times more expensive than the pm model for this test case consistent with the results as shown in table 2 7 conclusions to address the major technical challenges in the simulation of overland flow and surface water flooding processes over complex terrains novel variable reconstruction and discretisation schemes are developed in the context of a godunov type finite volume swe model for accurate and stable calculation of fluxes slope and friction source terms in comparison with alternative numerical schemes a detailed discussion has been provided to explain how the new schemes address the numerical challenges by avoiding the creation of spurious dry bed dam break cases more properly estimating slope source terms and reproducing the correct equilibrium stage of shallow water flow characterised by the balance between gravity and friction effects the new schemes both in first pm and second order pm 2nd accuracy are subsequently implemented to develop hydrodynamic models tested for the simulation of theoretical and real world test cases involving overland flow and surface water flooding processes from the simulations results some conclusions may be drawn the hr cn scheme cannot properly handle the partially wet case to effectively avoid creation of spurious dry bed dam breaks it is not recommended for an application involving predominant overland flow process featured with shallow depth whilst srm can be effectively implemented to prevent the numerical dam break issue it may provide an inaccurate estimation of bed slope source terms under certain circumstances and should be used with care for applications involving abruptly changing topography the corrected variable reconstruction scheme pm proposed in this work better addresses the numerical challenges as discussed in this work and the resulting model has been successfully validated against the considered test cases producing results compared more favourably with reference solutions than the alternative hr cn and srm schemes the corrected second order scheme pm 2nd is shown to predict more accurate results than the other first order schemes for the theoretical test cases however the second order scheme requires more than two times of runtime for the test cases considered in this work whilst the second order model seems to provide better predictions at coarse resolutions its benefit for a large scale real world simulation still cannot be clearly justified the proposed friction discretisation scheme corrects a subtle technical flaw of the original scheme leading to improved simulation accuracy and simplified implementation the new variable reconstruction and source term discretisation schemes at both first and second order accuracy have been shown to improve model performance in simulating catchment scale full process flooding process including overland flow although uniform grids are used in all of the test cases in this work the numerical methods are independent of mesh type and can be directly implemented on other types of grids e g triangular grids credit authorship contribution statement jiaheng zhao conceptualization methodology software formal analysis visualization writing original draft qiuhua liang conceptualization methodology writing review editing supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is funded by ukri through research projects river basins as living laboratories project ne s012427 1 valbgi project ne s00288x 1 living deltas hub ne s008926 1 
168,due to the recent advances in computing and data acquisition technologies numerical models solving the 2d shallow water equations swes are now widely used in predicting overland flow and surface water flooding process in natural catchments in catchment scale flood modelling in addition to the calculation of convective terms that are usually achieved through solving local riemann problems to capture transient flow dynamics correct discretisation of source terms is also essential to handle complex domain topography and ensure accurate and stable numerical solutions the external forces induced by gravity and bed friction create the key source terms that drive the change of momentum in the swes and affect the stability and accuracy of the adopted numerical scheme especially for applications involving very shallow water depth e g wet and dry fronts overland flows herein new schemes are introduced to reconstruct the face values of flow variables and a fully implicit algorithm is improved to discretise the stiff friction terms in the context of developing a godunov type finite volume hydrodynamic model for accurate and stable simulation of overland flow and surface water flooding involving very shallow water depth five analytical test cases are considered to validate the resulting models and the numerical results are compared with those produed by two alternative numerical schemes the performance of the models are further demonstrated by reproducing a flood event in the 2500 km2 eden catchment uk keywords finite volume method face value reconstruction shallow water equations overland flows hydrostatic reconstruction source term discretisation data source links and availability the links to the data sources used in storm desmond on the eden catchment uk are given as follows dem the dem of the catchment is obtained by merging the bare earth digital terrain model dtm https digimap edina ac uk with the digital surface model dsm https data gov uk dataset fba12e80 2010519f 20104be2 2010806f 201041be9e26ab96 lidar 2010composite 2010dsm 20102m to include buildings and key infrastructures additionally the river channels are considered by adding river bathymetry to the dem https datahub admiralty co uk portal apps sites marine data portal items q river land use land cover map is provided by the centre for ecology hydrology https www ceh ac uk services land cover map 2015 rainfall gridded radar rainfall data is available from the uk met office http badc nerc ac uk measurement and flood extent river gauge observations https environment data gov uk hydrology landing and the surveyed flood maps can be found in https data gov uk dataset 76292bec 20107d8b 201043e8 20109c98 201002734fd89c81 historic 2010flood 2010map wherein the dtm data is freely available to all users via subscribing to the digimap service the dsm data river gauge observations and surveyed flood maps are open to public users under the uk open government licence https data gov uk the landcover data is available upon request from the ceh data licensing team datalicensing ceh ac uk and the radar rainfall data is available upon request from ceda archive http archive ceda ac uk 1 introduction flooding is the most widespread natural hazard that affects most if not all of the countries and regions across the world hirabayashi et al 2013 it is recognised that climate change and rapid urbanisation will lead to an increase of flood risk in the 21st century and beyond ipcc 2021 it is crucial for governments of different levels across the world to effectively manage flood risk and enhance societal resilience to flooding and climate change to sustain socio economic development numerical modelling has become an indispensable tool in supporting different stages of flood risk management in catchment scale flood modelling hydrological models or simplified hydraulic models have been predominantly used in the past decades govindaraju et al 1988 jakeman and hornberger 1993 however these models do not enforce strict momentum conservation and are not suited for the simulation of transient flood dynamics induced by intense rainfall bout et al 2018 caviedes voullième et al 2020 cea and bladé 2015 xia et al 2017 many researchers recognise the necessity of adopting fully hydrodynamic models that solve the shallow water equations swes using a shock capturing numerical method to better account for the underlying physical processes and more reliably predict the rapidly varying flood waves interacting with complex terrain topographies and engineering structures e g cao et al 2010 to use this type of sophisticated hydrodynamic models to support catchment scale surface water flood modelling a key numerical challenge is to ensure a godunov type shock capturing scheme to maintain the so called c property bermudez and vazquez 1994 or more general well balanced property greenberg and leroux 1996 and this involves the implementation of compatible numerical methods to discretise the flux and source terms including bed slope and friction terms to accurately and stably predict overland flows with very shallow depth as part of the catchment scale rainfall flooding process xia et al 2017 numerous well balanced schemes have been developed and reported in the literature in the past three decades george 2008 greenberg and leroux 1996 kesserwani 2013 leveque 2002 murillo and garcía navarro 2010 zhou et al 2001 by directly considering slope source terms in the flux calculation the augmented roe riemann solver introduced by george 2008 can be implemented to effectively ensure well balanced property for simulations over variable topography in a roe riemann solver the source terms may be also projected onto the jacobian matrix to develop a path conservative scheme to achieve the required well balanced property e g castro et al 2006 murillo and garcía navarro 2010 parés and castro 2004 on the other hand without modifying a riemann solver the hydrostatic reconstruction hr method was first introduced by audusse et al 2004 to develop a well balanced scheme being able to effectively constrain accentuated bed slopes to avoid the creation of unphysically large velocities and ensure the positivity of water depth near to wet dry fronts the hr method has since been widely applied and improved in developing swe models for practical applications duran et al 2015 liang and marche 2009 liang and smith 2015 for the simulation of overland flow with relatively coarse resolutions the difference of bed elevation between two neighbouring cells may become bigger than the water depth the original hr method of audusse et al 2004 which is first order accurate will treat this as a wet dry front and convert the continuous overland flow into a local dam break problem with a dry bed at the downstream side leading to ill estimated riemann states and underestimated bed slope and eventually incorrect prediction of flow dynamics and flood peak delestre et al 2012 these numerical issues may be resolved by extending the numerical scheme to second order accuracy e g hou et al 2018 2013 liang and marche 2009 wittmann et al 2017 however a second order scheme is computationally more expensive and in most of the cases the numerical accuracy as gained will not be reflected in the real world applications as the major source of uncertainties to the numerical solutions comes more from data rather than the numerical scheme itself therefore hydrodynamic models based on a computationally more efficient first order scheme is more favourable in modelling or forecasting real world overland flow and flood events ming et al 2020 xia et al 2019 to improve the original hr scheme for simulation of overland flows over steep slopes xia et al 2017 proposed a surface reconstruction method srm which involves using a second order reconstruction approach to more realistically represent the actual bed slopes and a water surface reconstruction step to derive the riemann states of water depth for flux calculation after also implementing a fully implicit discretisation scheme for friction terms srm can successfully reproduce the equilibrium state of an overland flow controlled by the balance between the slope gravity and friction effects demonstratinggreat potential for catchment scale full process flood modelling xia et al 2019 however lu et al 2018 pointed out the existence of unexplained wiggles in the srm numerical solution which may be a numerical flaw that could potentially lead to unexpected predictions more recently chen and noelle 2017 reported a new hr scheme hr cn that adopts the lower water surface as the bed elevation at the interface shared by the two neighbouring cells under partial wet condition providing another way to deal with the slope source term discretisation for the shallow downhill flows consistent with the downhill overland flow conditions the hr cn scheme has since been adopted by other researchers e g buttinger kreuzhuber et al 2019 dong and li 2020 to develop hydrodynamic models for fluvial flood and dam break simulations but its performance in modelling overland flows has not been tested and confirmed another numerical challenge for hydrodynamic simulation of overland flows is to handle the friction source terms which can become stiff and lead to numerical instability due to the nonlinear relationship between water depth and velocity when the water depth becomes very small in overland flows the discretisation of the stiff friction terms should relax the numerical solution toward a correct equilibrium state represented by the balance between the friction and slope gravity effects to capture the underlying physics and ensure stable simulation teyssier 2015 this balance can be effectively restored and maintained by a simplified hydraulic model e g a kinematic wave model based on this yu and duan 2014 introduced an overland flow model based on the combined numerical solutions to the swes and the kinematic wave approximation swe kwa in which the kwa solution is imposed when the water depth becomes smaller than a prescribed threshold value later on yu and duan 2017 presented a similar modelling strategy for surface runoff in which the overland flow in those cells with a sufficiently shallow depth is calculated using the diffusion wave approximation dwa instead of kwa both the swe dwa and swe dwa models can restore the balance between the bed and friction slopes and effectively remove the stiffness of the friction terms providing satisfactory results for overland flow simulations however the models must introduce a water depth threshold to switch between the different modelling approaches as indicated by yu and duan 2014 the simulation results are to a certain extent sensitive to the threshold values which inevitably introduces an extra model parameter for calibration and gives rise to additional uncertainty moreover the simplified kwa and dwa solution approaches are subject to the loss of rotational invariance when the friction decoupling method is applied to update the flow discharge which will make the simulation results highly dependent on the alignment of the flow direction with the adopted reference coordinate framework cozzolino et al 2021 this clearly limits the capability of such models to accurately predict complex overland flow and flood dynamics induced by intense rainfall a fully implicit discretisation scheme may also remove the stiffness of the friction terms and provide a numerical solution consistent with the correct equilibrium state of overland flows xia et al 2017 research effort has been made to derive fully implicit solutions to the friction terms that support explicit implementation in an overall explicit godunov type finite volume scheme hou et al 2018 xia and liang 2018 however the solution procedures and the resulting formulations as reported are complicated and may also create incorrect estimation under certain flow conditions in summary there have been some attempts to develop robust numerical schemes in the context of a godunov type framework to support accurate and stable simulation of overland flows but certain numerical issues still exist this work will revisit the fundamental aspects of the technical challenges and accordingly devise novel variable reconstruction and source term discretisation schemes to solve the swes to develop a robust godunov type model to provide accurate and stable predictions of overland flow and flooding process at a catchment scale the rest of the paper is organised as follows section 2 introduces the governing equations section 3 presents the numerical models implemented with the novel reconstruction schemes for fluxes calculation and source term discretisation section 4 introduces the alternative variable reconstructions used for comparison section 5 reviews the technical challenges and explains how they are addressed in this work the resulting numerical models are validated in section 6 with the results compared with those produced using alternative approaches finally brief conclusions are drawn in section 7 2 governing equations with the horizontal extents much bigger than water depth flood hydrodynamics including overland flows can be mathematically described by the swes which may be written in the matrix form of the 2d hyperbolic conservation laws as 1 q t f x g y s where t is the time x and y are the two cartesian coordinates q is the vector of conserved flow variables f and g are the vectors of fluxes in the x and y directions respectively s contains different source or sink terms the vector terms are given by 2 q h q x q y f q x q x 2 h g h 2 2 q y q x h g q y q y q x h q y 2 h g h 2 2 s r s b s f r i 0 0 0 gh z x gh z y 0 c f q x q x 2 q y 2 h 2 c f q y q x 2 q y 2 h 2 in which h is the water depth qx uh and qy vh are the unit width discharges in the x and y directions with u and v being the corresponding depth averaged velocity components g 9 81 m s2 is the acceleration due to gravity the source term vector s is further subdivided into the mass source terms r including rainfall rate r and infiltration rate i the slope source terms s b and the friction source terms s f in the source terms z is the bed elevation and c f g n f 2 h 1 3 is the roughness coefficient with nf being the manning coefficient 3 new godunov type finite volume model for overland flow and flood simulation herein a godunov type finite volume model is implemented with novel variable reconstruction and source term discretisation schemes to solve the above swes 1 and 2 for stable and accurate prediction of overland flow and flooding process in a first order scheme the cell averaged flow variables in an arbitrary cell i may be updated using the following discretised time marching formula 3 q i n 1 q i n δ t ω t k 1 m f k q i n l k δ t r i n s bi n s fi n where δt is the time step the superscript n denotes the time level ω i is the area of cell i k and m are respectively the index and the total number of cell edges m 4 for the uniform cartesian grid used in this work f k f k n k x g k n k y represents the flux vector normal to cell edge k of length lk with n k n k x n k y being the outward normal unit vector to update the flow variables to the next time step it is necessary to calculate the flux and source terms based on the values of the flow variables at the current time level which will be introduced in the following sub sections for the mass source terms r the rainfall rate r is provided from numerical weather predictions or rainfall monitoring records to drive a simulation and the infiltration rate i is estimated using a green ampt method as implemented in xia et al 2019 the numerical scheme is overall explicit and an appropriate time step δt can be estimated according to the courant friedrichs lewy cfl criterion to maintain numerical stability refer to liang and marche 2009 zhao et al 2018 and zhao et al 2019 for detailed implementation 3 1 a well balanced hydrostatic reconstruction hr scheme to update the flow variables in the context of a godunov type framework using eq 3 the interface fluxes are calculated by solving local riemann problems defined by the riemann states at cell interfaces for example at interface i 1 2 shared by cell i and its neighbour i 1 in the x direction the interface fluxes f 1 1 2 can be obtained by solving the riemann problem f defined as 4 f 1 1 2 f q l q r where q l and q r are the riemann states defined at the left l and right r sides of the cell interface the fluxes in other cell interfaces can be similarly defined to derive the riemann states the face values of flow variables must be first reconstructed from their values available at the cell centres in a first order numerical scheme the face values of the flow variables are assumed to be the same as those at the corresponding cell centres known as the piecewise constant approximation which gives ql qi and qr q i 1 with q representing one of the flow variables i e h η qx or qy herein η z h is the water surface elevation or water level based on these face values a well balanced hr scheme originally reported by audusse et al 2004 may be implemented to derive the required riemann states firstly a single bed elevation zf is defined at the cell interface under consideration by 5 z f max η l h l η r h r based on which the riemann states of water depth can be now derived as 6 h l max 0 η l z f and h r max 0 η r z f which automatically ensure non negativity of depth the riemann states of the x direction unit width discharge are then obtained by 7 q x l h l u l and q x r h r u r where ul q x l hl and ur q x r hr the riemann states of qy can be similarly derived the local riemann problem in eq 4 is now defined using these riemann states which can be solved using an approximate riemann solver to calculate the interface fluxes in this work the harten lax and van leer approximate riemann solver with contact wave restored hllc is adopted and the detailed implementation can be found in toro 1999 and liang and borthwick 2009 to update the flow variables using eq 3 it is also necessary to estimate the source terms whilst the friction source terms will be calculated using a fully implicit scheme as explained later in section 3 3 the bed slope source terms are considered here and may be expressed in a divergence form as 8 s b 1 ω i k 1 m s bk l k herein s bk is calculated based on the reconstructed face values at cell edge k as 9 s b k 0 g 2 n k x h i h k l z k f l z i g 2 n k y h i h k l z k f l z i where z k f l η k l h k l is introduced as the bed elevation at the interior side of the cell edge under consideration it has been proved that this slope source term discretisation approach is compatible with the above flux calculation scheme jointly delivering a well balanced hr scheme that also ensures non negativity of water depth for a simulation involving wetting and drying hou et al 2013 however as already mentioned previously the above classic first order accurate hr scheme may underestimate the actual bed slope and numerically create a spurious dry bed dam break problem when the water depth becomes smaller than the bed discontinuity caused by the numerical discretisation on a coarse resolution grid leading to incorrect prediction of overland flow dynamics figure 1 illustrates the two typical cases to consider in the hr reconstruction at cell interface i 1 2 depending on the water surfaces and bed elevations in the two neighbouring cells as involved chen and noelle 2017 case 1 defines a fully wet interface where the lower reconstructed water surface i e η l is above the higher bed elevation between the two cells i e zr in case 2 a partially wet interface is reconstructed which corresponds to the situation when the lower water surface i e η l is equal to or lower than the higher bed elevation between the two cells i e zr case 1 represents the common flow conditions and the hr scheme can perform normally to provide reliable predictions case 2 occurs when the continuous flow depth is smaller than the bed difference between two neighbouring cells and so the water bodies become disconnected in the two neighbouring cells after reconstruction herein we assume a constant bed slope and the water level at cell i is lower than that in cell i 1 without losing generality in this case the hr scheme will reconstruct the riemann stages and bed elevation to locally create an unrealistic dry bed dam break at the cell interface furthermore the corresponding slope source terms at cell i are calculated as s b i 1 2 0 g h i 2 2 0 t and subsequently underestimate the actual bed slope for a large scale rainfall induced overland flow simulation shallow water depth may cover a large part of the domain case 2 may therefore become predominant in the overall numerical calculation and introduce large numerical errors and uncertainties to the simulation results 3 2 a new corrected first order reconstruction scheme an hr scheme estimates the face values of bed elevation by calculating the difference between the face values of the corresponding water surface elevation η and water depth h as shown on the right hand side of eq 5 to address the numerical issues of a classic hr scheme as mentioned in the previous section that limits its capability in stable and accurate simulation of overland flows xia et al 2017 proposed the srm to reconstruct the face values of η rather than directly deriving them from their cell centred values however lu et al 2018 pointed out that srm might give rise to unphysical wiggles in the numerical solutions to overcome this and provide a corrected solution to address the aforementioned issues the following new reconstruction scheme is introduced to evaluate the face values of η to support the implementation of an hr scheme 10 η l max min z l h i max η i η i 1 min η i η i 1 η r max min z r h i 1 max η i η i 1 min η i η i 1 in which zl and zr are the face values of bed elevation obtained through linear slope limited reconstruction to better represent the actual bed slope and maintain model stability xia et al 2017 11 z l z i if h i 1 ε z i 0 5 φ r z i i 1 z i 1 z i else z r z i 1 if h i ε z i 1 0 5 φ r z i 1 i z i z i 1 else where φ r z i i 1 and φ r z i 1 i are the slope limiting functions the minmod slope limiter is adopted here for simplicity defined respectively at the left and right sides of cell interface i 1 2 with r z i i 1 zi z i 1 z i 1 zi and r z i 1 i z i 1 z i 2 zi z i 1 being the corresponding upwind slope ratios ε 10 10 m is a threshold of water depth to define a dry cell eq 10 is derived by reinforcing the maximum principle i e min η i η i 1 η l η r max η i η i 1 introduced by park et al 2010 which ensures that the reconstructed water levels fall in the range between the local maximum and minimum to preserve monotonicity for a stable simulation to further avoid the presence of inconsistent water surface gradient after reconstruction i e η i η i 1 η l η r 0 the following condition must be also further implemented 12 η l η r η l η r 2 if η i η i 1 η l η r 0 the new reconstruction scheme eqs 10 12 provides the required face values of water level for implementing an hr scheme to effectively avoid the nonphysical wiggles in the srm solutions and also address the numerical issues of underestimated bed slope and spurious dam break in large scale overland flow and flood simulations this will be further explained and discussed later in section 5 3 3 friction source term discretisation when simulating overland flows involving a small water depth another numerical challenge is to handle the stiff friction terms to allow accurate and stable simulations using normal time steps attempts have been made to develop fully implicit discretisation schemes to handle these stiff friction terms hou et al 2018 xia and liang 2018 the fully implicit discretisation strategy reported by hou et al 2018 allows explicit implementation and was validated against different overland flow problems however the scheme may potentially give rise to numerical errors and therefore an improved scheme is presented herein to support more reliable simulations and also simplify its implementation to implement the proposed fully implicit friction discretisation scheme in the godunov type finite volume framework the flow variables are first updated explicitly to produce the following intermediate flow variables q i n without considering friction terms 13 q i n q i n δ t ω t k 1 m f k q i n l k δ t r i n s bi n the friction source terms are then calculated through a fully implicit scheme as 14 q i n 1 q i n δ t s f i n 1 since the continuity mass equation does not involve an effective friction term we have h i n 1 h i n and the friction term discretisation is only relevant to the two momentum equations taking the x direction momentum equation as an example the following scheme may be used to derive the required solution for explicit implementation 15 q x i n 1 q x i n δ t c f q x i n 1 h i n 1 2 q x i n 2 q y i n 2 q x i n where cf is calculated based on the already known h i n 1 notably the term q x i n 2 q y i n 2 q x i n defines the flow direction and is calculated explicitly using the already available intermediate flow variables which will not affect the overall simulation accuracy this is different from the original approach presented in hou et al 2018 in which the term is defined using the flow variables from the previous time step i e q x i n 2 q y i n 2 q x i n the subtle change is essential to ensure a technically correct simulation it is straightforward to calculate the modulus of the friction source term vector as s f g n f 2 h n 1 7 3 q x n 1 q y n 1 2 and this scheme therefore ensures the rotational invariance discussed in cozzolino et al 2021 is maintained whilst the flow direction is impossible to change between the intermediate and friction updating steps because the friction effect can cause the flow to slow down and stop but cannot revert the flow it may occur between successive time steps therefore hou et al 2018 s approach may lead to incorrect results under certain flow conditions clearly eq 15 provides a quadratic equation for q x i n 1 as all other variables including h i n 1 are already known from the intermediate updating step in eq 13 the roots can then be analytically derived to be 16 q x i n 1 1 1 4 c f δ t h i n 1 2 q x i n 2 q y i n 2 2 c f h i n 1 2 q x i n 2 q y i n 2 q x i n since the friction effect must not change the direction of the flow only the positive root retains leading to 17 q x i n 1 0 if h i n 1 ε or q x i n ζ 1 1 4 c f δ t h i n 1 2 q x i n 2 q y i n 2 2 c f h i n 1 2 q x i n 2 q y i n 2 q x i n else where ζ 10 10 m2 s is a discharge threshold to define a cell with still water this updated scheme provides a corrected solution to handle the stiff friction terms the updating process for qy can be implemented in a similar way this completes the whole updating process of the momentum equations to predict the flow variables to a new time step 3 4 model implementation the above corrected hr scheme is used to improve the high performance integrated hydrodynamic modelling system hipims and implemented using a python and cuda c hybrid programming framework to achieve multi gpu and multi node high performance computing for large scale simulations the procedure of implementing the scheme at each time step is summarised as follows 1 reconstruct the required face values of flow variables i e h qx and qy through piecewise constant approximation bed elevation using eq 11 and water level using eqs 10 and 12 2 define the single bed elevation using eq 5 based on which the riemann states can be defined accordingly by following eqs 6 and 7 3 calculate the riemann fluxes using an hllc solver 4 evaluate the slope source terms using eq 8 and 9 5 update the flow variables to an intermediate step using eq 13 which directly calculates the water depth h to the new time step 6 update the discharge variables to the new time step using eq 17 for qx and similarly for qy to take into account friction effect 7 finally update the time level and move to the next time step 4 alternative variable reconstruction schemes to better demonstrate the performance of the model designed with the new variable reconstruction scheme referred to as the present model pm in the rest of the text to simulate overland flows and surface water flooding two alternative variable reconstruction schemes are also considered and used for comparison moreover a second order scheme pm 2nd is also introduced and implemented for comparison the implicit friction term discretisation scheme as presented in the previous section is implemented in all of these numerical schemes 4 1 surface reconstruction method srm by xia et al 2017 the srm was introduced to improve the original first order hr method to handle the numerical issues caused by a partially wet interface in the context of a first order godunov type finite volume scheme the face values of η are reconstructed to cell interface i 1 2 using 18 η l η i max 0 min z i 1 z i δ z η i 1 η i a n d η r η i 1 max 0 min z i z i 1 δ z η i η i 1 with 19 δ z z r z l in which zl and zr are the face values of bed elevation reconstructed using the slope limited approach as introduced in section 3 i e eq 11 these face values of water level are then used to define the riemann states of water depth after defining the single bed elevation at the cell interface as follows 20 z f srm max z i z i 1 it should be noted that the above definition of single bed elevation is different from eq 5 the riemann states can be then derived by following eqs 6 and 7 the bed slope source terms may be also calculated in srm using eqs 8 and 9 but different from the present model the bed elevation at the interior side is defined as again taking cell interface i 1 2 as an example 21 z f l srm z f srm max 0 z f srm η i if h i 1 ε z f srm max 0 min δ z z f srm η i if h i 1 ε 4 2 hydrostatic reconstruction by chen and noelle 2017 hr cn in hr cn the single bed elevation is defined at cell interface i 1 2 as 22 z f cn min max z i z i 1 min η i η i 1 the riemann states of water depth are then defined as 23 h l min h i η i z f cn and h r min h i 1 η i 1 z f cn and the riemann states of unit discharges are subsequently derived from eq 7 when evaluating the bed slope source terms using eq 9 the bed elevation at the interior side takes the value of the single bed elevation i e z f l cn z f cn 4 3 second order godunov type finite volume scheme as discussed in the introduction the spurious dam break problem may be resolved by implementing a second order hr scheme however second order hr schemes have not been sufficiently considered and evaluated for real world applications in this context a second order godunov type finite volume scheme may be achieved by implementing a second order runge kutta integration method in time and applying a slope limited muscl reconstruction method to reconstruct the face values of the flow variables in space refer to liang 2010 and zhao et al 2019 for detailed implementation herein an improved second order hr scheme is introduced for stable overland flow modelling when developing a second order scheme slope limited muscl reconstruction is essential to ensure the monotonicity of the numerical scheme and avoid the spurious oscillations that would otherwise appear in the numerical solutions close to shocks the monotonicity of the numerical scheme can be confirmed if it is total variation diminishing tvd and muscl reconstruction automatically satisfies this necessary condition when a minmod limiter is adopted however an hr scheme usually calculates the face values of bed elevation by finding the difference between the water level and depth as shown in eq 5 as a result the reconstructed face values of bed elevation may not satisfy the tvd condition since they are not directly derived through the muscl reconstruction therefore a new additional treatment is introduced in this work to restrict the reconstructed bed elevations to ensure the tvd property of the scheme for stable simulations i e 24 z l z r z l z r 2 if z i z i 1 z l z r 0 based on which the face values of water depth are recalculated as follows 25 h l η l z l and h r η r z r which can then be used to find the single value bed elevation and derive riemann states since eq 24 only forces zl zr when the specific condition is met the tvd property of water depth will be the same as the water surface i e the tvd property of the water depth modified through eq 25 will not be affected by the proposed treatment in implementation the corrected face value reconstruction procedure will be applied in the two successive steps of the runge kutta integration to achieve a second order scheme both in space and time 5 numerical challenges and resolutions this section discusses how the different numerical schemes introduced in sections 3 and 4 approach the numerical challenges as mentioned in section 1 5 1 the spurious dry bed dam break problem as shown in figure 1 the water surface submerges the beds of both neighbouring cells in the fully wet case all of the hr schemes introduced in this work can effectively handle this case without creating the numerical issues as mentioned therefore the following discussion will focus only on the partially wet case as shown in section 4 2 hr cn defines the single bed elevation as the lower water level between the two sides of the interface subsequently the riemann states of water depth at the cell with a lower bed is reconstructed to be zero this effectively turns any partially wet case as illustrated in figure 1 into a dry bed dam break similar to the original hr scheme obviously this does not reflect the reality and may become problematic for large scale overland flow simulations srm and pm are devised to improve the indistinctive approach of the original hr scheme and hr cn in handling the partially wet case to better represent the actual bed slope and support the reconstruction of the face values of flow variables srm and pm apply the slope limited linear reconstruction eq 11 to approximate the values of bed elevation i e zl zr at the cell interface in a first order scheme the face values of water depth is assumed to be the same as cell centre values depending on whether the water depth at the lower side i e hi is larger than the bed difference across the cell interface i e zr zl the partially wet case can be further divided into two sub cases as illustrated in figure 2 1 a local wet bed dam break wdb case when hi zr zl and 2 a dry bed dam break ddb case when hi zr zl in the figure the bed elevation in cell i 1 is assumed to be higher consistent with figure 1 to better reflect the actual flow condition and produce more accurate numerical predictions wdb should not be switched into a spurious dry bed dam break problem for an overland flow simulation based on zl and zr pm and srm reconstruct the face values of water surface i e η l η r using eq 10 and eq 18 and then define the single bed elevation zf using eq 20 and eq 5 respectively whilst zf is defined differently in srm and pm as indicated in figure 2 it is trivial to prove that the derived riemann states of water depth are actually identical for wdb both schemes return a positive riemann state of water depth at the left side of the cell interface effectively avoiding the creation of a spurious dry bed dam break for the ddb case with hi zr zl both srm and pm reconstruct a dry bed dam break with a modified slope as necessary to ensure numerical stability as shown in figure 2 to accurately predict the dynamics of overland flows and surface water floods it is also required a numerical scheme to more precisely represent the actual bed slope to reliably account for the gravity effect as well as properly limiting the local excessive bed slope caused by discontinuous topography to maintain the numerical stability to better explain how the different variable reconstruction schemes handle the slope source terms the calculation formulae for the x direction slope source term are summarised in table 1 assuming a similar partially wet case at cell interface i 1 2 shared by i and i 1 for a wdb case we consider a particular example of a uniform flow over a uniform slope to explain how the different numerical schemes discretise the x direction slope source term for this special case the actual slope source term for inclusion in the variable update formula 3 becomes sb ghi z i 1 zi δx ghi zi z i 1 δx as shown in table 1 the discretised formula of sb from the original hr scheme clearly underestimates the actual slope as we have hi z i 1 zi for a wdb under the uniform flow assumption the hr cn formula for sb becomes sb ghi zi z i 1 hi 2 δx which again leads to underestimation of the actual slope on the other hand for srm we have δz 0 from eq 19 for a uniform slope and h l h i for a uniform flow apparently reproducing the actual slope source term sb ghi z i 1 zi δx similarly for pm we have z i 1 2 f z i 1 zi 2 and z i 1 2 f z i 1 zi 2 from eqs 5 10 and 11 which will again give sb ghi z i 1 zi δx at the end therefore whilst both the original hr and hr cn schemes underestimate the actual bed slope srm and pm can effectively reconstruct the correct slope in their slope source term discretisation schemes as demonstrated by this idealised example since both the original hr and hr cn do not separate wdb and ddb in their variable reconstruction and source term discretisation schemes the following discussion for a ddb case is only relevant to srm and pm we first take an example of a uniform flow from cells i 1 and i falling over a steep slope into cell i 1 e g waterfall which create a wdb case at interface i 1 2 and ddb at interface i 1 2 in this case although hr cn does not separate wdb and ddb for a partially wet case it may calculate an excessive slope source term due to the use of eq 22 to define the single bed elevation at a cell interface which may potentially lead to numerical instability since the calculation formulae for the slope source term at interface i 1 2 are identical for wdb and ddb table 1 the slope source term discretisation process for srm and pm is exactly the same as that has been introduced previously for the wdb case to better compare the different ways in handling ddb by srm and pm we further assume a ddb case at cell interface i 1 2 but a horizontal bed between cells i and i 1 to give a normal fully wet case at interface i 1 2 and also s b i 1 2 0 for all hr schemes as a first order scheme pm assumes the face values of the water surface elevation equal to the corresponding cell centred value in cell i i e η i 1 2 l η i this gives s b pm g h i 2 2 δ x in this case as can be easily deduced from the pm formula provided in table 1 as the slope across cells i and i 1 is zero we have δz 1 0 5φ r z i 1 i z i 1 zi from eq 19 for srm the srm slope source term can then be calculated as s b srm ghi 2 z i 1 min δz z i 1 η i zi δx the pm and srm slope source terms are clearly different and the difference is 26 δ s b s b srm s b pm g h i 2 max φ r z i 1 i z i 1 z i 2 h i 0 δ x this difference in slope source term calculation may cause numerical issues which will be further explored in section 6 5 2 equilibrium state and stability criteria an swe model should also relax the flow dynamics toward an equilibrium state characterised the balance between gravity and friction effect during an overland flow simulation when the water depth becomes small under this condition the source terms can be represented as 27 s q e q q t f where q eq contains the flow variables at the equilibrium state and tf is the corresponding time step required to relax to the equilibrium state which can be decided by reciprocating the jacobian matrix s trace of the source terms under consideration teyssier 2015 as 28 t f 1 tr s q since the numerical fluxes and slope source terms are updated explicitly in this work tf is only considered here for the friction terms and should be therefore based on the intermediate state obtained after updating flux and slope source terms if the time step of the numerical scheme restricted by the cfl criterion δ t cfl δ x u g h is sufficiently small the flow variables should be asymptotic to the equilibrium state i e u 1 n h 2 3 z x u 1 n f h 2 3 z x and h h e q for simplification a 1d case is considered here and the timescale ratio tf δt can be written as 29 t f δ t u gh cfl δ x 2 g n f 2 hu gh z x h 7 3 when tf δt is smaller than 1 we have δt tf to create a stiff friction term if an explicit scheme is adopted to calculate the friction source terms the non stiff region tf δt 1 of the friction source terms requires a large cfl number e g if nf 0 02 sm 1 3 z x 1 and δx 10 m the non stiff region will be defined by cfl 1 025 and cfl 4 127 for heq 0 1 m and heq 0 5 m which will clearly violate the stability criteria for an explicit swe model i e cfl 1 the corrected fully implicit scheme as previously introduced is adopted to handle the stiff source terms in this work which will be tested and further discussed in section 6 1 2 6 results and discussion in this section the proposed new solution schemes i e the present first order scheme pm and its second order counterpart pm 2nd are validated against four test cases with the numerical results compared with those predicted by the two alternative schemes as introduced in section 3 i e srm and hr cn to demonstrate their advantages cfl 0 5 is used in all of the simulations to support quantitative comparison of the numerical results with analytical solutions or observations the relative bias rb root mean square error rmse and nashe sutcliffe efficiency nse are adopted which are defined as follows 30 rb q 100 q sim q ana q ana 31 rmse q i 1 m q i obs q i sim 2 m 32 nse q 1 i 1 m q i obs q i sim 2 i 1 m q i obs q obs 2 herein rb evaluates the relative error between the simulation results q sim and the analytical solutions q ana rmse quantifies the overall difference between the simulated results q i sim and observations q i obs where in the equation m represents the size of observation data nse is an indicator representing the goodness of fit in which q obs is the average of the observed values nse returns a value ranging from negative infinity to 1 with nse 1 indicating a perfect fit between the prediction q sim and observation q obs nse 0 75 1 classified as very good nse 0 65 0 75 as good nse 0 5 0 65 as satisfactory and nse 0 5 as unsatisfactory moriasi et al 2007 the hit rate h false alarm ratio f and critical success index c are further used to qualify how well the inundation extent is captured by the numerical model and the relevant formulae can be found in ming et al 2020 6 1 moving shorelines in a 2d parabolic bowl two analytical test cases involving moving shorelines in a frictionless or frictional parabolic bowl are first considered to test the numerical schemes in handling flux and slope source terms involving wetting and drying with and without considering friction 6 1 1 moving shorelines in a 2d frictionless parabolic bowl the analytical solution for this planar rotation test case was presented by thacker 1981 the 2d frictionless bottom profile i e parabolic bowl is defined as 33 z b x y h 0 1 x x 0 2 y y 0 2 a 2 where x 0 y 0 represents the centre of the parabolic bowl h 0 is the initial water depth at the centre a is the distance from the centre to the shoreline the analytical solutions of the water level η and velocities u v are given by 34 η t σ h 0 a 2 2 x x 0 cos ω t 2 y y 0 sin ω t u t ω sin ω t v t ω σ cos ω t where σ is a constant value and ω 2 g h 0 a is the angular velocity of the rotating water body following hou et al 2013 the parameters are set to be h 0 0 1 m a 1 0 m and σ 0 5 and the computational domain is 4 4 m2 centred at 2 m 2 m the domain is respectively discretised using a fine uniform grid of 2000 2000 cells δx 0 002 m and a coarse grid of 200 200 cells δx 0 02 m to support different simulations the analytical solutions at t 0 s are used to initialise the simulations and all simulations are run for four periods 4t with t 4 4857 s figure 3 compares the numerical results in terms of the diagonal profiles of η u and v with the analytical solutions at t 3 625t the first and second order schemes present different levels of sensitivity to mesh resolution at the fine resolution all four numerical schemes i e pm pm 2nd srm and hr cn capture well the water surface profile when predicting flow velocities all first order schemes clearly produce less accurate results compared to pm 2nd due to more excessive numerical diffusion pm and srm perform similarly and both outperform hr cn in velocity prediction at the coarse grid resolution hr cn fails to predict water level with acceptable accuracy whilst both pm and srm can predict η with reasonable accuracy the results are clearly numerically more diffusive for this test case involving periodically changing water surface profile compared with those produced by pm 2nd in terms of flow velocities pm 2nd slightly underestimates u and overestimates v at the wet and dry fronts but the overall prediction accuracy is high pm and srm perform similarly except that srm predicts spurious wiggles that will be explained later but the resulting velocity profiles are less accurate than the second order solutions hr cn largely fails to correctly predict flow velocities in this case srm solutions present unphysical oscillations as shown in the zoomed in view in the second row in figure 3 lu et al 2018 and xia et al 2018 argued that the possible reason for the appearance of these numerical wiggles may be because the scheme fails to properly handle the so called water gravitational effect caused by the difference in water level and the subsequent problematic riemann states of water depth however when a simulation starts from the same initial conditions srm and pm effectively reconstruct identical riemann states including the riemann states of water depth at all cell edges for flux calculations but pm does not produce similar numerical wiggles the two schemes return different estimations for the slope source terms srm only calculated the slope source term at the interface shared with the neighbouring cell with higher bed elevation therefore srm s inappropriate estimation of the slope source terms and the following influence on the flow velocity and dynamics may actually cause the spurious wiggles under certain flow conditions 6 1 2 moving shorelines in a 2d frictional parabolic bowl the test case of moving shorelines in a 2d frictional parabolic bowl sampson et al 2006 is further considered to validate the proposed friction discretisation scheme the bed topography is defined as 35 z x y h 0 x x 0 2 y y 0 2 a 2 where the relevant coefficient are similarly defined as in eq 33 the analytical solution for the water level is given by 36 η x y t h 0 b 2 e τ t 2 g b e τ t 2 g τ sin s t 2 s cos s t x x 0 τ cos s t 2 s sin s t y y 0 and the analytical solution of the velocities are 37 u t b e τ t 2 sin s t v t b e τ t 2 cos s t where s p 2 τ 2 2 with p being the peak amplitude parameter calculated by p 8 g h 0 a if the frictional parameter τ is smaller than p b v 0 is a constant the frictional parameter cf in eq 17 is re defined as c f τ q x i n 2 q y i n 2 to allow direct application of the friction discretisation scheme following hou et al 2013 a square computational domain of 8000 8000 m2 is used centred at 4000 4000 the relevant parameters are h 0 10m a 3000 b 5 m s and τ 0 002 s 1 simulations are run for four periods i e 4t with t 1377 68 s respectively on a fine resolution grid of 800 800 cells δx 10m and a coarse resolution grid s of 160 160 cells δx 50 m since pm 2nd has been demonstrated to outperform other schemes in the previous test case it is used herein to validate the proposed friction discretisation scheme figure 4 presents simulation results predicted on the fine and coarse grids at t 1 375t in terms of diagonal profiles of η u and v compared with the analytical solutions and also the numerical results obtained using the friction discretisation scheme introduced by hou et al 2018 referred to as hou s method for short whilst both friction discretisation schemes can accurately predict η only pm 2nd produces acceptable numerical predictions for velocities as shown in the zoom in view of the v velocity profiles noticeable wiggles close to the wet dry fronts are predicted by hou s method on both fine and coarse grids it appears that calculation on a fine mesh is not able to reduce the numerical error resulting from the use of the cell averaged values from the previous time step in the implicit friction discretisation scheme the results confirm that the corrected friction discretisation scheme presented in eq 17 is essential to ensure a correct simulation 6 2 steady flows over slopes steady flows over different bed slopes are further considered to evaluate the performance of the different variable reconstruction schemes all of the resulting models are implemented with the corrected friction discretisation scheme different boundary conditions are considered to generate different steady flows in a 1000 m long 40 m wide idealised flume with varied slope topographies the manning coefficient is set to nf 0 033 s m1 3 and the domain is assumed to be initially dry for all of the simulations 6 2 1 steady flows on different uniform slopes steady flows on three different uniform slopes i e s 0 02 0 06 0 1 are used to validate the current source term discretisation schemes in handling the steady equilibrium achieved by balanced gravity and friction effects as shown in xia et al 2017 the equilibrium flow field over a uniform slope may be analytically derived as h ana n f q in s 0 6 and u ana q in h ana a unit width discharge of q in 0 005 m2 s and the corresponding equilibrium water depth h ana 0 01739m 0 0125m and 0 01073m corresponding to the three bed slopes are imposed at the upstream entrance of the flume to generate steady flows the downstream end is assumed as a free outflow with the same equilibrium depth the computational domain flume is respectively discretised with a fine δx 1 m and coarse δx 20 m grids to support different simulations under these settings the water depths h ana become smaller than the bed difference between adjacent grid cells e g the bed difference is 0 02 m for the case of s 0 02 and δx 1m which is larger than h ana 0 01739 m creating the partially wet condition as previously mentioned all of the simulations are run for 10 000 seconds to ensure the equilibrium flow condition is reached figure 5 presents the normalised velocity u u ana profiles predicted by the different numerical schemes at different grid resolutions pm pm 2nd and srm are shown to predict accurate results however hr cn fails to produce acceptable results in four out of the six simulations i e all of the three simulations at the fine grid resolution and the case with s 0 02 gentle slope and δx 20 m coarse resolution considering the uniform steady flow with a water depth of h ana refer to figure 2 the actual slope source term in the x direction is sb gh ana zi z i 1 δx through hr cn s b cn g h ana z i z i 1 δ x g h ana 2 2 δ x which obviously underestimates the actual bed slope and subsequently the flow velocity meanwhile the creation of the spurious local dry bed dam break may further provide an inaccurate prediction of flow velocity the relative error re calculated for the hr cn predicted slope term is re s b cn sb sb nfqin 0 6 2s 1 6δx with nf and qin being constants the increase of mesh size and or bed slope can reduce re thus providing better results in terms of computational efficiency without spatial reconstruction hr cn is computationally most efficient therefore the computational times required by different schemes are normalised against hr cn for direct comparison in table 2 for all of the simulations pm and srm are shown to require similar computational time and pm 2nd is almost two times more expensive compared to the first order schemes 6 2 2 steady flow over a varying slope the test case of a steady flow over a varying slope delestre et al 2013 macdonald 1996 thi 2008 is further considered the dimensions of the flume are assumed to be the same as the one used in section 6 2 1 but the bed is now featured with a varying slope specified as 38 d z d x 1 4 g h x 3 h x 4 n 2 h x 10 3 the analytical solution of the water depth was provided by macdonald 1996 and thi 2008 as 39 h x 4 g 1 3 1 0 5 exp 16 x 1000 0 5 2 and accordingly 40 h x 2 125 4 g 1 3 x 1000 0 5 exp 16 x 1000 0 5 2 to create a steady flow subcritical inflow boundary conditions specified by q 2 m2 s and h 0 748409 m are imposed at the upstream end of the flume and the same water depth is used to control the downstream outflow boundary the computational domain is discretised using four uniform grids with δx 1 5 10 20 m the steady flow depth is larger than the bed difference between adjacent cells leading to a fully wet condition in the whole domain all of the simulations are again run for 10 000 seconds figure 6 presents the simulation results obtained on the four grids of different resolutions in comparison with analytical solutions it is apparent that all four numerical schemes can accurately predict the water level without noticeable oscillations close examination of the relative bias rb calculated for the flow velocity rb u in figure 6 shows that the prediction errors increase with the gird size pm and pm 2nd are found to perform similarly in predicting flow velocity across different grid resolutions this is as expected because the flux limiting function returns zero value due to the constant discharge and pm 2nd is effectively equivalent to the first order scheme when reconstructing the face values of discharges in this specific case from the calculated rb u srm outperforms hr cn but is still less accurate than the first and second order pm schemes 6 3 overland flow in a v shape catchment an idealised frictional v shaped catchment is also simulated to validate the capability of the numerical schemes in predicting rainfall runoff and overland flow process as shown in figure 7 the v shaped catchment is formed by two 1000 m 800 m sloping sides connecting to a 20 m wide channel in the middle the slopes of the inclined side and the central channel are 0 05 and 0 02 respectively the sidewall of the central channel is sufficiently high to avoid overtopping the manning coefficients are 0 015 s m1 3 and 0 15 s m1 3 for the sloping sides and channel respectively during the simulations uniform rainfall of a constant intensity of 10 8 mm h is applied for a duration of 1 5 h over the whole domain which is discretised using a uniform grid with δx 10 m all of the domain boundaries are assumed to be close except for the outlet of the channel figure 8 presents the simulation results predicted by different numerical schemes in terms of discharge hydrographs recorded at a hillsides and the channel outlet and water depth at gauges p1 and p2 their locations are illustrated in figure 7 a the analytical solutions for discharge hydrographs were provided by di giammarco et al 1996 and are also plotted in figure 8 a b for comparison from the results pm pm 2nd and srm perform similarly for this case and are able to predict the changes of discharge and flow depth during the rainfall event to a high level of accuracy however hr cn can only satisfactorily predict the discharge at the hillside but substantially underestimates the discharge at the channel outlet and water depth at both of the gauge points in this case the maximum depth at the hillsides is about 0 005 m figure 8 c which is much smaller than the bed difference between adjacent cells 0 5 m leading to partially wet conditions over most of the domain hr cn cannot effectively handle the partially wet condition to provide a reliable prediction of overland flows flow falls from the hillsides into the central channel creating a local ddb interface shared by the hillside and channel cells as shown in figure 7 b since the channel along the x direction is flat this creates a flow that falls from a slope onto a flat area as discussed in section 5 1 the difference between slope source terms estimated by srm and pm is δsb ghi 2 max 0 5φ r z i 1 i z i 1 zi hi 0 δx for the current case of the hillside with a constant slope if the minmod slope limiter is adopted the value of 0 5φ r z i 1 i z i 1 zi 0 5 z i 2 z i 1 is related to the bed difference between the two neighbouring cells on the hillside and is dependent on the grid size therefore an additional simulation is conducted with δx 5 m to further explore the performance of srm and pm in this test case figure 9 compares the flow variables h and qx predicted by different schemes along the two cross sections s1 and s2 as illustrated in figure 7 c apparently the updated values of flow discharge e g qx are related to the slope source term therefore the difference between the slope terms returned from the numerical schemes can directly link to a varied discharge as shown in table 1 the slope source term in cell i is calculated by g h i 2 2 δ x from all numerical schemes except srm as shown in figure 9 the absolute value of qx calculated by srm scheme is significantly larger than that in pm when hi 0 5φ r z i 1 i z i 1 zi and the location where the difference disappears is perfectly captured when hi 0 5φ r z i 1 i z i 1 zi which can be explained through the difference of the slope terms calculated by eq 26 moreover the phenomenon is sensitive to the cell size as revealed by comparison between the numerical results in figure 9 the numerical issue associated with srm may be linked to its unexpected calculation of the slope source terms leading to inaccurate prediction of discharge at any ddb interfaces however the discharges qx into the central channel from the two hillsides have the same magnitude but opposite signs and are mixed when reaching the central channel therefore the prediction of y direction discharge is not influenced in this symmetric case this explains why srm can provide an acceptable prediction of discharge in the central channel on the other hand the pm predictions do not seem to be adversely influenced by the water depth and bed difference in the hillsides 6 4 the 2015 storm desmond flood on the eden catchment uk with their performance in simulating overland flows involving small water depth and wetting and drying confirmed in the previous test cases the first and second order pm schemes are finally applied to reproduce a rainfall induced surface water flood event across the 2500km2 eden catchment in england to further confirm their capability in real world applications the introduction of the flood event and details of the case study site and relevant datasets can be found in ming et al 2020 and xia et al 2019 according to ming et al 2020 the simulation at a 10 m spatial resolution provides a good balance between solution accuracy and computational cost therefore the 10 m dem and land use data figure 10 a b resampled from the original 5 m datasets are used in this work to support the following simulations simulations are also run on 20 m and 40 m uniform grids to explore the influence of grid resolution on the simulation results the rainfall radar data at 1 km spatial resolution and 5 min temporal resolution are obtained from the uk met office to drive the models the mean rainfall intensity over the whole catchment is shown in figure 10 c following ming et al 2020 and xia et al 2019 the calibrated manning coefficient of 0 055 s m1 3is used for river channels and 0 075 s m1 3 for other parts of the catchment zero infiltration is assumed due to antecedent rainfall events and fully saturated soil the rainfall data between 3rd 4th december 2005 is used to drive the models to produce the initial conditions for the following simulations of the 96 hrs flood event between 4th 8th december figure 11 compares the predicted water level hydrographs with the measurements at 16 river gauges overall both pm and pm 2nd schemes reproduce reasonably well the measured hydrographs and correctly capture the rising and falling limbs as well as the peak water levels in most of the gauges when the simulation resolution is sufficiently high e g 10 m due to the poor representation of topographic features and river streams using the 20 m and 40 m dems the simulation results are clearly much less favourable this is consistent with the results and conclusions reported by ming et al 2020 and xia et al 2019 who simulated the same events using a model based on srm even at 10 m resolution noticeable differences can be detected but this is as expected for such large scale simulations of a real world event in particular at the sands centre gauge the predicted water level largely deviates from the observations in both of the rising and falling stages this may be because the gauge is located in a smaller stream which is not sufficiently resolved by the dems being used at a high resolution of 10 m pm and pm 2nd are found to perform similarly in reproducing the measured hydrographs making it difficult to conclude which scheme works better at the coarse resolutions especially at 40m the performance of the two pm schemes is somehow mixed with pm 2nd capturing much better the shape of the hydrographs at several gauges in particular great musgrave bridge melbourne park and greenholme but pm is found to visually performed much better at kirkby stephen to quantitively evaluate the accuracy of numerical results nse and rmse are calculated against measurements at different gauges and presented in table 3 sand centre is excluded from the statistic analysis as the models supported by the currently available data clearly cannot produce reasonable results both of the pm schemes return a mean nse 0 65 for the corresponding simulations at 10 m resolution with the mean nse for pm being slightly higher 0 69 vs 0 67 this may be because the adopted values of the manning coefficient were originally calibrated for a first order scheme ming et al 2020 xia et al 2019 on the other hand the mean rmse calculated for the pm 2nd results is slightly better 0 36 vs 0 37 however the difference is too small to draw any meaningful conclusions at the coarser resolutions the simulation results become much less accurate as indicated by the calculated nses and rmses which is as expected due to the poor representation of terrain features especially river streams but it is interesting to observe that pm 2nd overall consistently outperforms the first order scheme in the coarse resolution simulations figure 12 presents the maximum inundation extents at the city of carlisle predicted by the two pm schemes at different resolutions the results compare reasonably well with the surveyed extent which do not seem to be significantly sensitive to grid resolution again consistent with previous studies ming et al 2020 xia et al 2019 examining more closely the results in the selected area as indicated in figures 12 the pm based model appears to predict more areas with inundation depth larger than 2 m and the water depth in these areas are shown to be relatively sensitive to grid resolution on the other hand the results from pm 2nd in this focused area are observed to be more consistent across the simulations at different grid resolutions following the approach adopted by ming et al 2020 the hit rate h false alarm ratio f and critical success index c are calculated for part of the city to provide quantitative assessment of the results in terms of inundation extent the selected area is focused on the north side of the newcastle carlisle railway as indicated by the area shaded by light green in figure 12 because the surveyed area does not seem to cover the south side of newcastle and carlisle railway e g newman catholic school from the calculated results as presented in table 4 the influence of mesh resolution on the results as indicated by the three statistical metrics is small with the hit rate varying between 0 97 0 98 the false alarm ratio 0 09 0 11 and the critical success index 0 88 0 89 the results from the pm scheme return slightly higher values for hit rate and false alarm ratio indicating a larger flood extent is predicted the critical success indices calculated for the results predicted by pm and pm 2nd on meshes of different resolutions are also consistent with pm 2nd results on δx 20 m being slightly better in comparison with the pm simulation results the pm 2nd predictions consistently return high hit rates and critical success indices but low false alarm ratios across all of the simulations this indicates that the predictions from the two schemes are consistent and it is difficult to justify the absolute benefit of using a 2nd order scheme in this field scale application due to the high uncertainty associated with the surveyed flood extent further conclusions can only be drawn through more thorough systematic analysis when high quality observation data are available for comparison the simulations are run on a gpu server with 2 nvidia tesla v100 and the runtimes for reproducing the 120 h flood event on different resolutions are listed in table 5 the pm 2nd based model is approximately 2 2 2 8 times more expensive than the pm model for this test case consistent with the results as shown in table 2 7 conclusions to address the major technical challenges in the simulation of overland flow and surface water flooding processes over complex terrains novel variable reconstruction and discretisation schemes are developed in the context of a godunov type finite volume swe model for accurate and stable calculation of fluxes slope and friction source terms in comparison with alternative numerical schemes a detailed discussion has been provided to explain how the new schemes address the numerical challenges by avoiding the creation of spurious dry bed dam break cases more properly estimating slope source terms and reproducing the correct equilibrium stage of shallow water flow characterised by the balance between gravity and friction effects the new schemes both in first pm and second order pm 2nd accuracy are subsequently implemented to develop hydrodynamic models tested for the simulation of theoretical and real world test cases involving overland flow and surface water flooding processes from the simulations results some conclusions may be drawn the hr cn scheme cannot properly handle the partially wet case to effectively avoid creation of spurious dry bed dam breaks it is not recommended for an application involving predominant overland flow process featured with shallow depth whilst srm can be effectively implemented to prevent the numerical dam break issue it may provide an inaccurate estimation of bed slope source terms under certain circumstances and should be used with care for applications involving abruptly changing topography the corrected variable reconstruction scheme pm proposed in this work better addresses the numerical challenges as discussed in this work and the resulting model has been successfully validated against the considered test cases producing results compared more favourably with reference solutions than the alternative hr cn and srm schemes the corrected second order scheme pm 2nd is shown to predict more accurate results than the other first order schemes for the theoretical test cases however the second order scheme requires more than two times of runtime for the test cases considered in this work whilst the second order model seems to provide better predictions at coarse resolutions its benefit for a large scale real world simulation still cannot be clearly justified the proposed friction discretisation scheme corrects a subtle technical flaw of the original scheme leading to improved simulation accuracy and simplified implementation the new variable reconstruction and source term discretisation schemes at both first and second order accuracy have been shown to improve model performance in simulating catchment scale full process flooding process including overland flow although uniform grids are used in all of the test cases in this work the numerical methods are independent of mesh type and can be directly implemented on other types of grids e g triangular grids credit authorship contribution statement jiaheng zhao conceptualization methodology software formal analysis visualization writing original draft qiuhua liang conceptualization methodology writing review editing supervision funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is funded by ukri through research projects river basins as living laboratories project ne s012427 1 valbgi project ne s00288x 1 living deltas hub ne s008926 1 
169,the identification of reactive hotspots in shallow porous aquifers is challenging and normally requires extensive geochemical data collection combined with computationally intensive reactive transport modeling in this study we propose an alternative practical approach combining conservative and reactive tracers using a tomographic setup to localize such zones tracer tomography combines multiple tracer tests to capture and reconstruct aquifer heterogeneities these methods generally use ideal tracers that are dominantly transported by conservative advection to provide a hydraulically accurate aquifer reconstruction we compare these conservative tracer tomography results to those obtained by using reactive tracers in a similar tomographic setup assuming that the conservative tracer is ideal the differences between the two tracers will be mainly related to reactive transport in natural systems the biogeochemical reactions happen in localized zones these will be the parts of the reconstructions with the greatest differences we present the viability of the approach in a simple 2 d example and on an aquifer analog model based complex example with extensive sensitivity analysis we test the effectiveness of the method under different reaction intensities to define the chemical requirements for an ideal reactive tracer for such investigations keywords tracer tomography reactive transport aquifer characterization redox processes 1 introduction the mobility and degradation of many pollutants are governed by redox reactions redox reactions depend on a remarkably large number of parameters e g concentrations of electron donors and acceptors ph specific surface area of mineral phases concentration and activity of microorganisms appelo and postma 2004 knabe et al 2018a these biogeochemical parameters together with hydrodynamic parameters hydraulic conductivity dispersivity porosity can be highly variable in space and time this heterogeneous distribution can lead to the presence of localized redox hotspots for example briggs et al 2018 found in sand gravel sediments below a small lake localized nitrate reducing conditions in lower permeable regions under still oxic conditions in the bulk aquifer due to the depletion of an electron donor or acceptor over time or changes in groundwater flow patterns redox zones and hotspots can move in time and space dwivedi et al 2018a hydrogeochemical analysis of groundwater samples is the principal tool to identify redox zones indicated by the presence or absence of associated redox sensitive species like o2 no3 and fe2 along a flow path but due to localized redox hot spots complex groundwater flow patterns different contaminant sources and secondary geochemical reactions like mineral dissolution and precipitation further methods might be necessary to adequately identify zones of redox activity additional isotopic analysis can improve the assessment of redox activity as redox reactions tend to enrich heavy isotopes in oxidized species causing a corresponding enrichment of light isotopes in reduced species wiederhold 2015 for example pittalis et al 2018 used o n and s isotopes to identify different nitrate sources and the occurrence of denitrification in a coastal aquifer system choi et al 2016 employed s isotopes to identify redox processes in overlapping iron and sulfate reducing zones in a fluvial aquifer numerical simulation of reactive transport processes including redox reactions is a well known method to estimate subsurface parameters which cannot be measured and to predict the future development of the system numerous studies exist which model redox reactions with varying complexity using different numerical codes for example trauth et al 2015 simulated the aerobic respiration and denitrification in an in stream gravel bar using min3p dwivedi et al 2018a modeled redox reactions up until sulfate reduction with pflotran for a 2d transect of a river meander knabe et al 2018a used hp1 to simulate nitrate reduction coupled to organic carbon and pyrite oxidation including growth and transport of mediating microorganisms in a column experiment fakhreddine et al 2016 presented a different approach to identify biogeochemical heterogeneities they used the reactive transport of arsenic in a tomographic setup to localize aquifer zones with high pyrite content they concluded that geochemical tomography is a viable approach to characterize the chemical subsurface properties their method however relied on cost intensive numerical modeling of reactive transport processes modern tomographic methods of hydrogeology are capable of capturing aquifer heterogeneities cirpka et al 2014 hydraulic tomography employs a series of pumping or slug tests with point sources with an array of pressure sensors scattered in the investigated domain hydraulic tomography is capable to reconstruct hydraulic conductivity and storativity distributions of aquifers with high resolution and low uncertainty berg and illman 2011 brauchler et al 2003 2011 jiménez et al 2013 liu et al 2007 vasco et al 2000 yeh and liu 2000 zhu and yeh 2005 tracer tomography uses a similar concept with the propagation of tracer substances as tomographic signals tracer tomography has the same imaging potential for hydraulic conductivity and could be used to identify the preferential transport pathways in aquifers but practical limitations make this method less favorable datta gupta et al 2002 jiménez et al 2016 vasco and datta gupta 1999 zhu et al 2009 repeated tracer injections are more difficult to implement and continuous sampling in multiple locations could result in hundreds of water samples being analyzed nevertheless successful applications have been done using different types of tracers such as traditional dye tracers jiménez et al 2016 dna labeled nanotracers kong et al 2018 or the application of thermal tracers somogyvári and bayer 2017 tracer tomography with fundamentally different tracers could provide information of tracer specific transport pathways and could provide different hydraulic conductivity reconstructions for example when comparing conservative solute tracers with thermal tracers somogyvári et al 2016 somogyvári and bayer 2017 thermal tracer tomography uses heat as a tracer which has non conservative transport properties somogyvári et al 2016 showed that in shallow alluvial groundwater systems when the dominant heat transport process is advection thermal tracer tomography is capable of similar performance as conservative tracers their scenario analysis showed that the effect of diffusive transport appears in the form of artifacts on the aquifer reconstructions where diffusion processes are dominant using a similar approach tracer tomography with reactive tracers could provide specific insight into heterogeneities of biogeochemical processes and could reveal the differences between conservative and reactive transport in this study we propose a novel methodology with the simultaneous use of conservative and reactive tracers in a tomographic setup our working concept is that the differences between the observed tracer breakthrough curves are the result of the biogeochemical processes that only affect the reactive tracer therefore the different tracers will image the aquifer heterogeneity from two different perspectives from a conservative and a reactive transport point of view hence we will obtain two different reconstructions of the effective hydraulic conductivities and the preferential transport pathways the comparison of these reconstructions will highlight reactive zones and pathways thus will help to localize redox hotspots we apply a travel time based tomographic inversion approach that avoids the use of complex numerical models and relies on mathematical proxies 2 methodology 2 1 tomographic inversion tracer tomography uses a series of tracer tests from point injection sources the tracer tests are performed independently from each other either with different substances or subsequently with sufficient pauses to avoid any overlapping of the tracer plumes tracer concentrations are measured at multiple observation points receivers to obtain breakthrough curves for multiple source receiver combinations an example of the tomographic setup is shown in fig 1 if the source receiver combinations are well distributed over the domain the obtained set of breakthrough curves are containing information of the transport properties of the domain within this information can be used to reconstruct the distribution of the dominant transport property this is a similar approach to seismic tomography where the first arrivals of seismic waves are used to infer the seismic wave velocity distribution of the subsurface assuming that the dominant transport process is advection when pe 1 the hydraulic conductivity k distribution of the aquifer will be responsible for the fastest tracer transport pathways therefore it is possible to use the tracer breakthrough times to estimate the k distribution this is the concept of the travel time based tomographic inversion the breakthrough time of a tracer between a source and a receiver can be written with the following line integral vasco and datta gupta 1999 1 t x r x s x r d s v s where xs is the location of the tracer injection source xr is the location of the observation sampling point receiver t xr is the breakthrough time at the receiver and v s is the mean tracer velocity along the pathway s this equation is also known as the eikonal equation and it is the basis of most tomographic inversion methods thus standard methods are available to infer the velocity distribution after this formula such as the algebraic reconstruction technique gilbert 1972 the tomographic inversion estimates the velocity distribution over a pre defined grid from the travel times of the different source receiver combinations the local mean tracer velocities can be expressed with the darcy velocity for an ideal advective tracer hence the eikonal formula can be extended as 2 t x r x s x r d s v s x s x r ϕ s d s k s i s r s here k s is the hydraulic conductivity i s is the hydraulic gradient r s is the retardation factor and φ s is the porosity distribution along a transport pathway the retardation factor is a correction term for non ideal tracers where the tracer is transported with a delay e g thermal retardation for heat tracers shook 1999 from these four parameters the value range for hydraulic conductivities spans orders of magnitudes while the range of the other three is significantly smaller therefore the variations in mean tracer velocity are mainly formed by the hydraulic conductivities because of this difference hydraulic gradient porosity and retardation factor could be considered as a constant along a transport pathway for the interpretation somogyvári et al 2016 note that this assumption is only valid for porous media as the hydraulic gradient could show strong local variations in fractured systems if these values can be estimated from other measurements the hydraulic conductivities can be directly calculated from the velocities but even without knowing these values the aquifer heterogeneities can be reconstructed this reconstruction is called the hydraulic conductivity tomogram note that the obtained hydraulic conductivity tomogram is not necessarily the true hydraulic conductivity distribution it is an apparent hydraulic conductivity distribution from the aspect of the used tracer whether it is a conservative or reactive solute nanoparticles or heat the obtained reconstruction will always be related to the tracer choice using a conservative and a reactive tracer will result in two different apparent hydraulic conductivity tomograms where the differences will be caused by chemical reactions affecting only the reactive tracer this is quantified in the line integrals for the two different tracer types which then will read as 3 t c o n s x r x s x r d s v c o n s s x s x r ϕ d s k c o n s s i r 4 t r e a c t x r x s x r d s v r e a c t s x s x r ϕ d s k r e a c t s i r assuming the only difference in transport processes for the two tracers are the chemical reactions which affect only the reactive not the conservative tracer the differences in the breakthrough time could then be related to the chemical processes the chemical reactions could slow down the propagation of the reactive tracer by getting into interaction with the reactive zone of the aquifer this leads to a difference in the apparent mean tracer propagation velocity which can be exploited by our proposed methodology this difference is denoted in the two tracer velocity terms vcons and vreact which is then translated into the two apparent hydraulic conductivities kcons and kreact the differences in the reconstructions will then read as 5 v c o n s x y z v r e a c t x y z k c o n s x y z k r e a c t x y z i r ϕ the product of this subtraction is an indicator of the spatial differences between conservative and reactive transport 6 h x y z k r e a c t x y z k c o n s x y z as both apparent hydraulic conductivity reconstructions are raster images on predefined grids by choosing these grids to be equal the hot spot identification is a simple matrix subtraction reactive hotspots will show as nonzero anomalies on the resulting map the complete workflow for reactive hot spot detection is shown in fig 2 when applying tomographic algorithms developed for seismic tomography in a hydrogeological context some additional problems need to be addressed tracer tomography always uses a significantly smaller number of source receiver combinations making the inverse problem ill posed to mitigate this problem the staggered grid method has been successfully used in multiple studies brauchler et al 2003 somogyvári et al 2016 vesnaver and böhm 2000 instead of solving the eikonal problem on a high resolution grid the staggered grid method employs a series of reconstructions on multiple coarser grids with shifted positions first a coarse grid is defined with n times larger cells than the desired final grid the coarse grid is shifted n times with the length of the desired final grid cells in all spatial directions x y in 2 d case and x y z in 3 d case the tomographic inversion is then run on all of these grids finally the inverted grids are averaged over the desired fine grid to obtain the final inversion result this method not only increases the nominal resolution of the reconstruction by keeping the inversion stable but also reduces the effect of the relative position of the grid and source receiver locations to characterize the spatial uncertainty of the reconstruction we calculate the null space energy map parts of the tomogram that are reached by more transport trajectories will be more reliable than the parts that were avoided this can be quantified by the singular value decomposition of the tomographic matrix this is a standard method for the uncertainty quantification of tomographic inversion hu et al 2015 jiménez et al 2013 the null space energy map can be used to mask out the unreliable parts of the aquifer reconstruction typically low k areas near the sides of the domain 2 2 transport modeling the simulated experiments of this study were done using pflotran a reactive transport model code for variably saturated conditions lichtner et al 2013 pflotran has been specifically designed to run on multiple processors and has demonstrated excellent scaling performance for simulating flow and transport mills et al 2007 pflotran has already been applied to field scale reactive transport problems such as transport and distribution of nitrogen within the rifle floodplain in colorado and co2 sequestration in deep geologic formations dwivedi et al 2018b knabe et al 2021 lu and lichtner 2007 to test the comparative tomography concept first a simple 2 d model is set up as shown in fig 3 this 2 d setup represents a porous sand gravel aquifer with a low k sand lens embedded in the center beside this the aquifer is homogeneous and isotropic with k 2 10 3 ms 1 the sand lens has a hydraulic conductivity of k 2 10 4 ms 1 below and above the aquifer two low k layers are introduced to hydraulically limit the system with k 4 10 6 ms 1 the hydraulic boundary conditions are defined at the two sides of the model to provide an average hydraulic gradient of 0 005 m m the model domain is 10 m 6 m using 0 1 m 0 1 m cells for each injection a conservative tracer and a reactive tracer are simulated the reactive tracer is assumed to be organic carbon for simplicity ch2o which is affected by a single microbiologically amended redox reaction 7 c h 2 o 4 5 n o 3 b i o m a s s h c o 3 2 5 n 2 2 5 h 2 o 1 5 h in this example for simplicity it is assumed that the concentrations of the other reactants are not affecting the reaction rate the rate is calculated using the typical monod style kinetics knabe et al 2018b 8 r k s b i o c c h 2 o c c h 2 o k c h 2 o where r is the oxidation rate of organic carbon mol l 1s 1 k is the rate constant mol l 1s 1 m3 bulk mol biomass 1 sbio is the biomass concentration mol biomass m3 bulk 1 c c h 2 o is the concentration of organic carbon mol l 1 and k c h 2 o is the monod half saturation constant of organic carbon mol l 1 the reaction hotspot shown in fig 3b is assumed to be caused by differences in the biomass concentration sbio which is set to 1 mol biomass m3 bulk 1 in the hotspot region and 0 001 mol biomass m3 bulk 1 in the bulk aquifer the rate constant was set to k 1 6 10 7 m o l l s m b u l k 3 m o l b i o m a s s and the monod half saturation constant to k c h 2 o 10 4 m o l l the aquifer is intersected on the two sides by two wells 8 5 m apart as shown in fig 3 the well on the right side is used for injections with injection points at 5 different levels with 1 m spacing the observation points are located at the same levels in the observation well on the right side the injections are simulated as independent experiments tracers are injected over 10 min with 200 liters of water volume the concentration of the injection is 0 1 mol l for both the conservative and reactive tracer the numerical simulations are run for 14 days this setup results in a total of 25 breakthrough curves covering the whole area of the aquifer to test the method under more realistic conditions we have prepared a 3 d pflotran model using the descalvado aquifer analog dataset höyng et al 2014 this is a high resolution digital recreation of a small block of the guarani aquifer system which was mapped and analyzed the aquifer is composed of fluvial aeolian sediments mainly sand gravel and some fine sediments this is a multi layer aquifer with strong hydraulic contrast between the different facies the aquifer geometry and hydraulic conductivity distribution are shown in fig 4 the dataset provides the hydraulic and thermal properties of the different facies but no chemical analysis was done therefore we have chosen one of the facies to represent the reactive hotspot in nature the biogeochemically active reaction hotspots are typically formed in low k parts of shallow aquifers these parts however usually avoided by the main flowpaths so to have a visible effect we have selected the layer marked by the green color in fig 4 this layer could represent a finer sand formation on top of a gravel sand high k layer shown by the red color similarly to the 2 d case constant head hydraulic boundary conditions are set at the left and right sides of the aquifer to provide an average hydraulic gradient of 0 005 m m the model domain is setup by 0 1 m 0 1 m 0 1 m cells to capture the 3 d aquifer structures properly in this example we are not limited to the tomographic setup to a 2 d profile but we used 5 observation wells along a perpendicular plane each with 5 observation points this allows for a 3 d reconstruction within a prism shaped subdomain the simulations were run for 40 days which was enough to capture the major parts of the breakthrough curves the total number of breakthrough curves in this case was 125 for each tomographic experiment 3 results and discussion 3 1 2 d case the inversion domain for the 2 d example is a 10 m 5 m area split up into 1 25 m 1 m cells for the inversion this coarse grid is then shifted vertically and horizontally 3 3 times by the staggering algorithm resulting in a final refined grid with 1 25 m 1 m grid cells hence the inverse problem is solved 16 times over 16 different grids in total because of the computationally very effective fast marching algorithm the full workflow took around one minute to complete on a regular pc the results of the 2 d example are shown in fig 5 the reconstruction based on the conservative tracer tomography provides a good match with the reference model fig 5c in the center of the profile the low k lens is very well visible with strong contrast in k the reconstructed k values of the main aquifer body are close to the original values on the top and bottom borders of the profile some low k pixels suggest the presence of aquiclude layers the reconstruction however is far from perfect the main aquifer body which is homogeneous in the original model is reconstructed with small internal variations by the inversion also the low k lens has a larger extent in the tomogram and its shape is very rounded the reconstructed k values here are much higher 1 3 10 3 ms 1 than the original values 1 3 10 4 ms 1 these are known limitations from the methodology similar artifacts were observed by multiple studies using similar methodologies kong et al 2018 somogyvári and bayer 2017 it is important to state that the reconstruction is based on the information along the transport pathways paths that are mainly in the higher conductivity aquifer zones this results that the reconstruction of the low k aquifer parts will be always less reliable compared to high resolution tomography applications tracer tomography will always provide more rounded geometries with smoother contrasts because of the limited number of source receiver combinations near the model boundaries the reconstruction algorithm often over or underestimate the k values because here the grid cells are not covered by that many transport pathways so the inversion is less stable in the presented case this is most notable near the injection well this is where the highest k values are shown the reactive tracer tomogram shows a very similar picture by the first look but with higher k values in the center the differences between the two tomograms are better visible when they are specifically plotted in fig 5e in this figure the reactive hot spot is well visible in the center of the figure as a zone where the reactive transport appears faster than the conservative this is because the obtained breakthrough curves are formed as a combination of multiple flow paths not just the fastest ones in the case of the reactive tracer for all flowpaths that travel through the reactive zone the mass of the tracer gets strongly reduced therefore the breakthrough curve of the reactive tracer is primarily composed of the fast flow paths through the less reactive zone which results in an earlier but smaller peak of the breakthrough curve compared to the conservative tracer this results in a higher effective hydraulic conductivity for the reactive tracer the reconstruction however does not show a single hotspot in the center but rather two loosely connected ones this can be explained by the local hydraulic heterogeneity which makes the main flowpaths avoid the low k sand lens because of this the tracer could only interact with the reactive zones at the two ends of the sand where the distortion in the flow field is still minimal 3 2 3 d case the 3 d inversion domain is defined as a box with sides 20 m 6 m 6 m with a grid size of 1 m 1 m 1 m similarly to the 2 d case 3 times staggering was used but in this case in 3 dimensions the inverse problem is solved 64 times and the final grid resolution is 0 25 m 0 25 m 0 25 m the results of the aquifer analog based 3 d reconstruction are shown in fig 6 compared to the reference model in fig 6c the reconstruction of the conservative tracer tomography in fig 6a shows a lot of similarities the low k layers on the top of the model are captured although their internal heterogeneities are not visible the central high k layer where the majority of the transport takes place is well reconstructed as a continuous layer similarly to the 2 d case the reconstruction shows some instability at the sides resulting in very high k values near the source and receiver points in the central zone also here a strong variation in the reconstructed k values is visible which can be explained as the distortion effect of the embedded smaller low k lenses near the center of the aquifer the central zone is very well distinguished from the bottom low k layer which is only partially reconstructed the central layer acts as a shortcut between the lowest source and receiver points so no inverted transport pathways were modeled here this resulted in a very low reliability reconstruction which got removed after the null space energy map calculations interestingly this effect does not appear in the top part of the model because there the hydraulic conductivity contrast is smaller with the layer below the overall appearance of the conservative tracer tomography reconstruction is very similar to the results of somogyvári et al 2016 where the same aquifer analog was used for a synthetic thermal tracer tomography experiment the reconstruction based on the reactive tracer in fig 6b shows some visible differences the low k zones are better reconstructed with much less volume masked out from the bottom layer in general the two tomograms look very similar again and can be interpreted similarly the plotted difference between the two tomograms in fig 6d highlights an area close to the actual reactive hotspots shown in fig 6c with higher effective k values for the reactive tracer the reconstructed reactive zone sits on the top of the high k central layer similarly to the reference model even some of its topology is visible on the reconstruction the large thickness on the left side and the thinning in the center based on these results the reactive hotspot could be well isolated further investigations showed that the redox zone is still captured if we reduce the number of observation wells to 3 or even to 1 although then the reconstruction will be limited to a profile 3 3 sensitivity analysis the most crucial factor regarding the success of the comparative tracer tomography approach is the choice of the reactive tracer to further investigate this we have conducted a series of simulations with different reaction rates in the redox hotspots to represent different tracers with different reactivity the sensitivity analysis was performed using the 2 d model example fig 2 all hydraulic parameters were kept the same only the reaction rate constant was modified using a multiplication factor f 9 k f k 0 where k 0 is the basis rate constant defined as 10 k 0 1 6 10 8 m o l l s m b u l k 3 m o l b i o m a s s the comparative tomography experiment was repeated using different multiplication factor values and the simulated datasets were interpreted using the tomographic inversion workflow fig 7 shows the reconstructed redox hotspots there is a clear relationship between the rate constant and the quality of the hotspot reconstruction at the lowest tested reaction rate in fig 7b the hotspot is barely visible it does not stand out of the surrounding aquifer area the background variability of the reconstructions shows similar amplitudes making it very difficult to localize the exact reactive zone in the latter examples as the reaction rates becoming more intensive the differences between conservative and reactive transport are becoming more prominent and these differences are highlighted on the hotspot reconstructions at the factor of 5 the hotspot is already well visible and is easy to localize higher reaction rates do not change the reconstructions too much the anomaly at the hotspot is getting stronger and its boundaries sharper in general we can say that larger reaction rates are improving the reconstruction quality and desired for a successful application while in the case of a field experiment many factors influencing the reaction rate are a fixed property of the subsurface and not controllable by choosing a different reactive tracer with higher reactivity we can still increase the visibility of the reaction hotspots some tracers could get into more intensive reactions with the hotspot and would differ more from conservative transport providing similar differences as shown in this sensitivity analysis this is a very important property to consider when planning the practical application of the proposed methodology where the tracer choice could be adjusted to localize the reactive zones by choosing a tracer that is specifically reactive to the investigated hot spots the reconstruction quality could be more accurate and the redox zones could be better localized 4 conclusions in this study we have shown that the combined use of conservative and reactive tracers for tracer tomography could reveal the location of reactive hotspots in shallow groundwater systems comparative tomography was capable to identify the main redox hotspots in multiple examples because the inversion of the conservative and the reactive tomography can be done using similar inversion grids the two methods are directly comparable the difference maps shown in the paper provide an intuitive look into the possible distribution of reaction hotspots in the aquifer the inversion algorithms are robust and computationally very effective and due to the extensive literature they are easy to implement compared to other existing methodologies our approach does not require any complex numerical modeling and can provide results very rapidly while conservative tracer tomography is state of the art reactive tracer tomography even reactive tracer testing is an area not very well explored because of this the choice of a reactive tracer is not straightforward reactive tracers need to be specifically chosen for the investigated problem for redox hotspot identification a good example could be the resazurin tracer which changes into resorufin and changes its color due to microbial activity knapp et al 2018 in practice the conservative and the reactive tracer tomography experiment can be performed simultaneously if the two substances do not affect each other or each other s detectability one of the main limitations of the proposed methodology originates from the resolution of the tomography in practice it is not feasible to use significantly more injections and observations than shown in this study a solution to overcome this challenge would be to combine this methodology with other aquifer characterization techniques like hydraulic tomography electric resistivity tomography or ground penetrating radar these methods could provide a high resolution aquifer reconstruction which could be used as a basis for the interpretation of the comparative tracer experiments credit authorship contribution statement márk somogyvári conceptualization methodology supervision visualization writing original draft dustin knabe conceptualization methodology investigation software irina engelhardt conceptualization methodology supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors we thank two anonymous reviewers for their constructive comments 
169,the identification of reactive hotspots in shallow porous aquifers is challenging and normally requires extensive geochemical data collection combined with computationally intensive reactive transport modeling in this study we propose an alternative practical approach combining conservative and reactive tracers using a tomographic setup to localize such zones tracer tomography combines multiple tracer tests to capture and reconstruct aquifer heterogeneities these methods generally use ideal tracers that are dominantly transported by conservative advection to provide a hydraulically accurate aquifer reconstruction we compare these conservative tracer tomography results to those obtained by using reactive tracers in a similar tomographic setup assuming that the conservative tracer is ideal the differences between the two tracers will be mainly related to reactive transport in natural systems the biogeochemical reactions happen in localized zones these will be the parts of the reconstructions with the greatest differences we present the viability of the approach in a simple 2 d example and on an aquifer analog model based complex example with extensive sensitivity analysis we test the effectiveness of the method under different reaction intensities to define the chemical requirements for an ideal reactive tracer for such investigations keywords tracer tomography reactive transport aquifer characterization redox processes 1 introduction the mobility and degradation of many pollutants are governed by redox reactions redox reactions depend on a remarkably large number of parameters e g concentrations of electron donors and acceptors ph specific surface area of mineral phases concentration and activity of microorganisms appelo and postma 2004 knabe et al 2018a these biogeochemical parameters together with hydrodynamic parameters hydraulic conductivity dispersivity porosity can be highly variable in space and time this heterogeneous distribution can lead to the presence of localized redox hotspots for example briggs et al 2018 found in sand gravel sediments below a small lake localized nitrate reducing conditions in lower permeable regions under still oxic conditions in the bulk aquifer due to the depletion of an electron donor or acceptor over time or changes in groundwater flow patterns redox zones and hotspots can move in time and space dwivedi et al 2018a hydrogeochemical analysis of groundwater samples is the principal tool to identify redox zones indicated by the presence or absence of associated redox sensitive species like o2 no3 and fe2 along a flow path but due to localized redox hot spots complex groundwater flow patterns different contaminant sources and secondary geochemical reactions like mineral dissolution and precipitation further methods might be necessary to adequately identify zones of redox activity additional isotopic analysis can improve the assessment of redox activity as redox reactions tend to enrich heavy isotopes in oxidized species causing a corresponding enrichment of light isotopes in reduced species wiederhold 2015 for example pittalis et al 2018 used o n and s isotopes to identify different nitrate sources and the occurrence of denitrification in a coastal aquifer system choi et al 2016 employed s isotopes to identify redox processes in overlapping iron and sulfate reducing zones in a fluvial aquifer numerical simulation of reactive transport processes including redox reactions is a well known method to estimate subsurface parameters which cannot be measured and to predict the future development of the system numerous studies exist which model redox reactions with varying complexity using different numerical codes for example trauth et al 2015 simulated the aerobic respiration and denitrification in an in stream gravel bar using min3p dwivedi et al 2018a modeled redox reactions up until sulfate reduction with pflotran for a 2d transect of a river meander knabe et al 2018a used hp1 to simulate nitrate reduction coupled to organic carbon and pyrite oxidation including growth and transport of mediating microorganisms in a column experiment fakhreddine et al 2016 presented a different approach to identify biogeochemical heterogeneities they used the reactive transport of arsenic in a tomographic setup to localize aquifer zones with high pyrite content they concluded that geochemical tomography is a viable approach to characterize the chemical subsurface properties their method however relied on cost intensive numerical modeling of reactive transport processes modern tomographic methods of hydrogeology are capable of capturing aquifer heterogeneities cirpka et al 2014 hydraulic tomography employs a series of pumping or slug tests with point sources with an array of pressure sensors scattered in the investigated domain hydraulic tomography is capable to reconstruct hydraulic conductivity and storativity distributions of aquifers with high resolution and low uncertainty berg and illman 2011 brauchler et al 2003 2011 jiménez et al 2013 liu et al 2007 vasco et al 2000 yeh and liu 2000 zhu and yeh 2005 tracer tomography uses a similar concept with the propagation of tracer substances as tomographic signals tracer tomography has the same imaging potential for hydraulic conductivity and could be used to identify the preferential transport pathways in aquifers but practical limitations make this method less favorable datta gupta et al 2002 jiménez et al 2016 vasco and datta gupta 1999 zhu et al 2009 repeated tracer injections are more difficult to implement and continuous sampling in multiple locations could result in hundreds of water samples being analyzed nevertheless successful applications have been done using different types of tracers such as traditional dye tracers jiménez et al 2016 dna labeled nanotracers kong et al 2018 or the application of thermal tracers somogyvári and bayer 2017 tracer tomography with fundamentally different tracers could provide information of tracer specific transport pathways and could provide different hydraulic conductivity reconstructions for example when comparing conservative solute tracers with thermal tracers somogyvári et al 2016 somogyvári and bayer 2017 thermal tracer tomography uses heat as a tracer which has non conservative transport properties somogyvári et al 2016 showed that in shallow alluvial groundwater systems when the dominant heat transport process is advection thermal tracer tomography is capable of similar performance as conservative tracers their scenario analysis showed that the effect of diffusive transport appears in the form of artifacts on the aquifer reconstructions where diffusion processes are dominant using a similar approach tracer tomography with reactive tracers could provide specific insight into heterogeneities of biogeochemical processes and could reveal the differences between conservative and reactive transport in this study we propose a novel methodology with the simultaneous use of conservative and reactive tracers in a tomographic setup our working concept is that the differences between the observed tracer breakthrough curves are the result of the biogeochemical processes that only affect the reactive tracer therefore the different tracers will image the aquifer heterogeneity from two different perspectives from a conservative and a reactive transport point of view hence we will obtain two different reconstructions of the effective hydraulic conductivities and the preferential transport pathways the comparison of these reconstructions will highlight reactive zones and pathways thus will help to localize redox hotspots we apply a travel time based tomographic inversion approach that avoids the use of complex numerical models and relies on mathematical proxies 2 methodology 2 1 tomographic inversion tracer tomography uses a series of tracer tests from point injection sources the tracer tests are performed independently from each other either with different substances or subsequently with sufficient pauses to avoid any overlapping of the tracer plumes tracer concentrations are measured at multiple observation points receivers to obtain breakthrough curves for multiple source receiver combinations an example of the tomographic setup is shown in fig 1 if the source receiver combinations are well distributed over the domain the obtained set of breakthrough curves are containing information of the transport properties of the domain within this information can be used to reconstruct the distribution of the dominant transport property this is a similar approach to seismic tomography where the first arrivals of seismic waves are used to infer the seismic wave velocity distribution of the subsurface assuming that the dominant transport process is advection when pe 1 the hydraulic conductivity k distribution of the aquifer will be responsible for the fastest tracer transport pathways therefore it is possible to use the tracer breakthrough times to estimate the k distribution this is the concept of the travel time based tomographic inversion the breakthrough time of a tracer between a source and a receiver can be written with the following line integral vasco and datta gupta 1999 1 t x r x s x r d s v s where xs is the location of the tracer injection source xr is the location of the observation sampling point receiver t xr is the breakthrough time at the receiver and v s is the mean tracer velocity along the pathway s this equation is also known as the eikonal equation and it is the basis of most tomographic inversion methods thus standard methods are available to infer the velocity distribution after this formula such as the algebraic reconstruction technique gilbert 1972 the tomographic inversion estimates the velocity distribution over a pre defined grid from the travel times of the different source receiver combinations the local mean tracer velocities can be expressed with the darcy velocity for an ideal advective tracer hence the eikonal formula can be extended as 2 t x r x s x r d s v s x s x r ϕ s d s k s i s r s here k s is the hydraulic conductivity i s is the hydraulic gradient r s is the retardation factor and φ s is the porosity distribution along a transport pathway the retardation factor is a correction term for non ideal tracers where the tracer is transported with a delay e g thermal retardation for heat tracers shook 1999 from these four parameters the value range for hydraulic conductivities spans orders of magnitudes while the range of the other three is significantly smaller therefore the variations in mean tracer velocity are mainly formed by the hydraulic conductivities because of this difference hydraulic gradient porosity and retardation factor could be considered as a constant along a transport pathway for the interpretation somogyvári et al 2016 note that this assumption is only valid for porous media as the hydraulic gradient could show strong local variations in fractured systems if these values can be estimated from other measurements the hydraulic conductivities can be directly calculated from the velocities but even without knowing these values the aquifer heterogeneities can be reconstructed this reconstruction is called the hydraulic conductivity tomogram note that the obtained hydraulic conductivity tomogram is not necessarily the true hydraulic conductivity distribution it is an apparent hydraulic conductivity distribution from the aspect of the used tracer whether it is a conservative or reactive solute nanoparticles or heat the obtained reconstruction will always be related to the tracer choice using a conservative and a reactive tracer will result in two different apparent hydraulic conductivity tomograms where the differences will be caused by chemical reactions affecting only the reactive tracer this is quantified in the line integrals for the two different tracer types which then will read as 3 t c o n s x r x s x r d s v c o n s s x s x r ϕ d s k c o n s s i r 4 t r e a c t x r x s x r d s v r e a c t s x s x r ϕ d s k r e a c t s i r assuming the only difference in transport processes for the two tracers are the chemical reactions which affect only the reactive not the conservative tracer the differences in the breakthrough time could then be related to the chemical processes the chemical reactions could slow down the propagation of the reactive tracer by getting into interaction with the reactive zone of the aquifer this leads to a difference in the apparent mean tracer propagation velocity which can be exploited by our proposed methodology this difference is denoted in the two tracer velocity terms vcons and vreact which is then translated into the two apparent hydraulic conductivities kcons and kreact the differences in the reconstructions will then read as 5 v c o n s x y z v r e a c t x y z k c o n s x y z k r e a c t x y z i r ϕ the product of this subtraction is an indicator of the spatial differences between conservative and reactive transport 6 h x y z k r e a c t x y z k c o n s x y z as both apparent hydraulic conductivity reconstructions are raster images on predefined grids by choosing these grids to be equal the hot spot identification is a simple matrix subtraction reactive hotspots will show as nonzero anomalies on the resulting map the complete workflow for reactive hot spot detection is shown in fig 2 when applying tomographic algorithms developed for seismic tomography in a hydrogeological context some additional problems need to be addressed tracer tomography always uses a significantly smaller number of source receiver combinations making the inverse problem ill posed to mitigate this problem the staggered grid method has been successfully used in multiple studies brauchler et al 2003 somogyvári et al 2016 vesnaver and böhm 2000 instead of solving the eikonal problem on a high resolution grid the staggered grid method employs a series of reconstructions on multiple coarser grids with shifted positions first a coarse grid is defined with n times larger cells than the desired final grid the coarse grid is shifted n times with the length of the desired final grid cells in all spatial directions x y in 2 d case and x y z in 3 d case the tomographic inversion is then run on all of these grids finally the inverted grids are averaged over the desired fine grid to obtain the final inversion result this method not only increases the nominal resolution of the reconstruction by keeping the inversion stable but also reduces the effect of the relative position of the grid and source receiver locations to characterize the spatial uncertainty of the reconstruction we calculate the null space energy map parts of the tomogram that are reached by more transport trajectories will be more reliable than the parts that were avoided this can be quantified by the singular value decomposition of the tomographic matrix this is a standard method for the uncertainty quantification of tomographic inversion hu et al 2015 jiménez et al 2013 the null space energy map can be used to mask out the unreliable parts of the aquifer reconstruction typically low k areas near the sides of the domain 2 2 transport modeling the simulated experiments of this study were done using pflotran a reactive transport model code for variably saturated conditions lichtner et al 2013 pflotran has been specifically designed to run on multiple processors and has demonstrated excellent scaling performance for simulating flow and transport mills et al 2007 pflotran has already been applied to field scale reactive transport problems such as transport and distribution of nitrogen within the rifle floodplain in colorado and co2 sequestration in deep geologic formations dwivedi et al 2018b knabe et al 2021 lu and lichtner 2007 to test the comparative tomography concept first a simple 2 d model is set up as shown in fig 3 this 2 d setup represents a porous sand gravel aquifer with a low k sand lens embedded in the center beside this the aquifer is homogeneous and isotropic with k 2 10 3 ms 1 the sand lens has a hydraulic conductivity of k 2 10 4 ms 1 below and above the aquifer two low k layers are introduced to hydraulically limit the system with k 4 10 6 ms 1 the hydraulic boundary conditions are defined at the two sides of the model to provide an average hydraulic gradient of 0 005 m m the model domain is 10 m 6 m using 0 1 m 0 1 m cells for each injection a conservative tracer and a reactive tracer are simulated the reactive tracer is assumed to be organic carbon for simplicity ch2o which is affected by a single microbiologically amended redox reaction 7 c h 2 o 4 5 n o 3 b i o m a s s h c o 3 2 5 n 2 2 5 h 2 o 1 5 h in this example for simplicity it is assumed that the concentrations of the other reactants are not affecting the reaction rate the rate is calculated using the typical monod style kinetics knabe et al 2018b 8 r k s b i o c c h 2 o c c h 2 o k c h 2 o where r is the oxidation rate of organic carbon mol l 1s 1 k is the rate constant mol l 1s 1 m3 bulk mol biomass 1 sbio is the biomass concentration mol biomass m3 bulk 1 c c h 2 o is the concentration of organic carbon mol l 1 and k c h 2 o is the monod half saturation constant of organic carbon mol l 1 the reaction hotspot shown in fig 3b is assumed to be caused by differences in the biomass concentration sbio which is set to 1 mol biomass m3 bulk 1 in the hotspot region and 0 001 mol biomass m3 bulk 1 in the bulk aquifer the rate constant was set to k 1 6 10 7 m o l l s m b u l k 3 m o l b i o m a s s and the monod half saturation constant to k c h 2 o 10 4 m o l l the aquifer is intersected on the two sides by two wells 8 5 m apart as shown in fig 3 the well on the right side is used for injections with injection points at 5 different levels with 1 m spacing the observation points are located at the same levels in the observation well on the right side the injections are simulated as independent experiments tracers are injected over 10 min with 200 liters of water volume the concentration of the injection is 0 1 mol l for both the conservative and reactive tracer the numerical simulations are run for 14 days this setup results in a total of 25 breakthrough curves covering the whole area of the aquifer to test the method under more realistic conditions we have prepared a 3 d pflotran model using the descalvado aquifer analog dataset höyng et al 2014 this is a high resolution digital recreation of a small block of the guarani aquifer system which was mapped and analyzed the aquifer is composed of fluvial aeolian sediments mainly sand gravel and some fine sediments this is a multi layer aquifer with strong hydraulic contrast between the different facies the aquifer geometry and hydraulic conductivity distribution are shown in fig 4 the dataset provides the hydraulic and thermal properties of the different facies but no chemical analysis was done therefore we have chosen one of the facies to represent the reactive hotspot in nature the biogeochemically active reaction hotspots are typically formed in low k parts of shallow aquifers these parts however usually avoided by the main flowpaths so to have a visible effect we have selected the layer marked by the green color in fig 4 this layer could represent a finer sand formation on top of a gravel sand high k layer shown by the red color similarly to the 2 d case constant head hydraulic boundary conditions are set at the left and right sides of the aquifer to provide an average hydraulic gradient of 0 005 m m the model domain is setup by 0 1 m 0 1 m 0 1 m cells to capture the 3 d aquifer structures properly in this example we are not limited to the tomographic setup to a 2 d profile but we used 5 observation wells along a perpendicular plane each with 5 observation points this allows for a 3 d reconstruction within a prism shaped subdomain the simulations were run for 40 days which was enough to capture the major parts of the breakthrough curves the total number of breakthrough curves in this case was 125 for each tomographic experiment 3 results and discussion 3 1 2 d case the inversion domain for the 2 d example is a 10 m 5 m area split up into 1 25 m 1 m cells for the inversion this coarse grid is then shifted vertically and horizontally 3 3 times by the staggering algorithm resulting in a final refined grid with 1 25 m 1 m grid cells hence the inverse problem is solved 16 times over 16 different grids in total because of the computationally very effective fast marching algorithm the full workflow took around one minute to complete on a regular pc the results of the 2 d example are shown in fig 5 the reconstruction based on the conservative tracer tomography provides a good match with the reference model fig 5c in the center of the profile the low k lens is very well visible with strong contrast in k the reconstructed k values of the main aquifer body are close to the original values on the top and bottom borders of the profile some low k pixels suggest the presence of aquiclude layers the reconstruction however is far from perfect the main aquifer body which is homogeneous in the original model is reconstructed with small internal variations by the inversion also the low k lens has a larger extent in the tomogram and its shape is very rounded the reconstructed k values here are much higher 1 3 10 3 ms 1 than the original values 1 3 10 4 ms 1 these are known limitations from the methodology similar artifacts were observed by multiple studies using similar methodologies kong et al 2018 somogyvári and bayer 2017 it is important to state that the reconstruction is based on the information along the transport pathways paths that are mainly in the higher conductivity aquifer zones this results that the reconstruction of the low k aquifer parts will be always less reliable compared to high resolution tomography applications tracer tomography will always provide more rounded geometries with smoother contrasts because of the limited number of source receiver combinations near the model boundaries the reconstruction algorithm often over or underestimate the k values because here the grid cells are not covered by that many transport pathways so the inversion is less stable in the presented case this is most notable near the injection well this is where the highest k values are shown the reactive tracer tomogram shows a very similar picture by the first look but with higher k values in the center the differences between the two tomograms are better visible when they are specifically plotted in fig 5e in this figure the reactive hot spot is well visible in the center of the figure as a zone where the reactive transport appears faster than the conservative this is because the obtained breakthrough curves are formed as a combination of multiple flow paths not just the fastest ones in the case of the reactive tracer for all flowpaths that travel through the reactive zone the mass of the tracer gets strongly reduced therefore the breakthrough curve of the reactive tracer is primarily composed of the fast flow paths through the less reactive zone which results in an earlier but smaller peak of the breakthrough curve compared to the conservative tracer this results in a higher effective hydraulic conductivity for the reactive tracer the reconstruction however does not show a single hotspot in the center but rather two loosely connected ones this can be explained by the local hydraulic heterogeneity which makes the main flowpaths avoid the low k sand lens because of this the tracer could only interact with the reactive zones at the two ends of the sand where the distortion in the flow field is still minimal 3 2 3 d case the 3 d inversion domain is defined as a box with sides 20 m 6 m 6 m with a grid size of 1 m 1 m 1 m similarly to the 2 d case 3 times staggering was used but in this case in 3 dimensions the inverse problem is solved 64 times and the final grid resolution is 0 25 m 0 25 m 0 25 m the results of the aquifer analog based 3 d reconstruction are shown in fig 6 compared to the reference model in fig 6c the reconstruction of the conservative tracer tomography in fig 6a shows a lot of similarities the low k layers on the top of the model are captured although their internal heterogeneities are not visible the central high k layer where the majority of the transport takes place is well reconstructed as a continuous layer similarly to the 2 d case the reconstruction shows some instability at the sides resulting in very high k values near the source and receiver points in the central zone also here a strong variation in the reconstructed k values is visible which can be explained as the distortion effect of the embedded smaller low k lenses near the center of the aquifer the central zone is very well distinguished from the bottom low k layer which is only partially reconstructed the central layer acts as a shortcut between the lowest source and receiver points so no inverted transport pathways were modeled here this resulted in a very low reliability reconstruction which got removed after the null space energy map calculations interestingly this effect does not appear in the top part of the model because there the hydraulic conductivity contrast is smaller with the layer below the overall appearance of the conservative tracer tomography reconstruction is very similar to the results of somogyvári et al 2016 where the same aquifer analog was used for a synthetic thermal tracer tomography experiment the reconstruction based on the reactive tracer in fig 6b shows some visible differences the low k zones are better reconstructed with much less volume masked out from the bottom layer in general the two tomograms look very similar again and can be interpreted similarly the plotted difference between the two tomograms in fig 6d highlights an area close to the actual reactive hotspots shown in fig 6c with higher effective k values for the reactive tracer the reconstructed reactive zone sits on the top of the high k central layer similarly to the reference model even some of its topology is visible on the reconstruction the large thickness on the left side and the thinning in the center based on these results the reactive hotspot could be well isolated further investigations showed that the redox zone is still captured if we reduce the number of observation wells to 3 or even to 1 although then the reconstruction will be limited to a profile 3 3 sensitivity analysis the most crucial factor regarding the success of the comparative tracer tomography approach is the choice of the reactive tracer to further investigate this we have conducted a series of simulations with different reaction rates in the redox hotspots to represent different tracers with different reactivity the sensitivity analysis was performed using the 2 d model example fig 2 all hydraulic parameters were kept the same only the reaction rate constant was modified using a multiplication factor f 9 k f k 0 where k 0 is the basis rate constant defined as 10 k 0 1 6 10 8 m o l l s m b u l k 3 m o l b i o m a s s the comparative tomography experiment was repeated using different multiplication factor values and the simulated datasets were interpreted using the tomographic inversion workflow fig 7 shows the reconstructed redox hotspots there is a clear relationship between the rate constant and the quality of the hotspot reconstruction at the lowest tested reaction rate in fig 7b the hotspot is barely visible it does not stand out of the surrounding aquifer area the background variability of the reconstructions shows similar amplitudes making it very difficult to localize the exact reactive zone in the latter examples as the reaction rates becoming more intensive the differences between conservative and reactive transport are becoming more prominent and these differences are highlighted on the hotspot reconstructions at the factor of 5 the hotspot is already well visible and is easy to localize higher reaction rates do not change the reconstructions too much the anomaly at the hotspot is getting stronger and its boundaries sharper in general we can say that larger reaction rates are improving the reconstruction quality and desired for a successful application while in the case of a field experiment many factors influencing the reaction rate are a fixed property of the subsurface and not controllable by choosing a different reactive tracer with higher reactivity we can still increase the visibility of the reaction hotspots some tracers could get into more intensive reactions with the hotspot and would differ more from conservative transport providing similar differences as shown in this sensitivity analysis this is a very important property to consider when planning the practical application of the proposed methodology where the tracer choice could be adjusted to localize the reactive zones by choosing a tracer that is specifically reactive to the investigated hot spots the reconstruction quality could be more accurate and the redox zones could be better localized 4 conclusions in this study we have shown that the combined use of conservative and reactive tracers for tracer tomography could reveal the location of reactive hotspots in shallow groundwater systems comparative tomography was capable to identify the main redox hotspots in multiple examples because the inversion of the conservative and the reactive tomography can be done using similar inversion grids the two methods are directly comparable the difference maps shown in the paper provide an intuitive look into the possible distribution of reaction hotspots in the aquifer the inversion algorithms are robust and computationally very effective and due to the extensive literature they are easy to implement compared to other existing methodologies our approach does not require any complex numerical modeling and can provide results very rapidly while conservative tracer tomography is state of the art reactive tracer tomography even reactive tracer testing is an area not very well explored because of this the choice of a reactive tracer is not straightforward reactive tracers need to be specifically chosen for the investigated problem for redox hotspot identification a good example could be the resazurin tracer which changes into resorufin and changes its color due to microbial activity knapp et al 2018 in practice the conservative and the reactive tracer tomography experiment can be performed simultaneously if the two substances do not affect each other or each other s detectability one of the main limitations of the proposed methodology originates from the resolution of the tomography in practice it is not feasible to use significantly more injections and observations than shown in this study a solution to overcome this challenge would be to combine this methodology with other aquifer characterization techniques like hydraulic tomography electric resistivity tomography or ground penetrating radar these methods could provide a high resolution aquifer reconstruction which could be used as a basis for the interpretation of the comparative tracer experiments credit authorship contribution statement márk somogyvári conceptualization methodology supervision visualization writing original draft dustin knabe conceptualization methodology investigation software irina engelhardt conceptualization methodology supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors we thank two anonymous reviewers for their constructive comments 
