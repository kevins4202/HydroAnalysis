index,text
25695,eutrophication causes structural changes to the ecosystem of a water body such as increased production of algae and aquatic plants depletion of fish species and a general deterioration of water quality we discuss the development of an augmented reality ar application app based on a generalised system dynamics model of chemical and biological processes involved in the development of algal blooms the objective was to generate user based decision making scenarios that could be visualised in real time to see what cannot be seen in real life for agricultural and environmental management training visualisation of the modelled processes is an important part of enhancing learning outcomes this was achieved by transposing the system dynamics sd model into c and the game engine unity for the development of a hands on learning environment through an ar app called affluent effluent on the microsoft platform hololens conceptual and technological challenges and solutions associated with the app development are discussed as is a brief evaluation keywords augmented reality system dynamics visualisation eutrophication training software data availability name of software affluent effluent developer steve leigh contact information steve leigh0421 782 845 steve leigh purple telstra com xwipeoutx gmail com year first available 2019 hardware required microsoft hololens first edition to run from unity editor see https docs unity3d com manual system requirements html software required hololens no extra software required unity windows 10 unity engine windows mixed reality module uwp build tools net and il2cpp availability cost n a program language c unity shaderlab cg program size 350 mb including 3 d assets software available at https doi org 10 48610 ded5b89 1 introduction for the uninitiated the term water management is about managing an amount of water being used for something be it domestic use in a household for showering and washing or for agricultural use such as in irrigation or environmental use such as managing water flows in rivers while this is part of water management there are additional complexities to understand if one is going to be involved in the successful management of water in the environment biological and chemical processes occur in a water body that have a profound effect on how living organisms react to each other and the environment and very rarely are these processes visible to the human eye they must however still be understood and in some way considered and evaluated if the water body is to be managed successfully many will be familiar with the old saying that seeing is believing this is an idiomatic expression people often use to express that they will only believe something if they can see it with their own eyes since most of the processes that take place within a water body are invisible they can only be seen by proxy for example phosphorous and nitrogen cycles are invisible until reflected in a coloured algal bloom the build up and depletion of oxygen as a result of an algal bloom are also invisible unless there are visible proxies such as littoral plants or animals such as fish living or dying the project reported here was about addressing the how of how do we visualise the invisible processes associated with waste water management so that they are easily understood by creating a training tool that could promote engagement interest and learning in specific science based disciplines associated with a waste water scenario khan and johnston 2019 salmi et al 2017 skinner 2020 to do this necessitated developing a robust and tightly defined system dynamics sd model of the biological and chemical processes occurring in water and then translating that model into an augmented reality ar app allowing visualisation of those processes importantly from a teaching and engagement perspective the ar app was designed such that the user could create a simulation scenario to address their interests here we discuss an ar app that illustrates a scenario where the user is managing an enclosed water body as a fish farm and where that water body is occasionally impacted by effluent from a pig farm which creates the potential for an algal bloom developing such a hypothetical scenario provides a complex set of scientific processes that can be modelled and visualised to provide a stimulating and integrative learning environment for the user of the app a brief review of the literature that was used as the knowledge base in creating the sd model and the development of the ar water management app as an educational tool called affluent effluent is described here to provide the theoretical backdrop to the tool a description of the development of the sd model and visualisations the technical challenges the workarounds and the outcome are then laid out in order to create an understanding of the value of such innovative tools but also the difficulties in developing them 1 1 theoretical context the theoretical context to developing a water management model is complex there is much literature and many different models of biological and chemical processes that occur in water indeed jorgensen in 2010 says that it is possible to find a suitable eutrophication model applicable to almost any lake with an available data set this does not imply that better and other additional lake models should or could not be developed and applied in the environmental management context jorgensen 2010 689 in the case described here a simple but robust system dynamics model was created using a combination of models it was based on understanding the processes of eutrophication the phosphorous and nitrogen cycles the photosynthetic process and the respiratory requirements of fish this included taking into account how these processes are affected by for example temperature visibility concentrations of oxygen phosphate ammonia and nitrate eutrophication is the excessive growth of phytoplankton algal bloom development caused by excess nutrient enrichment in water schindler 1977 2006 chislock et al 2013 smith 2003 serediak et al 2014 sinha et al 2017 in most freshwater systems cyanobacteria are the phytoplankton associated with these algal blooms paerl 1988 eutrophication can affect the ecological balance of a body of water alexander et al 2017 it can reduce dissolved oxygen and thus the capability of the water body to sustain other plant and animal life it can limit sunlight penetration which reduces growth and causes die off amongst littoral plants peterson et al 1987 scheffer 1999 smith 2003 lehtiniemi et al 2005 it can increase turbidity cloern 1987 budris 2017 change the colour of the water and reduce biodiversity seehausen et al 1997 there has been much work to show that eutrophication is a natural process that occurs slowly over time due to biodegradation greeson 1969 however human activities can accelerate the rate and extent of eutrophication by the addition of nutrients such as nitrogen and phosphorous into aquatic ecosystems such addition of nutrients can take place from both point source discharges sources with a specific point of origin such as municipal discharge and industrial outflows and from non point discharges sources such as agricultural and urban runoff and leachate from waste disposal sites where the exact point of origin cannot be identified chapman 1996 van puijenbroek et al 2004 this has been termed anthropogenic or cultural eutrophication hasler 1947 and has dramatic consequences for drinking water sources fisheries and recreational water bodies carpenter et al 1998 additionally when these dense algal blooms eventually die microbial decomposition severely depletes dissolved oxygen creating a hypoxic or anoxic dead zone lacking sufficient oxygen to support most organisms carpenter et al 1998 the nutrients that are primarily involved in agricultural systems that impact water bodies and strongly influence eutrophication and the development of algal blooms are phosphorous ammonia in the form of ammonium and nitrate vollenweider 1968 schindler 1974 2006 lee et al 1978 smith 2003 with excellent reviews of the literature on this topic by robeson 2014 and serediak and prepas 2014 1 2 system dynamics modelling of water bodies and algal bloom development system dynamics is a methodology and mathematical modelling technique to conceptualise understand and work with complex issues and problems and has application in a wide range of areas for example agricultural ecological and economic systems all of which usually interact strongly with each other ford 1999 it is a method for developing simulations using formal mathematical models to learn about and work with the dynamic complexity that integrated systems display it is also a good approach to gaining a qualitative insight into the workings of a system or the consequences of a decision forrester 1995 and as such is a good development framework for learning outcomes around complex systems the development of eutrophication in a water body has many of the characteristics of a complex system it is constantly changing it is tightly coupled in that everything in the system interacts it is governed by feedback from the system itself it is nonlinear which arises as multiple factors interact and it is history dependent that is many actions are irreversible sterman 2000 system dynamics modelling has been used for many years in relation to modelling water systems and is well discussed in the literature including svirezhev et al 1984 addresses multiple different models whitehead and hornberger 1984 dynamic modelling of algal behaviour whitehead and toms 1993 dynamic modelling of nitrates in reservoirs and lakes jorgensen and de bernardi 1998 use of structural dynamic models in water management scenarios zhang et al 2003 zhang et al 2004 dynamic modelling of water parameters in lakes arquitt and johnstone 2004 system dynamics modelling of toxic algal blooms atanasovaa et al 2006 automated modelling of aquatic ecosystems gurung 2007 systems dynamics model of a reservoir water body mwegoha et al 2010 modelling dissolved oxygen shahsavani et al 2019 modelling phosphorous prediction in this project we developed a simulation model of eutrophication in a pond system based on a system dynamics model that incorporated aspects from the above listed research the idea was that the system dynamics model could then be used as the basis for an ar waste water management app that would take into account the biological and chemical processes involved in the eutrophication process and included a simple financial model to address issues of financial sustainability 1 3 augmented reality there are many definitions of ar all very similar a combination of schueffel 2017 and rosenberg 1993 produces a solid definition augmented reality ar is a direct or indirect live view of a physical real world environment whose elements are augmented by computer generated perceptual information ideally across multiple sensory modalities including visual auditory haptic touch somatosensory sensation and olfactory smell schueffel 2017 the overlaid sensory information can be constructive i e additive to the natural environment or destructive i e masking of the natural environment and is spatially registered with the physical world such that it is perceived as an immersive aspect of the real environment rosenberg 1993 the power of ar according to klopfer and sheldon 2014 is that it enables the user to engage with realistic issues in a context with which they may already be connected i e the user can experience an immersive online experience whether it be for pleasure e g gaming skinner 2020 or education e g training salami et al 2017 daniela 2020 or doing e g maintaining a piece of machinery gupta 2019 ar enables a situation to be created that puts the user in a real world physical and social context while guiding and facilitating participation dunleavy and dede 2014 for example ar technology has the ability to render objects that are hard to imagine and turn them into 3d models then enable interaction with these models making it easier for the user to grasp abstract content it essentially offers a seamless interaction between the real and virtual worlds billinghurst 2002 bach et al 2018 additionally ar accelerates engagement with users and engagement of users with the content as it provides an opportunity to immerse the user in a visualisation whilst they are simultaneously experiencing the observed world environment this is important particularly with environmental management scenarios veas et al 2008 veas and schmalstieg 2013 meaningful environmental modelling with ar kamarainen et al 2018 location based ar to promote learning haynes et al 2018 flood visualisation coen et al 2019 creating pro environmental behaviour with ar because in a training context theoretical knowledge is not enough to obtain proper skills in professional areas and ar in the learning training context enables users to practice and gain hands on interactive experience in their areas of interest ar features can thus help perform virtual practice enabling participants in an ar scenario to acquire both theory and some experience sermet and demir 2020 daniela 2020 this paper describes the development of an app running on microsoft s ar technology the hololens v1 goggles https www microsoft com en au hololens with the aim being to develop a novel intuitive learning tool which could be used in multiple different science based learning contexts to engage students in interactive and useful learning an evaluation of the app addressed the research question is it possible to design and develop an app running on ar technology that is based in system dynamics and which can be used to visualise the invisible in order to engage interest and promote an integrative learning environment in multiple different scientific contexts 2 methodology affluent effluent ar app development the aim of the project was to develop a scientifically robust model of eutrophication that was based on existing research and associated visualisations to underpin the development of an ar app it was envisaged that this app called affluent effluent would be a hypothetical authentic inquiry driven wastewater management decision making system which could be used as a novel learning environment for training purposes in a number of scientific areas for example chemistry maths biology agribusiness a staged technical development of the app for use with microsoft s hololens technology was undertaken via an iterative app design process trialling the final version of the app with students in a number of different stem courses was also undertaken and is discussed in the results section the scenario for affluent effluent was that the user was the owner of small composite fish culture farm their aquaculture business used the pond system for raising fish which has the basic requirement that it be self sustaining as it grows plants and algae for fish food fish are very susceptible to changes in the quality of the water in which they live in other words they are complete captives of their environment they need oxygen to survive and anything that influences the concentration of dissolved oxygen including temperature needs to be managed very carefully furthermore to provide additional scientific complexity to the model the pond in this scenario was being used to manage effluent from a production situation piggery and nutrients such as ammonia nitrate and phosphate need to be monitored daily as these affect the build up of aquatic weeds the objectives of affluent effluent were for the user working in the scenario to be able to i identify the reasons that led them to believe there were issues in the pond ii evaluate potential factors that may have contributed to the health of the fish in the pond iii identify and describe the process behind the actions they needed to take to make sure this situation did not happen again iv be able to create a chart to test record and map the current levels of e g oxygen ph nutrients air and water temperatures rainfall and water turbidity against the optimum levels required to maintain a healthy pond the brief for the ar component of this project was to create a 3d holographic immersive online pond environment with interactive model components that the user could touch and interact with to demonstrate relationships and key processes this involved both above surface and below surface environments graphics that the user would interact with behind the graphics needed to run several biological and chemical models that would enable components of the pond water quality fish plants etc to change over a time stepped series using either real time data ingress from the field gunesekra et al 2018 or from historical databases a simple financial model should also be incorporated to take account of the amount of electricity used when pumps and aerators were used the idea of the ar app was for users to interact with the holographic model in order to pull apart the system and change a number of different variables dependent on the state of the situation in the pond as portrayed by the simulation furthermore users had to be able to make decisions that had real time effect on the model and what was being visually displayed as a result these decisions and their impacts on the variables of the model had to be logged so they could be reviewed after exiting the app for analysis of outcomes and reinforcement of further training if needed 2 1 underpinning model and ar app development there were two stages to the sd model and affluent effluent ar app development firstly a pilot app described in bryceson et al 2018 was developed using semantic systems s system dynamics freeware to create a simple model with basic graphics this pilot app was made available in 2018 to a small group of users via the hololens for trials relating to engagement and learning usefulness this trial was successful with a limited and informal evaluation amongst a mixed demographic of users indicating that they enjoyed the app and were engaged in what it was depicting however they also indicated that a they would like to see a more complex systems model underpinning the final app such that decisions were more complex and linked to more realistic outcomes in order to create deeper learnings and b that the graphics needed to be more professional this paper details the second stage of development to produce the final ar app affluent effluent 2 2 the final eutrophication model used in the ar app the objective of the final eutrophication sd model was to simulate the effects of a build up of nutrients in a pond on eutrophication development and on plankton seasonal dynamics the final version of the model used in the affluent effluent ar app was built using the extant research identified in table 1 the model was first developed in isee systems stella architect software https www iseesystems com and then transferred into c and unity https unity com as the game engine for visualisation and driving the app on the microsoft hololens the final sd model was arrived at after many design and development iterations including testing within the hololens environment in relation to the development of useful visualisation of the key issues for users to interact with the final model was eventually made up of seven modules that represented i phosphorous generation ii nitrate generation iii ammonia generation iv photosynthesis v dissolved oxygen vi the pond lake and vii self shading the attenuation of light entering the water by algal blooms with impact on littoral plant health and oxygen availability the seven modules are interconnected and affect one another via the interplay of variables within the modules key drivers in the overarching system dynamics model include i weather rainfall evaporation water temperature water use and sunlight intensity which when not overridden by the user is modelled within a southern hemisphere seasonal pattern ii lake depth which is driven primarily by the weather variables and includes an inflow of water from rainfall and runoff and an outflow due to leakage and evaporation iii nutrients phosphorous nitrate ammonia all of which are impacted in the same way by runoff due to rainfall algal decomposition water use by irrigation and algal growth and user defined increase decrease in number of pigs iv algae growth and death modelled as a single organism cyanobacteria with no competition between species for simplicity table 2 shows various factors involved in the modelling of the growth and death of algae v dissolved oxygen fish health is linked to available dissolved oxygen fish will die when dissolved oxygen drops below 6 mg l additionally fish energy swim speed in visualisation varies when oxygen is between 6 mg l and 12 mg l table 3 shows the factors impacting dissolved oxygen vi the bank which tracks income associated with fish and expenses associated with managing the pond this is a simple cash tracker based on fish numbers and expenses to run the aerator and irrigation pump for which there is a specific turn on my aerator irrigation pump action in the app and costs money as opposed to water usage abstraction which was built into the system and is free in terms of cost to use the final sd model had 36 basic variables and 22 converters that were kept track of and reported on figs 1 3 represent the seven modules as a series of system dynamics stock and flow diagrams which show the structure of each system with its constituent components and associated interactions stocks are components that can accumulate in the system for example in fig 1 nitrate and flows are components that make stocks increase or decrease for example in fig 1 nitrate inflow nitrate outflow in figs 1 3 the variables in each module are named in common english and the direction of model flow is shown by the arrows fig 4 identifies the interconnectedness between the modules of the sd models schematically note while the overall narrative for the ar app scenario development involved management of an on farm water body being impacted by effluent overflow from a nearby piggery the sd model itself does not know about pigs it only knows about phosphorus ammonia and nitrate the model has been constructed such that in the app pigs can be added removed by the user as a means of increasing decreasing nutrient levels in the water with average nutrient concentrations pig being taken from australian pork industry research on the level of nutrients in manure apl 2015 thus to provide a change in the amount of phosphorus ammonia and nitrate an independent variable representing the number of pigs pigpopulation as inputted by the user and which impacts phosphorusadded 1800 g pig day ammoniaadded 4050 g pig day and nitrateadded 450 g pig day was used this is also true of oxygen injection the aerator itself does not come into the lake model the amount of oxygen does irrigation usage again the model knows about the water used for irrigation not the pump itself the reasoning behind these distinctions was in order to separate the eutrophication model itself from the human factors that could be changed for example by changing pigs to cows or even to humans the underlying model would remain the same just the nutrient inputs would change the same applies to irrigation and aeration 3 implementation of the stella model in unity unity www unity com was selected as the game engine due to its broad platform supporting capabilities including a mature set of features and assets being available for use making it ideal for this visualisation on microsoft s hololens unity uses c as its scripting language and as such development work was required not only to provide an immersive experience for the users but also to simulate the eutrophication cycle the next section discusses the implementation of the system dynamics model in c 3 1 variables constants and units of measure variables and constants in the model were stored in class fields using value typed structuring microsoft 2020 to represent specific units of measure units of measure used in this app include area celldensity concentration concentrationflow currency currencyflow distance irradiance liquidflow liquidvolume mass massflow period speed temperature volume and volumeflow these units of measure incorporated operator overloading programis 2020 to make it possible to perform mathematical operations on them while ensuring the preservation of the unit of measure or enabling change if required for example dividing mass mg by liquidvolume l returns concentration mg l additionally using these strongly typed structures prevents accidental errors for example using such a structure it is not possible to subtract distance from currency the full implementation of units of measure can be found in the accompanying source code two examples are provided to illustrate i a code for the mass class a concept in the code called mass which has its own set of behaviours and actions in this case knowing how to add multiply divide and convert a mass which is shown in code listing 1 and ii a 1 line piece of code that calculates the algal daily growth rate is shown in code listing 2 code listing 1 code file for the mass class one of the units of measure image 1 code listing 2 an example of using the units of measure to calculate the algal daily growth rate image 2 3 2 data structures several data structures were used to split the model up into individual components and separate their concerns 1 complexwastewatermodel the primary interface to the data in the simulation including initialvalues complexmodelconstants complexmodelframes and a series of user specific independent variables such as pig population rain amounts etc 2 initialvalues initial values for the stocks in the model water nitrates ammonia phosphorus cyanobacteria oxygen concentration and bank balance 3 independentvariables independent variables in the model most of these are customisable by the user either directly or indirectly via the values in the complexwastewatermodel for example ammoniaadded is calculated as ammoniaperpig pigpopulation 4 complexmodelconstants constants used in this app 5 complexmodelframe a snapshot of the model at a particular t value contains the frame number number of timesteps elapsed day number independentvariables as well as subframe for each part of the model 6 subframe each subframe contains a snapshot of the variables for each part of the model lakeframe nitrateframe ammoniaframe phosphorusframe biologyframe oxygenframe bankframe fishframe please also refer to appendix 1 equations used in the model and appendix 2 constants used in the model 3 3 calculating the next time step for the initial frame t 0 the complexmodelframe is simply derived from the initial conditions see complexmodelrunner initial subsequent timesteps are done through complexmodelrunner stepforward which takes in as arguments the previous complexmodelframe independentvariables and the delta time in days from these it generates a new complexmodelframe with the system dynamics calculations applied to create the new frame it first calculates the stocks code listing 3 and then derives the transient flow values code listing 4 once a frame is calculated it is pushed into the complexwastewatermodel and is used as an input to the next frame code listing 3 phosphorus calculations stocks image 3 code listing 4 phosphorus calculations transient flow values image 4 note these calculations correspond closely to the system dynamics modules in figs 1 3 additionally the units of measure are used extensively in these calculations providing additional safety 3 4 passing of time and time travel in order to provide a feeling of the passage of time to the user and to keep them engaged the simulation had to be run at a greatly increased speed between 5 s per world day to 1 s per 5 world days by doing this a user can quickly experiment with different input values to the model and see the impact it has on the system to see a situation unfold slowly the user can slow down the progression of the simulation and even pause it for a closer analysis similarly when a user makes a decision that results in a negative effect from a management perspective it is valuable for them to be able to go back and tweak their inputs to achieve this time travel was provided which reverts the model to an earlier state this provides a safe consequence free avenue for experimentation and exploration which is not otherwise possible the wastewatermodelcontroller is responsible for the passage of time and time travel ensuring the simulated model has capability of including updated user inputs and is progressing at the proposed time to do this elapsedwalltime wall time is time as determined by a chronometer which differs from time as measured by counting microprocessor clock pulses or cycles is kept in sync with the computer s clock elapsed time when not paused as time progresses this value is compared against a scaling factor depending on the play speed for example 1 world second per 2 simulation days and if it goes past the threshold to calculate the next frame it does so by calling into complexmodelrunner stepforward 3 5 model outputs and interpolation given the large amount of wall time between timesteps as slow as 5 s per simulation day depending on the user s speed configuration smoothing was needed to ensure visuals changed gradually instead of jumping between the values in one day to the next linear interpolation smid 2007 between frames achieved this with the interpolation factor being the percentage between the simulation days see code listing 5 for an example code listing 5 interpolation of dissolved oxygen between 2 simulation frames a and b image 5 this was only done for variables that required visualisation for example the phosphorus limiting factor was not going to be visualised so was not interpolated to save on computational complexity 4 visualisation design and development of 3d visualisation assets to depict aspects of the stella modules that could not be seen for example the phosphorous and nitrogen cycles and photosynthesis algal bloom development and littoral plant growth death was critical to providing the authentic immersive experience bach et al 2018 that was required in the ar brief visualisation in this case was the creation of 3d holograms for the hololens which is a head mounted display unit that projects stereoscopic images of virtual content into a user s real world and allows for interaction in situ at the spatial position of the 3d hologram co design also known as generative design co creation participatory design or co operative design ncoss 2017 principles were used in the design of the visualisation assets users and developers of the app were encouraged for ideas on how to visualise the various components of the underlying models for example dissolved oxygen or the processes of eutrophication itself the ideas generated during discussion were then assessed and experimented with as to how well they produced a visualisation for users table 4 lists the key 3d holograms that were eventually agreed to and used a limited budget resulted in the choice of assets being sourced through the unity software asset shop https assetstore unity com 3d 4 1 visualisation examples this section provides examples of the model visualisations for various states of the system note the visuals are shown here with a plain black background to make the detail easier to see on the hololens the visuals are integrated with real world surfaces such as the room floor and tables fig 9 4 1 1 pond cross section the pond cross section can only be seen in design mode and displays a summary visualisation of the pond with exaggerated scale to allow for easy viewing there are a set of controls along the bottom of the cross section allowing control of environmental factors evapotranspiration rain runoff sunlight and water temperature fig 5 a is a cross section of a healthy pond the aerator is on the plants are healthy algal levels are low and the fish is alive in fig 5b increasing the pig count causes the water to become dirty representing an influx of nutrients from the runoff here the aerator has also been turned off in fig 5c high rainfall shows as raindrops falling onto the cross section white flecks on black background and in fig 5d the pond is in poor health the fish are dead algae is in full bloom and the littoral plants are no longer viable fig 6 shows the buttons on the bottom of the cross section that users can use to change variables fig 7 shows a lab report that can be obtained by tapping on the conical flask icon when the user is above water 4 1 2 the plinth the plinth is visible in both design and immersive mode and is the main control console for the app this is where the user can see a useful summary of the model state and control the simulation flow and some inputs fig 8 a shows the front view of the plinth while the pond is in poor health the warning light is on and the mini charts show low oxygen high algae low visibility and medium nutrient availability the time travel left dive in middle and play right controls are available here fig 8b shows the right side of the plinth where the nutrient influx pig population numbers and income values can be altered fig 8c shows the left side of the plinth where control of the irrigation pump abstraction and the aerator is undertaken 4 1 3 underwater visuals when in immersive mode the room is transformed into an underwater style look and feel with fish plant life bubbles and algae shown at a realistic scale and responding to pond conditions fig 9 fig 10 a shows the immersive view of a healthy pond high oxygen concentration low algae density and plants fully satisfied with nutrients and light fig 10b shows the immersive view of a pond in poor health high algae density low nutrient concentration and low light satisfaction for the plants and low oxygen availability for the fish these figures also include mood emoji that bubble up from the fish to give an indication of oxygen satisfaction fig 11 shows littoral plant assets and fig 12 high algal density and dead fish 4 2 visualisation implementation the underwater littoral plants figs 10 and 11 were 3d models that appeared underwater and changed their appearance based on the nutrient cycles plants were green and moving with a wavelike motion when healthy appropriate levels of nutrients brown and static when dead excess nutrients when health of the water body is restored the plants return to life so users can always have an idea of the nutrient and light satisfaction by looking at the plants the visuals were implemented using a basic lambert diffuse shader with an alpha cut off and tint that changed depending on the nutrient satisfaction fish fig 10 were rendered as 3d models that swam around in the user s field of view using a boid pattern reynolds 1987 for realism the swim speed was governed by the amount of dissolved oxygen and there were particle systems attached to govern the bubble rate and smiley vs angry emoji when the oxygen was too low the fish died as they do in nature with the smaller ones dying first followed by the larger ones boyd et al 2018 mdba 2019 o2 bubbles figs 10 and 11 were particle systems attached to the plants and represent the dissolved oxygen increase due to photosynthesis surface algae figs 5d and 12 was implemented as a basic quad rendering on the ceiling with a custom shader that blended in a cloud like texture depending on the algal concentration floating algae figs 10 and 12 was a large particle system that spawned larger and more frequently when there were high levels of algae in the system brightness of the lighting subtly changes as the light penetration visibility decreases to simulate the pond being darker this was kept subtle to capture the emotion of things becoming darker and less friendly while still making it easy to see 4 2 1 mapping outputs to visuals after calculating the final model values the visuals were rendered rendering or image synthesis is the process of generating a photorealistic or non photorealistic image from a 2d or 3d model by means of a computer program the resulting image is referred to as the render multiple models can be defined in a scene file containing objects in a strictly defined language or data structure the scene file contains geometry viewpoint texture lighting and shading information describing the virtual scene the data contained in the scene file can then be passed to a rendering program to be processed and output to a digital image or raster graphics image file microsoft academic 2020 the rationale for each kind of visualisation is described in the section below broadly speaking changing the environment to match the model fell into several categories 1 particle emission for the air bubbles happy sad faces effluent output rainfall aerator duty cycle and floating algae the emission rate of a particle system were adjusted based on the related model outputs so more photosynthesis produces more air bubbles for example to address this a monobehaviour which is a base class from which every unity script derives to cover all the particles was created to map the domain of model outputs to the range of particle system modifications code listing 6 code listing 6 synchronizing particle emission with model outputs image 6 2 material and property changes for algae concentration water dirtiness from runoff and fish swim speed linked to oxygen level i e concentration simple scripts were created to modify the specifically required properties these varied from visual to visual but generally involved mapping of a model value to an output range whether that be colour speed density etc and applying it code listing 7 code listing 7 changing the surface algal density by altering the density parameter of a shader based on the bloom amount derived from algal concentration image 7 3 mini chart the mini chart on the plinth shows an easy to read list of the model s most important values oxygen algal density light visibility and nutrient satisfaction this is a simple bar chart which was implemented in unity with basic coloured quads that cut off based on a scalar value code listing 8 code listing 8 implementation of the barrenderer and minichartcontroller to display a mini chart image 8 5 visualisation development challenges with the hololens the hololens is an emerging technology the first consumer level ar headset released and thus there are plenty of limitations and restrictions to implementing an app using this technology these include performance interaction and field of view bach et al 2018 performance delivering 3d apps for the hololens requires a commitment to maintaining a high performance in terms of presentation microsoft recommends running all apps at 60 frames per second at all times as going below this can lead to eye strain unstable holograms and fatigue for the user the hololens is portable and therefore not a high powered machine it contains the processing power similar to what you d expect from a high end phone in 2012 and needs to render at a resolution of 1268 720 per eye whilst continually keeping track of its position and orientation in the real world as a general rule of thumb hololens apps are developed with similar limitations to that of mobile phone application development for example overdraw drawing in the same screen location multiple times is particularly expensive in terms of computing power on the hololens requiring the need to limit usage of transparent objects relying on the additive blending in the holographic display which renders black things as transparent where possible alpha blending the process of combining a translucent foreground colour with a background colour thereby producing a new colour blended between the two was not used even if it meant a loss in visual appeal this is most evident in the user interface buttons where some transparency would have enhanced the visual appeal but ultimately was too expensive computing wise additionally the complexity of shaders used for calculating lighting and other visual effects was found to have a large impact on the low powered cpu of the hololens to that end mobile shaders were used along with simple custom developed shaders when more appealing visual effects were required finally actual polygon count was found to be another limiting factor rendering more than around 400 000 triangles on the screen at one time caused the framerate to drop and simpler models had to be used where possible interaction interactions with the hololens are limited to gestures usually a bloom palm up fingers closed and then opening like a flower blooming or air tap forefinger outstretched and straight making a tap motion dachis 2016 while more complex gestures are possible for example two fingered pinch tap and slide when trialling the software with various people it was discovered that these more complex gestures are not intuitive given the app was designed to be used without training and without much opportunity to practice i e it was not being used daily by the same person interactions had to be kept as simple as possible and were thus limited to the use of the air tap gesture the result of this is that everywhere a user can interact is a button on the plinth it is simple and discoverable but not terribly immersive this is a trade off that we felt gave a better overall result field of view another clear limitation of the hololens is its limited field of view around 35 the visualisations had to be designed around this so that the user could always easily find what they were looking for for example like ensuring the interaction always happens somewhere obvious like on a table or the same spot on the floor to make holograms appear more populated the action in the simulation happened in front of where the user was already looking for example having fish holograms tend to swim in front of the user instead of behind them animation was used to draw attention to off screen elements such as buttons that needed clicking it was tempting to do things in a virtual reality style i e transporting the user into a fully virtual environment underwater in a lake but this limited field of issue as well as the presence of the real world room made this experience feel disjointed playing to the hololens strength the main approach taken was instead to bring the virtual world into the room i e have fish swimming around the room or a plant planted on the floor of the room etc using this approach meant that an entire virtual environment was not being generated with each decision saving precious performance and development time and everything the user wanted to see could be seen at once in the device 6 results operation and use of the affluent effluent app users use the hololens to configure and view the simulation of eutrophication in a closed system aquaculture pond that has water ingress from a nearby pig farm users can experiment with the factors involved in the development of an algal bloom including rainfall runoff evaporation temperature aeration water abstraction irrigation pig effluent input fish stock as indicated in table 4 there are two primary views in the app an above water design mode and underwater immersive mode experimentation in decision making around management of the pond is enabled by tweaking the buttons on the plinth that appears in both the above water and immersive modes similarly the simulation speeds and time travel enable users to go back in time in the simulation to fix up earlier mistakes or try a different management strategy these are controlled through buttons on the plinth as discussed earlier some of the decision making tools enabling users to control variables include the ability to change the number of pigs which equates to the amount of nutrient pig added the ability to turn on vary and turn off the aerator amount of oxygen view the total dollars in the user s bank impacted by income and expenses display a lab report a real time report to user running the simulation on variables impacting water quality at any point in time while the wearer of the hololens sees a rich view of the pond with fish bubbles plant life and algae reacting to the simulation the app can also be played amongst a group of people who can log on to the accompanying website and view a highly detailed breakdown of the variables in the pond water in real time an example is shown in fig 13 a more detailed analysis can be conducted after the app is completed by using the associated website and downloading the excel spreadsheet containing a full history of the simulation run including time travel resets an example dataset after running the app is shown in fig 14 with this type of dataset available after the running of the app users particularly students can be tasked with an assignment based on the data they have created using the app such a task might be to create graphs to visually illustrate relationships in the data or to look for trends in that data that can reveal relationships between variables and thus potential management decisions that might be made the data collected via the app is comprehensive enough to enable teachers in different scientific subjects to create bespoke assignments based on it for their own subjects and for the user student to see the outcomes of their decisions thereby reinforcing the training and learning 6 1 evaluation this paper describes the development of a model and the ar app associated with it for completeness here a brief overview of the evaluation of its use in five higher education level courses where it was used in practical classes between 2018 and 2020 is presented the evaluation was carried out to answer the research question posed earlier about the role of system dynamics and the value of ar in visualizing the invisible in terms of the app s value as an educational tool this was accomplished via a formal ten item evaluation questionnaire under uq ethics clearance 2018002100 in order to generate quantitative and qualitative data from each of those courses a total of 335 responses were obtained with 186 having all the questions answered these 186 responses were used for calculating descriptive statistics range average median values with a further thematic content analysis also undertaken to determine the pedagogical implications of the data collected the descriptive statistics show overwhelmingly that students were engaged by the technology yes 88 1 and that 87 6 agreed that it enhanced the learning process additionally 72 9 of students felt the right balance was struck between the complexity of the simulation visual indicators and graphical representation comments included it was an immersive learning experience and is more engaging than just sitting and listening graphics were good but gestures take time to learn interesting simulation able to identify interrelationships between multiple factors provides hands on learning overall the majority of comments were positive across all evaluation questions key themes and associated concepts from content analysis included 1 technology augmented reality is different interesting and engaging 2 the model app is a good learning tool lots of different possibilities and courses it could work with users liked the real time 3d visualisations 3 learning tool enables first hand learning learning by doing fun and interactive individuals can control the pace of learning different things in different courses visual learners gain maximum benefit group learning and collaboration that was fun and useful the conclusion taken from this evaluation is that ar is useful for system dynamics and science learning and teaching but that complex design and visuals are required to maintain engagement and deep learning 7 conclusions the current affluent effluent ar app has been successfully built around a system dynamics model and simulation of eutrophication and the associated management scenarios as a tool for multidisciplinary training as discussed in this paper it has involved a more complex wastewater model underpinning the app and a substantial upgrade of graphics compared to earlier pilot versions bryceson et al 2017 2018 as requested by initial users this has meant that decision making by the users of the app can now be more complex dependent on what the user and or trainer would like the user to learn and thus training and deep learning objectives are better met the ar development has provided the means to render processes that are hard to imagine and visualise e g chemical processes into 3d models which have then enabled the ability to see what cannot be seen in real life pogue 2011 daniela 2020 skipper 2020 in this case being the chemical processes in water when eutrophication processes occur these visualisations have also enabled interaction with the underpinning system dynamics model making it easier for the user to grasp abstract content associated with these processes bach et al 2018 and to practice and gain hands on interactive experience in managing them in a risk free environment to facilitate the use of affluent effluent the following resources were developed a professional training video a fully illustrated user guide leigh and bryceson 2018 with instructions on using each feature e g aerator weather information number of pigs in the system bank time travel etc and a series of tasks that could be used as training assessments in different courses or training environments e g ecology biology chemistry precision agriculture agribusiness statistics additionally the industry partner with which the project was undertaken telstra purple https purple telstra com au created a youtube video showing some of the new features during student use which can be viewed here https www youtube com watch v fpkznbkmrj4 it should be noted that since completion of affluent effluent a new version of the microsoft hololens has been released https www wired com story microsoft hololens 2 headset this hololens v2 addresses some of the major shortcomings of the first release primarily that the field of view has been expanded from 34 to 52 enabling a bigger viewing area of the model not enough for a fully immersive vr experience but still a large improvement device comfort has been greatly improved allowing longer play time and less fiddly adjustment on first use there is now fully articulated hand tracking to enable direct manipulation of holograms and more intuitive user experience such as pinch sliders to control input values and finally the hololens v2 has a much improved computing performance enabling better renderings of visualisations while a minor effort has been made to port affluent effluent to the hololens 2 and initial testing shows the experience and immersion to be greatly improved this new version of the software has not been released at this point in time declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we acknowledge significant funding and support from the university of queensland information technology services group uqits and from telstra purple without this funding and support the project would not have taken place additionally we would like to thank mr rob moffat chief information officer uqits and mr gary stefano customers services director uqits for significant organisational support in facilitating access to hardware hololens uq networking facilities and personnel we would also like to thank those individuals who gave of their time freely to engage with us in brainstorming visualisation ideas and providing feedback appendix 1 table of equations as used in the eutrophication model described in this paper appendix 1 name units equation dwater dt m3 waterin waterout evapotranspiration dammonia dt mg ammoniain ammoniaout ammoniaconsumed dnitrate dt mg nitratein nitrateout nitrateconsumed dphosphorus dt mg phosphorusin phosphorusout phosphorusconsumed dalgae dt mg algaegrowth algaedeath doxygen dt mg l oxygenin oxygenout dfish dt 0 or 1 if oxygensatisfaction 0 then 1else if fishadded 1 then 1else 0 dbalance dt fishincome aeratorcosts restockcost fishadded watervolume l water 1000 precipitation m3 rainfall kcatchment area evapotranspiration m3 evaporation ksurface area waterout m3 waterusage waterin m3 precipitation kprec depth m water surface area algaeconcentration mg l algae watervolume selfshadingfactor kshade kshade algaeconcentration underwaterlightintensity w m2 lightintensity selfshadingfactor lightlimitingfactor underwaterlightintensity exp 1 underwaterlightintensity klight opt klight opt xt watertemperature ktemp ktemp low ktemp opt ktemp low templimitingfactor 2 1 kshape xt xt xt 2 kshape xt 1 nitrateconcentration mg l nitrate watervolume ammoniaconcentration mg l ammonia watervolume phosphorusconcentration mg l phosphorus watervolume nitratelimitingfactor nitrateconcentration exp kamm pref ammoniaconcentration nitrateconcentration knit monod ammonialimitingfactor ammoniaconcentration ammoniaconcentration kamm monod phosphoruslimitingfactor phosphorusconcentration phosphorusconcentration kphos monod nutrientlimitingfactor min phosphoruslimitingfactor ammonialimitingfactor nitratelimitingfactor algaegrowth mg algae kgrowth max lightlimitingfactor temperaturelimitingfactor nutrientlimitingfactor algaedeath mg algae kdeath kgrazing kresp ktemp resp watertemperature 20 y 10 kph opt ph 1 phlimitingfactor kph kph y o2temperaturelimitingfactor exp 2 3 xt aeration mg l 0 641 depth oxygeninjected substratelimitingfactor co2 kco2 co2 kt k20 ktoxid watertemperature 20 photosynthesis mg l kgrowth max o2 algaecontentration lightlimitingfactor substratelimitingfactor phlimitingfactor o2temperaturelimitingfactor respiration mg l krmax o2temperaturelimitingfactor algaeconcentration biodegredation mg l kt oxygen kdemand kdo oxygen decomposition mg l koxy decomp algaedeath watervolume oxygenin mg l photosynthesis aeration oxygenout mg l respiration biodegredation decomposition oxygensatisfaction oxygen kfish o2 min kfish o2 max kfish o2 min nitratefromrunoff mg waterin knit conc nitratefromdecomposition mg algaedeath knit decomp nitrateout mg waterout knit conc nitrateconsumed mg kcons algae lightlimitingfcctor temperaturelimitingfactor nitratelimitingfactor kgrowth max ammoniafromrunoff mg waterin kamm conc ammoniafromdecomposition mg algaedeath kamm decomp ammoniaout mg waterout kamm conc ammoniaconsumed mg kcons algae lightlimitingfcctor temperaturelimitingfactor ammonialimitingfactor kgrowth max phosphorusfromrunoff mg waterin kphos conc phosphorusfromdecomposition mg algaedeath kphos decomp phosphorusout mg waterout kphos conc phosphorusconsumed mg kcons algae lightlimitingfcctor temperaturelimitingfactor phosphoruslimitingfactor kgrowth max fishincome fish kfish profit aeratorcosts aeratordutycycle kaerator cost appendix 2 table of constants used in the eutrophication model described in this paper appendix 2 name unit value ksurface area m2 50 000 000 kcatchment area m2 67 000 000 kamm conc ammonia inflow concentration mg l 0 1 kphos conc phosphorus inflow concentration mg l 6 1 knit conc nitrate inflow concentration mg l 0 8 knit monod nitrate monod constant 0 01 kphos monod phosphorus monod constant 0 01 kamm monod ammonia monod constant 0 005 kamm pref ammonia preference factor 1 46 klight opt optimal light intensity w m2 250 ktemp opt optimal temperature c 23 ktemp low low lethal temperature c 5 kshape shape factor for temperature limiting factor 0 7 kgrowth max maximum growth rate day 1 0 95 kgrowth max o2 maximum growth rate for o2 model day 1 2 2 kgrazing grazing rate day 1 0 01 kdeath death rate day 1 0 3 kresp respiration rate day 1 0 1 ktemp resp temperature coefficient for respiration 1 kshade self shading factor mg l 50 kcons algae nutrient consumption factor 0 001 kphos decomp algae mass released as phosphorus for decomposition 0 00095 knit decomp algae mass released as nitrate for decomposition 0 kamm decomp algae mass released as ammonia for decomposition 0 koxy decomp algae mass consuming oxygen for decomposition 0 03 kprec precipitation fudge factor 0 55 ktemp temperature fudge factor 1 6 kfish o2 max oxygen concentration for a happy fish mg l 11 kfish o2 min oxygen concentration for a dead fish mg l 6 kco2 monod monod constant for co2 mg l 0 5 kph ph limiting factor 200 kph opt optimal ph 6 8 kresp max max respiration 0 5 k20 mg l 0 0015 ktoxid kdo mg l 0 1 kdemand chemical oxygen demand 100 kfish profit day 20 kaerator cost day 100 krestock day 1000 
25695,eutrophication causes structural changes to the ecosystem of a water body such as increased production of algae and aquatic plants depletion of fish species and a general deterioration of water quality we discuss the development of an augmented reality ar application app based on a generalised system dynamics model of chemical and biological processes involved in the development of algal blooms the objective was to generate user based decision making scenarios that could be visualised in real time to see what cannot be seen in real life for agricultural and environmental management training visualisation of the modelled processes is an important part of enhancing learning outcomes this was achieved by transposing the system dynamics sd model into c and the game engine unity for the development of a hands on learning environment through an ar app called affluent effluent on the microsoft platform hololens conceptual and technological challenges and solutions associated with the app development are discussed as is a brief evaluation keywords augmented reality system dynamics visualisation eutrophication training software data availability name of software affluent effluent developer steve leigh contact information steve leigh0421 782 845 steve leigh purple telstra com xwipeoutx gmail com year first available 2019 hardware required microsoft hololens first edition to run from unity editor see https docs unity3d com manual system requirements html software required hololens no extra software required unity windows 10 unity engine windows mixed reality module uwp build tools net and il2cpp availability cost n a program language c unity shaderlab cg program size 350 mb including 3 d assets software available at https doi org 10 48610 ded5b89 1 introduction for the uninitiated the term water management is about managing an amount of water being used for something be it domestic use in a household for showering and washing or for agricultural use such as in irrigation or environmental use such as managing water flows in rivers while this is part of water management there are additional complexities to understand if one is going to be involved in the successful management of water in the environment biological and chemical processes occur in a water body that have a profound effect on how living organisms react to each other and the environment and very rarely are these processes visible to the human eye they must however still be understood and in some way considered and evaluated if the water body is to be managed successfully many will be familiar with the old saying that seeing is believing this is an idiomatic expression people often use to express that they will only believe something if they can see it with their own eyes since most of the processes that take place within a water body are invisible they can only be seen by proxy for example phosphorous and nitrogen cycles are invisible until reflected in a coloured algal bloom the build up and depletion of oxygen as a result of an algal bloom are also invisible unless there are visible proxies such as littoral plants or animals such as fish living or dying the project reported here was about addressing the how of how do we visualise the invisible processes associated with waste water management so that they are easily understood by creating a training tool that could promote engagement interest and learning in specific science based disciplines associated with a waste water scenario khan and johnston 2019 salmi et al 2017 skinner 2020 to do this necessitated developing a robust and tightly defined system dynamics sd model of the biological and chemical processes occurring in water and then translating that model into an augmented reality ar app allowing visualisation of those processes importantly from a teaching and engagement perspective the ar app was designed such that the user could create a simulation scenario to address their interests here we discuss an ar app that illustrates a scenario where the user is managing an enclosed water body as a fish farm and where that water body is occasionally impacted by effluent from a pig farm which creates the potential for an algal bloom developing such a hypothetical scenario provides a complex set of scientific processes that can be modelled and visualised to provide a stimulating and integrative learning environment for the user of the app a brief review of the literature that was used as the knowledge base in creating the sd model and the development of the ar water management app as an educational tool called affluent effluent is described here to provide the theoretical backdrop to the tool a description of the development of the sd model and visualisations the technical challenges the workarounds and the outcome are then laid out in order to create an understanding of the value of such innovative tools but also the difficulties in developing them 1 1 theoretical context the theoretical context to developing a water management model is complex there is much literature and many different models of biological and chemical processes that occur in water indeed jorgensen in 2010 says that it is possible to find a suitable eutrophication model applicable to almost any lake with an available data set this does not imply that better and other additional lake models should or could not be developed and applied in the environmental management context jorgensen 2010 689 in the case described here a simple but robust system dynamics model was created using a combination of models it was based on understanding the processes of eutrophication the phosphorous and nitrogen cycles the photosynthetic process and the respiratory requirements of fish this included taking into account how these processes are affected by for example temperature visibility concentrations of oxygen phosphate ammonia and nitrate eutrophication is the excessive growth of phytoplankton algal bloom development caused by excess nutrient enrichment in water schindler 1977 2006 chislock et al 2013 smith 2003 serediak et al 2014 sinha et al 2017 in most freshwater systems cyanobacteria are the phytoplankton associated with these algal blooms paerl 1988 eutrophication can affect the ecological balance of a body of water alexander et al 2017 it can reduce dissolved oxygen and thus the capability of the water body to sustain other plant and animal life it can limit sunlight penetration which reduces growth and causes die off amongst littoral plants peterson et al 1987 scheffer 1999 smith 2003 lehtiniemi et al 2005 it can increase turbidity cloern 1987 budris 2017 change the colour of the water and reduce biodiversity seehausen et al 1997 there has been much work to show that eutrophication is a natural process that occurs slowly over time due to biodegradation greeson 1969 however human activities can accelerate the rate and extent of eutrophication by the addition of nutrients such as nitrogen and phosphorous into aquatic ecosystems such addition of nutrients can take place from both point source discharges sources with a specific point of origin such as municipal discharge and industrial outflows and from non point discharges sources such as agricultural and urban runoff and leachate from waste disposal sites where the exact point of origin cannot be identified chapman 1996 van puijenbroek et al 2004 this has been termed anthropogenic or cultural eutrophication hasler 1947 and has dramatic consequences for drinking water sources fisheries and recreational water bodies carpenter et al 1998 additionally when these dense algal blooms eventually die microbial decomposition severely depletes dissolved oxygen creating a hypoxic or anoxic dead zone lacking sufficient oxygen to support most organisms carpenter et al 1998 the nutrients that are primarily involved in agricultural systems that impact water bodies and strongly influence eutrophication and the development of algal blooms are phosphorous ammonia in the form of ammonium and nitrate vollenweider 1968 schindler 1974 2006 lee et al 1978 smith 2003 with excellent reviews of the literature on this topic by robeson 2014 and serediak and prepas 2014 1 2 system dynamics modelling of water bodies and algal bloom development system dynamics is a methodology and mathematical modelling technique to conceptualise understand and work with complex issues and problems and has application in a wide range of areas for example agricultural ecological and economic systems all of which usually interact strongly with each other ford 1999 it is a method for developing simulations using formal mathematical models to learn about and work with the dynamic complexity that integrated systems display it is also a good approach to gaining a qualitative insight into the workings of a system or the consequences of a decision forrester 1995 and as such is a good development framework for learning outcomes around complex systems the development of eutrophication in a water body has many of the characteristics of a complex system it is constantly changing it is tightly coupled in that everything in the system interacts it is governed by feedback from the system itself it is nonlinear which arises as multiple factors interact and it is history dependent that is many actions are irreversible sterman 2000 system dynamics modelling has been used for many years in relation to modelling water systems and is well discussed in the literature including svirezhev et al 1984 addresses multiple different models whitehead and hornberger 1984 dynamic modelling of algal behaviour whitehead and toms 1993 dynamic modelling of nitrates in reservoirs and lakes jorgensen and de bernardi 1998 use of structural dynamic models in water management scenarios zhang et al 2003 zhang et al 2004 dynamic modelling of water parameters in lakes arquitt and johnstone 2004 system dynamics modelling of toxic algal blooms atanasovaa et al 2006 automated modelling of aquatic ecosystems gurung 2007 systems dynamics model of a reservoir water body mwegoha et al 2010 modelling dissolved oxygen shahsavani et al 2019 modelling phosphorous prediction in this project we developed a simulation model of eutrophication in a pond system based on a system dynamics model that incorporated aspects from the above listed research the idea was that the system dynamics model could then be used as the basis for an ar waste water management app that would take into account the biological and chemical processes involved in the eutrophication process and included a simple financial model to address issues of financial sustainability 1 3 augmented reality there are many definitions of ar all very similar a combination of schueffel 2017 and rosenberg 1993 produces a solid definition augmented reality ar is a direct or indirect live view of a physical real world environment whose elements are augmented by computer generated perceptual information ideally across multiple sensory modalities including visual auditory haptic touch somatosensory sensation and olfactory smell schueffel 2017 the overlaid sensory information can be constructive i e additive to the natural environment or destructive i e masking of the natural environment and is spatially registered with the physical world such that it is perceived as an immersive aspect of the real environment rosenberg 1993 the power of ar according to klopfer and sheldon 2014 is that it enables the user to engage with realistic issues in a context with which they may already be connected i e the user can experience an immersive online experience whether it be for pleasure e g gaming skinner 2020 or education e g training salami et al 2017 daniela 2020 or doing e g maintaining a piece of machinery gupta 2019 ar enables a situation to be created that puts the user in a real world physical and social context while guiding and facilitating participation dunleavy and dede 2014 for example ar technology has the ability to render objects that are hard to imagine and turn them into 3d models then enable interaction with these models making it easier for the user to grasp abstract content it essentially offers a seamless interaction between the real and virtual worlds billinghurst 2002 bach et al 2018 additionally ar accelerates engagement with users and engagement of users with the content as it provides an opportunity to immerse the user in a visualisation whilst they are simultaneously experiencing the observed world environment this is important particularly with environmental management scenarios veas et al 2008 veas and schmalstieg 2013 meaningful environmental modelling with ar kamarainen et al 2018 location based ar to promote learning haynes et al 2018 flood visualisation coen et al 2019 creating pro environmental behaviour with ar because in a training context theoretical knowledge is not enough to obtain proper skills in professional areas and ar in the learning training context enables users to practice and gain hands on interactive experience in their areas of interest ar features can thus help perform virtual practice enabling participants in an ar scenario to acquire both theory and some experience sermet and demir 2020 daniela 2020 this paper describes the development of an app running on microsoft s ar technology the hololens v1 goggles https www microsoft com en au hololens with the aim being to develop a novel intuitive learning tool which could be used in multiple different science based learning contexts to engage students in interactive and useful learning an evaluation of the app addressed the research question is it possible to design and develop an app running on ar technology that is based in system dynamics and which can be used to visualise the invisible in order to engage interest and promote an integrative learning environment in multiple different scientific contexts 2 methodology affluent effluent ar app development the aim of the project was to develop a scientifically robust model of eutrophication that was based on existing research and associated visualisations to underpin the development of an ar app it was envisaged that this app called affluent effluent would be a hypothetical authentic inquiry driven wastewater management decision making system which could be used as a novel learning environment for training purposes in a number of scientific areas for example chemistry maths biology agribusiness a staged technical development of the app for use with microsoft s hololens technology was undertaken via an iterative app design process trialling the final version of the app with students in a number of different stem courses was also undertaken and is discussed in the results section the scenario for affluent effluent was that the user was the owner of small composite fish culture farm their aquaculture business used the pond system for raising fish which has the basic requirement that it be self sustaining as it grows plants and algae for fish food fish are very susceptible to changes in the quality of the water in which they live in other words they are complete captives of their environment they need oxygen to survive and anything that influences the concentration of dissolved oxygen including temperature needs to be managed very carefully furthermore to provide additional scientific complexity to the model the pond in this scenario was being used to manage effluent from a production situation piggery and nutrients such as ammonia nitrate and phosphate need to be monitored daily as these affect the build up of aquatic weeds the objectives of affluent effluent were for the user working in the scenario to be able to i identify the reasons that led them to believe there were issues in the pond ii evaluate potential factors that may have contributed to the health of the fish in the pond iii identify and describe the process behind the actions they needed to take to make sure this situation did not happen again iv be able to create a chart to test record and map the current levels of e g oxygen ph nutrients air and water temperatures rainfall and water turbidity against the optimum levels required to maintain a healthy pond the brief for the ar component of this project was to create a 3d holographic immersive online pond environment with interactive model components that the user could touch and interact with to demonstrate relationships and key processes this involved both above surface and below surface environments graphics that the user would interact with behind the graphics needed to run several biological and chemical models that would enable components of the pond water quality fish plants etc to change over a time stepped series using either real time data ingress from the field gunesekra et al 2018 or from historical databases a simple financial model should also be incorporated to take account of the amount of electricity used when pumps and aerators were used the idea of the ar app was for users to interact with the holographic model in order to pull apart the system and change a number of different variables dependent on the state of the situation in the pond as portrayed by the simulation furthermore users had to be able to make decisions that had real time effect on the model and what was being visually displayed as a result these decisions and their impacts on the variables of the model had to be logged so they could be reviewed after exiting the app for analysis of outcomes and reinforcement of further training if needed 2 1 underpinning model and ar app development there were two stages to the sd model and affluent effluent ar app development firstly a pilot app described in bryceson et al 2018 was developed using semantic systems s system dynamics freeware to create a simple model with basic graphics this pilot app was made available in 2018 to a small group of users via the hololens for trials relating to engagement and learning usefulness this trial was successful with a limited and informal evaluation amongst a mixed demographic of users indicating that they enjoyed the app and were engaged in what it was depicting however they also indicated that a they would like to see a more complex systems model underpinning the final app such that decisions were more complex and linked to more realistic outcomes in order to create deeper learnings and b that the graphics needed to be more professional this paper details the second stage of development to produce the final ar app affluent effluent 2 2 the final eutrophication model used in the ar app the objective of the final eutrophication sd model was to simulate the effects of a build up of nutrients in a pond on eutrophication development and on plankton seasonal dynamics the final version of the model used in the affluent effluent ar app was built using the extant research identified in table 1 the model was first developed in isee systems stella architect software https www iseesystems com and then transferred into c and unity https unity com as the game engine for visualisation and driving the app on the microsoft hololens the final sd model was arrived at after many design and development iterations including testing within the hololens environment in relation to the development of useful visualisation of the key issues for users to interact with the final model was eventually made up of seven modules that represented i phosphorous generation ii nitrate generation iii ammonia generation iv photosynthesis v dissolved oxygen vi the pond lake and vii self shading the attenuation of light entering the water by algal blooms with impact on littoral plant health and oxygen availability the seven modules are interconnected and affect one another via the interplay of variables within the modules key drivers in the overarching system dynamics model include i weather rainfall evaporation water temperature water use and sunlight intensity which when not overridden by the user is modelled within a southern hemisphere seasonal pattern ii lake depth which is driven primarily by the weather variables and includes an inflow of water from rainfall and runoff and an outflow due to leakage and evaporation iii nutrients phosphorous nitrate ammonia all of which are impacted in the same way by runoff due to rainfall algal decomposition water use by irrigation and algal growth and user defined increase decrease in number of pigs iv algae growth and death modelled as a single organism cyanobacteria with no competition between species for simplicity table 2 shows various factors involved in the modelling of the growth and death of algae v dissolved oxygen fish health is linked to available dissolved oxygen fish will die when dissolved oxygen drops below 6 mg l additionally fish energy swim speed in visualisation varies when oxygen is between 6 mg l and 12 mg l table 3 shows the factors impacting dissolved oxygen vi the bank which tracks income associated with fish and expenses associated with managing the pond this is a simple cash tracker based on fish numbers and expenses to run the aerator and irrigation pump for which there is a specific turn on my aerator irrigation pump action in the app and costs money as opposed to water usage abstraction which was built into the system and is free in terms of cost to use the final sd model had 36 basic variables and 22 converters that were kept track of and reported on figs 1 3 represent the seven modules as a series of system dynamics stock and flow diagrams which show the structure of each system with its constituent components and associated interactions stocks are components that can accumulate in the system for example in fig 1 nitrate and flows are components that make stocks increase or decrease for example in fig 1 nitrate inflow nitrate outflow in figs 1 3 the variables in each module are named in common english and the direction of model flow is shown by the arrows fig 4 identifies the interconnectedness between the modules of the sd models schematically note while the overall narrative for the ar app scenario development involved management of an on farm water body being impacted by effluent overflow from a nearby piggery the sd model itself does not know about pigs it only knows about phosphorus ammonia and nitrate the model has been constructed such that in the app pigs can be added removed by the user as a means of increasing decreasing nutrient levels in the water with average nutrient concentrations pig being taken from australian pork industry research on the level of nutrients in manure apl 2015 thus to provide a change in the amount of phosphorus ammonia and nitrate an independent variable representing the number of pigs pigpopulation as inputted by the user and which impacts phosphorusadded 1800 g pig day ammoniaadded 4050 g pig day and nitrateadded 450 g pig day was used this is also true of oxygen injection the aerator itself does not come into the lake model the amount of oxygen does irrigation usage again the model knows about the water used for irrigation not the pump itself the reasoning behind these distinctions was in order to separate the eutrophication model itself from the human factors that could be changed for example by changing pigs to cows or even to humans the underlying model would remain the same just the nutrient inputs would change the same applies to irrigation and aeration 3 implementation of the stella model in unity unity www unity com was selected as the game engine due to its broad platform supporting capabilities including a mature set of features and assets being available for use making it ideal for this visualisation on microsoft s hololens unity uses c as its scripting language and as such development work was required not only to provide an immersive experience for the users but also to simulate the eutrophication cycle the next section discusses the implementation of the system dynamics model in c 3 1 variables constants and units of measure variables and constants in the model were stored in class fields using value typed structuring microsoft 2020 to represent specific units of measure units of measure used in this app include area celldensity concentration concentrationflow currency currencyflow distance irradiance liquidflow liquidvolume mass massflow period speed temperature volume and volumeflow these units of measure incorporated operator overloading programis 2020 to make it possible to perform mathematical operations on them while ensuring the preservation of the unit of measure or enabling change if required for example dividing mass mg by liquidvolume l returns concentration mg l additionally using these strongly typed structures prevents accidental errors for example using such a structure it is not possible to subtract distance from currency the full implementation of units of measure can be found in the accompanying source code two examples are provided to illustrate i a code for the mass class a concept in the code called mass which has its own set of behaviours and actions in this case knowing how to add multiply divide and convert a mass which is shown in code listing 1 and ii a 1 line piece of code that calculates the algal daily growth rate is shown in code listing 2 code listing 1 code file for the mass class one of the units of measure image 1 code listing 2 an example of using the units of measure to calculate the algal daily growth rate image 2 3 2 data structures several data structures were used to split the model up into individual components and separate their concerns 1 complexwastewatermodel the primary interface to the data in the simulation including initialvalues complexmodelconstants complexmodelframes and a series of user specific independent variables such as pig population rain amounts etc 2 initialvalues initial values for the stocks in the model water nitrates ammonia phosphorus cyanobacteria oxygen concentration and bank balance 3 independentvariables independent variables in the model most of these are customisable by the user either directly or indirectly via the values in the complexwastewatermodel for example ammoniaadded is calculated as ammoniaperpig pigpopulation 4 complexmodelconstants constants used in this app 5 complexmodelframe a snapshot of the model at a particular t value contains the frame number number of timesteps elapsed day number independentvariables as well as subframe for each part of the model 6 subframe each subframe contains a snapshot of the variables for each part of the model lakeframe nitrateframe ammoniaframe phosphorusframe biologyframe oxygenframe bankframe fishframe please also refer to appendix 1 equations used in the model and appendix 2 constants used in the model 3 3 calculating the next time step for the initial frame t 0 the complexmodelframe is simply derived from the initial conditions see complexmodelrunner initial subsequent timesteps are done through complexmodelrunner stepforward which takes in as arguments the previous complexmodelframe independentvariables and the delta time in days from these it generates a new complexmodelframe with the system dynamics calculations applied to create the new frame it first calculates the stocks code listing 3 and then derives the transient flow values code listing 4 once a frame is calculated it is pushed into the complexwastewatermodel and is used as an input to the next frame code listing 3 phosphorus calculations stocks image 3 code listing 4 phosphorus calculations transient flow values image 4 note these calculations correspond closely to the system dynamics modules in figs 1 3 additionally the units of measure are used extensively in these calculations providing additional safety 3 4 passing of time and time travel in order to provide a feeling of the passage of time to the user and to keep them engaged the simulation had to be run at a greatly increased speed between 5 s per world day to 1 s per 5 world days by doing this a user can quickly experiment with different input values to the model and see the impact it has on the system to see a situation unfold slowly the user can slow down the progression of the simulation and even pause it for a closer analysis similarly when a user makes a decision that results in a negative effect from a management perspective it is valuable for them to be able to go back and tweak their inputs to achieve this time travel was provided which reverts the model to an earlier state this provides a safe consequence free avenue for experimentation and exploration which is not otherwise possible the wastewatermodelcontroller is responsible for the passage of time and time travel ensuring the simulated model has capability of including updated user inputs and is progressing at the proposed time to do this elapsedwalltime wall time is time as determined by a chronometer which differs from time as measured by counting microprocessor clock pulses or cycles is kept in sync with the computer s clock elapsed time when not paused as time progresses this value is compared against a scaling factor depending on the play speed for example 1 world second per 2 simulation days and if it goes past the threshold to calculate the next frame it does so by calling into complexmodelrunner stepforward 3 5 model outputs and interpolation given the large amount of wall time between timesteps as slow as 5 s per simulation day depending on the user s speed configuration smoothing was needed to ensure visuals changed gradually instead of jumping between the values in one day to the next linear interpolation smid 2007 between frames achieved this with the interpolation factor being the percentage between the simulation days see code listing 5 for an example code listing 5 interpolation of dissolved oxygen between 2 simulation frames a and b image 5 this was only done for variables that required visualisation for example the phosphorus limiting factor was not going to be visualised so was not interpolated to save on computational complexity 4 visualisation design and development of 3d visualisation assets to depict aspects of the stella modules that could not be seen for example the phosphorous and nitrogen cycles and photosynthesis algal bloom development and littoral plant growth death was critical to providing the authentic immersive experience bach et al 2018 that was required in the ar brief visualisation in this case was the creation of 3d holograms for the hololens which is a head mounted display unit that projects stereoscopic images of virtual content into a user s real world and allows for interaction in situ at the spatial position of the 3d hologram co design also known as generative design co creation participatory design or co operative design ncoss 2017 principles were used in the design of the visualisation assets users and developers of the app were encouraged for ideas on how to visualise the various components of the underlying models for example dissolved oxygen or the processes of eutrophication itself the ideas generated during discussion were then assessed and experimented with as to how well they produced a visualisation for users table 4 lists the key 3d holograms that were eventually agreed to and used a limited budget resulted in the choice of assets being sourced through the unity software asset shop https assetstore unity com 3d 4 1 visualisation examples this section provides examples of the model visualisations for various states of the system note the visuals are shown here with a plain black background to make the detail easier to see on the hololens the visuals are integrated with real world surfaces such as the room floor and tables fig 9 4 1 1 pond cross section the pond cross section can only be seen in design mode and displays a summary visualisation of the pond with exaggerated scale to allow for easy viewing there are a set of controls along the bottom of the cross section allowing control of environmental factors evapotranspiration rain runoff sunlight and water temperature fig 5 a is a cross section of a healthy pond the aerator is on the plants are healthy algal levels are low and the fish is alive in fig 5b increasing the pig count causes the water to become dirty representing an influx of nutrients from the runoff here the aerator has also been turned off in fig 5c high rainfall shows as raindrops falling onto the cross section white flecks on black background and in fig 5d the pond is in poor health the fish are dead algae is in full bloom and the littoral plants are no longer viable fig 6 shows the buttons on the bottom of the cross section that users can use to change variables fig 7 shows a lab report that can be obtained by tapping on the conical flask icon when the user is above water 4 1 2 the plinth the plinth is visible in both design and immersive mode and is the main control console for the app this is where the user can see a useful summary of the model state and control the simulation flow and some inputs fig 8 a shows the front view of the plinth while the pond is in poor health the warning light is on and the mini charts show low oxygen high algae low visibility and medium nutrient availability the time travel left dive in middle and play right controls are available here fig 8b shows the right side of the plinth where the nutrient influx pig population numbers and income values can be altered fig 8c shows the left side of the plinth where control of the irrigation pump abstraction and the aerator is undertaken 4 1 3 underwater visuals when in immersive mode the room is transformed into an underwater style look and feel with fish plant life bubbles and algae shown at a realistic scale and responding to pond conditions fig 9 fig 10 a shows the immersive view of a healthy pond high oxygen concentration low algae density and plants fully satisfied with nutrients and light fig 10b shows the immersive view of a pond in poor health high algae density low nutrient concentration and low light satisfaction for the plants and low oxygen availability for the fish these figures also include mood emoji that bubble up from the fish to give an indication of oxygen satisfaction fig 11 shows littoral plant assets and fig 12 high algal density and dead fish 4 2 visualisation implementation the underwater littoral plants figs 10 and 11 were 3d models that appeared underwater and changed their appearance based on the nutrient cycles plants were green and moving with a wavelike motion when healthy appropriate levels of nutrients brown and static when dead excess nutrients when health of the water body is restored the plants return to life so users can always have an idea of the nutrient and light satisfaction by looking at the plants the visuals were implemented using a basic lambert diffuse shader with an alpha cut off and tint that changed depending on the nutrient satisfaction fish fig 10 were rendered as 3d models that swam around in the user s field of view using a boid pattern reynolds 1987 for realism the swim speed was governed by the amount of dissolved oxygen and there were particle systems attached to govern the bubble rate and smiley vs angry emoji when the oxygen was too low the fish died as they do in nature with the smaller ones dying first followed by the larger ones boyd et al 2018 mdba 2019 o2 bubbles figs 10 and 11 were particle systems attached to the plants and represent the dissolved oxygen increase due to photosynthesis surface algae figs 5d and 12 was implemented as a basic quad rendering on the ceiling with a custom shader that blended in a cloud like texture depending on the algal concentration floating algae figs 10 and 12 was a large particle system that spawned larger and more frequently when there were high levels of algae in the system brightness of the lighting subtly changes as the light penetration visibility decreases to simulate the pond being darker this was kept subtle to capture the emotion of things becoming darker and less friendly while still making it easy to see 4 2 1 mapping outputs to visuals after calculating the final model values the visuals were rendered rendering or image synthesis is the process of generating a photorealistic or non photorealistic image from a 2d or 3d model by means of a computer program the resulting image is referred to as the render multiple models can be defined in a scene file containing objects in a strictly defined language or data structure the scene file contains geometry viewpoint texture lighting and shading information describing the virtual scene the data contained in the scene file can then be passed to a rendering program to be processed and output to a digital image or raster graphics image file microsoft academic 2020 the rationale for each kind of visualisation is described in the section below broadly speaking changing the environment to match the model fell into several categories 1 particle emission for the air bubbles happy sad faces effluent output rainfall aerator duty cycle and floating algae the emission rate of a particle system were adjusted based on the related model outputs so more photosynthesis produces more air bubbles for example to address this a monobehaviour which is a base class from which every unity script derives to cover all the particles was created to map the domain of model outputs to the range of particle system modifications code listing 6 code listing 6 synchronizing particle emission with model outputs image 6 2 material and property changes for algae concentration water dirtiness from runoff and fish swim speed linked to oxygen level i e concentration simple scripts were created to modify the specifically required properties these varied from visual to visual but generally involved mapping of a model value to an output range whether that be colour speed density etc and applying it code listing 7 code listing 7 changing the surface algal density by altering the density parameter of a shader based on the bloom amount derived from algal concentration image 7 3 mini chart the mini chart on the plinth shows an easy to read list of the model s most important values oxygen algal density light visibility and nutrient satisfaction this is a simple bar chart which was implemented in unity with basic coloured quads that cut off based on a scalar value code listing 8 code listing 8 implementation of the barrenderer and minichartcontroller to display a mini chart image 8 5 visualisation development challenges with the hololens the hololens is an emerging technology the first consumer level ar headset released and thus there are plenty of limitations and restrictions to implementing an app using this technology these include performance interaction and field of view bach et al 2018 performance delivering 3d apps for the hololens requires a commitment to maintaining a high performance in terms of presentation microsoft recommends running all apps at 60 frames per second at all times as going below this can lead to eye strain unstable holograms and fatigue for the user the hololens is portable and therefore not a high powered machine it contains the processing power similar to what you d expect from a high end phone in 2012 and needs to render at a resolution of 1268 720 per eye whilst continually keeping track of its position and orientation in the real world as a general rule of thumb hololens apps are developed with similar limitations to that of mobile phone application development for example overdraw drawing in the same screen location multiple times is particularly expensive in terms of computing power on the hololens requiring the need to limit usage of transparent objects relying on the additive blending in the holographic display which renders black things as transparent where possible alpha blending the process of combining a translucent foreground colour with a background colour thereby producing a new colour blended between the two was not used even if it meant a loss in visual appeal this is most evident in the user interface buttons where some transparency would have enhanced the visual appeal but ultimately was too expensive computing wise additionally the complexity of shaders used for calculating lighting and other visual effects was found to have a large impact on the low powered cpu of the hololens to that end mobile shaders were used along with simple custom developed shaders when more appealing visual effects were required finally actual polygon count was found to be another limiting factor rendering more than around 400 000 triangles on the screen at one time caused the framerate to drop and simpler models had to be used where possible interaction interactions with the hololens are limited to gestures usually a bloom palm up fingers closed and then opening like a flower blooming or air tap forefinger outstretched and straight making a tap motion dachis 2016 while more complex gestures are possible for example two fingered pinch tap and slide when trialling the software with various people it was discovered that these more complex gestures are not intuitive given the app was designed to be used without training and without much opportunity to practice i e it was not being used daily by the same person interactions had to be kept as simple as possible and were thus limited to the use of the air tap gesture the result of this is that everywhere a user can interact is a button on the plinth it is simple and discoverable but not terribly immersive this is a trade off that we felt gave a better overall result field of view another clear limitation of the hololens is its limited field of view around 35 the visualisations had to be designed around this so that the user could always easily find what they were looking for for example like ensuring the interaction always happens somewhere obvious like on a table or the same spot on the floor to make holograms appear more populated the action in the simulation happened in front of where the user was already looking for example having fish holograms tend to swim in front of the user instead of behind them animation was used to draw attention to off screen elements such as buttons that needed clicking it was tempting to do things in a virtual reality style i e transporting the user into a fully virtual environment underwater in a lake but this limited field of issue as well as the presence of the real world room made this experience feel disjointed playing to the hololens strength the main approach taken was instead to bring the virtual world into the room i e have fish swimming around the room or a plant planted on the floor of the room etc using this approach meant that an entire virtual environment was not being generated with each decision saving precious performance and development time and everything the user wanted to see could be seen at once in the device 6 results operation and use of the affluent effluent app users use the hololens to configure and view the simulation of eutrophication in a closed system aquaculture pond that has water ingress from a nearby pig farm users can experiment with the factors involved in the development of an algal bloom including rainfall runoff evaporation temperature aeration water abstraction irrigation pig effluent input fish stock as indicated in table 4 there are two primary views in the app an above water design mode and underwater immersive mode experimentation in decision making around management of the pond is enabled by tweaking the buttons on the plinth that appears in both the above water and immersive modes similarly the simulation speeds and time travel enable users to go back in time in the simulation to fix up earlier mistakes or try a different management strategy these are controlled through buttons on the plinth as discussed earlier some of the decision making tools enabling users to control variables include the ability to change the number of pigs which equates to the amount of nutrient pig added the ability to turn on vary and turn off the aerator amount of oxygen view the total dollars in the user s bank impacted by income and expenses display a lab report a real time report to user running the simulation on variables impacting water quality at any point in time while the wearer of the hololens sees a rich view of the pond with fish bubbles plant life and algae reacting to the simulation the app can also be played amongst a group of people who can log on to the accompanying website and view a highly detailed breakdown of the variables in the pond water in real time an example is shown in fig 13 a more detailed analysis can be conducted after the app is completed by using the associated website and downloading the excel spreadsheet containing a full history of the simulation run including time travel resets an example dataset after running the app is shown in fig 14 with this type of dataset available after the running of the app users particularly students can be tasked with an assignment based on the data they have created using the app such a task might be to create graphs to visually illustrate relationships in the data or to look for trends in that data that can reveal relationships between variables and thus potential management decisions that might be made the data collected via the app is comprehensive enough to enable teachers in different scientific subjects to create bespoke assignments based on it for their own subjects and for the user student to see the outcomes of their decisions thereby reinforcing the training and learning 6 1 evaluation this paper describes the development of a model and the ar app associated with it for completeness here a brief overview of the evaluation of its use in five higher education level courses where it was used in practical classes between 2018 and 2020 is presented the evaluation was carried out to answer the research question posed earlier about the role of system dynamics and the value of ar in visualizing the invisible in terms of the app s value as an educational tool this was accomplished via a formal ten item evaluation questionnaire under uq ethics clearance 2018002100 in order to generate quantitative and qualitative data from each of those courses a total of 335 responses were obtained with 186 having all the questions answered these 186 responses were used for calculating descriptive statistics range average median values with a further thematic content analysis also undertaken to determine the pedagogical implications of the data collected the descriptive statistics show overwhelmingly that students were engaged by the technology yes 88 1 and that 87 6 agreed that it enhanced the learning process additionally 72 9 of students felt the right balance was struck between the complexity of the simulation visual indicators and graphical representation comments included it was an immersive learning experience and is more engaging than just sitting and listening graphics were good but gestures take time to learn interesting simulation able to identify interrelationships between multiple factors provides hands on learning overall the majority of comments were positive across all evaluation questions key themes and associated concepts from content analysis included 1 technology augmented reality is different interesting and engaging 2 the model app is a good learning tool lots of different possibilities and courses it could work with users liked the real time 3d visualisations 3 learning tool enables first hand learning learning by doing fun and interactive individuals can control the pace of learning different things in different courses visual learners gain maximum benefit group learning and collaboration that was fun and useful the conclusion taken from this evaluation is that ar is useful for system dynamics and science learning and teaching but that complex design and visuals are required to maintain engagement and deep learning 7 conclusions the current affluent effluent ar app has been successfully built around a system dynamics model and simulation of eutrophication and the associated management scenarios as a tool for multidisciplinary training as discussed in this paper it has involved a more complex wastewater model underpinning the app and a substantial upgrade of graphics compared to earlier pilot versions bryceson et al 2017 2018 as requested by initial users this has meant that decision making by the users of the app can now be more complex dependent on what the user and or trainer would like the user to learn and thus training and deep learning objectives are better met the ar development has provided the means to render processes that are hard to imagine and visualise e g chemical processes into 3d models which have then enabled the ability to see what cannot be seen in real life pogue 2011 daniela 2020 skipper 2020 in this case being the chemical processes in water when eutrophication processes occur these visualisations have also enabled interaction with the underpinning system dynamics model making it easier for the user to grasp abstract content associated with these processes bach et al 2018 and to practice and gain hands on interactive experience in managing them in a risk free environment to facilitate the use of affluent effluent the following resources were developed a professional training video a fully illustrated user guide leigh and bryceson 2018 with instructions on using each feature e g aerator weather information number of pigs in the system bank time travel etc and a series of tasks that could be used as training assessments in different courses or training environments e g ecology biology chemistry precision agriculture agribusiness statistics additionally the industry partner with which the project was undertaken telstra purple https purple telstra com au created a youtube video showing some of the new features during student use which can be viewed here https www youtube com watch v fpkznbkmrj4 it should be noted that since completion of affluent effluent a new version of the microsoft hololens has been released https www wired com story microsoft hololens 2 headset this hololens v2 addresses some of the major shortcomings of the first release primarily that the field of view has been expanded from 34 to 52 enabling a bigger viewing area of the model not enough for a fully immersive vr experience but still a large improvement device comfort has been greatly improved allowing longer play time and less fiddly adjustment on first use there is now fully articulated hand tracking to enable direct manipulation of holograms and more intuitive user experience such as pinch sliders to control input values and finally the hololens v2 has a much improved computing performance enabling better renderings of visualisations while a minor effort has been made to port affluent effluent to the hololens 2 and initial testing shows the experience and immersion to be greatly improved this new version of the software has not been released at this point in time declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we acknowledge significant funding and support from the university of queensland information technology services group uqits and from telstra purple without this funding and support the project would not have taken place additionally we would like to thank mr rob moffat chief information officer uqits and mr gary stefano customers services director uqits for significant organisational support in facilitating access to hardware hololens uq networking facilities and personnel we would also like to thank those individuals who gave of their time freely to engage with us in brainstorming visualisation ideas and providing feedback appendix 1 table of equations as used in the eutrophication model described in this paper appendix 1 name units equation dwater dt m3 waterin waterout evapotranspiration dammonia dt mg ammoniain ammoniaout ammoniaconsumed dnitrate dt mg nitratein nitrateout nitrateconsumed dphosphorus dt mg phosphorusin phosphorusout phosphorusconsumed dalgae dt mg algaegrowth algaedeath doxygen dt mg l oxygenin oxygenout dfish dt 0 or 1 if oxygensatisfaction 0 then 1else if fishadded 1 then 1else 0 dbalance dt fishincome aeratorcosts restockcost fishadded watervolume l water 1000 precipitation m3 rainfall kcatchment area evapotranspiration m3 evaporation ksurface area waterout m3 waterusage waterin m3 precipitation kprec depth m water surface area algaeconcentration mg l algae watervolume selfshadingfactor kshade kshade algaeconcentration underwaterlightintensity w m2 lightintensity selfshadingfactor lightlimitingfactor underwaterlightintensity exp 1 underwaterlightintensity klight opt klight opt xt watertemperature ktemp ktemp low ktemp opt ktemp low templimitingfactor 2 1 kshape xt xt xt 2 kshape xt 1 nitrateconcentration mg l nitrate watervolume ammoniaconcentration mg l ammonia watervolume phosphorusconcentration mg l phosphorus watervolume nitratelimitingfactor nitrateconcentration exp kamm pref ammoniaconcentration nitrateconcentration knit monod ammonialimitingfactor ammoniaconcentration ammoniaconcentration kamm monod phosphoruslimitingfactor phosphorusconcentration phosphorusconcentration kphos monod nutrientlimitingfactor min phosphoruslimitingfactor ammonialimitingfactor nitratelimitingfactor algaegrowth mg algae kgrowth max lightlimitingfactor temperaturelimitingfactor nutrientlimitingfactor algaedeath mg algae kdeath kgrazing kresp ktemp resp watertemperature 20 y 10 kph opt ph 1 phlimitingfactor kph kph y o2temperaturelimitingfactor exp 2 3 xt aeration mg l 0 641 depth oxygeninjected substratelimitingfactor co2 kco2 co2 kt k20 ktoxid watertemperature 20 photosynthesis mg l kgrowth max o2 algaecontentration lightlimitingfactor substratelimitingfactor phlimitingfactor o2temperaturelimitingfactor respiration mg l krmax o2temperaturelimitingfactor algaeconcentration biodegredation mg l kt oxygen kdemand kdo oxygen decomposition mg l koxy decomp algaedeath watervolume oxygenin mg l photosynthesis aeration oxygenout mg l respiration biodegredation decomposition oxygensatisfaction oxygen kfish o2 min kfish o2 max kfish o2 min nitratefromrunoff mg waterin knit conc nitratefromdecomposition mg algaedeath knit decomp nitrateout mg waterout knit conc nitrateconsumed mg kcons algae lightlimitingfcctor temperaturelimitingfactor nitratelimitingfactor kgrowth max ammoniafromrunoff mg waterin kamm conc ammoniafromdecomposition mg algaedeath kamm decomp ammoniaout mg waterout kamm conc ammoniaconsumed mg kcons algae lightlimitingfcctor temperaturelimitingfactor ammonialimitingfactor kgrowth max phosphorusfromrunoff mg waterin kphos conc phosphorusfromdecomposition mg algaedeath kphos decomp phosphorusout mg waterout kphos conc phosphorusconsumed mg kcons algae lightlimitingfcctor temperaturelimitingfactor phosphoruslimitingfactor kgrowth max fishincome fish kfish profit aeratorcosts aeratordutycycle kaerator cost appendix 2 table of constants used in the eutrophication model described in this paper appendix 2 name unit value ksurface area m2 50 000 000 kcatchment area m2 67 000 000 kamm conc ammonia inflow concentration mg l 0 1 kphos conc phosphorus inflow concentration mg l 6 1 knit conc nitrate inflow concentration mg l 0 8 knit monod nitrate monod constant 0 01 kphos monod phosphorus monod constant 0 01 kamm monod ammonia monod constant 0 005 kamm pref ammonia preference factor 1 46 klight opt optimal light intensity w m2 250 ktemp opt optimal temperature c 23 ktemp low low lethal temperature c 5 kshape shape factor for temperature limiting factor 0 7 kgrowth max maximum growth rate day 1 0 95 kgrowth max o2 maximum growth rate for o2 model day 1 2 2 kgrazing grazing rate day 1 0 01 kdeath death rate day 1 0 3 kresp respiration rate day 1 0 1 ktemp resp temperature coefficient for respiration 1 kshade self shading factor mg l 50 kcons algae nutrient consumption factor 0 001 kphos decomp algae mass released as phosphorus for decomposition 0 00095 knit decomp algae mass released as nitrate for decomposition 0 kamm decomp algae mass released as ammonia for decomposition 0 koxy decomp algae mass consuming oxygen for decomposition 0 03 kprec precipitation fudge factor 0 55 ktemp temperature fudge factor 1 6 kfish o2 max oxygen concentration for a happy fish mg l 11 kfish o2 min oxygen concentration for a dead fish mg l 6 kco2 monod monod constant for co2 mg l 0 5 kph ph limiting factor 200 kph opt optimal ph 6 8 kresp max max respiration 0 5 k20 mg l 0 0015 ktoxid kdo mg l 0 1 kdemand chemical oxygen demand 100 kfish profit day 20 kaerator cost day 100 krestock day 1000 
25696,interacting entities in human society and their roles considering land exploration and occupation are crucial for studying land use cover change lucc in landscape ecology for investigation of games in lucc models this work presents an agent based decision model based on evolutionary game theory with three layers of interactive deliberation individual peer to peer and social implemented in mase egti multi agent system for environmental simulation with evolutionary game theory interactions the conceptual model details staip an acronym for cellular space time agents interactions and public policy the theoretical model baseline focus on evolutionary game theory the experiments use data from mapbiomas brazilian public geographic database the results show that agents peer to peer interactions may influence the final land usage and conversion of natural systems we believe the conducted simulations present insights for lucc studies that consider space transformation and population interactions keywords agent based simulation conflict resolution model evolutionary game theory hawk dove prisoner s dilemma peer to peer interactions 1 introduction game theoretic models have been employed in the last few decades to describe many strategic social phenomena not only in economics but also in multiple disciplines such as sociology anthropology and social networks cimini and sanchez 2015 hartshorn et al 2013 phelps 2016 schindler 2012 in evolutionary biology evolutionary game theory egt was defined by the understanding that individual fitness may be related to strategic situations between species with different behaviours and their respective fitnesses maynard smith and price 1973 vincent and brown 2005 weibull 1997 the strategic situations may be understood as games where individuals are players observed behaviours in a population could be strategies in a game and consequences in an encounter between individuals may be seen as their payoffs then evolution emerges by the general outcome of the population repeatedly playing the game since above the average payoff reflects into a better ability to survive and pass certain behaviours down to new generations highlighting a well fit strategy over time vincent and brown 2005 weibull 1997 such social phenomenon is not only applicable in evolutionary biology but many other scenarios including selection and evolution of behaviours in human society since humans are not guided exclusively by rationality but also subjective matters when making decisions mcnamara and weissing 2010 the emergence of many social phenomena could be interpreted as evolutionary games mohd ibrahim et al 2019 human society also faces a great variety of dilemmas in such strategic positions situations where a deviation from efficient collective response to a selfish one may benefit more one of the parts with a variable risk of having losses or being punished for example adami et al 2016 arefin et al 2020 tanimoto 2021 tanimoto and sagara 2007 wang et al 2015 particularly in this work we are interested in investigating the emergence of exploration profiles as a result of resource conflict in land use cover change lucc simulation models the arise of more adapted social behaviours in the population may be linked to landscape transformation in lucc scenarios alongside the particular spatial characteristics and general public policies for resource management bastian 2001 lambin et al 2003 wu and hobbs 2002 in this work we extended the mase bdi simulator coelho et al 2016 that implements cognitive agents based on the belief desire intention bdi model bratman 1987 braubach and pokahr 2012 and public policy layers but lacking social interaction protocols for peer to peer communication and conflict resolution that are necessary for agents strategic profiles and emerging societal configurations now we present the multi agent system for environmental simulation with evolutionary game theory interactions namely mase egti 1 1 mase project website http mase cic unb br software availability https gitlab com infoknow mase mase bdi mase egti a simulation tool with bdi cognitive agents based on egt interaction models developed at university of brasília brazil mase egti includes spatial and temporal aspects and the possibility of public policy usage that gives the user control of land exploration and occupation thus mase egti directly improves the mase bdi simulator with social interactions in the peer to peer intercommunication aspect 2nd level mainly replacing the manager centred conflict resolution system with a new protocol using games for interdependent socialization fig 1 the other two layers rule the internal mental organization of each agent represented by the bdi model 1st level and the public policies stating general regulations for the society 3rd level the main contribution of this work is an agent based decision model set over egt with three layers of interactive deliberation individual peer to peer and social implemented in the mase egti simulator moreover submodels were designed for agent peer to peer interactions with two strategy change decision behaviours reactive and registry based to complement the replication dynamics of the games we also investigated how those behaviours would impact the dilemma s strength tanimoto and sagara 2007 of the referenced game on the agents interaction model mase egti works with lucc models and egt interactions to highlight that different agent interactions and deliberations might influence the land configuration and the general exploration profile in population over time in ecological scenarios the presented work can bring important insights into environmental scenarios with lucc patterns and social stability through socioeconomic activities and interaction behaviours 2 mase egti simulator in this section we present mase egti including the conceptual and theoretical models and the project design 2 1 conceptual model mase egti is an agent based simulator for spatially explicit models with egt interaction sub models the model m can be formalized in a tuple of five main characteristics m s t a i p called staip where s is the cellular space t is time a are the agents i is the interaction model and p is the public policy from the user s perspective the model s characteristics are represented as simulation parameters as presented in the sequence 2 1 1 space space s is a set of cells ce organized in eight connected segments except for the borders that may have three or five segments connected fig 2 each cell has five attributes s pe exploration potential state s e attractiveness s a owner ow and queue of interest il s pe and s e are codependent variables since they represent how much resources an agent can exploit from a cell with s p e n and s e cell states attractiveness is calculated based on the proximity of spatial attributes such as transport routes or aquatic bodies attractiveness is linked to the agent s movement rules since attributes can repel or attract different agent types the value of attractiveness is in the set of real numbers s a r the owner of the cell ow is the agent that is occupying it or has already occupied it at some point in the simulation the interest queue il is a linear synchronized structure that registers agents who are interested in moving to the cell in case it does not have an owner formally we define the space of width n and height m as the matrix s ce 0 0 ce 0 1 ce 0 n 1 ce m 1 0 ce m 1 1 ce m 1 n 1 where each ce i j s pe s e s a ow il is a cell with its own exploration potential state and attractiveness values added to a possible ow owner agent or interested agents in the il queue 2 1 2 time time t is represented by a set of discrete and sequential steps that compose the simulation a step is a time interval of t in which all simulation agents have to successfully fulfil their main objective once then formally t t 0 t 1 t n 1 in a simulation with n steps in which each step is the time that all agents have successfully fulfilled their objectives over and over again fig 3 note that agents have an execution body in which is the main capacity with the main objective these concepts will be explained in section 2 1 3 2 1 3 agents agents in mase egti are based on the bdi paradigm an architectural system that defines mental states for agents decisions considering what the agent knows and wants to achieve bratman 1987 braubach and pokahr 2012 the basic components of the bdi paradigm are the facts that are taken as premises beliefs the states of the world that the agent wants objectives or desires and action plans that can lead agents to achieve their goals intentions or plans of action bratman 1987 beliefs objectives and plans can be organized as capacities structures that represent the different behaviour types the agent can assume braubach and pokahr 2012 agents have execution bodies called an agent body in which deliberation is made about the most appropriate action of its capacities to be performed at the moment algorithm 1 then formally an agent can be represented as 1 a c a a c a b c a α where a is the agent and ca x is the capacity in the agent s body that can be presented as a set 2 c a c s 1 c s n p l 1 p l m o b 1 o b o where cs 1 cs n is the set of beliefs pl 1 pl m is the set of plans and ob 1 ob o is the set of capacity plans in the model m agents a are elements of the set a a 0 a x 1 where x is the number of agents participating in the simulation and a k is an agent that has at least two basic capacities exploration ca e and movement ca m moreover agents have some additional beliefs a unique id id the current cell location where the agent is ce at the renewable exploration credits cr k and a strategic profile ps the ps and cr k beliefs compose the profile of the agent which describes how the agent interacts with the environment and with others agents agents with the same profile may be located in different areas of the space but will behave similarly when exploring and conflicting with other agents the agent s main objective fig 3 is to occupy and extract resources from a cell being part of its capacity ca e this objective is achieved as the agent executes the action plan that uses the credit cr when extracting resources from the cell ce at as shown in fig 4 when all agents of the set a successfully fulfil their main objectives the current simulation step ends a new step can be started and the credits cr k of each agent are renewed this credit renewal allows the agents to fulfil their main objective again at each step of the simulation in algorithm 1 this can be seen by the first condition fulfilment when the agent s credits and the cell s potential are both positive however if an agent has spare credit and its cell does not have sufficient resources to match it the main objective fails and the agent accesses the capacity ca m to change the cell in this capacity the agent analyzes its neighbourhood the set of n adjacent cells ce 0 ce 1 ce i ce n 1 within a radius of view r and accesses their attractiveness values s ai adding all of them to the ss a value 3 s s a i 0 n 1 s a i where s ai is the attractiveness value of the cell ce i section 2 1 1 the attractiveness value of each cell is accessed again cumulatively added and divided by ss a this process generates a probability matrix ranging from 0 to 1 in which agents randomly choose a value the index of the first value of the probability matrix that is greater than the chosen random value corresponds to the cell that the agent will try to conquer after choosing the intended cell ce intended the agent registers its plans of moving by putting his agent id in the queue of interest il of the ce intended if there is no other agent id in the queue of interest il of the ce intended then there s no resource conflict so the objective of the capacity ca m automatically succeed and the agent becomes the owner ow he then moves to this new location and returns to fulfilling his primary objective until its credit cr k is depleted otherwise the agent must resolve the conflict with the other interested agents of the ce intended using the current interaction model that considers the agent s ps strategic profile to decide who will be the owner of the resource fig 4 this process will be detailed in section 2 1 4 the movement and the conflict resolution is represented by the second and third conditions in the deliberation method of the agent in algorithm 1 finally if the agent depletes its exploration credits cr k it will finish its actions for the current step and it will wait for the other agents to finish as well final else condition in algorithm 1 algorithm 1 the internal deliberation for the agent s next action image 1 2 1 4 interaction model the interaction model i consists of an egt game g and one of the decision algorithm defined in mase egti reactive or registry based all agents who need a new cell register their interests in the cell queue il the agents resolve the conflicts two by two in the queue the first resolves the conflict with the second the winner settles with the third in line while the loser looks for another space the winner of this second conflict settles with the fourth in line and so on until only one agent stands and becomes the owner ow of the cell the conflict is resolved based on the chosen egt game the chosen game g must consist of m strategies and the associated payoff matrix with m 2 values that represent the profit or loss of an encounter between two of the m strategies the ps belief which is the strategy the agent uses section 2 1 3 must belong to the set of m game strategies about interaction model i therefore when two agents a i and a j conflict they use the payoff matrix to determine what are their respective v i and v j profit or loss values the agent with the highest v v i v j wins the conflict and in the event of v i v j the winner of the conflict is chosen randomly the decision algorithm is represented by the exchange of strategies after a conflict situation the outcome of the game can influence the ps belief of the agents involved besides determining the winner of the cell disputed by two or more agents the strategy change can be of two behaviours reactive or registry based in the reactive behaviour the losing agent of a conflict changes the strategy to which he would have a preferably better or at least equal payoff if he found the strategy that won him again in the registry based the agent records all payoffs received from conflict situations adding them up after a conflict he changes the strategy to the one that is currently the most advantageous or has the largest sum since the beginning of the simulation 2 1 5 public policy a public policy in the staip model is linked to space s and determines special areas that influence the exploration movement and occupation of agents the public policy determines polygons that divide s into three use categories restricted diversified and encouraged these categories generate sets of cells that have an associated attractiveness multiplier the public policy p is formally defined as 4 p c a t 0 c o n j c e a a c e a b c e z z m u l t c a t 1 c o n j c e a a c e a b c e z z m u l t c a t 2 c o n j c e a a c e a b c e z z m u l t where cat x is the category of public policy conj is the set of cells encompassed and mult is the multiplier that affects the attractiveness of the cells encompassed in conj 2 2 theoretical model mase egti interactions are based on egt games as a representation of resource competition in social activity among agents seeking spatial assets for exploration section 2 1 4 resource competition occurs because cells may not be explored by more than one agent per simulation step resulting in mutual exclusion between peers and conflict successful cell acquisition after a conflict may increase the winning agent s fitness in the simulation context while decreasing the adaptability of the losers who may change strategies to a more suitable one thus an egt game must be defined as a set of rules for population characterization and conflict resolution to select more fitted individuals for exploring the space and a decision algorithm must be chosen for population evolution promoting social transformation both game and decision algorithm influence the population final profile and consequently the resultant simulated landscape in section 2 1 4 it was mentioned that in the staip model the interaction model was composed of a game g with m strategies alongside one of the defined decision algorithms the generic form of the game is described in section 2 2 1 with the replicator dynamics being correlated with the decision algorithms in section 2 2 2 2 2 1 the static game games in game theory and egt are strategic situations where individuals players or agents may perform a possible action strategy or behaviour that results in a outcome payoff that depends on the collective actions of all players the payoff in egt is also a measure of the fitness of an individual in strategic scenarios the greater is its payoff in comparison with the group s mean payoff the better its adaptability studies on stability equilibrium payoff maximization or loss minimization for example rely on these elements in many different games in theoretic and real life scenarios weibull 1997 in mase egti the game g is part of the interaction model and its payoffs are the rules used for conflict resolution between agents for simplicity consider that g has only two strategies i and j two player game and payoffs that are represented by the function u on table 1 also let n be the total amount of agents in the simulation with n i and n j being the numbers of agents with their belief ps i j respectively the variables η i n i n and η j n j n are the proportion of agents with their ps equal to i and j respectively with η i η j 1 the expected payoff u for the agents is not only dependant on their strategies beliefs but also on the probability θ of the occurrence of encounters in the space with 0 θ 1 with θ 0 when the agents are far enough away from each other with enough resources and θ 1 when there s very little available space left so all agents have to compete over a small number of cells since agent movement is probabilistic and based on space availability and the cell s attraction values θ is empirically determined so the expected payoffs u i and u j can be expressed as equations 5 and 6 5 u i θ η i u i i η j u i j 6 u j θ η i u j i η j u j j 2 2 2 the game evolving when losing a conflict agents have the opportunity to update their ps beliefs into a more suitable strategy eventually diminishing or increasing the number of individuals playing i or j in the population n the dynamic replicator equation defined by hofbauer et al 1998 was used to evaluate the population evolution over the simulation steps 7 n i n i u i u i j 8 n j n j u j u i j where u i j is the mean payoff of the population expressed by 9 u i j η i u i η j u j dividing both sides of equations 7 and 8 by n then substituting u i j by equation 9 the following is obtained 10 η i η i η j θ η i u i i u j i η j u i j u j j 11 η j η i η j θ η i u j i u i i η j u j j u i j now with the definitions of equations 10 and 11 consider the two decision algorithms defined in mase egti the way agents decide their next strategy may impact the final population proportions η i and η j of i and j agents and since their ps belief is correlated with their cr k exploration credit section 2 1 3 it also might impact on the final simulated landscape state as a tool for further analysis on the agent s preferred strategy we also consider the concept of dilemma strength referenced in the works of tanimoto and sagara 2007 and wang et al 2015 considering a parallel between the prisoner s dilemma and the generic game presented in table 1 let u j j p u i i r u i j s and u j i t on those works d g t r u j i u i i is the dilemma potential for solutions where a gambling response is inclined while d r p s u j j u i j is the risk averting inclination it s is possible to describe a game s dilemma existence in terms of these by these two static parameters alone arefin et al 2020 tanimoto 2021 tanimoto and sagara 2007 2 2 2 1 reactive behaviour the reactive behaviour for strategy change was designed as a variation of the tit for tat strategy where the players mimic the strategy of the opponent in the last round in a reactive way weibull 1997 the difference is that in reactive behaviour when an encounter occurs the loser counterpart of the resource competition changes its strategy to the one that could have a better chance to win if it encounters the winner again in the future to illustrate this consider that in the game g j the only strategy that has chances to win both against i and j so either way u i i u j j or u i i u j j but always u i j u j i as mentioned in section 2 1 4 the agent with the highest payoff in a conflict wins the resource and in case of a draw they decide who is the winner randomly with equal chances so in this scenario an agent has 50 of a chance of winning against another one playing the same strategy however j always wins against i table 2 presents the probability ρ x x of the agent 1 staying in its strategy x after encountering with agent 2 if the strategy y with 0 ρ 1 since i has a 50 chance of winning a conflict against another i it has 50 of remaining being an i since the loser must change its strategy to j to have a greater chance of winning against another i on a possible next conflict however i has no chance of remaining an i after encountering an agent j since it ll always lose and will have to convert to j changing the perspective to j it ll always remain as a j since it ll win every conflict against an i and even if it loses to j it ll remain a j to have a chance to win in another future conflict adapting equations 5 and 6 to refer to the value of ρ instead of the value of u then substituting the values of table 2 on these equations and finally applying their results in equations 10 and 11 would give 12 η i η i η j θ η i 0 5 1 η j 0 1 η i 0 5 η 2 i η j θ η 2 j η i θ 13 η j η i η j θ η i 1 0 5 η j 1 0 η j 0 5 η 2 i η j θ η 2 j η i θ which highlights that the η i proportion of the population will decrease over time consequentially increasing η j depending on the probability of conflicts and the current proportions of the population this implies that a game that has two strategies where one of them has an equal or greater payoff than the other u j j u i j and u j i u i i is converted to a trivial game when agents behave reactively thus a game g that may have a dilemma and is originally set to be used as a rule for conflict resolution may be reduced to a trivial game when agents are set to be reactive in their interactions with each other this decision making behaviour could model a society where the best social payoffs are preferred in the population excluding agents that receive less than the average individual payoff in the game until their total extinction from the simulation 2 2 2 2 registry based behaviour the registry based behaviour was designed as a counterpoint to the reactive one agents using this algorithm as a method of strategy change keep a summation of all of the payoffs received from conflict resolution on internal registers for each possible strategy when competing for resources and with their ps belief equals to i for example the agents i sum all u i i and u i j received in the register r i after each conflict winning or not then the agents who lost the conflict check their registers to measure which strategy is doing better overall since the beginning of the simulation finally they change to that better suited strategy instead of immediately changing to the one that could win the next conflict as described in the reactive behaviour for example if an agent i loses a conflict it adds the received payoff to its internal register r i then it checks if r j r i if this is true then it changes its strategy to j if it isn t it keeps playing i the same is valid if the agent was playing j adjusting the proper verification of its registers to analyze this decision change consider the same game described in table 1 but instead of describing the possibility of encounters as θ like in equations 7 and 8 consider that α and β are the more specific probabilities of an agent i encountering another i and i encountering another j respectively also consider that γ and δ are the probabilities of an agent j encountering i and j encountering j so the internal registers r i and r j for the very first step of the simulation are 14 r i 0 α η i u i i β η j u i j 15 r j 0 γ η i u j i δ η j u j j and after n simulations steps the overall registers of the game are 16 r i n 1 s 0 n 1 r i s 17 r j n 1 s 0 n 1 r j s for simplicity from now on r i n and r j n will be referenced as r i and r j after a conflict loss and adding the received payoff to the respective register the agent chooses the strategy x i j that have the greater register value r x max r i r j even though α β γ and δ are more specific than θ they also depend on the model s spatial attributes the spatial density of the agents and the number of available spatial resources because of this they are also empirically determined nonetheless it is possible to analyze agent gains based on their registered states consider again an agent with their belief ps i after ending a conflict it will only keep playing i if r i r j and will change if r i r j symmetrically if the agent was playing j and its register s state is now r i r j then the agent will change strategies it will keep playing j otherwise in both scenarios there s a possibility of the increase or decrease of the population of i and j consider the function ξ that empirically measures the probability of one of the registers being greater than or equal to the other after a given two agents encounter equations 18 and 19 synthesize these relationships in terms of η i and η j 18 η i η i ξ r i r j i i η j ξ r i r j j i η i ξ r j r i i j 19 η j η j ξ r j r i j j η i ξ r j r i i j η j ξ r i r j j i these equations show that to be there a positive gain in a population the register for that population strategy must be greater than the others population strategy if that happens for the agents that entered in a conflict in a simulation step not only the agents that were playing the better strategy will keep playing that one but also the other players will consider changing their strategies increasing the more adapted population for example if r i r j to some agent in the nth simulation step the following is true r i n 1 r j n 1 s 0 n 1 r i s s 0 n 1 r j s α 0 η i 0 u i i β 0 η j 0 u i j α n 1 η i n 1 u i i β n 1 η j n 1 u i j γ 0 η i 0 u j i δ 0 η j 0 u j j γ n 1 η i n 1 u j i δ n 1 η j n 1 u j j since the values of u x y for x y i j don t change over the simulation but the values of α β γ δ η i and η j are all mutable at every simulation step some conclusions may be drawn from the population dynamics as follows u i i u j i with d g 0 or u i i u j i with d g 0 and u i j u j j with d r 0 since the beginning of the simulation which means that i strategy gives better payoffs than the average when encountering other strategies than its counterpart because of that r i r j holds over the simulation steps also since d g 0 and d r 0 this possibility converts the game g into harmony or pure cooperation where a social dilemma practically doesn t exists tanimoto and sagara 2007 y c d α y y c d γ y with d g 0 or y c d α y y c d γ y with d g 0 and y c d β y y c d δ y with d r 0 where c d 0 n 1 this means that between steps c and d the rating of encounter of agent playing i was more probable than the others playing j while the agent played i creating a bias in the respective register that means that the summation for the strategy i was secured as a better suitable one over the time making the agent decide that one was suitable for an next simulation step as well this also means in this case that this situation is similar to the previous one with d g 0 and d r 0 the social dilemma practically doesn t take place tanimoto and sagara 2007 y c d α y y c d γ y with d g 0 or y c d α y y c d γ y with d g 0 and y c d β y y c d δ y with d r 0 where c d 0 n 1 this is the equivalent situation to the previous one but with the registers in reversed bias indicating to the agent that the j strategy is better in this case however d g 0 and d r 0 creating a situation similar to the prisoner s dilemma where j exploits i tanimoto and sagara 2007 α k η i k u i i γ k η i k u j i or β k η j k u i j δ k η j k u j j with d g 0 or d r 0 for some k 0 n 1 this means that in some simulation step the quantity of positives payoffs from positive encounters the agent had while playing i surpassed the positive encounter while playing j achieving a temporary stability until the nth step the parameters d g 0 or d r 0 may characterize the situation as pure cooperation or as some known dilemma such as stag hunt d g 0 and d r 0 for example α k η i k u i i γ k η i k u j i or β k η j k u i j δ k η j k u j j with d g 0 or d r 0 for some k 0 n 1 this is the reversed configuration of the previous situation but towards j in this case the parameters d g 0 or d r 0 always configure some sort of social dilemma as demonstrated by equations 10 11 18 and 19 agent positioning is important for the simulation and influences the evolution of the agent population by creating situations of dilemma or harmony since agents have their strategy profile bound to their exploration credits it might change the final observed simulated landscape this is especially relevant for the registry based decision algorithm once the encounter rate for some strategies may expand the difference between the agent s registers making a strategy seems better than the other a similar concept was explored by de andrade et al 2009 where agents were motivated to play mixed strategies to occupy cells in space eventually reaching populations profiles near to the nash equilibrium the two main differences were in their work conflict losers were eventually eliminated from the simulation while in mase egti they convert to the more suitable strategy according to the current decision algorithm 2 3 project design mase egti is the enhanced version of mase bdi coelho et al 2016 in both simulators agents have at least two capabilities exploration ca e and movement ca m as described in section 2 1 4 there are two extra capabilities that mase egti agents implement conflict resolution ca cr and strategy change ca sc capacities mase egti was built with java 7 and jadex 2 4 agents are defined as classes with annotations from the framework and are instantiated on the jadex platform an environment that allows them to have a well defined life cycle communication and autonomy braubach and pokahr 2012 mase egti is a freeware software and its code is publicly available at https gitlab com infoknow mase mase bdi mase egti 2 3 1 architecture the mase egti s architecture is divided into three layers user interface control and simulation fig 5 the user interface allows the configuration of the models the initialization of the simulation and the quick visualization of the state of space throughout the execution it is possible to start the tool either through the graphical interface or via the operating system terminal the control layer has three main components configuration loader simulation control and statistics collector the configuration loader parses the user s settings recording the initial simulation variables such as the number of agents and the conflict resolution model the loader also initiates variables pertinent to the common belief database a module belonging to the lower layer the simulation control instantiates the simulation the platform and the agents based on the settings and has synchronization mechanisms for starting and ending steps based on the states of the agents the simulation layer has two main modules the common belief database and the jadex platform where the agents reside in the common belief database are the variables that agents can query and update the platform is an object of the jadex library where agents are instantiated and can perform their actions also move through their states of life and communicate with each other on the platform there are agents capacities with sets of objectives plans and beliefs in which the agents cognition and actions are organized 2 3 2 agent interaction protocol during the simulation resource conflicts are almost unavoidable since agents cannot perceive if other nearby agents intend to move to the same cell therefore there is a cell acquisition process implemented in mase egti that is associated with the interaction model including five phases target cell decision interest queue registration waiting for the movement of other agents winner decision and cell change this process is organized in an agent capacity the conflict resolution capacity ca cr when the main objective of exploring the cell fails before trying again the agent needs to move to another space the agent analyzes its neighbourhood as described in section 2 1 3 obtaining the sum of all adjacent cells attractions ss a equation 3 and choosing the next cell to conquer ce intended as soon as the agent decides the cell ce intended it proceeds to the second phase registering its interest in the queue il then it accesses the cell s synchronized method that captures the agent s id and places it at the end of the queue the synchronized method prevents concurrent writing ensuring that each agent interested in the cell enters it in a consistent order the agent waits for other agents who need to move to register in the respective cell lines of interest the fourth stage begins with the winner s collective decision the agents analyze the respective queue in which they registered interest if the queue has more than one element a loop starts as presented in algorithm 2 the first agent is considered the temporary winner while the second is the opponent then the winner sends a message with two values to the opponent his ps1 strategy and a b1 bet bet b1 is a random number between 0 and 1 the opponent receives the message and before opening its content it generates a b2 bet also randomly generated between 0 and 1 the opponent then receives the winner s ps1 and b1 values and analyzes considering the current game his own ps2 strategy and his bet b2 considering again that the payoff functions u a b returns the value of the payoff that the agent with strategy a receives when it conflicts with the agent with the strategy b we have four possible scenarios with two results 1 u ps1 ps2 u ps2 ps1 the winner remains victorious and the opponent becomes loser 2 u ps1 ps2 u ps2 ps1 the opponent becomes the new winner and the old winner becomes the loser 3 u ps1 ps2 u ps2 ps1 and b1 b2 the winner remains victorious and the opponent becomes loser and 4 u ps1 ps2 u ps2 ps1 and b1 b2 the opponent becomes the new winner and the old winner becomes the loser the opponent regardless of having won or lost the conflict responds to the message with four values who was the winner who was the loser what is the payoff value u ps1 ps2 and what is the payoff value u ps2 ps1 then the two agents involved in the conflict access their fourth capacity the strategy change capacity ca sc in this capacity they update their ps beliefs according to the decision algorithm chosen in the model and the payoffs calculated in this recent conflict finally the loser of the conflict looks for another cell while the winner returns to the starting point of the decision loop checking if there s more than one element in the queue if there s not the loop ends and the winner becomes the owner of the cell the last phase of the ca cr begins as soon as the winner s decision loop ends the winning agent changes its location belief to the current cell and its goal of exploring cells is resumed in the meantime the losing agents return to the first phase of the ca m capacity to find a new cell to explore algorithm 2 collective decision algorithm for cell s winner image 2 2 3 3 interface the mase egti interface can be graphical or by commands in an operating system terminal fig 6 in the graphical part space and agents occupation can be seen along with the simulation steps the colours are related to agents strategic profile ps it can see also some quick visualization of the simulation results as graphs on the lateral side of the interface as well as messages from the java virtual machine output alternatively the user can run mase egti from the command line with the java jar mase jar call this can be useful for carrying out large volumes of simulations for statistical analysis the simulation and graphics are not shown live to the user but the results are written to a file at the end of the steps to be analyzed by the user 3 related work there are several works and tools involving the areas of environmental modelling agent based simulation bdi agents and interactions based on egt with any combination of elements and concepts relevant to these aspects 3 1 terrame terrame was first introduced by carneiro 2003 when it was based mainly on cellular automata improving gradually as a programming environment carneiro et al 2013 which is currently in version 2 0 1 de andrade 2020 terrame model includes i anisotropic spaces spatial configurations that use natural spatial attributes and patterns of anthropic movement and ii hybrid cellular automata an abstract model intended for systems that have continuous and discrete characteristics terrame integrates a gis database for the contextualization of simulated models allowing its development environment with a modelling language based on lua ierusalimschy 2016 this programming environment was extended to integrate an egt module for the agents common resource competition de andrade et al 2009 however that work does not use lucc models for validation but explores different perspectives of the chicken game similar to the hawk dove game agents compete for spaces on a grid increasing or decreasing their overall satisfaction through local payoffs that result from conflicts during the simulation new agents are not created but agents with low overall satisfactions leave the simulation grid the purpose of the experiments is to find the agents configuration that their satisfactions are sufficiently stable so there is no need to change spaces achieving a balanced situation mase egti and terrame are similar performing simulations of spatial models for lucc with an agent based approach and interactions based on egt terrame also stands out for having a platform for the construction of spatially explicit models with the possibility to integrate georeferenced data however mase egti implements bdi agents and public policies in lucc scenarios with a three layer interactive deliberation between agents focused on conflict resolution for spatially explicit models 3 2 abed izquierdo et al 2019 presents an abed agent based simulation tool with egt interactions to assist the observation of properties arising from the interactions in different scenarios and models the main objective is to facilitate the study of classic game variations and the introduction of new strategic situations with several model configurations agents and games abed is divided into two models abed 1pop simulation of one population of n individuals playing mixed strategies from a pre configured game and abed 2pop simulation of two populations each playing pure game strategies the game can be chosen from a list of pre configured classic games such as hawk dove chicken game rock paper scissors or with a custom payoff matrix aspects of the replicator dynamics are presented in the simulation configuration i how agents are chosen in pairs to enter into conflict ii what percentage of the players will review their strategy and iii what kind of strategy review will be used among the possible review forms there are reactive methods where the agent decides which is the best answer for the strategy he is facing and probabilistic with different probability distributions considering the accumulated payoffs and the distribution of the strategies in the population mase egti and abed have several points in common as the simulation strategies distribution and the replicator of strategies both have configuration parameters for the initial distribution and the strategy review form without changing the number of individuals in the simulation abed has methods and games with more flexible possibilities than mase egti nevertheless mase egti is directed towards spatial simulations implements bdi agents and performs validation in real models aspects absent in abed 3 3 dynamo franchetti and sandholm 2013 introduced dynamo suite of notebooks for the mathematica software for diagrams generation vector fields and other types of graphics related to egt dynamo generates visual schematics from egt scenario data important for the study of replicator dynamics and evidence of stability and balance in populations the various notebooks bring the possibility of representing games with two three or four different strategies with their respective payoffs and initial distribution the replicator dynamics are configurable by the user who can choose from a list of possible methods the output is a visualization graph of the population evolution over time showing phenomena such as the formation of an ess and the most adapted strategy mase egti and dynamo are similar using egt in multi agent simulation scenarios however their purposes and project conceptions are different dynamo is designed to create graphs using egt calculations for the evaluation and forecasting of a population after n seasons while mase egti uses simulation rules for population evolution in spatial scenarios table 3 presents the related work overview considering five characteristics i tool the tool main features including type technology licence and latest version ii space the use of spatial models considering spatially explicit models space attributes in agent movement and public space with planning policies iii egt the egt games used and replicator dynamics features iv model the model used in the referenced article and if it uses real data and v bdi the use of bdi cognitive agents note that mase egti is the only work that uses public space planning policies real data with the brazilian case and bdi cognitive agents 4 experiments the brazilian case study named cerrado mapbiomas is presented to illustrate mase egti this model is included in the mase egti code available at https gitlab com infoknow mase mase bdi mase egti 4 1 the cerrado mapbiomas model the cerrado mapbiomas model is based on data from mapbiomas an open brazilian digital repository of geographic data with many categories and temporal series on brazilian biomes inpe 2018a inpe 2018b mapbiomas started in 2015 seeking a better comprehension of lucc processes in brazilian natural landscapes and scenarios the collections are updated annually with data that comprehend specific areas in an observed time interval we used collection 4 of cerrado that was released in august of 2018 and rounds data from 1985 to 2018 this model is an evolution of the cerrado df model used to validate mase bdi simulator coelho et al 2016 the biggest difference is the images in mapbiomas s collections are larger in resolution than the ibama s ones used in cerrado df moreover agents in the mase bdi simulator have a restricted interaction model interacting only with a manager agent to conflict resolution finally the mase bdi simulator didn t have the staip model definition section 2 1 however cerrado df and cerrado mapbiomas have some common characteristics the spatial proximal attributes section 2 1 1 the agent cell decision algorithm section 2 1 4 and the public policy section 2 1 5 in the sequence we illustrate the presented staip model 4 1 1 space the initial space was configured to be the cerrado biome in the federal district in year 2000 fig 7 with six spatial proximal attributes water bodies watercourses urban spaces train lines roads and conservation units fig 8 pixel colours determine both the cells exploration potential s pe and their state s e table 4 the proximal attributes in fig 8 are used to calculate the attraction matrix attributing an attraction value s a to each cell the attributes are separated into two sets attractive and repulsive which draw or move away from agents respectively the attraction matrix is calculated by first obtaining the images for the proximal spatial attributes and applying a grayscale transformation afterwards copies of these original images are created transformed by a gaussian blur filter and inverted each copy is subtracted from its original image pixel by pixel and the resultant images of the attractive attributes are summed up finally the resultant images of the repulsive layers are subtracted resulting in the attraction matrix in which the higher the value of the pixel the more attractive space is fig 9 the pixel values are assigned to the cells in the same fashion as the cells are formed setting the attraction value of each cell the attractive attributes are water bodies watercourses roads and train lines the repulsive ones are urban spaces and conservation units 4 1 2 time the mase bdi simulator was validated with the cerrado df case where the starting point was the space in the federal district in 2002 at the time the only open data in ibama was the period comprehending 2002 2008 years ralha et al 2013 coelho et al 2016 to situate the new model in the same context we set the starting time of the model at the space in the federal district in the year 2000 which was the closest point in time at the mapbiomas dataset also the agents were allowed to simulate as many steps as they could to the maximum of 1000 steps so they could explore the maximum available space possible 4 1 3 agents two classes of agents were defined group agents and individual agents group agents represent big producers such as cooperative or agriculture companies while individual agents produce for local markets and subsistence in terms of exploration credits both of them were given arbitrarily 2000 units of explorations credits so it would be easier to register in the simulation how many spaces they were occupying moreover the maximum exploration potential in one cell was 1500 table 4 so they had to change their position at least once per step increasing resource conflicts in the simulation their attached ps strategy is described in the following section 4 1 4 interaction model two sub models were defined for the experiments using games hawk dove and prisoner s dilemma with payoff values described in table 7 those games were chosen since the hawk dove was first analyzed by maynard smith and price 1973 as a representation of animal behaviour when conflicting for resources a similar problem that is explored in this work the prisoner s dilemma was chosen because it s a classic game that offers a good baseline in egt studies for strategy emergence adami et al 2016 these sub models were branched into two other sub models considering one of the decision algorithms reactive or registry based regarding the role assignment group agents have always programmed to play pure hawk on the hawk dove experiments or defect strategy in the prisoner s dilemma individual agents where always set to be doves or cooperate respectively table 5 shows the four variations of the cerrado mapbiomas model based on the interactions sub models and the strategic decision algorithm from now on these variations are going to be referenced by their experiment identifier presented in the last column of the table 4 1 5 public policy the public policy in the cerrado mapbiomas model defines three areas with different usages restricted diversified and incentive use demonstrated respectively as red blue and green areas in fig 10 in restricted use the land exploration is prohibited so the attraction values of the cells in this area are negative in the diversified use any activity is allowed so the attraction values of the cells in this area remain as calculated from the proximal attributes in incentive use agents are encouraged to explore with the attraction values of 10 boost 4 2 results and discussion four experiments were developed with the cerrado mapbiomas model one with the prisoner s dilemma game and reactive behaviour pdr one with the prisoner s dilemma and registry based pdb one with hawk dove game and reactive behaviour hdr and one with hawk dove and registry based hdb table 5 all of them were initialized with an arbitrary number of 300 agents distributed in 50 hawks defect group agents and 50 doves cooperate individual agents and all of them starting in the same initial positions the initial positions were determined by the proximal matrix selecting the first 300 best positions and placing alternately each agent of the two strategies each experiment was simulated 20 times and the simulation results were organized and analyzed as follows in the final simulated images the spaces occupied by each strategy were counted and compared with each other generating the images shown in fig 11 spaces mostly occupied by hawks defect agents were represented in red while the ones mostly occupied by doves cooperate agents were represented by blue green spaces are overlaps where both of them occupied the same amount of times in the average visually it is clear to perceive similarities between fig 11a and c since the agents playing hawk defect occupy almost every cell of the available space on the other side fig 11b and d have a predominance of dove cooperate agents and hawks defect agents respectively the data in table 6 shows the percentage of the occupied spaces in the four experiments confirming that experiments with reactive behaviour have the same distribution while hdb experiments tend to have a dove dominance and pdb experiments tend to have a defect dominance the graphs presented in fig 12 were created with a simple strategy count of what agents were playing at the end of each simulation step since there s no variation in agent quantity the sum of agents will be 300 note in figs 11 and 12 that the chosen game and the decision algorithm may change the affected landscape and the final population configuration results drastically but some questions remain since hawk dove and prisoner s dilemma have different payoffs how their reactive experiment results are very similar and their best accumulates payoff are not alike does the decision algorithm outweigh the defined payoff matrix to answer these questions we developed a scoring system based on the strategies payoffs and measured the acquired points over the simulation to determine which one was the best in each experiment for the reactive behaviour this analysis is trivial the dominant strategy is always the best one since it always wins against the other strategy and 50 of the time will win against itself the other strategy only wins when encountering itself and even when this occurs each agent only wins 50 of the time we determined two matrices that scores encounter between two agents that play strategies s or t based on the payoffs found in table 7 if the agent remains with its strategy after the encounter it ll score 1 if it changes its strategy 50 of the time it ll score 0 5 if it changes every time then it scores 1 this way scoring is an indirect form of measuring success considering both the payoff and the decision algorithm the scores used in these experiments are presented in table 8 considering s to be hawk defect agents t to be dove cooperate and the scoring system on table 8 the mean score of each type of agent and the mean score of the whole population is shown in fig 13 following the same reasoning of the individual strategy mean payoff and the population mean payoff shown in equation 7 it becomes even more obvious that in both scenarios the hawk defect are more successful than the dove cooperate strategy especially because the latter is above the mean population payoff by a large difference the scoring system for registry based simulations also depends indirectly on the payoffs defined for the model for this method we defined rules based on the agent s registers states considering again a game with strategies s t if after a encounter r s r t and the agent maintains their strategy then the agent scores 1 but if r s r t and consequentially s changes to t then it scores 1 the symmetric rules applies for agents playing t table 9 with the scoring system described in table 9 and considering that s is hawk defect and t is dove cooperate the graphs for mean payoffs for each strategy and the population mean payoff are shown in fig 14 using the same reasoning for the payoff differences in the simulations in the hdb simulations the dove strategy starts scoring worse than the hawk strategy but after some steps the doves get better than hawks that s because the accumulated payoff for hawks may start better when encountering other doves but quickly degrades when the population of hawks increases a high concentration of hawks in the space is detrimental for the accumulated payoff of the hawk strategy while a high concentration of doves increases the internal register for doves this means that clusters of dove agents may thrive over time while clusters of hawks could fade into new clusters of doves in this scenario over time it might be better to be a dove even if it s not a dominant strategy on the other side in pdb simulations the cooperate agents never do better than the defect agents but the defect in registry based doesn t do as good as the defects in reactive table 7 shows that only defect defect encounters brings negatives payoffs for the agents while cooperate defect and cooperate cooperate both brings negative encounters to the agent moreover defect encounter bring worse payoffs to cooperate agents than to defect agents then explaining why in this scenario defect agents do better than the cooperate ones these results show that the defined interaction model in the simulated model may bring different landscape transformation and population profiles the defined decision algorithm plays two roles in the interaction model first it highlights in an agent a reactive reactive or a registry based response to other agents strategies when competing for resources second it may feature the best strategy in the population over time promoting the dominant strategy in reactive and enhancing the most lucrative or less prejudicial strategy in registry based particularly in reactive is it possible to infer that the dominant strategy arises independently of the strategies current distribution in the population if the game has only one dominant strategy meanwhile in registry based the population configuration and the chosen game s payoffs are relevant for the final population distribution and landscape configuration games also play an important role in the interaction model since their payoffs are not only used to create rules for conflict resolution and strategy change but also to influence the final simulation configuration as said before the only dominant strategy in a 2 strategy game is quickly highlighted in reactive strategy change however since the scoring system for both hawk dove and prisoner s dilemma is the same their payoffs still play an important role to populate the agents registers which ultimately will impact the decision for the agent to change its strategy with mase egti s interaction model featuring both games and decision algorithm for strategy change we seek to present to modellers environmental managers and decision makers possible scenarios of lucc based not only on spatial features and public policies but also on peer to peer interaction regarding resource competition studies in lucc include many aspects and phenomena not limited to biophysical and spatial characteristics but also human behaviour in individual regional and global levels lambin et al 2003 by modelling resource competition as evolutionary games with bdi cognitive agents it could be possible to elucidate processes regarding exploration and conversion of natural areas to anthropic spaces the evolution of social distribution on spaces and their impact on earth systems and many other cases and narratives that could be interpreted and aggregated in an integrative human environment transformation set of rules and theories 5 conclusion in this paper we presented an agent based decision model set over egt with three layers of interactive deliberation individual peer to peer and social the model was implemented in the mase egti simulator to investigate landscape occupation and change by modelling resource competition the simulation parameters of mase egti are based on five dimensions called staip including space time agents interaction model and public policy the interaction model defines how agents resolve spatial conflicts and how the strategy will change based on evolutionary games and decision algorithms four experiments were defined using real data from the brazilian mapbiomas database combining two classic games hawk dove and prisoner s dilemma with two behaviours algorithms reactive and registry based the results show that the reactive behaviour highlights the dominant strategy in a 2 strategy game making agents quickly convert their profiles to the only strategy that could win the conflict reactive can convert the entire population independently of the initial strategy distribution the registry based is sensitive to the chosen game payoffs ultimately featuring strategies that are more lucrative or less prejudicial to the agents registry based may feature a cluster of agents that have positive payoffs when competing for resources and the interaction model may influence landscape transformation where the most successful strategy dominates most of the spatial area for future work we intend to develop new experiments featuring games without dominant or with more than one dominant strategy e g battle of sexes matching pennies games with more than two strategies e g rock paper scissors hawk dove retaliator and decision algorithms that include the amount of exploration made by agents while playing a strategy also the model and experiments presented would benefit from parameter tuning and sensitivity analysis techniques for better adjustment of parameters and further comprehension of the results these modifications and adjustments could bring more diverse spatial configuration and population profiles to ecological scenarios declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported in part by grant 311301 2018 5 given to prof c g ralha by the brazilian national council for scientific and technological development cnpq which is not responsible for its contents appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105252 
25696,interacting entities in human society and their roles considering land exploration and occupation are crucial for studying land use cover change lucc in landscape ecology for investigation of games in lucc models this work presents an agent based decision model based on evolutionary game theory with three layers of interactive deliberation individual peer to peer and social implemented in mase egti multi agent system for environmental simulation with evolutionary game theory interactions the conceptual model details staip an acronym for cellular space time agents interactions and public policy the theoretical model baseline focus on evolutionary game theory the experiments use data from mapbiomas brazilian public geographic database the results show that agents peer to peer interactions may influence the final land usage and conversion of natural systems we believe the conducted simulations present insights for lucc studies that consider space transformation and population interactions keywords agent based simulation conflict resolution model evolutionary game theory hawk dove prisoner s dilemma peer to peer interactions 1 introduction game theoretic models have been employed in the last few decades to describe many strategic social phenomena not only in economics but also in multiple disciplines such as sociology anthropology and social networks cimini and sanchez 2015 hartshorn et al 2013 phelps 2016 schindler 2012 in evolutionary biology evolutionary game theory egt was defined by the understanding that individual fitness may be related to strategic situations between species with different behaviours and their respective fitnesses maynard smith and price 1973 vincent and brown 2005 weibull 1997 the strategic situations may be understood as games where individuals are players observed behaviours in a population could be strategies in a game and consequences in an encounter between individuals may be seen as their payoffs then evolution emerges by the general outcome of the population repeatedly playing the game since above the average payoff reflects into a better ability to survive and pass certain behaviours down to new generations highlighting a well fit strategy over time vincent and brown 2005 weibull 1997 such social phenomenon is not only applicable in evolutionary biology but many other scenarios including selection and evolution of behaviours in human society since humans are not guided exclusively by rationality but also subjective matters when making decisions mcnamara and weissing 2010 the emergence of many social phenomena could be interpreted as evolutionary games mohd ibrahim et al 2019 human society also faces a great variety of dilemmas in such strategic positions situations where a deviation from efficient collective response to a selfish one may benefit more one of the parts with a variable risk of having losses or being punished for example adami et al 2016 arefin et al 2020 tanimoto 2021 tanimoto and sagara 2007 wang et al 2015 particularly in this work we are interested in investigating the emergence of exploration profiles as a result of resource conflict in land use cover change lucc simulation models the arise of more adapted social behaviours in the population may be linked to landscape transformation in lucc scenarios alongside the particular spatial characteristics and general public policies for resource management bastian 2001 lambin et al 2003 wu and hobbs 2002 in this work we extended the mase bdi simulator coelho et al 2016 that implements cognitive agents based on the belief desire intention bdi model bratman 1987 braubach and pokahr 2012 and public policy layers but lacking social interaction protocols for peer to peer communication and conflict resolution that are necessary for agents strategic profiles and emerging societal configurations now we present the multi agent system for environmental simulation with evolutionary game theory interactions namely mase egti 1 1 mase project website http mase cic unb br software availability https gitlab com infoknow mase mase bdi mase egti a simulation tool with bdi cognitive agents based on egt interaction models developed at university of brasília brazil mase egti includes spatial and temporal aspects and the possibility of public policy usage that gives the user control of land exploration and occupation thus mase egti directly improves the mase bdi simulator with social interactions in the peer to peer intercommunication aspect 2nd level mainly replacing the manager centred conflict resolution system with a new protocol using games for interdependent socialization fig 1 the other two layers rule the internal mental organization of each agent represented by the bdi model 1st level and the public policies stating general regulations for the society 3rd level the main contribution of this work is an agent based decision model set over egt with three layers of interactive deliberation individual peer to peer and social implemented in the mase egti simulator moreover submodels were designed for agent peer to peer interactions with two strategy change decision behaviours reactive and registry based to complement the replication dynamics of the games we also investigated how those behaviours would impact the dilemma s strength tanimoto and sagara 2007 of the referenced game on the agents interaction model mase egti works with lucc models and egt interactions to highlight that different agent interactions and deliberations might influence the land configuration and the general exploration profile in population over time in ecological scenarios the presented work can bring important insights into environmental scenarios with lucc patterns and social stability through socioeconomic activities and interaction behaviours 2 mase egti simulator in this section we present mase egti including the conceptual and theoretical models and the project design 2 1 conceptual model mase egti is an agent based simulator for spatially explicit models with egt interaction sub models the model m can be formalized in a tuple of five main characteristics m s t a i p called staip where s is the cellular space t is time a are the agents i is the interaction model and p is the public policy from the user s perspective the model s characteristics are represented as simulation parameters as presented in the sequence 2 1 1 space space s is a set of cells ce organized in eight connected segments except for the borders that may have three or five segments connected fig 2 each cell has five attributes s pe exploration potential state s e attractiveness s a owner ow and queue of interest il s pe and s e are codependent variables since they represent how much resources an agent can exploit from a cell with s p e n and s e cell states attractiveness is calculated based on the proximity of spatial attributes such as transport routes or aquatic bodies attractiveness is linked to the agent s movement rules since attributes can repel or attract different agent types the value of attractiveness is in the set of real numbers s a r the owner of the cell ow is the agent that is occupying it or has already occupied it at some point in the simulation the interest queue il is a linear synchronized structure that registers agents who are interested in moving to the cell in case it does not have an owner formally we define the space of width n and height m as the matrix s ce 0 0 ce 0 1 ce 0 n 1 ce m 1 0 ce m 1 1 ce m 1 n 1 where each ce i j s pe s e s a ow il is a cell with its own exploration potential state and attractiveness values added to a possible ow owner agent or interested agents in the il queue 2 1 2 time time t is represented by a set of discrete and sequential steps that compose the simulation a step is a time interval of t in which all simulation agents have to successfully fulfil their main objective once then formally t t 0 t 1 t n 1 in a simulation with n steps in which each step is the time that all agents have successfully fulfilled their objectives over and over again fig 3 note that agents have an execution body in which is the main capacity with the main objective these concepts will be explained in section 2 1 3 2 1 3 agents agents in mase egti are based on the bdi paradigm an architectural system that defines mental states for agents decisions considering what the agent knows and wants to achieve bratman 1987 braubach and pokahr 2012 the basic components of the bdi paradigm are the facts that are taken as premises beliefs the states of the world that the agent wants objectives or desires and action plans that can lead agents to achieve their goals intentions or plans of action bratman 1987 beliefs objectives and plans can be organized as capacities structures that represent the different behaviour types the agent can assume braubach and pokahr 2012 agents have execution bodies called an agent body in which deliberation is made about the most appropriate action of its capacities to be performed at the moment algorithm 1 then formally an agent can be represented as 1 a c a a c a b c a α where a is the agent and ca x is the capacity in the agent s body that can be presented as a set 2 c a c s 1 c s n p l 1 p l m o b 1 o b o where cs 1 cs n is the set of beliefs pl 1 pl m is the set of plans and ob 1 ob o is the set of capacity plans in the model m agents a are elements of the set a a 0 a x 1 where x is the number of agents participating in the simulation and a k is an agent that has at least two basic capacities exploration ca e and movement ca m moreover agents have some additional beliefs a unique id id the current cell location where the agent is ce at the renewable exploration credits cr k and a strategic profile ps the ps and cr k beliefs compose the profile of the agent which describes how the agent interacts with the environment and with others agents agents with the same profile may be located in different areas of the space but will behave similarly when exploring and conflicting with other agents the agent s main objective fig 3 is to occupy and extract resources from a cell being part of its capacity ca e this objective is achieved as the agent executes the action plan that uses the credit cr when extracting resources from the cell ce at as shown in fig 4 when all agents of the set a successfully fulfil their main objectives the current simulation step ends a new step can be started and the credits cr k of each agent are renewed this credit renewal allows the agents to fulfil their main objective again at each step of the simulation in algorithm 1 this can be seen by the first condition fulfilment when the agent s credits and the cell s potential are both positive however if an agent has spare credit and its cell does not have sufficient resources to match it the main objective fails and the agent accesses the capacity ca m to change the cell in this capacity the agent analyzes its neighbourhood the set of n adjacent cells ce 0 ce 1 ce i ce n 1 within a radius of view r and accesses their attractiveness values s ai adding all of them to the ss a value 3 s s a i 0 n 1 s a i where s ai is the attractiveness value of the cell ce i section 2 1 1 the attractiveness value of each cell is accessed again cumulatively added and divided by ss a this process generates a probability matrix ranging from 0 to 1 in which agents randomly choose a value the index of the first value of the probability matrix that is greater than the chosen random value corresponds to the cell that the agent will try to conquer after choosing the intended cell ce intended the agent registers its plans of moving by putting his agent id in the queue of interest il of the ce intended if there is no other agent id in the queue of interest il of the ce intended then there s no resource conflict so the objective of the capacity ca m automatically succeed and the agent becomes the owner ow he then moves to this new location and returns to fulfilling his primary objective until its credit cr k is depleted otherwise the agent must resolve the conflict with the other interested agents of the ce intended using the current interaction model that considers the agent s ps strategic profile to decide who will be the owner of the resource fig 4 this process will be detailed in section 2 1 4 the movement and the conflict resolution is represented by the second and third conditions in the deliberation method of the agent in algorithm 1 finally if the agent depletes its exploration credits cr k it will finish its actions for the current step and it will wait for the other agents to finish as well final else condition in algorithm 1 algorithm 1 the internal deliberation for the agent s next action image 1 2 1 4 interaction model the interaction model i consists of an egt game g and one of the decision algorithm defined in mase egti reactive or registry based all agents who need a new cell register their interests in the cell queue il the agents resolve the conflicts two by two in the queue the first resolves the conflict with the second the winner settles with the third in line while the loser looks for another space the winner of this second conflict settles with the fourth in line and so on until only one agent stands and becomes the owner ow of the cell the conflict is resolved based on the chosen egt game the chosen game g must consist of m strategies and the associated payoff matrix with m 2 values that represent the profit or loss of an encounter between two of the m strategies the ps belief which is the strategy the agent uses section 2 1 3 must belong to the set of m game strategies about interaction model i therefore when two agents a i and a j conflict they use the payoff matrix to determine what are their respective v i and v j profit or loss values the agent with the highest v v i v j wins the conflict and in the event of v i v j the winner of the conflict is chosen randomly the decision algorithm is represented by the exchange of strategies after a conflict situation the outcome of the game can influence the ps belief of the agents involved besides determining the winner of the cell disputed by two or more agents the strategy change can be of two behaviours reactive or registry based in the reactive behaviour the losing agent of a conflict changes the strategy to which he would have a preferably better or at least equal payoff if he found the strategy that won him again in the registry based the agent records all payoffs received from conflict situations adding them up after a conflict he changes the strategy to the one that is currently the most advantageous or has the largest sum since the beginning of the simulation 2 1 5 public policy a public policy in the staip model is linked to space s and determines special areas that influence the exploration movement and occupation of agents the public policy determines polygons that divide s into three use categories restricted diversified and encouraged these categories generate sets of cells that have an associated attractiveness multiplier the public policy p is formally defined as 4 p c a t 0 c o n j c e a a c e a b c e z z m u l t c a t 1 c o n j c e a a c e a b c e z z m u l t c a t 2 c o n j c e a a c e a b c e z z m u l t where cat x is the category of public policy conj is the set of cells encompassed and mult is the multiplier that affects the attractiveness of the cells encompassed in conj 2 2 theoretical model mase egti interactions are based on egt games as a representation of resource competition in social activity among agents seeking spatial assets for exploration section 2 1 4 resource competition occurs because cells may not be explored by more than one agent per simulation step resulting in mutual exclusion between peers and conflict successful cell acquisition after a conflict may increase the winning agent s fitness in the simulation context while decreasing the adaptability of the losers who may change strategies to a more suitable one thus an egt game must be defined as a set of rules for population characterization and conflict resolution to select more fitted individuals for exploring the space and a decision algorithm must be chosen for population evolution promoting social transformation both game and decision algorithm influence the population final profile and consequently the resultant simulated landscape in section 2 1 4 it was mentioned that in the staip model the interaction model was composed of a game g with m strategies alongside one of the defined decision algorithms the generic form of the game is described in section 2 2 1 with the replicator dynamics being correlated with the decision algorithms in section 2 2 2 2 2 1 the static game games in game theory and egt are strategic situations where individuals players or agents may perform a possible action strategy or behaviour that results in a outcome payoff that depends on the collective actions of all players the payoff in egt is also a measure of the fitness of an individual in strategic scenarios the greater is its payoff in comparison with the group s mean payoff the better its adaptability studies on stability equilibrium payoff maximization or loss minimization for example rely on these elements in many different games in theoretic and real life scenarios weibull 1997 in mase egti the game g is part of the interaction model and its payoffs are the rules used for conflict resolution between agents for simplicity consider that g has only two strategies i and j two player game and payoffs that are represented by the function u on table 1 also let n be the total amount of agents in the simulation with n i and n j being the numbers of agents with their belief ps i j respectively the variables η i n i n and η j n j n are the proportion of agents with their ps equal to i and j respectively with η i η j 1 the expected payoff u for the agents is not only dependant on their strategies beliefs but also on the probability θ of the occurrence of encounters in the space with 0 θ 1 with θ 0 when the agents are far enough away from each other with enough resources and θ 1 when there s very little available space left so all agents have to compete over a small number of cells since agent movement is probabilistic and based on space availability and the cell s attraction values θ is empirically determined so the expected payoffs u i and u j can be expressed as equations 5 and 6 5 u i θ η i u i i η j u i j 6 u j θ η i u j i η j u j j 2 2 2 the game evolving when losing a conflict agents have the opportunity to update their ps beliefs into a more suitable strategy eventually diminishing or increasing the number of individuals playing i or j in the population n the dynamic replicator equation defined by hofbauer et al 1998 was used to evaluate the population evolution over the simulation steps 7 n i n i u i u i j 8 n j n j u j u i j where u i j is the mean payoff of the population expressed by 9 u i j η i u i η j u j dividing both sides of equations 7 and 8 by n then substituting u i j by equation 9 the following is obtained 10 η i η i η j θ η i u i i u j i η j u i j u j j 11 η j η i η j θ η i u j i u i i η j u j j u i j now with the definitions of equations 10 and 11 consider the two decision algorithms defined in mase egti the way agents decide their next strategy may impact the final population proportions η i and η j of i and j agents and since their ps belief is correlated with their cr k exploration credit section 2 1 3 it also might impact on the final simulated landscape state as a tool for further analysis on the agent s preferred strategy we also consider the concept of dilemma strength referenced in the works of tanimoto and sagara 2007 and wang et al 2015 considering a parallel between the prisoner s dilemma and the generic game presented in table 1 let u j j p u i i r u i j s and u j i t on those works d g t r u j i u i i is the dilemma potential for solutions where a gambling response is inclined while d r p s u j j u i j is the risk averting inclination it s is possible to describe a game s dilemma existence in terms of these by these two static parameters alone arefin et al 2020 tanimoto 2021 tanimoto and sagara 2007 2 2 2 1 reactive behaviour the reactive behaviour for strategy change was designed as a variation of the tit for tat strategy where the players mimic the strategy of the opponent in the last round in a reactive way weibull 1997 the difference is that in reactive behaviour when an encounter occurs the loser counterpart of the resource competition changes its strategy to the one that could have a better chance to win if it encounters the winner again in the future to illustrate this consider that in the game g j the only strategy that has chances to win both against i and j so either way u i i u j j or u i i u j j but always u i j u j i as mentioned in section 2 1 4 the agent with the highest payoff in a conflict wins the resource and in case of a draw they decide who is the winner randomly with equal chances so in this scenario an agent has 50 of a chance of winning against another one playing the same strategy however j always wins against i table 2 presents the probability ρ x x of the agent 1 staying in its strategy x after encountering with agent 2 if the strategy y with 0 ρ 1 since i has a 50 chance of winning a conflict against another i it has 50 of remaining being an i since the loser must change its strategy to j to have a greater chance of winning against another i on a possible next conflict however i has no chance of remaining an i after encountering an agent j since it ll always lose and will have to convert to j changing the perspective to j it ll always remain as a j since it ll win every conflict against an i and even if it loses to j it ll remain a j to have a chance to win in another future conflict adapting equations 5 and 6 to refer to the value of ρ instead of the value of u then substituting the values of table 2 on these equations and finally applying their results in equations 10 and 11 would give 12 η i η i η j θ η i 0 5 1 η j 0 1 η i 0 5 η 2 i η j θ η 2 j η i θ 13 η j η i η j θ η i 1 0 5 η j 1 0 η j 0 5 η 2 i η j θ η 2 j η i θ which highlights that the η i proportion of the population will decrease over time consequentially increasing η j depending on the probability of conflicts and the current proportions of the population this implies that a game that has two strategies where one of them has an equal or greater payoff than the other u j j u i j and u j i u i i is converted to a trivial game when agents behave reactively thus a game g that may have a dilemma and is originally set to be used as a rule for conflict resolution may be reduced to a trivial game when agents are set to be reactive in their interactions with each other this decision making behaviour could model a society where the best social payoffs are preferred in the population excluding agents that receive less than the average individual payoff in the game until their total extinction from the simulation 2 2 2 2 registry based behaviour the registry based behaviour was designed as a counterpoint to the reactive one agents using this algorithm as a method of strategy change keep a summation of all of the payoffs received from conflict resolution on internal registers for each possible strategy when competing for resources and with their ps belief equals to i for example the agents i sum all u i i and u i j received in the register r i after each conflict winning or not then the agents who lost the conflict check their registers to measure which strategy is doing better overall since the beginning of the simulation finally they change to that better suited strategy instead of immediately changing to the one that could win the next conflict as described in the reactive behaviour for example if an agent i loses a conflict it adds the received payoff to its internal register r i then it checks if r j r i if this is true then it changes its strategy to j if it isn t it keeps playing i the same is valid if the agent was playing j adjusting the proper verification of its registers to analyze this decision change consider the same game described in table 1 but instead of describing the possibility of encounters as θ like in equations 7 and 8 consider that α and β are the more specific probabilities of an agent i encountering another i and i encountering another j respectively also consider that γ and δ are the probabilities of an agent j encountering i and j encountering j so the internal registers r i and r j for the very first step of the simulation are 14 r i 0 α η i u i i β η j u i j 15 r j 0 γ η i u j i δ η j u j j and after n simulations steps the overall registers of the game are 16 r i n 1 s 0 n 1 r i s 17 r j n 1 s 0 n 1 r j s for simplicity from now on r i n and r j n will be referenced as r i and r j after a conflict loss and adding the received payoff to the respective register the agent chooses the strategy x i j that have the greater register value r x max r i r j even though α β γ and δ are more specific than θ they also depend on the model s spatial attributes the spatial density of the agents and the number of available spatial resources because of this they are also empirically determined nonetheless it is possible to analyze agent gains based on their registered states consider again an agent with their belief ps i after ending a conflict it will only keep playing i if r i r j and will change if r i r j symmetrically if the agent was playing j and its register s state is now r i r j then the agent will change strategies it will keep playing j otherwise in both scenarios there s a possibility of the increase or decrease of the population of i and j consider the function ξ that empirically measures the probability of one of the registers being greater than or equal to the other after a given two agents encounter equations 18 and 19 synthesize these relationships in terms of η i and η j 18 η i η i ξ r i r j i i η j ξ r i r j j i η i ξ r j r i i j 19 η j η j ξ r j r i j j η i ξ r j r i i j η j ξ r i r j j i these equations show that to be there a positive gain in a population the register for that population strategy must be greater than the others population strategy if that happens for the agents that entered in a conflict in a simulation step not only the agents that were playing the better strategy will keep playing that one but also the other players will consider changing their strategies increasing the more adapted population for example if r i r j to some agent in the nth simulation step the following is true r i n 1 r j n 1 s 0 n 1 r i s s 0 n 1 r j s α 0 η i 0 u i i β 0 η j 0 u i j α n 1 η i n 1 u i i β n 1 η j n 1 u i j γ 0 η i 0 u j i δ 0 η j 0 u j j γ n 1 η i n 1 u j i δ n 1 η j n 1 u j j since the values of u x y for x y i j don t change over the simulation but the values of α β γ δ η i and η j are all mutable at every simulation step some conclusions may be drawn from the population dynamics as follows u i i u j i with d g 0 or u i i u j i with d g 0 and u i j u j j with d r 0 since the beginning of the simulation which means that i strategy gives better payoffs than the average when encountering other strategies than its counterpart because of that r i r j holds over the simulation steps also since d g 0 and d r 0 this possibility converts the game g into harmony or pure cooperation where a social dilemma practically doesn t exists tanimoto and sagara 2007 y c d α y y c d γ y with d g 0 or y c d α y y c d γ y with d g 0 and y c d β y y c d δ y with d r 0 where c d 0 n 1 this means that between steps c and d the rating of encounter of agent playing i was more probable than the others playing j while the agent played i creating a bias in the respective register that means that the summation for the strategy i was secured as a better suitable one over the time making the agent decide that one was suitable for an next simulation step as well this also means in this case that this situation is similar to the previous one with d g 0 and d r 0 the social dilemma practically doesn t take place tanimoto and sagara 2007 y c d α y y c d γ y with d g 0 or y c d α y y c d γ y with d g 0 and y c d β y y c d δ y with d r 0 where c d 0 n 1 this is the equivalent situation to the previous one but with the registers in reversed bias indicating to the agent that the j strategy is better in this case however d g 0 and d r 0 creating a situation similar to the prisoner s dilemma where j exploits i tanimoto and sagara 2007 α k η i k u i i γ k η i k u j i or β k η j k u i j δ k η j k u j j with d g 0 or d r 0 for some k 0 n 1 this means that in some simulation step the quantity of positives payoffs from positive encounters the agent had while playing i surpassed the positive encounter while playing j achieving a temporary stability until the nth step the parameters d g 0 or d r 0 may characterize the situation as pure cooperation or as some known dilemma such as stag hunt d g 0 and d r 0 for example α k η i k u i i γ k η i k u j i or β k η j k u i j δ k η j k u j j with d g 0 or d r 0 for some k 0 n 1 this is the reversed configuration of the previous situation but towards j in this case the parameters d g 0 or d r 0 always configure some sort of social dilemma as demonstrated by equations 10 11 18 and 19 agent positioning is important for the simulation and influences the evolution of the agent population by creating situations of dilemma or harmony since agents have their strategy profile bound to their exploration credits it might change the final observed simulated landscape this is especially relevant for the registry based decision algorithm once the encounter rate for some strategies may expand the difference between the agent s registers making a strategy seems better than the other a similar concept was explored by de andrade et al 2009 where agents were motivated to play mixed strategies to occupy cells in space eventually reaching populations profiles near to the nash equilibrium the two main differences were in their work conflict losers were eventually eliminated from the simulation while in mase egti they convert to the more suitable strategy according to the current decision algorithm 2 3 project design mase egti is the enhanced version of mase bdi coelho et al 2016 in both simulators agents have at least two capabilities exploration ca e and movement ca m as described in section 2 1 4 there are two extra capabilities that mase egti agents implement conflict resolution ca cr and strategy change ca sc capacities mase egti was built with java 7 and jadex 2 4 agents are defined as classes with annotations from the framework and are instantiated on the jadex platform an environment that allows them to have a well defined life cycle communication and autonomy braubach and pokahr 2012 mase egti is a freeware software and its code is publicly available at https gitlab com infoknow mase mase bdi mase egti 2 3 1 architecture the mase egti s architecture is divided into three layers user interface control and simulation fig 5 the user interface allows the configuration of the models the initialization of the simulation and the quick visualization of the state of space throughout the execution it is possible to start the tool either through the graphical interface or via the operating system terminal the control layer has three main components configuration loader simulation control and statistics collector the configuration loader parses the user s settings recording the initial simulation variables such as the number of agents and the conflict resolution model the loader also initiates variables pertinent to the common belief database a module belonging to the lower layer the simulation control instantiates the simulation the platform and the agents based on the settings and has synchronization mechanisms for starting and ending steps based on the states of the agents the simulation layer has two main modules the common belief database and the jadex platform where the agents reside in the common belief database are the variables that agents can query and update the platform is an object of the jadex library where agents are instantiated and can perform their actions also move through their states of life and communicate with each other on the platform there are agents capacities with sets of objectives plans and beliefs in which the agents cognition and actions are organized 2 3 2 agent interaction protocol during the simulation resource conflicts are almost unavoidable since agents cannot perceive if other nearby agents intend to move to the same cell therefore there is a cell acquisition process implemented in mase egti that is associated with the interaction model including five phases target cell decision interest queue registration waiting for the movement of other agents winner decision and cell change this process is organized in an agent capacity the conflict resolution capacity ca cr when the main objective of exploring the cell fails before trying again the agent needs to move to another space the agent analyzes its neighbourhood as described in section 2 1 3 obtaining the sum of all adjacent cells attractions ss a equation 3 and choosing the next cell to conquer ce intended as soon as the agent decides the cell ce intended it proceeds to the second phase registering its interest in the queue il then it accesses the cell s synchronized method that captures the agent s id and places it at the end of the queue the synchronized method prevents concurrent writing ensuring that each agent interested in the cell enters it in a consistent order the agent waits for other agents who need to move to register in the respective cell lines of interest the fourth stage begins with the winner s collective decision the agents analyze the respective queue in which they registered interest if the queue has more than one element a loop starts as presented in algorithm 2 the first agent is considered the temporary winner while the second is the opponent then the winner sends a message with two values to the opponent his ps1 strategy and a b1 bet bet b1 is a random number between 0 and 1 the opponent receives the message and before opening its content it generates a b2 bet also randomly generated between 0 and 1 the opponent then receives the winner s ps1 and b1 values and analyzes considering the current game his own ps2 strategy and his bet b2 considering again that the payoff functions u a b returns the value of the payoff that the agent with strategy a receives when it conflicts with the agent with the strategy b we have four possible scenarios with two results 1 u ps1 ps2 u ps2 ps1 the winner remains victorious and the opponent becomes loser 2 u ps1 ps2 u ps2 ps1 the opponent becomes the new winner and the old winner becomes the loser 3 u ps1 ps2 u ps2 ps1 and b1 b2 the winner remains victorious and the opponent becomes loser and 4 u ps1 ps2 u ps2 ps1 and b1 b2 the opponent becomes the new winner and the old winner becomes the loser the opponent regardless of having won or lost the conflict responds to the message with four values who was the winner who was the loser what is the payoff value u ps1 ps2 and what is the payoff value u ps2 ps1 then the two agents involved in the conflict access their fourth capacity the strategy change capacity ca sc in this capacity they update their ps beliefs according to the decision algorithm chosen in the model and the payoffs calculated in this recent conflict finally the loser of the conflict looks for another cell while the winner returns to the starting point of the decision loop checking if there s more than one element in the queue if there s not the loop ends and the winner becomes the owner of the cell the last phase of the ca cr begins as soon as the winner s decision loop ends the winning agent changes its location belief to the current cell and its goal of exploring cells is resumed in the meantime the losing agents return to the first phase of the ca m capacity to find a new cell to explore algorithm 2 collective decision algorithm for cell s winner image 2 2 3 3 interface the mase egti interface can be graphical or by commands in an operating system terminal fig 6 in the graphical part space and agents occupation can be seen along with the simulation steps the colours are related to agents strategic profile ps it can see also some quick visualization of the simulation results as graphs on the lateral side of the interface as well as messages from the java virtual machine output alternatively the user can run mase egti from the command line with the java jar mase jar call this can be useful for carrying out large volumes of simulations for statistical analysis the simulation and graphics are not shown live to the user but the results are written to a file at the end of the steps to be analyzed by the user 3 related work there are several works and tools involving the areas of environmental modelling agent based simulation bdi agents and interactions based on egt with any combination of elements and concepts relevant to these aspects 3 1 terrame terrame was first introduced by carneiro 2003 when it was based mainly on cellular automata improving gradually as a programming environment carneiro et al 2013 which is currently in version 2 0 1 de andrade 2020 terrame model includes i anisotropic spaces spatial configurations that use natural spatial attributes and patterns of anthropic movement and ii hybrid cellular automata an abstract model intended for systems that have continuous and discrete characteristics terrame integrates a gis database for the contextualization of simulated models allowing its development environment with a modelling language based on lua ierusalimschy 2016 this programming environment was extended to integrate an egt module for the agents common resource competition de andrade et al 2009 however that work does not use lucc models for validation but explores different perspectives of the chicken game similar to the hawk dove game agents compete for spaces on a grid increasing or decreasing their overall satisfaction through local payoffs that result from conflicts during the simulation new agents are not created but agents with low overall satisfactions leave the simulation grid the purpose of the experiments is to find the agents configuration that their satisfactions are sufficiently stable so there is no need to change spaces achieving a balanced situation mase egti and terrame are similar performing simulations of spatial models for lucc with an agent based approach and interactions based on egt terrame also stands out for having a platform for the construction of spatially explicit models with the possibility to integrate georeferenced data however mase egti implements bdi agents and public policies in lucc scenarios with a three layer interactive deliberation between agents focused on conflict resolution for spatially explicit models 3 2 abed izquierdo et al 2019 presents an abed agent based simulation tool with egt interactions to assist the observation of properties arising from the interactions in different scenarios and models the main objective is to facilitate the study of classic game variations and the introduction of new strategic situations with several model configurations agents and games abed is divided into two models abed 1pop simulation of one population of n individuals playing mixed strategies from a pre configured game and abed 2pop simulation of two populations each playing pure game strategies the game can be chosen from a list of pre configured classic games such as hawk dove chicken game rock paper scissors or with a custom payoff matrix aspects of the replicator dynamics are presented in the simulation configuration i how agents are chosen in pairs to enter into conflict ii what percentage of the players will review their strategy and iii what kind of strategy review will be used among the possible review forms there are reactive methods where the agent decides which is the best answer for the strategy he is facing and probabilistic with different probability distributions considering the accumulated payoffs and the distribution of the strategies in the population mase egti and abed have several points in common as the simulation strategies distribution and the replicator of strategies both have configuration parameters for the initial distribution and the strategy review form without changing the number of individuals in the simulation abed has methods and games with more flexible possibilities than mase egti nevertheless mase egti is directed towards spatial simulations implements bdi agents and performs validation in real models aspects absent in abed 3 3 dynamo franchetti and sandholm 2013 introduced dynamo suite of notebooks for the mathematica software for diagrams generation vector fields and other types of graphics related to egt dynamo generates visual schematics from egt scenario data important for the study of replicator dynamics and evidence of stability and balance in populations the various notebooks bring the possibility of representing games with two three or four different strategies with their respective payoffs and initial distribution the replicator dynamics are configurable by the user who can choose from a list of possible methods the output is a visualization graph of the population evolution over time showing phenomena such as the formation of an ess and the most adapted strategy mase egti and dynamo are similar using egt in multi agent simulation scenarios however their purposes and project conceptions are different dynamo is designed to create graphs using egt calculations for the evaluation and forecasting of a population after n seasons while mase egti uses simulation rules for population evolution in spatial scenarios table 3 presents the related work overview considering five characteristics i tool the tool main features including type technology licence and latest version ii space the use of spatial models considering spatially explicit models space attributes in agent movement and public space with planning policies iii egt the egt games used and replicator dynamics features iv model the model used in the referenced article and if it uses real data and v bdi the use of bdi cognitive agents note that mase egti is the only work that uses public space planning policies real data with the brazilian case and bdi cognitive agents 4 experiments the brazilian case study named cerrado mapbiomas is presented to illustrate mase egti this model is included in the mase egti code available at https gitlab com infoknow mase mase bdi mase egti 4 1 the cerrado mapbiomas model the cerrado mapbiomas model is based on data from mapbiomas an open brazilian digital repository of geographic data with many categories and temporal series on brazilian biomes inpe 2018a inpe 2018b mapbiomas started in 2015 seeking a better comprehension of lucc processes in brazilian natural landscapes and scenarios the collections are updated annually with data that comprehend specific areas in an observed time interval we used collection 4 of cerrado that was released in august of 2018 and rounds data from 1985 to 2018 this model is an evolution of the cerrado df model used to validate mase bdi simulator coelho et al 2016 the biggest difference is the images in mapbiomas s collections are larger in resolution than the ibama s ones used in cerrado df moreover agents in the mase bdi simulator have a restricted interaction model interacting only with a manager agent to conflict resolution finally the mase bdi simulator didn t have the staip model definition section 2 1 however cerrado df and cerrado mapbiomas have some common characteristics the spatial proximal attributes section 2 1 1 the agent cell decision algorithm section 2 1 4 and the public policy section 2 1 5 in the sequence we illustrate the presented staip model 4 1 1 space the initial space was configured to be the cerrado biome in the federal district in year 2000 fig 7 with six spatial proximal attributes water bodies watercourses urban spaces train lines roads and conservation units fig 8 pixel colours determine both the cells exploration potential s pe and their state s e table 4 the proximal attributes in fig 8 are used to calculate the attraction matrix attributing an attraction value s a to each cell the attributes are separated into two sets attractive and repulsive which draw or move away from agents respectively the attraction matrix is calculated by first obtaining the images for the proximal spatial attributes and applying a grayscale transformation afterwards copies of these original images are created transformed by a gaussian blur filter and inverted each copy is subtracted from its original image pixel by pixel and the resultant images of the attractive attributes are summed up finally the resultant images of the repulsive layers are subtracted resulting in the attraction matrix in which the higher the value of the pixel the more attractive space is fig 9 the pixel values are assigned to the cells in the same fashion as the cells are formed setting the attraction value of each cell the attractive attributes are water bodies watercourses roads and train lines the repulsive ones are urban spaces and conservation units 4 1 2 time the mase bdi simulator was validated with the cerrado df case where the starting point was the space in the federal district in 2002 at the time the only open data in ibama was the period comprehending 2002 2008 years ralha et al 2013 coelho et al 2016 to situate the new model in the same context we set the starting time of the model at the space in the federal district in the year 2000 which was the closest point in time at the mapbiomas dataset also the agents were allowed to simulate as many steps as they could to the maximum of 1000 steps so they could explore the maximum available space possible 4 1 3 agents two classes of agents were defined group agents and individual agents group agents represent big producers such as cooperative or agriculture companies while individual agents produce for local markets and subsistence in terms of exploration credits both of them were given arbitrarily 2000 units of explorations credits so it would be easier to register in the simulation how many spaces they were occupying moreover the maximum exploration potential in one cell was 1500 table 4 so they had to change their position at least once per step increasing resource conflicts in the simulation their attached ps strategy is described in the following section 4 1 4 interaction model two sub models were defined for the experiments using games hawk dove and prisoner s dilemma with payoff values described in table 7 those games were chosen since the hawk dove was first analyzed by maynard smith and price 1973 as a representation of animal behaviour when conflicting for resources a similar problem that is explored in this work the prisoner s dilemma was chosen because it s a classic game that offers a good baseline in egt studies for strategy emergence adami et al 2016 these sub models were branched into two other sub models considering one of the decision algorithms reactive or registry based regarding the role assignment group agents have always programmed to play pure hawk on the hawk dove experiments or defect strategy in the prisoner s dilemma individual agents where always set to be doves or cooperate respectively table 5 shows the four variations of the cerrado mapbiomas model based on the interactions sub models and the strategic decision algorithm from now on these variations are going to be referenced by their experiment identifier presented in the last column of the table 4 1 5 public policy the public policy in the cerrado mapbiomas model defines three areas with different usages restricted diversified and incentive use demonstrated respectively as red blue and green areas in fig 10 in restricted use the land exploration is prohibited so the attraction values of the cells in this area are negative in the diversified use any activity is allowed so the attraction values of the cells in this area remain as calculated from the proximal attributes in incentive use agents are encouraged to explore with the attraction values of 10 boost 4 2 results and discussion four experiments were developed with the cerrado mapbiomas model one with the prisoner s dilemma game and reactive behaviour pdr one with the prisoner s dilemma and registry based pdb one with hawk dove game and reactive behaviour hdr and one with hawk dove and registry based hdb table 5 all of them were initialized with an arbitrary number of 300 agents distributed in 50 hawks defect group agents and 50 doves cooperate individual agents and all of them starting in the same initial positions the initial positions were determined by the proximal matrix selecting the first 300 best positions and placing alternately each agent of the two strategies each experiment was simulated 20 times and the simulation results were organized and analyzed as follows in the final simulated images the spaces occupied by each strategy were counted and compared with each other generating the images shown in fig 11 spaces mostly occupied by hawks defect agents were represented in red while the ones mostly occupied by doves cooperate agents were represented by blue green spaces are overlaps where both of them occupied the same amount of times in the average visually it is clear to perceive similarities between fig 11a and c since the agents playing hawk defect occupy almost every cell of the available space on the other side fig 11b and d have a predominance of dove cooperate agents and hawks defect agents respectively the data in table 6 shows the percentage of the occupied spaces in the four experiments confirming that experiments with reactive behaviour have the same distribution while hdb experiments tend to have a dove dominance and pdb experiments tend to have a defect dominance the graphs presented in fig 12 were created with a simple strategy count of what agents were playing at the end of each simulation step since there s no variation in agent quantity the sum of agents will be 300 note in figs 11 and 12 that the chosen game and the decision algorithm may change the affected landscape and the final population configuration results drastically but some questions remain since hawk dove and prisoner s dilemma have different payoffs how their reactive experiment results are very similar and their best accumulates payoff are not alike does the decision algorithm outweigh the defined payoff matrix to answer these questions we developed a scoring system based on the strategies payoffs and measured the acquired points over the simulation to determine which one was the best in each experiment for the reactive behaviour this analysis is trivial the dominant strategy is always the best one since it always wins against the other strategy and 50 of the time will win against itself the other strategy only wins when encountering itself and even when this occurs each agent only wins 50 of the time we determined two matrices that scores encounter between two agents that play strategies s or t based on the payoffs found in table 7 if the agent remains with its strategy after the encounter it ll score 1 if it changes its strategy 50 of the time it ll score 0 5 if it changes every time then it scores 1 this way scoring is an indirect form of measuring success considering both the payoff and the decision algorithm the scores used in these experiments are presented in table 8 considering s to be hawk defect agents t to be dove cooperate and the scoring system on table 8 the mean score of each type of agent and the mean score of the whole population is shown in fig 13 following the same reasoning of the individual strategy mean payoff and the population mean payoff shown in equation 7 it becomes even more obvious that in both scenarios the hawk defect are more successful than the dove cooperate strategy especially because the latter is above the mean population payoff by a large difference the scoring system for registry based simulations also depends indirectly on the payoffs defined for the model for this method we defined rules based on the agent s registers states considering again a game with strategies s t if after a encounter r s r t and the agent maintains their strategy then the agent scores 1 but if r s r t and consequentially s changes to t then it scores 1 the symmetric rules applies for agents playing t table 9 with the scoring system described in table 9 and considering that s is hawk defect and t is dove cooperate the graphs for mean payoffs for each strategy and the population mean payoff are shown in fig 14 using the same reasoning for the payoff differences in the simulations in the hdb simulations the dove strategy starts scoring worse than the hawk strategy but after some steps the doves get better than hawks that s because the accumulated payoff for hawks may start better when encountering other doves but quickly degrades when the population of hawks increases a high concentration of hawks in the space is detrimental for the accumulated payoff of the hawk strategy while a high concentration of doves increases the internal register for doves this means that clusters of dove agents may thrive over time while clusters of hawks could fade into new clusters of doves in this scenario over time it might be better to be a dove even if it s not a dominant strategy on the other side in pdb simulations the cooperate agents never do better than the defect agents but the defect in registry based doesn t do as good as the defects in reactive table 7 shows that only defect defect encounters brings negatives payoffs for the agents while cooperate defect and cooperate cooperate both brings negative encounters to the agent moreover defect encounter bring worse payoffs to cooperate agents than to defect agents then explaining why in this scenario defect agents do better than the cooperate ones these results show that the defined interaction model in the simulated model may bring different landscape transformation and population profiles the defined decision algorithm plays two roles in the interaction model first it highlights in an agent a reactive reactive or a registry based response to other agents strategies when competing for resources second it may feature the best strategy in the population over time promoting the dominant strategy in reactive and enhancing the most lucrative or less prejudicial strategy in registry based particularly in reactive is it possible to infer that the dominant strategy arises independently of the strategies current distribution in the population if the game has only one dominant strategy meanwhile in registry based the population configuration and the chosen game s payoffs are relevant for the final population distribution and landscape configuration games also play an important role in the interaction model since their payoffs are not only used to create rules for conflict resolution and strategy change but also to influence the final simulation configuration as said before the only dominant strategy in a 2 strategy game is quickly highlighted in reactive strategy change however since the scoring system for both hawk dove and prisoner s dilemma is the same their payoffs still play an important role to populate the agents registers which ultimately will impact the decision for the agent to change its strategy with mase egti s interaction model featuring both games and decision algorithm for strategy change we seek to present to modellers environmental managers and decision makers possible scenarios of lucc based not only on spatial features and public policies but also on peer to peer interaction regarding resource competition studies in lucc include many aspects and phenomena not limited to biophysical and spatial characteristics but also human behaviour in individual regional and global levels lambin et al 2003 by modelling resource competition as evolutionary games with bdi cognitive agents it could be possible to elucidate processes regarding exploration and conversion of natural areas to anthropic spaces the evolution of social distribution on spaces and their impact on earth systems and many other cases and narratives that could be interpreted and aggregated in an integrative human environment transformation set of rules and theories 5 conclusion in this paper we presented an agent based decision model set over egt with three layers of interactive deliberation individual peer to peer and social the model was implemented in the mase egti simulator to investigate landscape occupation and change by modelling resource competition the simulation parameters of mase egti are based on five dimensions called staip including space time agents interaction model and public policy the interaction model defines how agents resolve spatial conflicts and how the strategy will change based on evolutionary games and decision algorithms four experiments were defined using real data from the brazilian mapbiomas database combining two classic games hawk dove and prisoner s dilemma with two behaviours algorithms reactive and registry based the results show that the reactive behaviour highlights the dominant strategy in a 2 strategy game making agents quickly convert their profiles to the only strategy that could win the conflict reactive can convert the entire population independently of the initial strategy distribution the registry based is sensitive to the chosen game payoffs ultimately featuring strategies that are more lucrative or less prejudicial to the agents registry based may feature a cluster of agents that have positive payoffs when competing for resources and the interaction model may influence landscape transformation where the most successful strategy dominates most of the spatial area for future work we intend to develop new experiments featuring games without dominant or with more than one dominant strategy e g battle of sexes matching pennies games with more than two strategies e g rock paper scissors hawk dove retaliator and decision algorithms that include the amount of exploration made by agents while playing a strategy also the model and experiments presented would benefit from parameter tuning and sensitivity analysis techniques for better adjustment of parameters and further comprehension of the results these modifications and adjustments could bring more diverse spatial configuration and population profiles to ecological scenarios declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported in part by grant 311301 2018 5 given to prof c g ralha by the brazilian national council for scientific and technological development cnpq which is not responsible for its contents appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105252 
25697,hyperparameter optimization approaches were applied to improve performance and accuracy of groundwater flow models freely available new software nwtopt is described that uses tree of parzen estimators tpe and random search algorithms to optimize modflow nwt s solver settings we ran 3500 trials on a steady state and transient model to quantify the performance of candidate solver settings we defined a loss function based on time elapsed and mass balance error of the modflow nwt forward run before optimization the steady state model ran in 12 min and the transient model ran in 5 h with acceptable mass balance error 1 after optimization runtimes were reduced to 2 7 min steady state and 48 min transient with errors below 0 1 in both cases tpe found hyperparameters that resulted in faster running and lower error models than those found by random search the time to complete the optimization trials was also shorter with the tpe algorithm keywords groundwater modeling hyperparameter optimization parameter estimation uncertainty calibration 1 introduction the release of powerful new solvers for environmental models provides opportunities to improve simulations used for decision making however these sophisticated algorithms are often mathematically complex which makes it difficult to select optimal settings a priori moreover their sophistication also provides more inputs to vary which makes attaining sufficient experience to optimize their selection more time consuming here we describe new software that applies a nonlinear optimization approach to the newton raphson solver inputs of modflow nwt niswonger et al 2011 a commonly applied groundwater flow model used for decision support e g bakinam et al 2018 rossetto et al 2018 the software described here can be applied to steady state and transient modflow nwt models is constructed for python on a linux operating system and is freely available online newcomer et al 2021 moreover the approach can be extended to any complex solver optimization of solver settings is not well suited for traditional optimization such as the derivative methods used by pest doherty 2021 and pest white et al 2020 because of the potential for model instability and exceedingly nonlinear responses resulting from small changes of solver input thus the solver optimization problem is not characterized by coherent and systematic model responses to small changes in solver settings but instead consists of tipping points and thresholds where small changes can lead to large and often non monotonic behavior or model failure in this work we use an algorithm intended for similar extremely nonlinear machine learning problems hyperparameter optimization as a means to overcome the inherent issues of nonlinearity and model instability in the machine learning context hyperparameters are parameters whose values are used to determine how a machine learning process the model itself is conducted in the modflow context hyperparameters are the adjustable input available in the modflow codes that manipulates solver behavior thus the hyperparameter optimization manipulates the solver input runs the model and evaluates how well the solver settings performed traditionally simple hyperparameter optimization techniques include approaches such as grid search lavalle et al 2004 and random search bergstra and bengio 2012 for a modflow nwt model there are 26 solver inputs hyperparameters to optimize many of which are continuous rather than discrete thus a grid search approach is computationally infeasible for most users although a random search may fortuitously identify solver settings that provide superior performance than the initial solver settings it was provided it does not learn from previous trials so is not able to optimize those settings that is a random search result may be better than manually derived solver settings but it likely is not among the best settings possible given such limitations of simple techniques other hyperparameter optimization approaches have also been developed in this work we investigate the use of tree of parzen estimators tpe a more advanced hyperparameter optimization algorithm that has been shown to have high performance in semi high dimensionality optimization problems thus the work here investigates whether tpe can efficiently learn from running modflow to identify candidate solver settings that improve performance 2 methods the optimization approach we coded in newcomer et al 2021 is a distributed parallel computing and modflow specific extension of hyperopt bergstra et al 2013a 2013b hyperopt provides random search and tpe hyperparameter optimization algorithms a worker script and a python interface used to communicate with workers through a persistent database that tracks and synthesizes the workers results the software provided by newcomer et al 2021 includes extensions to hyperopt that facilitate optimization of modflow nwt newton raphson solver settings in a high throughput computing environment i e see fienen and hunt 2015 the hyperparameter optimization described here is considered an embarrassingly or pleasingly parallel computing problem one without the need for communication of intermediate results between workers while they run as a result the hyperparameter optimization can attain great speedups from high throughput computing as more workers are brought to bear which allows nwtopt to return results faster to demonstrate the benefits of the advanced tpe hyperparameter optimization method tpe optimization was compared to random search optimization using previously published steady state and transient modflow models whereas random search uses a specified seed to develop a random set of candidate hyperparameter sets trials that are simply run sequentially a more advanced approach to hyperparameter optimization involves learning from early trials to tune the candidates tested as follows koehrsen 2018 the standard hyperparameter optimization problem can be represented as x a r g min x x f x the function f x represents an objective function to minimize x represents the search space and x represents optimal hyperparameter settings in order to translate this optimization problem into a modflow specific problem the search space and objective function hereafter referred to by its commonly used term loss need to be defined the search space simply represents the range of possible values for each of the newton raphson solver inputs and represents the range of hyperparameters considered during optimization definition of a representative loss term for evaluating the performance of candidate solver settings is more involved for numerical modflow models solver performance is judged using time elapsed and mass balance error that results from solving a finite difference formulation of the groundwater flow equation e g a widely used guideline of anderson et al 2015 suggests that mass balance errors larger than 1 reflect unacceptably performing models to align with this dual metric for solver performance the nwtopt loss calculation accounts for both time elapsed and mass balance error when reporting performance of a given set of solver settings after reading information on the elapsed time and mass balance error from the modflow list file a loss is computed where mass balance errors higher than 1 are more highly penalized and lower time elapsed are favored formally the loss term is calculated as loss x e m 2 t here t represents elapsed runtime in seconds and m represent the mass balance error of a run using hyperparameters x tpe along with gaussian processes and random forest regression optimization techniques works by defining a surrogate function often referred to as a response surface which is a probabilistic representation of the loss based on previous trials tpe works by separating trials into good g x and bad b x trial groups based on a predetermined quantile of losses from finished trials the boundary between good and bad trial groups is represented by loss y with y representing the loss boundary if y y the trial is in g x otherwise the trial is in b x these trial groups are then used to define an expected improvement ei function derived from the widely used bayes theorem e g page 451 in anderson et al 2015 koehrsen 2018 for our purposes this function can be expressed as follows e i y x y y y p y x d y using the definitions above and by substituting γ p y y after simplification this becomes e i y x γg x g x y p y d y γg x 1 γ b x γ b x g x 1 γ 1 this formulation of the ei can be thought of as surrogate model for a modflow run with given set of solver settings x ei is proportional to g x b x and thus to maximize the ei we should maximize this ratio additional discussion of bayesian methods and the tpe algorithm itself is beyond our scope and have already been explored by bergstra et al 2013a and koehrsen 2018 for our purposes it is sufficient to note that this ei function uses information from previous trials and associated hyperparameter sets to identify candidate hyperparameter sets that have greatest probability to further reduce the loss term i e have shorter runtimes with acceptably small mass balance error this ei is directly correlated to g x b x so a higher performing optimization will minimize the bad trial groups b x in the denominator and maximize the good trial groups g x in the numerator after every loss function evaluation the tpe algorithm processes previous trials and generates an ei updated response surface from this response surface new candidates for solver hyperparameters are generated by selecting a hyperparameter with the largest expected improvement essentially the tpe algorithm is always looking to improve looking back on previous trials seeing what worked and what did not and generating a new set of hyperparameters that have the highest probability of improving model performance as judged by our previously reported loss function unlike commonly used nonlinear regression algorithms like gauss levenberg marquardt e g page 9 in white et al 2020 tpe will revisit other locations in hyperparameter space when the improvement slows which makes it more resistant to suboptimal solutions caused by local minima therefore tpe learns thus is more efficient than random search approaches and is less hampered by local minima thus more robustly identifies global minima in addition to the advantages of the tpe algorithm hyperopt also provides a run management framework to implement the algorithm across multiple workers simultaneously substantially speeding the identification of optimal hyperparameters there are three main elements to the hyperopt run management the worker the master and the database which facilitates restarting nwtopt mid optimization and provides a central repository of results fig 1 the nwtopt software of newcomer et al 2021 adapts the hyperopt system for modflow nwt and augments the hyperopt run management by leveraging high throughput computing capabilities htcondor team 2021 fienen and hunt 2015 and database capabilities through use of a mongodb https www mongodb com the nwtopt workflow uses this augmented framework to 1 start a database to store results 2 send the nwtopt and modflow runfiles the payload to each worker 3 run and post process modflow runs and 4 evaluate the loss calculated from each modflow run to update the candidate set of hyperparameters optimization efficiencies are realized when larger numbers of trials 25 are tested concurrently the nwtopt scripts facilitate this scaling up through htcondor which is used to create distribute and start the nwtopt worker as well as clean the remote machines after the nwtopt job completes the nwtopt payload contains a python environment as well as all the files needed to run the modflow model the workers run a nwtopt worker script that establishes communication with the mongodb and requests hyperparameters to run on the given modflow model while the workers are connecting to the mongodb the master connects to the mongodb and populates it with hyperparameters using the tpe algorithm initial picks are random sets of hyperparameters until the first trial evaluations are completed after receiving hyperparameters from the mongodb the workers run modflow afterwards the loss of the run is locally calculated on the worker by extracting the elapsed time and mass balance errors from the modflow list output file which gets sent back to the mongodb where it is used to inform the running table of candidate hyperparameters candidate hyperparameters are sent out when a worker becomes free because the mongodb running tables are dynamically updated as new results are obtained nwtopt can quickly drop regions of hyperparameter space when it has learned they are suboptimal the optimization continues until a user selected number of hyperparameter trials are completed all jobs and runs are keyed and stored allowing users to stop restart or extend a nwtopt optimization at any point 3 design of modflow tests to demonstrate the utility of the nwtopt hyperparameter optimization approach we tested its ability to optimize solver hyperparameters using both steady state and transient modflow models that included both unconfined and confined conditions the steady state test used a modflow nwt model for the partridge river in northern minnesota described by haserodt et al 2021 which challenged the solver because the model domain is dominated by wetlands on a thin unconsolidated aquifer overlying mined precambrian bedrock having a range of hydraulic conductivity values and extensive stream network the transient test used a modflow nwt model used for decision support to assess long term declining water levels in the mississippi alluvial plain map described by hunt et al 2021 the transient wetting and drying as well as thin model layers high levels of pumping and dense stream network resulted in a nonlinear model that challenged the modflow solver before optimization the partridge river steady state model ran in 12 min and the map transient model ran in 5 h with acceptable mass balance error i e less than 1 the list of modflow nwt hyperparameters optimized and ranges tested are shown in table 1 full description of each hyperparameter is given by niswonger et al 2011 to assess the importance of learning for identifying optimal hyperparameters the nwtopt system was run twice using two different optimization algorithms tpe the default of nwtopt and random search 4 results and discussion because the nwtopt tpe algorithm begins with a random search of the hyperparameter space both random search and tpe perform relatively similarly for the first 500 trials of the steady state model fig 2 after 500 trials nwtopt s tpe algorithm has sufficiently characterized the hyperparameter space that it begins to outperform the random search approach the sharp dip in loss realized around trial 500 in fig 2 at the completion of 3500 trials nwtopt s tpe best loss was 163 520 a reported mass balance error of 0 01 running in 163 504 s and random search s best loss was 247 774 a mass balance error of 0 01 running in 247 749 s thus the tpe algorithm was able to identify hyperparameters that had an optimal runtime 66 of that obtained from random search with a similar small mass balance error nwtopt s tpe algorithm also outperformed random search when applied to the map transient model fig 3 where tpe took almost 1500 trials to identify an optimal hyperparameter region that appreciably reduced the loss as evidenced by the sharp dip in loss near 1500 trials fig 3 indeed near 750 trials tpe was outperformed by random search however given its random nature a superior performance of random search for any subset of trials is more coincidental rather than an expected outcome of comparison of the two approaches moreover the effectiveness of tpe s learning process is easily seen around trial 1500 tpe learning was sufficient such that an optimal hyperparameter space was found and began to systematically improve hyperparameters fig 3 at 3500 trials tpe s best trial had a loss of 2887 704 a mass balance error of 0 04 running in 48 05 min and random search s best trial had a loss of 3543 231 a mass balance error of 0 30 running in 53 97 min again tpe was able to find better solver hyperparameters ones that resulted in faster running and lower mass balance error models than that those found by random search it should be noted that both approaches provided solver hyperparameters far superior to the initial hyperparameters provided to nwtopt selected based on trial and error ad hoc evaluations and user experience more testing may elucidate which modflow problem characteristics affect tpe s finding of an optimal hyperparameter region to exploit e g 500 trials for the steady state model and 1500 trials for the transient model in the test models shown here yet it is not unreasonable to expect that the difference between steady state and transient solver optimization is systematic where transient models are expected to typically require higher number of trials to identify and explore the optimal region in steady state models the solver is typically evaluated using one stress period which provides a clear signal of how changes in solver parameters affect the elapsed time of the run and the mass balance error in transient models solver settings apply to multiple stress periods however therefore solver settings beneficial for one type of stress and time discretization may be deleterious for others thus identifying the largest net gain over the entire modflow run likely takes more trials to ascertain although not obvious from these results modflow solver hyperparameters are extremely nonlinear where changing one parameter slightly can cause erroneously short or anomalously long runtimes trials are deemed error runs using two metrics first nwtopt determines unacceptable or failed trials by examining the elapsed time reported in the modflow list file list files lacking the modflow nwt standard reporting of elapsed time are interpreted to represent an unstable model where solver settings were too extreme for the modflow problem being solved thus modflow could not complete secondly unacceptably long reported runtimes e g runtimes longer than a modeler provided expected runtime are considered error runs as they are worse than initial performance provided by the preoptimization user supplied solver settings in the case of long runtimes nwtopt also performs preemption the nwtopt worker tracks the modflow forward runtime and terminates the run when its runtime exceeds the user supplied time and reports the trial as an error run nwtopt assigns both types of error runs with a very high loss 1012 informing the optimization that those regions of the hyperparameter space are suboptimal in general the learning associated with tpe is expected to reduce the number of failed runs because it does not learn random search can be expected to experience a similar rate of failed runs even in late trials fig 4 whereas this holds true for the map transient optimization e g few or no failed runs in later trials fig 4 the partridge river steady state optimization results in fig 4 does not reflect this general observation the steady state result likely stems from using more nwtopt trials than needed because the optimal hyperparameters were found quickly subsequent trials could only further investigate the hyperspace region using more extreme solver hyperparameters which results in more failed runs in later trials or put another way after the optimal hyperparameter space is found 500 trials for the steady state model tpe can end up searching the remaining hyperparameter space in a random search manner if there is nothing more to be learned moreover results in fig 4 further support the finding that steady state modflow models are expected to need fewer trials to find optimal solver settings than transient models at first viewing results of our testing could be interpreted that random search performs adequately especially considering initial nonoptimized runtimes however there are additional benefits of a nwtopt s tpe optimization that can be noted first the tpe distribution of trials has more non error trials and is centered on runs with lower loss as compared to random search figs 5 and 6 the random search distribution on the other hand has more less desirable higher losses examination of the modflow outputs that comprise those runs show that much of the higher loss is due to higher mass balance errors an unambiguous flag for unacceptable modflow runs nwtopt provides helper scripts that allow the modeler to look at each trial in depth and choose from the family of possible optimal hyperparameter settings that reflect their preferred trade off of mass balance error and speed furthermore tpe s larger exploration of the low loss portions of hyperparameter space result in a more thorough evaluation of the global minima the second and perhaps equally important benefit of tpe learning over random search is that as tpe learns it starts directing additional runs at the optimal hyperparameter space figs 5 and 6 characterized by faster running models resulting in the higher likelihood that runtimes of each trial will be lower than random search with faster running models the total time for tpe to complete the number of trials specified by the user can be expected to be appreciably shorter than with random search a result observed in our tests here for both the partridge river steady state and map transient modflow models 3500 trials were asynchronously distributed across 375 workers using the htcondor capabilities of nwtopt the partridge river steady state tpe optimization took 81 min to complete 3500 trials the random search optimization took 101 min for the map transient modflow model tpe optimization took 17 4 h to complete 3500 trials random search took 34 9 h to finish 3500 trials over twice as long as the tpe optimization 5 conclusions and importance for applied modeling manual hyperparameter optimization can be tedious and inefficient in the best cases and ineffective in the worst case sophisticated modflow solvers have many settings and feedbacks between settings that make gaining experience about optimal settings difficult the nwtopt software described here provides a framework that facilitates systematic evaluation of the modflow solver hyperparameters that can attain appreciable speed ups while maintaining acceptable mass balance errors due to the distributed computing nature of nwtopt thousands of hyperparameter trials can be evaluated quickly it should be noted however that improvements to solver inputs typically cannot overcome conceptual structural or model input errors that often occur in complex groundwater models e g specifying unrealistically high pumping rates in low hydraulic conductivity layers thus poor convergence should first be evaluated with these modeling errors in mind and such errors corrected before applying tools such as nwtopt limited testing indicates that reduced runtimes at initial parameters retain speedups over manually selected solver hyperparameters even as other modflow input changes e g hydraulic conductivity field recharge rates attaining speedup in such forward runs in turn can greatly reduce the time needed to run parameter estimation and uncertainty analyses which typically run the forward model thousands to hundreds of thousands of times for example one test reported in hunt et al 2021 needed 31 469 forward runs of the modflow nwt transient model used in our testing had nwtopt optimized solver settings been available and been optimal for the entire parameter estimation run it would have reduced the total central processing unit cpu time needed to complete the test by about 5600 days 15 3 years such savings can be especially important when these analyses are performed using cloud computing e g hunt et al 2010 where cost is calculated according to the time a cloud resource is used however although the nwtopt optimal hyperparameters can be expected to retain superiority over manually selected solver settings it is likely hyperparameters will not be strictly optimal as other modflow inputs move away from the initial parameter values tested by nwtopt finally the centralized database and type of information provided by nwtopt analysis facilitates formal investigation of the relation of hyperparameters of complex model solvers to model performance which provides insight into the inner workings and important feedbacks between solver settings and performance such insight in turn can be used to identify solver settings suited for different classes of model spatial temporal and process conceptualizations although currently focusing on modflow nwt and related codes like the current versions of gsflow markstrom et al 2008 the nwtopt code base can be adapted for optimizing the solvers of many models including the newton raphson solvers in modflow 6 langevin et al 2017 and modflow usg panday et al 2013 finally with improved performance from tpe demonstrated additional tests of other advanced hyperparameter optimization techniques become logical areas for future work article impact statement applying machine learning to model solver parameters can appreciably reduce forward model runtime which facilitates calibration and uncertainty analysis software and data availability the software nwtopt was created by the authors listed above and made freely available to the public via a u s geological survey software release of newcomer et al 2021 the software currently supports the linux operating system centos7 and is written in python declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to acknowledge support from the u s geological survey land change science climate research development and water availability and use science programs jeremy white intera inc richard niswonger usgs and an anonymous reviewer are thanked for their review of the manuscript any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 
25697,hyperparameter optimization approaches were applied to improve performance and accuracy of groundwater flow models freely available new software nwtopt is described that uses tree of parzen estimators tpe and random search algorithms to optimize modflow nwt s solver settings we ran 3500 trials on a steady state and transient model to quantify the performance of candidate solver settings we defined a loss function based on time elapsed and mass balance error of the modflow nwt forward run before optimization the steady state model ran in 12 min and the transient model ran in 5 h with acceptable mass balance error 1 after optimization runtimes were reduced to 2 7 min steady state and 48 min transient with errors below 0 1 in both cases tpe found hyperparameters that resulted in faster running and lower error models than those found by random search the time to complete the optimization trials was also shorter with the tpe algorithm keywords groundwater modeling hyperparameter optimization parameter estimation uncertainty calibration 1 introduction the release of powerful new solvers for environmental models provides opportunities to improve simulations used for decision making however these sophisticated algorithms are often mathematically complex which makes it difficult to select optimal settings a priori moreover their sophistication also provides more inputs to vary which makes attaining sufficient experience to optimize their selection more time consuming here we describe new software that applies a nonlinear optimization approach to the newton raphson solver inputs of modflow nwt niswonger et al 2011 a commonly applied groundwater flow model used for decision support e g bakinam et al 2018 rossetto et al 2018 the software described here can be applied to steady state and transient modflow nwt models is constructed for python on a linux operating system and is freely available online newcomer et al 2021 moreover the approach can be extended to any complex solver optimization of solver settings is not well suited for traditional optimization such as the derivative methods used by pest doherty 2021 and pest white et al 2020 because of the potential for model instability and exceedingly nonlinear responses resulting from small changes of solver input thus the solver optimization problem is not characterized by coherent and systematic model responses to small changes in solver settings but instead consists of tipping points and thresholds where small changes can lead to large and often non monotonic behavior or model failure in this work we use an algorithm intended for similar extremely nonlinear machine learning problems hyperparameter optimization as a means to overcome the inherent issues of nonlinearity and model instability in the machine learning context hyperparameters are parameters whose values are used to determine how a machine learning process the model itself is conducted in the modflow context hyperparameters are the adjustable input available in the modflow codes that manipulates solver behavior thus the hyperparameter optimization manipulates the solver input runs the model and evaluates how well the solver settings performed traditionally simple hyperparameter optimization techniques include approaches such as grid search lavalle et al 2004 and random search bergstra and bengio 2012 for a modflow nwt model there are 26 solver inputs hyperparameters to optimize many of which are continuous rather than discrete thus a grid search approach is computationally infeasible for most users although a random search may fortuitously identify solver settings that provide superior performance than the initial solver settings it was provided it does not learn from previous trials so is not able to optimize those settings that is a random search result may be better than manually derived solver settings but it likely is not among the best settings possible given such limitations of simple techniques other hyperparameter optimization approaches have also been developed in this work we investigate the use of tree of parzen estimators tpe a more advanced hyperparameter optimization algorithm that has been shown to have high performance in semi high dimensionality optimization problems thus the work here investigates whether tpe can efficiently learn from running modflow to identify candidate solver settings that improve performance 2 methods the optimization approach we coded in newcomer et al 2021 is a distributed parallel computing and modflow specific extension of hyperopt bergstra et al 2013a 2013b hyperopt provides random search and tpe hyperparameter optimization algorithms a worker script and a python interface used to communicate with workers through a persistent database that tracks and synthesizes the workers results the software provided by newcomer et al 2021 includes extensions to hyperopt that facilitate optimization of modflow nwt newton raphson solver settings in a high throughput computing environment i e see fienen and hunt 2015 the hyperparameter optimization described here is considered an embarrassingly or pleasingly parallel computing problem one without the need for communication of intermediate results between workers while they run as a result the hyperparameter optimization can attain great speedups from high throughput computing as more workers are brought to bear which allows nwtopt to return results faster to demonstrate the benefits of the advanced tpe hyperparameter optimization method tpe optimization was compared to random search optimization using previously published steady state and transient modflow models whereas random search uses a specified seed to develop a random set of candidate hyperparameter sets trials that are simply run sequentially a more advanced approach to hyperparameter optimization involves learning from early trials to tune the candidates tested as follows koehrsen 2018 the standard hyperparameter optimization problem can be represented as x a r g min x x f x the function f x represents an objective function to minimize x represents the search space and x represents optimal hyperparameter settings in order to translate this optimization problem into a modflow specific problem the search space and objective function hereafter referred to by its commonly used term loss need to be defined the search space simply represents the range of possible values for each of the newton raphson solver inputs and represents the range of hyperparameters considered during optimization definition of a representative loss term for evaluating the performance of candidate solver settings is more involved for numerical modflow models solver performance is judged using time elapsed and mass balance error that results from solving a finite difference formulation of the groundwater flow equation e g a widely used guideline of anderson et al 2015 suggests that mass balance errors larger than 1 reflect unacceptably performing models to align with this dual metric for solver performance the nwtopt loss calculation accounts for both time elapsed and mass balance error when reporting performance of a given set of solver settings after reading information on the elapsed time and mass balance error from the modflow list file a loss is computed where mass balance errors higher than 1 are more highly penalized and lower time elapsed are favored formally the loss term is calculated as loss x e m 2 t here t represents elapsed runtime in seconds and m represent the mass balance error of a run using hyperparameters x tpe along with gaussian processes and random forest regression optimization techniques works by defining a surrogate function often referred to as a response surface which is a probabilistic representation of the loss based on previous trials tpe works by separating trials into good g x and bad b x trial groups based on a predetermined quantile of losses from finished trials the boundary between good and bad trial groups is represented by loss y with y representing the loss boundary if y y the trial is in g x otherwise the trial is in b x these trial groups are then used to define an expected improvement ei function derived from the widely used bayes theorem e g page 451 in anderson et al 2015 koehrsen 2018 for our purposes this function can be expressed as follows e i y x y y y p y x d y using the definitions above and by substituting γ p y y after simplification this becomes e i y x γg x g x y p y d y γg x 1 γ b x γ b x g x 1 γ 1 this formulation of the ei can be thought of as surrogate model for a modflow run with given set of solver settings x ei is proportional to g x b x and thus to maximize the ei we should maximize this ratio additional discussion of bayesian methods and the tpe algorithm itself is beyond our scope and have already been explored by bergstra et al 2013a and koehrsen 2018 for our purposes it is sufficient to note that this ei function uses information from previous trials and associated hyperparameter sets to identify candidate hyperparameter sets that have greatest probability to further reduce the loss term i e have shorter runtimes with acceptably small mass balance error this ei is directly correlated to g x b x so a higher performing optimization will minimize the bad trial groups b x in the denominator and maximize the good trial groups g x in the numerator after every loss function evaluation the tpe algorithm processes previous trials and generates an ei updated response surface from this response surface new candidates for solver hyperparameters are generated by selecting a hyperparameter with the largest expected improvement essentially the tpe algorithm is always looking to improve looking back on previous trials seeing what worked and what did not and generating a new set of hyperparameters that have the highest probability of improving model performance as judged by our previously reported loss function unlike commonly used nonlinear regression algorithms like gauss levenberg marquardt e g page 9 in white et al 2020 tpe will revisit other locations in hyperparameter space when the improvement slows which makes it more resistant to suboptimal solutions caused by local minima therefore tpe learns thus is more efficient than random search approaches and is less hampered by local minima thus more robustly identifies global minima in addition to the advantages of the tpe algorithm hyperopt also provides a run management framework to implement the algorithm across multiple workers simultaneously substantially speeding the identification of optimal hyperparameters there are three main elements to the hyperopt run management the worker the master and the database which facilitates restarting nwtopt mid optimization and provides a central repository of results fig 1 the nwtopt software of newcomer et al 2021 adapts the hyperopt system for modflow nwt and augments the hyperopt run management by leveraging high throughput computing capabilities htcondor team 2021 fienen and hunt 2015 and database capabilities through use of a mongodb https www mongodb com the nwtopt workflow uses this augmented framework to 1 start a database to store results 2 send the nwtopt and modflow runfiles the payload to each worker 3 run and post process modflow runs and 4 evaluate the loss calculated from each modflow run to update the candidate set of hyperparameters optimization efficiencies are realized when larger numbers of trials 25 are tested concurrently the nwtopt scripts facilitate this scaling up through htcondor which is used to create distribute and start the nwtopt worker as well as clean the remote machines after the nwtopt job completes the nwtopt payload contains a python environment as well as all the files needed to run the modflow model the workers run a nwtopt worker script that establishes communication with the mongodb and requests hyperparameters to run on the given modflow model while the workers are connecting to the mongodb the master connects to the mongodb and populates it with hyperparameters using the tpe algorithm initial picks are random sets of hyperparameters until the first trial evaluations are completed after receiving hyperparameters from the mongodb the workers run modflow afterwards the loss of the run is locally calculated on the worker by extracting the elapsed time and mass balance errors from the modflow list output file which gets sent back to the mongodb where it is used to inform the running table of candidate hyperparameters candidate hyperparameters are sent out when a worker becomes free because the mongodb running tables are dynamically updated as new results are obtained nwtopt can quickly drop regions of hyperparameter space when it has learned they are suboptimal the optimization continues until a user selected number of hyperparameter trials are completed all jobs and runs are keyed and stored allowing users to stop restart or extend a nwtopt optimization at any point 3 design of modflow tests to demonstrate the utility of the nwtopt hyperparameter optimization approach we tested its ability to optimize solver hyperparameters using both steady state and transient modflow models that included both unconfined and confined conditions the steady state test used a modflow nwt model for the partridge river in northern minnesota described by haserodt et al 2021 which challenged the solver because the model domain is dominated by wetlands on a thin unconsolidated aquifer overlying mined precambrian bedrock having a range of hydraulic conductivity values and extensive stream network the transient test used a modflow nwt model used for decision support to assess long term declining water levels in the mississippi alluvial plain map described by hunt et al 2021 the transient wetting and drying as well as thin model layers high levels of pumping and dense stream network resulted in a nonlinear model that challenged the modflow solver before optimization the partridge river steady state model ran in 12 min and the map transient model ran in 5 h with acceptable mass balance error i e less than 1 the list of modflow nwt hyperparameters optimized and ranges tested are shown in table 1 full description of each hyperparameter is given by niswonger et al 2011 to assess the importance of learning for identifying optimal hyperparameters the nwtopt system was run twice using two different optimization algorithms tpe the default of nwtopt and random search 4 results and discussion because the nwtopt tpe algorithm begins with a random search of the hyperparameter space both random search and tpe perform relatively similarly for the first 500 trials of the steady state model fig 2 after 500 trials nwtopt s tpe algorithm has sufficiently characterized the hyperparameter space that it begins to outperform the random search approach the sharp dip in loss realized around trial 500 in fig 2 at the completion of 3500 trials nwtopt s tpe best loss was 163 520 a reported mass balance error of 0 01 running in 163 504 s and random search s best loss was 247 774 a mass balance error of 0 01 running in 247 749 s thus the tpe algorithm was able to identify hyperparameters that had an optimal runtime 66 of that obtained from random search with a similar small mass balance error nwtopt s tpe algorithm also outperformed random search when applied to the map transient model fig 3 where tpe took almost 1500 trials to identify an optimal hyperparameter region that appreciably reduced the loss as evidenced by the sharp dip in loss near 1500 trials fig 3 indeed near 750 trials tpe was outperformed by random search however given its random nature a superior performance of random search for any subset of trials is more coincidental rather than an expected outcome of comparison of the two approaches moreover the effectiveness of tpe s learning process is easily seen around trial 1500 tpe learning was sufficient such that an optimal hyperparameter space was found and began to systematically improve hyperparameters fig 3 at 3500 trials tpe s best trial had a loss of 2887 704 a mass balance error of 0 04 running in 48 05 min and random search s best trial had a loss of 3543 231 a mass balance error of 0 30 running in 53 97 min again tpe was able to find better solver hyperparameters ones that resulted in faster running and lower mass balance error models than that those found by random search it should be noted that both approaches provided solver hyperparameters far superior to the initial hyperparameters provided to nwtopt selected based on trial and error ad hoc evaluations and user experience more testing may elucidate which modflow problem characteristics affect tpe s finding of an optimal hyperparameter region to exploit e g 500 trials for the steady state model and 1500 trials for the transient model in the test models shown here yet it is not unreasonable to expect that the difference between steady state and transient solver optimization is systematic where transient models are expected to typically require higher number of trials to identify and explore the optimal region in steady state models the solver is typically evaluated using one stress period which provides a clear signal of how changes in solver parameters affect the elapsed time of the run and the mass balance error in transient models solver settings apply to multiple stress periods however therefore solver settings beneficial for one type of stress and time discretization may be deleterious for others thus identifying the largest net gain over the entire modflow run likely takes more trials to ascertain although not obvious from these results modflow solver hyperparameters are extremely nonlinear where changing one parameter slightly can cause erroneously short or anomalously long runtimes trials are deemed error runs using two metrics first nwtopt determines unacceptable or failed trials by examining the elapsed time reported in the modflow list file list files lacking the modflow nwt standard reporting of elapsed time are interpreted to represent an unstable model where solver settings were too extreme for the modflow problem being solved thus modflow could not complete secondly unacceptably long reported runtimes e g runtimes longer than a modeler provided expected runtime are considered error runs as they are worse than initial performance provided by the preoptimization user supplied solver settings in the case of long runtimes nwtopt also performs preemption the nwtopt worker tracks the modflow forward runtime and terminates the run when its runtime exceeds the user supplied time and reports the trial as an error run nwtopt assigns both types of error runs with a very high loss 1012 informing the optimization that those regions of the hyperparameter space are suboptimal in general the learning associated with tpe is expected to reduce the number of failed runs because it does not learn random search can be expected to experience a similar rate of failed runs even in late trials fig 4 whereas this holds true for the map transient optimization e g few or no failed runs in later trials fig 4 the partridge river steady state optimization results in fig 4 does not reflect this general observation the steady state result likely stems from using more nwtopt trials than needed because the optimal hyperparameters were found quickly subsequent trials could only further investigate the hyperspace region using more extreme solver hyperparameters which results in more failed runs in later trials or put another way after the optimal hyperparameter space is found 500 trials for the steady state model tpe can end up searching the remaining hyperparameter space in a random search manner if there is nothing more to be learned moreover results in fig 4 further support the finding that steady state modflow models are expected to need fewer trials to find optimal solver settings than transient models at first viewing results of our testing could be interpreted that random search performs adequately especially considering initial nonoptimized runtimes however there are additional benefits of a nwtopt s tpe optimization that can be noted first the tpe distribution of trials has more non error trials and is centered on runs with lower loss as compared to random search figs 5 and 6 the random search distribution on the other hand has more less desirable higher losses examination of the modflow outputs that comprise those runs show that much of the higher loss is due to higher mass balance errors an unambiguous flag for unacceptable modflow runs nwtopt provides helper scripts that allow the modeler to look at each trial in depth and choose from the family of possible optimal hyperparameter settings that reflect their preferred trade off of mass balance error and speed furthermore tpe s larger exploration of the low loss portions of hyperparameter space result in a more thorough evaluation of the global minima the second and perhaps equally important benefit of tpe learning over random search is that as tpe learns it starts directing additional runs at the optimal hyperparameter space figs 5 and 6 characterized by faster running models resulting in the higher likelihood that runtimes of each trial will be lower than random search with faster running models the total time for tpe to complete the number of trials specified by the user can be expected to be appreciably shorter than with random search a result observed in our tests here for both the partridge river steady state and map transient modflow models 3500 trials were asynchronously distributed across 375 workers using the htcondor capabilities of nwtopt the partridge river steady state tpe optimization took 81 min to complete 3500 trials the random search optimization took 101 min for the map transient modflow model tpe optimization took 17 4 h to complete 3500 trials random search took 34 9 h to finish 3500 trials over twice as long as the tpe optimization 5 conclusions and importance for applied modeling manual hyperparameter optimization can be tedious and inefficient in the best cases and ineffective in the worst case sophisticated modflow solvers have many settings and feedbacks between settings that make gaining experience about optimal settings difficult the nwtopt software described here provides a framework that facilitates systematic evaluation of the modflow solver hyperparameters that can attain appreciable speed ups while maintaining acceptable mass balance errors due to the distributed computing nature of nwtopt thousands of hyperparameter trials can be evaluated quickly it should be noted however that improvements to solver inputs typically cannot overcome conceptual structural or model input errors that often occur in complex groundwater models e g specifying unrealistically high pumping rates in low hydraulic conductivity layers thus poor convergence should first be evaluated with these modeling errors in mind and such errors corrected before applying tools such as nwtopt limited testing indicates that reduced runtimes at initial parameters retain speedups over manually selected solver hyperparameters even as other modflow input changes e g hydraulic conductivity field recharge rates attaining speedup in such forward runs in turn can greatly reduce the time needed to run parameter estimation and uncertainty analyses which typically run the forward model thousands to hundreds of thousands of times for example one test reported in hunt et al 2021 needed 31 469 forward runs of the modflow nwt transient model used in our testing had nwtopt optimized solver settings been available and been optimal for the entire parameter estimation run it would have reduced the total central processing unit cpu time needed to complete the test by about 5600 days 15 3 years such savings can be especially important when these analyses are performed using cloud computing e g hunt et al 2010 where cost is calculated according to the time a cloud resource is used however although the nwtopt optimal hyperparameters can be expected to retain superiority over manually selected solver settings it is likely hyperparameters will not be strictly optimal as other modflow inputs move away from the initial parameter values tested by nwtopt finally the centralized database and type of information provided by nwtopt analysis facilitates formal investigation of the relation of hyperparameters of complex model solvers to model performance which provides insight into the inner workings and important feedbacks between solver settings and performance such insight in turn can be used to identify solver settings suited for different classes of model spatial temporal and process conceptualizations although currently focusing on modflow nwt and related codes like the current versions of gsflow markstrom et al 2008 the nwtopt code base can be adapted for optimizing the solvers of many models including the newton raphson solvers in modflow 6 langevin et al 2017 and modflow usg panday et al 2013 finally with improved performance from tpe demonstrated additional tests of other advanced hyperparameter optimization techniques become logical areas for future work article impact statement applying machine learning to model solver parameters can appreciably reduce forward model runtime which facilitates calibration and uncertainty analysis software and data availability the software nwtopt was created by the authors listed above and made freely available to the public via a u s geological survey software release of newcomer et al 2021 the software currently supports the linux operating system centos7 and is written in python declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to acknowledge support from the u s geological survey land change science climate research development and water availability and use science programs jeremy white intera inc richard niswonger usgs and an anonymous reviewer are thanked for their review of the manuscript any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government 
25698,organizations collect an ever increasing amount of data concerning environmental parameters non experts may be confronted with an information overload making the data meaningless a way to circumvent this problem is visualizing the trends in the pollution concentrations measured over time however non experts do not have a mental model to derive air quality information from displayed concentration profiles therefore a large fraction of the stakeholders remains unable to read interpret such data effectively to improve communication with stakeholders we superposed health risk information from 9 different air quality indices aqis on different kinds of graphs the visualization methods are applied on data collected by the belgian environment agency from two monitoring stations located in contrasting regions ghent and vielsalm supplementary spatially distributed pollution is shown using data collected from sentinel 5p satellite despite some limitations of the aqis the applied visualizations methods successfully translate the data obtained into actionable information keywords data visualization human health air quality assessment visual analytics intuitively readable data 1 introduction ambient air pollution has a substantial impact on human health according to the world health organization who ambient air pollution in both cities and rural areas was estimated to cause 4 2 million premature deaths worldwide per year in 2016 world health organization 2018 consequently society attaches increasing importance in the assessment and analysis of environmental parameters according to the world air quality index project in recent years the number of air quality monitoring stations has grown from 8000 in 2014 to more than 30 000 in 2019 world air quality index project 2020 the same source states that measurements of such stations are available in more than 1000 major cities from 100 countries over the world besides such reference measurements clemens et al 2002 kakareka 2012 lissens et al 2000 citizen science projects use an internet of things approach to combine air quality monitoring systems from different locations into a single platform schaefer et al 2020 examples of such networks are luftdaten that consists of over 4000 measuring locations in europe purpleair with about 900 purple air nodes across 5 continents with several nodes growing by about 30 a day smart citizen air quality egg etc black and white 2015 ilieva et al 2018 jiang et al 2016 stavroulas et al 2020 besides remote satellite sensing technologies e g sentinel 5p with tropomi have become an option in the evaluation of air quality bauwens et al 2020 borsdorff et al 2018 de smedt et al 2018 ermioni et al 2020 fu et al 2016 veefkind et al 2012 therefore the amount of data related to air quality is higher than ever and is expected to increase faster in the future a large fraction of the collected data is freely available to citizens and decision makers through the internet scientists prefer to process such data as time series plots and use the raw data behind the graphs to assess air quality however this way of working is not useful when communicating with decision makers and citizens because most of them have limited experience in data science or have an insufficient background in environmental science as a result these individuals are overwhelmed by data and will refrain from taking action gouws and tarp 2017 the situation becomes even more problematic when one realizes that most scientists are focussing on the measurement of environmental parameters while people are mainly interested in the impact of these parameters on their health that impact on health is not always shown in the graphs cognitive psychology tells us that people only use information that is explicitly displayed in the stimulus object e g the data matrix or the graph information that must be stored in memory inferred from the explicit display or created by some mental transformation tends to be discounted or ignored slovic 1972 making information available is not sufficient information must be easily processable slovic 2000 to involve all stakeholders in environmental related decision making processes alternative forms of scientific visualization are needed suggestions to make better figures or to improve the effectiveness of communication can be found in the literature cook and thomas 2005 dasgupta et al 2016 gratton and gratton 2020 kosara and mackinlay 2013 kerren et al 2007 payne et al 1993 rougier et al 2014 by considering the concreteness principle communication can be improved by reducing the cognitive strain of information processing slovic 1972 several strategies can be used to achieve that goal such as making the huge amount of data accessible by illustrating the results e g graphs infographics photos by reducing the amount of data e g weekly monthly or yearly reports calculating averages and their standard deviations or by simplifying the information e g converting measurements of several environmental parameters into a single air quality index at the same time most stakeholders are able to handle more information than only an idiot light i e any warning light or indicator on the dashboard of a car a balance between in depth and easy to read information can be found by exploring different visualization methods such as combining classical graphs with the attribution of colours to data points according to the colour scale of an air quality index representing a quality class du et al 2016 li et al 2016a b liao et al 2014 qu et al 2007 zheng et al 2016 satellite information gives an insight into hotspots while time series gives an insight into hot moments the development of new visualization techniques has become relevant not only in environmental related data but also to other kinds of data in for example satellite information x ray spectra processing or big data there are a meaningful number of publications that reinforce this idea eldawy et al 2015 van der snickt 2016 syed et al 2016 for this purpose a software platform was developed to process publicly available data from several sources i e two measuring stations of the belgian environment agency and satellite information allowing its integration into one unified view one of the problems with the air quality indices aqis is the existence of several guidelines to their calculation leeuw et al 2005 plaia and ruggieri 2011 several publications mention differences between the most important guidelines but the impact of the different aqis on the same data set remains unclear for that reason data from 2 contrasting locations in belgium ghent urban region and vielsalm rural region are processed and the effect of different aqis on the data visualization investigated the practical implication of this contribution is that despite the different aqis and their limitations they are a suitable tool to transform complex data to simpler information moreover it allows an intuitive comparison of datasets from different monitoring stations another powerful tool is combining alternative visualizations such as carpet plots or using supplementary information such as satellite images with the more conventional plots of raw data it facilitates the identification of patterns or features in the data and offers the possibility to reach quick conclusions that would not be so easy to do by simply plotting raw data 2 air quality index guidelines calculation of aqis is a widespread method used to express the concentration of individual pollutants on a common scale describing the overall impact of air on human health afroz et al 2003 gayer et al 2018 kanchan et al 2015 lemeš 2018 therefore its use allows the compression of information collected from multiple compounds into one single value providing a simpler interpretation of the data several countries and institutions define the aqi in different ways table 1 gives an overview of 9 popular methods and how are they defined the aqis in table 1 are a representative sample of some of the most popular ones it is not meant as a complete overview the selection is an attempt to cover different regions in the world and a variety of aqi calculation algorithms all aqis allow the association of a colour to each data point in a time series for a pollutant mentioned in the guideline and the possibility to merge information of several pollutants to an overall appreciation of the air quality table 1 also shows the multiple aspects that make them different from one guideline to the other examples of such aspects are the number of pollutants considered sampling rate definition of the quality classes measurement units etc based on the type of algorithm used to calculate the aqis the methods in table 1 can be classified into 3 groups each algorithm type is briefly described in more detail here below it should be mentioned that the belaqi includes a disclaimer mentioning that the index qualitatively treats air quality and not quantitatively and that it is only meaningful for the visualization of short term effects the belaqi does not make statements about long term effects aqi epa imeca the continuous concentration scale for the past 15 min hour and 8 h for an individual pollutant is divided into consecutive quality classes for this group of guidelines the lower limit and upper limit of each quality category are defined by the concentration of the pollutant and the corresponding sub index the aqi value within a quality category can be calculated by linear interpolation the aqi is defined as a piecewise linear function of the pollutant concentration the overall aqi is determined by the pollutant with the worst score to each quality class of the overall aqi a verbal descriptor and a colour code is associated aqhi can aqhi hk this group of aqis do not rely on different quality classes as is the case with the previously described aqi type instead these aqis combine the concentration of several pollutants of the preceding 3 h in a single mathematical formula that number is rounded to the nearest positive integer and represents the overall air quality each integer between 1 and 10 is associated with a verbal descriptor a colour code a health message for the at risk population and a health message for the general population eaqi caqi daqi uk belaqi belatmo the concentration of the past 15 min past hour and past 8 h of an individual pollutant is a sliding scale that is divided in a series of consecutive quality classes the quality classes are defined by lower and upper concentrations the guidelines attribute to each quality class a verbal descriptor a numeric quantifier i e a scale for 1 to 10 and a colour code the overall air quality is determined by the pollutant with the worst health impact this means that the overall air quality is described by 10 discrete values the aqis in table 1 are also used in different kinds of applications such as in citizen science platforms and apps for mobile devices as can be seen in table 2 a variety of apps can be found in google play store or the app store of apple that visualize air quality all of them are using aqis besides the current individual pollutant concentration and the overall aqi representation additional information such as air quality forecasting historical data visualization comparison between different locations etc are also included in some apps table 2 clearly shows that these apps use a variety of aqis and data visualizations methods some of these apps visualize global information of several countries e g airvisual and airmatters but only from the perspective of one single aqi type the integration of data from several monitoring networks makes it possible to explore and compare air quality between locations from different regions or countries however there are other apps that only show information limited to some specific country or region e g epa s airnow hk aqi or aqhi canada some apps such as breezometer automatically adapts the aqi type depending on the location from where the data is visualized for other cases such as aircare it is possible to select between two different aqi types regardless of the location of the device some of the apps provide supplementary information about weather pollen fire alerts wind speed and direction etc this can be helpful to understand possible causes in the fluctuation of air quality in general all apps have a notification system to let the user know when there is some variation of the air quality state the overview in table 2 suggests that aqis are a popular tool in air quality communication with a substantial group of citizens 3 experimental 3 1 raw data the raw data collected from two monitoring stations belonging to the belgian interregional environmental agency ircel celine measurement network is obtained from the internet ircel celine interactive viewer 2020 the selected locations are vielsalm station 43n085 and ghent station 44r701 besides the obvious difference in location the selection is also based on the availability of data about all environmental parameters needed for the calculation of the 9 air quality indices the parameters analysed are no2 co so2 o3 pm2 5 and pm10 the data selected corresponds to the period between january 1st and december 31st of 2019 collected with a sampling rate of 1 h the concentration values of the parameters studied are expressed in μg m3 data with information about the tropospheric vertical column density vcd of no2 concentration is also collected from the european union s earth observation programme copernicus the vcd of no2 derived from satellite measurements can be considered as an effective proxy for surface nox concentration and it is often used in air quality applications prunet et al 2020 note that in our visualization we did not yet incorporate the potential impact of transportation of chemical compounds as suggested by some authors lamsal et al 2015 the spatial information over the area of interest is only available at certain moments in time the resulting surface concentration of the compounds obtained from the tropospheric column is given in mol m2 de vries et al 2016 the region analysed is defined by the boundaries west longitude 1 9699 east longitude 6 9699 south latitude 47 0039 north latitude 52 0039 this region contains the two locations analysed ghent longitude 3 7174 latitude 51 0543 and vielsalm longitude 5 9147 latitude 50 2840 the measurements are performed by the tropomi instrument in sentinel 5p the no2 data products have a spatial resolution of 7 3 5 km2 van geffen et al 2019 the data can be freely obtained through the copernicus open access hub sentinel 5p pre operations data hub 2020 where it is possible to access the desired data by defining different parameters such as product type product level geographical area or time 3 2 data pre processing air quality indices the visualization of the satellite data is performed without any further data pre processing for the data of the monitoring stations a central moving median with a window size of 8 h is applied to remove outliers and to estimate concentration values at times where no measurement is reported estimation of concentration values is not possible in periods longer than 8 h of missing data the window size is sufficiently small to minimize the distortion of peaks and valleys in the time series some aqi guidelines set different specifications about how the concentrations measured in a certain period should be handled for example the eaqi guideline specifies that the calculation of the air quality index is based on a 24 h average for pm2 5 while the canadian aqhi prescribes a 3 h average consequently a right aligned moving average considering the prescribed time window size is applied to the preceding concentration values to calculate the different aqis mentioned in table 1 3 3 visualization methods it is used different visualization methods to represent the concentration and air quality trends these visualizations show the air quality index translated as colours this allows a more intuitive analysis of the results and the identification of moments where the air quality suddenly changes the colour scale used in these visualization methods varies depending on the guideline applied to the data the visualization methods are the following time series plot hopke et al 2001 kai 2008 modarres and khosravi 2005 shumway and stoffer 2017 a direct interpretation of the time series regarding the impact on human health is made possible by filling the area below the curve with the corresponding colour of the calculated air quality index such plots are generated by code developed in python 3 using the matplotlib library devert 2014 van rossum and drake 2011 temporal raster plot carpet plots carslaw and ropkins 2012 mintz 2018 carpet plots are generated by plotting data in this case the air quality index over days x axis and time of day in hours y axis while varying the aqi related colour of each data point the advantage of this type of chart is that it visualizes both short term hourly and long term monthly patterns carpet plots are created by in house developed software in python 3 using the matplotlib library correlation matrix a table showing correlation coefficients between variables is generated with in house developed software in python 3 the correlation matrix is subdivided in tiles each tile contains a number that can vary between 1 and 1 this number is the correlation coefficient and it is a measure of the strength and direction of the linear relationship between two variables in this case the different aqis evaluated on the irceline data the tiles are also represented with a colour which depends on the correlation coefficient determined this facilitates the interpretation of the matrix dendrogram it is a tree like structure that visualizes the result of hierarchical clustering it visualizes the similarity between data points within a set and allows the user to classify the data points in several clusters this type of analysis is performed on the columns of the correlation matrix the dendrogram is created with python 3 using the scipy library virtanen 2020 percentage stacked bars this type of plot is used to represent the percentage of time air quality falls in each class by using this visualization method the relation between air quality and time is lost but it is easier to compare the air quality between different locations for a given period the graphs are generated with microsoft excel macdonald 2007 spatial plot o sullivan 2014 the measurements of no2 obtained from the tropomi instrument over belgium are plotted in a map the plots are created with python 3 and the cartopy library met office 2017 3 3 1 user experience study to evaluate the effectiveness of the different visualization methods a thinking aloud test is carried out boren 2000 van samoren 1994 this kind of study consists of recruiting representative users and give them representative tasks while verbalizing their thoughts in the case of this work 4 stakeholders are involved each with a different age and scientific background a scenario context is provided where the user compares the air quality situation between vielsalm and ghent based on the visualization methods previously described 4 results and discussion 4 1 classic graphs showing trends the visualization method shown in fig 1 provides the possibility to obtain valuable information about trends and moments where concentration suddenly changes for humans comparisons within a graph or between graphs are more meaningful than the absolute numbers slovic 2000 kong et al 2019 by comparing the graphs of both locations it is possible to see that ghent has higher concentration values of no2 than vielsalm although with this type of visualization it is relatively easy to detect changes within a graph and to identify peaks it is difficult to determine how relevant these peaks are from fig 1 it is also possible to see that for both locations there is an increase in o3 during spring and summer this visualization represents an advantage compared to a tabular representation of concentrations but it has severe limitations the impact of the concentration variations on our health is not shown directly meaning that this information must be extracted by inference people can focus their attention to a small part of the sensory input e g a peak in the time series attracting our attention is seen by the fovea centralis within the retina in our eye while the rest of the time series is located at the edge of our visual field where vision is poor irizarry 2019 kong 2019 ware 2010 unfortunately there is no direct clear contrast in the plots between moments of elevated health risk and periods with acceptable risk that can attract our attention therefore non experts do not know where to look first a way to avoid this problem is by zooming in and only showing the concentration behaviour of a short period see right details in fig 1 however by changing the plot timescale some important features or patterns may be missed the thinking aloud study reveals that at first glance when people see the plots they relate the extent of pollution with amplitude and frequency concentration variations this might lead to wrong conclusions for example in some cases steady behaviour of high concentration values can be more meaningful to human health than peaks with low maximum values the number of peaks provides valuable information on how the air quality changes but at the same time it might provide irrelevant information about its impact on human health moreover when users want to analyse pollutants individually they tend to look at how many times the concentration reached the top of the graph this illustrates the importance of using the same scale when comparing two graphs in general the users found this type of graph helpful to know how the different substances behave along a year or to compare between different seasons at the same time it is difficult for them to evaluate how the concentration values shown in the graphs affect their health 4 2 graphs with superposed aqi information fig 2 shows the same data as in fig 1 but each moment in time is coloured using the belaqi the guideline that corresponds with the region where the data are collected according to the thinking aloud study the colours make the data simpler to read because 1 the plots contain health information 2 peaks with elevated health risk are shown in a contrasting colour and 3 the same parameter of the 2 locations are easier to compare the following conclusions can be made in a visual way no 2 the air quality related to no2 in vielsalm i e a rural region is predominantly excellent while in ghent i e an urban region with more traffic it varies from excellent to fairly good the seasonal trend in ghent shows that the air quality is slightly better in summer times than in winter for o3 the opposite behaviour is seen in ghent however this conclusion demands a more detailed analysis of the graph o 3 in vielsalm the air quality related to ozone varies from excellent to moderate while in ghent it fluctuates between excellent to fairly good in both cases the impact of ozone on our health is higher during the summer moreover the situation in rural vielsalm is worse than in ghent this is an expected phenomenon the occurrence of this so called ozone paradox is a phenomenon based on the chemical transformation of o3 by nox compounds this transformation occurs more often in cities than in rural areas because of the higher amounts of nox bocci et al 2009 jenkins and ryu 2004 peter 2007 pm 2 5 and pm 10 the air quality changes from excellent to moderate in vielsalm while in ghent it fluctuates from excellent to horrible when looking at particulate matter there is a clear difference in air quality between both regions illustrating the intuitive comparison of both plots pm peaks seem to occur in all seasons 4 3 heatmaps with superposed aqi information fig 3 shows the same o3 and no2 belaqi results as in fig 2 but this time the data is presented as a carpet plot the x axis represents the 365 days in a year while the y axis are the 24 h in a day the grey pixels are missing data during the measurement this occurs because there are periods higher than 8 h where no measurements are performed and therefore it is not possible to estimate the concentration value by performing the moving median used the thinking aloud study showed that one of the advantages of carpet plots such as in fig 3 is that they can easily visualize both long term seasonal cycles and short term daily cycles directly moreover it also shows the correlation between both cycles something that remains invisible in figs 1 and 2 from a visual analysis of the carpet plot the following can be learned o 3 during spring and summer there is a clear impact of o3 on our health and more specific after noon i e the hottest moment of the day the comparison between vielsalm and ghent indicates that the air quality due to o3 is better in ghent because the moments of elevated ozone during the day are somewhat shorter however this is difficult to be distinguished by looking at the graph because the difference is small especially when you look at the highest peaks in yellow no 2 in vielsalm the concentration is so low that it is not possible to visualize fluctuations using the belaqi colour codes for the no2 in ghent it is possible to see an anticorrelation with the o3 behaviour although the no2 signal visualized with belaqi colour codes is rather weak 4 4 correlation between aqis to have an idea of the level of discrepancy between different aqis mentioned in table 1 that are calculated for both locations a linear correlation analysis is performed see fig 4 in the current analysis all values are higher than 0 meaning that the correlation between the aqis is directly proportional however large variations in the correlations are found between the different aqis this means that the selection of the aqi affects the interpretation of the graphs on top of that the following can be learned from these correlations high correlations between some aqis some aqis show a high correlation such as eaqi and caqi with a correlation of 0 97 tile is represented in a darker red in both locations eaqi and caqi use more similar algorithms in other cases some aqi methods also show high correlation even when algorithms are not similar as the case of imeca and aqhi can in vielsalm with a correlation 0 92 this occurs because the concentration values of the substances used to calculate these aqis have little variation in vielsalm for the same reason the correlation matrix of vielsalm shows darker red colours compared to the one of ghent low correlations between other aqis at the same time other aqis show a low linear correlation such as the case between eaqi and aqi epa who have the lowest correlation coefficients tiles are represented in a lighter red they use a different algorithm to estimate the overall aqi this means that comparisons of different time series are only possible when the same aqi is applied to further understand the similarity between different aqis a hierarchical clustering between the columns of the correlation matrix is performed and the result is visualized as a dendrogram shrestha and kazama 2007 the cluster analysis see fig 5 reveals that for the two locations it is possible to identify two main aqi clusters the first one contains caqi eaqi see green lines in fig 5 and depending on the location either belaqi or daqi uk the second cluster see red lines in fig 5 contains all the other aqis this illustrates that the similarity between aqis depends on the timeseries themselves most of the primary clusters are composed of aqi pairs that have been defined by neighbouring regions such as imeca epa belaqi belatmo caqi eaqi some pairs are not from neighbouring regions such as aqhi can and aqi hk but they have similarities in the way they calculate the index fig 6 shows the behaviour of the pm10 data measured in ghent the health related information presented in this dataset is visualized using 7 different aqi guidelines mentioned in table 1 the aqhi can and aqhi hk are omitted because they merge data from several environmental parameters into a single aqi using one mathematical formula and cannot be used to evaluate only one independent parameter fig 6 illustrates that all aqis emphasize some periods in time that attract our attention in that sense all aqis adds a similar information layer on top of the plots the colour contrasts also emphasize the highest peaks in pm10 the thinking aloud studies revealed that the colour palette facilitates the distinction between important and less important moments especially the case where blue or green represents the highest air quality and red and orange the lowest is easy to understand differences in quality assessments the same data point in time is evaluated differently when considering other aqi guidelines for example the epa gives the impression that most of the time the situation is good i e green colour while for a limited number of small periods the situation is moderate yellow peaks the same data visualized with belaqi shows a much larger palette of colours with at least one horrible moment this shows again that there is no agreement between the guidelines no agreement in visualizing peaks with a contrasting colour an important feature of the guidelines is that some moments are associated with a contrasting colour some aqi guidelines clearly show peaks in a contrasting colour while other guidelines epa and to some extent imeca do not emphasize the same peaks at all the contrast between low risk moments and elevated risk moments are not always clear to attract the attention of the analyst it is better to visualize background concentrations with low risk and peaks with higher health risk by using contrasting colours this is the case for belaqi and belatmo blue background and red peaks for citeair and imeca the colour scale generated a smaller colour contrast and are for that reason less efficient in risk communication impact of colour blindness colours play a central role in data visualization but this additional information is partly lost for colour blind persons the horizontal stacked bars in fig 7 show the percentage of the total period that the air quality falls in one of the given quality categories the stacked bars can be read in 2 different ways 1 a positive approach where one assumes that the region with the highest percentage of time with best quality classes can be considered as the healthiest location and 2 a negative approach where people consider the lowest percentage of worse quality classes as the healthiest this is in agreement with the known asymmetric effect of favourable and unfavourable information on decision making kahneman and tversky 1979 negative events information carry greater weight than positive events moreover harm to a person s health is only determined by the classes of worse quality since in the western world people read from left to right we have deliberately placed the most important information i e the negative information on the left side of the bars for both views most of the aqis state that the overall air quality in vielsalm is better than in ghent except for the case of aqi epa and imeca where the results are not conclusive this occurs because of the influence of o3 concentration on the calculation of these aqis this comparison also shows clear differences between the aqis the goodness of ambient air varies when looking at the percentage of time where ambient air falls in the best quality category it is not clear that all aqi guidelines are equally strict for the united kingdom daqi uk hong kong aqhi hk the ambient air quality is excellent but the same air is considered as rather poor by canada aqhi can there is no international agreement about absolute air quality assessments the difference between ghent and vielsalm varies for some aqis air quality is better in vielsalm than ghent e g eaqi and caqi for these cases the bar sections representing the worst air quality classes are higher in ghent and the bar sections representing the best air quality are bigger in vielsalm for other aqis this is not the case e g aqi epa imeca for such cases both bar sections representing the worst and best quality classes are bigger in vielsalm this produces certain vagueness in the comparison of air quality between the 2 locations the thinking aloud confirms that based on the stacked bar graphs in fig 7 it is possible to determine whether the overall air quality is better in vielsalm or ghent e g eaqi belaqi and caqi users do so by comparing the sizes of the coloured bars although in some cases the lack of alignment of the bars proved difficult for direct comparison with small differences e g the good category section in the belaqi stacked bar besides these graphs compared to the previous graphs provide more simplified information about air quality but misses important information such as information about individual pollutants or behaviour of air quality at one specific time 4 5 satellite information the measuring stations in vielsalm and ghent provide information with high temporal resolution but they do not contain any spatial information fig 8 shows the distribution of the no2 concentration in a tropospheric column as measured by the sentinel 5p satellite over vielsalm and ghent at two different moments the first one on may 24th of 2019 and the second one on september 3rd of 2019 these times are selected to compare both tropomi and ground level monitoring stations at moments when higher levels of pollution are reported the concentration in the complete tropospheric column is not converted into a surface concentration the major limitation is that the concentration of no2 through the vertical atmospheric axe is not homogeneous therefore it is not straightforward to perform a unit s conversion from mol m2 i e surface concentration to ppb i e a volume concentration for that reason it is not possible to associate an aqi colour code to the satellite information however the maps show concentration gradients that are easy to read by non experts fig 8a shows a moment where the concentration is nearly similar at ground level fig 8a corresponds with the detail in figs 1 and 2 where an elevated concentration of no2 in ghent when compared to vielsalm can be observed fig 8b shows the migration of pollutants over longer distances resulting in a higher no2 in vielsalm than in ghent however at ground level that difference is not observed a difference in pollution at ground level and elevated height explains this behaviour wong et al 2017 wang et al 2020 qin et al 2016 satellite information does not generate a unified view with ground level data in a simple way the thinking aloud studies revealed that all users agree that information is easy to understand but there is also a discrepancy in opinions about the relevance of this type of graph to perform the comparison between vielsalm and ghent according to some users the distribution of the pollutants can vary rapidly depending on environmental conditions therefore it is necessary to visualize the air pollution at different time frames to perform a correct comparison however users stated that these graphs might be informative to evaluate where the pollution is coming from or heading to the graphs also provide information on how pollution is distributed over bigger territories the users also were interest on this type of visualization showing the impact on their health by using aqis 5 conclusions this paper explores different visualization methods to make a huge amount of data about environmental parameters understandable to different stakeholders to achieve that goal data points in the time series plots are attributed to a colour code using air quality index guidelines for this several aqis are used and the coloured information is incorporated in different kinds of plots from the thinking aloud study all plots where aqi information is superposed on the time series allowed easier air quality evaluation when compared to the plots where only the trends are shown from the think aloud study we made the following conclusions people arrive at similar conclusions about air quality when locations are compared using the same aqi however the comparison is affected by the selected aqi the colour scale of the aqi highlights peaks with other colours emphasizing the health risk of certain moments relevant moments highlighted with colours are more actionable for example red coloured moments attract our attention therefore this helps to select moments where events are detrimental to human s health are occurring and to take action to avoid such moments however depending on the selected aqi the colour scale can highlight other moments with elevated health risk for the stacked bars the negative information is considered as more important than positive information for that reason we moved it to the left side of the bars the carpet plots are a fairly easy way to explore correlations between short term and long term cycles in an intuitive way the map visualization of satellite information is an intuitive way to show pollution data people see pollution in belgium in a direct way however information can only be shown at specific time frames this is a limitation when people want to know the pollution levels over long periods furthermore users remarked the importance of plotting aqis on maps to have a better understanding of the real impact of pollution on their health this study has shown the advantage of attributing colours to the plots using the aqis this method is already used by most of the consulted apps webpages reports however a major problem with the aqi selection is demonstrated although it seemed logical to use the belgium guideline belaqi for the analysed regions in belgium the conclusions drawn from the figures can be biased by the selected guideline there is no clear correlation between the different algorithms calculating the aqis although the actual health risk for a person exposed to air in a specific location at a specific moment must be the same the analysis performed resulted in the following conclusions some aqis are less strict for some pollutants e g aqi epa than others e g belaqi and this affects the visualization of the data some colour palettes resulted in lower contrasts between background and moments with elevated risk peaks e g caqi intuitive comparison of time series can only be done correctly when the same aqi is applied the similarity between different aqis seems to be affected by the time series itself regardless of the selected aqi guideline users concluded correctly that vielsalm had a better air quality than ghent except for o3 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the belgian federal public planning service science policy belspo under project number br 132 a6 airchecq and by the university of antwerp bof academiseringsproject proof of concept for a decision support system to reduce the occupational risks of seafarers due to air quality appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105230 
25698,organizations collect an ever increasing amount of data concerning environmental parameters non experts may be confronted with an information overload making the data meaningless a way to circumvent this problem is visualizing the trends in the pollution concentrations measured over time however non experts do not have a mental model to derive air quality information from displayed concentration profiles therefore a large fraction of the stakeholders remains unable to read interpret such data effectively to improve communication with stakeholders we superposed health risk information from 9 different air quality indices aqis on different kinds of graphs the visualization methods are applied on data collected by the belgian environment agency from two monitoring stations located in contrasting regions ghent and vielsalm supplementary spatially distributed pollution is shown using data collected from sentinel 5p satellite despite some limitations of the aqis the applied visualizations methods successfully translate the data obtained into actionable information keywords data visualization human health air quality assessment visual analytics intuitively readable data 1 introduction ambient air pollution has a substantial impact on human health according to the world health organization who ambient air pollution in both cities and rural areas was estimated to cause 4 2 million premature deaths worldwide per year in 2016 world health organization 2018 consequently society attaches increasing importance in the assessment and analysis of environmental parameters according to the world air quality index project in recent years the number of air quality monitoring stations has grown from 8000 in 2014 to more than 30 000 in 2019 world air quality index project 2020 the same source states that measurements of such stations are available in more than 1000 major cities from 100 countries over the world besides such reference measurements clemens et al 2002 kakareka 2012 lissens et al 2000 citizen science projects use an internet of things approach to combine air quality monitoring systems from different locations into a single platform schaefer et al 2020 examples of such networks are luftdaten that consists of over 4000 measuring locations in europe purpleair with about 900 purple air nodes across 5 continents with several nodes growing by about 30 a day smart citizen air quality egg etc black and white 2015 ilieva et al 2018 jiang et al 2016 stavroulas et al 2020 besides remote satellite sensing technologies e g sentinel 5p with tropomi have become an option in the evaluation of air quality bauwens et al 2020 borsdorff et al 2018 de smedt et al 2018 ermioni et al 2020 fu et al 2016 veefkind et al 2012 therefore the amount of data related to air quality is higher than ever and is expected to increase faster in the future a large fraction of the collected data is freely available to citizens and decision makers through the internet scientists prefer to process such data as time series plots and use the raw data behind the graphs to assess air quality however this way of working is not useful when communicating with decision makers and citizens because most of them have limited experience in data science or have an insufficient background in environmental science as a result these individuals are overwhelmed by data and will refrain from taking action gouws and tarp 2017 the situation becomes even more problematic when one realizes that most scientists are focussing on the measurement of environmental parameters while people are mainly interested in the impact of these parameters on their health that impact on health is not always shown in the graphs cognitive psychology tells us that people only use information that is explicitly displayed in the stimulus object e g the data matrix or the graph information that must be stored in memory inferred from the explicit display or created by some mental transformation tends to be discounted or ignored slovic 1972 making information available is not sufficient information must be easily processable slovic 2000 to involve all stakeholders in environmental related decision making processes alternative forms of scientific visualization are needed suggestions to make better figures or to improve the effectiveness of communication can be found in the literature cook and thomas 2005 dasgupta et al 2016 gratton and gratton 2020 kosara and mackinlay 2013 kerren et al 2007 payne et al 1993 rougier et al 2014 by considering the concreteness principle communication can be improved by reducing the cognitive strain of information processing slovic 1972 several strategies can be used to achieve that goal such as making the huge amount of data accessible by illustrating the results e g graphs infographics photos by reducing the amount of data e g weekly monthly or yearly reports calculating averages and their standard deviations or by simplifying the information e g converting measurements of several environmental parameters into a single air quality index at the same time most stakeholders are able to handle more information than only an idiot light i e any warning light or indicator on the dashboard of a car a balance between in depth and easy to read information can be found by exploring different visualization methods such as combining classical graphs with the attribution of colours to data points according to the colour scale of an air quality index representing a quality class du et al 2016 li et al 2016a b liao et al 2014 qu et al 2007 zheng et al 2016 satellite information gives an insight into hotspots while time series gives an insight into hot moments the development of new visualization techniques has become relevant not only in environmental related data but also to other kinds of data in for example satellite information x ray spectra processing or big data there are a meaningful number of publications that reinforce this idea eldawy et al 2015 van der snickt 2016 syed et al 2016 for this purpose a software platform was developed to process publicly available data from several sources i e two measuring stations of the belgian environment agency and satellite information allowing its integration into one unified view one of the problems with the air quality indices aqis is the existence of several guidelines to their calculation leeuw et al 2005 plaia and ruggieri 2011 several publications mention differences between the most important guidelines but the impact of the different aqis on the same data set remains unclear for that reason data from 2 contrasting locations in belgium ghent urban region and vielsalm rural region are processed and the effect of different aqis on the data visualization investigated the practical implication of this contribution is that despite the different aqis and their limitations they are a suitable tool to transform complex data to simpler information moreover it allows an intuitive comparison of datasets from different monitoring stations another powerful tool is combining alternative visualizations such as carpet plots or using supplementary information such as satellite images with the more conventional plots of raw data it facilitates the identification of patterns or features in the data and offers the possibility to reach quick conclusions that would not be so easy to do by simply plotting raw data 2 air quality index guidelines calculation of aqis is a widespread method used to express the concentration of individual pollutants on a common scale describing the overall impact of air on human health afroz et al 2003 gayer et al 2018 kanchan et al 2015 lemeš 2018 therefore its use allows the compression of information collected from multiple compounds into one single value providing a simpler interpretation of the data several countries and institutions define the aqi in different ways table 1 gives an overview of 9 popular methods and how are they defined the aqis in table 1 are a representative sample of some of the most popular ones it is not meant as a complete overview the selection is an attempt to cover different regions in the world and a variety of aqi calculation algorithms all aqis allow the association of a colour to each data point in a time series for a pollutant mentioned in the guideline and the possibility to merge information of several pollutants to an overall appreciation of the air quality table 1 also shows the multiple aspects that make them different from one guideline to the other examples of such aspects are the number of pollutants considered sampling rate definition of the quality classes measurement units etc based on the type of algorithm used to calculate the aqis the methods in table 1 can be classified into 3 groups each algorithm type is briefly described in more detail here below it should be mentioned that the belaqi includes a disclaimer mentioning that the index qualitatively treats air quality and not quantitatively and that it is only meaningful for the visualization of short term effects the belaqi does not make statements about long term effects aqi epa imeca the continuous concentration scale for the past 15 min hour and 8 h for an individual pollutant is divided into consecutive quality classes for this group of guidelines the lower limit and upper limit of each quality category are defined by the concentration of the pollutant and the corresponding sub index the aqi value within a quality category can be calculated by linear interpolation the aqi is defined as a piecewise linear function of the pollutant concentration the overall aqi is determined by the pollutant with the worst score to each quality class of the overall aqi a verbal descriptor and a colour code is associated aqhi can aqhi hk this group of aqis do not rely on different quality classes as is the case with the previously described aqi type instead these aqis combine the concentration of several pollutants of the preceding 3 h in a single mathematical formula that number is rounded to the nearest positive integer and represents the overall air quality each integer between 1 and 10 is associated with a verbal descriptor a colour code a health message for the at risk population and a health message for the general population eaqi caqi daqi uk belaqi belatmo the concentration of the past 15 min past hour and past 8 h of an individual pollutant is a sliding scale that is divided in a series of consecutive quality classes the quality classes are defined by lower and upper concentrations the guidelines attribute to each quality class a verbal descriptor a numeric quantifier i e a scale for 1 to 10 and a colour code the overall air quality is determined by the pollutant with the worst health impact this means that the overall air quality is described by 10 discrete values the aqis in table 1 are also used in different kinds of applications such as in citizen science platforms and apps for mobile devices as can be seen in table 2 a variety of apps can be found in google play store or the app store of apple that visualize air quality all of them are using aqis besides the current individual pollutant concentration and the overall aqi representation additional information such as air quality forecasting historical data visualization comparison between different locations etc are also included in some apps table 2 clearly shows that these apps use a variety of aqis and data visualizations methods some of these apps visualize global information of several countries e g airvisual and airmatters but only from the perspective of one single aqi type the integration of data from several monitoring networks makes it possible to explore and compare air quality between locations from different regions or countries however there are other apps that only show information limited to some specific country or region e g epa s airnow hk aqi or aqhi canada some apps such as breezometer automatically adapts the aqi type depending on the location from where the data is visualized for other cases such as aircare it is possible to select between two different aqi types regardless of the location of the device some of the apps provide supplementary information about weather pollen fire alerts wind speed and direction etc this can be helpful to understand possible causes in the fluctuation of air quality in general all apps have a notification system to let the user know when there is some variation of the air quality state the overview in table 2 suggests that aqis are a popular tool in air quality communication with a substantial group of citizens 3 experimental 3 1 raw data the raw data collected from two monitoring stations belonging to the belgian interregional environmental agency ircel celine measurement network is obtained from the internet ircel celine interactive viewer 2020 the selected locations are vielsalm station 43n085 and ghent station 44r701 besides the obvious difference in location the selection is also based on the availability of data about all environmental parameters needed for the calculation of the 9 air quality indices the parameters analysed are no2 co so2 o3 pm2 5 and pm10 the data selected corresponds to the period between january 1st and december 31st of 2019 collected with a sampling rate of 1 h the concentration values of the parameters studied are expressed in μg m3 data with information about the tropospheric vertical column density vcd of no2 concentration is also collected from the european union s earth observation programme copernicus the vcd of no2 derived from satellite measurements can be considered as an effective proxy for surface nox concentration and it is often used in air quality applications prunet et al 2020 note that in our visualization we did not yet incorporate the potential impact of transportation of chemical compounds as suggested by some authors lamsal et al 2015 the spatial information over the area of interest is only available at certain moments in time the resulting surface concentration of the compounds obtained from the tropospheric column is given in mol m2 de vries et al 2016 the region analysed is defined by the boundaries west longitude 1 9699 east longitude 6 9699 south latitude 47 0039 north latitude 52 0039 this region contains the two locations analysed ghent longitude 3 7174 latitude 51 0543 and vielsalm longitude 5 9147 latitude 50 2840 the measurements are performed by the tropomi instrument in sentinel 5p the no2 data products have a spatial resolution of 7 3 5 km2 van geffen et al 2019 the data can be freely obtained through the copernicus open access hub sentinel 5p pre operations data hub 2020 where it is possible to access the desired data by defining different parameters such as product type product level geographical area or time 3 2 data pre processing air quality indices the visualization of the satellite data is performed without any further data pre processing for the data of the monitoring stations a central moving median with a window size of 8 h is applied to remove outliers and to estimate concentration values at times where no measurement is reported estimation of concentration values is not possible in periods longer than 8 h of missing data the window size is sufficiently small to minimize the distortion of peaks and valleys in the time series some aqi guidelines set different specifications about how the concentrations measured in a certain period should be handled for example the eaqi guideline specifies that the calculation of the air quality index is based on a 24 h average for pm2 5 while the canadian aqhi prescribes a 3 h average consequently a right aligned moving average considering the prescribed time window size is applied to the preceding concentration values to calculate the different aqis mentioned in table 1 3 3 visualization methods it is used different visualization methods to represent the concentration and air quality trends these visualizations show the air quality index translated as colours this allows a more intuitive analysis of the results and the identification of moments where the air quality suddenly changes the colour scale used in these visualization methods varies depending on the guideline applied to the data the visualization methods are the following time series plot hopke et al 2001 kai 2008 modarres and khosravi 2005 shumway and stoffer 2017 a direct interpretation of the time series regarding the impact on human health is made possible by filling the area below the curve with the corresponding colour of the calculated air quality index such plots are generated by code developed in python 3 using the matplotlib library devert 2014 van rossum and drake 2011 temporal raster plot carpet plots carslaw and ropkins 2012 mintz 2018 carpet plots are generated by plotting data in this case the air quality index over days x axis and time of day in hours y axis while varying the aqi related colour of each data point the advantage of this type of chart is that it visualizes both short term hourly and long term monthly patterns carpet plots are created by in house developed software in python 3 using the matplotlib library correlation matrix a table showing correlation coefficients between variables is generated with in house developed software in python 3 the correlation matrix is subdivided in tiles each tile contains a number that can vary between 1 and 1 this number is the correlation coefficient and it is a measure of the strength and direction of the linear relationship between two variables in this case the different aqis evaluated on the irceline data the tiles are also represented with a colour which depends on the correlation coefficient determined this facilitates the interpretation of the matrix dendrogram it is a tree like structure that visualizes the result of hierarchical clustering it visualizes the similarity between data points within a set and allows the user to classify the data points in several clusters this type of analysis is performed on the columns of the correlation matrix the dendrogram is created with python 3 using the scipy library virtanen 2020 percentage stacked bars this type of plot is used to represent the percentage of time air quality falls in each class by using this visualization method the relation between air quality and time is lost but it is easier to compare the air quality between different locations for a given period the graphs are generated with microsoft excel macdonald 2007 spatial plot o sullivan 2014 the measurements of no2 obtained from the tropomi instrument over belgium are plotted in a map the plots are created with python 3 and the cartopy library met office 2017 3 3 1 user experience study to evaluate the effectiveness of the different visualization methods a thinking aloud test is carried out boren 2000 van samoren 1994 this kind of study consists of recruiting representative users and give them representative tasks while verbalizing their thoughts in the case of this work 4 stakeholders are involved each with a different age and scientific background a scenario context is provided where the user compares the air quality situation between vielsalm and ghent based on the visualization methods previously described 4 results and discussion 4 1 classic graphs showing trends the visualization method shown in fig 1 provides the possibility to obtain valuable information about trends and moments where concentration suddenly changes for humans comparisons within a graph or between graphs are more meaningful than the absolute numbers slovic 2000 kong et al 2019 by comparing the graphs of both locations it is possible to see that ghent has higher concentration values of no2 than vielsalm although with this type of visualization it is relatively easy to detect changes within a graph and to identify peaks it is difficult to determine how relevant these peaks are from fig 1 it is also possible to see that for both locations there is an increase in o3 during spring and summer this visualization represents an advantage compared to a tabular representation of concentrations but it has severe limitations the impact of the concentration variations on our health is not shown directly meaning that this information must be extracted by inference people can focus their attention to a small part of the sensory input e g a peak in the time series attracting our attention is seen by the fovea centralis within the retina in our eye while the rest of the time series is located at the edge of our visual field where vision is poor irizarry 2019 kong 2019 ware 2010 unfortunately there is no direct clear contrast in the plots between moments of elevated health risk and periods with acceptable risk that can attract our attention therefore non experts do not know where to look first a way to avoid this problem is by zooming in and only showing the concentration behaviour of a short period see right details in fig 1 however by changing the plot timescale some important features or patterns may be missed the thinking aloud study reveals that at first glance when people see the plots they relate the extent of pollution with amplitude and frequency concentration variations this might lead to wrong conclusions for example in some cases steady behaviour of high concentration values can be more meaningful to human health than peaks with low maximum values the number of peaks provides valuable information on how the air quality changes but at the same time it might provide irrelevant information about its impact on human health moreover when users want to analyse pollutants individually they tend to look at how many times the concentration reached the top of the graph this illustrates the importance of using the same scale when comparing two graphs in general the users found this type of graph helpful to know how the different substances behave along a year or to compare between different seasons at the same time it is difficult for them to evaluate how the concentration values shown in the graphs affect their health 4 2 graphs with superposed aqi information fig 2 shows the same data as in fig 1 but each moment in time is coloured using the belaqi the guideline that corresponds with the region where the data are collected according to the thinking aloud study the colours make the data simpler to read because 1 the plots contain health information 2 peaks with elevated health risk are shown in a contrasting colour and 3 the same parameter of the 2 locations are easier to compare the following conclusions can be made in a visual way no 2 the air quality related to no2 in vielsalm i e a rural region is predominantly excellent while in ghent i e an urban region with more traffic it varies from excellent to fairly good the seasonal trend in ghent shows that the air quality is slightly better in summer times than in winter for o3 the opposite behaviour is seen in ghent however this conclusion demands a more detailed analysis of the graph o 3 in vielsalm the air quality related to ozone varies from excellent to moderate while in ghent it fluctuates between excellent to fairly good in both cases the impact of ozone on our health is higher during the summer moreover the situation in rural vielsalm is worse than in ghent this is an expected phenomenon the occurrence of this so called ozone paradox is a phenomenon based on the chemical transformation of o3 by nox compounds this transformation occurs more often in cities than in rural areas because of the higher amounts of nox bocci et al 2009 jenkins and ryu 2004 peter 2007 pm 2 5 and pm 10 the air quality changes from excellent to moderate in vielsalm while in ghent it fluctuates from excellent to horrible when looking at particulate matter there is a clear difference in air quality between both regions illustrating the intuitive comparison of both plots pm peaks seem to occur in all seasons 4 3 heatmaps with superposed aqi information fig 3 shows the same o3 and no2 belaqi results as in fig 2 but this time the data is presented as a carpet plot the x axis represents the 365 days in a year while the y axis are the 24 h in a day the grey pixels are missing data during the measurement this occurs because there are periods higher than 8 h where no measurements are performed and therefore it is not possible to estimate the concentration value by performing the moving median used the thinking aloud study showed that one of the advantages of carpet plots such as in fig 3 is that they can easily visualize both long term seasonal cycles and short term daily cycles directly moreover it also shows the correlation between both cycles something that remains invisible in figs 1 and 2 from a visual analysis of the carpet plot the following can be learned o 3 during spring and summer there is a clear impact of o3 on our health and more specific after noon i e the hottest moment of the day the comparison between vielsalm and ghent indicates that the air quality due to o3 is better in ghent because the moments of elevated ozone during the day are somewhat shorter however this is difficult to be distinguished by looking at the graph because the difference is small especially when you look at the highest peaks in yellow no 2 in vielsalm the concentration is so low that it is not possible to visualize fluctuations using the belaqi colour codes for the no2 in ghent it is possible to see an anticorrelation with the o3 behaviour although the no2 signal visualized with belaqi colour codes is rather weak 4 4 correlation between aqis to have an idea of the level of discrepancy between different aqis mentioned in table 1 that are calculated for both locations a linear correlation analysis is performed see fig 4 in the current analysis all values are higher than 0 meaning that the correlation between the aqis is directly proportional however large variations in the correlations are found between the different aqis this means that the selection of the aqi affects the interpretation of the graphs on top of that the following can be learned from these correlations high correlations between some aqis some aqis show a high correlation such as eaqi and caqi with a correlation of 0 97 tile is represented in a darker red in both locations eaqi and caqi use more similar algorithms in other cases some aqi methods also show high correlation even when algorithms are not similar as the case of imeca and aqhi can in vielsalm with a correlation 0 92 this occurs because the concentration values of the substances used to calculate these aqis have little variation in vielsalm for the same reason the correlation matrix of vielsalm shows darker red colours compared to the one of ghent low correlations between other aqis at the same time other aqis show a low linear correlation such as the case between eaqi and aqi epa who have the lowest correlation coefficients tiles are represented in a lighter red they use a different algorithm to estimate the overall aqi this means that comparisons of different time series are only possible when the same aqi is applied to further understand the similarity between different aqis a hierarchical clustering between the columns of the correlation matrix is performed and the result is visualized as a dendrogram shrestha and kazama 2007 the cluster analysis see fig 5 reveals that for the two locations it is possible to identify two main aqi clusters the first one contains caqi eaqi see green lines in fig 5 and depending on the location either belaqi or daqi uk the second cluster see red lines in fig 5 contains all the other aqis this illustrates that the similarity between aqis depends on the timeseries themselves most of the primary clusters are composed of aqi pairs that have been defined by neighbouring regions such as imeca epa belaqi belatmo caqi eaqi some pairs are not from neighbouring regions such as aqhi can and aqi hk but they have similarities in the way they calculate the index fig 6 shows the behaviour of the pm10 data measured in ghent the health related information presented in this dataset is visualized using 7 different aqi guidelines mentioned in table 1 the aqhi can and aqhi hk are omitted because they merge data from several environmental parameters into a single aqi using one mathematical formula and cannot be used to evaluate only one independent parameter fig 6 illustrates that all aqis emphasize some periods in time that attract our attention in that sense all aqis adds a similar information layer on top of the plots the colour contrasts also emphasize the highest peaks in pm10 the thinking aloud studies revealed that the colour palette facilitates the distinction between important and less important moments especially the case where blue or green represents the highest air quality and red and orange the lowest is easy to understand differences in quality assessments the same data point in time is evaluated differently when considering other aqi guidelines for example the epa gives the impression that most of the time the situation is good i e green colour while for a limited number of small periods the situation is moderate yellow peaks the same data visualized with belaqi shows a much larger palette of colours with at least one horrible moment this shows again that there is no agreement between the guidelines no agreement in visualizing peaks with a contrasting colour an important feature of the guidelines is that some moments are associated with a contrasting colour some aqi guidelines clearly show peaks in a contrasting colour while other guidelines epa and to some extent imeca do not emphasize the same peaks at all the contrast between low risk moments and elevated risk moments are not always clear to attract the attention of the analyst it is better to visualize background concentrations with low risk and peaks with higher health risk by using contrasting colours this is the case for belaqi and belatmo blue background and red peaks for citeair and imeca the colour scale generated a smaller colour contrast and are for that reason less efficient in risk communication impact of colour blindness colours play a central role in data visualization but this additional information is partly lost for colour blind persons the horizontal stacked bars in fig 7 show the percentage of the total period that the air quality falls in one of the given quality categories the stacked bars can be read in 2 different ways 1 a positive approach where one assumes that the region with the highest percentage of time with best quality classes can be considered as the healthiest location and 2 a negative approach where people consider the lowest percentage of worse quality classes as the healthiest this is in agreement with the known asymmetric effect of favourable and unfavourable information on decision making kahneman and tversky 1979 negative events information carry greater weight than positive events moreover harm to a person s health is only determined by the classes of worse quality since in the western world people read from left to right we have deliberately placed the most important information i e the negative information on the left side of the bars for both views most of the aqis state that the overall air quality in vielsalm is better than in ghent except for the case of aqi epa and imeca where the results are not conclusive this occurs because of the influence of o3 concentration on the calculation of these aqis this comparison also shows clear differences between the aqis the goodness of ambient air varies when looking at the percentage of time where ambient air falls in the best quality category it is not clear that all aqi guidelines are equally strict for the united kingdom daqi uk hong kong aqhi hk the ambient air quality is excellent but the same air is considered as rather poor by canada aqhi can there is no international agreement about absolute air quality assessments the difference between ghent and vielsalm varies for some aqis air quality is better in vielsalm than ghent e g eaqi and caqi for these cases the bar sections representing the worst air quality classes are higher in ghent and the bar sections representing the best air quality are bigger in vielsalm for other aqis this is not the case e g aqi epa imeca for such cases both bar sections representing the worst and best quality classes are bigger in vielsalm this produces certain vagueness in the comparison of air quality between the 2 locations the thinking aloud confirms that based on the stacked bar graphs in fig 7 it is possible to determine whether the overall air quality is better in vielsalm or ghent e g eaqi belaqi and caqi users do so by comparing the sizes of the coloured bars although in some cases the lack of alignment of the bars proved difficult for direct comparison with small differences e g the good category section in the belaqi stacked bar besides these graphs compared to the previous graphs provide more simplified information about air quality but misses important information such as information about individual pollutants or behaviour of air quality at one specific time 4 5 satellite information the measuring stations in vielsalm and ghent provide information with high temporal resolution but they do not contain any spatial information fig 8 shows the distribution of the no2 concentration in a tropospheric column as measured by the sentinel 5p satellite over vielsalm and ghent at two different moments the first one on may 24th of 2019 and the second one on september 3rd of 2019 these times are selected to compare both tropomi and ground level monitoring stations at moments when higher levels of pollution are reported the concentration in the complete tropospheric column is not converted into a surface concentration the major limitation is that the concentration of no2 through the vertical atmospheric axe is not homogeneous therefore it is not straightforward to perform a unit s conversion from mol m2 i e surface concentration to ppb i e a volume concentration for that reason it is not possible to associate an aqi colour code to the satellite information however the maps show concentration gradients that are easy to read by non experts fig 8a shows a moment where the concentration is nearly similar at ground level fig 8a corresponds with the detail in figs 1 and 2 where an elevated concentration of no2 in ghent when compared to vielsalm can be observed fig 8b shows the migration of pollutants over longer distances resulting in a higher no2 in vielsalm than in ghent however at ground level that difference is not observed a difference in pollution at ground level and elevated height explains this behaviour wong et al 2017 wang et al 2020 qin et al 2016 satellite information does not generate a unified view with ground level data in a simple way the thinking aloud studies revealed that all users agree that information is easy to understand but there is also a discrepancy in opinions about the relevance of this type of graph to perform the comparison between vielsalm and ghent according to some users the distribution of the pollutants can vary rapidly depending on environmental conditions therefore it is necessary to visualize the air pollution at different time frames to perform a correct comparison however users stated that these graphs might be informative to evaluate where the pollution is coming from or heading to the graphs also provide information on how pollution is distributed over bigger territories the users also were interest on this type of visualization showing the impact on their health by using aqis 5 conclusions this paper explores different visualization methods to make a huge amount of data about environmental parameters understandable to different stakeholders to achieve that goal data points in the time series plots are attributed to a colour code using air quality index guidelines for this several aqis are used and the coloured information is incorporated in different kinds of plots from the thinking aloud study all plots where aqi information is superposed on the time series allowed easier air quality evaluation when compared to the plots where only the trends are shown from the think aloud study we made the following conclusions people arrive at similar conclusions about air quality when locations are compared using the same aqi however the comparison is affected by the selected aqi the colour scale of the aqi highlights peaks with other colours emphasizing the health risk of certain moments relevant moments highlighted with colours are more actionable for example red coloured moments attract our attention therefore this helps to select moments where events are detrimental to human s health are occurring and to take action to avoid such moments however depending on the selected aqi the colour scale can highlight other moments with elevated health risk for the stacked bars the negative information is considered as more important than positive information for that reason we moved it to the left side of the bars the carpet plots are a fairly easy way to explore correlations between short term and long term cycles in an intuitive way the map visualization of satellite information is an intuitive way to show pollution data people see pollution in belgium in a direct way however information can only be shown at specific time frames this is a limitation when people want to know the pollution levels over long periods furthermore users remarked the importance of plotting aqis on maps to have a better understanding of the real impact of pollution on their health this study has shown the advantage of attributing colours to the plots using the aqis this method is already used by most of the consulted apps webpages reports however a major problem with the aqi selection is demonstrated although it seemed logical to use the belgium guideline belaqi for the analysed regions in belgium the conclusions drawn from the figures can be biased by the selected guideline there is no clear correlation between the different algorithms calculating the aqis although the actual health risk for a person exposed to air in a specific location at a specific moment must be the same the analysis performed resulted in the following conclusions some aqis are less strict for some pollutants e g aqi epa than others e g belaqi and this affects the visualization of the data some colour palettes resulted in lower contrasts between background and moments with elevated risk peaks e g caqi intuitive comparison of time series can only be done correctly when the same aqi is applied the similarity between different aqis seems to be affected by the time series itself regardless of the selected aqi guideline users concluded correctly that vielsalm had a better air quality than ghent except for o3 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the belgian federal public planning service science policy belspo under project number br 132 a6 airchecq and by the university of antwerp bof academiseringsproject proof of concept for a decision support system to reduce the occupational risks of seafarers due to air quality appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105230 
25699,soil erosion is a significant challenge for agricultural regions with cascading impacts to waterways land productivity soil carbon and ecological health we provide the first national scale soil erosion model that incorporates the impacts of grazing on ground cover cgr and soil erodibility ktr into the rusle framework surface erosion rates for winter forage paddocks 11 t ha 1 y 1 were substantially higher than pastoral grasslands 0 83 t ha 1 y 1 woody grasslands 0 098 t ha 1 y 1 forests 0 103 t ha 1 y 1 and natural soil production rates 1 2 t ha 1 y 1 validation with empirical measurements from sediment traps sediment cores and chemical fingerprinting demonstrated strong linear regressions r2 0 86 terrain impacted soil erosion directly through slope steepness and flow convergence and indirectly through strong orographic effects on rainfall erosivity r2 0 39 0 83 annual surface erosion across aotearoa new zealand could reach 16 5 29 2 mt y 1 representing 20m annually and up to 24 31 of sediment yield for two catchments graphical abstract image 1 keywords soil degradation surface erosion water quality grazing agriculture rusle 1 introduction 1 1 background accelerated rates of soil degradation and erosion resulting from agricultural and pastoral land activities have been observed globally borrelli et al 2017 fao itps 2015 smetanová et al 2020 and are increasingly recognized as a threat to food production systems pimentel and burgess 2013 landscape stability trimble and mendel 1995 water quality mcculloch et al 2003 and ecosystem functioning larned et al 2020 natural rates of soil production and erosion under native vegetation are generally far less than soil loss rates from agricultural and pastoral landscapes by multiple orders of magnitude hancock et al 2020 montgomery 2007 recent reviews have identified spatially and temporally explicit soil erosion models as one of the greatest opportunities for meeting soil conservation and sustainable development goals lefèvre et al 2020 smetanová et al 2020 to date such models have not incorporated the effects of grazing on surficial erosion despite the well documented impacts of increasing livestock grazing pressures on soil damage accelerated soil and nutrient losses and impaired water quality drewry et al 2008 greenwood and mckenzie 2001 hancock et al 2020 houlbrooke et al 2009 larned et al 2020 mcdowell et al 2003 merten and minella 2013 monaghan et al 2017 in order to account for the impacts of livestock treading and grazing on surficial erosion we apply a novel grazing model that captures the respective impacts of grazing and stock treading on soil physical properties and ground cover donovan and monaghan 2021 that integrates seamlessly with a seasonal and spatially explicit version the revised universal soil loss equation rusle incorporating grazing impacts on soil erosion will improve understanding of where landscapes are most and least susceptible to soil loss and degradation in doing so proactive decisions can help to minimize overlap between intensive landuse pressures and erosion prone lands rather than implementing costly reactive strategies 1 2 revised universal soil loss equation rusle and its predecessor usle predict mean annual soil loss from surface erosion based on a set of equations derived from empirical measurements of soil losses from agricultural plots renard et al 1997 rusle encapsulates seasonal rainfall erosivity r slope length l and steepness s soil erodibility k ground cover and management factors c each of which is considered an important influence on soil loss selby 1993 recent modelling developments have enabled additional precision through calculating seasonally variable grazing adjusted ground cover cgr and treaded soil erodibility ktr for pastoral lands donovan and monaghan 2021 surface erosion models based on rusle are widely accepted and increasingly used for applications spanning field catchment national and even global scales borrelli et al 2017 the components of the model include both inherent land characteristics including soil properties slope length and slope steepness as well as allogenic external factors such as rainfall erosivity ground cover and or crop cover type and some land management practices among the factors included in rusle rainfall erosivity r is often most closely correlated with temporal trends in soil loss hedding et al 2020 owing to the fact that sediments are rarely detached without sufficient rainfall the impact of rainfall s kinetic energy on soil erosion is captured in the r factor both rainfall volume and intensity should be included in calculating r due to the importance of short but highly erosive storm events nearing et al 2005 we improve upon the calculation of r for new zealand by using monthly rainfall data niwa 2012 as input to seasonal and regionally variable linear and power functions klik et al 2015 that were not previously available slope steepness s seeks to capture the rate of change in soil loss with varying gradients while slope length l accounts for the distance over which a slope gradient occurs the equations for both s and l have been reformulated for use in geographic information systems gis and are thoroughly summarized and compared bircher et al 2019 estimates of s and l for new zealand are improved via the use of enhanced hydrological flow routing and empirical equations describing s and l found in literature soil erodibility is captured in the k factor which incorporates physical and chemical properties of soil including fractions of sand silt and clay permeability structural stability and organic matter content a recent global review of rusle formulations suggested that previous soil erodibility k factors for new zealand dymond et al 2010 did not adequately account for soil texture due to broad generalization benavidez et al 2018 herein soil erodibility is enhanced by incorporating the soil structural vulnerability index hewitt and shepherd 1997 and additional soil physical and chemical properties including particle size factions surficial gravel and rock content permeability drainage classes organic matter and phosphate retention the cover and management factor c factor is used to estimate the effect of canopy and ground cover as well as land use management in reducing surficial soil loss wischmeier and smith 1978 known mechanisms through which cover impacts erosion include intercepting rainfall slowing wind speed and transport altering soil water content providing root structure cohesion reducing surface water runoff and adding soil carbon content the effectiveness in reducing erosion varies spatially and seasonally with the height density and fraction of cover alexandridis et al 2015 basher et al 2008 benavides et al 2009 schmidt et al 2018 yang 2014 c factor estimates include added complexity when incorporating prior and current land practices such as tillage crop residues cover crops and grazing that can each significantly alter soil exposure abdalla et al 2018 hoffman et al 1983 monaghan et al 2017 zhou et al 2010 to date formulations of the c factor for new zealand have only included 3 cover scenarios i e bare ground grass and tree cover that remain static throughout the year dymond et al 2010 which does not capture the spatial or temporal variability across the many land covers of aotearoa despite the many strengths of rusle it has limitations such that it does not capture gullying and or shallow landslides is not process based does not capture feedbacks between model components nor does it estimate delivery deposition of eroded sediments being an empirical model that was developed in north america rusle applications elsewhere should be validated and or calibrated when estimating surface erosion previous reviews have covered these limitations in detail alewell et al 2019 benavidez et al 2018 and further demonstrated that rusle outputs have similar ranges of uncertainty when compared with more complex process based physical models such as wepp and pesera lastly rusle has previously not accounted for the impact of grazing and treading on soil loss despite significant impacts to ground cover and soil physical properties thus we apply a nested grazing model donovan and monaghan 2021 to calculate treaded soil physical properties rusle ktr and grazed ground cover rusle cgr the grazing model uses empirical relationships between grazing treading intensity i e stock hoof pressure grazing density duration and history and damage to soil physical properties i e permeability and structure and further account for susceptibility due to clay content and soil moisture donovan and monaghan 2021 initial results demonstrated that this framework vastly improved estimates of soil loss from pastoral lands donovan and monaghan 2021 which is essential for accurately quantifying soil loss across new zealand catchments where pastoral lands occupy over 40 of land area fig 1 ministry for the environment and stats nz 2019 2 material methods the surface erosion model is applied to aotearoa new zealand fig 1 with broad analyses encompassing 22 catchments 2400 21 960 km2 followed by specific analysis of six prominent catchments with cultural and or environmental importance the waikato rangitiaiki manawatu aparima clutha and motueka rivers the catchments were chosen to represent a range of environments geologies and characteristic land use distributions specifically annual and perennial croplands grazed and ungrazed grasslands natural and planted forests and winter forage paddocks descriptions of catchment soils terrain land use and management can be found in donovan and monaghan 2021 we used a 15 m digital elevation model dem with seamless coverage of new zealand prepared and made publicly available by the national school of survey at the university of otago columbus et al 2011 the dem has vertical rmse of 7 1 m and was interpolated from 20 m topographic contours using anudem a 2 d thin plate smoothing spline that optimizes for hydrologically connected terrain while minimizing interpolation artefacts prior to any calculations of slope steepness s and length l spurious features were removed from the dem to ensure proper flow routing the formulation used herein is based on upslope contributing area desmet and govers 1996 and has been used for applications spanning field watershed national and even global scales borrelli et al 2017 panagos et al 2014 schmidt et al 2019 zhang et al 2017 ls factor values were limited to pixels with slopes 50 26 6 and drainage areas above 13 500 km2 equivalent to 60 cells of flow accumulations similar to previously used thresholds found in literature bircher et al 2019 ls factor values were further masked for rivers and open waterbodies using new zealand s hydrologic geodatabase for additional details on calculating l and s factors see appendix a eqs a 1 a 4 mean monthly rainfall grids based on new zealand rainfall records spanning 1981 2010 niwa 2012 tait et al 2012 were resampled using nearest neighbor interpolation from 100 m to 15 m resolution to align with the 15 m dem the 30 year rainfall data thus reflect the central tendency of each month s long term conditions monthly grids were summed to calculate seasonal rainfall totals for spring september november summer december february autumn march may and winter june august subsequently we used previously derived linear and power functions to calculate seasonal rainfall erosivity r klik et al 2015 which captures the kinetic energy potential of rainfall that drives soil erosion by water the linear and power function coefficients vary seasonally and spatially e g table 3 of klik et al 2015 across four distinct climatic zones of new zealand fig 2 each was derived using mean monthly rainfall maximum 30 min rainfall intensity and rainfall erosivity at 632 weather stations with r2 of 0 82 0 98 additional details on rainfall erosivity r calculations are provided in appendix b and klik et al 2015 we explore the role of elevation on rainfall intensity by evaluating the significance of regressions between elevation meters above sea level and seasonal rainfall intensity log10 to normalize data for four regions with distinct rainfall erosivity patterns klik et al 2015 we compared a series of regressions that included the additive effects of location x and y coordinates to confirm the most parsimonious model using aic and bic criteria where lower scores indicate more optimal model fit and complexity the land use and carbon analysis system lucas 2016 geodatabase was used to classify land use and land cover types across new zealand fig 1 lucas is derived from 10 m sentinel 2 satellite imagery further refined using data fusion with other new zealand landuse geodatabases newsome et al 2018 herein lucas was also used to omit land areas not suited for the rusle model or those not susceptible to surface erosion from overland flow such as open water bodies wetlands urban areas settlements bedrock outcrops sand dunes beaches coastal cliffs mines or quarries and permanent ice or glaciers we assume the spatial error in landuse boundary delineations are negligible across national scales donovan et al 2019 baseline values of the cover and management factor c for each crop and cover class were derived from a comprehensive review of relevant measurements found in previous studies basher et al 2016 gabriels 2003 panagos et al 2015b puente et al 2011 vatandaşlar and yavuz 2017 with seasonal adjustments to account for typical growth and senescence for pastoral land covers a nested sub model was applied to account for differences in the density and fraction of cover following grazing denoted as cgr season cover and management factor c cgr values are summarized in table 1 along with details and calculations within appendix c eqs c 5 c 6 and donovan and monaghan 2021 soil characteristics used to calculate soil erodibility k factor were derived from the fundamental soil layer fsl a free and publicly available national soil map newsome et al 2008 with physical chemical and mineralogical information the maps were compiled at 1 63 360 inch to mile from 1500 soil profiles that were refined using local surveys topographic maps and aerial photos much of the methods data attributes and categories are described in the lris data dictionary newsome et al 2008 and were derived from either webb and wilson 1995 or clayden and webb 1994 inherent soil erodibility k was calculated using the standard derivation eq 1 renard et al 1997 with additional adjustments for stoniness kst found in panagos et al 2014 poesen and ingelmo sanchez 1992 poesen et al 1994 1 k 2 1 10 4 m 1 14 12 o m 3 25 s 2 2 5 p 3 100 0 1317 for pastoral lands we calculated a seasonal grazing adjusted soil erodibility ktr using recent livestock treading model donovan and monaghan 2021 that captures the change in two subfactors soil structure s tr and permeability p tr that are impacted by livestock treading eq 2 the model uses grazing information livestock hoof pressure stocking density grazing duration and grazing history along with soil properties structural vulnerability soil moisture soil texture the supporting material e g section 1 4 describes the details of each soil attribute used to calculate inherent k and treaded soil erodibility ktr 2 k t r 2 1 10 4 m 1 14 12 o m 3 25 s t r 2 2 5 p t r 3 100 0 1317 for each season soil loss from surface erosion es eq 3 was calculated as the product of all factors rseason l s k or ktr season and cseason or cgr season which were then summed to produce grids of annual erosion e yr eq 4 3 e s e a s o n r s e a s o n k t r s e a s o n l s c g r s e a s o n 4 e y r e s p e s u e a u e w i where e sp e su e au and e wi are soil losses for the spring summer autumn and winter we omitted non erosive areas and those considered unsuitable for rusle modelling including urban developments glaciers bedrock exposures mountainous environments mines quarries wetlands and open water for additional details on the data processing algorithms and software used see appendices a e for evaluating accuracy and uncertainty of the soil loss estimates the annual soil loss grid was clipped and summed for specific paddocks farms land use classes and or catchments with published measurements of soil loss the studies used for assessing uncertainty included all comparable known research on soil losses measured using in field sediment traps flumes storm and baseflow sampling estuary and lake sediment cores and caesium 137 areal distributions fig 8 table 3 finally we use recent economic analyses that valued the cost of surficial soil loss at 1 2 nzd per tonne soliman and walsh 2020 to calculate the total cost associated with surficial soil losses across new zealand specifically the product of total soil losses for new zealand tonnes and average cost rate 1 2 nzd per t yr 1 yielded the cost associated with surficial soil loss the cost estimates only account for the value of soil lost not costs associated with environmental remediation or loss of farm productivity 3 results discussion 3 1 rainfall erosivity r across new zealand annual rainfall erosivity varied from 100 to 20 000 mj mm ha 1 hr 1 yr 1 fig 2 table 2 with mean values of 2481 and 2952 for the north and south islands respectively the mean annual values for each island lie at the high end of the range for oceanic climates cfb in the köppen climate classification found in previous global rainfall analyses panagos et al 2017 e g fig 3 extremely high values 10 000 exist within small portions of new zealand s southern alps comparable to some of the highest values found worldwide in regions such as the amazon da silva 2004 panagos et al 2017 the spatial distribution seasonal variation and range of r factor values align with previous values modelled for new zealand klik et al 2015 confirming the precision and seasonal distribution of rainfall erosivity results despite using distinct input datasets seasonally both islands experience maximum rainfall erosivity in summer while the lowest quartile of values occurred in spring seasonal variability in mean r factor values were slightly different the highest mean rainfall erosivity for the north island occurred during autumn while average erosivity for the south island peaked during summer in other words maximum potential erosive power of rainfall does not coincide with the period during which the majority of land area is experiencing elevated erosive forces from rainfall terrain is known to have an important role in rainfall quantity at local scales via orographic effects hutchinson 1968 however this effect was thought to be negligible at sufficiently broad scales tait et al 2006 within each region fig 2 elevation explained 14 50 of the variability in log rainfall erosivity based on linear regressions which was improved further 39 86 when incorporating latitude and longitude supplementary material appendix b figs b1 b5 this is the first analysis and results to show such a pronounced impact of terrain on rainfall erosivity over broad spatial scales across new zealand 3 2 terrain slope length and steepness factors ls the methods used herein are the first national scale map of the ls factor for aotearoa new zealand providing a high resolution map of how terrain slope length and steepness impact risk to surficial erosion respectively the north and south islands had mean ls factor values of 1 62 and 1 29 σ 1 83 and 1 91 which are comparable to the central tendency μ 1 63 of the european union panagos et al 2015a while the vast majority of land area across new zealand has low ls factor values fig 3 the southern alps give rise to more land with intermediate to high ls factor values fig 3b compared to the north island this is evident in a 25 higher mean ls value for the south island compared to the north island which is equivalent to a 25 increase in average susceptibility to surface erosion arising from topographic differences alone 3 3 soil erodibility k ktr soil erodibility values k are directly proportional to actual erodibility so higher k values indicate soils are more susceptible to being eroded and vice versa soil erodibility exhibited moderate spatial variability with bimodal distributions for both islands fig 4 mean and median ktr values were 0 029 σ 0 018 for the south island which were similar but slightly higher than the north island 0 020 σ 0 014 the mean values are slightly lower than that of the european union μ 0 032 which may reflect the significant impact of the loess belt across eu which is generally associated with higher erodibility panagos et al 2014 new zealand soil erodibility was generally inversely related to permeability fig 5 b and directly related to dominant particle size fig 5c indicating agreement with expected trends of soil erodibility ktr values were previously validated by comparing the change in soil erodibility with field measurements of pre and post grazing changes in soil macroporosity for a range of grazing pressures spanning pastures and forage crops donovan and monaghan 2021 the inclusion of stoniness reduced mean soil erodibility by 4 5 demonstrating the importance of including the stoniness effect into soil erodibility calculations across grazed areas of new zealand the effect of including livestock treading increased soil erodibility ktr by 3 3 9 with the most pronounced effects occurred in winter due to the impacts of intensive forage crop grazing practices 3 4 cover management factor the range of cover factors cf across new zealand varied from 0 to 1 with similar variability for both north and south islands fig 6 c factor values are inversely proportional to the effectiveness in reducing erosion thus c factor values of 0 reflect 100 reductions in surface erosion while 1 reflects no reduction in surface erosion differences across seasons were minor apart from croplands and winter forage paddocks which exhibit strong variation due to harvesting senescence and or grazing significant variability existed across land use and cover classes reflecting the effectiveness of each vegetation in mitigating surface erosion forests were consistently the most effective in reducing surface erosion followed by grasslands with woody biomass ungrazed grasslands grazed grasslands perennial and annual croplands and winter forage crop paddocks fig 7 prior to grazing low and high producing grasslands exhibit negligible differences in cf values blue boxplots fig 7 however after grazing mean cgr values were 53 greater for high producing grasslands orange boxplots fig 7 equivalent to doubling in susceptibility to surface erosion further investigation revealed the increased susceptibility for high producing grasslands reflected the predominance of grazing by dairy cattle which generally leave 10 less residual ground cover 75 80 cover compared to grazing by sheep and beef cattle 85 90 cover elliott and carlson 2004 pande et al 2000 based on empirical relationships between ground cover and soil loss silburn et al 2011 this modest 10 15 gap in ground cover is equivalent to a 53 difference in mean annual soil loss these results demonstrate how retaining ground cover residuals following grazing can be used as a simple means to enhance soil retention in grazed lands 3 5 uncertainty and validation in order to provide a thorough and transparent view of the uncertainty associated with rusle surface erosion outputs we expand upon uncertainty analyses presented in donovan and monaghan 2021 herein we go beyond pastoral grasslands and winter forage crop paddocks to incorporate mixed landuse catchments oldman et al 2009 forested lands o loughlin et al 1978 croplands basher and ross 2002 and natural tussock grasslands o loughlin et al 1984 the studies used for assessing uncertainty include all comparable measurements of soil losses that are dominated by surficial erosion processes rill and interrill erosion rather than gullying and or shallow landslides we compare with studies using a diverse set of measurement techniques spanning in field sediment traps flumes storm and baseflow sampling estuary and lake sediment cores and caesium 137 areal distributions fig 8 table 3 comparisons between measured and modelled soil losses indicate that modelled rates fall within the range measured at each field location found in literature fig 8a this is supported further by a strong linear regression r2 0 86 between measured and modelled rates fig 8b additional details on the studies used for validation and uncertainty analyses are provided in appendix e ongoing work is underway to validate improve upon and assess uncertainty in calculated soil losses at farm and catchment scales from these comparisons we can suggest that the soil losses modelled via the grazing adjusted rusle framework adequately capture the magnitude of soil losses expected under average rainfall conditions for the range of land uses considered the broad scale comparisons herein and in donovan and monaghan 2021 demonstrate significant alignment between modelled and measured long term surficial soil losses from surface erosion where such losses are transported this is further supported by favorable alignment between spatially explicit comparisons between modelled and measured soil losses rusle based modelled soil losses represent total potential soil losses from surficial erosion processes and thus we do not suggest they represent sediment transport or the total soil losses entering waterways of new zealand future work should assess the fate of surface erosion using sediment transport modelling alongside bank erosion measurements from aerial imagery to gain a more complete picture of catchment sediment sources six subcatchments within manawatu catchment which have been the focus of numerous monitoring and modelling studies are used to understand how surficial erosion compares to total catchment sediment yields fig 9 this comparison provided the first estimate of potential contributions from surface erosion over timescales sufficiently long to assume 100 sediment delivery over annual timescales this approach cannot ascertain how much modelled surface erosion is contributing to sediment yield due to temporal lags in sediment delivery of rivers impacted by land use change donovan et al 2021 donovan and belmont 2019 and because rusle does not account for sediment transport and deposition thus the comparisons likely reflect the high end of potential contributions to annual sediment yields found in literature vale et al 2016 modelled surface erosion across all six subcatchments represented 30 of the total annual sediment yield measured at each outlet which varied from 23 48 of downstream sediment yields in five of the six subcatchments vale et al 2016 for the sixth mangatainoka subcatchment modelled soil losses were 73 greater than downstream yields which is similar to previous modelling efforts with sednetnz dymond et al 2014 vale et al 2016 the consistency of this overestimate suggests that either significant deposition occurs measurements underestimate sediment contributions vale et al 2016 or land conditions exist within the catchment that are not captured within the models or input datasets mapping potential hotspots of soil loss across the catchment fig 9 illustrates how sparse vegetation cover along steep uplands in the northwest portion of the pohangina subcatchment may be driving elevated surface erosion rates an abrupt shift along the boundary of the pohangina and upper manawatu catchments with dense vegetation cover appear much less susceptible evident in the abrupt spatial transition from orange red to green hues we confirmed this shift is not explained by changes in slope soils and or rainfall erosivity southwest portions of the manawatu catchment also exhibit low rates of surface erosion and are thus unlikely to contribute significant sediment to the river this excludes streambank erosion which is often a significant source of sediment remobilized into downstream estuaries donovan et al 2015 2016 3 6 soil loss contributions across catchments and land uses modelled soil losses for all lands subject to surface erosion processes across new zealand fig 9 have the potential to reach 16 5 29 2 mt y 1 fig 10 table 4 over timescales sufficiently broad to encompass transport to rivers these could account for 15 of the 192 mt y 1 of sediments estimated to reach waterways and oceans annually ministry for the environment stats nz 2018 while eroded sediments may not reach waterways over sub decadal timescales due to intermediary deposition such soil losses nevertheless are a loss of significant social spiritual and economic value to communities of aotearoa new zealand especially māori asher and naulls 1987 harmsworth 2020 hutchings et al 2018 previous economic evaluations valued new zealand soils at an average of 1 2 nzd per t y 1 soliman and walsh 2020 applying this value to the soil loss estimates herein indicates that surficial soil losses represent a loss of 20 40m annually excluding the environmental costs of remediation and losses in agricultural productivity for the 22 catchments considered the average soil loss via surface erosion from pastoral grasslands was 0 84 t ha 1 y 1 and ranged from 0 02 to 2 5 t ha 1 y 1 fig 11 which is lower than average soil losses for the european union 2 02 t ha 1 y 1 and globally 1 70 t ha 1 y 1 doetterl et al 2012 panagos et al 2015c within the six catchments examined further these rates accounted for 34 92 of the total soil loss from surface erosion fig 11 while the fraction of pastoral grassland landcover occupied 15 76 of the catchments erodible area on the other hand winter forage crop paddocks on slopes greater than 7 were 1 of the catchment areas but accounted for up to 12 of the catchments total surface erosion fig 12 put another way grazed forage crop paddocks contribute 7 to 120 fold more soil loss relative to proportion of land area they occupy the average rates of soil loss 11 t ha 1 y 1 from forage crop paddocks were the highest of any land use cover modelled with considerable variability 3 90 24 52 t ha 1 y 1 previous analyses of soil losses from forage crop paddocks demonstrated that such high rates reflect the combination of negligible ground cover following grazing intensive treading on wet soils and relatively steep terrain donovan and monaghan 2021 natural ungrazed grasslands had highly variable rates of soil loss μ 2 0 range 0 02 10 t ha 1 y 1 and generally contributed higher proportions of surface erosion relative to their fractional land area additional investigation revealed that natural grasslands generally inherit high rates of surface erosion from steep slopes and highly erodible soils donovan and monaghan 2021 which was the same for natural grasslands across the european union panagos et al 2015c thus natural grasslands are not comparable equivalents of pastoral grasslands are typically characterized by shallower terrain and improved soil quality that support high pasture production further higher soil losses from natural grasslands relative to pastoral grasslands cannot be used to infer that grazing reduces surface erosion grasslands with native woody biomass exhibited negligible rates fig 11 and proportions 2 of soil loss from surface erosion across the catchments fig 12 forested lands also contributed very minor proportions of surface erosion apart from the rangitaiki catchment which contained 80 forested land cover in both land classes the reduced soil losses were primarily the result of dense vegetation cover in mitigating soil loss as we found no differences in slope soil quality or rainfall for such land classes this is supported by mechanistic understanding of vegetation s ability to mitigate surface erosion through rainfall interception root cohesion surface matting effects and reducing the volume and rate of overland flow despite having moderate rates of soil loss at local scales 0 09 5 01 t ha 1 y 1 annual and perennial crops represented a small proportion of surface erosion at catchment scales because they occupied a small fraction of land cover together annual and perennial crops represented 1 4 2 3 of the land area and generally contributed 5 of soil loss via surface erosion 4 conclusions and future directions we present the first national scale model of soil lost via surface erosion that accounts for the impacts of grazing and treading on ground cover and soil erodibility donovan and monaghan 2021 using a rusle modelling framework that is capable for modelling alternative grazing management scenarios to understand the impacts to soil loss and catchment water quality modelled soil losses herein exhibited favorable alignment with spatially explicit comparisons of field measurements lake sediment cores and chemical fingerprinting measurements of soil losses across a variety of land uses soil losses via surface erosion across aotearoa new zealand may reach 29 2 mt y 1 representing a loss of 20m 110m annually apart from forests and grasslands with woody biomass annual soil losses were generally higher than natural rates of soil production and loss expected from native landscapes hancock et al 2020 montgomery 2007 the effect of land management was most pronounced for intensive winter forage crop paddocks which exhibited high rates 11 t ha 1 y 1 and highest relative proportions of annual soil loss figs 11 and 12 across the 22 catchments analyzed this reflected the combined effects of negligible residual ground cover following grazing soil treading damage to wet soils and relatively high slopes such areas thus represent the highest cost benefit ratio for reducing soil losses across new zealand where increasing post grazing residual ground cover could greatly enhance soil retention thereby reducing the gap between rates of soil production and erosion hancock et al 2020 montgomery 2007 comparing modelled soil loss with downstream sediment yields from the mahurangi estuary supplementary material appendix e and subcatchments to the manawatu river fig 9 suggested that surface erosion could account for up to 24 32 of sediment yield over timescales sufficiently long to allow 100 sediment delivery future work should aim to model sediment transport and deposition to better resolve the proportion of soil losses from surface erosion that reach waterways lastly we demonstrate that topography has a dual role in impacting soil loss from surface erosion directly through slope steepness and length and indirectly through strong orographic effects that explained up to 50 variance of rainfall erosivity future work should further explore the relationship between elevation and rainfall erosivity at varying spatial scales to determine the scale at which orographic effects diminish in importance relative to broader weather patterns further by incorporating expected climate change impacts to rainfall quantity and intensity the framework herein could be used to project seasonal and annual soil loss scenarios funding this work was funded by the new zealand ministry for business innovation and employment s our land and water national science challenge toitū te whenua toiora te wai contract c10x1901 phase 2 c10x1507 phase 1 as part of the whitiwhiti ora land use opportunities research programme supporting holistic decision making to improve future landscapes of aotearoa new zealand declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements many thanks are extended to all those who contributed to the development of this research specifically ross monaghan peter pletnyakov richard muirhead hugh smith david houlbrooke and dan sun for their expertise technical support and critique that helped improve the quality of work presented herein a special thanks to desneiges murray for her statistical graphing and personal support of the work throughout its various iterations we extend appreciation to lucy burkitt fiona curran cournane andrew tait and wei hu for sharing data and materials that helped quantify uncertainty of this model and promote transparent data sharing lastly we thank the people of aotearoa new zealand who continue to support the conservation and restoration of the land ngā mihi nui ki a koutou he waka eke noa appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105228 
25699,soil erosion is a significant challenge for agricultural regions with cascading impacts to waterways land productivity soil carbon and ecological health we provide the first national scale soil erosion model that incorporates the impacts of grazing on ground cover cgr and soil erodibility ktr into the rusle framework surface erosion rates for winter forage paddocks 11 t ha 1 y 1 were substantially higher than pastoral grasslands 0 83 t ha 1 y 1 woody grasslands 0 098 t ha 1 y 1 forests 0 103 t ha 1 y 1 and natural soil production rates 1 2 t ha 1 y 1 validation with empirical measurements from sediment traps sediment cores and chemical fingerprinting demonstrated strong linear regressions r2 0 86 terrain impacted soil erosion directly through slope steepness and flow convergence and indirectly through strong orographic effects on rainfall erosivity r2 0 39 0 83 annual surface erosion across aotearoa new zealand could reach 16 5 29 2 mt y 1 representing 20m annually and up to 24 31 of sediment yield for two catchments graphical abstract image 1 keywords soil degradation surface erosion water quality grazing agriculture rusle 1 introduction 1 1 background accelerated rates of soil degradation and erosion resulting from agricultural and pastoral land activities have been observed globally borrelli et al 2017 fao itps 2015 smetanová et al 2020 and are increasingly recognized as a threat to food production systems pimentel and burgess 2013 landscape stability trimble and mendel 1995 water quality mcculloch et al 2003 and ecosystem functioning larned et al 2020 natural rates of soil production and erosion under native vegetation are generally far less than soil loss rates from agricultural and pastoral landscapes by multiple orders of magnitude hancock et al 2020 montgomery 2007 recent reviews have identified spatially and temporally explicit soil erosion models as one of the greatest opportunities for meeting soil conservation and sustainable development goals lefèvre et al 2020 smetanová et al 2020 to date such models have not incorporated the effects of grazing on surficial erosion despite the well documented impacts of increasing livestock grazing pressures on soil damage accelerated soil and nutrient losses and impaired water quality drewry et al 2008 greenwood and mckenzie 2001 hancock et al 2020 houlbrooke et al 2009 larned et al 2020 mcdowell et al 2003 merten and minella 2013 monaghan et al 2017 in order to account for the impacts of livestock treading and grazing on surficial erosion we apply a novel grazing model that captures the respective impacts of grazing and stock treading on soil physical properties and ground cover donovan and monaghan 2021 that integrates seamlessly with a seasonal and spatially explicit version the revised universal soil loss equation rusle incorporating grazing impacts on soil erosion will improve understanding of where landscapes are most and least susceptible to soil loss and degradation in doing so proactive decisions can help to minimize overlap between intensive landuse pressures and erosion prone lands rather than implementing costly reactive strategies 1 2 revised universal soil loss equation rusle and its predecessor usle predict mean annual soil loss from surface erosion based on a set of equations derived from empirical measurements of soil losses from agricultural plots renard et al 1997 rusle encapsulates seasonal rainfall erosivity r slope length l and steepness s soil erodibility k ground cover and management factors c each of which is considered an important influence on soil loss selby 1993 recent modelling developments have enabled additional precision through calculating seasonally variable grazing adjusted ground cover cgr and treaded soil erodibility ktr for pastoral lands donovan and monaghan 2021 surface erosion models based on rusle are widely accepted and increasingly used for applications spanning field catchment national and even global scales borrelli et al 2017 the components of the model include both inherent land characteristics including soil properties slope length and slope steepness as well as allogenic external factors such as rainfall erosivity ground cover and or crop cover type and some land management practices among the factors included in rusle rainfall erosivity r is often most closely correlated with temporal trends in soil loss hedding et al 2020 owing to the fact that sediments are rarely detached without sufficient rainfall the impact of rainfall s kinetic energy on soil erosion is captured in the r factor both rainfall volume and intensity should be included in calculating r due to the importance of short but highly erosive storm events nearing et al 2005 we improve upon the calculation of r for new zealand by using monthly rainfall data niwa 2012 as input to seasonal and regionally variable linear and power functions klik et al 2015 that were not previously available slope steepness s seeks to capture the rate of change in soil loss with varying gradients while slope length l accounts for the distance over which a slope gradient occurs the equations for both s and l have been reformulated for use in geographic information systems gis and are thoroughly summarized and compared bircher et al 2019 estimates of s and l for new zealand are improved via the use of enhanced hydrological flow routing and empirical equations describing s and l found in literature soil erodibility is captured in the k factor which incorporates physical and chemical properties of soil including fractions of sand silt and clay permeability structural stability and organic matter content a recent global review of rusle formulations suggested that previous soil erodibility k factors for new zealand dymond et al 2010 did not adequately account for soil texture due to broad generalization benavidez et al 2018 herein soil erodibility is enhanced by incorporating the soil structural vulnerability index hewitt and shepherd 1997 and additional soil physical and chemical properties including particle size factions surficial gravel and rock content permeability drainage classes organic matter and phosphate retention the cover and management factor c factor is used to estimate the effect of canopy and ground cover as well as land use management in reducing surficial soil loss wischmeier and smith 1978 known mechanisms through which cover impacts erosion include intercepting rainfall slowing wind speed and transport altering soil water content providing root structure cohesion reducing surface water runoff and adding soil carbon content the effectiveness in reducing erosion varies spatially and seasonally with the height density and fraction of cover alexandridis et al 2015 basher et al 2008 benavides et al 2009 schmidt et al 2018 yang 2014 c factor estimates include added complexity when incorporating prior and current land practices such as tillage crop residues cover crops and grazing that can each significantly alter soil exposure abdalla et al 2018 hoffman et al 1983 monaghan et al 2017 zhou et al 2010 to date formulations of the c factor for new zealand have only included 3 cover scenarios i e bare ground grass and tree cover that remain static throughout the year dymond et al 2010 which does not capture the spatial or temporal variability across the many land covers of aotearoa despite the many strengths of rusle it has limitations such that it does not capture gullying and or shallow landslides is not process based does not capture feedbacks between model components nor does it estimate delivery deposition of eroded sediments being an empirical model that was developed in north america rusle applications elsewhere should be validated and or calibrated when estimating surface erosion previous reviews have covered these limitations in detail alewell et al 2019 benavidez et al 2018 and further demonstrated that rusle outputs have similar ranges of uncertainty when compared with more complex process based physical models such as wepp and pesera lastly rusle has previously not accounted for the impact of grazing and treading on soil loss despite significant impacts to ground cover and soil physical properties thus we apply a nested grazing model donovan and monaghan 2021 to calculate treaded soil physical properties rusle ktr and grazed ground cover rusle cgr the grazing model uses empirical relationships between grazing treading intensity i e stock hoof pressure grazing density duration and history and damage to soil physical properties i e permeability and structure and further account for susceptibility due to clay content and soil moisture donovan and monaghan 2021 initial results demonstrated that this framework vastly improved estimates of soil loss from pastoral lands donovan and monaghan 2021 which is essential for accurately quantifying soil loss across new zealand catchments where pastoral lands occupy over 40 of land area fig 1 ministry for the environment and stats nz 2019 2 material methods the surface erosion model is applied to aotearoa new zealand fig 1 with broad analyses encompassing 22 catchments 2400 21 960 km2 followed by specific analysis of six prominent catchments with cultural and or environmental importance the waikato rangitiaiki manawatu aparima clutha and motueka rivers the catchments were chosen to represent a range of environments geologies and characteristic land use distributions specifically annual and perennial croplands grazed and ungrazed grasslands natural and planted forests and winter forage paddocks descriptions of catchment soils terrain land use and management can be found in donovan and monaghan 2021 we used a 15 m digital elevation model dem with seamless coverage of new zealand prepared and made publicly available by the national school of survey at the university of otago columbus et al 2011 the dem has vertical rmse of 7 1 m and was interpolated from 20 m topographic contours using anudem a 2 d thin plate smoothing spline that optimizes for hydrologically connected terrain while minimizing interpolation artefacts prior to any calculations of slope steepness s and length l spurious features were removed from the dem to ensure proper flow routing the formulation used herein is based on upslope contributing area desmet and govers 1996 and has been used for applications spanning field watershed national and even global scales borrelli et al 2017 panagos et al 2014 schmidt et al 2019 zhang et al 2017 ls factor values were limited to pixels with slopes 50 26 6 and drainage areas above 13 500 km2 equivalent to 60 cells of flow accumulations similar to previously used thresholds found in literature bircher et al 2019 ls factor values were further masked for rivers and open waterbodies using new zealand s hydrologic geodatabase for additional details on calculating l and s factors see appendix a eqs a 1 a 4 mean monthly rainfall grids based on new zealand rainfall records spanning 1981 2010 niwa 2012 tait et al 2012 were resampled using nearest neighbor interpolation from 100 m to 15 m resolution to align with the 15 m dem the 30 year rainfall data thus reflect the central tendency of each month s long term conditions monthly grids were summed to calculate seasonal rainfall totals for spring september november summer december february autumn march may and winter june august subsequently we used previously derived linear and power functions to calculate seasonal rainfall erosivity r klik et al 2015 which captures the kinetic energy potential of rainfall that drives soil erosion by water the linear and power function coefficients vary seasonally and spatially e g table 3 of klik et al 2015 across four distinct climatic zones of new zealand fig 2 each was derived using mean monthly rainfall maximum 30 min rainfall intensity and rainfall erosivity at 632 weather stations with r2 of 0 82 0 98 additional details on rainfall erosivity r calculations are provided in appendix b and klik et al 2015 we explore the role of elevation on rainfall intensity by evaluating the significance of regressions between elevation meters above sea level and seasonal rainfall intensity log10 to normalize data for four regions with distinct rainfall erosivity patterns klik et al 2015 we compared a series of regressions that included the additive effects of location x and y coordinates to confirm the most parsimonious model using aic and bic criteria where lower scores indicate more optimal model fit and complexity the land use and carbon analysis system lucas 2016 geodatabase was used to classify land use and land cover types across new zealand fig 1 lucas is derived from 10 m sentinel 2 satellite imagery further refined using data fusion with other new zealand landuse geodatabases newsome et al 2018 herein lucas was also used to omit land areas not suited for the rusle model or those not susceptible to surface erosion from overland flow such as open water bodies wetlands urban areas settlements bedrock outcrops sand dunes beaches coastal cliffs mines or quarries and permanent ice or glaciers we assume the spatial error in landuse boundary delineations are negligible across national scales donovan et al 2019 baseline values of the cover and management factor c for each crop and cover class were derived from a comprehensive review of relevant measurements found in previous studies basher et al 2016 gabriels 2003 panagos et al 2015b puente et al 2011 vatandaşlar and yavuz 2017 with seasonal adjustments to account for typical growth and senescence for pastoral land covers a nested sub model was applied to account for differences in the density and fraction of cover following grazing denoted as cgr season cover and management factor c cgr values are summarized in table 1 along with details and calculations within appendix c eqs c 5 c 6 and donovan and monaghan 2021 soil characteristics used to calculate soil erodibility k factor were derived from the fundamental soil layer fsl a free and publicly available national soil map newsome et al 2008 with physical chemical and mineralogical information the maps were compiled at 1 63 360 inch to mile from 1500 soil profiles that were refined using local surveys topographic maps and aerial photos much of the methods data attributes and categories are described in the lris data dictionary newsome et al 2008 and were derived from either webb and wilson 1995 or clayden and webb 1994 inherent soil erodibility k was calculated using the standard derivation eq 1 renard et al 1997 with additional adjustments for stoniness kst found in panagos et al 2014 poesen and ingelmo sanchez 1992 poesen et al 1994 1 k 2 1 10 4 m 1 14 12 o m 3 25 s 2 2 5 p 3 100 0 1317 for pastoral lands we calculated a seasonal grazing adjusted soil erodibility ktr using recent livestock treading model donovan and monaghan 2021 that captures the change in two subfactors soil structure s tr and permeability p tr that are impacted by livestock treading eq 2 the model uses grazing information livestock hoof pressure stocking density grazing duration and grazing history along with soil properties structural vulnerability soil moisture soil texture the supporting material e g section 1 4 describes the details of each soil attribute used to calculate inherent k and treaded soil erodibility ktr 2 k t r 2 1 10 4 m 1 14 12 o m 3 25 s t r 2 2 5 p t r 3 100 0 1317 for each season soil loss from surface erosion es eq 3 was calculated as the product of all factors rseason l s k or ktr season and cseason or cgr season which were then summed to produce grids of annual erosion e yr eq 4 3 e s e a s o n r s e a s o n k t r s e a s o n l s c g r s e a s o n 4 e y r e s p e s u e a u e w i where e sp e su e au and e wi are soil losses for the spring summer autumn and winter we omitted non erosive areas and those considered unsuitable for rusle modelling including urban developments glaciers bedrock exposures mountainous environments mines quarries wetlands and open water for additional details on the data processing algorithms and software used see appendices a e for evaluating accuracy and uncertainty of the soil loss estimates the annual soil loss grid was clipped and summed for specific paddocks farms land use classes and or catchments with published measurements of soil loss the studies used for assessing uncertainty included all comparable known research on soil losses measured using in field sediment traps flumes storm and baseflow sampling estuary and lake sediment cores and caesium 137 areal distributions fig 8 table 3 finally we use recent economic analyses that valued the cost of surficial soil loss at 1 2 nzd per tonne soliman and walsh 2020 to calculate the total cost associated with surficial soil losses across new zealand specifically the product of total soil losses for new zealand tonnes and average cost rate 1 2 nzd per t yr 1 yielded the cost associated with surficial soil loss the cost estimates only account for the value of soil lost not costs associated with environmental remediation or loss of farm productivity 3 results discussion 3 1 rainfall erosivity r across new zealand annual rainfall erosivity varied from 100 to 20 000 mj mm ha 1 hr 1 yr 1 fig 2 table 2 with mean values of 2481 and 2952 for the north and south islands respectively the mean annual values for each island lie at the high end of the range for oceanic climates cfb in the köppen climate classification found in previous global rainfall analyses panagos et al 2017 e g fig 3 extremely high values 10 000 exist within small portions of new zealand s southern alps comparable to some of the highest values found worldwide in regions such as the amazon da silva 2004 panagos et al 2017 the spatial distribution seasonal variation and range of r factor values align with previous values modelled for new zealand klik et al 2015 confirming the precision and seasonal distribution of rainfall erosivity results despite using distinct input datasets seasonally both islands experience maximum rainfall erosivity in summer while the lowest quartile of values occurred in spring seasonal variability in mean r factor values were slightly different the highest mean rainfall erosivity for the north island occurred during autumn while average erosivity for the south island peaked during summer in other words maximum potential erosive power of rainfall does not coincide with the period during which the majority of land area is experiencing elevated erosive forces from rainfall terrain is known to have an important role in rainfall quantity at local scales via orographic effects hutchinson 1968 however this effect was thought to be negligible at sufficiently broad scales tait et al 2006 within each region fig 2 elevation explained 14 50 of the variability in log rainfall erosivity based on linear regressions which was improved further 39 86 when incorporating latitude and longitude supplementary material appendix b figs b1 b5 this is the first analysis and results to show such a pronounced impact of terrain on rainfall erosivity over broad spatial scales across new zealand 3 2 terrain slope length and steepness factors ls the methods used herein are the first national scale map of the ls factor for aotearoa new zealand providing a high resolution map of how terrain slope length and steepness impact risk to surficial erosion respectively the north and south islands had mean ls factor values of 1 62 and 1 29 σ 1 83 and 1 91 which are comparable to the central tendency μ 1 63 of the european union panagos et al 2015a while the vast majority of land area across new zealand has low ls factor values fig 3 the southern alps give rise to more land with intermediate to high ls factor values fig 3b compared to the north island this is evident in a 25 higher mean ls value for the south island compared to the north island which is equivalent to a 25 increase in average susceptibility to surface erosion arising from topographic differences alone 3 3 soil erodibility k ktr soil erodibility values k are directly proportional to actual erodibility so higher k values indicate soils are more susceptible to being eroded and vice versa soil erodibility exhibited moderate spatial variability with bimodal distributions for both islands fig 4 mean and median ktr values were 0 029 σ 0 018 for the south island which were similar but slightly higher than the north island 0 020 σ 0 014 the mean values are slightly lower than that of the european union μ 0 032 which may reflect the significant impact of the loess belt across eu which is generally associated with higher erodibility panagos et al 2014 new zealand soil erodibility was generally inversely related to permeability fig 5 b and directly related to dominant particle size fig 5c indicating agreement with expected trends of soil erodibility ktr values were previously validated by comparing the change in soil erodibility with field measurements of pre and post grazing changes in soil macroporosity for a range of grazing pressures spanning pastures and forage crops donovan and monaghan 2021 the inclusion of stoniness reduced mean soil erodibility by 4 5 demonstrating the importance of including the stoniness effect into soil erodibility calculations across grazed areas of new zealand the effect of including livestock treading increased soil erodibility ktr by 3 3 9 with the most pronounced effects occurred in winter due to the impacts of intensive forage crop grazing practices 3 4 cover management factor the range of cover factors cf across new zealand varied from 0 to 1 with similar variability for both north and south islands fig 6 c factor values are inversely proportional to the effectiveness in reducing erosion thus c factor values of 0 reflect 100 reductions in surface erosion while 1 reflects no reduction in surface erosion differences across seasons were minor apart from croplands and winter forage paddocks which exhibit strong variation due to harvesting senescence and or grazing significant variability existed across land use and cover classes reflecting the effectiveness of each vegetation in mitigating surface erosion forests were consistently the most effective in reducing surface erosion followed by grasslands with woody biomass ungrazed grasslands grazed grasslands perennial and annual croplands and winter forage crop paddocks fig 7 prior to grazing low and high producing grasslands exhibit negligible differences in cf values blue boxplots fig 7 however after grazing mean cgr values were 53 greater for high producing grasslands orange boxplots fig 7 equivalent to doubling in susceptibility to surface erosion further investigation revealed the increased susceptibility for high producing grasslands reflected the predominance of grazing by dairy cattle which generally leave 10 less residual ground cover 75 80 cover compared to grazing by sheep and beef cattle 85 90 cover elliott and carlson 2004 pande et al 2000 based on empirical relationships between ground cover and soil loss silburn et al 2011 this modest 10 15 gap in ground cover is equivalent to a 53 difference in mean annual soil loss these results demonstrate how retaining ground cover residuals following grazing can be used as a simple means to enhance soil retention in grazed lands 3 5 uncertainty and validation in order to provide a thorough and transparent view of the uncertainty associated with rusle surface erosion outputs we expand upon uncertainty analyses presented in donovan and monaghan 2021 herein we go beyond pastoral grasslands and winter forage crop paddocks to incorporate mixed landuse catchments oldman et al 2009 forested lands o loughlin et al 1978 croplands basher and ross 2002 and natural tussock grasslands o loughlin et al 1984 the studies used for assessing uncertainty include all comparable measurements of soil losses that are dominated by surficial erosion processes rill and interrill erosion rather than gullying and or shallow landslides we compare with studies using a diverse set of measurement techniques spanning in field sediment traps flumes storm and baseflow sampling estuary and lake sediment cores and caesium 137 areal distributions fig 8 table 3 comparisons between measured and modelled soil losses indicate that modelled rates fall within the range measured at each field location found in literature fig 8a this is supported further by a strong linear regression r2 0 86 between measured and modelled rates fig 8b additional details on the studies used for validation and uncertainty analyses are provided in appendix e ongoing work is underway to validate improve upon and assess uncertainty in calculated soil losses at farm and catchment scales from these comparisons we can suggest that the soil losses modelled via the grazing adjusted rusle framework adequately capture the magnitude of soil losses expected under average rainfall conditions for the range of land uses considered the broad scale comparisons herein and in donovan and monaghan 2021 demonstrate significant alignment between modelled and measured long term surficial soil losses from surface erosion where such losses are transported this is further supported by favorable alignment between spatially explicit comparisons between modelled and measured soil losses rusle based modelled soil losses represent total potential soil losses from surficial erosion processes and thus we do not suggest they represent sediment transport or the total soil losses entering waterways of new zealand future work should assess the fate of surface erosion using sediment transport modelling alongside bank erosion measurements from aerial imagery to gain a more complete picture of catchment sediment sources six subcatchments within manawatu catchment which have been the focus of numerous monitoring and modelling studies are used to understand how surficial erosion compares to total catchment sediment yields fig 9 this comparison provided the first estimate of potential contributions from surface erosion over timescales sufficiently long to assume 100 sediment delivery over annual timescales this approach cannot ascertain how much modelled surface erosion is contributing to sediment yield due to temporal lags in sediment delivery of rivers impacted by land use change donovan et al 2021 donovan and belmont 2019 and because rusle does not account for sediment transport and deposition thus the comparisons likely reflect the high end of potential contributions to annual sediment yields found in literature vale et al 2016 modelled surface erosion across all six subcatchments represented 30 of the total annual sediment yield measured at each outlet which varied from 23 48 of downstream sediment yields in five of the six subcatchments vale et al 2016 for the sixth mangatainoka subcatchment modelled soil losses were 73 greater than downstream yields which is similar to previous modelling efforts with sednetnz dymond et al 2014 vale et al 2016 the consistency of this overestimate suggests that either significant deposition occurs measurements underestimate sediment contributions vale et al 2016 or land conditions exist within the catchment that are not captured within the models or input datasets mapping potential hotspots of soil loss across the catchment fig 9 illustrates how sparse vegetation cover along steep uplands in the northwest portion of the pohangina subcatchment may be driving elevated surface erosion rates an abrupt shift along the boundary of the pohangina and upper manawatu catchments with dense vegetation cover appear much less susceptible evident in the abrupt spatial transition from orange red to green hues we confirmed this shift is not explained by changes in slope soils and or rainfall erosivity southwest portions of the manawatu catchment also exhibit low rates of surface erosion and are thus unlikely to contribute significant sediment to the river this excludes streambank erosion which is often a significant source of sediment remobilized into downstream estuaries donovan et al 2015 2016 3 6 soil loss contributions across catchments and land uses modelled soil losses for all lands subject to surface erosion processes across new zealand fig 9 have the potential to reach 16 5 29 2 mt y 1 fig 10 table 4 over timescales sufficiently broad to encompass transport to rivers these could account for 15 of the 192 mt y 1 of sediments estimated to reach waterways and oceans annually ministry for the environment stats nz 2018 while eroded sediments may not reach waterways over sub decadal timescales due to intermediary deposition such soil losses nevertheless are a loss of significant social spiritual and economic value to communities of aotearoa new zealand especially māori asher and naulls 1987 harmsworth 2020 hutchings et al 2018 previous economic evaluations valued new zealand soils at an average of 1 2 nzd per t y 1 soliman and walsh 2020 applying this value to the soil loss estimates herein indicates that surficial soil losses represent a loss of 20 40m annually excluding the environmental costs of remediation and losses in agricultural productivity for the 22 catchments considered the average soil loss via surface erosion from pastoral grasslands was 0 84 t ha 1 y 1 and ranged from 0 02 to 2 5 t ha 1 y 1 fig 11 which is lower than average soil losses for the european union 2 02 t ha 1 y 1 and globally 1 70 t ha 1 y 1 doetterl et al 2012 panagos et al 2015c within the six catchments examined further these rates accounted for 34 92 of the total soil loss from surface erosion fig 11 while the fraction of pastoral grassland landcover occupied 15 76 of the catchments erodible area on the other hand winter forage crop paddocks on slopes greater than 7 were 1 of the catchment areas but accounted for up to 12 of the catchments total surface erosion fig 12 put another way grazed forage crop paddocks contribute 7 to 120 fold more soil loss relative to proportion of land area they occupy the average rates of soil loss 11 t ha 1 y 1 from forage crop paddocks were the highest of any land use cover modelled with considerable variability 3 90 24 52 t ha 1 y 1 previous analyses of soil losses from forage crop paddocks demonstrated that such high rates reflect the combination of negligible ground cover following grazing intensive treading on wet soils and relatively steep terrain donovan and monaghan 2021 natural ungrazed grasslands had highly variable rates of soil loss μ 2 0 range 0 02 10 t ha 1 y 1 and generally contributed higher proportions of surface erosion relative to their fractional land area additional investigation revealed that natural grasslands generally inherit high rates of surface erosion from steep slopes and highly erodible soils donovan and monaghan 2021 which was the same for natural grasslands across the european union panagos et al 2015c thus natural grasslands are not comparable equivalents of pastoral grasslands are typically characterized by shallower terrain and improved soil quality that support high pasture production further higher soil losses from natural grasslands relative to pastoral grasslands cannot be used to infer that grazing reduces surface erosion grasslands with native woody biomass exhibited negligible rates fig 11 and proportions 2 of soil loss from surface erosion across the catchments fig 12 forested lands also contributed very minor proportions of surface erosion apart from the rangitaiki catchment which contained 80 forested land cover in both land classes the reduced soil losses were primarily the result of dense vegetation cover in mitigating soil loss as we found no differences in slope soil quality or rainfall for such land classes this is supported by mechanistic understanding of vegetation s ability to mitigate surface erosion through rainfall interception root cohesion surface matting effects and reducing the volume and rate of overland flow despite having moderate rates of soil loss at local scales 0 09 5 01 t ha 1 y 1 annual and perennial crops represented a small proportion of surface erosion at catchment scales because they occupied a small fraction of land cover together annual and perennial crops represented 1 4 2 3 of the land area and generally contributed 5 of soil loss via surface erosion 4 conclusions and future directions we present the first national scale model of soil lost via surface erosion that accounts for the impacts of grazing and treading on ground cover and soil erodibility donovan and monaghan 2021 using a rusle modelling framework that is capable for modelling alternative grazing management scenarios to understand the impacts to soil loss and catchment water quality modelled soil losses herein exhibited favorable alignment with spatially explicit comparisons of field measurements lake sediment cores and chemical fingerprinting measurements of soil losses across a variety of land uses soil losses via surface erosion across aotearoa new zealand may reach 29 2 mt y 1 representing a loss of 20m 110m annually apart from forests and grasslands with woody biomass annual soil losses were generally higher than natural rates of soil production and loss expected from native landscapes hancock et al 2020 montgomery 2007 the effect of land management was most pronounced for intensive winter forage crop paddocks which exhibited high rates 11 t ha 1 y 1 and highest relative proportions of annual soil loss figs 11 and 12 across the 22 catchments analyzed this reflected the combined effects of negligible residual ground cover following grazing soil treading damage to wet soils and relatively high slopes such areas thus represent the highest cost benefit ratio for reducing soil losses across new zealand where increasing post grazing residual ground cover could greatly enhance soil retention thereby reducing the gap between rates of soil production and erosion hancock et al 2020 montgomery 2007 comparing modelled soil loss with downstream sediment yields from the mahurangi estuary supplementary material appendix e and subcatchments to the manawatu river fig 9 suggested that surface erosion could account for up to 24 32 of sediment yield over timescales sufficiently long to allow 100 sediment delivery future work should aim to model sediment transport and deposition to better resolve the proportion of soil losses from surface erosion that reach waterways lastly we demonstrate that topography has a dual role in impacting soil loss from surface erosion directly through slope steepness and length and indirectly through strong orographic effects that explained up to 50 variance of rainfall erosivity future work should further explore the relationship between elevation and rainfall erosivity at varying spatial scales to determine the scale at which orographic effects diminish in importance relative to broader weather patterns further by incorporating expected climate change impacts to rainfall quantity and intensity the framework herein could be used to project seasonal and annual soil loss scenarios funding this work was funded by the new zealand ministry for business innovation and employment s our land and water national science challenge toitū te whenua toiora te wai contract c10x1901 phase 2 c10x1507 phase 1 as part of the whitiwhiti ora land use opportunities research programme supporting holistic decision making to improve future landscapes of aotearoa new zealand declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements many thanks are extended to all those who contributed to the development of this research specifically ross monaghan peter pletnyakov richard muirhead hugh smith david houlbrooke and dan sun for their expertise technical support and critique that helped improve the quality of work presented herein a special thanks to desneiges murray for her statistical graphing and personal support of the work throughout its various iterations we extend appreciation to lucy burkitt fiona curran cournane andrew tait and wei hu for sharing data and materials that helped quantify uncertainty of this model and promote transparent data sharing lastly we thank the people of aotearoa new zealand who continue to support the conservation and restoration of the land ngā mihi nui ki a koutou he waka eke noa appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105228 
