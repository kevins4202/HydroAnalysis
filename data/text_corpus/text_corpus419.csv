index,text
2095,deep neural network dnn models have become increasingly popular in the hydrology community however most studies are related to rainfall runoff simulation and comparatively fewer studies have focused on runoff forecasting in this study quantile based q 0 05 0 5 0 95 encoder decoder ed models that use long short term memory network lstm and dense network dn blocks were developed for three and five days ahead runoff forecasting through linear lw and non linear nlw wavelet selection hybrid models lstm dn lstm dn lw lstm dn nlw ed ed lw and ed nlw were developed for each lead time lt 3 5 and value of q different model configurations were created using different input lag lengths il 15 45 180 the developed models were tested for runoff forecasting using three basins with different characteristics from the catchment attributes and meteorology for large sample studies camels dataset the models were compared using deterministic e g the kling gupta efficiency kge metric and probabilistic e g reliability statistical metrics while the models showed high variability in performance across the three basins kge 0 308 0 979 for the q 0 5 models very high accuracy up to kge 0 979 was achieved for one of the basins with high snowmelt the ed nlw model was found to generally outperform the other models although the lstm dn model had the highest median kge 0 434 across all configurations the ed and ed nlw models had higher reliability than lstm dn 90 and 91 respectively considering a 90 confidence level models coupled with nlw performed superior to those that used lw all ed models had high reliability despite two of the basins achieving median kge values of 0 390 highlighting that quantile based models can generate reliable forecast intervals even when the kge of the median forecast q 0 5 is low an additional experiment generated synthetic precipitation forecasts with varying degrees of accuracy the models were trained using accurate precipitation forecasts and tested using both accurate and inaccurate precipitation forecasts while up to a 120 improvement in kge was found when accurate precipitation forecasts were used as input to the models using inaccurate precipitation forecasts resulted in a substantial decrease in reliability overall the results of this study can serve as a benchmark for future studies developing probabilistic dnn models for runoff forecasting keywords encoder decoder deep learning lstm wavelet decomposition runoff forecasting hydrological forecasting data availability the camels dataset used in this research can be obtained from https ral ucar edu solutions products camels 1 introduction accurate runoff forecasts play an important role in reservoir management flood risk evaluation irrigation power generation drought monitoring and many other initiatives due to its importance and the complex relationship between rainfall and runoff process based models e g samadi et al 2019 tasdighi et al 2018 knoben et al 2019 zhou et al 2019 chlumsky et al 2021 and data driven models e g li et al 2014 mehdizadeh et al 2019 kratzert et al 2018 2019 sezen et al 2019 cheng et al 2020 ni et al 2020 xie et al 2021 yin et al 2021 zhang et al 2021 have been thoroughly investigated for runoff modeling process based models leverage linear and differential equations bittelli et al 2010 partington et al 2012 zhang et al 2021 to describe the sub processes driven by physical mechanisms e g evapotranspiration interception infiltration xie et al 2021 although process based models can improve understanding of the physical processes of interest zhang et al 2015 there are two significant drawbacks associated with these models mehr et al 2013 ni et al 2020 first many simplifying assumptions are made in the modeling procedure due to the complexity of physical processes resulting in model uncertainties which are often neglected during model development and application moges et al 2020 the second drawback is that process based models are inefficient and require considerable information as model input on the other hand data driven models estimate statistical relationships between input explanatory and target response variables without considering the governing equations of the modeled system solomatine and ostfeld 2008 liu et al 2015 thus data driven models are often referred to as black box models compared to process based models data driven models are efficient and require less data during model development while process based models and data driven models have typically been developed independently there is an evolving area of research on coupling data driven models with process based models boucher et al 2020 chadalawada et al 2020 li et al 2021b lu et al 2021 sikorska senoner and quilty 2021 han and morrison 2022 alizadeh et al 2021 quilty et al 2022 although these coupled models have shown promise such approaches are outside the scope of this work and the interested reader is referred to studies devoted to this topic such as adombi et al 2021 for more details in the runoff simulation and forecasting literature data driven models can be divided into two groups the first group is statistical methods mainly comprising of linear methods such as autoregressive integrated moving average arima valipour 2015 the second group is machine learning methods ml where non linear methods such as multi layer perceptron mlp asadi et al 2013 and support vector machine svm li et al 2014 are adopted for formulating the non linear relationship between rainfall and runoff a sub set of the second group includes a new set of models based on deep learning of which deep neural networks dnns are most popular in hydrological applications recurrent neural networks rnns and especially long short term memory networks lstms both specific forms of dnns have been shown to outperform classical ml models such as mlp and svm for runoff prediction kratzert et al 2018 hu et al 2018 le et al 2019 rnns were designed to model temporal dynamics elman 1990 making them suitable for modeling non linear hydrological time series coulibaly and baldwin 2005 ni et al 2020 lstms first introduced by hochreiter and schmidhuber 1997 overcame the rnn s gradient exploding vanishing problem that occurs due to processing long sequences of data through internal non linear gates by regulating how much information is passed from one memory block to another please see section 3 for details although previous research has utilized dnns for runoff prediction there are existing shortcomings with their application that the current study seeks to address previous studies have mainly focused on runoff simulation using dnn e g kratzert et al 2018 xie et al 2021 not runoff forecasting the term forecasting is used in the ml domain when future values are predicted using information from the past and or present many articles e g lv et al 2020 alizadeh et al 2021 apaydin and sibtain 2021 have used the terms prediction and simulation interchangeably thus it is common to see the term runoff prediction used to represent runoff simulation or rainfall runoff modeling hereafter the term simulation will be used when runoff at time t q t is predicted using for instance precipitation and meteorological variables at time t the term forecast will be used when future runoff values q t l t where lt is the lead time are predicted using data at time t or further in the past ml studies that have focused on forecasting can be divided into two groups the first group uses future data such as meteorological forecasts in their forecast models while the second group utilizes only past observations in their models examples from both groups are described in the following two paragraphs in the first group of forecasting models cheng et al 2020 showed that lstm generally outperforms mlp for multi step ahead streamflow forecasting when precipitation forecasts for the following time steps were used as input data xiang et al 2020 introduced an lstm based multi step 24 h ahead runoff forecasting encoder decoder ed framework they used future rainfall observations and upstream runoff forecasts as inputs to their models yin et al 2021 introduced a novel lstm based sequence to sequence model for seven days ahead runoff forecasting where future meteorological data were utilized cui et al 2022 introduced a new lstm based ed framework for hourly flood forecasting where runoff forecasts from a hydrological model are fed to the decoder as exogenous variables recently girihagama et al 2022 evaluated attention based ed models for daily multi step ahead runoff forecasting up to five days ahead in 10 different watersheds in canada their proposed method showed the effectiveness of utilizing the attention mechanism coupled with lstm for runoff forecasting in practical engineering cases models such as the one introduced by xiang et al 2020 cannot be easily employed for forecasting as it is impossible to access future observations although meteorological forecasts can be used as input in many cases the temporal scales of the model of interest and meteorological forecasts are not the same furthermore meteorological forecasts include uncertainties due to different factors spatial interpolation model error etc which should be considered when quantifying the runoff forecasting model s uncertainty in the second group of forecasting models kao et al 2020 used an lstm based ed for multi step 6 h ahead flood forecasting they did not incorporate any future data in their model their results showed that their proposed method outperforms the feed forward neural network ed especially for longer lead times han et al 2021 compared an lstm based ed for multi step 6 h ahead runoff forecasting with svm where only past runoff data and upstream observations were used their proposed ed framework outperformed svm and a standard lstm this research explores runoff forecasting using two distinct approaches in the first approach the second group s methodology is followed where only past observations of runoff and meteorological data are used as input in the second approach to demonstrate the impact of meteorological forecast uncertainty on the accuracy and reliability of runoff forecasts the use of synthetic precipitation forecasts as a model input is explored where the developed data driven forecast models mimic process based hydrological forecasting models in contrast to process based hydrological forecasting models however air temperature and potential evapotranspiration forecasts are not included in the second approach in an ed framework the input data are encoded through an encoder model into a vector for capturing the impact of local factors afterwards the local factors are mapped to the target by decoding the encoded vector through a decoder model apaydin and sibtain 2021 the ed structure is capable of extracting temporal associations among time series cui et al 2022 therefore the ed configuration is able to better model non linear relationships between time series compared to single network configurations bian et al 2020 the latter is of great importance in hydrological forecasting due to the complex properties of hydrological processes one of the main advantages of an ed framework compared to other ml models is that there is no restriction on designing the encoder or decoder thus the encoder and decoder can be any type of neural network recurrent convolutional or dense the latter results in a multitude of choices for designing the ed components therefore ed frameworks offer a flexible approach to obtaining accurate hydrological forecasts 1 1 novelty although previous research e g apaydin and sibtain 2021 han et al 2021 has shown that lstm based ed frameworks outperform vanilla standard lstm models for runoff forecasting there exists important aspects of these frameworks that have not yet been thoroughly investigated a major drawback of the previous studies is their focus on deterministic forecasting in contrast this work develops quantile based dl models see section 3 for details for probabilistic runoff forecasting although several earlier studies have adopted quantile regression methods in the context of hydrology and water resources cannon 2011 bürger et al 2013 tyralis et al 2019 papacharalampous et al 2019 papacharalampous and langousis 2022 development of quantile based dl models for hydrological forecasting has not yet been explored in this work predictive uncertainty is modeled through conditional quantiles acharya et al 2020 instead of simply obtaining the predictive mean using a neural network prediction quantiles q are obtained although this research only focuses on estimating three quantiles 0 05 0 5 0 95 the proposed methodology can be adopted to approximate the full predictive distribution by increasing the number of estimated quantiles the three estimated quantiles are associated with the median as well as the lower and upper bound of a prediction interval which hereafter is referred to as a forecast interval one of the benefits of choosing quantile based models is that no prior assumption a source of uncertainty itself is made regarding the predictive distribution type see nevo et al 2022 unlike many previous studies see section 3 for details all forecast lead times are obtained simultaneously therefore the joint predictive distribution of the quantiles is estimated to the best of the authors knowledge no previous study has used dl models in this context other than a recent study ponnoprat 2021 which compared dl models trained using mean squared error and pinball loss functions for daily rainfall forecasting while the pinball loss function can be used for quantile based forecasting see section 3 3 the author included only a brief experiment in an appendix tabulating the performance of the two loss functions the latter for different quantiles without a detailed analysis of the results furthermore hyper parameter selection is important for dnn models goodfellow et al 2016 while grid search is commonly used for hyper parameter selection in dnn models kratzert et al 2018 the traversed hyper parameter search space is limited due to the computational burden of this approach in this work bayesian optimization alizadeh et al 2021 is adopted for efficient hyper parameter selection and model structure design wu et al 2019 bayesian optimization is a recent method for selecting the hyper parameters of data driven models in hydrological and water resources applications e g malik et al 2020 zuo et al 2020 sikorska senoner and quilty 2021 quilty et al 2022 and has seldom been used with dl based models e g alizadeh et al 2021 alibabaei et al 2021 hah et al 2022 including those used for hydrological forecasting barzegar et al 2021 bai et al 2021 lian et al 2022 given its recency bayesian optimization has not yet been used for selecting the hyper parameters of ed based models furthermore to account for multiscale change this study uses wavelet decomposition to extract important time frequency information from the hydro meteorological input variables wavelet decomposition has been shown to improve the accuracy and or reliability of hydrological and water resources forecasts quilty and adamowski 2018 2020 2021 ghaemi et al 2019 zhou et al 2020 hammad et al 2021 barzegar et al 2021 hao et al 2022 chidepudi et al 2023 jamei et al 2023 however the selection of the decomposition level and wavelet filter hereafter referred to as wavelet selection has not yet been considered for dl based hydrological forecasting hence this study proposes two different approaches for wavelet selection couples them with probabilistic dl models and evaluates which method leads to better runoff forecasts in general wavelet selection can be seen as identifying the wavelet sub series that result in the best forecasts where each unique wavelet filter and decomposition level combination results in different wavelet sub series in the next sub section the main contributions of this work are summarized 1 2 contributions and outline this research develops tests and applies a novel ed framework for multi step ahead runoff forecasting we make modifications to the framework of kao et al 2020 when building the ed models and adopt the approach taken by yin et al 2021 for creating precipitation forecasts see section 3 for details various model configurations are applied to three basins from the catchment attributes and meteorology for large sample studies camels dataset addor et al 2017 the four main contributions of this study are as follows 1 the present work is the first study to explore dnns and eds in particular for quantile based probabilistic hydrological forecasting with an application to runoff obtaining an appropriate forecast interval is of great importance in multi step ahead runoff forecasting as forecast uncertainty increases with increasing lead time this method can be beneficial for many water resources applications flood warning systems reservoir operations etc even when the median forecast is inaccurate 2 unlike previous studies e g yin et al 2021 girihagama et al 2022 kao et al 2020 bayesian optimization is used for optimizing the hyper parameters of the ed models furthermore in contrast to earlier studies e g han et al 2021 kao et al 2020 multiple model configurations are evaluated for ed 3 although this research focuses on developing models that use past observations as input the impact of using precipitation forecasts as input to the models is also evaluated 4 this research explores wavelet decomposition for improving the models accuracy and reliability two different methods namely linear lw and non linear nlw wavelet selection are adopted to select the best wavelet sub series with the goal of reducing the number of input variables while improving the accuracy and reliability of the models the rest of the paper is organized as follows the adopted data and its key descriptive statistics are explored in section 2 ed model structure design and optimization wavelet decomposition and input variable selection ivs are described in section 3 results are analyzed and discussed in section 4 the paper is concluded in section 5 2 dataset the camels dataset is a publicly accessible dataset containing 674 basins across the contiguous united states with minimal human disturbance the basins are divided into 18 hydrological units hus following the u s geological survey usgs hu map daily runoff values are obtained from the usgs national water information system covering october 1 1980 to december 31 2014 three different daily gridded forcing datasets are included in the camels dataset in this study the daymet data thornton et al 2014 is used for the same period as daily runoff 1980 2014 due to its higher spatial resolution 1 km grid compared to the other two gridded forcing datasets 12 km grid similar to yin et al 2021 daymet variables including precipitation maximum and minimum air temperature shortwave downward radiation and vapor pressure were considered when developing the models more details about the camels data can be found in newman et al 2015 and addor et al 2017 from the available 674 basins three basins associated with different hus were selected to evaluate the proposed models the three basins ids 09081600 14154500 03488000 are shown along with their hus in fig 1 for simplicity basins 09081600 14154500 03 488 000 will be referred to as basin 1 basin 2 and basin 3 respectively basin 1 belongs to hu 14 upper colorado approximately 432 k m 2 basin 2 belongs to hu 17 pacific northwest approximately 546 k m 2 and basin 3 belongs to hu 6 tennessee approximately 572 k m 2 this study aims to find the best model configuration for each basin through hyper parameter optimization therefore three basins with different properties were selected the three selected basins have different hydro climatological properties and are dissimilar based on the complexity of the rainfall runoff process within each basin for each basin the camels dataset includes the simulated discharge generated by the sacramento soil moisture accounting model coupled with the snow 17 model addor et al 2017 considering the calibration period 1980 1995 the highest nash sutcliffe efficiency nse mccuen et al 2006 belongs to the simulations associated with basin 1 nse 0 90 followed by basin 2 nse 0 82 and basin 3 nse 0 74 the past observations of meteorological data daymet forcing data and runoff were considered as base inputs to the models for forecasting runoff at three and five days ahead the data was split into three partitions namely training 75 of data validation 15 of data and testing 10 of data for each basin a similar testing period with approximately three years of samples 1 2 2012 12 31 2014 1095 samples was used for all models to make the comparison of the models forecasts meaningful the training set was used to train the models optimize the network weights and biases while the models hyper parameters and wavelet decomposed sub series were selected using the validation set see section 3 the results provided in section 4 are associated with the testing set which was not used to train or validate the models in order to evaluate out of sample performance the relationship between rainfall and runoff in the testing set is depicted at the bottom of fig 1 descriptive statistics of the base inputs i e without wavelet decomposition for each basin and each partition training validation and testing are shown in table 1 3 methods this section explores the methods adopted for building the forecast models an overview of the general process is given in the sub section below 3 1 overview two different types of models based on structure design were considered in the first structure type a lstm block containing lstm cells see section 3 2 is used for feature extraction and is connected to a dense network dn this structure is called lstm dn in the second model type an ed model is designed where the encoder is a lstm and the decoder is a lstm dn see section 3 2 the latter configuration is referred to as ed in the rest of the paper the two model types are coupled with three input scenarios resulting in six different model configurations in the first input scenario only the previous meteorological data and runoff observations are used as input lstm dn and ed in the second and third input scenarios the same inputs are considered as the first scenario along with their corresponding wavelet decomposed sub series selected through either linear lstm dn lw and ed lw or non linear lstm dn nlw and ed nlw wavelet selection respectively see section 3 4 the edgeworth approximation based conditional mutual information ea ivs method was combined with nlw to build parsimonious models and improve the forecasts as it has been shown in earlier studies to be an effective non linear ivs method when considering wavelet decomposed inputs in data driven models quilty and adamowski 2018 2020 2021 all inputs were individually z normalized by subtracting the mean and dividing by the standard deviation before being fed to the models the mean and standard deviation were obtained based on the training set to prohibit future data leakage into the training phase previous studies have adopted z normalization to improving the training of lstm based models e g yin et al 2021 the models hyper parameters lstm size number of layers in the dn dn layers size and activation functions and dropout rate were obtained by maximizing the validation set nse using bayesian optimization see section 3 5 the overall workflow is shown in fig 2 two different optimization routines were used for different purposes when optimizing the models the first routine used a modified version of stochastic gradient descent see section 3 3 for training all forecast models estimating the networks weights and biases through minimizing the pinball loss function while bayesian optimization was used for selecting the best hyper parameters in the beginning stage zero optimal hyper parameters were obtained for three different input lags il 15 45 180 and two lead times lt 3 5 for all three basins requiring 2700 models to be trained see section 3 5 for details all six model types were developed and optimized for multi step multivariate forecasting i e forecasting all the lead times simultaneously see the bottom of fig 2 in stage one for every basin basin 1 basin 2 basin 3 three models based on each quantile q 0 05 0 5 0 95 were trained using the best configuration il and lt from stage zero in total 324 six configurations three quantiles two lead times one input lag and three basins models were trained and employed for forecasting in stage one without precipitation forecasts different statistical metrics were used for analyzing the accuracy and reliability of the results see section 3 6 the developed models from stage one did not use any future data for forecasting making them ideal for locations where meteorological forecasts are not available however the developed models were further modified to accommodate cases where meteorological forecasts are available therefore in stage two the impact of using precipitation forecasts as input to the models was evaluated for the top three performing models associated with il 15 thus in stage two an additional 54 models three configurations three quantiles two lead times one input lag and three basins 1 were trained see section 4 as the camels dataset does not include meteorological forecasts following yin et al 2021 synthetic precipitation forecasts were generated by adding noise drawn from a normal distribution to the precipitation data before training the models in stage two noise from a normal distribution with zero mean and a standard deviation ranging from 0 1 for lead time one to 0 14 for lead time five with step increments of 0 01 was added to the training set precipitation data at lead times one through five respectively this resulted in precipitation forecasts with very high accuracy kge 0 95 in the testing phase the same approach was used but with different base standard deviations ranging from 0 1 to 1 with increments ranging from 0 01 to 0 1 following this approach the sensitivity of the models to precipitation forecast accuracy was assessed this experiment was meant to mimic how process based hydrological models are used for forecasting typically process based hydrological models use data from rain gauges or gridded reanalysis products for model training calibration but afterwards use precipitation forecasts from different products when running the hydrological model in forecasting mode boucher et al 2012 the precipitation forecasts are added to the dnn models through a separate dn with one layer and then concatenated to the lstm states in the next two sub sections an overview is given on lstm including the attention mechanism and ed models respectively 3 2 long short term memory lstm and attention layer this section follows a slightly modified version of the notation used by kratzert et al 2019 the lstm block is shown in fig 3 the internal structure of lstm enables the network to process and memorize data sequences where the latter is done by each cell s gates as shown in fig 3 for a specific lag time t t 1 il and input series x t 2 dimensional number of samples number of features an lstm can be formulated as kratzert et al 2019 1 i t s i g m o i d w i x t u i h t 1 b i 2 f t s i g m o i d w f x t u f h t 1 b f 3 g t t a n h w g x t u g h t 1 b g 4 o t s i g m o i d w o x t u o h t 1 b o 5 c t f t c t 1 i t g t 6 h t o t tanh c t where w and u are weight matrices b is the bias vector the operator is element wise multiplication while i t f t and o t are the input forget and output gates at step t respectively whereas g t h t c t are the cell input recurrent state and cell state at step t respectively and i f g and o represent indices associated with the weights and biases of their respective gates or states the sigmoid function is defined as s i g m o i d x 1 1 e x while the hyperbolic tangent function is defined as t a n h x e 2 x 1 e 2 x 1 the output gate o t controls which saved information is outputted and a loop is used for feeding the previous cell states c t 1 and recurrent states h t 1 into the network in the lstm dn configuration the output from the last iteration h t 1 is fed to a dn with 1 3 layers the forecast values were obtained by placing a linear layer on top of the dn with the number of nodes equal to the lead time 3 or 5 therefore all forecast steps were obtained simultaneously also known as many to many networks in addition to the standard lstm lstm with a dot product attention layer luong et al 2015 was also evaluated in this study the custom self attention layer measures the alignment score of the states h giving more weight to previous lags than a standard lstm instead of using the last state values h t 1 on its own all state matrices are utilized in this setup 3 3 encoder decoder framework in this study a novel ed framework is introduced by modifying the structure used by kao et al 2020 the main differences include using a dn to process the lstm block s output states and training the models with a quantile based loss function employing a dn as the final block of the forecast model makes the model more robust to error summation in direct sequence to sequence models such as the ones adopted by kao et al 2020 and xiang et al 2020 the recurrent states of the final lstm block are reported as the forecasted values where each step forecast has a direct impact on the successive step in this work first the meteorological data including their wavelet decomposed sub series are fed to an lstm the first lstm acts as an encoder where it processes the input sequence and outputs the recurrent h and cell c states see fig 2 3 and eqs 5 6 the state vectors are used as initial states h 0 c 0 for the second lstm block the decoder where the past observations of runoff including their wavelet decomposed sub series are directly fed to the second lstm block similar to lstm dn the final recurrent state is passed to a single layer dn followed by a linear output layer in the proposed model the meteorological data and runoff are processed separately which is beneficial in cases where different ils are used for different input variables instead of using the mean square error mse as the objective function for training the models the pinball loss function is adopted wang et al 2019 7 l o s s q t 1 q q t q q t q t q q t q q t q t q q t q q t where q is the quantile of interest q 0 05 0 5 0 95 q t is the observed runoff at step t and q t q denotes the q th quantile of the forecasted runoff at step t output of the forecast model see fig 2 all models lstm dn ed were developed using custom scripts and available libraries in python 3 7 including the tensorflow 2 4 1 library abadi et al 2016 with keras api chollet 2015 models were trained using the adaptive moment estimation adam optimization method kingma and ba 2014 a dynamic learning rate was used for all models with an initial value of 1e 4 and a decreasing factor of 0 5 the decreasing factor was applied every three epochs when no improvement was observed on the validation set the training procedure was terminated when the validation set accuracy did not increase for ten consecutive epochs 3 4 wavelet decomposition this paper follows the best practices for wavelet based forecasting proposed by quilty and adamowski 2018 when decomposing the meteorological data and previous runoff observations the maximal overlap discrete wavelet transform modwt was adopted to avoid the future data problem and boundary affected indices samples were removed from the dataset for brevity the interested reader is directed to walden 2001 and quilty and adamowski 2018 for details regarding modwt and its implementation each input was decomposed into wavelet and scaling coefficients according to a given wavelet filter and decomposition level see quilty and adamowski 2018 the selection of the wavelet filters was done by grid search through fitting multivariate linear regression mlr lw method and svm nlw method for each basin and lead time lt 3 5 in total 19 wavelet filters from seven families covering filter lengths 2 14 haar daubechies d2 d7 symlets sym4 sym6 fejer korovkin fk4 fk6 fk8 fk14 coiflets coif1 coif2 least asymmetric la8 la12 la14 and best localized bl14 and decomposition levels 1 6 were considered details regarding the wavelet filters can be found in crowley 2007 nielsen 2001 olhede and walden 2004 percival and walden 2000 rathinasamy et al 2013 and zhang et al 2016 overall 114 combinations six decomposition levels and 19 wavelet filters were evaluated using mlr and svm for each basin and lead time resulting in 1368 114 wavelet combinations two wavelet selection methods three basins two lead times models using the validation set the wavelet filter decomposition level combination resulting in the highest nse was selected and used for decomposing the input data for use in the dnn models lstm dn ed lstm lw ed lw lstm nlw ed nlw for lw the scaling coefficients associated with the last decomposition level and the wavelet coefficients from all decomposition levels collectively referred to as wavelet decomposed sub series were used together with the original input data in the dnn models however for nlw the ea method with a tolerance value equal to 0 05 was used to reduce the number of input variables used in the lw method resulting in dnn models that were more parsimonious the ea ivs method was selected since it can capture non linear relationships amongst inputs that may be missed by linear ivs methods details regarding the implementation of the ea method can be found in jahangir et al 2021 wavelet decomposition along with the ea method were implemented using custom scripts in r team 2013 while the mlr3 package lang et al 2019 was adopted for fitting the svm models with a radial basis function kernel the default package settings were used for fitting the svm models 3 5 bayesian optimization bayesian optimization derived from bayes theorem is an effective method for solving computationally demanding optimization problems that do not have closed form solutions brochu et al 2010 thus bayesian optimization can be leveraged for optimizing the structure of dnns through hyper parameter selection the objective function i e the pinball loss function is approximated by a gaussian process seeger 2004 by considering an acquisition function where high values of the acquisition function are associated with high values of the objective function the maximum value of the objective function is obtained through an iterative process of maximizing the acquisition function at each step of bayesian optimization multiple dn layers were considered in the dnn models developed in this study accordingly when applying bayesian optimization to the dnn models the dn layers properties i e architecture were obtained dynamically i e the size and activation type of the dn layers were not required to be similar details regarding bayesian optimization for ml hyper parameter selection may be found in wu et al 2019 in this study the hyper parameters that required optimization and their ranges include lstm hidden state size 16 256 the number of layers in the dn 1 3 the number of nodes in each layer 8 128 the dn layers dropout rate 0 0 3 and dn layers activation function type relu rectified linear unit sigmoid and tanh the hyper parameters were optimized based on q 0 5 i e for a specific model type similar hyper parameters were used for all quantiles the hyper parameters were optimized via bayesian optimization using 25 iterations for each iteration the dnn was trained for 20 epochs overall there were 2700 six configurations one quantile two lead times three input lags three basins and 25 iterations models developed via bayesian optimization the optimal hyper parameters for each configuration according to the validation set nse were selected and used to train a new dnn of the same configuration for 150 epochs resulting in 324 models see section 3 1 bayesian optimization was implemented via custom scripts and the scikit optimize 0 8 1 library head et al 2020 in python 3 7 3 6 forecast evaluation statistical metrics commonly adopted in hydrology were used to analyze the forecasts including the nse kling gupta efficiency kge gupta et al 2009 normalized root mean square nrmse mean absolute error mae reliability smith et al 2015 and the mean interval score mis gneiting and raftery 2007 the reliability metric measures the coverage interval of forecasts i e the percentage of observations positioned within the designated forecast interval on the other hand mis gives the weighted average width of forecast intervals based on designated quantiles penalizing instances where the observation lies outside the forecast interval mis is defined as gneiting and raftery 2007 15 m i s 1 n t 1 n q t 0 95 q t 0 05 2 0 9 q t 0 05 q t 2 0 9 q t q t 0 95 where q t q is the quantile forecast at time step t obtained through the forecast model n is the total number of samples and is the heaviside function since q 0 05 and q 0 95 were used as upper and lower quantiles the models desired reliability confidence level is 90 with low mis values being preferred li et al 2021a the percentage of missed peaks was also used to evaluate the forecasts and is defined as the percentage of observed values above the q 0 95 models forecasts to complement the missed peaks metric the percentage of negative forecasts associated with the q 0 05 models along with the average magnitude of negative forecasts were also reported the deterministic metrics nse kge nrmse and mae are associated with the median q 0 5 model and are used to assess forecast accuracy all the results are associated with the testing set a rolling window with a length equal to lt was used to produce the forecasts therefore similar to how the models were optimized non overlapping forecasts were used when comparing the performance of different models see fig 2 4 results and discussion six dnn model configurations lstm dn lstm lw lstm nlw ed ed lw ed nlw were used for forecasting runoff at two different lead times lt 3 5 the models are compared based on both il and lt for model comparison each point in the testing set is evaluated once a rolling window equal to lt as the models produce forecasts for all steps simultaneously sequential forecasting the forecast reliability and kge of each step 1 to lt is also reported individually to demonstrate how the performance changes for intermediate steps in an lt step forecast 4 1 comparing forecast performance across model configurations the models performance for all model configurations associated with lt 3 and il 15 shortest lead time and input lag length is shown as an example in table 2 as expected based on the runoff simulation accuracy see section 2 the nse and kge of the models for basin 1 are higher than the two other basins also for basins 2 and 3 as precipitation forecasts are not used as input at this stage a large gap is observed between simulation accuracy see section 2 and the obtained forecast accuracy the models for basin 3 have lower accuracy compared to the other two basins the highest nse values for basins 1 2 and 3 are associated with ed lw 0 968 ed 0 391 and ed 0 310 respectively the best kge for basin 1 is associated with lstm dn 0 959 while for basins 2 and 3 the ed model 0 475 and 0 407 respectively has higher kge values the nse for basins 2 and 3 indicates that 0 5 quantile median models resulted in inaccurate forecasts moriasi et al 2007 although the median forecasts have low accuracy for basins 2 and 3 see nse and kge values in table 2 the models forecast reliability is close to the desired 90 the results indicate that the models trained to forecast the upper q 0 95 quantile generally captured the peaks with missed peaks lower than 7 for all developed models while expecting 5 based on the designated upper quantile for basins 1 and 2 the best model configuration according to the missed peaks is lstm dn nlw while for basin 3 ed lw resulted in the lowest missed peaks value evaluating the lower q 0 05 quantile models indicates that models associated with basin 2 especially lstm dn lw produce high rates of negative values the results show that in general models utilizing nlw obtain a lower mis compared to models using lw the results in table 2 pertain to lt 3 and il 15 for lt 5 the ed model resulted in the highest reliability ideally equal to 90 for basins 1 and 2 when using il 45 reliability 90 046 kge 0 928 and il 15 reliability 89 954 kge 0 418 respectively lstm dn nlw with il 15 resulted in the most reliable forecasts for basin 3 reliability 89 770 kge 0 384 while it is common to use very high il e g 365 days for runoff simulation gauch et al 2021 kratzert et al 2021 wang and karimi 2022 the results highlight that improved performance can be obtained through model structure optimization as opposed to solely increasing the il of the models which based on the recurrent structure of lstms increases computation time 4 1 1 selecting the best model configuration for a more thorough comparison of the models the performance of the models for all 18 configurations all basins three input lags three and lead times two are assessed the results are illustrated in fig 4 the dashed lines show the desired value for both metrics for each metric the boxplots whiskers are associated with the central 90 of the data and the solid line shows the median value based on kge lstm dn outperforms the other models while the poorest performance is related to lstm dn lw all three ed based models show similar performance while ed lw has slightly better performance for basins 2 and 3 according to the forecasts reliability the three model types ed ed lw and ed nlw generally show superior performance compared to the other models for the desired confidence level 90 ed lw generally shows the best performance among the models however its counterpart lstm dn lw has the lowest reliability the results show that the ed framework outperforms the other models based on the missed peaks metric 5 based on the defined forecast interval and has the fewest negative values based on fig 4 two points should be highlighted first lstm dn lw has inferior performance compared to the other models second coupling wavelet decomposition with ed models e g ed lw results in more accurate forecasts compared to when wavelet decomposition is coupled with lstm models e g lstm dn nlw the latter result is clear in fig 4b where lstm dn lw and lstm dn nlw perform poorly relative to the other models based on the results reported in table 2 and fig 4 the dnn models show dissimilar performance for different il and lt since the best hyper parameters were obtained for each configuration through bayesian optimization the comparison of the models among basins is meaningful thus to enable a more general comparison of the different model configurations their average rank according to different statistical metrics is presented in table 3 the model with the lowest rank corresponds to the best performing model configuration for a given metric bolded font except for the kge lstm dn and missed peaks ed lw metrics the model with the lowest best rank is ed nlw the results confirm that lstm dn outperforms other models according to kge while the ed has the lowest missed peaks values furthermore kge and the percentage of negative values are improved by coupling wavelet decomposition with ed models comparing results in table 3 and fig 4 indicates that while coupling lw and nlw with the ed structure is beneficial in general nlw is superior 4 2 input lags length impact evaluation one of the hyper parameters controlling the performance of the forecast models is il due to the recurrent structure of lstm networks il plays a significant role in the training time of lstm based models in the literature the il is commonly set to high values il 270 365 in lstm networks to obtain more accurate models feng et al 2020 kratzert et al 2019 however the above results confirm that model structure optimization model design and hyper parameter selection and input data preprocessing wavelet decomposition can be combined with lower il values to achieve similar or higher accuracy than models with high il values 4 2 1 model comparison for lt 5 the percentage of times each model is chosen as the best model based on the reliability and mae metrics for each of the 219 forecast instances see fig 2 is shown in fig 5 if any number of models had the best performance e g lowest mae at each forecast instance each model was selected as the best model therefore the selection percentage in fig 5 does not necessarily add to 100 across all models and il values for a given basin and performance metric reliability and mae the results confirm that a single model and specific il does not dominate the other models based on the reliability metric and for basins 1 and 2 lstm dn nlw with il 45 and 15 is selected more frequently than the others for basin 3 the ed and lstm dn models with il 15 have the highest selection rates based on mae the models with highest selection rate in basins 1 2 and 3 are associated with ed il 45 ed lw il 180 and ed lw il 15 respectively it is important to note that the results shown in fig 5 are associated with the models which on average are more frequently selected across forecast instances and is not equal to the reliability or mae computed across the testing set in other words models with a higher selection rate are not necessarily the most accurate or reliable models for example the results indicate see section 4 1 that the most reliable forecast model associated with lt 5 for basins 1 and 2 is the ed model and not the lstm dn nlw model which has a higher selection rate see fig 5 thus even though the different models show similar reliability on average across forecast instances the models show a high level of variability when considering the model accuracy i e associated with the q 0 5 models 4 2 2 the impact of input lags on a single model the lstm dn ed and ed nlw models were identified as the best performing models based on table 3 and fig 5 and were selected for further analysis in prior research il has been determined either through trial and error xiang et al 2020 or set to a large fixed value kratzert et al 2019 for lstm models high il values result in longer training times thus when computing resources are limited lower il values are preferred in fig 6 the kge associated with lstm dn ed and ed nlw for different combinations of input lag lengths are compared across all basins and lead times leading to six different scenarios in each sub plot if a point falls below the bisector line it indicates that the model built with the il value in the arrow pointing to the x axis resulted in higher kge than the il value in the arrow pointing to the y axis and vice versa if the point falls above the bisector for example in fig 6 lstm dn il 15 had higher kge than il 45 for most scenarios the top left sub plot in general for lstm dn il 15 resulted in the best performance for the same model the kge increased when il was increased from 45 to 180 for most scenarios the lstm dn performed best when using il 15 for the ed model see row 2 in fig 6 the best performance was obtained using il 45 the results confirm that adopting wavelet decomposition nlw along with ivs is beneficial as models with lower il have accuracy similar to models with higher il see row 3 in fig 6 although wavelet decomposition increases the lstm network size since more inputs are considered and bayesian optimization can be computationally demanding when combining both approaches low il values tend to be selected since these two approaches affect model training more than model testing such an approach may be preferred to the commonly adopted practice of selecting high il values which is computationally demanding for both training and executing lstm networks in real time 4 3 lead time assessment the current study explores multi step ahead i e multivariate forecasting where forecasts at multiple lead times are obtained simultaneously in particular for lt 3 one to three day s ahead forecasts are obtained and for lt 5 one to five day s ahead forecasts are obtained although the lt 5 forecasts contain the same lead times as lt 3 for the first three time steps the former requires the forecast models to be optimized using five outputs instead of three and this leads to an increase in the error and uncertainty of this models forecasts in other words it is expected that there will be a higher level of error and uncertainty in the one to three days ahead forecasts from the lt 5 models than the lt 3 models hence it is important to develop models whose forecast reliability does not diminish considerably as the forecast lead time increases this is of particular importance when multi step ahead forecasting models are to be used for operations requiring short and long term forecasts and it is not feasible to develop separate models for multiple time scales the kge and reliability for each time step for five step ahead forecasting is depicted in fig 7 for the developed models considering all basins the violin plots in fig 7 show the central 90 interval of each metric and its median solid line as expected kge was found to decrease as the forecast horizon lead time increased for lead time one the median kge is higher than 0 6 for all models except lstm dn lw the results confirm that for basin 1 coupling wavelet decomposition with the forecast models produce a smaller decrease in accuracy as forecast horizon increases for basins 2 and 3 increasing the lead time rapidly decreases forecast accuracy the reliability results are aligned with the median forecast evaluation results the forecast reliability decreases as lead time increases see fig 7 third and fourth rows the reliability of ed and ed nlw is high with low variation across lead times this finding is aligned with the results of han et al 2021 lstm dn shows inferior reliability heavier lower tails compared to the ed based models for instance it is observed that the reliability of lstm dn is highly variable for the five day ahead lead time comparing ed with ed lw ed lw exhibits higher performance when forecasting one to three days ahead whereas the ed model has superior performance when forecasting four to five days ahead for both metrics kge and reliability lstm dn lw has the poorest performance as mentioned earlier it was expected that the one to three days ahead forecasts associated with the lt 5 models i e the first three lead times would have higher error and uncertainty than the one to three days ahead forecasts obtained directly from the lt 3 models while it was found that increasing the lt from three to five generally decreased the kge for all time steps one to three days up to 20 reliability was not substantially affected results not shown for brevity in fig 8 hydrographs of observed runoff and three step ahead lt 3 forecasts associated with lstm dn are shown for all three basins for basins 2 and 3 a large gap is observed between the first and third step ahead accuracies kge the results indicate that although the forecast accuracy decreases over consecutive time steps the reliability of the forecasts remain high and close to the desired value for basins 2 and 3 the models are not able to capture the observed peaks very well this is mainly due to the large gap between the observed peaks and the 95 percentile of the runoff time series as well as the precipitation time series having higher variance in basins 2 and 3 compared to basin 1 see table 1 4 4 using precipitation forecasts as additional inputs no future data such as precipitation forecasts were used for developing the models up to this point as camels does not include meteorological forecast data the approach taken by yin et al 2021 was followed here the impact of using meteorological forecasts as inputs to the models is assessed by adding noise to z normalized precipitation observations and using this as input to the models it is noted that the adopted approach is different than using numerical weather prediction products yucel et al 2015 as input to the forecast models which is outside of the scope of this study the purpose of this section is to highlight the importance of probabilistic evaluation of the results when precipitation forecasts are fed to the model different levels of noise were added to the training and testing data with the noise level increasing linearly with respect to the time steps in the forecast horizon to mimic the use of forecasts in process based hydrological models that often use different precipitation data for model training calibration and forecasting section 3 1 includes further details on how the precipitation forecasts were generated and used in the dnns for evaluation three models lstm dn ed and ed nlw were tested for both lead times lt 3 5 with il 15 since the models using precipitation forecasts as input are considered new models bayesian optimization was used to obtain the best hyper parameters for each of the three new models once the best hyper parameters were obtained the new models were trained following the procedure outlined in section 3 5 using the lstm dn without precipitation forecasts as input as a benchmark relative changes in the kge mis and missed peaks metrics were computed for the three models the results are shown in table 4 the results confirm that if accurate precipitation forecasts are available the relative change in kge and mis metrics for basins 2 and 3 can be improved substantially for all models and the two selected lead times the relative change in kge indicates that in general lstm dn performance improves more substantially than ed and ed nlw when precipitation forecasts are used as input whereas ed shows the lowest relative change in missed peaks when precipitation forecasts are used in the models basin 3 shows the highest sensitivity to using precipitation forecasts as input where the relative change in kge for lstm dn improved by 123 797 and 108 523 for lt 3 and lt 5 respectively see table 4 interestingly for basin 1 incorporating precipitation forecasts into the model does not improve the models performance and even results in lower mis for all models this may be due to the high autocorrelation of the runoff time series in basin 1 where the autocorrelation is higher and diminishes for increasing time lags at a much lower rate compared to basins 2 and 3 it is important to note that runoff in basin 1 is mostly snowmelt driven these results are aligned with previous studies where it has been shown that lstm based models perform well in snowmelt driven catchments feng et al 2020 the precipitation forecasts have very high kge scores kge 0 95 consequently the results in table 4 are considered the potential improvements and not the real improvements i e the results are meaningful for simulation but not forecasting in real world cases it can be either very difficult or impossible to obtain multi step ahead daily precipitation forecasts with the same level of accuracy as the ones used here i e with kge 0 95 for example ponnoprat 2021 was only able to achieve daily precipitation forecasts with pearson correlation of 0 3 one step ahead 0 15 two steps ahead and 0 08 three steps ahead using a very sophisticated ed framework given the challenging nature of obtaining accurate daily precipitation forecasts the sensitivity of the models results all trained with accurate forecasts to the accuracy of the input forecasts in the testing phase is analysed and the results are shown in fig 9 see section 3 1 for details on the difference between training and testing set precipitation forecasts the results shown in fig 9 are associated with lt 3 and il 15 overall 400 different precipitation forecasts with varying average kge scores measured between observed and forecasted precipitation for the testing set were fed to the models results shown in fig 9 indicate that except for basin 1 lstm dn outperforms ed and ed nlw based on kge two important points should be understood from the results first even with inaccurate precipitation forecasts the models show higher forecast accuracy for basins 2 and 3 compared to when the precipitation forecasts are not used for basin 1 the model performance was not dependent on the precipitation forecast accuracy this is an interesting finding since it suggests that for catchments with highly autocorrelated runoff using precipitation forecasts will not necessarily improve the runoff forecasts secondly the reliability of the models decreases substantially with inaccurate precipitation forecast as input compared to the case when precipitation forecasts were not used this finding is of great importance as it is common in process based hydrological modeling to calibrate a model with meteorological observations i e perfect forecasts but use meteorological forecasts as input during the testing or operational phase see section 3 1 in such cases the results indicate that caution should be exercised and probabilistic performance measures should be considered to avoid using unreliable models for risk assessment and or decision making 4 5 attention mechanism several recent studies have shown that including an attention layer can improve the performance of lstm based models for runoff simulation alizadeh et al 2021 liu et al 2021 and forecasting ding et al 2020 girihagama et al 2022 thus in addition to the six model configurations lstm dn lstm lw lstm nlw ed ed lw ed nlw an attention layer was added to lstm dn ed and ed nlw resulting in three additional model configurations lstm att dn ed att ed att nlw the attention based models were built and evaluated in the same manner as the other six model configurations see section 3 the relative change in nrmse and mis associated with il 15 and lt 5 when forecasting runoff with the attention based models is depicted in fig 10 the results suggest that the impact of the attention layer is case dependent for example adding the attention layer to ed nlw resulted in lower nrmse and mis for basin 1 while increasing the mis for basins 2 and 3 adding the attention layer increased the nrmse for all model configurations for basin 2 except ed at lt 3 in general these results align with the findings of yin et al 2021 where using an attention mechanism coupled with ed models did not result in substantially improved forecasts however utilizing the attention mechanism to its full potential for hydrological forecasting requires further exploration e g examining different type of attention layers and internal attention mechanisms such as the one used in alizadeh et al 2021 which is considered out of scope of the present study 5 conclusion limitations and future work this study explored the development of quantile based encoder decoder ed frameworks coupled with wavelet decomposition and input variable selection ivs for multi step ahead runoff forecasting the quantile based ed frameworks presented here are straightforward approaches for probabilistic forecasting using deep neural network dnn models the primary findings of this work are as follows in general the ed nlw framework outperformed the other competing models lstm dn lstm dn lw lstm dn nlw ed ed lw while lstm dn showed higher kge and nse ed nlw and ed showed higher reliability the high reliability of the forecasts suggests that quantile based forecasting can be used as a suitable alternative to deterministic forecasting especially when the nse and kge are not high for the deterministic models models coupled with non linear wavelet selection performed superior to the models that used linear wavelet selection the nlw models showed less sensitivity to input lag length whereas no meaningful improvement was observed by increasing the input lag length the latter signifies the importance of input preprocessing and hyper parameter optimization for building dnn based hydrological forecasting models as properly selected models may results in parsimonious and more reliable dnn forecast models the results confirmed that the reliability of the ed and ed nlw forecasts unlike lstm dn forecasts reliability does not drastically decrease with the increase of the forecast lead time caution should be exercised when using precipitation forecasts as input data although models showed up to 120 improvement in accuracy when accurate precipitation forecasts were used as input to the models during both training and testing using inaccurate precipitation data in the testing phase was shown to substantially reduce the reliability of the runoff forecasts adding an attention layer to different models lstm dn ed ed nlw did not improve the forecasts consistently and the impact of the attention mechanism on forecast performance was case dependent more investigation is required for evaluating the effect of different attention mechanism types on hydrological forecasts overall the main findings of this study could be used as a benchmark for future research on state of art probabilistic runoff forecast models the present study attempted to solve some of the shortcomings and limitations of previous work related to multi step ahead runoff forecasting however this study includes several limitations that future work could address the number of selected basins was low and the results are not necessarily indicative of performance across the different hydrological units contained in the camels dataset however due to the computational burden of optimizing the model structures training 2700 models for 20 epochs only three basins were selected for model assessment which is reasonable when introducing a set of new dnn based hydrological forecasting models the dnn models explored here can easily be extended to a large sample study this study adopted quantile based forecasting while probabilistic models such as bayesian encoder decoder jin et al 2021 and wavenets borovykh et al 2017 which have shown strong performance in other fields but have not yet been investigated for runoff forecasting represent promising alternatives to assess in future work other optimization techniques such as hyperband li et al 2017 or sequential uniform designs yang and zhang 2021 could be used for hyper parameter optimization of dnn hydrological forecasting models the data driven forecasts could be compared to forecasts obtained from process based models yucel et al 2015 although this research focused on multi step ahead runoff forecasting the proposed methods could be utilized for forecasting important variables in other related domains such as meteorology precipitation environment dissolved oxygen water resources management urban water demand and agriculture evapotranspiration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors gratefully acknowledge funding from the university of waterloo via an international doctoral student award msj and a graduate student support allowance jq the ontario ministry of colleges and universities via a queen elizabeth ii graduate scholarship in science and technology jy and the natural sciences and engineering research council via discovery grant rgpin 2021 03194 jq the authors are thankful for the constructive comments of three anonymous reviewers that helped improve the paper the authors also wish to thank editor in chief andras bardossy and associate editor shailesh kumar singh for handling this manuscript authors contribution the authors contributed to this study as follows msj designed the methodology developed the deep learning models performed the analysis and wrote the initial manuscript jy carried out the wavelet analysis and implemented the linear and non linear wavelet selection and jq provided technical comments helped with evaluation design and edited the paper all authors contributed to the revision of the manuscript 
2095,deep neural network dnn models have become increasingly popular in the hydrology community however most studies are related to rainfall runoff simulation and comparatively fewer studies have focused on runoff forecasting in this study quantile based q 0 05 0 5 0 95 encoder decoder ed models that use long short term memory network lstm and dense network dn blocks were developed for three and five days ahead runoff forecasting through linear lw and non linear nlw wavelet selection hybrid models lstm dn lstm dn lw lstm dn nlw ed ed lw and ed nlw were developed for each lead time lt 3 5 and value of q different model configurations were created using different input lag lengths il 15 45 180 the developed models were tested for runoff forecasting using three basins with different characteristics from the catchment attributes and meteorology for large sample studies camels dataset the models were compared using deterministic e g the kling gupta efficiency kge metric and probabilistic e g reliability statistical metrics while the models showed high variability in performance across the three basins kge 0 308 0 979 for the q 0 5 models very high accuracy up to kge 0 979 was achieved for one of the basins with high snowmelt the ed nlw model was found to generally outperform the other models although the lstm dn model had the highest median kge 0 434 across all configurations the ed and ed nlw models had higher reliability than lstm dn 90 and 91 respectively considering a 90 confidence level models coupled with nlw performed superior to those that used lw all ed models had high reliability despite two of the basins achieving median kge values of 0 390 highlighting that quantile based models can generate reliable forecast intervals even when the kge of the median forecast q 0 5 is low an additional experiment generated synthetic precipitation forecasts with varying degrees of accuracy the models were trained using accurate precipitation forecasts and tested using both accurate and inaccurate precipitation forecasts while up to a 120 improvement in kge was found when accurate precipitation forecasts were used as input to the models using inaccurate precipitation forecasts resulted in a substantial decrease in reliability overall the results of this study can serve as a benchmark for future studies developing probabilistic dnn models for runoff forecasting keywords encoder decoder deep learning lstm wavelet decomposition runoff forecasting hydrological forecasting data availability the camels dataset used in this research can be obtained from https ral ucar edu solutions products camels 1 introduction accurate runoff forecasts play an important role in reservoir management flood risk evaluation irrigation power generation drought monitoring and many other initiatives due to its importance and the complex relationship between rainfall and runoff process based models e g samadi et al 2019 tasdighi et al 2018 knoben et al 2019 zhou et al 2019 chlumsky et al 2021 and data driven models e g li et al 2014 mehdizadeh et al 2019 kratzert et al 2018 2019 sezen et al 2019 cheng et al 2020 ni et al 2020 xie et al 2021 yin et al 2021 zhang et al 2021 have been thoroughly investigated for runoff modeling process based models leverage linear and differential equations bittelli et al 2010 partington et al 2012 zhang et al 2021 to describe the sub processes driven by physical mechanisms e g evapotranspiration interception infiltration xie et al 2021 although process based models can improve understanding of the physical processes of interest zhang et al 2015 there are two significant drawbacks associated with these models mehr et al 2013 ni et al 2020 first many simplifying assumptions are made in the modeling procedure due to the complexity of physical processes resulting in model uncertainties which are often neglected during model development and application moges et al 2020 the second drawback is that process based models are inefficient and require considerable information as model input on the other hand data driven models estimate statistical relationships between input explanatory and target response variables without considering the governing equations of the modeled system solomatine and ostfeld 2008 liu et al 2015 thus data driven models are often referred to as black box models compared to process based models data driven models are efficient and require less data during model development while process based models and data driven models have typically been developed independently there is an evolving area of research on coupling data driven models with process based models boucher et al 2020 chadalawada et al 2020 li et al 2021b lu et al 2021 sikorska senoner and quilty 2021 han and morrison 2022 alizadeh et al 2021 quilty et al 2022 although these coupled models have shown promise such approaches are outside the scope of this work and the interested reader is referred to studies devoted to this topic such as adombi et al 2021 for more details in the runoff simulation and forecasting literature data driven models can be divided into two groups the first group is statistical methods mainly comprising of linear methods such as autoregressive integrated moving average arima valipour 2015 the second group is machine learning methods ml where non linear methods such as multi layer perceptron mlp asadi et al 2013 and support vector machine svm li et al 2014 are adopted for formulating the non linear relationship between rainfall and runoff a sub set of the second group includes a new set of models based on deep learning of which deep neural networks dnns are most popular in hydrological applications recurrent neural networks rnns and especially long short term memory networks lstms both specific forms of dnns have been shown to outperform classical ml models such as mlp and svm for runoff prediction kratzert et al 2018 hu et al 2018 le et al 2019 rnns were designed to model temporal dynamics elman 1990 making them suitable for modeling non linear hydrological time series coulibaly and baldwin 2005 ni et al 2020 lstms first introduced by hochreiter and schmidhuber 1997 overcame the rnn s gradient exploding vanishing problem that occurs due to processing long sequences of data through internal non linear gates by regulating how much information is passed from one memory block to another please see section 3 for details although previous research has utilized dnns for runoff prediction there are existing shortcomings with their application that the current study seeks to address previous studies have mainly focused on runoff simulation using dnn e g kratzert et al 2018 xie et al 2021 not runoff forecasting the term forecasting is used in the ml domain when future values are predicted using information from the past and or present many articles e g lv et al 2020 alizadeh et al 2021 apaydin and sibtain 2021 have used the terms prediction and simulation interchangeably thus it is common to see the term runoff prediction used to represent runoff simulation or rainfall runoff modeling hereafter the term simulation will be used when runoff at time t q t is predicted using for instance precipitation and meteorological variables at time t the term forecast will be used when future runoff values q t l t where lt is the lead time are predicted using data at time t or further in the past ml studies that have focused on forecasting can be divided into two groups the first group uses future data such as meteorological forecasts in their forecast models while the second group utilizes only past observations in their models examples from both groups are described in the following two paragraphs in the first group of forecasting models cheng et al 2020 showed that lstm generally outperforms mlp for multi step ahead streamflow forecasting when precipitation forecasts for the following time steps were used as input data xiang et al 2020 introduced an lstm based multi step 24 h ahead runoff forecasting encoder decoder ed framework they used future rainfall observations and upstream runoff forecasts as inputs to their models yin et al 2021 introduced a novel lstm based sequence to sequence model for seven days ahead runoff forecasting where future meteorological data were utilized cui et al 2022 introduced a new lstm based ed framework for hourly flood forecasting where runoff forecasts from a hydrological model are fed to the decoder as exogenous variables recently girihagama et al 2022 evaluated attention based ed models for daily multi step ahead runoff forecasting up to five days ahead in 10 different watersheds in canada their proposed method showed the effectiveness of utilizing the attention mechanism coupled with lstm for runoff forecasting in practical engineering cases models such as the one introduced by xiang et al 2020 cannot be easily employed for forecasting as it is impossible to access future observations although meteorological forecasts can be used as input in many cases the temporal scales of the model of interest and meteorological forecasts are not the same furthermore meteorological forecasts include uncertainties due to different factors spatial interpolation model error etc which should be considered when quantifying the runoff forecasting model s uncertainty in the second group of forecasting models kao et al 2020 used an lstm based ed for multi step 6 h ahead flood forecasting they did not incorporate any future data in their model their results showed that their proposed method outperforms the feed forward neural network ed especially for longer lead times han et al 2021 compared an lstm based ed for multi step 6 h ahead runoff forecasting with svm where only past runoff data and upstream observations were used their proposed ed framework outperformed svm and a standard lstm this research explores runoff forecasting using two distinct approaches in the first approach the second group s methodology is followed where only past observations of runoff and meteorological data are used as input in the second approach to demonstrate the impact of meteorological forecast uncertainty on the accuracy and reliability of runoff forecasts the use of synthetic precipitation forecasts as a model input is explored where the developed data driven forecast models mimic process based hydrological forecasting models in contrast to process based hydrological forecasting models however air temperature and potential evapotranspiration forecasts are not included in the second approach in an ed framework the input data are encoded through an encoder model into a vector for capturing the impact of local factors afterwards the local factors are mapped to the target by decoding the encoded vector through a decoder model apaydin and sibtain 2021 the ed structure is capable of extracting temporal associations among time series cui et al 2022 therefore the ed configuration is able to better model non linear relationships between time series compared to single network configurations bian et al 2020 the latter is of great importance in hydrological forecasting due to the complex properties of hydrological processes one of the main advantages of an ed framework compared to other ml models is that there is no restriction on designing the encoder or decoder thus the encoder and decoder can be any type of neural network recurrent convolutional or dense the latter results in a multitude of choices for designing the ed components therefore ed frameworks offer a flexible approach to obtaining accurate hydrological forecasts 1 1 novelty although previous research e g apaydin and sibtain 2021 han et al 2021 has shown that lstm based ed frameworks outperform vanilla standard lstm models for runoff forecasting there exists important aspects of these frameworks that have not yet been thoroughly investigated a major drawback of the previous studies is their focus on deterministic forecasting in contrast this work develops quantile based dl models see section 3 for details for probabilistic runoff forecasting although several earlier studies have adopted quantile regression methods in the context of hydrology and water resources cannon 2011 bürger et al 2013 tyralis et al 2019 papacharalampous et al 2019 papacharalampous and langousis 2022 development of quantile based dl models for hydrological forecasting has not yet been explored in this work predictive uncertainty is modeled through conditional quantiles acharya et al 2020 instead of simply obtaining the predictive mean using a neural network prediction quantiles q are obtained although this research only focuses on estimating three quantiles 0 05 0 5 0 95 the proposed methodology can be adopted to approximate the full predictive distribution by increasing the number of estimated quantiles the three estimated quantiles are associated with the median as well as the lower and upper bound of a prediction interval which hereafter is referred to as a forecast interval one of the benefits of choosing quantile based models is that no prior assumption a source of uncertainty itself is made regarding the predictive distribution type see nevo et al 2022 unlike many previous studies see section 3 for details all forecast lead times are obtained simultaneously therefore the joint predictive distribution of the quantiles is estimated to the best of the authors knowledge no previous study has used dl models in this context other than a recent study ponnoprat 2021 which compared dl models trained using mean squared error and pinball loss functions for daily rainfall forecasting while the pinball loss function can be used for quantile based forecasting see section 3 3 the author included only a brief experiment in an appendix tabulating the performance of the two loss functions the latter for different quantiles without a detailed analysis of the results furthermore hyper parameter selection is important for dnn models goodfellow et al 2016 while grid search is commonly used for hyper parameter selection in dnn models kratzert et al 2018 the traversed hyper parameter search space is limited due to the computational burden of this approach in this work bayesian optimization alizadeh et al 2021 is adopted for efficient hyper parameter selection and model structure design wu et al 2019 bayesian optimization is a recent method for selecting the hyper parameters of data driven models in hydrological and water resources applications e g malik et al 2020 zuo et al 2020 sikorska senoner and quilty 2021 quilty et al 2022 and has seldom been used with dl based models e g alizadeh et al 2021 alibabaei et al 2021 hah et al 2022 including those used for hydrological forecasting barzegar et al 2021 bai et al 2021 lian et al 2022 given its recency bayesian optimization has not yet been used for selecting the hyper parameters of ed based models furthermore to account for multiscale change this study uses wavelet decomposition to extract important time frequency information from the hydro meteorological input variables wavelet decomposition has been shown to improve the accuracy and or reliability of hydrological and water resources forecasts quilty and adamowski 2018 2020 2021 ghaemi et al 2019 zhou et al 2020 hammad et al 2021 barzegar et al 2021 hao et al 2022 chidepudi et al 2023 jamei et al 2023 however the selection of the decomposition level and wavelet filter hereafter referred to as wavelet selection has not yet been considered for dl based hydrological forecasting hence this study proposes two different approaches for wavelet selection couples them with probabilistic dl models and evaluates which method leads to better runoff forecasts in general wavelet selection can be seen as identifying the wavelet sub series that result in the best forecasts where each unique wavelet filter and decomposition level combination results in different wavelet sub series in the next sub section the main contributions of this work are summarized 1 2 contributions and outline this research develops tests and applies a novel ed framework for multi step ahead runoff forecasting we make modifications to the framework of kao et al 2020 when building the ed models and adopt the approach taken by yin et al 2021 for creating precipitation forecasts see section 3 for details various model configurations are applied to three basins from the catchment attributes and meteorology for large sample studies camels dataset addor et al 2017 the four main contributions of this study are as follows 1 the present work is the first study to explore dnns and eds in particular for quantile based probabilistic hydrological forecasting with an application to runoff obtaining an appropriate forecast interval is of great importance in multi step ahead runoff forecasting as forecast uncertainty increases with increasing lead time this method can be beneficial for many water resources applications flood warning systems reservoir operations etc even when the median forecast is inaccurate 2 unlike previous studies e g yin et al 2021 girihagama et al 2022 kao et al 2020 bayesian optimization is used for optimizing the hyper parameters of the ed models furthermore in contrast to earlier studies e g han et al 2021 kao et al 2020 multiple model configurations are evaluated for ed 3 although this research focuses on developing models that use past observations as input the impact of using precipitation forecasts as input to the models is also evaluated 4 this research explores wavelet decomposition for improving the models accuracy and reliability two different methods namely linear lw and non linear nlw wavelet selection are adopted to select the best wavelet sub series with the goal of reducing the number of input variables while improving the accuracy and reliability of the models the rest of the paper is organized as follows the adopted data and its key descriptive statistics are explored in section 2 ed model structure design and optimization wavelet decomposition and input variable selection ivs are described in section 3 results are analyzed and discussed in section 4 the paper is concluded in section 5 2 dataset the camels dataset is a publicly accessible dataset containing 674 basins across the contiguous united states with minimal human disturbance the basins are divided into 18 hydrological units hus following the u s geological survey usgs hu map daily runoff values are obtained from the usgs national water information system covering october 1 1980 to december 31 2014 three different daily gridded forcing datasets are included in the camels dataset in this study the daymet data thornton et al 2014 is used for the same period as daily runoff 1980 2014 due to its higher spatial resolution 1 km grid compared to the other two gridded forcing datasets 12 km grid similar to yin et al 2021 daymet variables including precipitation maximum and minimum air temperature shortwave downward radiation and vapor pressure were considered when developing the models more details about the camels data can be found in newman et al 2015 and addor et al 2017 from the available 674 basins three basins associated with different hus were selected to evaluate the proposed models the three basins ids 09081600 14154500 03488000 are shown along with their hus in fig 1 for simplicity basins 09081600 14154500 03 488 000 will be referred to as basin 1 basin 2 and basin 3 respectively basin 1 belongs to hu 14 upper colorado approximately 432 k m 2 basin 2 belongs to hu 17 pacific northwest approximately 546 k m 2 and basin 3 belongs to hu 6 tennessee approximately 572 k m 2 this study aims to find the best model configuration for each basin through hyper parameter optimization therefore three basins with different properties were selected the three selected basins have different hydro climatological properties and are dissimilar based on the complexity of the rainfall runoff process within each basin for each basin the camels dataset includes the simulated discharge generated by the sacramento soil moisture accounting model coupled with the snow 17 model addor et al 2017 considering the calibration period 1980 1995 the highest nash sutcliffe efficiency nse mccuen et al 2006 belongs to the simulations associated with basin 1 nse 0 90 followed by basin 2 nse 0 82 and basin 3 nse 0 74 the past observations of meteorological data daymet forcing data and runoff were considered as base inputs to the models for forecasting runoff at three and five days ahead the data was split into three partitions namely training 75 of data validation 15 of data and testing 10 of data for each basin a similar testing period with approximately three years of samples 1 2 2012 12 31 2014 1095 samples was used for all models to make the comparison of the models forecasts meaningful the training set was used to train the models optimize the network weights and biases while the models hyper parameters and wavelet decomposed sub series were selected using the validation set see section 3 the results provided in section 4 are associated with the testing set which was not used to train or validate the models in order to evaluate out of sample performance the relationship between rainfall and runoff in the testing set is depicted at the bottom of fig 1 descriptive statistics of the base inputs i e without wavelet decomposition for each basin and each partition training validation and testing are shown in table 1 3 methods this section explores the methods adopted for building the forecast models an overview of the general process is given in the sub section below 3 1 overview two different types of models based on structure design were considered in the first structure type a lstm block containing lstm cells see section 3 2 is used for feature extraction and is connected to a dense network dn this structure is called lstm dn in the second model type an ed model is designed where the encoder is a lstm and the decoder is a lstm dn see section 3 2 the latter configuration is referred to as ed in the rest of the paper the two model types are coupled with three input scenarios resulting in six different model configurations in the first input scenario only the previous meteorological data and runoff observations are used as input lstm dn and ed in the second and third input scenarios the same inputs are considered as the first scenario along with their corresponding wavelet decomposed sub series selected through either linear lstm dn lw and ed lw or non linear lstm dn nlw and ed nlw wavelet selection respectively see section 3 4 the edgeworth approximation based conditional mutual information ea ivs method was combined with nlw to build parsimonious models and improve the forecasts as it has been shown in earlier studies to be an effective non linear ivs method when considering wavelet decomposed inputs in data driven models quilty and adamowski 2018 2020 2021 all inputs were individually z normalized by subtracting the mean and dividing by the standard deviation before being fed to the models the mean and standard deviation were obtained based on the training set to prohibit future data leakage into the training phase previous studies have adopted z normalization to improving the training of lstm based models e g yin et al 2021 the models hyper parameters lstm size number of layers in the dn dn layers size and activation functions and dropout rate were obtained by maximizing the validation set nse using bayesian optimization see section 3 5 the overall workflow is shown in fig 2 two different optimization routines were used for different purposes when optimizing the models the first routine used a modified version of stochastic gradient descent see section 3 3 for training all forecast models estimating the networks weights and biases through minimizing the pinball loss function while bayesian optimization was used for selecting the best hyper parameters in the beginning stage zero optimal hyper parameters were obtained for three different input lags il 15 45 180 and two lead times lt 3 5 for all three basins requiring 2700 models to be trained see section 3 5 for details all six model types were developed and optimized for multi step multivariate forecasting i e forecasting all the lead times simultaneously see the bottom of fig 2 in stage one for every basin basin 1 basin 2 basin 3 three models based on each quantile q 0 05 0 5 0 95 were trained using the best configuration il and lt from stage zero in total 324 six configurations three quantiles two lead times one input lag and three basins models were trained and employed for forecasting in stage one without precipitation forecasts different statistical metrics were used for analyzing the accuracy and reliability of the results see section 3 6 the developed models from stage one did not use any future data for forecasting making them ideal for locations where meteorological forecasts are not available however the developed models were further modified to accommodate cases where meteorological forecasts are available therefore in stage two the impact of using precipitation forecasts as input to the models was evaluated for the top three performing models associated with il 15 thus in stage two an additional 54 models three configurations three quantiles two lead times one input lag and three basins 1 were trained see section 4 as the camels dataset does not include meteorological forecasts following yin et al 2021 synthetic precipitation forecasts were generated by adding noise drawn from a normal distribution to the precipitation data before training the models in stage two noise from a normal distribution with zero mean and a standard deviation ranging from 0 1 for lead time one to 0 14 for lead time five with step increments of 0 01 was added to the training set precipitation data at lead times one through five respectively this resulted in precipitation forecasts with very high accuracy kge 0 95 in the testing phase the same approach was used but with different base standard deviations ranging from 0 1 to 1 with increments ranging from 0 01 to 0 1 following this approach the sensitivity of the models to precipitation forecast accuracy was assessed this experiment was meant to mimic how process based hydrological models are used for forecasting typically process based hydrological models use data from rain gauges or gridded reanalysis products for model training calibration but afterwards use precipitation forecasts from different products when running the hydrological model in forecasting mode boucher et al 2012 the precipitation forecasts are added to the dnn models through a separate dn with one layer and then concatenated to the lstm states in the next two sub sections an overview is given on lstm including the attention mechanism and ed models respectively 3 2 long short term memory lstm and attention layer this section follows a slightly modified version of the notation used by kratzert et al 2019 the lstm block is shown in fig 3 the internal structure of lstm enables the network to process and memorize data sequences where the latter is done by each cell s gates as shown in fig 3 for a specific lag time t t 1 il and input series x t 2 dimensional number of samples number of features an lstm can be formulated as kratzert et al 2019 1 i t s i g m o i d w i x t u i h t 1 b i 2 f t s i g m o i d w f x t u f h t 1 b f 3 g t t a n h w g x t u g h t 1 b g 4 o t s i g m o i d w o x t u o h t 1 b o 5 c t f t c t 1 i t g t 6 h t o t tanh c t where w and u are weight matrices b is the bias vector the operator is element wise multiplication while i t f t and o t are the input forget and output gates at step t respectively whereas g t h t c t are the cell input recurrent state and cell state at step t respectively and i f g and o represent indices associated with the weights and biases of their respective gates or states the sigmoid function is defined as s i g m o i d x 1 1 e x while the hyperbolic tangent function is defined as t a n h x e 2 x 1 e 2 x 1 the output gate o t controls which saved information is outputted and a loop is used for feeding the previous cell states c t 1 and recurrent states h t 1 into the network in the lstm dn configuration the output from the last iteration h t 1 is fed to a dn with 1 3 layers the forecast values were obtained by placing a linear layer on top of the dn with the number of nodes equal to the lead time 3 or 5 therefore all forecast steps were obtained simultaneously also known as many to many networks in addition to the standard lstm lstm with a dot product attention layer luong et al 2015 was also evaluated in this study the custom self attention layer measures the alignment score of the states h giving more weight to previous lags than a standard lstm instead of using the last state values h t 1 on its own all state matrices are utilized in this setup 3 3 encoder decoder framework in this study a novel ed framework is introduced by modifying the structure used by kao et al 2020 the main differences include using a dn to process the lstm block s output states and training the models with a quantile based loss function employing a dn as the final block of the forecast model makes the model more robust to error summation in direct sequence to sequence models such as the ones adopted by kao et al 2020 and xiang et al 2020 the recurrent states of the final lstm block are reported as the forecasted values where each step forecast has a direct impact on the successive step in this work first the meteorological data including their wavelet decomposed sub series are fed to an lstm the first lstm acts as an encoder where it processes the input sequence and outputs the recurrent h and cell c states see fig 2 3 and eqs 5 6 the state vectors are used as initial states h 0 c 0 for the second lstm block the decoder where the past observations of runoff including their wavelet decomposed sub series are directly fed to the second lstm block similar to lstm dn the final recurrent state is passed to a single layer dn followed by a linear output layer in the proposed model the meteorological data and runoff are processed separately which is beneficial in cases where different ils are used for different input variables instead of using the mean square error mse as the objective function for training the models the pinball loss function is adopted wang et al 2019 7 l o s s q t 1 q q t q q t q t q q t q q t q t q q t q q t where q is the quantile of interest q 0 05 0 5 0 95 q t is the observed runoff at step t and q t q denotes the q th quantile of the forecasted runoff at step t output of the forecast model see fig 2 all models lstm dn ed were developed using custom scripts and available libraries in python 3 7 including the tensorflow 2 4 1 library abadi et al 2016 with keras api chollet 2015 models were trained using the adaptive moment estimation adam optimization method kingma and ba 2014 a dynamic learning rate was used for all models with an initial value of 1e 4 and a decreasing factor of 0 5 the decreasing factor was applied every three epochs when no improvement was observed on the validation set the training procedure was terminated when the validation set accuracy did not increase for ten consecutive epochs 3 4 wavelet decomposition this paper follows the best practices for wavelet based forecasting proposed by quilty and adamowski 2018 when decomposing the meteorological data and previous runoff observations the maximal overlap discrete wavelet transform modwt was adopted to avoid the future data problem and boundary affected indices samples were removed from the dataset for brevity the interested reader is directed to walden 2001 and quilty and adamowski 2018 for details regarding modwt and its implementation each input was decomposed into wavelet and scaling coefficients according to a given wavelet filter and decomposition level see quilty and adamowski 2018 the selection of the wavelet filters was done by grid search through fitting multivariate linear regression mlr lw method and svm nlw method for each basin and lead time lt 3 5 in total 19 wavelet filters from seven families covering filter lengths 2 14 haar daubechies d2 d7 symlets sym4 sym6 fejer korovkin fk4 fk6 fk8 fk14 coiflets coif1 coif2 least asymmetric la8 la12 la14 and best localized bl14 and decomposition levels 1 6 were considered details regarding the wavelet filters can be found in crowley 2007 nielsen 2001 olhede and walden 2004 percival and walden 2000 rathinasamy et al 2013 and zhang et al 2016 overall 114 combinations six decomposition levels and 19 wavelet filters were evaluated using mlr and svm for each basin and lead time resulting in 1368 114 wavelet combinations two wavelet selection methods three basins two lead times models using the validation set the wavelet filter decomposition level combination resulting in the highest nse was selected and used for decomposing the input data for use in the dnn models lstm dn ed lstm lw ed lw lstm nlw ed nlw for lw the scaling coefficients associated with the last decomposition level and the wavelet coefficients from all decomposition levels collectively referred to as wavelet decomposed sub series were used together with the original input data in the dnn models however for nlw the ea method with a tolerance value equal to 0 05 was used to reduce the number of input variables used in the lw method resulting in dnn models that were more parsimonious the ea ivs method was selected since it can capture non linear relationships amongst inputs that may be missed by linear ivs methods details regarding the implementation of the ea method can be found in jahangir et al 2021 wavelet decomposition along with the ea method were implemented using custom scripts in r team 2013 while the mlr3 package lang et al 2019 was adopted for fitting the svm models with a radial basis function kernel the default package settings were used for fitting the svm models 3 5 bayesian optimization bayesian optimization derived from bayes theorem is an effective method for solving computationally demanding optimization problems that do not have closed form solutions brochu et al 2010 thus bayesian optimization can be leveraged for optimizing the structure of dnns through hyper parameter selection the objective function i e the pinball loss function is approximated by a gaussian process seeger 2004 by considering an acquisition function where high values of the acquisition function are associated with high values of the objective function the maximum value of the objective function is obtained through an iterative process of maximizing the acquisition function at each step of bayesian optimization multiple dn layers were considered in the dnn models developed in this study accordingly when applying bayesian optimization to the dnn models the dn layers properties i e architecture were obtained dynamically i e the size and activation type of the dn layers were not required to be similar details regarding bayesian optimization for ml hyper parameter selection may be found in wu et al 2019 in this study the hyper parameters that required optimization and their ranges include lstm hidden state size 16 256 the number of layers in the dn 1 3 the number of nodes in each layer 8 128 the dn layers dropout rate 0 0 3 and dn layers activation function type relu rectified linear unit sigmoid and tanh the hyper parameters were optimized based on q 0 5 i e for a specific model type similar hyper parameters were used for all quantiles the hyper parameters were optimized via bayesian optimization using 25 iterations for each iteration the dnn was trained for 20 epochs overall there were 2700 six configurations one quantile two lead times three input lags three basins and 25 iterations models developed via bayesian optimization the optimal hyper parameters for each configuration according to the validation set nse were selected and used to train a new dnn of the same configuration for 150 epochs resulting in 324 models see section 3 1 bayesian optimization was implemented via custom scripts and the scikit optimize 0 8 1 library head et al 2020 in python 3 7 3 6 forecast evaluation statistical metrics commonly adopted in hydrology were used to analyze the forecasts including the nse kling gupta efficiency kge gupta et al 2009 normalized root mean square nrmse mean absolute error mae reliability smith et al 2015 and the mean interval score mis gneiting and raftery 2007 the reliability metric measures the coverage interval of forecasts i e the percentage of observations positioned within the designated forecast interval on the other hand mis gives the weighted average width of forecast intervals based on designated quantiles penalizing instances where the observation lies outside the forecast interval mis is defined as gneiting and raftery 2007 15 m i s 1 n t 1 n q t 0 95 q t 0 05 2 0 9 q t 0 05 q t 2 0 9 q t q t 0 95 where q t q is the quantile forecast at time step t obtained through the forecast model n is the total number of samples and is the heaviside function since q 0 05 and q 0 95 were used as upper and lower quantiles the models desired reliability confidence level is 90 with low mis values being preferred li et al 2021a the percentage of missed peaks was also used to evaluate the forecasts and is defined as the percentage of observed values above the q 0 95 models forecasts to complement the missed peaks metric the percentage of negative forecasts associated with the q 0 05 models along with the average magnitude of negative forecasts were also reported the deterministic metrics nse kge nrmse and mae are associated with the median q 0 5 model and are used to assess forecast accuracy all the results are associated with the testing set a rolling window with a length equal to lt was used to produce the forecasts therefore similar to how the models were optimized non overlapping forecasts were used when comparing the performance of different models see fig 2 4 results and discussion six dnn model configurations lstm dn lstm lw lstm nlw ed ed lw ed nlw were used for forecasting runoff at two different lead times lt 3 5 the models are compared based on both il and lt for model comparison each point in the testing set is evaluated once a rolling window equal to lt as the models produce forecasts for all steps simultaneously sequential forecasting the forecast reliability and kge of each step 1 to lt is also reported individually to demonstrate how the performance changes for intermediate steps in an lt step forecast 4 1 comparing forecast performance across model configurations the models performance for all model configurations associated with lt 3 and il 15 shortest lead time and input lag length is shown as an example in table 2 as expected based on the runoff simulation accuracy see section 2 the nse and kge of the models for basin 1 are higher than the two other basins also for basins 2 and 3 as precipitation forecasts are not used as input at this stage a large gap is observed between simulation accuracy see section 2 and the obtained forecast accuracy the models for basin 3 have lower accuracy compared to the other two basins the highest nse values for basins 1 2 and 3 are associated with ed lw 0 968 ed 0 391 and ed 0 310 respectively the best kge for basin 1 is associated with lstm dn 0 959 while for basins 2 and 3 the ed model 0 475 and 0 407 respectively has higher kge values the nse for basins 2 and 3 indicates that 0 5 quantile median models resulted in inaccurate forecasts moriasi et al 2007 although the median forecasts have low accuracy for basins 2 and 3 see nse and kge values in table 2 the models forecast reliability is close to the desired 90 the results indicate that the models trained to forecast the upper q 0 95 quantile generally captured the peaks with missed peaks lower than 7 for all developed models while expecting 5 based on the designated upper quantile for basins 1 and 2 the best model configuration according to the missed peaks is lstm dn nlw while for basin 3 ed lw resulted in the lowest missed peaks value evaluating the lower q 0 05 quantile models indicates that models associated with basin 2 especially lstm dn lw produce high rates of negative values the results show that in general models utilizing nlw obtain a lower mis compared to models using lw the results in table 2 pertain to lt 3 and il 15 for lt 5 the ed model resulted in the highest reliability ideally equal to 90 for basins 1 and 2 when using il 45 reliability 90 046 kge 0 928 and il 15 reliability 89 954 kge 0 418 respectively lstm dn nlw with il 15 resulted in the most reliable forecasts for basin 3 reliability 89 770 kge 0 384 while it is common to use very high il e g 365 days for runoff simulation gauch et al 2021 kratzert et al 2021 wang and karimi 2022 the results highlight that improved performance can be obtained through model structure optimization as opposed to solely increasing the il of the models which based on the recurrent structure of lstms increases computation time 4 1 1 selecting the best model configuration for a more thorough comparison of the models the performance of the models for all 18 configurations all basins three input lags three and lead times two are assessed the results are illustrated in fig 4 the dashed lines show the desired value for both metrics for each metric the boxplots whiskers are associated with the central 90 of the data and the solid line shows the median value based on kge lstm dn outperforms the other models while the poorest performance is related to lstm dn lw all three ed based models show similar performance while ed lw has slightly better performance for basins 2 and 3 according to the forecasts reliability the three model types ed ed lw and ed nlw generally show superior performance compared to the other models for the desired confidence level 90 ed lw generally shows the best performance among the models however its counterpart lstm dn lw has the lowest reliability the results show that the ed framework outperforms the other models based on the missed peaks metric 5 based on the defined forecast interval and has the fewest negative values based on fig 4 two points should be highlighted first lstm dn lw has inferior performance compared to the other models second coupling wavelet decomposition with ed models e g ed lw results in more accurate forecasts compared to when wavelet decomposition is coupled with lstm models e g lstm dn nlw the latter result is clear in fig 4b where lstm dn lw and lstm dn nlw perform poorly relative to the other models based on the results reported in table 2 and fig 4 the dnn models show dissimilar performance for different il and lt since the best hyper parameters were obtained for each configuration through bayesian optimization the comparison of the models among basins is meaningful thus to enable a more general comparison of the different model configurations their average rank according to different statistical metrics is presented in table 3 the model with the lowest rank corresponds to the best performing model configuration for a given metric bolded font except for the kge lstm dn and missed peaks ed lw metrics the model with the lowest best rank is ed nlw the results confirm that lstm dn outperforms other models according to kge while the ed has the lowest missed peaks values furthermore kge and the percentage of negative values are improved by coupling wavelet decomposition with ed models comparing results in table 3 and fig 4 indicates that while coupling lw and nlw with the ed structure is beneficial in general nlw is superior 4 2 input lags length impact evaluation one of the hyper parameters controlling the performance of the forecast models is il due to the recurrent structure of lstm networks il plays a significant role in the training time of lstm based models in the literature the il is commonly set to high values il 270 365 in lstm networks to obtain more accurate models feng et al 2020 kratzert et al 2019 however the above results confirm that model structure optimization model design and hyper parameter selection and input data preprocessing wavelet decomposition can be combined with lower il values to achieve similar or higher accuracy than models with high il values 4 2 1 model comparison for lt 5 the percentage of times each model is chosen as the best model based on the reliability and mae metrics for each of the 219 forecast instances see fig 2 is shown in fig 5 if any number of models had the best performance e g lowest mae at each forecast instance each model was selected as the best model therefore the selection percentage in fig 5 does not necessarily add to 100 across all models and il values for a given basin and performance metric reliability and mae the results confirm that a single model and specific il does not dominate the other models based on the reliability metric and for basins 1 and 2 lstm dn nlw with il 45 and 15 is selected more frequently than the others for basin 3 the ed and lstm dn models with il 15 have the highest selection rates based on mae the models with highest selection rate in basins 1 2 and 3 are associated with ed il 45 ed lw il 180 and ed lw il 15 respectively it is important to note that the results shown in fig 5 are associated with the models which on average are more frequently selected across forecast instances and is not equal to the reliability or mae computed across the testing set in other words models with a higher selection rate are not necessarily the most accurate or reliable models for example the results indicate see section 4 1 that the most reliable forecast model associated with lt 5 for basins 1 and 2 is the ed model and not the lstm dn nlw model which has a higher selection rate see fig 5 thus even though the different models show similar reliability on average across forecast instances the models show a high level of variability when considering the model accuracy i e associated with the q 0 5 models 4 2 2 the impact of input lags on a single model the lstm dn ed and ed nlw models were identified as the best performing models based on table 3 and fig 5 and were selected for further analysis in prior research il has been determined either through trial and error xiang et al 2020 or set to a large fixed value kratzert et al 2019 for lstm models high il values result in longer training times thus when computing resources are limited lower il values are preferred in fig 6 the kge associated with lstm dn ed and ed nlw for different combinations of input lag lengths are compared across all basins and lead times leading to six different scenarios in each sub plot if a point falls below the bisector line it indicates that the model built with the il value in the arrow pointing to the x axis resulted in higher kge than the il value in the arrow pointing to the y axis and vice versa if the point falls above the bisector for example in fig 6 lstm dn il 15 had higher kge than il 45 for most scenarios the top left sub plot in general for lstm dn il 15 resulted in the best performance for the same model the kge increased when il was increased from 45 to 180 for most scenarios the lstm dn performed best when using il 15 for the ed model see row 2 in fig 6 the best performance was obtained using il 45 the results confirm that adopting wavelet decomposition nlw along with ivs is beneficial as models with lower il have accuracy similar to models with higher il see row 3 in fig 6 although wavelet decomposition increases the lstm network size since more inputs are considered and bayesian optimization can be computationally demanding when combining both approaches low il values tend to be selected since these two approaches affect model training more than model testing such an approach may be preferred to the commonly adopted practice of selecting high il values which is computationally demanding for both training and executing lstm networks in real time 4 3 lead time assessment the current study explores multi step ahead i e multivariate forecasting where forecasts at multiple lead times are obtained simultaneously in particular for lt 3 one to three day s ahead forecasts are obtained and for lt 5 one to five day s ahead forecasts are obtained although the lt 5 forecasts contain the same lead times as lt 3 for the first three time steps the former requires the forecast models to be optimized using five outputs instead of three and this leads to an increase in the error and uncertainty of this models forecasts in other words it is expected that there will be a higher level of error and uncertainty in the one to three days ahead forecasts from the lt 5 models than the lt 3 models hence it is important to develop models whose forecast reliability does not diminish considerably as the forecast lead time increases this is of particular importance when multi step ahead forecasting models are to be used for operations requiring short and long term forecasts and it is not feasible to develop separate models for multiple time scales the kge and reliability for each time step for five step ahead forecasting is depicted in fig 7 for the developed models considering all basins the violin plots in fig 7 show the central 90 interval of each metric and its median solid line as expected kge was found to decrease as the forecast horizon lead time increased for lead time one the median kge is higher than 0 6 for all models except lstm dn lw the results confirm that for basin 1 coupling wavelet decomposition with the forecast models produce a smaller decrease in accuracy as forecast horizon increases for basins 2 and 3 increasing the lead time rapidly decreases forecast accuracy the reliability results are aligned with the median forecast evaluation results the forecast reliability decreases as lead time increases see fig 7 third and fourth rows the reliability of ed and ed nlw is high with low variation across lead times this finding is aligned with the results of han et al 2021 lstm dn shows inferior reliability heavier lower tails compared to the ed based models for instance it is observed that the reliability of lstm dn is highly variable for the five day ahead lead time comparing ed with ed lw ed lw exhibits higher performance when forecasting one to three days ahead whereas the ed model has superior performance when forecasting four to five days ahead for both metrics kge and reliability lstm dn lw has the poorest performance as mentioned earlier it was expected that the one to three days ahead forecasts associated with the lt 5 models i e the first three lead times would have higher error and uncertainty than the one to three days ahead forecasts obtained directly from the lt 3 models while it was found that increasing the lt from three to five generally decreased the kge for all time steps one to three days up to 20 reliability was not substantially affected results not shown for brevity in fig 8 hydrographs of observed runoff and three step ahead lt 3 forecasts associated with lstm dn are shown for all three basins for basins 2 and 3 a large gap is observed between the first and third step ahead accuracies kge the results indicate that although the forecast accuracy decreases over consecutive time steps the reliability of the forecasts remain high and close to the desired value for basins 2 and 3 the models are not able to capture the observed peaks very well this is mainly due to the large gap between the observed peaks and the 95 percentile of the runoff time series as well as the precipitation time series having higher variance in basins 2 and 3 compared to basin 1 see table 1 4 4 using precipitation forecasts as additional inputs no future data such as precipitation forecasts were used for developing the models up to this point as camels does not include meteorological forecast data the approach taken by yin et al 2021 was followed here the impact of using meteorological forecasts as inputs to the models is assessed by adding noise to z normalized precipitation observations and using this as input to the models it is noted that the adopted approach is different than using numerical weather prediction products yucel et al 2015 as input to the forecast models which is outside of the scope of this study the purpose of this section is to highlight the importance of probabilistic evaluation of the results when precipitation forecasts are fed to the model different levels of noise were added to the training and testing data with the noise level increasing linearly with respect to the time steps in the forecast horizon to mimic the use of forecasts in process based hydrological models that often use different precipitation data for model training calibration and forecasting section 3 1 includes further details on how the precipitation forecasts were generated and used in the dnns for evaluation three models lstm dn ed and ed nlw were tested for both lead times lt 3 5 with il 15 since the models using precipitation forecasts as input are considered new models bayesian optimization was used to obtain the best hyper parameters for each of the three new models once the best hyper parameters were obtained the new models were trained following the procedure outlined in section 3 5 using the lstm dn without precipitation forecasts as input as a benchmark relative changes in the kge mis and missed peaks metrics were computed for the three models the results are shown in table 4 the results confirm that if accurate precipitation forecasts are available the relative change in kge and mis metrics for basins 2 and 3 can be improved substantially for all models and the two selected lead times the relative change in kge indicates that in general lstm dn performance improves more substantially than ed and ed nlw when precipitation forecasts are used as input whereas ed shows the lowest relative change in missed peaks when precipitation forecasts are used in the models basin 3 shows the highest sensitivity to using precipitation forecasts as input where the relative change in kge for lstm dn improved by 123 797 and 108 523 for lt 3 and lt 5 respectively see table 4 interestingly for basin 1 incorporating precipitation forecasts into the model does not improve the models performance and even results in lower mis for all models this may be due to the high autocorrelation of the runoff time series in basin 1 where the autocorrelation is higher and diminishes for increasing time lags at a much lower rate compared to basins 2 and 3 it is important to note that runoff in basin 1 is mostly snowmelt driven these results are aligned with previous studies where it has been shown that lstm based models perform well in snowmelt driven catchments feng et al 2020 the precipitation forecasts have very high kge scores kge 0 95 consequently the results in table 4 are considered the potential improvements and not the real improvements i e the results are meaningful for simulation but not forecasting in real world cases it can be either very difficult or impossible to obtain multi step ahead daily precipitation forecasts with the same level of accuracy as the ones used here i e with kge 0 95 for example ponnoprat 2021 was only able to achieve daily precipitation forecasts with pearson correlation of 0 3 one step ahead 0 15 two steps ahead and 0 08 three steps ahead using a very sophisticated ed framework given the challenging nature of obtaining accurate daily precipitation forecasts the sensitivity of the models results all trained with accurate forecasts to the accuracy of the input forecasts in the testing phase is analysed and the results are shown in fig 9 see section 3 1 for details on the difference between training and testing set precipitation forecasts the results shown in fig 9 are associated with lt 3 and il 15 overall 400 different precipitation forecasts with varying average kge scores measured between observed and forecasted precipitation for the testing set were fed to the models results shown in fig 9 indicate that except for basin 1 lstm dn outperforms ed and ed nlw based on kge two important points should be understood from the results first even with inaccurate precipitation forecasts the models show higher forecast accuracy for basins 2 and 3 compared to when the precipitation forecasts are not used for basin 1 the model performance was not dependent on the precipitation forecast accuracy this is an interesting finding since it suggests that for catchments with highly autocorrelated runoff using precipitation forecasts will not necessarily improve the runoff forecasts secondly the reliability of the models decreases substantially with inaccurate precipitation forecast as input compared to the case when precipitation forecasts were not used this finding is of great importance as it is common in process based hydrological modeling to calibrate a model with meteorological observations i e perfect forecasts but use meteorological forecasts as input during the testing or operational phase see section 3 1 in such cases the results indicate that caution should be exercised and probabilistic performance measures should be considered to avoid using unreliable models for risk assessment and or decision making 4 5 attention mechanism several recent studies have shown that including an attention layer can improve the performance of lstm based models for runoff simulation alizadeh et al 2021 liu et al 2021 and forecasting ding et al 2020 girihagama et al 2022 thus in addition to the six model configurations lstm dn lstm lw lstm nlw ed ed lw ed nlw an attention layer was added to lstm dn ed and ed nlw resulting in three additional model configurations lstm att dn ed att ed att nlw the attention based models were built and evaluated in the same manner as the other six model configurations see section 3 the relative change in nrmse and mis associated with il 15 and lt 5 when forecasting runoff with the attention based models is depicted in fig 10 the results suggest that the impact of the attention layer is case dependent for example adding the attention layer to ed nlw resulted in lower nrmse and mis for basin 1 while increasing the mis for basins 2 and 3 adding the attention layer increased the nrmse for all model configurations for basin 2 except ed at lt 3 in general these results align with the findings of yin et al 2021 where using an attention mechanism coupled with ed models did not result in substantially improved forecasts however utilizing the attention mechanism to its full potential for hydrological forecasting requires further exploration e g examining different type of attention layers and internal attention mechanisms such as the one used in alizadeh et al 2021 which is considered out of scope of the present study 5 conclusion limitations and future work this study explored the development of quantile based encoder decoder ed frameworks coupled with wavelet decomposition and input variable selection ivs for multi step ahead runoff forecasting the quantile based ed frameworks presented here are straightforward approaches for probabilistic forecasting using deep neural network dnn models the primary findings of this work are as follows in general the ed nlw framework outperformed the other competing models lstm dn lstm dn lw lstm dn nlw ed ed lw while lstm dn showed higher kge and nse ed nlw and ed showed higher reliability the high reliability of the forecasts suggests that quantile based forecasting can be used as a suitable alternative to deterministic forecasting especially when the nse and kge are not high for the deterministic models models coupled with non linear wavelet selection performed superior to the models that used linear wavelet selection the nlw models showed less sensitivity to input lag length whereas no meaningful improvement was observed by increasing the input lag length the latter signifies the importance of input preprocessing and hyper parameter optimization for building dnn based hydrological forecasting models as properly selected models may results in parsimonious and more reliable dnn forecast models the results confirmed that the reliability of the ed and ed nlw forecasts unlike lstm dn forecasts reliability does not drastically decrease with the increase of the forecast lead time caution should be exercised when using precipitation forecasts as input data although models showed up to 120 improvement in accuracy when accurate precipitation forecasts were used as input to the models during both training and testing using inaccurate precipitation data in the testing phase was shown to substantially reduce the reliability of the runoff forecasts adding an attention layer to different models lstm dn ed ed nlw did not improve the forecasts consistently and the impact of the attention mechanism on forecast performance was case dependent more investigation is required for evaluating the effect of different attention mechanism types on hydrological forecasts overall the main findings of this study could be used as a benchmark for future research on state of art probabilistic runoff forecast models the present study attempted to solve some of the shortcomings and limitations of previous work related to multi step ahead runoff forecasting however this study includes several limitations that future work could address the number of selected basins was low and the results are not necessarily indicative of performance across the different hydrological units contained in the camels dataset however due to the computational burden of optimizing the model structures training 2700 models for 20 epochs only three basins were selected for model assessment which is reasonable when introducing a set of new dnn based hydrological forecasting models the dnn models explored here can easily be extended to a large sample study this study adopted quantile based forecasting while probabilistic models such as bayesian encoder decoder jin et al 2021 and wavenets borovykh et al 2017 which have shown strong performance in other fields but have not yet been investigated for runoff forecasting represent promising alternatives to assess in future work other optimization techniques such as hyperband li et al 2017 or sequential uniform designs yang and zhang 2021 could be used for hyper parameter optimization of dnn hydrological forecasting models the data driven forecasts could be compared to forecasts obtained from process based models yucel et al 2015 although this research focused on multi step ahead runoff forecasting the proposed methods could be utilized for forecasting important variables in other related domains such as meteorology precipitation environment dissolved oxygen water resources management urban water demand and agriculture evapotranspiration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors gratefully acknowledge funding from the university of waterloo via an international doctoral student award msj and a graduate student support allowance jq the ontario ministry of colleges and universities via a queen elizabeth ii graduate scholarship in science and technology jy and the natural sciences and engineering research council via discovery grant rgpin 2021 03194 jq the authors are thankful for the constructive comments of three anonymous reviewers that helped improve the paper the authors also wish to thank editor in chief andras bardossy and associate editor shailesh kumar singh for handling this manuscript authors contribution the authors contributed to this study as follows msj designed the methodology developed the deep learning models performed the analysis and wrote the initial manuscript jy carried out the wavelet analysis and implemented the linear and non linear wavelet selection and jq provided technical comments helped with evaluation design and edited the paper all authors contributed to the revision of the manuscript 
2096,lagrangian based transport model 2 2 global sensitivity analysis methods 2 2 1 morris method 2 2 2 variance based method 3 study examples 3 1 borden site 3 2 qqhe site 4 results and discussion 4 1 sorptive and non reactive solute dispersion 4 2 screening sensitive parameters for sorptive solute 4 3 sensitivity analysis results of α 11 s and α 11 nr 4 4 effect of heterogeneity scale 5 conclusions credit authorship contribution statement appendix a supplementary data allenking 1998 385 396 r bayer 2015 1 10 p bellin 1993 4019 4030 a bianchi 2011 m bosma 1993 4031 4043 w brusseau 1997 115 155 m burhenne 2011 s burr 1994 791 815 d campolongo 2007 1509 1518 f cirpka 2011 o connolly 2016 29 50 m dagan 1984 151 177 g dagan 1989 g flowtransportinporousformations dagan 2013 67 85 g dai 2017 4327 4343 h dai 2004 68 86 z dai 2005 z dai 2007 z dai 2015 286 300 h dai 2020 124516 z debarros 2012 f deng 2013 248 257 h dentz 2011 1 17 m desbarats 1990 153 163 a devlin 2015 837 844 j dige 2018 431 454 n edery 2014 1490 1505 y frippiat 2008 150 176 c he 2014 3147 3169 x helton 2003 23 69 j herman 2013 2893 2903 j homma 1996 1 17 t huang 2008 689 704 c iooss 2015 101 122 b janssen 2013 123 132 h liefvendahl 2006 3231 3247 m mackay 1986 2017 2029 d maghrebi 2013 8600 8604 m mckay 2000 55 61 m morris 1991 161 174 m nabi 2021 60900 60912 s oladyshkin 2012 10 22 s pan 2011 238 249 f pauloo 2021 e2020wr028655 r puyguiraud 2020 103782 a ramanathan 2008 r ramanathan 2010 r refsgaard 2012 36 50 j ren 2022 e2021wr031886 w renard 2013 168 196 p renardy 2021 108593 m ritzi 2007 r ritzi 2013 1901 1913 r ritzi 2015 31 39 r riva 2010 955 970 m roberts 1986 2047 2058 p robin 1991 2619 2632 m rubin 2003 y appliedstochastichydrogeology 2006 hydrogeophysics saltelli 2008 a globalsensitivityanalysisprimer saltelli 2010 259 270 a sanchezvila 2006 x sarrazin 2016 135 152 f scheibe 2015 38 56 t sheikholeslami 2019 282 299 r sivakumar 2005 211 218 b sobol 1993 407 414 i soltanian 2015 709 726 m soltanian 2015 235 244 m soltanian 2015 1586 1600 m soltanian 2015 1601 1618 m soltanian 2017 379 386 m soltanian 2020 125025 m song 2015 739 757 x sudret 2008 964 979 b sun 2008 a tarantola 2012 1061 1072 s vangriensven 2006 10 23 vanwerkhoven 2008 k wainwright 2014 84 94 h weissmann 1999 1761 1770 g white 2000 389 419 c willmann 2008 m wu 2004 j yin 2020 124515 m zhan 2022 e2021gl095823 c zhang 2013 78 99 y zhang 2022 127550 x zhang 2021 117603 x zhang 2022 128287 x zhang 2019 1011 1030 y renx2023x129274 renx2023x129274xw 2025 02 22t00 00 00 000z 2025 02 22t00 00 00 000z http creativecommons org licenses by nc nd 4 0 2023 elsevier b v all rights reserved 2023 02 28t06 57 50 861z http vtw elsevier com data voc addontypes 50 7 aggregated refined https doi org 10 15223 policy 017 https doi org 10 15223 policy 037 https doi org 10 15223 policy 012 https doi org 10 15223 policy 029 https doi org 10 15223 policy 004 item s0022 1694 23 00216 0 s0022169423002160 1 s2 0 s0022169423002160 10 1016 j jhydrol 2023 129274 271842 2023 05 29t12 20 35 766253z 2023 04 01 2023 04 30 1 s2 0 s0022169423002160 main pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 main application pdf 4b2deafabab513265c95ea36a1a8735b main pdf main pdf pdf true 6205112 main 14 1 s2 0 s0022169423002160 main 1 png https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 preview image png df22512590c16bb1b1c251fdbdb7da0f main 1 png main 1 png png 58192 849 656 image web pdf 1 1 s2 0 s0022169423002160 gr7 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr7 downsampled image jpeg b390204055747965c79a386287e7cc6c gr7 jpg gr7 gr7 jpg jpg 40444 394 383 image downsampled 1 s2 0 s0022169423002160 gr8 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr8 downsampled image jpeg 6ee5910e593abce1979a60783f3f317a gr8 jpg gr8 gr8 jpg jpg 20043 225 378 image downsampled 1 s2 0 s0022169423002160 gr9 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr9 downsampled image jpeg 6931dc007e07e763966614c4baf4749b gr9 jpg gr9 gr9 jpg jpg 21067 257 383 image downsampled 1 s2 0 s0022169423002160 gr11 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr11 downsampled image jpeg 46e3e7c9f82bfc0bb3fe52c4cdbb7ca9 gr11 jpg gr11 gr11 jpg jpg 26730 261 535 image downsampled 1 s2 0 s0022169423002160 gr1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr1 downsampled image jpeg a0aa5b98360c5b039baede7d32ecc91b gr1 jpg gr1 gr1 jpg jpg 52472 411 600 image downsampled 1 s2 0 s0022169423002160 gr2 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr2 downsampled image jpeg 01b8ed66e9ea1de9c783d47033234e08 gr2 jpg gr2 gr2 jpg jpg 158341 594 743 image downsampled 1 s2 0 s0022169423002160 gr3 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr3 downsampled image jpeg d0257a5e590c88548452109e4ed75510 gr3 jpg gr3 gr3 jpg jpg 104839 784 610 image downsampled 1 s2 0 s0022169423002160 gr4 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr4 downsampled image jpeg 2119b5960ce84be9cc8be7bfc04f83bf gr4 jpg gr4 gr4 jpg jpg 20701 231 383 image downsampled 1 s2 0 s0022169423002160 gr5 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr5 downsampled image jpeg 3e85985a193526e6a0542892c3604dbc gr5 jpg gr5 gr5 jpg jpg 11502 240 371 image downsampled 1 s2 0 s0022169423002160 gr6 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr6 downsampled image jpeg d44e79bce860591396037f75eafde7ae gr6 jpg gr6 gr6 jpg jpg 37087 391 383 image downsampled 1 s2 0 s0022169423002160 gr10 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr10 downsampled image jpeg 0b5c2464b6de370260f9fb9f764034ec gr10 jpg gr10 gr10 jpg jpg 60440 544 535 image downsampled 1 s2 0 s0022169423002160 gr7 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr7 thumbnail image gif 3aa94724216a4d47d8d87900e4300e0f gr7 sml gr7 gr7 sml sml 7694 164 159 image thumbnail 1 s2 0 s0022169423002160 gr8 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr8 thumbnail image gif bad5d12aa44534011c73a3fd57d45d90 gr8 sml gr8 gr8 sml sml 7848 131 219 image thumbnail 1 s2 0 s0022169423002160 gr9 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr9 thumbnail image gif 75da9963cdeff9ecccbeb79748334fba gr9 sml gr9 gr9 sml sml 8013 147 219 image thumbnail 1 s2 0 s0022169423002160 gr11 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr11 thumbnail image gif 6f2267da08c98968c76a78dc4c0b2f3a gr11 sml gr11 gr11 sml sml 5644 107 219 image thumbnail 1 s2 0 s0022169423002160 gr1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr1 thumbnail image gif b3a6b83f50e158d1ce8dbce8d2157daa gr1 sml gr1 gr1 sml sml 9724 150 219 image thumbnail 1 s2 0 s0022169423002160 gr2 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr2 thumbnail image gif 055d1d3d2c938f9320f3e6689b456bea gr2 sml gr2 gr2 sml sml 16782 164 205 image thumbnail 1 s2 0 s0022169423002160 gr3 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr3 thumbnail image gif f032f182e70ea6bf488c8d3d8b7de81d gr3 sml gr3 gr3 sml sml 8767 163 127 image thumbnail 1 s2 0 s0022169423002160 gr4 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr4 thumbnail image gif 4d9a6c06688dba04d2adc079d8ab9a31 gr4 sml gr4 gr4 sml sml 5871 132 219 image thumbnail 1 s2 0 s0022169423002160 gr5 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr5 thumbnail image gif 6fe90e078e340993f5b82fb03ae7e36a gr5 sml gr5 gr5 sml sml 4169 142 219 image thumbnail 1 s2 0 s0022169423002160 gr6 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr6 thumbnail image gif b4ede013a1d49362a141029d5a0ede52 gr6 sml gr6 gr6 sml sml 6546 163 160 image thumbnail 1 s2 0 s0022169423002160 gr10 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr10 thumbnail image gif f78e890496a24a2a06f0f6d1a6a177b4 gr10 sml gr10 gr10 sml sml 6820 164 161 image thumbnail 1 s2 0 s0022169423002160 gr7 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg a9e7b8cba7518b79e275c41c6d39ed27 gr7 lrg jpg gr7 gr7 lrg jpg jpg 239020 1749 1699 image high res 1 s2 0 s0022169423002160 gr8 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg 4f82843b71e2187aba88e8d321c28e1c gr8 lrg jpg gr8 gr8 lrg jpg jpg 143565 997 1673 image high res 1 s2 0 s0022169423002160 gr9 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg 09ad1c4fe7308c402b6053f75024d9a0 gr9 lrg jpg gr9 gr9 lrg jpg jpg 146886 1139 1699 image high res 1 s2 0 s0022169423002160 gr11 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg 150d0272dbad3a4c55cdf7e7f1e0c022 gr11 lrg jpg gr11 gr11 lrg jpg jpg 173065 1156 2368 image high res 1 s2 0 s0022169423002160 gr1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg 72606f965d0b592fa51267aa6dbf671d gr1 lrg jpg gr1 gr1 lrg jpg jpg 299289 1821 2658 image high res 1 s2 0 s0022169423002160 gr2 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg b8841268ae895a564d818e87855e95e3 gr2 lrg jpg gr2 gr2 lrg jpg jpg 968700 2633 3292 image high res 1 s2 0 s0022169423002160 gr3 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg 9122aff2ab98a68c8c5da9dc023af5c4 gr3 lrg jpg gr3 gr3 lrg jpg jpg 592443 3469 2700 image high res 1 s2 0 s0022169423002160 gr4 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg dfa52e49cea3c859c1a4af84f12c705f gr4 lrg jpg gr4 gr4 lrg jpg jpg 111093 1026 1699 image high res 1 s2 0 s0022169423002160 gr5 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg 55ce232cc8b22c0268a18ee4330cdf4c gr5 lrg jpg gr5 gr5 lrg jpg jpg 62231 1064 1646 image high res 1 s2 0 s0022169423002160 gr6 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg fe239fcc1b6f4020c8343cabd3fd238a gr6 lrg jpg gr6 gr6 lrg jpg jpg 204110 1735 1699 image high res 1 s2 0 s0022169423002160 gr10 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg c928a0133e954a2afff45bcc277d7d10 gr10 lrg jpg gr10 gr10 lrg jpg jpg 323979 2407 2368 image high res 1 s2 0 s0022169423002160 mmc1 docx https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 mmc1 main application vnd openxmlformats officedocument wordprocessingml document 7c18491cf7f7d2a27f5ae96a2e48db1e mmc1 docx mmc1 mmc1 docx docx 8917975 application 1 s2 0 s0022169423002160 si1 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 02d530ae74bad22162a3f075a1cafa81 si1 svg si1 si1 svg svg 2202 altimg 1 s2 0 s0022169423002160 si10 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 2b98399e11fadba1684506de78879042 si10 svg si10 si10 svg svg 1921 altimg 1 s2 0 s0022169423002160 si11 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 80745c3c57e7e5cedc6e1110537bb6b3 si11 svg si11 si11 svg svg 3579 altimg 1 s2 0 s0022169423002160 si12 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 1915bdc36ce1957aeafbb36bd4493be3 si12 svg si12 si12 svg svg 4580 altimg 1 s2 0 s0022169423002160 si13 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 04be2fb419e34d3e290dea77417c81ce si13 svg si13 si13 svg svg 4015 altimg 1 s2 0 s0022169423002160 si14 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 4d642c7f8c66a4e21dd57e4b504b422d si14 svg si14 si14 svg svg 4410 altimg 1 s2 0 s0022169423002160 si15 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 34d9b9c7d08d3dc6b11448451821974e si15 svg si15 si15 svg svg 3843 altimg 1 s2 0 s0022169423002160 si16 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml ea3f347438633425c43408be195cfe52 si16 svg si16 si16 svg svg 5185 altimg 1 s2 0 s0022169423002160 si17 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 1c0261e8073fd5845942ef46e702e50e si17 svg si17 si17 svg svg 11237 altimg 1 s2 0 s0022169423002160 si18 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 68dbbcee9c6b33b7ce20b62e0c3b04cd si18 svg si18 si18 svg svg 23661 altimg 1 s2 0 s0022169423002160 si19 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml d0b5d0cb1536a18c3b7327e0deae3123 si19 svg si19 si19 svg svg 3843 altimg 1 s2 0 s0022169423002160 si2 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 85630e76f42bb1450b0741fd7a59e8ff si2 svg si2 si2 svg svg 4851 altimg 1 s2 0 s0022169423002160 si20 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 2a5856271127f84fdbf8da2c73999ed3 si20 svg si20 si20 svg svg 4482 altimg 1 s2 0 s0022169423002160 si21 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 26fd19aa70bc81aa98d93923f9cbe45b si21 svg si21 si21 svg svg 11690 altimg 1 s2 0 s0022169423002160 si22 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml ff0d9a064586ad85b0b47041203d3e23 si22 svg si22 si22 svg svg 19130 altimg 1 s2 0 s0022169423002160 si23 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 3974d50829afa65284751b5e8522b7fd si23 svg si23 si23 svg svg 35228 altimg 1 s2 0 s0022169423002160 si24 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 4fb1990db2c44adad38248825f2872c6 si24 svg si24 si24 svg svg 11649 altimg 1 s2 0 s0022169423002160 si25 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml b36b41b6a3c387d8a7a2da8970e32434 si25 svg si25 si25 svg svg 9512 altimg 1 s2 0 s0022169423002160 si26 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml d9fb2065e3eeb89b1520346df7f66d37 si26 svg si26 si26 svg svg 6237 altimg 1 s2 0 s0022169423002160 si27 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml e53613b0b3de00e286e40a8414aac308 si27 svg si27 si27 svg svg 107160 altimg 1 s2 0 s0022169423002160 si28 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml e8b81abae1e8d3d31afa54fc22bcdac4 si28 svg si28 si28 svg svg 47825 altimg 1 s2 0 s0022169423002160 si29 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 7e8ee05271d7a1558cfe14d6279e6c1a si29 svg si29 si29 svg svg 27943 altimg 1 s2 0 s0022169423002160 si3 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml f89529d4f3cde565b44b68395020a761 si3 svg si3 si3 svg svg 3837 altimg 1 s2 0 s0022169423002160 si30 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 39fd49625410b2123d80aa4ce0f275f2 si30 svg si30 si30 svg svg 7644 altimg 1 s2 0 s0022169423002160 si31 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 775640e97bc7281efa3e348907c00e16 si31 svg si31 si31 svg svg 1796 altimg 1 s2 0 s0022169423002160 si32 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml b66e15cb789f8f7b9732ace49bf2abc7 si32 svg si32 si32 svg svg 1966 altimg 1 s2 0 s0022169423002160 si33 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 9d411bedb33faa875c8131dab476a928 si33 svg si33 si33 svg svg 7842 altimg 1 s2 0 s0022169423002160 si34 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 72414c4099c8b16cb5e7c732d39d8a4a si34 svg si34 si34 svg svg 34025 altimg 1 s2 0 s0022169423002160 si35 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 96eb837b3c450872d14184ecdba33134 si35 svg si35 si35 svg svg 21975 altimg 1 s2 0 s0022169423002160 si36 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 3114f2d6ee29663e7018dc62177176fc si36 svg si36 si36 svg svg 15919 altimg 1 s2 0 s0022169423002160 si37 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 1c3f3234efec70d634e06750c25248a4 si37 svg si37 si37 svg svg 17238 altimg 1 s2 0 s0022169423002160 si38 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml eae2a51dd02e9e9a53240fdc9bab9b5e si38 svg si38 si38 svg svg 4489 altimg 1 s2 0 s0022169423002160 si39 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 1f665f85e05fa0a406648c9e6e86149b si39 svg si39 si39 svg svg 14331 altimg 1 s2 0 s0022169423002160 si4 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 6ed43419fa7fb80d9a981a59213eae5b si4 svg si4 si4 svg svg 4091 altimg 1 s2 0 s0022169423002160 si40 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml beea4a9a420e74cfd374e0236b928124 si40 svg si40 si40 svg svg 14830 altimg 1 s2 0 s0022169423002160 si41 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 84b87267d1fb0549e49d4d3ae811844a si41 svg si41 si41 svg svg 15563 altimg 1 s2 0 s0022169423002160 si42 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml b52f9574f5687869a79257010b3512d6 si42 svg si42 si42 svg svg 13555 altimg 1 s2 0 s0022169423002160 si43 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 1e332e94cc2c82e5a106d37aa94cbaf0 si43 svg si43 si43 svg svg 15623 altimg 1 s2 0 s0022169423002160 si44 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml c6b9e2a1a5ad2dafce2350ceafd18450 si44 svg si44 si44 svg svg 19553 altimg 1 s2 0 s0022169423002160 si45 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 4a2d6ca863cb5ba0508bb2d23ece52c1 si45 svg si45 si45 svg svg 27556 altimg 1 s2 0 s0022169423002160 si46 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml f5212a2dafdad0885805c837ba146e11 si46 svg si46 si46 svg svg 26034 altimg 1 s2 0 s0022169423002160 si47 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 19936c39f5ed08406cbf56c8b988e986 si47 svg si47 si47 svg svg 3551 altimg 1 s2 0 s0022169423002160 si48 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 2e89b285aefcc6e8c1bfd441e0208db2 si48 svg si48 si48 svg svg 7980 altimg 1 s2 0 s0022169423002160 si49 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml cf89a407d950fb71d6670f96fbf5e0f3 si49 svg si49 si49 svg svg 6215 altimg 1 s2 0 s0022169423002160 si5 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 80861c49ab5cd5f852c520307978f8b0 si5 svg si5 si5 svg svg 6003 altimg 1 s2 0 s0022169423002160 si50 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 9a7306478001a9c4bd9d5dde9b1db31f si50 svg si50 si50 svg svg 2092 altimg 1 s2 0 s0022169423002160 si51 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 53a4abc62d891ee32821893df7d11777 si51 svg si51 si51 svg svg 11477 altimg 1 s2 0 s0022169423002160 si52 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 39e4a4e3f3f44f54b84005d7052962d6 si52 svg si52 si52 svg svg 2783 altimg 1 s2 0 s0022169423002160 si53 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml c45ed1363fcc5caceaf1c1a48a80daf4 si53 svg si53 si53 svg svg 13512 altimg 1 s2 0 s0022169423002160 si54 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 7d03550186cdbb03f954529cfc158c9e si54 svg si54 si54 svg svg 3583 altimg 1 s2 0 s0022169423002160 si6 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 6900ec4f12592b52777dcde9b082abbf si6 svg si6 si6 svg svg 3043 altimg 1 s2 0 s0022169423002160 si7 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 86fd87b1f58bfae637d9515d672ef979 si7 svg si7 si7 svg svg 2664 altimg 1 s2 0 s0022169423002160 si8 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml cff7e631509fda8b87e0064539dc38c6 si8 svg si8 si8 svg svg 2093 altimg 1 s2 0 s0022169423002160 si9 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml c8a67490266007cb1977b8c2905bf83a si9 svg si9 si9 svg svg 3903 altimg 1 s2 0 s0022169423002160 am pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content egi 10z2v8mm55l main application pdf 616b8164b3eb7f0a0839adedc4cc26c1 am pdf am am pdf pdf false 2772346 aam pdf hydrol 129274 129274 s0022 1694 23 00216 0 10 1016 j jhydrol 2023 129274 fig 1 conceptual model of the organization of heterogeneous facies across two mutually exclusive scales left and cumulative frequency of physical e g ln k or geochemical ln k d attributes ξ as subdivided into subpopulations by facies type at top level ii and bottom level i right the larger scale facies types are composed of assemblages of smaller scale facies types modified from soltanian et al 2020 fig 2 comparison of parameter space coverage under different sampling schemes the meaning of selected parameters is shown in table 3 in this paper fig 3 location of the study area in tsitsihar and the location of boreholes fig 4 longitudinal dispersivities for sorptive and non reactive solutes calculated from the lagrangian model parameterized using the borden aquifer data fig 5 mean μ and standard deviation σ values of the parameter ees for the sorptive solute dispersivity model fig 6 convergence of total sobol indice and first order sobol indice of α 11 nr under mc lhs sobol sampling methods fig 7 convergence of total sobol indice and first order sobol indice of α 11 s under mc lhs sobol sampling methods fig 8 sobol indices of input parameters for α 11 nr for the borden site fig 9 sobol indices of input parameters for α 11 s for the borden site fig 10 convergence of total sobol indice and first order sobol indice of α 11 nr under mc lhs sobol sampling methods for qqhe site fig 11 sobol indices of input parameters for α 11 nr for the qqhe site table 1 univariate statistics of larger scale facies types of the borden site shown are proportions po of facies types mean length in the horizontal and vertical directions mean y ξ and variance σ y 2 σ ξ 2 of y and ξ and the associated integral scales in two directions mean length and integral scale are in meters k is in m day k d is in l kg modified from ramanathan et al 2010 ritzi et al 2013 facies type po lh lz y σ y 2 ξ σ ξ 2 λh a λz b m 0 39 3 00 0 18 2 06 0 22 0 94 0 48 1 83 0 11 fz 0 61 5 85 0 30 1 22 0 17 1 52 0 29 2 29 0 12 a proportion weighted average value is 2 11 m b proportion weighted average value is 0 12 m table 2 univariate statistics of facies types of the qqhe site shown are proportions po of facies types mean length l in the horizontal and vertical directions mean y and variance σ y 2 of y and the associated integral scales facies type po lh lz y σ y 2 λh a λz b gc 0 50 2162 21 7 35 3 83 0 08 1081 11 3 67 mf 0 39 1472 33 5 06 2 34 0 40 898 12 3 08 sc 0 11 830 24 2 60 2 11 0 69 738 91 2 32 a proportion weighted average value is 972 10 m b proportion weighted average value is 3 29 m table 3 the sampling range and distribution type of α 11 s inputs that is used for sensitivity analyses variable meaning distribution type sampling range ε anisotropy ratio uniform 0 01 1 0 p1 facies proportion uniform 0 1 p2 uniform 0 1 l1 horizontal mean length uniform 1 50 l2 uniform 1 50 λy1 correlation length of y uniform 0 1 25 λy1 uniform 0 1 25 λξ1 correlation length of ξ uniform 0 1 25 λξ2 uniform 0 1 25 my1 mean of y gaussian 2 06 0 47 a my1 gaussian 1 22 0 41 b mξ1 mean of ξ gaussian 0 94 0 69 mξ2 gaussian 1 52 0 55 σ y 1 2 variance of y uniform 0 2 σ y 2 2 uniform 0 2 σ ξ 1 2 variance of ξ uniform 0 2 σ ξ 2 2 uniform 0 2 a correlation coefficient uniform 1 1 j hydraulic gradient uniform 0 001 0 01 n porosity uniform 0 3 0 4 tips numbers 1 and 2 stands for facies m and fz respectively a 0 47 is modified to 0 94 for the α 11 nr model b 0 41 is modified to 0 82 for the α 11 nr model table 4 the range of value and distribution type of model inputs used for sensitivity analyses for non reactive solutes in qqhe site the parameter meaning is the same as table 3 variable distribution type sampling range ε uniform 0 001 0 01 p1 uniform 0 1 p2 uniform 0 1 p3 uniform 0 1 l1 uniform 500 3000 l2 uniform 500 3000 l3 uniform 500 3000 λy1 uniform 1 100 λy2 uniform 1 100 λy3 uniform 1 100 my1 gaussian 3 729 0 56 my2 gaussian 2 536 0 63 my3 gaussian 2 105 0 83 σ y 1 2 uniform 0 2 σ y 2 2 uniform 0 2 σ y 3 2 uniform 0 2 j uniform 0 001 0 01 n uniform 0 3 0 4 tips numbers 1 2 and 3 stand for facies gc mf and sc respectively research papers global sensitivity study of non reactive and sorptive solute dispersivity in multiscale heterogeneous sediments wanli ren formal analysis data curation investigation visualization writing original draft a d heng dai conceptualization methodology software validation funding acquisition a songhu yuan writing review editing a zhenxue dai writing review editing b ming ye methodology writing review editing c mohamad reza soltanian conceptualization methodology writing review editing supervision d a state key laboratory of biogeology and environmental geology china university of geosciences wuhan 430078 hubei china state key laboratory of biogeology and environmental geology china university of geosciences wuhan 430078 hubei china state key laboratory of biogeology and environmental geology china university of geosciences wuhan 430078 hubei china b college of construction engineering jilin university changchun 130061 jilin china college of construction engineering jilin university changchun 130061 jilin china college of construction engineering jilin university changchun 130061 jilin china c department of earth ocean and atmospheric science florida state university tallahassee 32306 fl usa department of earth ocean and atmospheric science florida state university tallahassee 32306 fl usa department of earth ocean and atmospheric science florida state university tallahassee 32306 fl usa d departments of geosciences and environmental engineering university of cincinnati cincinnati 45221 oh usa departments of geosciences and environmental engineering university of cincinnati cincinnati 45221 oh usa departments of geosciences and environmental engineering university of cincinnati cincinnati 45221 oh usa corresponding author this manuscript was handled by huaming guo editor in chief with the assistance of jianying shang associate editor lagrangian based transport models provide effective ways of understanding mass transport processes within aquifer systems the models provide a direct relationship between sparse data on sedimentary architecture e g facies proportions and mean lengths and physical and geochemical sediment properties e g hydraulic conductivity k and sorption distribution coefficient k d to transport observables such as dispersion data sparsity leads to parameter uncertainty which in turn makes model prediction uncertain this study identifies the key uncertain inputs for both non reactive and sorptive solute dispersivity through a global sensitivity analysis estimates of the individual and correlation contributions of input parameters to model output are provided data from two sites with different scales of heterogeneities are used to evaluate the sensitive parameters of non reactive dispersivity at different scales the results show that sorptive solute dispersivity is most sensitive to in facies mean k d followed by k d variance while non reactive plume dispersivity is most sensitive to in facies mean k followed by the volume proportions and mean lengths of facies types when the heterogeneity integral scale increases to 102 103m hydraulic gradient becomes a non negligible factor controlling the non reactive solutes transport the convergence of the sensitivity indices and the effect of different sampling methods on the results are also evaluated in this study the results show that the number of input parameters and the complexity of the model determine the sampling size to achieve the ranking convergence of the sensitive indices sobol sequence sampling scheme outperforms in terms of convergence rate and accuracy over the monte carlo and latin hypercube sampling schemes the results of this study will improve our understanding of the complex model system and also provide guidance for further field investigation and data collection keywords sorptive solute transport lagrangian based model heterogeneous porous media sensitivity analysis multiscale data availability data will be made available on request 1 introduction aquifers exhibit heterogeneities in physical and geochemical sediment properties e g hydraulic conductivity k and sorption distribution coefficient k d which can significantly impact flow and solute transport processes bianchi et al 2011 de barros et al 2012 puyguiraud et al 2020 sanchez vila et al 2006 sun et al 2008 the spatial variation of physical and geochemical attributes has been shown to be responsible for the spatial and or temporal scale dependence of transport parameters such as dispersivity mixing and retardation factor cirpka et al 2011 deng et al 2013 connolly and johns 2016 soltanian et al 2015b 2020 to practically deal with scale dependent transport parameters different upscaling methods have been proposed to study the solute transport processes by incorporating small scale hydrogeological information dagan et al 2013 dentz et al 2011 rubin 2003 scheibe et al 2015 zhang et al 2021 2022a among them the stochastic averaging method by using lagrangian based theory developed by dagan 1984 has been widely used the theory has gone through several phases of evolution soltanian et al 2015c 2015d ritzi and soltanian 2015 recent modifications have allowed the incorporation of the physical attributes of sediments e g volume proportions and mean lengths in the derivation in order to provide a direct link between the observed geology and transport observables soltanian et al 2015a more importantly for the multiscale aquifer architecture shown conceptually in fig 1 similar to dai et al 2004 deng et al 2013 where facies types defined at the larger scale are comprising assemblages of facies types at a smaller scale the theory allows separating and studying the effect of each observables scales on solute spreading and mixing dai et al 2004 2020 ramanathan et al 2008 ritzi and soltanian 2015 however parameter uncertainties impact the performance of lagrangian based transport models lbtm for example there are uncertainties in volume proportions and mean lengths which are calculated based on the discretely distributed boreholes and limited outcrop data boreholes can provide rich facies information in the vertical direction while lateral distribution between boreholes needs to be interpolated based on certain assumptions weissmann et al 1999 he et al 2014 zhan et al 2022 this will introduce errors in calculating volume proportions and transverse mean lengths partially exposed outcrops may also lead to statistical errors in facies lengths white and willis 2000 dai et al 2005 note that the outcrops and well logs only represent small windows of subsurface analogous outcrops may not even be available for many aquifer systems the geological data estimated from discrete and uncertain data have been shown to be the main source of uncertainty in numerical simulations of flow and solute transport refsgaard et al 2012 bayer et al 2015 k is usually determined from laboratory measurements or well site aquifer tests and is sometimes converted from some physical quantities measured by surface geophysical surveys however these estimations are quite local and the accuracy of measurements needs to be improved for the site scale flow problems readers are encouraged to access the detailed information about hydrogeophysical methods and applications in estimating k summarized by rubin and hubbard 2006 the measurement of k d is closely related to the experimental conditions zhang et al 2022b consequently the retardation factor r is preferred in practice instead of extrapolating the k d obtained from a particular experimental setting moreover the data on physical and geochemical measurements are far less abundant than sedimentary architecture data and there are always expense issues sanchez vila et al 2006 riva et al 2010 soltanian et al 2015a 2015c zhang and schaap 2019 ritzi and soltanian 2015 as a result there is also a high degree of uncertainty associated with such measurements sensitivity analysis has been a powerful tool to identify the most critical uncertain parameters in hydrological modeling which can be divided into two groups local and global saltelli et al 2010 dai et al 2017 compared with local sensitivity analysis lsa there is a growing trend of using global sensitivity analysis gsa which not only provides the rank of the importance of uncertain parameters but also takes into account the whole range of model inputs and interactions between different parameters sudret 2008 herman et al 2013 song et al 2015 among various gsa methods the variance based method also called sobol method and the screening method such as the morris method are the two commonly used methods sobol 1993 morris 1991 van werkhoven et al 2008 however for a complex model the computational cost of performing gsa is a limiting factor that cannot be ignored because monte carlo mc simulation with a large number of model executions is required to attain convergence sobol 1993 van griensven et al 2006 saltelli et al 2010 dai and ye 2015 therefore a variety of methods have been developed to improve computational efficiency conducting a morris analysis first to screen out the most important parameters is a common and efficient way for the multiple inputs problem nabi et al 2021 another useful method is to build less expensive surrogate models however a relatively large number of model executions may still be required to develop accurate surrogates iooss and lemaître 2015 it has been shown that the required sample size can be reduced by selecting a suitable sampling scheme in the absence of a sufficiently inexpensive surrogate model tarantola et al 2012 renardy et al 2021 even if the required sample size cannot be reduced it may also provide additional benefits such as more robust estimation of uncertainties fast stability and convergence sudret 2008 iooss and lemaître 2015 sheikholeslami et al 2019 there has been extensive research on different sampling algorithms and their properties helton and davis 2003 tarantola et al 2012 dige and diwekar 2018 note however that all sampling schemes may also lead to inaccurate evaluation results when the sample size is small as mentioned before the available hydrogeological measurements at any certain heterogeneous aquifer are limited and sparse therefore the transport model requires uncertainty estimation due to the ubiquitous lack of parameter knowledge caused by spatial heterogeneity of physical and geochemical sediment properties in combination with incomplete characterization rubin 2003 this enables future measurements to be targeted to the most influential parameters to better utilize limited resources to reduce prediction uncertainty in flow and solute transport pan et al 2011 oladyshkin et al 2012 the sensitivity of lbtm has been investigated by several studies for instance dai et al 2004 investigated the influence of anisotropy ratio indicator scale integral scale mean conductivity on dispersion coefficient of non reactive solutes they found that the values of dispersion coefficient vary with the changes of these parameters the contrast in mean conductivity is one of the parameters that the dispersion coefficient is most sensitive and the sensitivity of indicator correlation scale is larger than the local integral scale soltanian et al 2015a further discussed the impact of heterogeneity in ln k mean variance and the integral scale of k and in ln k d mean variance and the integral scale of k d on dispersivity for the sorptive solute they concluded that the longitudinal dispersivity of the reactive solute is sensitive to changes in the mean and variance of both ln k and ln k d the integral scales of ln k and ln k d for the reactive minerals with larger volume proportion is the most sensitive parameter soltanian et al 2017 also discussed the influence of mean length and volume proportion however these discussions are all based on lsa method and they do not provide quantitative sensitivity measures for different parameters dai et al 2020 supplemented the gsa on the input parameters of the dispersivity model but their study is limited to non reactive solutes and the parameters they used were obtained from the 400 m long outcrops with the corresponding correlation scale of around 10 m in addition they set the same sampling range to facies related variables such as they used the same range of ln k for different facies in fact it is the strong difference of the physical and chemical properties that leads to the abnormal solute transport process such simple sampling range setting may miss the most important information about sensitive parameters in this paper we use a lagrangian based sorptive solutes transport model lbstm derived by soltanian et al 2015a within a gsa framework to complement our understanding of the sensitive parameters controlling sorptive solute dispersion to parameterize the model we use statistics of high resolution sedimentary architecture data and univariate statistics of k and k d from canadian force base at borden scale of hundred meter ritzi et al 2013 soltanian et al 2020 furthermore it has been studied that the geological structure drastically influences contaminant transport for larger field sites mainly through the effects of stratification and the presence of lenses with higher or lower conductivity which lead to velocity variations over several orders of magnitude de barros et al 2012 frippiat and holeyman 2008 various numerical studies have also identified that correlation length is one of the key parameters that determine solute breakthrough renard and allard 2013 willmann et al 2008 therefore in order to explore the important parameters affecting solute dispersion in natural systems characterized by layered structures or zones with distinct properties with a large correlation scale we also provide the same analysis for the qqhe site in china with scales of kilometers during the sensitivity analysis we also discuss the influence of different sampling methods on the sensitivity analysis results and evaluate the convergence of sensitivity indices the structure of this paper is as follows the lbstm used in this work that incorporates the multiscale sedimentary architecture is first reviewed in section 2 we also introduced the method for the gsa and the related convergence definitions in this section in section 3 we briefly introduce the study examples section 4 presents the results and discussion section 5 highlights the conclusions of this work based on such detailed gsa of models with physical and chemical heterogeneity we expect to gain a better understanding of the influence of hydraulic and sorption properties on solute transport and also to obtain some guidance information on data acquisition in dealing with practical problems 2 method 2 1 lagrangian based transport model following the hierarchical organization shown in fig 1 it has been shown that the solute dispersion is sensitive to the architecture of facies defined at one scale but not to the heterogeneity at smaller scales within those facies desbarats 1990 dai et al 2004 ramanathan et al 2010 at a certain scale the physical and chemical properties of different facies vary by order of magnitude and the heterogeneous k and k d fields usually exhibit multimodal distribution characters for a two facies media it is simplified to a bimodal character by following the conceptual model for heterogeneous porous media with the multimodal organization of reactive minerals soltanian et al 2015a developed the lbstm to analyze the sorptive solute dispersion undergoing equilibrium sorption the model directly relates sorptive solute dispersivity to quantifiable physical properties of the sediments such as the mean lengths and volume proportions of facies note that the models are not limited to a particular facies organization framework besides there is an assumption during the model derivation that k covaries with the mineralogic facies which controls reactivity because of this this lbstm considers both the spatial correlation structure of k and k d and their cross correlations soltanian et al 2017 have validated this model by using high resolution flow and reactive transport simulations and have investigated how the heterogeneity scales affect long term radionuclide transport at yucca mountain consider a domain ω filled with n number of facies of mutually exclusive occurrences ζ x is assumed to be a multimodal spatial random variable for y ln k or ξ ln k d at location x in the domain ω by using the indicator geostatic expression ζ x can be written as soltanian et al 2015a 1 ζ x i 1 n i i x ζ i x where ii x is an indicator variable within the domain ω and ζi x are random variables of the i th facies the ii x is defined as 2 i i x 1 if facies is i at location x 0 otherwise by using the facies volume proportion pi mean and variance of y or ξ mi and σ i 2 the expected global mean mζ and variance σ ζ 2 can be computed as dai et al 2004 huang and dai 2008 3 m ζ i 1 n p i m i 4 σ ζ 2 i 1 n p i σ i 2 1 2 j 1 n i 1 n p i p j m i m j 2 therefore the multimodal covariance function of y or ξ is 5 c ζ ψ i 1 n p i 2 σ i 2 e ψ λ i i 1 n p i 1 p i σ i 2 e ψ λ φ 1 2 i 1 n j 1 n p i p j m i m j 2 e ψ λ i where λi is the indicator integral scale and λi is the i th facies integral scale correlation length respectively they can be calculated by dai et al 2007 6 λ i i 1 n p i l i 1 p i 7 λ i l i 1 p i 8 λ φ λ i λ i λ i λ i where li is the mean length of the facies soltanian et al 2015a assumed that the effect of pore scale dispersion on the spatial moment of the sorptive plume is negligible in addition the y and ξ are correlated by ξ ay b where a and b are constants as a result for the multimodal heterogeneous sediments the final form of longitudinal dispersivity is expressed as soltanian et al 2015a 9 α 11 s t i 1 n p i 2 σ yi 2 λ i f 1 λ i i 1 n p i 1 p i σ yi 2 λ φ f 1 λ φ 1 2 i 1 n j 1 n p i p j m yi m yj 2 λ i f 1 λ i v r 3 ρ b n k d g 2 e σ ξ 2 0 t e i 1 n p i 2 σ ξ i 2 e v r λ i s i 1 n p i 1 p i σ ξ i 2 e v r λ φ s 1 2 i 1 n j 1 n p i p j m ξ i m ξ j 2 e v r λ i s 1 ds 2 r ρ b n 2 k g k d g j a sinh σ ξ σ ξ i 1 n p i 2 σ yi 2 λ i f 2 λ i i 1 n p i 1 p i σ yi 2 λ φ f 2 λ φ 1 2 i 1 n j 1 n p i p j m yi m yj 2 λ i f 2 λ i where 10a f 1 λ 1 e τ ε 0 2 rj 1 β 2 u 3 2 ε r v 2 u v 2 u 3 2 2 β 2 j 1 β β j 0 β r τ 2 ε 3 r 3 4 u v 5 v 4 u u 3 2 v 3 u 3 2 d r 10b f 2 λ 1 e τ 2 ε 2 0 r 2 1 2 vu 3 2 1 v 2 u 1 2 1 v 2 ε r j 1 β dr β rτ u 1 r2 v 1 r2 ε2r2 τ v t r λ where v is the mean flow velocity m d r is the retardation factor which is related to the k d by the relationship r x 1 ρb n k d x and r is the geometric mean of r ρb is the bulk density g cm3 n is the porosity k d g and k g are the geometric mean of k d x and k x fields respectively σ ξ 2 and σ y 2 are the variance of ξ and y j is the hydraulic gradient the term j 0 and j 1 are the zero and first order bessel functions respectively the ε is the anisotropy ratio defined as the vertical integral scale of the hydraulic conductivity to the horizontal component the above transport model can be simplified to represent non reactive solute dispersivity when ignoring the parameters associated with the sorption process thus equation 9 can be written as 11 α 11 nr t i 1 n p i 2 σ yi 2 λ i f 1 λ i i 1 n p i 1 p i σ yi 2 λ φ f 1 λ φ 1 2 i 1 n j 1 n p i p j m yi m yj 2 λ i f 1 λ i 2 2 global sensitivity analysis methods 2 2 1 morris method since the total number of model executions during the sensitivity analysis only increases linearly with the number of model parameters it can be seen that there are a large number of geological and geochemical parameters in the dispersivity model from equations 9 10 which would cause a high computational cost when performing gsa therefore the morris method morris 1991 is used first to screen the sensitive parameters the principle of this method is to perform the one at a time oat local sensitivity analysis to calculate the so called elementary effects ee which is defined as the ratio between the change of the output and the change of one input the mean μ and standard deviation σ of calculated ees are then used as sensitivity measurements however to avoid failing to identify the inputs with considerable influence on the model campolongo et al 2007 proposed to use the absolute mean value μ rather than the original μ value as the measurement by discretizing the parameter space into multiple levels and generating the input samples randomly the ee of the i th input parameter at the j th repetition is estimated as follows 12 e e i j y θ j e i y θ j β β 1 p 1 where p is the discretization level ei is a vector of the canonical base θ is one of the input parameters and y θ is the corresponding model output δ is a step length denoting n as the number of oat designs the sensitivity indices for each parameter can be obtained from statistics of ees as follows 13 μ i j 1 n e e i j n 14 σ i j 1 n e e i j j 1 n e e i j n 2 n μ i measures the influence of the i th parameter on the model output and σi measures interaction effects between parameters a high value of μ i indicates that the corresponding input parameter contributes significant uncertainty to the model output the large value of σi represents that the effect of the parameter is significant mostly because of the interactions with other input parameters 2 2 2 variance based method the variance based gsa method aims to decompose the total variance of the model output into the contribution of each single input parameter or their combinations and to use quantitative sensitivity measurements to evaluate the influence of input parameters on model output i e dispersivity in this work sobol 1993 saltelli et al 2008 suppose that f x is a mathematical model containing a set of input variables x1 xm described by a random vector x and the scalar model output is y f x the total output variance v of f x can be defined as sobol 1993 15 v y i 1 m v i i 1 m j i m v i j v 1 m where vi represents the partial variances contributed by i th parameter xi and vi j to v1 m represents the partial variances contributed by interactions among parameters according to the law of total variance the output variance can be defined as 16 v y v x i e x i y x i e x i v x i y x i the first item on the right hand side of equation 16 is the partial variance or the main effect caused by the parameter xi the inner expectation is the mean of the output calculated on all changing values that is all parameters except fixed xi from equation 16 the first order sobol indice can be expressed as 17 s i f v x i e x i y x i v y since this value evaluates the percentage of output uncertainty that is contributed by the input subset xi it is therefore a quantitative metric for the influence of the individual input parameter uncertainty source on model output sobol 1993 another commonly used sensitivity measurement is the total sensitivity indice which considers both the individual effect of xi and interactions of xi with other parameters homma and saltelli 1996 18 s i t v i v i j v i j m v y similar to equation 16 the total output variance is decomposed as 19 v y v x i e x i y x i e x i v x i y x i the first item on the right hand side of equation 19 measures the variance caused by x i from equation 19 the total sensitivity indice can be defined as 20 s i t e x i v x i y x i v y v y v x i e x i y x i v y following janssen 2013 and saltelli et al 2010 when based on a large number of model executions the mean and variance are calculated via 21 v x i e x i y x i 1 n m j 1 n f b j f a b j i f a j 22 e x i v x i y x i 1 2 n j 1 n f a j f a b j i 2 two independent parameter sample matrixes a and b are required in the calculation note that a and b should have the same dimension of nxm where n is the sample size and m is the number of input parameters matrix a b i is the same as a except that its i th column is extracted from the i th column of b a total number of 2 m xn model evaluations are required for the estimation of the sensitive indices the larger the first order sobol indice the greater the impact of the variable s change on the final output differences between total sobol indice and first order sobol indice represent the contribution of interactions between parameters if the difference is small the interactions between the variable with other variables have a small effect on the output sudret 2008 wainwright et al 2014 dai et al 2020 to attain convergence of the mc simulation the number of parameter samples that is the total number of model executions must be sufficiently large as mentioned in the introduction section different sampling schemes could help reduce the computational cost here we review three schemes used in this study van griensven et al 2006 renardy et al 2021 fig 2 shows the random sampling distribution of some sensitive parameters studied in this paper under different sampling schemes it can be seen intuitively that the sampling results are obviously different overall sobol method has a more uniform distribution of sampling points and a better control of boundaries the monte carlo method mc is the simplest sampling method to generate each sample point independently by using a random number generator however there is a risk of missing portions of parameter space fig 2a latin hypercube sampling lhs is an efficient sampling scheme based on the mc method which divides the parameter space into several equiprobable subspaces and performs mc sampling in each of them mckay et al 2000 janssen 2013 this method ensures that the entire range of each parameter can be sampled effectively fig 2b sobol sequences sobol are low discrepancy sequences also known as quasi random sequences which are deterministic sequences of numbers that converge quickly to a uniform distribution kucherenko et al 2015 previous studies have shown that these three sampling methods may perform differently for different calculation purposes and sample sizes but for most problems sobol sequences produce smaller errors and is more robust liefvendahl and stocki 2006 burhenne et al 2011 besides the computational cost of sobol sampling is relatively low only slightly higher than mc and much lower than lhs by reviewing the existing gsa literature sarrazin et al 2016 found that the previous research on the convergence of sensitivity analysis has no unified definition for different types of convergence evaluation and there is a lack of clear convergence criteria in general different types of convergence require different sample sizes which will directly affect the computational burden of the model based on this they defined quantitative criteria for different types of convergence and developed a method to quantitatively validate screening results three definitions of convergence of gsa results they proposed are see the example in fig 3 in sarrazin et al 2016 1 sensitivity indice convergence means the values of the indices remain stable 2 ranking convergence means the ordering between the parameters remains stable 3 screening convergence means the partitioning between sensitive and insensitive parameters remains stable among these convergence properties the degree of convergence from screening to sensitivity indices is getting higher and higher and more model evaluations are needed to attain convergence dai et al 2020 provided their sensitivity results with a sample size of 3000 but they did not evaluate the convergence of indices in order to make our evaluation results more accurate this paper also evaluates the convergence of the sensitivity indices in this paper gsa is performed by using the open source matlab package uqlab marelli et al 2022 this module is a powerful and flexible tool for performing sensitivity analysis with a number of different techniques the details methods and algorithms as well as the overview of the relevant up to date literature can be found in the manuals marelli et al 2022 3 study examples 3 1 borden site borden aquifer data is used to show the time evolution of dispersivity and to perform the sensitivity analysis for the sorptive solute the facies architecture of the borden aquifer consists of two levels and previous studies soltanian et al 2015c 2015d ren et al 2022 have shown that the larger scale facies heterogeneous structure is sufficient to effectively represent the dispersion process therefore we only focus on the two facies type aquifer architecture which is related to the larger scale heterogeneity of the borden site the facies consists of medium sand m and fine sand and silt fz by collecting facies samples k and k d samples from 67 boreholes see fig 1 in ritzi et al 2013 the physical and geochemical parameters of the facies types are presented in table 1 the velocity field is assumed to be uniform in average and the mean groundwater velocity v is aligned with the axis which can be calculated as 23 v k g j n thus the lagrangian velocity for sorptive solutes is expressed as follows 24 u v r where mean retardation factor r is calculated by r 1 ρ b n kd according to mackay et al 1986 the average bulk density ρ b and the average porosity n are 1 81 g cm3 and 0 33 respectively the hydraulic gradient j is 0 005 by equation 10 we can easily calculate the correlation lengths according to previous studies the proportion weighted correlation length for the system are 2 11 m and 0 12 m in in the horizontal and vertical directions dai et al 2005 ramanathan et al 2010 consequently the average anisotropy ratio ε is calculated as 0 06 3 2 qqhe site the qqhe site is located in tsitsihar heilongjiang province china which is in the middle reaches of the nen river fig 3a the nen river flows from northeast to southwest in the west of the study area controlled by topography features the groundwater flow direction in the area is basically the same as that of surface water there are huge and thick quaternary fluvial and lacustrine facies loose deposits exhibiting the multiscale sedimentary architecture this site is used to evaluate the sensitivity parameters for solute dispersion at a much larger correlation scale due to the difficulty of obtaining k d observations only the discussion related to non reactive solute dispersion is performed 57 boreholes fig 3b are used to reveal the sedimentary architecture about 50 m below ground surface similar to the facies classification of the borden site the sediments collected in the boreholes can be categorized based on a set of quantifiable textural attributes into three stratal facies the facies gc represents coarse grained media such as gravel and coarse sand facies mf stands for medium and fine sand facies sc is silts and clays by directly using facies information in discrete boreholes we first determined the volume proportions and the vertical mean length of each facies a series of facies sections along and perpendicular to the groundwater flow direction see appendix a were used to determine the facies mean length in the horizontal direction on this basis with the combination of the bayesian update algorithm proposed by white and willis 2000 and dai et al 2005 the calculated lengths were further corrected to avoid the bias of incomplete exposure pumping tests empirical methods are used to estimate k we collected 54 pumping test results to calculate k values related to medium to coarse grained media we also adopted the formula to calculate k based on deposits grain size devlin 2015 25 k ρ g μ c φ n d e 2 where μ is the temperature dependent dynamic viscosity of water g cm s ρ is the temperature dependent water density g ml g is the gravitational constant cm s2 c is an empirical factor φ n is a function of porosity and d e is an effective grain size for different applicable conditions the values of c φ n and d e are different readers are encouraged to access the details in devlin 2015 besides we also collected more data according to previously completed studies of the relationship between plasticity index and conductivity all the physical parameters of facies and the statistical information of k are presented in table 2 similar as borden site the proportion weighted correlation length for the system is 972 10 m in the horizontal direction and 3 29 m in the vertical direction and the average anisotropy ratio ε is then calculated as 0 003 the average hydraulic gradient is 0 01 and the mean porosity is 0 35 4 results and discussion 4 1 sorptive and non reactive solute dispersion fig 4 plots sorptive and non reactive solute dispersivity as a function of travel time in the borden site and cases with positive correlation a 1 and negative correlation a 1 are also shown overall the dispersivity increases monotonically with time and finally converges to a constant value when time is sufficiently large by comparing the dispersivity results of sorptive and non reactive solutes it is observed that the sorptive solute dispersivity is enhanced approximately by a factor of 2 moreover it takes a longer time for the sorptive solute to reach its asymptotic dispersivity value this nonideal transport character i e enhanced dispersion was actually observed in the borden site reactive solute transport experiment roberts et al 1986 and the interpretive modeling works bosma et al 1993 burr et al 1994 brusseau and srivastava 1997 under steady flow conditions the spatial variability in the solute velocity which is caused by variations in the k field is the principal mechanism for solute dispersion dagan 1989 dentz et al 2011 soltanian et al 2015d however the k d variability further promotes the velocity fluctuation for the sorptive solute transport which leads to the enhancement of dispersion in addition the cross correlation between k and k d appears to be a major factor in controlling the dispersion although there are still no conclusive indications of exactly what the correlation between k and k d is robin et al 1991 allen king et al 1998 ritzi et al 2013 the combined impact of these two parameters on solute transport has been intensively studied based on the assumption of linear and exponential correlations bellin et al 1993 bosma et al 1993 wu et al 2004 maghrebi et al 2013 soltanian et al 2015c from fig 4 it can be seen that the sorptive solute dispersivity is enhanced when k and k d are negatively correlated and decreased when positively correlated bellin et al 1993 through the analytical solution and bosma et al 1993 through numerical simulation pointed out that in the case of positive correlation the counteracting effects of k and k d heterogeneity lead to the decrease of dispersivity the outcome of the counteracting effects depends on the mean variance and integral scales of the spatially variable properties they also pointed out that the enhancement in negative correlation is stronger if the mean of k d is large 4 2 screening sensitive parameters for sorptive solute to reduce computational cost of the variance based gsa method we first used morris method to identify the more sensitivity parameters of sorptive solute dispersivity model this study considered all uncertainty input parameters in the dispersion model 20 in total including anisotropy ratio volume proportions mean lengths integral scale in facies mean and variance of y and ξ correlation coefficient hydraulic conductivity and porosity they can be broadly divided into three groups such as geological structure related parameters k related parameters and k d related parameters the upper and bottom boundaries and distribution types of these parameters for sensitivity analysis are listed in table 3 for the gaussian distribution type the values in square brackets represent mean and standard deviation respectively based on the time evolution results of dispersivity in fig 4 the sensitivity analysis was performed on the transport time of 1000 days when the dispersivity reached its asymptotic stable value fig 5 demonstrates the importance ranking of parameters for the model outputs in which the top ranking parameters with high mean μ and standard deviation σ values are labeled the sensitivity analysis results suggested that the parameters which control the α 11 s is mainly related to sorption distribution coefficient k d and geological structure overall α 11 s is more sensitive to parameters related to k d among them the mean sorption distribution coefficient values m ξ are the most influential factors for α 11 s the larger the m ξ is the greater the influence is in addition from fig 5 we can see that the σ values are very large indicating that the interactions between parameters have an obvious influence on α 11 s by using the morris method seven of the more sensitive parameters were finally selected for subsequent variance based sensitivity analysis which has significantly decreased the computational requirements 4 3 sensitivity analysis results of α 11 s and α 11 nr in this section we evaluate the sensitive parameters of α 11 s and α 11 nr based on parameters of the borden site a total of 13 evaluation parameters were involved in the α 11 nr model after removing the parameters associated with k d including m ξ σ ξ 2 λ ξ and a the distribution types and sampling ranges of inputs are consistent with those given in table 3 one point to add is that in order to obtain an obvious result for the α 11 nr sensitivity analysis the standard deviations of the sampling range of the mean y were set from 0 47 to 0 94 for m facies and from 0 41 to 0 82 for fz facies respectively for the α 11 s model we evaluate seven parameters screened out by the morris method we first evaluated the convergence of the sensitivity indices under different sampling sizes and different sampling schemes the results are shown in fig 6 and fig 7 in order to simplify the discussion and for a more intuitive presentation we have chosen to present the results only for two parameters with the largest total sobol indice and the parameters with a first order sobol indice greater than 0 1 the results suggest that the sobol indices for α 11 nr converge at a sample size of approximately 4000 fig 6 while the indices for α 11 s stabilize at around 10 000 sampling size fig 7 it can be concluded that although the number of parameters in the model is reduced 13 for α 11 nr and 7 for α 11 s the convergence rate of the sensitivity indices is still slower when there are strong interactions between parameters by comparing the indices results of the three sampling methods it can be seen that the sobol sequence sampling method outperforms in terms of convergence rate and in terms of accuracy which is directly related to its more uniform distribution interestingly we noted that the total sobol indice for α 11 s is more volatile than the results for the first order sobol indice and all of the indices for α 11 s are more volatile than those for α 11 nr as discussed in section 4 1 the k d variability promotes the velocity fluctuation of sorptive solute and thus promotes solute dispersion regardless of the correlation between k and k d therefore under the strong influence of k d related parameters the variability of α 11 s was enhanced exhibiting greater fluctuations in the sensitivity indices shown in fig 7 as shown in fig 5 the standard deviation value is large indicating that the interaction between parameters is strong so the total sobol indice fluctuates more and it is more difficult to achieve convergence the results further indicate the importance of first verifying the convergence of the sensitivity indices when conducting sensitivity analysis to obtain reliable results fig 8 plots the sobol indices results for α 11 nr obtained by using sobol sampling method under the convergence condition in previous studies the effects of the indicator integral scale on α 11 nr were discussed by soltanian et al 2015a and dai et al 2020 in particular the study by dai et al 2020 concluded that the indicator integral scale is the most sensitive parameter to α 11 nr with an uncertainty contribution of approximately 0 42 to the model output however as shown in equation 6 this parameter is determined by the volume proportions and the mean lengths of facies types therefore this paper directly incorporates proportions and lengths into the sensitivity analysis instead of using the integral scale as shown in fig 8 α 11 nr is most sensitive to the mean of y with a total uncertainty contribution of approximately 0 7 to the model output and the sedimentary structure parameters e g proportions and lengths are the second part of sensitive parameters followed by the variance of y we think that the reason for the discrepancy with the results of dai et al 2020 is that the indicator integral scale is a more comprehensive structure parameter this parameter reflects the joint influence of proportions and lengths with the exception of this parameter the sensitivity ranking of the remaining parameters is identical furthermore by combining actual site measurements and assigning different sampling ranges to facies related variables such as mean y for different facies our results more accurately capture the parameters that need the most attention in the simulation process than dai et al 2020 who assigned the same sampling ranges for analysis at the same time our results also deepen the understanding of the role of model parameters for example in general the parameters associated with highly permeable facies are more sensitive to α 11 nr in complex heterogeneous structures the interconnected architecture relative abundance and geometry of both high and low permeable facies affect solute transport significantly zhang et al 2013 in particular proportion and mean length of facies can affect water and solute transport by forming so called preferential flow pathways the effects of these two parameters on solute breakthrough curves travel time and diffusion characters have been extensively and detailly evaluated sivakumar et al 2005 zhang et al 2013 yin et al 2020 however there is still a lack of understanding of the dominant parameter our results show that the mean length of the high permeability facies m has the largest influence on α 11 nr followed by the proportion generally the proportion value can be well estimated by boreholes or outcrops therefore this result further reflects the importance of obtaining accurate facies length for dispersivity estimation note that the sensitivity contribution of the above mentioned parameters all comes from the difference between the total and the first order sobol indice indicating that the interactions between parameters affect α 11 nr a lot especially the interactions between mean y and other variables have the largest interaction effect correlation length and variance of y have weak impaction on α 11 nr and the sensitivity contribution mainly comes from the first order sobol indices indicating a strong direct effect on α 11 nr the total and first order sobol indices of anisotropy ratio porosity and mean length of fz facies are close to zero indicating that the change of these parameters has an insignificant impact on α 11 nr fig 9 plots the sobol indices results for α 11 s the mean of ξ is the most sensitive parameter and the variance of ξ seems to be the second sensitive parameter followed closely by the volume proportions and the m facies mean length soltanian et al 2015a pointed out through their lsa that σ ξ 2 is more sensitive to α 11 s than σ y 2 especially for multimodal media the larger the volume proportion of the facies the more sensitive σ ξ 2 is our results are consistent with this conclusion different from α 11 nr the sensitivity contribution of parameters all comes from the difference between the total and the first order sobol indice indicating that the interactions between parameters have a large effect on α 11 s one can see from equation 9 that a large number of multiplications integrations and exponential operations are involved in the calculation of the α 11 s so the interactions between the parameters are bound to be strong compared with the sedimentary structure parameters it is not easy to obtain the k data and it is even more difficult to estimate the adsorption parameters soltanian et al 2017 2020 although it is concluded in section 4 1 of this paper and previous studies that the correlation between k and k d seems to have a significant impact on the estimation of dispersivity it can be seen from the sensitivity analysis results that the accurate measurement of the k d data is the most important factor 4 4 effect of heterogeneity scale different hydrogeologically relevant scales control the scale dependence of k and k d thus impacting the solute transport processes ritzi and allen king 2007 ritzi and soltanian 2015 to determine whether the sensitive parameters controlling solute transport are the same in heterogeneous aquifers at different scales we performed the gsa at the qqhe site and then compared the results with the borden aquifer there is a total number of 18 parameters for sensitivity analysis and the distribution type and sampling ranges are given in table 4 fig 10 is the convergence plots of the total and first order sobol indices of the non reactive transport model it can be seen that the sensitivity indices reach rank convergence when the sample size reaches 6000 similarly the sensitive indices curves fluctuate less compared with fig 7 despite the significant number of input parameters fig 11 shows the total and first sobol indices for α 11 nr in contrast to the results obtained at the borden site j becomes the most sensitive parameter to α 11 nr followed by facies proportion and then the mean of y according to table 1 and table 2 the proportion averaged horizontal integral scale is 2 11 m for the borden site and 972 10 m for the qqhe site it is thus clear that the dominant factor of non reactive solute transport in heterogeneous aquifers is quite different at the site scale and regional scale there have been studies to investigate the effect of structures and dynamics on solute transport in heterogeneous media edery et al 2014 pauloo et al 2021 they concluded that transport cannot be explained solely by the structural knowledge of the medium dynamic flow controls are also critical factors especially for regional scale study this is because preferential flow along connected high k networks causes increased spatial spreading along the mean flow direction for the horizontal flow at the same time the mass transfer process in low k material shifts from diffusion and slow advection dominance to advection dominance therefore under a larger heterogeneous scale 102 103 the controlling factor for dispersivity is hydraulic note that around 70 of the total sobol indice comes from first order sobol indices indicating that the interaction of the hydraulic gradient with the other parameters has a weak effect on solute dispersivity it must be noted however that the effect of sedimentary structure and k heterogeneity are still important the larger the my is the greater the influence is besides the sensitivity contribution of my mainly comes from the first order sobol indice compared to fig 8 proportion accounts for more uncertainty contribution than my reflecting the strong control of heterogeneous structure on solute dispersion 5 conclusions we perform the global sensitivity analysis based on the lbstm developed by soltanian et al 2015a to have a better understanding of the impact of each parameter and the interactions between parameters on solute transport two sites were selected for the sensitivity analysis the first one is the borden site with an integral scale of about 100 101m at this site we quantified the sensitivity of the sorptive solute transport model undergoing equilibrium adsorption and non reactive solute transport model to parameters within possible ranges another site is the qqhe site with an integral scale of about 102 103m this site was used to investigate whether the sensitive parameters controlling solute transport are the same for heterogeneous aquifers at different scales during the process of sensitivity analysis we also conducted the convergence analysis of sensitivity indices and discussed the impact of different sampling methods on the sensitivity analysis results the major conclusion of this study are as follows 1 the sampling size required to attain the ranking convergence of sensitive indices will increase with the increase of the number of parameters however a more direct influencing factor is the complexity of the model the more complex the model more interactions between parameters the larger the sample size required in addition sobol sequence sampling scheme outperforms in terms of convergence rate and in terms of accuracy instead of mc and lhs sampling schemes 2 when the integral scale of the heterogeneous field is about 100 101m the sorptive solute dispersivity α 11 s is most sensitive to the mean adsorption coefficients k d followed by the variance of adsorption coefficient and then the volume proportions and mean lengths of facies the non reactive solute dispersivity α 11 nr is most sensitive to the mean conductivities followed by the proportions and mean lengths of facies and then the variance of conductivity and integral scales overall α 11 s is more sensitive to parameters related to k d α 11 nr is more sensitive to parameters related to k 3 in this study the possible range of each parameter is assigned according to the measured data of the site at the same time the use of intermediate variables is avoided for example the indicator integral scale which can be calculated by the volume proportions and the mean lengths of the facies will not participate in the sensitivity analysis such a parameter set up ensures that we can evaluate the sensitivity parameters of α 11 r and α 11 nr from the perspective of the simplest physical and chemical heterogeneity and also provides an insight into the solute transport process and related controlling parameters 4 when the heterogeneity integral scale increases to 102 103m α 11 nr is most sensitive to the hydraulic gradient followed by the volume proportions and mean lengths of facies and then the mean conductivities in other words under this condition the flow velocity fluctuation caused by the heterogeneity of conductivity will no longer be the main factor controlling solute transport but will be replaced by hydraulic gradient and heterogeneous sedimentary architecture sensitivity analysis is of considerable practical interest in subsurface contaminant hydrology the dynamic components of solute transport in groundwater are strongly influenced by the underlying heterogeneity of the aquifer including not only the sedimentary architecture but also the hydraulic properties although substantial progress has been achieved in solute transport in heterogeneous aquifers in recent decades one of the most serious drawbacks is still the lack of sufficient geological data through our study we are expecting to provide the direction of reducing the model uncertainty to enhance our understanding of the transport mechanisms and to identify the controlling factors the analysis of the independent and interaction effects of parameters is also expected to provide a scientific basis for the study of fluid flow and mass transport upscaling credit authorship contribution statement wanli ren formal analysis data curation investigation visualization writing original draft heng dai conceptualization methodology software validation funding acquisition songhu yuan writing review editing zhenxue dai writing review editing ming ye methodology writing review editing mohamad reza soltanian conceptualization methodology writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is funded by the national natural science foundation of china no 42172280 no u2267217 no 42141011 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2023 129274 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2096,lagrangian based transport model 2 2 global sensitivity analysis methods 2 2 1 morris method 2 2 2 variance based method 3 study examples 3 1 borden site 3 2 qqhe site 4 results and discussion 4 1 sorptive and non reactive solute dispersion 4 2 screening sensitive parameters for sorptive solute 4 3 sensitivity analysis results of α 11 s and α 11 nr 4 4 effect of heterogeneity scale 5 conclusions credit authorship contribution statement appendix a supplementary data allenking 1998 385 396 r bayer 2015 1 10 p bellin 1993 4019 4030 a bianchi 2011 m bosma 1993 4031 4043 w brusseau 1997 115 155 m burhenne 2011 s burr 1994 791 815 d campolongo 2007 1509 1518 f cirpka 2011 o connolly 2016 29 50 m dagan 1984 151 177 g dagan 1989 g flowtransportinporousformations dagan 2013 67 85 g dai 2017 4327 4343 h dai 2004 68 86 z dai 2005 z dai 2007 z dai 2015 286 300 h dai 2020 124516 z debarros 2012 f deng 2013 248 257 h dentz 2011 1 17 m desbarats 1990 153 163 a devlin 2015 837 844 j dige 2018 431 454 n edery 2014 1490 1505 y frippiat 2008 150 176 c he 2014 3147 3169 x helton 2003 23 69 j herman 2013 2893 2903 j homma 1996 1 17 t huang 2008 689 704 c iooss 2015 101 122 b janssen 2013 123 132 h liefvendahl 2006 3231 3247 m mackay 1986 2017 2029 d maghrebi 2013 8600 8604 m mckay 2000 55 61 m morris 1991 161 174 m nabi 2021 60900 60912 s oladyshkin 2012 10 22 s pan 2011 238 249 f pauloo 2021 e2020wr028655 r puyguiraud 2020 103782 a ramanathan 2008 r ramanathan 2010 r refsgaard 2012 36 50 j ren 2022 e2021wr031886 w renard 2013 168 196 p renardy 2021 108593 m ritzi 2007 r ritzi 2013 1901 1913 r ritzi 2015 31 39 r riva 2010 955 970 m roberts 1986 2047 2058 p robin 1991 2619 2632 m rubin 2003 y appliedstochastichydrogeology 2006 hydrogeophysics saltelli 2008 a globalsensitivityanalysisprimer saltelli 2010 259 270 a sanchezvila 2006 x sarrazin 2016 135 152 f scheibe 2015 38 56 t sheikholeslami 2019 282 299 r sivakumar 2005 211 218 b sobol 1993 407 414 i soltanian 2015 709 726 m soltanian 2015 235 244 m soltanian 2015 1586 1600 m soltanian 2015 1601 1618 m soltanian 2017 379 386 m soltanian 2020 125025 m song 2015 739 757 x sudret 2008 964 979 b sun 2008 a tarantola 2012 1061 1072 s vangriensven 2006 10 23 vanwerkhoven 2008 k wainwright 2014 84 94 h weissmann 1999 1761 1770 g white 2000 389 419 c willmann 2008 m wu 2004 j yin 2020 124515 m zhan 2022 e2021gl095823 c zhang 2013 78 99 y zhang 2022 127550 x zhang 2021 117603 x zhang 2022 128287 x zhang 2019 1011 1030 y renx2023x129274 renx2023x129274xw 2025 02 22t00 00 00 000z 2025 02 22t00 00 00 000z http creativecommons org licenses by nc nd 4 0 2023 elsevier b v all rights reserved 2023 02 28t06 57 50 861z http vtw elsevier com data voc addontypes 50 7 aggregated refined https doi org 10 15223 policy 017 https doi org 10 15223 policy 037 https doi org 10 15223 policy 012 https doi org 10 15223 policy 029 https doi org 10 15223 policy 004 item s0022 1694 23 00216 0 s0022169423002160 1 s2 0 s0022169423002160 10 1016 j jhydrol 2023 129274 271842 2023 05 29t12 20 35 766253z 2023 04 01 2023 04 30 1 s2 0 s0022169423002160 main pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 main application pdf 4b2deafabab513265c95ea36a1a8735b main pdf main pdf pdf true 6205112 main 14 1 s2 0 s0022169423002160 main 1 png https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 preview image png df22512590c16bb1b1c251fdbdb7da0f main 1 png main 1 png png 58192 849 656 image web pdf 1 1 s2 0 s0022169423002160 gr7 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr7 downsampled image jpeg b390204055747965c79a386287e7cc6c gr7 jpg gr7 gr7 jpg jpg 40444 394 383 image downsampled 1 s2 0 s0022169423002160 gr8 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr8 downsampled image jpeg 6ee5910e593abce1979a60783f3f317a gr8 jpg gr8 gr8 jpg jpg 20043 225 378 image downsampled 1 s2 0 s0022169423002160 gr9 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr9 downsampled image jpeg 6931dc007e07e763966614c4baf4749b gr9 jpg gr9 gr9 jpg jpg 21067 257 383 image downsampled 1 s2 0 s0022169423002160 gr11 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr11 downsampled image jpeg 46e3e7c9f82bfc0bb3fe52c4cdbb7ca9 gr11 jpg gr11 gr11 jpg jpg 26730 261 535 image downsampled 1 s2 0 s0022169423002160 gr1 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr1 downsampled image jpeg a0aa5b98360c5b039baede7d32ecc91b gr1 jpg gr1 gr1 jpg jpg 52472 411 600 image downsampled 1 s2 0 s0022169423002160 gr2 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr2 downsampled image jpeg 01b8ed66e9ea1de9c783d47033234e08 gr2 jpg gr2 gr2 jpg jpg 158341 594 743 image downsampled 1 s2 0 s0022169423002160 gr3 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr3 downsampled image jpeg d0257a5e590c88548452109e4ed75510 gr3 jpg gr3 gr3 jpg jpg 104839 784 610 image downsampled 1 s2 0 s0022169423002160 gr4 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr4 downsampled image jpeg 2119b5960ce84be9cc8be7bfc04f83bf gr4 jpg gr4 gr4 jpg jpg 20701 231 383 image downsampled 1 s2 0 s0022169423002160 gr5 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr5 downsampled image jpeg 3e85985a193526e6a0542892c3604dbc gr5 jpg gr5 gr5 jpg jpg 11502 240 371 image downsampled 1 s2 0 s0022169423002160 gr6 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr6 downsampled image jpeg d44e79bce860591396037f75eafde7ae gr6 jpg gr6 gr6 jpg jpg 37087 391 383 image downsampled 1 s2 0 s0022169423002160 gr10 jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr10 downsampled image jpeg 0b5c2464b6de370260f9fb9f764034ec gr10 jpg gr10 gr10 jpg jpg 60440 544 535 image downsampled 1 s2 0 s0022169423002160 gr7 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr7 thumbnail image gif 3aa94724216a4d47d8d87900e4300e0f gr7 sml gr7 gr7 sml sml 7694 164 159 image thumbnail 1 s2 0 s0022169423002160 gr8 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr8 thumbnail image gif bad5d12aa44534011c73a3fd57d45d90 gr8 sml gr8 gr8 sml sml 7848 131 219 image thumbnail 1 s2 0 s0022169423002160 gr9 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr9 thumbnail image gif 75da9963cdeff9ecccbeb79748334fba gr9 sml gr9 gr9 sml sml 8013 147 219 image thumbnail 1 s2 0 s0022169423002160 gr11 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr11 thumbnail image gif 6f2267da08c98968c76a78dc4c0b2f3a gr11 sml gr11 gr11 sml sml 5644 107 219 image thumbnail 1 s2 0 s0022169423002160 gr1 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr1 thumbnail image gif b3a6b83f50e158d1ce8dbce8d2157daa gr1 sml gr1 gr1 sml sml 9724 150 219 image thumbnail 1 s2 0 s0022169423002160 gr2 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr2 thumbnail image gif 055d1d3d2c938f9320f3e6689b456bea gr2 sml gr2 gr2 sml sml 16782 164 205 image thumbnail 1 s2 0 s0022169423002160 gr3 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr3 thumbnail image gif f032f182e70ea6bf488c8d3d8b7de81d gr3 sml gr3 gr3 sml sml 8767 163 127 image thumbnail 1 s2 0 s0022169423002160 gr4 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr4 thumbnail image gif 4d9a6c06688dba04d2adc079d8ab9a31 gr4 sml gr4 gr4 sml sml 5871 132 219 image thumbnail 1 s2 0 s0022169423002160 gr5 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr5 thumbnail image gif 6fe90e078e340993f5b82fb03ae7e36a gr5 sml gr5 gr5 sml sml 4169 142 219 image thumbnail 1 s2 0 s0022169423002160 gr6 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr6 thumbnail image gif b4ede013a1d49362a141029d5a0ede52 gr6 sml gr6 gr6 sml sml 6546 163 160 image thumbnail 1 s2 0 s0022169423002160 gr10 sml https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 gr10 thumbnail image gif f78e890496a24a2a06f0f6d1a6a177b4 gr10 sml gr10 gr10 sml sml 6820 164 161 image thumbnail 1 s2 0 s0022169423002160 gr7 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg a9e7b8cba7518b79e275c41c6d39ed27 gr7 lrg jpg gr7 gr7 lrg jpg jpg 239020 1749 1699 image high res 1 s2 0 s0022169423002160 gr8 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg 4f82843b71e2187aba88e8d321c28e1c gr8 lrg jpg gr8 gr8 lrg jpg jpg 143565 997 1673 image high res 1 s2 0 s0022169423002160 gr9 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg 09ad1c4fe7308c402b6053f75024d9a0 gr9 lrg jpg gr9 gr9 lrg jpg jpg 146886 1139 1699 image high res 1 s2 0 s0022169423002160 gr11 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg 150d0272dbad3a4c55cdf7e7f1e0c022 gr11 lrg jpg gr11 gr11 lrg jpg jpg 173065 1156 2368 image high res 1 s2 0 s0022169423002160 gr1 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg 72606f965d0b592fa51267aa6dbf671d gr1 lrg jpg gr1 gr1 lrg jpg jpg 299289 1821 2658 image high res 1 s2 0 s0022169423002160 gr2 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg b8841268ae895a564d818e87855e95e3 gr2 lrg jpg gr2 gr2 lrg jpg jpg 968700 2633 3292 image high res 1 s2 0 s0022169423002160 gr3 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg 9122aff2ab98a68c8c5da9dc023af5c4 gr3 lrg jpg gr3 gr3 lrg jpg jpg 592443 3469 2700 image high res 1 s2 0 s0022169423002160 gr4 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg dfa52e49cea3c859c1a4af84f12c705f gr4 lrg jpg gr4 gr4 lrg jpg jpg 111093 1026 1699 image high res 1 s2 0 s0022169423002160 gr5 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg 55ce232cc8b22c0268a18ee4330cdf4c gr5 lrg jpg gr5 gr5 lrg jpg jpg 62231 1064 1646 image high res 1 s2 0 s0022169423002160 gr6 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg fe239fcc1b6f4020c8343cabd3fd238a gr6 lrg jpg gr6 gr6 lrg jpg jpg 204110 1735 1699 image high res 1 s2 0 s0022169423002160 gr10 lrg jpg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 highres image jpeg c928a0133e954a2afff45bcc277d7d10 gr10 lrg jpg gr10 gr10 lrg jpg jpg 323979 2407 2368 image high res 1 s2 0 s0022169423002160 mmc1 docx https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 mmc1 main application vnd openxmlformats officedocument wordprocessingml document 7c18491cf7f7d2a27f5ae96a2e48db1e mmc1 docx mmc1 mmc1 docx docx 8917975 application 1 s2 0 s0022169423002160 si1 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 02d530ae74bad22162a3f075a1cafa81 si1 svg si1 si1 svg svg 2202 altimg 1 s2 0 s0022169423002160 si10 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 2b98399e11fadba1684506de78879042 si10 svg si10 si10 svg svg 1921 altimg 1 s2 0 s0022169423002160 si11 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 80745c3c57e7e5cedc6e1110537bb6b3 si11 svg si11 si11 svg svg 3579 altimg 1 s2 0 s0022169423002160 si12 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 1915bdc36ce1957aeafbb36bd4493be3 si12 svg si12 si12 svg svg 4580 altimg 1 s2 0 s0022169423002160 si13 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 04be2fb419e34d3e290dea77417c81ce si13 svg si13 si13 svg svg 4015 altimg 1 s2 0 s0022169423002160 si14 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 4d642c7f8c66a4e21dd57e4b504b422d si14 svg si14 si14 svg svg 4410 altimg 1 s2 0 s0022169423002160 si15 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 34d9b9c7d08d3dc6b11448451821974e si15 svg si15 si15 svg svg 3843 altimg 1 s2 0 s0022169423002160 si16 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml ea3f347438633425c43408be195cfe52 si16 svg si16 si16 svg svg 5185 altimg 1 s2 0 s0022169423002160 si17 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 1c0261e8073fd5845942ef46e702e50e si17 svg si17 si17 svg svg 11237 altimg 1 s2 0 s0022169423002160 si18 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 68dbbcee9c6b33b7ce20b62e0c3b04cd si18 svg si18 si18 svg svg 23661 altimg 1 s2 0 s0022169423002160 si19 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml d0b5d0cb1536a18c3b7327e0deae3123 si19 svg si19 si19 svg svg 3843 altimg 1 s2 0 s0022169423002160 si2 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 85630e76f42bb1450b0741fd7a59e8ff si2 svg si2 si2 svg svg 4851 altimg 1 s2 0 s0022169423002160 si20 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 2a5856271127f84fdbf8da2c73999ed3 si20 svg si20 si20 svg svg 4482 altimg 1 s2 0 s0022169423002160 si21 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 26fd19aa70bc81aa98d93923f9cbe45b si21 svg si21 si21 svg svg 11690 altimg 1 s2 0 s0022169423002160 si22 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml ff0d9a064586ad85b0b47041203d3e23 si22 svg si22 si22 svg svg 19130 altimg 1 s2 0 s0022169423002160 si23 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 3974d50829afa65284751b5e8522b7fd si23 svg si23 si23 svg svg 35228 altimg 1 s2 0 s0022169423002160 si24 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 4fb1990db2c44adad38248825f2872c6 si24 svg si24 si24 svg svg 11649 altimg 1 s2 0 s0022169423002160 si25 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml b36b41b6a3c387d8a7a2da8970e32434 si25 svg si25 si25 svg svg 9512 altimg 1 s2 0 s0022169423002160 si26 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml d9fb2065e3eeb89b1520346df7f66d37 si26 svg si26 si26 svg svg 6237 altimg 1 s2 0 s0022169423002160 si27 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml e53613b0b3de00e286e40a8414aac308 si27 svg si27 si27 svg svg 107160 altimg 1 s2 0 s0022169423002160 si28 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml e8b81abae1e8d3d31afa54fc22bcdac4 si28 svg si28 si28 svg svg 47825 altimg 1 s2 0 s0022169423002160 si29 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 7e8ee05271d7a1558cfe14d6279e6c1a si29 svg si29 si29 svg svg 27943 altimg 1 s2 0 s0022169423002160 si3 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml f89529d4f3cde565b44b68395020a761 si3 svg si3 si3 svg svg 3837 altimg 1 s2 0 s0022169423002160 si30 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 39fd49625410b2123d80aa4ce0f275f2 si30 svg si30 si30 svg svg 7644 altimg 1 s2 0 s0022169423002160 si31 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 775640e97bc7281efa3e348907c00e16 si31 svg si31 si31 svg svg 1796 altimg 1 s2 0 s0022169423002160 si32 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml b66e15cb789f8f7b9732ace49bf2abc7 si32 svg si32 si32 svg svg 1966 altimg 1 s2 0 s0022169423002160 si33 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 9d411bedb33faa875c8131dab476a928 si33 svg si33 si33 svg svg 7842 altimg 1 s2 0 s0022169423002160 si34 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 72414c4099c8b16cb5e7c732d39d8a4a si34 svg si34 si34 svg svg 34025 altimg 1 s2 0 s0022169423002160 si35 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 96eb837b3c450872d14184ecdba33134 si35 svg si35 si35 svg svg 21975 altimg 1 s2 0 s0022169423002160 si36 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 3114f2d6ee29663e7018dc62177176fc si36 svg si36 si36 svg svg 15919 altimg 1 s2 0 s0022169423002160 si37 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 1c3f3234efec70d634e06750c25248a4 si37 svg si37 si37 svg svg 17238 altimg 1 s2 0 s0022169423002160 si38 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml eae2a51dd02e9e9a53240fdc9bab9b5e si38 svg si38 si38 svg svg 4489 altimg 1 s2 0 s0022169423002160 si39 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 1f665f85e05fa0a406648c9e6e86149b si39 svg si39 si39 svg svg 14331 altimg 1 s2 0 s0022169423002160 si4 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 6ed43419fa7fb80d9a981a59213eae5b si4 svg si4 si4 svg svg 4091 altimg 1 s2 0 s0022169423002160 si40 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml beea4a9a420e74cfd374e0236b928124 si40 svg si40 si40 svg svg 14830 altimg 1 s2 0 s0022169423002160 si41 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 84b87267d1fb0549e49d4d3ae811844a si41 svg si41 si41 svg svg 15563 altimg 1 s2 0 s0022169423002160 si42 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml b52f9574f5687869a79257010b3512d6 si42 svg si42 si42 svg svg 13555 altimg 1 s2 0 s0022169423002160 si43 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 1e332e94cc2c82e5a106d37aa94cbaf0 si43 svg si43 si43 svg svg 15623 altimg 1 s2 0 s0022169423002160 si44 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml c6b9e2a1a5ad2dafce2350ceafd18450 si44 svg si44 si44 svg svg 19553 altimg 1 s2 0 s0022169423002160 si45 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 4a2d6ca863cb5ba0508bb2d23ece52c1 si45 svg si45 si45 svg svg 27556 altimg 1 s2 0 s0022169423002160 si46 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml f5212a2dafdad0885805c837ba146e11 si46 svg si46 si46 svg svg 26034 altimg 1 s2 0 s0022169423002160 si47 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 19936c39f5ed08406cbf56c8b988e986 si47 svg si47 si47 svg svg 3551 altimg 1 s2 0 s0022169423002160 si48 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 2e89b285aefcc6e8c1bfd441e0208db2 si48 svg si48 si48 svg svg 7980 altimg 1 s2 0 s0022169423002160 si49 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml cf89a407d950fb71d6670f96fbf5e0f3 si49 svg si49 si49 svg svg 6215 altimg 1 s2 0 s0022169423002160 si5 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 80861c49ab5cd5f852c520307978f8b0 si5 svg si5 si5 svg svg 6003 altimg 1 s2 0 s0022169423002160 si50 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 9a7306478001a9c4bd9d5dde9b1db31f si50 svg si50 si50 svg svg 2092 altimg 1 s2 0 s0022169423002160 si51 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 53a4abc62d891ee32821893df7d11777 si51 svg si51 si51 svg svg 11477 altimg 1 s2 0 s0022169423002160 si52 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 39e4a4e3f3f44f54b84005d7052962d6 si52 svg si52 si52 svg svg 2783 altimg 1 s2 0 s0022169423002160 si53 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml c45ed1363fcc5caceaf1c1a48a80daf4 si53 svg si53 si53 svg svg 13512 altimg 1 s2 0 s0022169423002160 si54 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 7d03550186cdbb03f954529cfc158c9e si54 svg si54 si54 svg svg 3583 altimg 1 s2 0 s0022169423002160 si6 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 6900ec4f12592b52777dcde9b082abbf si6 svg si6 si6 svg svg 3043 altimg 1 s2 0 s0022169423002160 si7 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml 86fd87b1f58bfae637d9515d672ef979 si7 svg si7 si7 svg svg 2664 altimg 1 s2 0 s0022169423002160 si8 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml cff7e631509fda8b87e0064539dc38c6 si8 svg si8 si8 svg svg 2093 altimg 1 s2 0 s0022169423002160 si9 svg https s3 eu west 1 amazonaws com prod ucs content store eu west content pii s0022169423002160 image svg xml c8a67490266007cb1977b8c2905bf83a si9 svg si9 si9 svg svg 3903 altimg 1 s2 0 s0022169423002160 am pdf https s3 eu west 1 amazonaws com prod ucs content store eu west content egi 10z2v8mm55l main application pdf 616b8164b3eb7f0a0839adedc4cc26c1 am pdf am am pdf pdf false 2772346 aam pdf hydrol 129274 129274 s0022 1694 23 00216 0 10 1016 j jhydrol 2023 129274 fig 1 conceptual model of the organization of heterogeneous facies across two mutually exclusive scales left and cumulative frequency of physical e g ln k or geochemical ln k d attributes ξ as subdivided into subpopulations by facies type at top level ii and bottom level i right the larger scale facies types are composed of assemblages of smaller scale facies types modified from soltanian et al 2020 fig 2 comparison of parameter space coverage under different sampling schemes the meaning of selected parameters is shown in table 3 in this paper fig 3 location of the study area in tsitsihar and the location of boreholes fig 4 longitudinal dispersivities for sorptive and non reactive solutes calculated from the lagrangian model parameterized using the borden aquifer data fig 5 mean μ and standard deviation σ values of the parameter ees for the sorptive solute dispersivity model fig 6 convergence of total sobol indice and first order sobol indice of α 11 nr under mc lhs sobol sampling methods fig 7 convergence of total sobol indice and first order sobol indice of α 11 s under mc lhs sobol sampling methods fig 8 sobol indices of input parameters for α 11 nr for the borden site fig 9 sobol indices of input parameters for α 11 s for the borden site fig 10 convergence of total sobol indice and first order sobol indice of α 11 nr under mc lhs sobol sampling methods for qqhe site fig 11 sobol indices of input parameters for α 11 nr for the qqhe site table 1 univariate statistics of larger scale facies types of the borden site shown are proportions po of facies types mean length in the horizontal and vertical directions mean y ξ and variance σ y 2 σ ξ 2 of y and ξ and the associated integral scales in two directions mean length and integral scale are in meters k is in m day k d is in l kg modified from ramanathan et al 2010 ritzi et al 2013 facies type po lh lz y σ y 2 ξ σ ξ 2 λh a λz b m 0 39 3 00 0 18 2 06 0 22 0 94 0 48 1 83 0 11 fz 0 61 5 85 0 30 1 22 0 17 1 52 0 29 2 29 0 12 a proportion weighted average value is 2 11 m b proportion weighted average value is 0 12 m table 2 univariate statistics of facies types of the qqhe site shown are proportions po of facies types mean length l in the horizontal and vertical directions mean y and variance σ y 2 of y and the associated integral scales facies type po lh lz y σ y 2 λh a λz b gc 0 50 2162 21 7 35 3 83 0 08 1081 11 3 67 mf 0 39 1472 33 5 06 2 34 0 40 898 12 3 08 sc 0 11 830 24 2 60 2 11 0 69 738 91 2 32 a proportion weighted average value is 972 10 m b proportion weighted average value is 3 29 m table 3 the sampling range and distribution type of α 11 s inputs that is used for sensitivity analyses variable meaning distribution type sampling range ε anisotropy ratio uniform 0 01 1 0 p1 facies proportion uniform 0 1 p2 uniform 0 1 l1 horizontal mean length uniform 1 50 l2 uniform 1 50 λy1 correlation length of y uniform 0 1 25 λy1 uniform 0 1 25 λξ1 correlation length of ξ uniform 0 1 25 λξ2 uniform 0 1 25 my1 mean of y gaussian 2 06 0 47 a my1 gaussian 1 22 0 41 b mξ1 mean of ξ gaussian 0 94 0 69 mξ2 gaussian 1 52 0 55 σ y 1 2 variance of y uniform 0 2 σ y 2 2 uniform 0 2 σ ξ 1 2 variance of ξ uniform 0 2 σ ξ 2 2 uniform 0 2 a correlation coefficient uniform 1 1 j hydraulic gradient uniform 0 001 0 01 n porosity uniform 0 3 0 4 tips numbers 1 and 2 stands for facies m and fz respectively a 0 47 is modified to 0 94 for the α 11 nr model b 0 41 is modified to 0 82 for the α 11 nr model table 4 the range of value and distribution type of model inputs used for sensitivity analyses for non reactive solutes in qqhe site the parameter meaning is the same as table 3 variable distribution type sampling range ε uniform 0 001 0 01 p1 uniform 0 1 p2 uniform 0 1 p3 uniform 0 1 l1 uniform 500 3000 l2 uniform 500 3000 l3 uniform 500 3000 λy1 uniform 1 100 λy2 uniform 1 100 λy3 uniform 1 100 my1 gaussian 3 729 0 56 my2 gaussian 2 536 0 63 my3 gaussian 2 105 0 83 σ y 1 2 uniform 0 2 σ y 2 2 uniform 0 2 σ y 3 2 uniform 0 2 j uniform 0 001 0 01 n uniform 0 3 0 4 tips numbers 1 2 and 3 stand for facies gc mf and sc respectively research papers global sensitivity study of non reactive and sorptive solute dispersivity in multiscale heterogeneous sediments wanli ren formal analysis data curation investigation visualization writing original draft a d heng dai conceptualization methodology software validation funding acquisition a songhu yuan writing review editing a zhenxue dai writing review editing b ming ye methodology writing review editing c mohamad reza soltanian conceptualization methodology writing review editing supervision d a state key laboratory of biogeology and environmental geology china university of geosciences wuhan 430078 hubei china state key laboratory of biogeology and environmental geology china university of geosciences wuhan 430078 hubei china state key laboratory of biogeology and environmental geology china university of geosciences wuhan 430078 hubei china b college of construction engineering jilin university changchun 130061 jilin china college of construction engineering jilin university changchun 130061 jilin china college of construction engineering jilin university changchun 130061 jilin china c department of earth ocean and atmospheric science florida state university tallahassee 32306 fl usa department of earth ocean and atmospheric science florida state university tallahassee 32306 fl usa department of earth ocean and atmospheric science florida state university tallahassee 32306 fl usa d departments of geosciences and environmental engineering university of cincinnati cincinnati 45221 oh usa departments of geosciences and environmental engineering university of cincinnati cincinnati 45221 oh usa departments of geosciences and environmental engineering university of cincinnati cincinnati 45221 oh usa corresponding author this manuscript was handled by huaming guo editor in chief with the assistance of jianying shang associate editor lagrangian based transport models provide effective ways of understanding mass transport processes within aquifer systems the models provide a direct relationship between sparse data on sedimentary architecture e g facies proportions and mean lengths and physical and geochemical sediment properties e g hydraulic conductivity k and sorption distribution coefficient k d to transport observables such as dispersion data sparsity leads to parameter uncertainty which in turn makes model prediction uncertain this study identifies the key uncertain inputs for both non reactive and sorptive solute dispersivity through a global sensitivity analysis estimates of the individual and correlation contributions of input parameters to model output are provided data from two sites with different scales of heterogeneities are used to evaluate the sensitive parameters of non reactive dispersivity at different scales the results show that sorptive solute dispersivity is most sensitive to in facies mean k d followed by k d variance while non reactive plume dispersivity is most sensitive to in facies mean k followed by the volume proportions and mean lengths of facies types when the heterogeneity integral scale increases to 102 103m hydraulic gradient becomes a non negligible factor controlling the non reactive solutes transport the convergence of the sensitivity indices and the effect of different sampling methods on the results are also evaluated in this study the results show that the number of input parameters and the complexity of the model determine the sampling size to achieve the ranking convergence of the sensitive indices sobol sequence sampling scheme outperforms in terms of convergence rate and accuracy over the monte carlo and latin hypercube sampling schemes the results of this study will improve our understanding of the complex model system and also provide guidance for further field investigation and data collection keywords sorptive solute transport lagrangian based model heterogeneous porous media sensitivity analysis multiscale data availability data will be made available on request 1 introduction aquifers exhibit heterogeneities in physical and geochemical sediment properties e g hydraulic conductivity k and sorption distribution coefficient k d which can significantly impact flow and solute transport processes bianchi et al 2011 de barros et al 2012 puyguiraud et al 2020 sanchez vila et al 2006 sun et al 2008 the spatial variation of physical and geochemical attributes has been shown to be responsible for the spatial and or temporal scale dependence of transport parameters such as dispersivity mixing and retardation factor cirpka et al 2011 deng et al 2013 connolly and johns 2016 soltanian et al 2015b 2020 to practically deal with scale dependent transport parameters different upscaling methods have been proposed to study the solute transport processes by incorporating small scale hydrogeological information dagan et al 2013 dentz et al 2011 rubin 2003 scheibe et al 2015 zhang et al 2021 2022a among them the stochastic averaging method by using lagrangian based theory developed by dagan 1984 has been widely used the theory has gone through several phases of evolution soltanian et al 2015c 2015d ritzi and soltanian 2015 recent modifications have allowed the incorporation of the physical attributes of sediments e g volume proportions and mean lengths in the derivation in order to provide a direct link between the observed geology and transport observables soltanian et al 2015a more importantly for the multiscale aquifer architecture shown conceptually in fig 1 similar to dai et al 2004 deng et al 2013 where facies types defined at the larger scale are comprising assemblages of facies types at a smaller scale the theory allows separating and studying the effect of each observables scales on solute spreading and mixing dai et al 2004 2020 ramanathan et al 2008 ritzi and soltanian 2015 however parameter uncertainties impact the performance of lagrangian based transport models lbtm for example there are uncertainties in volume proportions and mean lengths which are calculated based on the discretely distributed boreholes and limited outcrop data boreholes can provide rich facies information in the vertical direction while lateral distribution between boreholes needs to be interpolated based on certain assumptions weissmann et al 1999 he et al 2014 zhan et al 2022 this will introduce errors in calculating volume proportions and transverse mean lengths partially exposed outcrops may also lead to statistical errors in facies lengths white and willis 2000 dai et al 2005 note that the outcrops and well logs only represent small windows of subsurface analogous outcrops may not even be available for many aquifer systems the geological data estimated from discrete and uncertain data have been shown to be the main source of uncertainty in numerical simulations of flow and solute transport refsgaard et al 2012 bayer et al 2015 k is usually determined from laboratory measurements or well site aquifer tests and is sometimes converted from some physical quantities measured by surface geophysical surveys however these estimations are quite local and the accuracy of measurements needs to be improved for the site scale flow problems readers are encouraged to access the detailed information about hydrogeophysical methods and applications in estimating k summarized by rubin and hubbard 2006 the measurement of k d is closely related to the experimental conditions zhang et al 2022b consequently the retardation factor r is preferred in practice instead of extrapolating the k d obtained from a particular experimental setting moreover the data on physical and geochemical measurements are far less abundant than sedimentary architecture data and there are always expense issues sanchez vila et al 2006 riva et al 2010 soltanian et al 2015a 2015c zhang and schaap 2019 ritzi and soltanian 2015 as a result there is also a high degree of uncertainty associated with such measurements sensitivity analysis has been a powerful tool to identify the most critical uncertain parameters in hydrological modeling which can be divided into two groups local and global saltelli et al 2010 dai et al 2017 compared with local sensitivity analysis lsa there is a growing trend of using global sensitivity analysis gsa which not only provides the rank of the importance of uncertain parameters but also takes into account the whole range of model inputs and interactions between different parameters sudret 2008 herman et al 2013 song et al 2015 among various gsa methods the variance based method also called sobol method and the screening method such as the morris method are the two commonly used methods sobol 1993 morris 1991 van werkhoven et al 2008 however for a complex model the computational cost of performing gsa is a limiting factor that cannot be ignored because monte carlo mc simulation with a large number of model executions is required to attain convergence sobol 1993 van griensven et al 2006 saltelli et al 2010 dai and ye 2015 therefore a variety of methods have been developed to improve computational efficiency conducting a morris analysis first to screen out the most important parameters is a common and efficient way for the multiple inputs problem nabi et al 2021 another useful method is to build less expensive surrogate models however a relatively large number of model executions may still be required to develop accurate surrogates iooss and lemaître 2015 it has been shown that the required sample size can be reduced by selecting a suitable sampling scheme in the absence of a sufficiently inexpensive surrogate model tarantola et al 2012 renardy et al 2021 even if the required sample size cannot be reduced it may also provide additional benefits such as more robust estimation of uncertainties fast stability and convergence sudret 2008 iooss and lemaître 2015 sheikholeslami et al 2019 there has been extensive research on different sampling algorithms and their properties helton and davis 2003 tarantola et al 2012 dige and diwekar 2018 note however that all sampling schemes may also lead to inaccurate evaluation results when the sample size is small as mentioned before the available hydrogeological measurements at any certain heterogeneous aquifer are limited and sparse therefore the transport model requires uncertainty estimation due to the ubiquitous lack of parameter knowledge caused by spatial heterogeneity of physical and geochemical sediment properties in combination with incomplete characterization rubin 2003 this enables future measurements to be targeted to the most influential parameters to better utilize limited resources to reduce prediction uncertainty in flow and solute transport pan et al 2011 oladyshkin et al 2012 the sensitivity of lbtm has been investigated by several studies for instance dai et al 2004 investigated the influence of anisotropy ratio indicator scale integral scale mean conductivity on dispersion coefficient of non reactive solutes they found that the values of dispersion coefficient vary with the changes of these parameters the contrast in mean conductivity is one of the parameters that the dispersion coefficient is most sensitive and the sensitivity of indicator correlation scale is larger than the local integral scale soltanian et al 2015a further discussed the impact of heterogeneity in ln k mean variance and the integral scale of k and in ln k d mean variance and the integral scale of k d on dispersivity for the sorptive solute they concluded that the longitudinal dispersivity of the reactive solute is sensitive to changes in the mean and variance of both ln k and ln k d the integral scales of ln k and ln k d for the reactive minerals with larger volume proportion is the most sensitive parameter soltanian et al 2017 also discussed the influence of mean length and volume proportion however these discussions are all based on lsa method and they do not provide quantitative sensitivity measures for different parameters dai et al 2020 supplemented the gsa on the input parameters of the dispersivity model but their study is limited to non reactive solutes and the parameters they used were obtained from the 400 m long outcrops with the corresponding correlation scale of around 10 m in addition they set the same sampling range to facies related variables such as they used the same range of ln k for different facies in fact it is the strong difference of the physical and chemical properties that leads to the abnormal solute transport process such simple sampling range setting may miss the most important information about sensitive parameters in this paper we use a lagrangian based sorptive solutes transport model lbstm derived by soltanian et al 2015a within a gsa framework to complement our understanding of the sensitive parameters controlling sorptive solute dispersion to parameterize the model we use statistics of high resolution sedimentary architecture data and univariate statistics of k and k d from canadian force base at borden scale of hundred meter ritzi et al 2013 soltanian et al 2020 furthermore it has been studied that the geological structure drastically influences contaminant transport for larger field sites mainly through the effects of stratification and the presence of lenses with higher or lower conductivity which lead to velocity variations over several orders of magnitude de barros et al 2012 frippiat and holeyman 2008 various numerical studies have also identified that correlation length is one of the key parameters that determine solute breakthrough renard and allard 2013 willmann et al 2008 therefore in order to explore the important parameters affecting solute dispersion in natural systems characterized by layered structures or zones with distinct properties with a large correlation scale we also provide the same analysis for the qqhe site in china with scales of kilometers during the sensitivity analysis we also discuss the influence of different sampling methods on the sensitivity analysis results and evaluate the convergence of sensitivity indices the structure of this paper is as follows the lbstm used in this work that incorporates the multiscale sedimentary architecture is first reviewed in section 2 we also introduced the method for the gsa and the related convergence definitions in this section in section 3 we briefly introduce the study examples section 4 presents the results and discussion section 5 highlights the conclusions of this work based on such detailed gsa of models with physical and chemical heterogeneity we expect to gain a better understanding of the influence of hydraulic and sorption properties on solute transport and also to obtain some guidance information on data acquisition in dealing with practical problems 2 method 2 1 lagrangian based transport model following the hierarchical organization shown in fig 1 it has been shown that the solute dispersion is sensitive to the architecture of facies defined at one scale but not to the heterogeneity at smaller scales within those facies desbarats 1990 dai et al 2004 ramanathan et al 2010 at a certain scale the physical and chemical properties of different facies vary by order of magnitude and the heterogeneous k and k d fields usually exhibit multimodal distribution characters for a two facies media it is simplified to a bimodal character by following the conceptual model for heterogeneous porous media with the multimodal organization of reactive minerals soltanian et al 2015a developed the lbstm to analyze the sorptive solute dispersion undergoing equilibrium sorption the model directly relates sorptive solute dispersivity to quantifiable physical properties of the sediments such as the mean lengths and volume proportions of facies note that the models are not limited to a particular facies organization framework besides there is an assumption during the model derivation that k covaries with the mineralogic facies which controls reactivity because of this this lbstm considers both the spatial correlation structure of k and k d and their cross correlations soltanian et al 2017 have validated this model by using high resolution flow and reactive transport simulations and have investigated how the heterogeneity scales affect long term radionuclide transport at yucca mountain consider a domain ω filled with n number of facies of mutually exclusive occurrences ζ x is assumed to be a multimodal spatial random variable for y ln k or ξ ln k d at location x in the domain ω by using the indicator geostatic expression ζ x can be written as soltanian et al 2015a 1 ζ x i 1 n i i x ζ i x where ii x is an indicator variable within the domain ω and ζi x are random variables of the i th facies the ii x is defined as 2 i i x 1 if facies is i at location x 0 otherwise by using the facies volume proportion pi mean and variance of y or ξ mi and σ i 2 the expected global mean mζ and variance σ ζ 2 can be computed as dai et al 2004 huang and dai 2008 3 m ζ i 1 n p i m i 4 σ ζ 2 i 1 n p i σ i 2 1 2 j 1 n i 1 n p i p j m i m j 2 therefore the multimodal covariance function of y or ξ is 5 c ζ ψ i 1 n p i 2 σ i 2 e ψ λ i i 1 n p i 1 p i σ i 2 e ψ λ φ 1 2 i 1 n j 1 n p i p j m i m j 2 e ψ λ i where λi is the indicator integral scale and λi is the i th facies integral scale correlation length respectively they can be calculated by dai et al 2007 6 λ i i 1 n p i l i 1 p i 7 λ i l i 1 p i 8 λ φ λ i λ i λ i λ i where li is the mean length of the facies soltanian et al 2015a assumed that the effect of pore scale dispersion on the spatial moment of the sorptive plume is negligible in addition the y and ξ are correlated by ξ ay b where a and b are constants as a result for the multimodal heterogeneous sediments the final form of longitudinal dispersivity is expressed as soltanian et al 2015a 9 α 11 s t i 1 n p i 2 σ yi 2 λ i f 1 λ i i 1 n p i 1 p i σ yi 2 λ φ f 1 λ φ 1 2 i 1 n j 1 n p i p j m yi m yj 2 λ i f 1 λ i v r 3 ρ b n k d g 2 e σ ξ 2 0 t e i 1 n p i 2 σ ξ i 2 e v r λ i s i 1 n p i 1 p i σ ξ i 2 e v r λ φ s 1 2 i 1 n j 1 n p i p j m ξ i m ξ j 2 e v r λ i s 1 ds 2 r ρ b n 2 k g k d g j a sinh σ ξ σ ξ i 1 n p i 2 σ yi 2 λ i f 2 λ i i 1 n p i 1 p i σ yi 2 λ φ f 2 λ φ 1 2 i 1 n j 1 n p i p j m yi m yj 2 λ i f 2 λ i where 10a f 1 λ 1 e τ ε 0 2 rj 1 β 2 u 3 2 ε r v 2 u v 2 u 3 2 2 β 2 j 1 β β j 0 β r τ 2 ε 3 r 3 4 u v 5 v 4 u u 3 2 v 3 u 3 2 d r 10b f 2 λ 1 e τ 2 ε 2 0 r 2 1 2 vu 3 2 1 v 2 u 1 2 1 v 2 ε r j 1 β dr β rτ u 1 r2 v 1 r2 ε2r2 τ v t r λ where v is the mean flow velocity m d r is the retardation factor which is related to the k d by the relationship r x 1 ρb n k d x and r is the geometric mean of r ρb is the bulk density g cm3 n is the porosity k d g and k g are the geometric mean of k d x and k x fields respectively σ ξ 2 and σ y 2 are the variance of ξ and y j is the hydraulic gradient the term j 0 and j 1 are the zero and first order bessel functions respectively the ε is the anisotropy ratio defined as the vertical integral scale of the hydraulic conductivity to the horizontal component the above transport model can be simplified to represent non reactive solute dispersivity when ignoring the parameters associated with the sorption process thus equation 9 can be written as 11 α 11 nr t i 1 n p i 2 σ yi 2 λ i f 1 λ i i 1 n p i 1 p i σ yi 2 λ φ f 1 λ φ 1 2 i 1 n j 1 n p i p j m yi m yj 2 λ i f 1 λ i 2 2 global sensitivity analysis methods 2 2 1 morris method since the total number of model executions during the sensitivity analysis only increases linearly with the number of model parameters it can be seen that there are a large number of geological and geochemical parameters in the dispersivity model from equations 9 10 which would cause a high computational cost when performing gsa therefore the morris method morris 1991 is used first to screen the sensitive parameters the principle of this method is to perform the one at a time oat local sensitivity analysis to calculate the so called elementary effects ee which is defined as the ratio between the change of the output and the change of one input the mean μ and standard deviation σ of calculated ees are then used as sensitivity measurements however to avoid failing to identify the inputs with considerable influence on the model campolongo et al 2007 proposed to use the absolute mean value μ rather than the original μ value as the measurement by discretizing the parameter space into multiple levels and generating the input samples randomly the ee of the i th input parameter at the j th repetition is estimated as follows 12 e e i j y θ j e i y θ j β β 1 p 1 where p is the discretization level ei is a vector of the canonical base θ is one of the input parameters and y θ is the corresponding model output δ is a step length denoting n as the number of oat designs the sensitivity indices for each parameter can be obtained from statistics of ees as follows 13 μ i j 1 n e e i j n 14 σ i j 1 n e e i j j 1 n e e i j n 2 n μ i measures the influence of the i th parameter on the model output and σi measures interaction effects between parameters a high value of μ i indicates that the corresponding input parameter contributes significant uncertainty to the model output the large value of σi represents that the effect of the parameter is significant mostly because of the interactions with other input parameters 2 2 2 variance based method the variance based gsa method aims to decompose the total variance of the model output into the contribution of each single input parameter or their combinations and to use quantitative sensitivity measurements to evaluate the influence of input parameters on model output i e dispersivity in this work sobol 1993 saltelli et al 2008 suppose that f x is a mathematical model containing a set of input variables x1 xm described by a random vector x and the scalar model output is y f x the total output variance v of f x can be defined as sobol 1993 15 v y i 1 m v i i 1 m j i m v i j v 1 m where vi represents the partial variances contributed by i th parameter xi and vi j to v1 m represents the partial variances contributed by interactions among parameters according to the law of total variance the output variance can be defined as 16 v y v x i e x i y x i e x i v x i y x i the first item on the right hand side of equation 16 is the partial variance or the main effect caused by the parameter xi the inner expectation is the mean of the output calculated on all changing values that is all parameters except fixed xi from equation 16 the first order sobol indice can be expressed as 17 s i f v x i e x i y x i v y since this value evaluates the percentage of output uncertainty that is contributed by the input subset xi it is therefore a quantitative metric for the influence of the individual input parameter uncertainty source on model output sobol 1993 another commonly used sensitivity measurement is the total sensitivity indice which considers both the individual effect of xi and interactions of xi with other parameters homma and saltelli 1996 18 s i t v i v i j v i j m v y similar to equation 16 the total output variance is decomposed as 19 v y v x i e x i y x i e x i v x i y x i the first item on the right hand side of equation 19 measures the variance caused by x i from equation 19 the total sensitivity indice can be defined as 20 s i t e x i v x i y x i v y v y v x i e x i y x i v y following janssen 2013 and saltelli et al 2010 when based on a large number of model executions the mean and variance are calculated via 21 v x i e x i y x i 1 n m j 1 n f b j f a b j i f a j 22 e x i v x i y x i 1 2 n j 1 n f a j f a b j i 2 two independent parameter sample matrixes a and b are required in the calculation note that a and b should have the same dimension of nxm where n is the sample size and m is the number of input parameters matrix a b i is the same as a except that its i th column is extracted from the i th column of b a total number of 2 m xn model evaluations are required for the estimation of the sensitive indices the larger the first order sobol indice the greater the impact of the variable s change on the final output differences between total sobol indice and first order sobol indice represent the contribution of interactions between parameters if the difference is small the interactions between the variable with other variables have a small effect on the output sudret 2008 wainwright et al 2014 dai et al 2020 to attain convergence of the mc simulation the number of parameter samples that is the total number of model executions must be sufficiently large as mentioned in the introduction section different sampling schemes could help reduce the computational cost here we review three schemes used in this study van griensven et al 2006 renardy et al 2021 fig 2 shows the random sampling distribution of some sensitive parameters studied in this paper under different sampling schemes it can be seen intuitively that the sampling results are obviously different overall sobol method has a more uniform distribution of sampling points and a better control of boundaries the monte carlo method mc is the simplest sampling method to generate each sample point independently by using a random number generator however there is a risk of missing portions of parameter space fig 2a latin hypercube sampling lhs is an efficient sampling scheme based on the mc method which divides the parameter space into several equiprobable subspaces and performs mc sampling in each of them mckay et al 2000 janssen 2013 this method ensures that the entire range of each parameter can be sampled effectively fig 2b sobol sequences sobol are low discrepancy sequences also known as quasi random sequences which are deterministic sequences of numbers that converge quickly to a uniform distribution kucherenko et al 2015 previous studies have shown that these three sampling methods may perform differently for different calculation purposes and sample sizes but for most problems sobol sequences produce smaller errors and is more robust liefvendahl and stocki 2006 burhenne et al 2011 besides the computational cost of sobol sampling is relatively low only slightly higher than mc and much lower than lhs by reviewing the existing gsa literature sarrazin et al 2016 found that the previous research on the convergence of sensitivity analysis has no unified definition for different types of convergence evaluation and there is a lack of clear convergence criteria in general different types of convergence require different sample sizes which will directly affect the computational burden of the model based on this they defined quantitative criteria for different types of convergence and developed a method to quantitatively validate screening results three definitions of convergence of gsa results they proposed are see the example in fig 3 in sarrazin et al 2016 1 sensitivity indice convergence means the values of the indices remain stable 2 ranking convergence means the ordering between the parameters remains stable 3 screening convergence means the partitioning between sensitive and insensitive parameters remains stable among these convergence properties the degree of convergence from screening to sensitivity indices is getting higher and higher and more model evaluations are needed to attain convergence dai et al 2020 provided their sensitivity results with a sample size of 3000 but they did not evaluate the convergence of indices in order to make our evaluation results more accurate this paper also evaluates the convergence of the sensitivity indices in this paper gsa is performed by using the open source matlab package uqlab marelli et al 2022 this module is a powerful and flexible tool for performing sensitivity analysis with a number of different techniques the details methods and algorithms as well as the overview of the relevant up to date literature can be found in the manuals marelli et al 2022 3 study examples 3 1 borden site borden aquifer data is used to show the time evolution of dispersivity and to perform the sensitivity analysis for the sorptive solute the facies architecture of the borden aquifer consists of two levels and previous studies soltanian et al 2015c 2015d ren et al 2022 have shown that the larger scale facies heterogeneous structure is sufficient to effectively represent the dispersion process therefore we only focus on the two facies type aquifer architecture which is related to the larger scale heterogeneity of the borden site the facies consists of medium sand m and fine sand and silt fz by collecting facies samples k and k d samples from 67 boreholes see fig 1 in ritzi et al 2013 the physical and geochemical parameters of the facies types are presented in table 1 the velocity field is assumed to be uniform in average and the mean groundwater velocity v is aligned with the axis which can be calculated as 23 v k g j n thus the lagrangian velocity for sorptive solutes is expressed as follows 24 u v r where mean retardation factor r is calculated by r 1 ρ b n kd according to mackay et al 1986 the average bulk density ρ b and the average porosity n are 1 81 g cm3 and 0 33 respectively the hydraulic gradient j is 0 005 by equation 10 we can easily calculate the correlation lengths according to previous studies the proportion weighted correlation length for the system are 2 11 m and 0 12 m in in the horizontal and vertical directions dai et al 2005 ramanathan et al 2010 consequently the average anisotropy ratio ε is calculated as 0 06 3 2 qqhe site the qqhe site is located in tsitsihar heilongjiang province china which is in the middle reaches of the nen river fig 3a the nen river flows from northeast to southwest in the west of the study area controlled by topography features the groundwater flow direction in the area is basically the same as that of surface water there are huge and thick quaternary fluvial and lacustrine facies loose deposits exhibiting the multiscale sedimentary architecture this site is used to evaluate the sensitivity parameters for solute dispersion at a much larger correlation scale due to the difficulty of obtaining k d observations only the discussion related to non reactive solute dispersion is performed 57 boreholes fig 3b are used to reveal the sedimentary architecture about 50 m below ground surface similar to the facies classification of the borden site the sediments collected in the boreholes can be categorized based on a set of quantifiable textural attributes into three stratal facies the facies gc represents coarse grained media such as gravel and coarse sand facies mf stands for medium and fine sand facies sc is silts and clays by directly using facies information in discrete boreholes we first determined the volume proportions and the vertical mean length of each facies a series of facies sections along and perpendicular to the groundwater flow direction see appendix a were used to determine the facies mean length in the horizontal direction on this basis with the combination of the bayesian update algorithm proposed by white and willis 2000 and dai et al 2005 the calculated lengths were further corrected to avoid the bias of incomplete exposure pumping tests empirical methods are used to estimate k we collected 54 pumping test results to calculate k values related to medium to coarse grained media we also adopted the formula to calculate k based on deposits grain size devlin 2015 25 k ρ g μ c φ n d e 2 where μ is the temperature dependent dynamic viscosity of water g cm s ρ is the temperature dependent water density g ml g is the gravitational constant cm s2 c is an empirical factor φ n is a function of porosity and d e is an effective grain size for different applicable conditions the values of c φ n and d e are different readers are encouraged to access the details in devlin 2015 besides we also collected more data according to previously completed studies of the relationship between plasticity index and conductivity all the physical parameters of facies and the statistical information of k are presented in table 2 similar as borden site the proportion weighted correlation length for the system is 972 10 m in the horizontal direction and 3 29 m in the vertical direction and the average anisotropy ratio ε is then calculated as 0 003 the average hydraulic gradient is 0 01 and the mean porosity is 0 35 4 results and discussion 4 1 sorptive and non reactive solute dispersion fig 4 plots sorptive and non reactive solute dispersivity as a function of travel time in the borden site and cases with positive correlation a 1 and negative correlation a 1 are also shown overall the dispersivity increases monotonically with time and finally converges to a constant value when time is sufficiently large by comparing the dispersivity results of sorptive and non reactive solutes it is observed that the sorptive solute dispersivity is enhanced approximately by a factor of 2 moreover it takes a longer time for the sorptive solute to reach its asymptotic dispersivity value this nonideal transport character i e enhanced dispersion was actually observed in the borden site reactive solute transport experiment roberts et al 1986 and the interpretive modeling works bosma et al 1993 burr et al 1994 brusseau and srivastava 1997 under steady flow conditions the spatial variability in the solute velocity which is caused by variations in the k field is the principal mechanism for solute dispersion dagan 1989 dentz et al 2011 soltanian et al 2015d however the k d variability further promotes the velocity fluctuation for the sorptive solute transport which leads to the enhancement of dispersion in addition the cross correlation between k and k d appears to be a major factor in controlling the dispersion although there are still no conclusive indications of exactly what the correlation between k and k d is robin et al 1991 allen king et al 1998 ritzi et al 2013 the combined impact of these two parameters on solute transport has been intensively studied based on the assumption of linear and exponential correlations bellin et al 1993 bosma et al 1993 wu et al 2004 maghrebi et al 2013 soltanian et al 2015c from fig 4 it can be seen that the sorptive solute dispersivity is enhanced when k and k d are negatively correlated and decreased when positively correlated bellin et al 1993 through the analytical solution and bosma et al 1993 through numerical simulation pointed out that in the case of positive correlation the counteracting effects of k and k d heterogeneity lead to the decrease of dispersivity the outcome of the counteracting effects depends on the mean variance and integral scales of the spatially variable properties they also pointed out that the enhancement in negative correlation is stronger if the mean of k d is large 4 2 screening sensitive parameters for sorptive solute to reduce computational cost of the variance based gsa method we first used morris method to identify the more sensitivity parameters of sorptive solute dispersivity model this study considered all uncertainty input parameters in the dispersion model 20 in total including anisotropy ratio volume proportions mean lengths integral scale in facies mean and variance of y and ξ correlation coefficient hydraulic conductivity and porosity they can be broadly divided into three groups such as geological structure related parameters k related parameters and k d related parameters the upper and bottom boundaries and distribution types of these parameters for sensitivity analysis are listed in table 3 for the gaussian distribution type the values in square brackets represent mean and standard deviation respectively based on the time evolution results of dispersivity in fig 4 the sensitivity analysis was performed on the transport time of 1000 days when the dispersivity reached its asymptotic stable value fig 5 demonstrates the importance ranking of parameters for the model outputs in which the top ranking parameters with high mean μ and standard deviation σ values are labeled the sensitivity analysis results suggested that the parameters which control the α 11 s is mainly related to sorption distribution coefficient k d and geological structure overall α 11 s is more sensitive to parameters related to k d among them the mean sorption distribution coefficient values m ξ are the most influential factors for α 11 s the larger the m ξ is the greater the influence is in addition from fig 5 we can see that the σ values are very large indicating that the interactions between parameters have an obvious influence on α 11 s by using the morris method seven of the more sensitive parameters were finally selected for subsequent variance based sensitivity analysis which has significantly decreased the computational requirements 4 3 sensitivity analysis results of α 11 s and α 11 nr in this section we evaluate the sensitive parameters of α 11 s and α 11 nr based on parameters of the borden site a total of 13 evaluation parameters were involved in the α 11 nr model after removing the parameters associated with k d including m ξ σ ξ 2 λ ξ and a the distribution types and sampling ranges of inputs are consistent with those given in table 3 one point to add is that in order to obtain an obvious result for the α 11 nr sensitivity analysis the standard deviations of the sampling range of the mean y were set from 0 47 to 0 94 for m facies and from 0 41 to 0 82 for fz facies respectively for the α 11 s model we evaluate seven parameters screened out by the morris method we first evaluated the convergence of the sensitivity indices under different sampling sizes and different sampling schemes the results are shown in fig 6 and fig 7 in order to simplify the discussion and for a more intuitive presentation we have chosen to present the results only for two parameters with the largest total sobol indice and the parameters with a first order sobol indice greater than 0 1 the results suggest that the sobol indices for α 11 nr converge at a sample size of approximately 4000 fig 6 while the indices for α 11 s stabilize at around 10 000 sampling size fig 7 it can be concluded that although the number of parameters in the model is reduced 13 for α 11 nr and 7 for α 11 s the convergence rate of the sensitivity indices is still slower when there are strong interactions between parameters by comparing the indices results of the three sampling methods it can be seen that the sobol sequence sampling method outperforms in terms of convergence rate and in terms of accuracy which is directly related to its more uniform distribution interestingly we noted that the total sobol indice for α 11 s is more volatile than the results for the first order sobol indice and all of the indices for α 11 s are more volatile than those for α 11 nr as discussed in section 4 1 the k d variability promotes the velocity fluctuation of sorptive solute and thus promotes solute dispersion regardless of the correlation between k and k d therefore under the strong influence of k d related parameters the variability of α 11 s was enhanced exhibiting greater fluctuations in the sensitivity indices shown in fig 7 as shown in fig 5 the standard deviation value is large indicating that the interaction between parameters is strong so the total sobol indice fluctuates more and it is more difficult to achieve convergence the results further indicate the importance of first verifying the convergence of the sensitivity indices when conducting sensitivity analysis to obtain reliable results fig 8 plots the sobol indices results for α 11 nr obtained by using sobol sampling method under the convergence condition in previous studies the effects of the indicator integral scale on α 11 nr were discussed by soltanian et al 2015a and dai et al 2020 in particular the study by dai et al 2020 concluded that the indicator integral scale is the most sensitive parameter to α 11 nr with an uncertainty contribution of approximately 0 42 to the model output however as shown in equation 6 this parameter is determined by the volume proportions and the mean lengths of facies types therefore this paper directly incorporates proportions and lengths into the sensitivity analysis instead of using the integral scale as shown in fig 8 α 11 nr is most sensitive to the mean of y with a total uncertainty contribution of approximately 0 7 to the model output and the sedimentary structure parameters e g proportions and lengths are the second part of sensitive parameters followed by the variance of y we think that the reason for the discrepancy with the results of dai et al 2020 is that the indicator integral scale is a more comprehensive structure parameter this parameter reflects the joint influence of proportions and lengths with the exception of this parameter the sensitivity ranking of the remaining parameters is identical furthermore by combining actual site measurements and assigning different sampling ranges to facies related variables such as mean y for different facies our results more accurately capture the parameters that need the most attention in the simulation process than dai et al 2020 who assigned the same sampling ranges for analysis at the same time our results also deepen the understanding of the role of model parameters for example in general the parameters associated with highly permeable facies are more sensitive to α 11 nr in complex heterogeneous structures the interconnected architecture relative abundance and geometry of both high and low permeable facies affect solute transport significantly zhang et al 2013 in particular proportion and mean length of facies can affect water and solute transport by forming so called preferential flow pathways the effects of these two parameters on solute breakthrough curves travel time and diffusion characters have been extensively and detailly evaluated sivakumar et al 2005 zhang et al 2013 yin et al 2020 however there is still a lack of understanding of the dominant parameter our results show that the mean length of the high permeability facies m has the largest influence on α 11 nr followed by the proportion generally the proportion value can be well estimated by boreholes or outcrops therefore this result further reflects the importance of obtaining accurate facies length for dispersivity estimation note that the sensitivity contribution of the above mentioned parameters all comes from the difference between the total and the first order sobol indice indicating that the interactions between parameters affect α 11 nr a lot especially the interactions between mean y and other variables have the largest interaction effect correlation length and variance of y have weak impaction on α 11 nr and the sensitivity contribution mainly comes from the first order sobol indices indicating a strong direct effect on α 11 nr the total and first order sobol indices of anisotropy ratio porosity and mean length of fz facies are close to zero indicating that the change of these parameters has an insignificant impact on α 11 nr fig 9 plots the sobol indices results for α 11 s the mean of ξ is the most sensitive parameter and the variance of ξ seems to be the second sensitive parameter followed closely by the volume proportions and the m facies mean length soltanian et al 2015a pointed out through their lsa that σ ξ 2 is more sensitive to α 11 s than σ y 2 especially for multimodal media the larger the volume proportion of the facies the more sensitive σ ξ 2 is our results are consistent with this conclusion different from α 11 nr the sensitivity contribution of parameters all comes from the difference between the total and the first order sobol indice indicating that the interactions between parameters have a large effect on α 11 s one can see from equation 9 that a large number of multiplications integrations and exponential operations are involved in the calculation of the α 11 s so the interactions between the parameters are bound to be strong compared with the sedimentary structure parameters it is not easy to obtain the k data and it is even more difficult to estimate the adsorption parameters soltanian et al 2017 2020 although it is concluded in section 4 1 of this paper and previous studies that the correlation between k and k d seems to have a significant impact on the estimation of dispersivity it can be seen from the sensitivity analysis results that the accurate measurement of the k d data is the most important factor 4 4 effect of heterogeneity scale different hydrogeologically relevant scales control the scale dependence of k and k d thus impacting the solute transport processes ritzi and allen king 2007 ritzi and soltanian 2015 to determine whether the sensitive parameters controlling solute transport are the same in heterogeneous aquifers at different scales we performed the gsa at the qqhe site and then compared the results with the borden aquifer there is a total number of 18 parameters for sensitivity analysis and the distribution type and sampling ranges are given in table 4 fig 10 is the convergence plots of the total and first order sobol indices of the non reactive transport model it can be seen that the sensitivity indices reach rank convergence when the sample size reaches 6000 similarly the sensitive indices curves fluctuate less compared with fig 7 despite the significant number of input parameters fig 11 shows the total and first sobol indices for α 11 nr in contrast to the results obtained at the borden site j becomes the most sensitive parameter to α 11 nr followed by facies proportion and then the mean of y according to table 1 and table 2 the proportion averaged horizontal integral scale is 2 11 m for the borden site and 972 10 m for the qqhe site it is thus clear that the dominant factor of non reactive solute transport in heterogeneous aquifers is quite different at the site scale and regional scale there have been studies to investigate the effect of structures and dynamics on solute transport in heterogeneous media edery et al 2014 pauloo et al 2021 they concluded that transport cannot be explained solely by the structural knowledge of the medium dynamic flow controls are also critical factors especially for regional scale study this is because preferential flow along connected high k networks causes increased spatial spreading along the mean flow direction for the horizontal flow at the same time the mass transfer process in low k material shifts from diffusion and slow advection dominance to advection dominance therefore under a larger heterogeneous scale 102 103 the controlling factor for dispersivity is hydraulic note that around 70 of the total sobol indice comes from first order sobol indices indicating that the interaction of the hydraulic gradient with the other parameters has a weak effect on solute dispersivity it must be noted however that the effect of sedimentary structure and k heterogeneity are still important the larger the my is the greater the influence is besides the sensitivity contribution of my mainly comes from the first order sobol indice compared to fig 8 proportion accounts for more uncertainty contribution than my reflecting the strong control of heterogeneous structure on solute dispersion 5 conclusions we perform the global sensitivity analysis based on the lbstm developed by soltanian et al 2015a to have a better understanding of the impact of each parameter and the interactions between parameters on solute transport two sites were selected for the sensitivity analysis the first one is the borden site with an integral scale of about 100 101m at this site we quantified the sensitivity of the sorptive solute transport model undergoing equilibrium adsorption and non reactive solute transport model to parameters within possible ranges another site is the qqhe site with an integral scale of about 102 103m this site was used to investigate whether the sensitive parameters controlling solute transport are the same for heterogeneous aquifers at different scales during the process of sensitivity analysis we also conducted the convergence analysis of sensitivity indices and discussed the impact of different sampling methods on the sensitivity analysis results the major conclusion of this study are as follows 1 the sampling size required to attain the ranking convergence of sensitive indices will increase with the increase of the number of parameters however a more direct influencing factor is the complexity of the model the more complex the model more interactions between parameters the larger the sample size required in addition sobol sequence sampling scheme outperforms in terms of convergence rate and in terms of accuracy instead of mc and lhs sampling schemes 2 when the integral scale of the heterogeneous field is about 100 101m the sorptive solute dispersivity α 11 s is most sensitive to the mean adsorption coefficients k d followed by the variance of adsorption coefficient and then the volume proportions and mean lengths of facies the non reactive solute dispersivity α 11 nr is most sensitive to the mean conductivities followed by the proportions and mean lengths of facies and then the variance of conductivity and integral scales overall α 11 s is more sensitive to parameters related to k d α 11 nr is more sensitive to parameters related to k 3 in this study the possible range of each parameter is assigned according to the measured data of the site at the same time the use of intermediate variables is avoided for example the indicator integral scale which can be calculated by the volume proportions and the mean lengths of the facies will not participate in the sensitivity analysis such a parameter set up ensures that we can evaluate the sensitivity parameters of α 11 r and α 11 nr from the perspective of the simplest physical and chemical heterogeneity and also provides an insight into the solute transport process and related controlling parameters 4 when the heterogeneity integral scale increases to 102 103m α 11 nr is most sensitive to the hydraulic gradient followed by the volume proportions and mean lengths of facies and then the mean conductivities in other words under this condition the flow velocity fluctuation caused by the heterogeneity of conductivity will no longer be the main factor controlling solute transport but will be replaced by hydraulic gradient and heterogeneous sedimentary architecture sensitivity analysis is of considerable practical interest in subsurface contaminant hydrology the dynamic components of solute transport in groundwater are strongly influenced by the underlying heterogeneity of the aquifer including not only the sedimentary architecture but also the hydraulic properties although substantial progress has been achieved in solute transport in heterogeneous aquifers in recent decades one of the most serious drawbacks is still the lack of sufficient geological data through our study we are expecting to provide the direction of reducing the model uncertainty to enhance our understanding of the transport mechanisms and to identify the controlling factors the analysis of the independent and interaction effects of parameters is also expected to provide a scientific basis for the study of fluid flow and mass transport upscaling credit authorship contribution statement wanli ren formal analysis data curation investigation visualization writing original draft heng dai conceptualization methodology software validation funding acquisition songhu yuan writing review editing zhenxue dai writing review editing ming ye methodology writing review editing mohamad reza soltanian conceptualization methodology writing review editing supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is funded by the national natural science foundation of china no 42172280 no u2267217 no 42141011 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2023 129274 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2097,the multiple flow paths existing in urban environments lead to complex flow fields during urban flooding modelling these flow processes with three dimensional numerical models may be scientifically sound however such numerical models are computationally demanding to ascertain whether urban floods can be modelled with faster tools this study investigated for the first time the capacity of the 2d shallow water equations swe in modelling the flow patterns within and around urban blocks with openings i e involving flow exchanges between the flows in the streets and within the urban blocks e g through alleys leading to courtyards or through broken windows or doors laboratory experiments of idealized urban floods were simulated with two academic 2d swe models with their most notable difference being the parameterization of the eddy viscosity specifically the first model had a turbulence closure based on flow depth and friction velocity while the second model had a depth averaged k ε turbulence closure thirteen urban layouts were considered with steady flow and five with unsteady flow both models simulated the flow depths accurately for the steady cases the discharge distribution in the streets and the flow velocities were predicted with lower accuracy particularly in layouts with large open spaces the average deviation of the modelled discharge distribution at the outlets was 2 5 and 7 3 for the first and second model respectively for the unsteady cases only the first model was tested it predicted well the velocity pattern during the falling limb of a flood wave while it did not reproduce all recirculation zones in the rising limb the peak flow depths in the streets and the peak discharges at the outlets were predicted with an average deviation of 6 7 and 8 6 respectively even though some aspects of the flow in an urban setup are 3d the findings of this study support the modelling of such processes with 2d swe models keywords experimental hydraulics numerical modelling open channel flow shallow water equations turbulence urban flood data availability data will be made available on request 1 introduction urban flood risk is a growing concern addison atkinson et al 2022 chen et al 2015 doocy et al 2013 given the high urbanization rate birkmann et al 2016 chen et al 2022 gross 2016 and the intense anticipated rainfall events due to climate change hettiarachchi et al 2018 pfahl et al 2017 sanderson et al 2019 the flood risk mapping of an urban area remains a challenging task due to the variability in the direct and indirect flood impacts kreibich et al 2014 and in the flood vulnerability chen et al 2019 huggel et al 2013 lv et al 2022 associated with various socioeconomic contexts in different parts of a city as well as due to intricate urban layouts that induce complex flow patterns influencing the flood hazard leandro et al 2016 li et al 2021a lin et al 2021 urban flood numerical modelling is a vital component of flood risk assessment rosenzweig et al 2021 and management guo et al 2021 jongman 2018 and supports design strategies for sustainable and resilient urban infrastructures qi et al 2022 zhou et al 2018 contrary to one dimensional 1d kitsikoudis et al 2020 and 1d 2d bates 2022 simplifications that can be made in river modelling aiming mostly at estimating inundation extents numerical modelling of multidirectional flows in flooded urban areas should be at least 2d li et al 2021a mignot et al 2006 with a focus on the spatial distribution of not only flow depths but also flow velocities kreibich et al 2009 and specific discharges costabile et al 2020 to express the flood hazard degree in the street network this is particularly true for large impervious surfaces upstream of and in urban areas that can lead to an excessive amount of runoff which cannot be conveyed by the drainage systems such high flow discharges may threaten the stability of pedestrians arrighi et al 2017 bernardini et al 2020 postacchini et al 2021 xia et al 2014 and can cause the entrainment of vehicles martinez gomariz et al 2018 smith et al 2019 xia et al 2011 hence the accurate spatial quantification of hydraulic variables within an urban area is of utmost importance 1 1 role of laboratory experiments for model validation a large number of numerical modelling studies simulated urban flows in real world cases guo et al 2021 luo et al 2022 with some of them using lidar data with high resolution digital elevation models of the urban topography almeida et al 2018 ozdemir et al 2013 yalcin 2020 however validation field data including both flow depths and velocities are usually lacking or insufficient costabile et al 2020 which may lead to equifinality issues remote sensing techniques can provide inundation extents and water levels although with certain limitations as tall buildings within the urban environment may obscure some measurements neal et al 2009 but flow velocity measurements in urban floods are more challenging such measurements are dangerous and can be costly and as a result are limited brown and chanson 2013 flow depths and surface velocities can alternatively be determined by monitoring parts of a flooded urban area with unmanned aerial vehicles perks et al 2016 and by analyzing existing footage and crowdsourced data from flooded street networks mignot and dewals 2022 re et al 2022 however there are uncertainties related to the boundary conditions in complex urban terrains with large spatial variability and to the interplay between surface flow and flow in underground drainage systems bazin et al 2014 chang et al 2018 kitsikoudis et al 2021 rubinato et al 2022 finally the typically short duration of pluvial flooding and its local character do not allow for detailed measurements over long durations experimental measurements in laboratory facilities provide an alternative option for models validation in carefully designed experiments the flow and boundary conditions can be accurately controlled mignot et al 2019 and besides offering a better understanding of the governing physical processes such studies can contribute to the validation of numerical models which may subsequently be used for scenario analyses of field cases 1 2 performance of 2d shallow water models the 2d shallow water equations swe can be used to simulate the flow in flooded streets with typically large width to depth ratios however at street intersections the interacting flows coming from various branches generate complex patterns mignot et al 2008 and 3d flow structures el kadi abderrezzak et al 2011 ramamurthy et al 2007 while 3d models can capture most features of diverging flows in bifurcations mignot et al 2013 neary et al 1999 ramamurthy et al 2007 and converging flows in junctions huang et al 2002 luo et al 2018 schindfessel et al 2015 it is important to examine whether these flow processes can be satisfactorily reproduced by 2d operational models that are much faster than 3d models and can be used for real time modelling the 2d swe approach has been proven capable to replicate experimental measurements of flow depths and discharge partitioning in bifurcations bazin et al 2017 el kadi abderrezzak and paquier 2009 khan et al 2000 li et al 2021b shettar and murthy 1996 in junctions li et al 2021b in crossroads mignot et al 2008 as well as in larger and more complicated street networks such as that of arrault et al 2016 with 49 intersections and that of li et al 2021b with four intersections li et al 2021a incorporated various urban layouts in their experimental setup and also modelled successfully the flow depths and discharge partition with a 2d swe model despite the successful applications of 2d swe in modelling water surface profiles and discharge distributions some open questions remain li et al 2020 regarding the accuracy of 2d swe in predicting flow velocities in intersections the extents of recirculating flow areas occurring due to flow separation in some of the branches and the role of the turbulence closure model rodi 2017 shettar and murthy 1996 modelled depth averaged flow velocities in a bifurcation with a k ε turbulence closure and their modelled velocities in the main channel and the length of the recirculation zone agreed well with the experimental measurements however their modelled velocities in the branch of the bifurcation were less accurate khan et al 2000 also modelled the flow in a bifurcation but with a mixing length model and reported that the modelled depth averaged velocities compared well with the measurements while the dimensions of the recirculation zone were predicted by the model satisfactorily bazin et al 2017 used a constant eddy viscosity model to simulate flows in a bifurcation with a branch with a 90 degree angle with and without obstacles at the intersection and the modelled depth averaged flow velocities in the recirculation zone on the upstream side of the bifurcation branch deviated from the measurements bruwier et al 2017 argued that a k ε turbulence closure model should be more suitable than a constant eddy viscosity model for modelling flow interactions in intersections given that since a k ε model does not necessarily require calibration its computational demand can be similar to a constant eddy viscosity model that requires calibration arrault et al 2016 showed in a more complex setup that the turbulence closure model was not particularly influential in the estimation of discharge distribution in the various streets however a k ε turbulence closure model modified significantly the estimates of the recirculation lengths compared to a simulation without a turbulence model no velocity measurements were available however to compare the modelled velocities more recently li et al 2021a modelled depth averaged velocities in an urban district with various urban forms with a k ε turbulence closure model and achieved good agreement with surface velocities in areas of flow contraction however the results were less accurate in large open areas supercritical bazin et al 2017 mignot et al 2008 and transcritical el kadi abderrezzak et al 2011 flows in crossroads may pose additional challenges in 2d swe models since the occurrence and structure of hydraulic jumps can significantly affect the discharge partitioning and water surface profiles 1 3 flow intrusion into buildings an extra challenge numerical and experimental studies of urban flooding typically consider flow around non porous residential blocks haltas et al 2016 van emelen et al 2012 however in reality urban blocks may have corridors leading to backyards while during intense flooding windows and doors labeled as openings from now on of buildings may break leading to lateral flow exchanges between a street and the inside area of the buildings mignot et al 2020 causing significant damages in their interiors dottori et al 2016 martinez gomariz et al 2021 mejia morales et al 2021 conducted a systematic experimental analysis of the effect of the location and size of openings in an urban block located within an idealized urban district they showed that the flow exchanges between the streets and the block interior can alter the flow depth and the flow velocity in the surrounding streets by 12 and 70 respectively when compared to a reference case with a non porous block besides the recent study of mejia morales et al 2021 there is only a limited number of studies that investigated how the porosity of urban blocks affects the hydraulic characteristics of a flood mignot et al 2020 measured the flow discharge entering a building through an open door window or gate in case of an urban flood and they noticed that in some cases the intruding discharge can be approximated by formulas for side weirs however the authors also observed that this intruding discharge can be significantly affected by surrounding urban obstacles wüthrich et al 2020 showed with a flume experiment how the hydrostatic force and the form drag exerted by a steady flow on a building are modified by the porosity and the orientation of the building while sturm et al 2018 measured the flood impact forces on physical models of buildings with openings on a torrential fan in other experiments liu et al 2018 showed how the orientation of a house with respect to the incoming flow affects the forcing on the house door for a dam break case and zhou et al 2016 found differences in the wakes of simplified porous and non porous buildings in a numerical study of a torrential flood gems et al 2016 modelled how the different openings of a building affect the flow pattern within its interior the associated hydrodynamic forcing and the near building flow pattern the findings of these studies show that the openings in buildings affect the spatial distribution of flood hazard and thus the number and types of openings should be considered in flood modelling 1 4 objective of the study the flow exchanges between a street and the interior of a building in combination with bifurcations and junctions at crossroads lead to complex and potentially 3d flow patterns around urban blocks during urban floods since urban areas are typically densely populated there is a need for fast computational tools that could be utilized for real time modelling of not only the flow depths but also the flow velocities for the accurate estimation of the flood hazard 3d numerical models can potentially capture the flow processes of urban floods however they are computationally demanding and slow for real time modelling in practice the 2d swe are used for operational flood hazard and risk modelling while previous studies have already analysed the ability of the 2d swe to simulate flow fields in various settings such as bifurcations junctions 4 branch crossroads and street networks they all assumed that the street boundaries i e building facades were impervious no existing study has focused on the performance of the 2d swe to predict the flow intrusion into flooded buildings or building blocks nor on the flow patterns in the streets and within the urban blocks in urban configurations with openings in the building facades the objective of this study is to examine for the first time whether the flow patterns within and around porous urban blocks i e with openings can be quickly and accurately predicted with numerical modelling based on 2d swe and to determine what is the most effective modelling strategy for the accurate estimation of flow velocities and flow depths to this end the experiments of mejia morales et al 2021 2023a for flow around and within a porous urban block are replicated using two different academic numerical modelling tools to investigate the importance of eddy viscosity parameterization on the accuracy of the models complementary steady flow experiments with additional geometric configurations are also presented for the first time based on the same experimental approach as mejia morales et al 2021 the paper is organized as follows in section 2 the experimental procedure is briefly described and the numerical models are presented the new experimental results and the results of the numerical modelling are presented and discussed in section 3 finally conclusions are drawn in section 4 2 experiments and numerical modelling this section presents the experimental setup section 2 1 the various porous urban block configurations that were tested section 2 2 the numerical models that were used to simulate the experimental data section 2 3 and the prescribed boundary and initial conditions section 2 4 both steady and unsteady flow conditions were simulated with the numerical models for steady flow conditions the experimental data are a combination of the data presented by mejia morales et al 2021 and new data collected from the same urban physical model in the same facility for unsteady flow conditions the experimental data of mejia morales et al 2023a are used only a brief overview of the experimental setup and methods is provided here since they are described in detail in the aforementioned papers 2 1 experimental setup mejia morales et al 2021 2023a experimentally investigated urban floods at the city block scale using a physical model of a rectangular urban block surrounded by four streets under steady fig 1 a and unsteady fig 1b flow conditions for the steady flow experiments the length of the two streets in the x direction named right street and left street was 5 4 m and the length of the two streets in the y direction named downstream street and upstream street was 3 2 m all four streets had the same rectangular cross section with a width b 0 15 m the experimental setup for the unsteady flow experiments was the same except for the initial part of the left street which was closed upstream of the upstream street fig 1b the physical model had a slope s 0 x 0 12 in the x direction and s 0 y 0 in the y direction whereas the bed of the model was constructed with pvc and the sidewalls of the streets and the urban block were constructed with plastic various configurations of the urban block were tested section 2 2 and fig 2 however its total lengths in the x and y directions remained fixed at l x 1 56 m and l y 0 96 m respectively the thickness and the height of the walls of the porous block were 2 cm and 15 cm respectively the model inlets were located at the upstream ends of the streets in the x direction as such the steady flow experiments had two inlets with fixed inlet discharges q in 1 and q in 2 for the right street and left street respectively while for the unsteady experiments discharge was fed only through the right street since the upstream reach of the left street was closed the inlet discharges were set using separate valve flowmeter systems with a precision of 3 smooth inlet conditions were secured by placing a plastic honeycomb grid at the point entrance of the right street and of the left street each one of the four streets of the physical model had an outlet with a vertical tail weir that regulated the flow depth for the steady flow cases the weir height of outlet 1 in the right street was 4 cm and of outlet 2 in the left street was 3 cm with respective outlet discharges q out 1 and q out 2 in the two streets in the y direction the outlet 3 in the downstream street and the outlet 4 in the upstream street had the same 3 cm weir height with outlet discharges q out 3 and q out 4 respectively for the unsteady flow cases the weir height was set to zero in all outlets to avoid the reflection of the floodwaves on the weir the outflow discharges at the four outlets were monitored using electromagnetic flowmeters specifically the water overflowing the weir in each outlet was collected in a separate tank and subsequently the flow exiting each tank was measured with an optiflux 2000 flowmeter manufactured by krohne the flow depths in the physical model were measured using ultrasonic distance measuring sensors baumer undk 20i6914 s35a with a 0 65 mm uncertainty for the steady flow cases a sensor was attached on a mechanical gantry system that allowed horizontal movement with measurements being taken every 5 cm along the longitudinal direction of each street and at three locations across the street width with 6 5 cm spacing flow depth measurements within the porous urban block were conducted every 12 cm in both x and y directions each depth measurement was conducted with a sampling frequency of 50 hz for a duration of 50 s mejia morales et al 2021 for the unsteady flow cases flow depths were measured at the eleven locations depicted in fig 1b for the whole duration of each hydrograph the reported flow depths are the results of ensemble averaging of 50 identical floodwaves that were fed sequentially into the model with a steady base flow separating two sequential floodwaves the number of required repeated floodwaves was selected by increasing the number until the ensemble average standard deviation of the flow depth became smaller than 1 mm the floodwaves characteristics are detailed in section 2 4 for the steady flow cases surface flow velocities were measured using large scale particle image velocimetry lspiv fujita et al 1998 floating wood shavings 1 4 mm were used as tracers a panasonic hc v770 camera was positioned 2 8 m above the physical model monitoring the plan view at a rate of 25 frames per second with a resolution of 1920 px by 1080 px the time averaged surface velocities estimated by the lspiv technique stabilized after different periods of time for the various areas of the model but none of them exceeded 60 s mejia morales et al 2021 more details about the seeding of the flow the flow monitoring the data post processing and a validation of the lspiv measurements against measurements with an acoustic doppler velocimeter adv are provided in mejia morales et al 2021 for the unsteady flow cases it was not feasible to monitor the flow velocities in the whole flow area only the surface velocities within the porous block and at two points in the right street and left street shown in fig 1b were monitored moreover ensemble averaging was not used for the lspiv due to prohibitive post processing load mejia morales et al 2023a a sony zv 1 camera with a sampling rate of 25 frames per second was used and the collected frames were averaged over periods of 2 s to filter the data 2 2 urban block configurations in every experiment the urban block was in the same position near the downstream end in the x direction and had the same dimensions l x and l y fig 1 however the conveyance porosity i e the porosity of each sidewall of the urban block ψ as defined by the number and locations of openings differed in each experiment each opening had a width lop 6 cm and each sidewall of the block had no more than three openings in all tests the water surface elevation remained lower than the height of the openings in the present paper three series of configurations for the porous block are examined fig 2 the first series comprises the eight configurations presented by mejia morales et al 2021 without obstruction within the block fig 2a the conveyance porosity of each configuration is presented as cxx yy where xx and yy denote the ratio of the total length of the openings in a side of the porous block to the length of that side in percent in the x and y directions respectively the locations of the openings in the configuration with the largest conveyance porosity c19 12 are shown in fig 1a the conveyance porosity in the rest of the configurations is determined by closing some of the openings of c19 12 while maintaining symmetry in the porous block openings the second series comprises five new configurations constructed and tested with the same experimental approach as mejia morales et al 2021 also without obstructions within the block fig 2b the common trait of these configurations is that each configuration has four openings in its perimeter the remaining ones after blocking eight openings in c19 12 shown in fig 1a since there is no symmetry in every configuration these configurations are simply named c1 c5 in order of appearance the configurations in the third series presented in mejia morales et al 2023a have one opening in the middle of each wall of the block and a non porous rectangular obstacle in the center of the block the footprint area of this obstacle was varied as shown in fig 2c leading to an areal porosity ϕ for each case that is determined as the ratio of the empty area within the block to its total internal area note that the concept of porosity is introduced here for the sole purpose of providing a macroscopic description of the considered geometric layouts fig 2 while the flow models used in this study are not porosity shallow water models e g dewals et al 2021 they aim to fully resolve the flow field on the considered computational mesh the first and second series were used with steady flow conditions while the third series was used with both steady and unsteady flow conditions details about the upstream boundary conditions of each case are presented in section 2 4 the physical models were designed by assuming a geometrically distorted scale with horizontal and vertical scale ratios equal to 50 and 10 respectively this means that a studied flow in the physical model may be interpreted as a representation of a real world flow in streets with 7 5 m in width around an urban block with dimensions 78 m 48 m and openings 3 m wide the upscaled studied flow depths are around 60 cm this approach ensures relatively large depths in the physical model to enable a satisfactory measurement accuracy heller 2011 li et al 2021b 2 3 numerical modelling the laboratory experiments were simulated using two academic numerical codes that solve the 2d swe equations the two models have differences in their mathematical formulation and their numerical discretization the first model is implemented in the software rubar20 mignot et al 2008 developed by the riverly research unit of inrae in lyon and the second one is implemented in wolf 2d erpicum et al 2009 developed by the hece group at the university of liege table 1 provides an overview of the characteristics of each model referred to as model 1 for rubar20 and model 2 for wolf 2d the steady flow cases were simulated with both numerical models while only model 1 was used for the simulation of the unsteady flow cases 2 3 1 governing equations the two codes solve the conservative form of the 2d swe which means that the main unknowns are the flow depth h and the specific discharges hu and hv with u and v denoting the depth averaged flow velocities along the x and y direction respectively the 2d swe in conservative form are formulated as in eqs 1 3 wu 2008 1 h t h u x h v y 0 2 h u t x h u 2 g h 2 2 h u v y τ bx ρ 1 ρ h τ xx x 1 ρ h τ xy y 3 h v t h u v x y h v 2 g h 2 2 τ by ρ 1 ρ h τ xy x 1 ρ h τ yy y where g is the acceleration of gravity ρ is the water density t is the time τ xx τ yy and τ xy are the depth averaged stresses comprising both the reynolds and molecular stresses erpicum et al 2009 and τ bx and τ by are the bed shear stresses in the x and y direction respectively calculated from eqs 4 and 5 in line with camnasio et al 2014 4 τ bx ρ f u u 2 v 2 8 5 τ by ρ f v u 2 v 2 8 where f is the darcy weisbach bed friction coefficient the darcy weisbach formulation is used in both models but the friction coefficient f of the bottom and side walls is estimated by the colebrook white formula eq 6 idel cik 1969 in model 2 and by its explicit equivalent formula eq 7 yen 2002 in model 1 6 1 f 2 log k s 14 8 h 2 51 re f 7 f 1 4 log k s 12 h 6 79 re 0 9 2 where k s is the roughness height and r e is a reynolds number r e 4 u 2 v 2 h ν with ν the kinematic viscosity of water although both models were derived by depth averaging the reynolds averaged navier stokes equations together with boussinesq s assumption for expressing the depth averaged turbulent stresses they differ by the type of turbulence closure used model 1 is based on a turbulence closure in which the eddy viscosity ν t is estimated by elder s formula ν t λ h u with u the friction velocity computed from the free surface slope and λ a parameter set by the user with a default value of 1 mejia morales et al 2020 in model 2 a two equation turbulence closure is implemented it consists in a two length scale depth averaged k ε turbulence model as detailed by erpicum et al 2009 and camnasio et al 2014 2 3 2 numerical discretization in both models the computational domain was meshed with a cartesian square grid aligned with the street sidewalls depending on the model run the grid spacing δ x was varied between 5 mm and 30 mm with the resulting ratio of the grid size to the length of one opening in the porous block l op ranging from 1 2 to 1 12 both models are solved with a finite volume technique in model 1 a godunov type scheme is used mignot et al 2008 while model 2 is based on a flux vector splitting technique erpicum et al 2010 in both models the variables at the cell edges are evaluated from a linear reconstruction achieving second order accuracy in space for steady flow calculations the models were run in unsteady mode until a steady state was reached the time step used in the simulations was of the order of 10 3 s as it was constrained by the courant friedrichs lewy cfl stability condition in both models the cfl number was set at 0 5 the computational time necessary to reach convergence towards a steady state varied with the considered geometric configuration and initial conditions it was generally of the order of an hour on a standard desktop 2 4 boundary and initial conditions the computational domain was delimited by three types of boundaries sidewalls inlets and outlets at each sidewall the component of the specific discharge normal to the sidewall was set to zero at the inlets the specific discharge in the streamwise direction was prescribed and the normal component of the specific discharge was set to zero the two inlets that are considered in the left street and right street were positioned at a distance of 2 94 m upstream of the uppermost street intersections fig 1 i e at the location of the honeycomb grid at the entrance of each street in the experiments for the steady flow cases in the first and second series of tests fig 2a and b steady inflow discharges were prescribed q in 1 4 5 l s and q in 2 2 0 l s fig 1 in consistency with the measured values for the unsteady flow cases in the third test series fig 2c the inflow discharge was fed only through the right street as a sequence of 50 consecutive identical flood waves three different floodwaves were tested fig 3 and each one was examined separately each floodwave had the same peak flow of 5 l s fig 3 but was characterized by a different unsteadiness degree mejia morales et al 2023a the floodwaves were distinguished based on the discharge rising time the discharge falling time and the total volume of floodwater while their names were formed by using an l or an s for large and small magnitude for each one of the floodwave characteristics respectively for example h lss denotes a hydrograph with large discharge rising time small discharge falling time and small total volume of floodwater as a reference case steady flow experiments with inlet discharge of 5 l s i e equal to the peak of the floodwaves through the right street were also carried out in the geometrical setup of test series 3 fig 1b with the urban blocks of fig 2c at the outlets the outflow discharge was prescribed as a function of the computed flow depth the outlet boundaries were positioned as follows fig 1 in the right street and the left street at a distance of 0 6 m downstream of the easternmost street intersection in the upstream street and the downstream street at 1 94 m downstream of the northernmost street intersection for test series 1 and 2 fig 2a and b the outflow discharge q 0 in each outlet was determined from the following weir formula e g roger et al 2009 8 q 0 l c d 2 g h w 3 where l is the weir length c d is the discharge coefficient and w is the weir height the implementation of eq 8 is slightly different in the two models in model 1 the value of l is set equal to the mesh size and distinct values of q 0 are computed at each cell edge along the outlet boundary as a function of the flow depth computed at the relevant cell in model 2 the length l is taken equal to the actual weir length i e the street width b and a single value of q 0 is evaluated assumed uniformly distributed over the weir length as a function of the average of the computed flow depths over the cells next to the outlet boundary for test series 3 fig 2c the downstream boundary condition was set to critical flow for all the edges of an outlet because the flow goes directly from the street to the outlet tank without a weir in the steady flow runs of model 2 the initial condition was either a converged solution from a previous run or a calm body of water with an initial flow depth equal to 0 05 m for model 1 the initial condition for the steady flow calculations was a water level close to the experimental value and for the unsteady flow calculations was zero flow depth across the flow domain 3 results and discussion 3 1 sensitivity analysis and calibration of the numerical models model 2 was used systematically in a series of preliminary computations to assess the effect of the variation in the i grid spacing δ x ii roughness height k s iii discharge coefficient c d of the weirs at the outlets and iv initial conditions model 1 was also used in these preliminary computations but not in a systematic way moreover model 1 was used to verify whether considering a theoretical bottom topography flat bed instead of the real one influences the results these sensitivity analyses were conducted for a single geometric configuration c19 12 in fig 2a which includes the largest number of openings and leads to the most complex flow fields the comparison of the computed y i c and observed y i o hydraulic variables was carried out based on the bias and the root mean square error rmse e g chen et al 2010 9 bias i 1 n y i c y i o n 10 rmse i 1 n y i c y i o 2 n where n is the number of points where both measured and modelled data were available 3 1 1 grid spacing the grid cell size for model 2 was selected after repeating the computations for c19 12 three times with all parameters being kept the same except the grid cell size the three mesh grids that were tested had square grid cells with side length δ x equal to 30 mm 10 mm and 5 mm respectively the bias and rmse of the flow depths and velocities for different areas of the model were significantly reduced when the grid cell size was reduced from 30 mm to 10 mm but did not vary much when the cell size was further reduced from 10 mm to 5 mm fig s1a in the supplementary material fig s1a in the supplementary material also confirms the second order accuracy of the finite volume numerical scheme implemented in model 2 consistently with the linear reconstruction used in this model however the features of the simulated flow velocity patterns i e number and size of recirculating flow areas within the porous block were more consistent with the features of the measured patterns when the cell size was 5 mm fig s2a in the supplementary material even though some flow recirculations were not captured entirely therefore the 5 mm cell size was kept for the rest of the analyses with model 2 the number of cells is close to 160 000 and it varies slightly depending on the geometric configuration number of openings model 1 exhibited similar behavior with model 2 when varying the cell size with the rest of the parameters being kept the same however with model 1 the flow velocity patterns were similar for mesh sizes of 10 mm and 5 mm fig s3a in the supplementary material thus to reduce computational times the 10 mm mesh was kept for the rest of the analyses with model 1 leading to about 40 000 cells with these mesh configurations the computed flow depths exhibited a systematic bias compared to the observations which motivated the extension of the sensitivity analysis to the roughness height and the discharge coefficients of the weir outlets 3 1 2 roughness height the roughness height was taken at a small value corresponding to the pvc surface of the laboratory model the tested values of k s were equal to 2 10 4 m 8 10 5 m and 3 6 10 5 m this sensitivity analysis was conducted with model 2 with δ x 5 mm and c d 0 527 for all outlets with a previously converged flow field as initial condition the three tested values for the roughness height did not affect significantly the flow depths and velocities results fig s1b in the supplementary material nor the flow patterns fig s2b in the supplementary material the flow depth bias and rmse values for the lowest value of k s were slightly lower compared to the other k s values but at the same time the flow velocity bias and rmse values slightly increased the k s value of 3 6 10 5 m was calibrated from water surface measurements in a single street without openings considering the very small influence of the tested k s values on the simulated results with model 2 a similar sensitivity analysis was not repeated with model 1 and k s 3 6 10 5 m was used in both models 3 1 3 discharge coefficient of the weirs the computations presented in section 3 1 1 used discharge coefficients that were experimentally derived from the laboratory tests however the location where the flow depth is measured upstream of the weirs in the lab does not correspond exactly to the location where the model 2 considers flow depth for estimating the outflow discharge hence the discharge coefficient c d which lumps all flow processes in the near field of the weirs including vertical acceleration which cannot be represented explicitly by shallow water equations was recalibrated so that the computed flow depths agree on average with the observations to this end several values of c d were tested the lowest difference between modelled and measured flow depths for model 2 was obtained with c d 0 453 and thus this value was selected for the rest of the numerical simulations using model 2 for model 1 the lowest difference between modelled and measured flow depths was obtained with c d 0 467 and this value was chosen for the rest of the simulations with model 1 although a value of 0 55 for outlets 1 and 2 and 0 53 for outlets 3 and 4 led to a better distribution of the outflows this was also the case for all the urban blocks in fig 2a nevertheless the effect of c d on the street and block intrusion discharges and on the flow patterns fig s2c and fig s3b in the supplementary material is rather small the small difference between the chosen discharge coefficients for the two models may be attributed to the different ways that the downstream boundary conditions were implemented in the models and to the different turbulent closures 3 1 4 initial conditions a converged solution for a steady flow simulation may depend on the initial conditions dewals et al 2012 particularly in the presence of complex patterns of recirculating flow therefore by using model 2 for the case with the c19 12 block fig 2a we repeated the computations for two different initial conditions i the computed steady flow field obtained with the experimentally derived discharge coefficient i e a previously converged solution and ii water at rest with flow depth equal to 5 cm as expected the initial condition influenced the computed steady flow field for the flow in the porous block the results obtained when the computations were initiated with water at rest agree better with the observations fig s1c and fig s2d in the supplementary material this initial condition setting was kept for the rest of the analysis for model 2 while the initial condition for model 1 was a water level close to the experimental value for model 1 the results were generally independent of the initial conditions but exceptions could be found for the more complex patterns inside the block the simulation parameters obtained from the sensitivity analysis are summarized in table 2 and these parameters were used for the numerical modelling of the rest of the experimental configurations 3 1 5 topography the topography of the experimental platform may change in time since it was constructed with boards supported by beams for most numerical calculations the theoretical topography of an inclined plane with a constant slope in the x direction of 0 12 was used however two detailed topographies that were surveyed in 2019 before the first series of experiments i e fig 2a and in 2021 between the second and third series of experiments i e fig 2b and fig 2c respectively showed some elevation differences compared to the theoretical topography and between the two topographical surveys of less than 2 mm the effect of this change in topography was tested using model 1 and c d 0 4 results show a weak influence on the flow velocity pattern and all the other results table s1 in the supplementary material thus the theoretical topography was used for the rest of the cases 3 2 steady flow tests 3 2 1 flow depths fig 4 shows that both models and hence the 2d swe are able to reproduce fairly accurately the measured flow depth patterns for cases with steady flow fig 2a and b there is a flow depth difference between the right and left streets because the weir height in outlet 1 is larger than in outlet 2 and the discharge in inlet 1 is larger than that in inlet 2 the larger flow depths in the right street compared to the left street induce a pressure gradient that enhances the transverse flow through the porous block openings both models are capable to reproduce the increasing flow depth at the right street the decreasing flow depth at the left street and the relatively constant water level within the block which is a result of the very low velocities within the block the differences between the results of the two models are minimal both within the porous block and in the streets which implies that at a large scale the turbulence closure model does not affect the flow depth predictive capabilities of a 2d swe model in urban floods with steady flow 3 2 2 discharge partition the two models reproduce well the discharge partition both in the streets and within the porous block without any of the two exhibiting clearly superior performance fig 5 a model 1 predicts more accurately the discharge partitioning at the four outlets with a rmse that is less than half of that of model 2 fig 5c model 2 overestimates q out 4 and both models underestimate q out 1 except for the case c100 100 configuration without a block and approximate well q out 2 fig 5b the two models exhibit a different behavior in outlet 3 with model 1 overpredicting and model 2 underpredicting q out 3 fig 5b overall model 1 and model 2 miscalculate the discharge distribution at the outlets by 2 5 and 7 3 on average respectively in the streets surrounding three of the most complex porous blocks c06 04 c19 12 c3 model 2 overestimates the discharge in the right street which is the street that conveys most of the discharge while model 1 exhibits a more erratic pattern with this discharge fig 6 the street that conveys the second largest discharge in these three cases is the downstream street in which both models give good results besides model 2 overpredicting the discharge in c19 12 the overpredictions of model 2 and underpredictions of model 1 at the large discharges in the right and downstream streets are partially compensated by respective underpredictions and overpredictions of the two models at the street with the smallest discharge i e the upstream street fig 6 the discharge distribution for all cases is presented in figure s4 in the supplementary material overall the maximum discharge deviation occurs for c100 100 fig 5c similar disagreements between measurements and 2d swe computations in large open areas were also noted by li et al 2021a generally the flow distribution at the outlets corresponds to the experimental ones error less than 2 5 of the total inflow except the case c100 100 but this distribution is relatively constant due to the general configuration of the street network flow discharges in the streets and through the openings of the block are more influenced although the rmse remains below 2 of the total discharge however due to the small portion of the flow that enters the block the relative error can be high for the flow passing through the building figure s4 in the supplementary material 3 2 3 velocity flow fields in this section the depth averaged flow velocities modelled with 2d swe are compared to the surface velocities measured with lspiv mejia morales et al 2021 compared the lspiv surface velocity measurements to adv measurements across the flow depth and showed that the surface velocities are mostly well approximated by depth averaged velocities starting with the two reference cases c00 00 non porous block and c100 100 no block the two models reproduce qualitatively all the flow features that were observed in the experiments fig 7 in c00 00 the interaction of the flows from the different branches at the junctions matches the measurements well with a correct distribution of the discharge between the outlets fig 5c in c100 100 even though the modelled discharge distribution at the outlets exhibits the largest deviation from the measurements fig 5c the two models reproduce fairly well particularly model 2 the two large recirculation zones however they are uneven compared to the measurements with the downstream and upstream recirculation zones being modelled larger and smaller respectively than what was observed the modelled flow patterns within and around the porous blocks in the first test series fig 2a agree well with the measurements with the number and direction of the recirculation zones being modelled correctly in almost all cases fig 7 for the cases with no more than one opening per side i e c00 04 c06 00 and c06 04 only model 2 in c06 04 exhibits a notable difference in the size of the recirculation zone in the lower left corner when there are three openings at two opposite sides of the porous block the flow pattern becomes much more complex the two models are still able to simulate the direction of the streamlines quite correctly but the sizes of some of the recirculation zones are a little different than the measured ones for c00 12 model 1 adds two small recirculation zones at the right part of the block and model 2 augments one in the center the second test series of steady flow cases presented in fig 2b generally exhibits complex flow recirculations fig 7 because of the several openings on one side of the block in each case and the asymmetric distribution of the other openings at another side of the porous block the case c1 is the only exception in the sense that it has two symmetric openings at the sides at the right street and left street however the flow pattern within the block for c1 is quite complex with three main uneven recirculation zones that the two models cannot reproduce in their correct location moreover the two models do not obtain the same pattern in case c2 from the three openings at the left street the middle one influences the flow pattern the most and the flow pattern in the porous block resembles c00 04 the two models reproduce this pattern accurately cases c3 to c5 are the more complex ones and the two models are not always able to reproduce entirely the observed flow patterns the left part of the pattern in c3 is generally well reproduced by model 1 but the right part with an interaction of three openings is not similar to the measurements on the other hand model 2 predicts quite accurately the flow pattern in c3 case c4 is the most challenging one the two models provide similar patterns but fail to accurately predict the shape and size of the recirculation zones as a result the two observed large counter rotating recirculation zones are modelled as one and the two smaller ones next to the right street have the opposite directionality the structure of the smaller recirculation zones from the models seems more influenced by the opening at the upstream street compared to the measurements on the contrary in a mirrored configuration the modelled flow patterns in c5 relatively similar for the two models seem less influenced by the opening in the downstream street compared to the measurements and as a result the recirculation zone at the right side of the block is modelled larger than what it actually is 3 2 4 comparative analysis of the performance of models 1 and 2 the computational results reveal that both 2d models predict well the flow depths with limited difference between the two models fig 4 this is consistent with existing knowledge that the flow depth predictive capability of a 2d swe model is little influenced by the turbulence closure as multiple previous studies reported a good agreement between computed and observed flow depths while they used different approaches for the turbulence closure arrault et al 2016 bazin et al 2017 shettar and murthy 1996 khan et al 2000 el kadi abderrezzak et al 2009 the two models reproduce similarly well the experimentally observed discharge partition in the streets fig 5a in contrast the considered error metrics suggest that the discharge partition at the outlets is better reproduced by model 1 than by model 2 fig 5b and 5c this may result from the difference in the implementation of the downstream boundary conditions between models 1 and 2 as detailed in section 2 4 except in one configuration c100 100 the differences between the computed and measured discharges do not exceed 2 5 of the total inflow discharge these differences should be set in perspective compared to the experimental uncertainties the valve flowmeter system used in the laboratory experiments have an error of 3 of the measured flow rate mejia morales et al 2021 accordingly the time convergence criterion used for the laboratory measurements was also set at 3 mejia morales et al 2021 besides the experimental method used to estimate the discharge in the streets requires assumptions to cover the blind zones near the boundaries bed walls and free surface as well as the inconvenience of using an intrusive instrument adv in a narrow cross section this leads to an estimated error of 1 5 on average mejia morales et al 2021 the maximum deviation between computed and observed discharges occurs for configuration c100 100 empty central area this is consistent with similar disagreements between measurements and 2d swe computations in large open areas as reported by li et al 2021a table 3 provides an overview of the agreement between the experimentally observed and computed flow fields by models 1 and 2 the flow fields are visible in fig 7 the following observations can be made in several configurations generally with only a single opening per side c00 04 c06 00 both 2d models perform comparatively well and succeed in reproducing the number and relative size of flow recirculations in a limited number of configurations c1 c4 and c5 leading to particularly complex patterns of flow recirculations neither model 1 nor model 2 correctly predict the flow patterns in configuration c19 12 the number of computed recirculations by both models is in line with the experimental observations but their relative sizes diverge from the observations in all other cases with only one exception c06 04 model 2 provides a better prediction of the flow field than model 1 does in configuration c00 12 the number of large recirculations computed by model 2 is correct while it is not for model 1 in configuration c19 00 the right upstream recirculation is computed by model 2 while model 1 fails to capture it similarly in configuration c2 the smaller recirculation in the top left corner is predicted by model 2 while it is not by model 1 in configuration c3 the flow field computed by model 2 is also closer to the experimental observations than the one predicted by model 1 only in configuration c06 04 model 1 provides a flow field more similar to the experimental one therefore although model 1 performs better than model 2 for the prediction of the outflow discharge partition this does not hold true for predicting the flow field within the urban block moreover the constant λ used in model 1 value of 1 m2 s was carefully set based on the modeler s past experience in reproducing reduced scale laboratory experiments paquier et al 2022 however this value has limited chance to be transferrable across scales particularly for the application of the model to real world examples this is another advantage of model 2 with a depth averaged k ε turbulence closure over model 1 as in model 2 the parameters of the turbulence closure are all non dimensional and as such they are not changed when applying the model at different scales e g laboratory experiment vs real world application this aspect was discussed earlier by bruwier et al 2017 3 3 unsteady flow tests 3 3 1 flow depths the unsteady flow simulations were carried out only with model 1 the presence of hydraulic jumps at different locations in the experiments and in the calculations causes a lower agreement of peak flow depths compared to the steady flow cases with an average deviation of 6 7 between calculations and measurements in the streets around the block model 1 slightly overestimates the peak flow depth in the right street which is the highest peak flow depth in the test domain with an error of less than 4 fig 8 the model performs best in the right street for ϕ 0 75 for every tested hydrograph h lss h lll and h sls no trend is detected between the rest of the block porosities and the performance of the model in predicting peak flow depths in the right street the absolute error in the other three streets around the block is similar to that in the right street however the peak flow depth is lower and thus percentagewise model 1 is less accurate in predicting flow depths there in these three streets model 1 predicts flow depths best in h sls the hydrograph with the greatest unsteadiness followed by h lll and h lss the predictive performance of the model in the h sls hydrograph deteriorates with decreasing block porosity whereas for h lll and h lss there is a more erratic pattern on the agreement between depth modelling results and measurements for all flow cases the flow depth is underestimated in the left street fig 8 and in the block figure s5 in the supplementary material fig 9 shows how the flow depth evolves in time at different measuring locations fig 1b of the test domain for the hydrograph h lss and ϕ 1 i e the block without any interior obstruction the model captures the evolution of the flow depths in the right left and upstream street relatively accurately after the first 20 s particularly in the rising limb of the hydrograph however it cannot correctly reproduce the flow depth at the location p in 3 3 2 discharge partition for steady flow in the configurations of test series 3 fig 2c the discharge at outlet 4 is miscalculated by approximately 0 05 l s on average while the discharge at outlet 2 is underestimated by about 0 1 l s fig 10 as for test series 1 and 2 fig 2a b the downstream boundary conditions should be adapted to obtain a more correct distribution however it should be noted that changing critical flow to free outflow at outlets 1 and 2 in which the flow is partly supercritical did not change the outflow distribution the discharges at the outlets for the steady flow case of test series 3 exhibit a slightly increasing trend with increasing porosity in outlet 2 and rather constant values besides ϕ 0 in the other outlets fig 10 for the unsteady flows the peak outflow discharge in outlet 1 is consistently higher than the peak discharges in the other outlets for every tested hydrograph and porosity value as for the respective steady flow test fig 10 the outflow in outlet 1 becomes the highest when the block has no porosity ϕ 0 while it reaches a plateau for each flow case when the block has porosity for the unsteady cases model 1 predicts accurately the peak discharge in outlet 1 for the non porous block for every hydrograph but it overestimates this peak discharge by less than 4 for the porous blocks model 1 performs even better in predicting the peak discharge in outlet 1 in the steady flow case with a slight underestimation of the non porous block case and a few overestimations for the porous block cases the second highest peak outflow discharge occurs in outlet 4 where model 1 overestimates the peak discharge by around 0 085 l s for the non porous block for all flow cases fig 10 the predictive performance of model 1 mostly deteriorates with increasing porosity of the block for all three hydrographs particularly for h sls while this is not observed in the steady flow cases where only a slight overestimation is noted the overestimations in outlets 1 and 4 are partially compensated by some underestimations in the peak outflow discharge in outlet 2 where percentagewise the model predictions deviate from the measurements the most for all flow cases besides the hydrograph h sls finally model 1 predicts accurately the peak outflow discharge in outlet 3 overall for all unsteady cases the average discrepancy between calculations and measurements of the peak discharges at the outlets is 8 6 a comparison between the measured and modelled peak flow depths at the locations p o u t 1 p o u t 4 near the outlets fig 1b is provided in figure s6 in the supplementary material 3 3 3 velocity flow fields as in section 3 2 3 the depth averaged flow velocities modelled with 2d swe are compared to the surface velocities measured with lspiv for areal porosity ϕ 1 in steady flow the flow pattern of the third series is similar to c06 04 with two main nearly symmetrical recirculation zones fig 11 for the unsteady case with the hydrograph h lss after the flow peak the flow pattern remains quite similar for a long time the initial part of this process is reproduced well by model 1 before the flow peak the block is filling and the observed flow pattern comprises four main recirculation zones that are not reproduced by model 1 which instead generates a flow pattern that tends more rapidly to a flow pattern with two main recirculation zones reducing ϕ leads to reduced water volume in the block and an increase in the number of recirculation zones within the porous block which are fairly well reproduced by model 1 fig 12 4 conclusions accurate and fast computational tools for the estimation of urban flood hazard are of vital importance although in such cases the flow can be 3d in parts of the urban layout it is important from a management perspective to understand when these 3d processes are dominant and when the flow can be reliably modelled with 2d shallow water equations in this paper we demonstrated the capacity of two 2d shallow water flow solvers to simulate urban floods involving flow exchanges with the interior of an urban block in eighteen idealized urban layouts the computations were compared against published and new experimental observations in steady and unsteady conditions the tested computational models differed mostly by the turbulence closure used for estimating the eddy viscosity both models reproduced accurately the measured flow depth for all cases the prediction of the discharge distribution and the flow velocity patterns within and around the urban block was in general satisfactory but deteriorated when the flow exchanges between the urban block and the surrounding streets increased and became asymmetrical the average difference between the modelled discharge distributions and the measurements at the outlets was 2 5 and 7 3 for model 1 and model 2 respectively with respect to the flow velocities none of the two models outperformed consistently the other which implies that both tested turbulence closure models are suitable to model the flow patterns within and around an urban block although with different accuracy at different flow patterns for unsteady conditions the difficulties increased because of the occurrence of hydraulic jumps and the sequence of a filling phase and an emptying phase of the block the error thus rose in parameters such as the peak flow depths in the streets and the peak discharges at the outlets which were miscalculated by 6 7 and 8 6 respectively however the influence of the porosity of the urban block was generally simulated in the right way and except during rapid filling of the block the computed velocity pattern inside the block reproduced sufficiently well the main process even if the discharge partition at the outlets is only a little sensitive to a change in the urban block openings local modifications of the flow field can be particularly important for urban planning under climate change scenarios since the building density and the distance between neighboring buildings are the most influential parameters affecting pluvial flooding bruwier et al 2020 the geometric configurations considered here are highly simplified compared to real world urbanized floodplains which have considerably more intricate flowpaths street profiles opening shapes and indoor arrangement of buildings in addition in reality the flow exchanges between the streets and the urban blocks are influenced by obstructions near the openings such as parked cars and street furniture mignot et al 2020 and the interaction of surface flows with surcharging sewers kitsikoudis et al 2021 these aspects highlight the limitations of the present study and need to be investigated in future studies with either large scale experiments or field data to additionally address potential scale effects that affected our results in practice evaluating accurately the flow intrusion into buildings and building blocks would require particularly fine mesh resolution in the near field of the opening or the use of parametrizations such as weir equations such aspects affect the operationality of models for simulating large urban floodplains and need to be investigated the performance of 1d modelling in the streets combined with side discharge equations for the exchanges through building openings could also be investigated in a follow up study declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors from inrae and insa lyon acknowledge the financial support offered by the french national research agency anr for the project deufi under grant anr 18 ce01 0020 the authors gratefully acknowledge msc students yann nicol and eliott crestey who contributed to the numerical computations insightful comments by the editor and three anonymous reviewers have contributed to significantly improve the quality of the manuscript data availability all experimental observations used in this research are available at https doi org 10 57745 ujocj8 mejia morales et al 2023b for the steady flows and at https doi org 10 57745 bfhgo3 for the unsteady flows mejía morales et al 2022 authors contributions the study was designed by a p b d s p and e m who also defined the methodology all laboratory experiments were conducted by m m m under the supervision of s p and e m computations with model 1 were conducted by a p and those with model 2 by students under the guidance of p a b d s e and m p the original draft of the manuscript was prepared by v k with the support of b d a p and m m m it was revised by v k b d e m and s p appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2023 129231 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2097,the multiple flow paths existing in urban environments lead to complex flow fields during urban flooding modelling these flow processes with three dimensional numerical models may be scientifically sound however such numerical models are computationally demanding to ascertain whether urban floods can be modelled with faster tools this study investigated for the first time the capacity of the 2d shallow water equations swe in modelling the flow patterns within and around urban blocks with openings i e involving flow exchanges between the flows in the streets and within the urban blocks e g through alleys leading to courtyards or through broken windows or doors laboratory experiments of idealized urban floods were simulated with two academic 2d swe models with their most notable difference being the parameterization of the eddy viscosity specifically the first model had a turbulence closure based on flow depth and friction velocity while the second model had a depth averaged k ε turbulence closure thirteen urban layouts were considered with steady flow and five with unsteady flow both models simulated the flow depths accurately for the steady cases the discharge distribution in the streets and the flow velocities were predicted with lower accuracy particularly in layouts with large open spaces the average deviation of the modelled discharge distribution at the outlets was 2 5 and 7 3 for the first and second model respectively for the unsteady cases only the first model was tested it predicted well the velocity pattern during the falling limb of a flood wave while it did not reproduce all recirculation zones in the rising limb the peak flow depths in the streets and the peak discharges at the outlets were predicted with an average deviation of 6 7 and 8 6 respectively even though some aspects of the flow in an urban setup are 3d the findings of this study support the modelling of such processes with 2d swe models keywords experimental hydraulics numerical modelling open channel flow shallow water equations turbulence urban flood data availability data will be made available on request 1 introduction urban flood risk is a growing concern addison atkinson et al 2022 chen et al 2015 doocy et al 2013 given the high urbanization rate birkmann et al 2016 chen et al 2022 gross 2016 and the intense anticipated rainfall events due to climate change hettiarachchi et al 2018 pfahl et al 2017 sanderson et al 2019 the flood risk mapping of an urban area remains a challenging task due to the variability in the direct and indirect flood impacts kreibich et al 2014 and in the flood vulnerability chen et al 2019 huggel et al 2013 lv et al 2022 associated with various socioeconomic contexts in different parts of a city as well as due to intricate urban layouts that induce complex flow patterns influencing the flood hazard leandro et al 2016 li et al 2021a lin et al 2021 urban flood numerical modelling is a vital component of flood risk assessment rosenzweig et al 2021 and management guo et al 2021 jongman 2018 and supports design strategies for sustainable and resilient urban infrastructures qi et al 2022 zhou et al 2018 contrary to one dimensional 1d kitsikoudis et al 2020 and 1d 2d bates 2022 simplifications that can be made in river modelling aiming mostly at estimating inundation extents numerical modelling of multidirectional flows in flooded urban areas should be at least 2d li et al 2021a mignot et al 2006 with a focus on the spatial distribution of not only flow depths but also flow velocities kreibich et al 2009 and specific discharges costabile et al 2020 to express the flood hazard degree in the street network this is particularly true for large impervious surfaces upstream of and in urban areas that can lead to an excessive amount of runoff which cannot be conveyed by the drainage systems such high flow discharges may threaten the stability of pedestrians arrighi et al 2017 bernardini et al 2020 postacchini et al 2021 xia et al 2014 and can cause the entrainment of vehicles martinez gomariz et al 2018 smith et al 2019 xia et al 2011 hence the accurate spatial quantification of hydraulic variables within an urban area is of utmost importance 1 1 role of laboratory experiments for model validation a large number of numerical modelling studies simulated urban flows in real world cases guo et al 2021 luo et al 2022 with some of them using lidar data with high resolution digital elevation models of the urban topography almeida et al 2018 ozdemir et al 2013 yalcin 2020 however validation field data including both flow depths and velocities are usually lacking or insufficient costabile et al 2020 which may lead to equifinality issues remote sensing techniques can provide inundation extents and water levels although with certain limitations as tall buildings within the urban environment may obscure some measurements neal et al 2009 but flow velocity measurements in urban floods are more challenging such measurements are dangerous and can be costly and as a result are limited brown and chanson 2013 flow depths and surface velocities can alternatively be determined by monitoring parts of a flooded urban area with unmanned aerial vehicles perks et al 2016 and by analyzing existing footage and crowdsourced data from flooded street networks mignot and dewals 2022 re et al 2022 however there are uncertainties related to the boundary conditions in complex urban terrains with large spatial variability and to the interplay between surface flow and flow in underground drainage systems bazin et al 2014 chang et al 2018 kitsikoudis et al 2021 rubinato et al 2022 finally the typically short duration of pluvial flooding and its local character do not allow for detailed measurements over long durations experimental measurements in laboratory facilities provide an alternative option for models validation in carefully designed experiments the flow and boundary conditions can be accurately controlled mignot et al 2019 and besides offering a better understanding of the governing physical processes such studies can contribute to the validation of numerical models which may subsequently be used for scenario analyses of field cases 1 2 performance of 2d shallow water models the 2d shallow water equations swe can be used to simulate the flow in flooded streets with typically large width to depth ratios however at street intersections the interacting flows coming from various branches generate complex patterns mignot et al 2008 and 3d flow structures el kadi abderrezzak et al 2011 ramamurthy et al 2007 while 3d models can capture most features of diverging flows in bifurcations mignot et al 2013 neary et al 1999 ramamurthy et al 2007 and converging flows in junctions huang et al 2002 luo et al 2018 schindfessel et al 2015 it is important to examine whether these flow processes can be satisfactorily reproduced by 2d operational models that are much faster than 3d models and can be used for real time modelling the 2d swe approach has been proven capable to replicate experimental measurements of flow depths and discharge partitioning in bifurcations bazin et al 2017 el kadi abderrezzak and paquier 2009 khan et al 2000 li et al 2021b shettar and murthy 1996 in junctions li et al 2021b in crossroads mignot et al 2008 as well as in larger and more complicated street networks such as that of arrault et al 2016 with 49 intersections and that of li et al 2021b with four intersections li et al 2021a incorporated various urban layouts in their experimental setup and also modelled successfully the flow depths and discharge partition with a 2d swe model despite the successful applications of 2d swe in modelling water surface profiles and discharge distributions some open questions remain li et al 2020 regarding the accuracy of 2d swe in predicting flow velocities in intersections the extents of recirculating flow areas occurring due to flow separation in some of the branches and the role of the turbulence closure model rodi 2017 shettar and murthy 1996 modelled depth averaged flow velocities in a bifurcation with a k ε turbulence closure and their modelled velocities in the main channel and the length of the recirculation zone agreed well with the experimental measurements however their modelled velocities in the branch of the bifurcation were less accurate khan et al 2000 also modelled the flow in a bifurcation but with a mixing length model and reported that the modelled depth averaged velocities compared well with the measurements while the dimensions of the recirculation zone were predicted by the model satisfactorily bazin et al 2017 used a constant eddy viscosity model to simulate flows in a bifurcation with a branch with a 90 degree angle with and without obstacles at the intersection and the modelled depth averaged flow velocities in the recirculation zone on the upstream side of the bifurcation branch deviated from the measurements bruwier et al 2017 argued that a k ε turbulence closure model should be more suitable than a constant eddy viscosity model for modelling flow interactions in intersections given that since a k ε model does not necessarily require calibration its computational demand can be similar to a constant eddy viscosity model that requires calibration arrault et al 2016 showed in a more complex setup that the turbulence closure model was not particularly influential in the estimation of discharge distribution in the various streets however a k ε turbulence closure model modified significantly the estimates of the recirculation lengths compared to a simulation without a turbulence model no velocity measurements were available however to compare the modelled velocities more recently li et al 2021a modelled depth averaged velocities in an urban district with various urban forms with a k ε turbulence closure model and achieved good agreement with surface velocities in areas of flow contraction however the results were less accurate in large open areas supercritical bazin et al 2017 mignot et al 2008 and transcritical el kadi abderrezzak et al 2011 flows in crossroads may pose additional challenges in 2d swe models since the occurrence and structure of hydraulic jumps can significantly affect the discharge partitioning and water surface profiles 1 3 flow intrusion into buildings an extra challenge numerical and experimental studies of urban flooding typically consider flow around non porous residential blocks haltas et al 2016 van emelen et al 2012 however in reality urban blocks may have corridors leading to backyards while during intense flooding windows and doors labeled as openings from now on of buildings may break leading to lateral flow exchanges between a street and the inside area of the buildings mignot et al 2020 causing significant damages in their interiors dottori et al 2016 martinez gomariz et al 2021 mejia morales et al 2021 conducted a systematic experimental analysis of the effect of the location and size of openings in an urban block located within an idealized urban district they showed that the flow exchanges between the streets and the block interior can alter the flow depth and the flow velocity in the surrounding streets by 12 and 70 respectively when compared to a reference case with a non porous block besides the recent study of mejia morales et al 2021 there is only a limited number of studies that investigated how the porosity of urban blocks affects the hydraulic characteristics of a flood mignot et al 2020 measured the flow discharge entering a building through an open door window or gate in case of an urban flood and they noticed that in some cases the intruding discharge can be approximated by formulas for side weirs however the authors also observed that this intruding discharge can be significantly affected by surrounding urban obstacles wüthrich et al 2020 showed with a flume experiment how the hydrostatic force and the form drag exerted by a steady flow on a building are modified by the porosity and the orientation of the building while sturm et al 2018 measured the flood impact forces on physical models of buildings with openings on a torrential fan in other experiments liu et al 2018 showed how the orientation of a house with respect to the incoming flow affects the forcing on the house door for a dam break case and zhou et al 2016 found differences in the wakes of simplified porous and non porous buildings in a numerical study of a torrential flood gems et al 2016 modelled how the different openings of a building affect the flow pattern within its interior the associated hydrodynamic forcing and the near building flow pattern the findings of these studies show that the openings in buildings affect the spatial distribution of flood hazard and thus the number and types of openings should be considered in flood modelling 1 4 objective of the study the flow exchanges between a street and the interior of a building in combination with bifurcations and junctions at crossroads lead to complex and potentially 3d flow patterns around urban blocks during urban floods since urban areas are typically densely populated there is a need for fast computational tools that could be utilized for real time modelling of not only the flow depths but also the flow velocities for the accurate estimation of the flood hazard 3d numerical models can potentially capture the flow processes of urban floods however they are computationally demanding and slow for real time modelling in practice the 2d swe are used for operational flood hazard and risk modelling while previous studies have already analysed the ability of the 2d swe to simulate flow fields in various settings such as bifurcations junctions 4 branch crossroads and street networks they all assumed that the street boundaries i e building facades were impervious no existing study has focused on the performance of the 2d swe to predict the flow intrusion into flooded buildings or building blocks nor on the flow patterns in the streets and within the urban blocks in urban configurations with openings in the building facades the objective of this study is to examine for the first time whether the flow patterns within and around porous urban blocks i e with openings can be quickly and accurately predicted with numerical modelling based on 2d swe and to determine what is the most effective modelling strategy for the accurate estimation of flow velocities and flow depths to this end the experiments of mejia morales et al 2021 2023a for flow around and within a porous urban block are replicated using two different academic numerical modelling tools to investigate the importance of eddy viscosity parameterization on the accuracy of the models complementary steady flow experiments with additional geometric configurations are also presented for the first time based on the same experimental approach as mejia morales et al 2021 the paper is organized as follows in section 2 the experimental procedure is briefly described and the numerical models are presented the new experimental results and the results of the numerical modelling are presented and discussed in section 3 finally conclusions are drawn in section 4 2 experiments and numerical modelling this section presents the experimental setup section 2 1 the various porous urban block configurations that were tested section 2 2 the numerical models that were used to simulate the experimental data section 2 3 and the prescribed boundary and initial conditions section 2 4 both steady and unsteady flow conditions were simulated with the numerical models for steady flow conditions the experimental data are a combination of the data presented by mejia morales et al 2021 and new data collected from the same urban physical model in the same facility for unsteady flow conditions the experimental data of mejia morales et al 2023a are used only a brief overview of the experimental setup and methods is provided here since they are described in detail in the aforementioned papers 2 1 experimental setup mejia morales et al 2021 2023a experimentally investigated urban floods at the city block scale using a physical model of a rectangular urban block surrounded by four streets under steady fig 1 a and unsteady fig 1b flow conditions for the steady flow experiments the length of the two streets in the x direction named right street and left street was 5 4 m and the length of the two streets in the y direction named downstream street and upstream street was 3 2 m all four streets had the same rectangular cross section with a width b 0 15 m the experimental setup for the unsteady flow experiments was the same except for the initial part of the left street which was closed upstream of the upstream street fig 1b the physical model had a slope s 0 x 0 12 in the x direction and s 0 y 0 in the y direction whereas the bed of the model was constructed with pvc and the sidewalls of the streets and the urban block were constructed with plastic various configurations of the urban block were tested section 2 2 and fig 2 however its total lengths in the x and y directions remained fixed at l x 1 56 m and l y 0 96 m respectively the thickness and the height of the walls of the porous block were 2 cm and 15 cm respectively the model inlets were located at the upstream ends of the streets in the x direction as such the steady flow experiments had two inlets with fixed inlet discharges q in 1 and q in 2 for the right street and left street respectively while for the unsteady experiments discharge was fed only through the right street since the upstream reach of the left street was closed the inlet discharges were set using separate valve flowmeter systems with a precision of 3 smooth inlet conditions were secured by placing a plastic honeycomb grid at the point entrance of the right street and of the left street each one of the four streets of the physical model had an outlet with a vertical tail weir that regulated the flow depth for the steady flow cases the weir height of outlet 1 in the right street was 4 cm and of outlet 2 in the left street was 3 cm with respective outlet discharges q out 1 and q out 2 in the two streets in the y direction the outlet 3 in the downstream street and the outlet 4 in the upstream street had the same 3 cm weir height with outlet discharges q out 3 and q out 4 respectively for the unsteady flow cases the weir height was set to zero in all outlets to avoid the reflection of the floodwaves on the weir the outflow discharges at the four outlets were monitored using electromagnetic flowmeters specifically the water overflowing the weir in each outlet was collected in a separate tank and subsequently the flow exiting each tank was measured with an optiflux 2000 flowmeter manufactured by krohne the flow depths in the physical model were measured using ultrasonic distance measuring sensors baumer undk 20i6914 s35a with a 0 65 mm uncertainty for the steady flow cases a sensor was attached on a mechanical gantry system that allowed horizontal movement with measurements being taken every 5 cm along the longitudinal direction of each street and at three locations across the street width with 6 5 cm spacing flow depth measurements within the porous urban block were conducted every 12 cm in both x and y directions each depth measurement was conducted with a sampling frequency of 50 hz for a duration of 50 s mejia morales et al 2021 for the unsteady flow cases flow depths were measured at the eleven locations depicted in fig 1b for the whole duration of each hydrograph the reported flow depths are the results of ensemble averaging of 50 identical floodwaves that were fed sequentially into the model with a steady base flow separating two sequential floodwaves the number of required repeated floodwaves was selected by increasing the number until the ensemble average standard deviation of the flow depth became smaller than 1 mm the floodwaves characteristics are detailed in section 2 4 for the steady flow cases surface flow velocities were measured using large scale particle image velocimetry lspiv fujita et al 1998 floating wood shavings 1 4 mm were used as tracers a panasonic hc v770 camera was positioned 2 8 m above the physical model monitoring the plan view at a rate of 25 frames per second with a resolution of 1920 px by 1080 px the time averaged surface velocities estimated by the lspiv technique stabilized after different periods of time for the various areas of the model but none of them exceeded 60 s mejia morales et al 2021 more details about the seeding of the flow the flow monitoring the data post processing and a validation of the lspiv measurements against measurements with an acoustic doppler velocimeter adv are provided in mejia morales et al 2021 for the unsteady flow cases it was not feasible to monitor the flow velocities in the whole flow area only the surface velocities within the porous block and at two points in the right street and left street shown in fig 1b were monitored moreover ensemble averaging was not used for the lspiv due to prohibitive post processing load mejia morales et al 2023a a sony zv 1 camera with a sampling rate of 25 frames per second was used and the collected frames were averaged over periods of 2 s to filter the data 2 2 urban block configurations in every experiment the urban block was in the same position near the downstream end in the x direction and had the same dimensions l x and l y fig 1 however the conveyance porosity i e the porosity of each sidewall of the urban block ψ as defined by the number and locations of openings differed in each experiment each opening had a width lop 6 cm and each sidewall of the block had no more than three openings in all tests the water surface elevation remained lower than the height of the openings in the present paper three series of configurations for the porous block are examined fig 2 the first series comprises the eight configurations presented by mejia morales et al 2021 without obstruction within the block fig 2a the conveyance porosity of each configuration is presented as cxx yy where xx and yy denote the ratio of the total length of the openings in a side of the porous block to the length of that side in percent in the x and y directions respectively the locations of the openings in the configuration with the largest conveyance porosity c19 12 are shown in fig 1a the conveyance porosity in the rest of the configurations is determined by closing some of the openings of c19 12 while maintaining symmetry in the porous block openings the second series comprises five new configurations constructed and tested with the same experimental approach as mejia morales et al 2021 also without obstructions within the block fig 2b the common trait of these configurations is that each configuration has four openings in its perimeter the remaining ones after blocking eight openings in c19 12 shown in fig 1a since there is no symmetry in every configuration these configurations are simply named c1 c5 in order of appearance the configurations in the third series presented in mejia morales et al 2023a have one opening in the middle of each wall of the block and a non porous rectangular obstacle in the center of the block the footprint area of this obstacle was varied as shown in fig 2c leading to an areal porosity ϕ for each case that is determined as the ratio of the empty area within the block to its total internal area note that the concept of porosity is introduced here for the sole purpose of providing a macroscopic description of the considered geometric layouts fig 2 while the flow models used in this study are not porosity shallow water models e g dewals et al 2021 they aim to fully resolve the flow field on the considered computational mesh the first and second series were used with steady flow conditions while the third series was used with both steady and unsteady flow conditions details about the upstream boundary conditions of each case are presented in section 2 4 the physical models were designed by assuming a geometrically distorted scale with horizontal and vertical scale ratios equal to 50 and 10 respectively this means that a studied flow in the physical model may be interpreted as a representation of a real world flow in streets with 7 5 m in width around an urban block with dimensions 78 m 48 m and openings 3 m wide the upscaled studied flow depths are around 60 cm this approach ensures relatively large depths in the physical model to enable a satisfactory measurement accuracy heller 2011 li et al 2021b 2 3 numerical modelling the laboratory experiments were simulated using two academic numerical codes that solve the 2d swe equations the two models have differences in their mathematical formulation and their numerical discretization the first model is implemented in the software rubar20 mignot et al 2008 developed by the riverly research unit of inrae in lyon and the second one is implemented in wolf 2d erpicum et al 2009 developed by the hece group at the university of liege table 1 provides an overview of the characteristics of each model referred to as model 1 for rubar20 and model 2 for wolf 2d the steady flow cases were simulated with both numerical models while only model 1 was used for the simulation of the unsteady flow cases 2 3 1 governing equations the two codes solve the conservative form of the 2d swe which means that the main unknowns are the flow depth h and the specific discharges hu and hv with u and v denoting the depth averaged flow velocities along the x and y direction respectively the 2d swe in conservative form are formulated as in eqs 1 3 wu 2008 1 h t h u x h v y 0 2 h u t x h u 2 g h 2 2 h u v y τ bx ρ 1 ρ h τ xx x 1 ρ h τ xy y 3 h v t h u v x y h v 2 g h 2 2 τ by ρ 1 ρ h τ xy x 1 ρ h τ yy y where g is the acceleration of gravity ρ is the water density t is the time τ xx τ yy and τ xy are the depth averaged stresses comprising both the reynolds and molecular stresses erpicum et al 2009 and τ bx and τ by are the bed shear stresses in the x and y direction respectively calculated from eqs 4 and 5 in line with camnasio et al 2014 4 τ bx ρ f u u 2 v 2 8 5 τ by ρ f v u 2 v 2 8 where f is the darcy weisbach bed friction coefficient the darcy weisbach formulation is used in both models but the friction coefficient f of the bottom and side walls is estimated by the colebrook white formula eq 6 idel cik 1969 in model 2 and by its explicit equivalent formula eq 7 yen 2002 in model 1 6 1 f 2 log k s 14 8 h 2 51 re f 7 f 1 4 log k s 12 h 6 79 re 0 9 2 where k s is the roughness height and r e is a reynolds number r e 4 u 2 v 2 h ν with ν the kinematic viscosity of water although both models were derived by depth averaging the reynolds averaged navier stokes equations together with boussinesq s assumption for expressing the depth averaged turbulent stresses they differ by the type of turbulence closure used model 1 is based on a turbulence closure in which the eddy viscosity ν t is estimated by elder s formula ν t λ h u with u the friction velocity computed from the free surface slope and λ a parameter set by the user with a default value of 1 mejia morales et al 2020 in model 2 a two equation turbulence closure is implemented it consists in a two length scale depth averaged k ε turbulence model as detailed by erpicum et al 2009 and camnasio et al 2014 2 3 2 numerical discretization in both models the computational domain was meshed with a cartesian square grid aligned with the street sidewalls depending on the model run the grid spacing δ x was varied between 5 mm and 30 mm with the resulting ratio of the grid size to the length of one opening in the porous block l op ranging from 1 2 to 1 12 both models are solved with a finite volume technique in model 1 a godunov type scheme is used mignot et al 2008 while model 2 is based on a flux vector splitting technique erpicum et al 2010 in both models the variables at the cell edges are evaluated from a linear reconstruction achieving second order accuracy in space for steady flow calculations the models were run in unsteady mode until a steady state was reached the time step used in the simulations was of the order of 10 3 s as it was constrained by the courant friedrichs lewy cfl stability condition in both models the cfl number was set at 0 5 the computational time necessary to reach convergence towards a steady state varied with the considered geometric configuration and initial conditions it was generally of the order of an hour on a standard desktop 2 4 boundary and initial conditions the computational domain was delimited by three types of boundaries sidewalls inlets and outlets at each sidewall the component of the specific discharge normal to the sidewall was set to zero at the inlets the specific discharge in the streamwise direction was prescribed and the normal component of the specific discharge was set to zero the two inlets that are considered in the left street and right street were positioned at a distance of 2 94 m upstream of the uppermost street intersections fig 1 i e at the location of the honeycomb grid at the entrance of each street in the experiments for the steady flow cases in the first and second series of tests fig 2a and b steady inflow discharges were prescribed q in 1 4 5 l s and q in 2 2 0 l s fig 1 in consistency with the measured values for the unsteady flow cases in the third test series fig 2c the inflow discharge was fed only through the right street as a sequence of 50 consecutive identical flood waves three different floodwaves were tested fig 3 and each one was examined separately each floodwave had the same peak flow of 5 l s fig 3 but was characterized by a different unsteadiness degree mejia morales et al 2023a the floodwaves were distinguished based on the discharge rising time the discharge falling time and the total volume of floodwater while their names were formed by using an l or an s for large and small magnitude for each one of the floodwave characteristics respectively for example h lss denotes a hydrograph with large discharge rising time small discharge falling time and small total volume of floodwater as a reference case steady flow experiments with inlet discharge of 5 l s i e equal to the peak of the floodwaves through the right street were also carried out in the geometrical setup of test series 3 fig 1b with the urban blocks of fig 2c at the outlets the outflow discharge was prescribed as a function of the computed flow depth the outlet boundaries were positioned as follows fig 1 in the right street and the left street at a distance of 0 6 m downstream of the easternmost street intersection in the upstream street and the downstream street at 1 94 m downstream of the northernmost street intersection for test series 1 and 2 fig 2a and b the outflow discharge q 0 in each outlet was determined from the following weir formula e g roger et al 2009 8 q 0 l c d 2 g h w 3 where l is the weir length c d is the discharge coefficient and w is the weir height the implementation of eq 8 is slightly different in the two models in model 1 the value of l is set equal to the mesh size and distinct values of q 0 are computed at each cell edge along the outlet boundary as a function of the flow depth computed at the relevant cell in model 2 the length l is taken equal to the actual weir length i e the street width b and a single value of q 0 is evaluated assumed uniformly distributed over the weir length as a function of the average of the computed flow depths over the cells next to the outlet boundary for test series 3 fig 2c the downstream boundary condition was set to critical flow for all the edges of an outlet because the flow goes directly from the street to the outlet tank without a weir in the steady flow runs of model 2 the initial condition was either a converged solution from a previous run or a calm body of water with an initial flow depth equal to 0 05 m for model 1 the initial condition for the steady flow calculations was a water level close to the experimental value and for the unsteady flow calculations was zero flow depth across the flow domain 3 results and discussion 3 1 sensitivity analysis and calibration of the numerical models model 2 was used systematically in a series of preliminary computations to assess the effect of the variation in the i grid spacing δ x ii roughness height k s iii discharge coefficient c d of the weirs at the outlets and iv initial conditions model 1 was also used in these preliminary computations but not in a systematic way moreover model 1 was used to verify whether considering a theoretical bottom topography flat bed instead of the real one influences the results these sensitivity analyses were conducted for a single geometric configuration c19 12 in fig 2a which includes the largest number of openings and leads to the most complex flow fields the comparison of the computed y i c and observed y i o hydraulic variables was carried out based on the bias and the root mean square error rmse e g chen et al 2010 9 bias i 1 n y i c y i o n 10 rmse i 1 n y i c y i o 2 n where n is the number of points where both measured and modelled data were available 3 1 1 grid spacing the grid cell size for model 2 was selected after repeating the computations for c19 12 three times with all parameters being kept the same except the grid cell size the three mesh grids that were tested had square grid cells with side length δ x equal to 30 mm 10 mm and 5 mm respectively the bias and rmse of the flow depths and velocities for different areas of the model were significantly reduced when the grid cell size was reduced from 30 mm to 10 mm but did not vary much when the cell size was further reduced from 10 mm to 5 mm fig s1a in the supplementary material fig s1a in the supplementary material also confirms the second order accuracy of the finite volume numerical scheme implemented in model 2 consistently with the linear reconstruction used in this model however the features of the simulated flow velocity patterns i e number and size of recirculating flow areas within the porous block were more consistent with the features of the measured patterns when the cell size was 5 mm fig s2a in the supplementary material even though some flow recirculations were not captured entirely therefore the 5 mm cell size was kept for the rest of the analyses with model 2 the number of cells is close to 160 000 and it varies slightly depending on the geometric configuration number of openings model 1 exhibited similar behavior with model 2 when varying the cell size with the rest of the parameters being kept the same however with model 1 the flow velocity patterns were similar for mesh sizes of 10 mm and 5 mm fig s3a in the supplementary material thus to reduce computational times the 10 mm mesh was kept for the rest of the analyses with model 1 leading to about 40 000 cells with these mesh configurations the computed flow depths exhibited a systematic bias compared to the observations which motivated the extension of the sensitivity analysis to the roughness height and the discharge coefficients of the weir outlets 3 1 2 roughness height the roughness height was taken at a small value corresponding to the pvc surface of the laboratory model the tested values of k s were equal to 2 10 4 m 8 10 5 m and 3 6 10 5 m this sensitivity analysis was conducted with model 2 with δ x 5 mm and c d 0 527 for all outlets with a previously converged flow field as initial condition the three tested values for the roughness height did not affect significantly the flow depths and velocities results fig s1b in the supplementary material nor the flow patterns fig s2b in the supplementary material the flow depth bias and rmse values for the lowest value of k s were slightly lower compared to the other k s values but at the same time the flow velocity bias and rmse values slightly increased the k s value of 3 6 10 5 m was calibrated from water surface measurements in a single street without openings considering the very small influence of the tested k s values on the simulated results with model 2 a similar sensitivity analysis was not repeated with model 1 and k s 3 6 10 5 m was used in both models 3 1 3 discharge coefficient of the weirs the computations presented in section 3 1 1 used discharge coefficients that were experimentally derived from the laboratory tests however the location where the flow depth is measured upstream of the weirs in the lab does not correspond exactly to the location where the model 2 considers flow depth for estimating the outflow discharge hence the discharge coefficient c d which lumps all flow processes in the near field of the weirs including vertical acceleration which cannot be represented explicitly by shallow water equations was recalibrated so that the computed flow depths agree on average with the observations to this end several values of c d were tested the lowest difference between modelled and measured flow depths for model 2 was obtained with c d 0 453 and thus this value was selected for the rest of the numerical simulations using model 2 for model 1 the lowest difference between modelled and measured flow depths was obtained with c d 0 467 and this value was chosen for the rest of the simulations with model 1 although a value of 0 55 for outlets 1 and 2 and 0 53 for outlets 3 and 4 led to a better distribution of the outflows this was also the case for all the urban blocks in fig 2a nevertheless the effect of c d on the street and block intrusion discharges and on the flow patterns fig s2c and fig s3b in the supplementary material is rather small the small difference between the chosen discharge coefficients for the two models may be attributed to the different ways that the downstream boundary conditions were implemented in the models and to the different turbulent closures 3 1 4 initial conditions a converged solution for a steady flow simulation may depend on the initial conditions dewals et al 2012 particularly in the presence of complex patterns of recirculating flow therefore by using model 2 for the case with the c19 12 block fig 2a we repeated the computations for two different initial conditions i the computed steady flow field obtained with the experimentally derived discharge coefficient i e a previously converged solution and ii water at rest with flow depth equal to 5 cm as expected the initial condition influenced the computed steady flow field for the flow in the porous block the results obtained when the computations were initiated with water at rest agree better with the observations fig s1c and fig s2d in the supplementary material this initial condition setting was kept for the rest of the analysis for model 2 while the initial condition for model 1 was a water level close to the experimental value for model 1 the results were generally independent of the initial conditions but exceptions could be found for the more complex patterns inside the block the simulation parameters obtained from the sensitivity analysis are summarized in table 2 and these parameters were used for the numerical modelling of the rest of the experimental configurations 3 1 5 topography the topography of the experimental platform may change in time since it was constructed with boards supported by beams for most numerical calculations the theoretical topography of an inclined plane with a constant slope in the x direction of 0 12 was used however two detailed topographies that were surveyed in 2019 before the first series of experiments i e fig 2a and in 2021 between the second and third series of experiments i e fig 2b and fig 2c respectively showed some elevation differences compared to the theoretical topography and between the two topographical surveys of less than 2 mm the effect of this change in topography was tested using model 1 and c d 0 4 results show a weak influence on the flow velocity pattern and all the other results table s1 in the supplementary material thus the theoretical topography was used for the rest of the cases 3 2 steady flow tests 3 2 1 flow depths fig 4 shows that both models and hence the 2d swe are able to reproduce fairly accurately the measured flow depth patterns for cases with steady flow fig 2a and b there is a flow depth difference between the right and left streets because the weir height in outlet 1 is larger than in outlet 2 and the discharge in inlet 1 is larger than that in inlet 2 the larger flow depths in the right street compared to the left street induce a pressure gradient that enhances the transverse flow through the porous block openings both models are capable to reproduce the increasing flow depth at the right street the decreasing flow depth at the left street and the relatively constant water level within the block which is a result of the very low velocities within the block the differences between the results of the two models are minimal both within the porous block and in the streets which implies that at a large scale the turbulence closure model does not affect the flow depth predictive capabilities of a 2d swe model in urban floods with steady flow 3 2 2 discharge partition the two models reproduce well the discharge partition both in the streets and within the porous block without any of the two exhibiting clearly superior performance fig 5 a model 1 predicts more accurately the discharge partitioning at the four outlets with a rmse that is less than half of that of model 2 fig 5c model 2 overestimates q out 4 and both models underestimate q out 1 except for the case c100 100 configuration without a block and approximate well q out 2 fig 5b the two models exhibit a different behavior in outlet 3 with model 1 overpredicting and model 2 underpredicting q out 3 fig 5b overall model 1 and model 2 miscalculate the discharge distribution at the outlets by 2 5 and 7 3 on average respectively in the streets surrounding three of the most complex porous blocks c06 04 c19 12 c3 model 2 overestimates the discharge in the right street which is the street that conveys most of the discharge while model 1 exhibits a more erratic pattern with this discharge fig 6 the street that conveys the second largest discharge in these three cases is the downstream street in which both models give good results besides model 2 overpredicting the discharge in c19 12 the overpredictions of model 2 and underpredictions of model 1 at the large discharges in the right and downstream streets are partially compensated by respective underpredictions and overpredictions of the two models at the street with the smallest discharge i e the upstream street fig 6 the discharge distribution for all cases is presented in figure s4 in the supplementary material overall the maximum discharge deviation occurs for c100 100 fig 5c similar disagreements between measurements and 2d swe computations in large open areas were also noted by li et al 2021a generally the flow distribution at the outlets corresponds to the experimental ones error less than 2 5 of the total inflow except the case c100 100 but this distribution is relatively constant due to the general configuration of the street network flow discharges in the streets and through the openings of the block are more influenced although the rmse remains below 2 of the total discharge however due to the small portion of the flow that enters the block the relative error can be high for the flow passing through the building figure s4 in the supplementary material 3 2 3 velocity flow fields in this section the depth averaged flow velocities modelled with 2d swe are compared to the surface velocities measured with lspiv mejia morales et al 2021 compared the lspiv surface velocity measurements to adv measurements across the flow depth and showed that the surface velocities are mostly well approximated by depth averaged velocities starting with the two reference cases c00 00 non porous block and c100 100 no block the two models reproduce qualitatively all the flow features that were observed in the experiments fig 7 in c00 00 the interaction of the flows from the different branches at the junctions matches the measurements well with a correct distribution of the discharge between the outlets fig 5c in c100 100 even though the modelled discharge distribution at the outlets exhibits the largest deviation from the measurements fig 5c the two models reproduce fairly well particularly model 2 the two large recirculation zones however they are uneven compared to the measurements with the downstream and upstream recirculation zones being modelled larger and smaller respectively than what was observed the modelled flow patterns within and around the porous blocks in the first test series fig 2a agree well with the measurements with the number and direction of the recirculation zones being modelled correctly in almost all cases fig 7 for the cases with no more than one opening per side i e c00 04 c06 00 and c06 04 only model 2 in c06 04 exhibits a notable difference in the size of the recirculation zone in the lower left corner when there are three openings at two opposite sides of the porous block the flow pattern becomes much more complex the two models are still able to simulate the direction of the streamlines quite correctly but the sizes of some of the recirculation zones are a little different than the measured ones for c00 12 model 1 adds two small recirculation zones at the right part of the block and model 2 augments one in the center the second test series of steady flow cases presented in fig 2b generally exhibits complex flow recirculations fig 7 because of the several openings on one side of the block in each case and the asymmetric distribution of the other openings at another side of the porous block the case c1 is the only exception in the sense that it has two symmetric openings at the sides at the right street and left street however the flow pattern within the block for c1 is quite complex with three main uneven recirculation zones that the two models cannot reproduce in their correct location moreover the two models do not obtain the same pattern in case c2 from the three openings at the left street the middle one influences the flow pattern the most and the flow pattern in the porous block resembles c00 04 the two models reproduce this pattern accurately cases c3 to c5 are the more complex ones and the two models are not always able to reproduce entirely the observed flow patterns the left part of the pattern in c3 is generally well reproduced by model 1 but the right part with an interaction of three openings is not similar to the measurements on the other hand model 2 predicts quite accurately the flow pattern in c3 case c4 is the most challenging one the two models provide similar patterns but fail to accurately predict the shape and size of the recirculation zones as a result the two observed large counter rotating recirculation zones are modelled as one and the two smaller ones next to the right street have the opposite directionality the structure of the smaller recirculation zones from the models seems more influenced by the opening at the upstream street compared to the measurements on the contrary in a mirrored configuration the modelled flow patterns in c5 relatively similar for the two models seem less influenced by the opening in the downstream street compared to the measurements and as a result the recirculation zone at the right side of the block is modelled larger than what it actually is 3 2 4 comparative analysis of the performance of models 1 and 2 the computational results reveal that both 2d models predict well the flow depths with limited difference between the two models fig 4 this is consistent with existing knowledge that the flow depth predictive capability of a 2d swe model is little influenced by the turbulence closure as multiple previous studies reported a good agreement between computed and observed flow depths while they used different approaches for the turbulence closure arrault et al 2016 bazin et al 2017 shettar and murthy 1996 khan et al 2000 el kadi abderrezzak et al 2009 the two models reproduce similarly well the experimentally observed discharge partition in the streets fig 5a in contrast the considered error metrics suggest that the discharge partition at the outlets is better reproduced by model 1 than by model 2 fig 5b and 5c this may result from the difference in the implementation of the downstream boundary conditions between models 1 and 2 as detailed in section 2 4 except in one configuration c100 100 the differences between the computed and measured discharges do not exceed 2 5 of the total inflow discharge these differences should be set in perspective compared to the experimental uncertainties the valve flowmeter system used in the laboratory experiments have an error of 3 of the measured flow rate mejia morales et al 2021 accordingly the time convergence criterion used for the laboratory measurements was also set at 3 mejia morales et al 2021 besides the experimental method used to estimate the discharge in the streets requires assumptions to cover the blind zones near the boundaries bed walls and free surface as well as the inconvenience of using an intrusive instrument adv in a narrow cross section this leads to an estimated error of 1 5 on average mejia morales et al 2021 the maximum deviation between computed and observed discharges occurs for configuration c100 100 empty central area this is consistent with similar disagreements between measurements and 2d swe computations in large open areas as reported by li et al 2021a table 3 provides an overview of the agreement between the experimentally observed and computed flow fields by models 1 and 2 the flow fields are visible in fig 7 the following observations can be made in several configurations generally with only a single opening per side c00 04 c06 00 both 2d models perform comparatively well and succeed in reproducing the number and relative size of flow recirculations in a limited number of configurations c1 c4 and c5 leading to particularly complex patterns of flow recirculations neither model 1 nor model 2 correctly predict the flow patterns in configuration c19 12 the number of computed recirculations by both models is in line with the experimental observations but their relative sizes diverge from the observations in all other cases with only one exception c06 04 model 2 provides a better prediction of the flow field than model 1 does in configuration c00 12 the number of large recirculations computed by model 2 is correct while it is not for model 1 in configuration c19 00 the right upstream recirculation is computed by model 2 while model 1 fails to capture it similarly in configuration c2 the smaller recirculation in the top left corner is predicted by model 2 while it is not by model 1 in configuration c3 the flow field computed by model 2 is also closer to the experimental observations than the one predicted by model 1 only in configuration c06 04 model 1 provides a flow field more similar to the experimental one therefore although model 1 performs better than model 2 for the prediction of the outflow discharge partition this does not hold true for predicting the flow field within the urban block moreover the constant λ used in model 1 value of 1 m2 s was carefully set based on the modeler s past experience in reproducing reduced scale laboratory experiments paquier et al 2022 however this value has limited chance to be transferrable across scales particularly for the application of the model to real world examples this is another advantage of model 2 with a depth averaged k ε turbulence closure over model 1 as in model 2 the parameters of the turbulence closure are all non dimensional and as such they are not changed when applying the model at different scales e g laboratory experiment vs real world application this aspect was discussed earlier by bruwier et al 2017 3 3 unsteady flow tests 3 3 1 flow depths the unsteady flow simulations were carried out only with model 1 the presence of hydraulic jumps at different locations in the experiments and in the calculations causes a lower agreement of peak flow depths compared to the steady flow cases with an average deviation of 6 7 between calculations and measurements in the streets around the block model 1 slightly overestimates the peak flow depth in the right street which is the highest peak flow depth in the test domain with an error of less than 4 fig 8 the model performs best in the right street for ϕ 0 75 for every tested hydrograph h lss h lll and h sls no trend is detected between the rest of the block porosities and the performance of the model in predicting peak flow depths in the right street the absolute error in the other three streets around the block is similar to that in the right street however the peak flow depth is lower and thus percentagewise model 1 is less accurate in predicting flow depths there in these three streets model 1 predicts flow depths best in h sls the hydrograph with the greatest unsteadiness followed by h lll and h lss the predictive performance of the model in the h sls hydrograph deteriorates with decreasing block porosity whereas for h lll and h lss there is a more erratic pattern on the agreement between depth modelling results and measurements for all flow cases the flow depth is underestimated in the left street fig 8 and in the block figure s5 in the supplementary material fig 9 shows how the flow depth evolves in time at different measuring locations fig 1b of the test domain for the hydrograph h lss and ϕ 1 i e the block without any interior obstruction the model captures the evolution of the flow depths in the right left and upstream street relatively accurately after the first 20 s particularly in the rising limb of the hydrograph however it cannot correctly reproduce the flow depth at the location p in 3 3 2 discharge partition for steady flow in the configurations of test series 3 fig 2c the discharge at outlet 4 is miscalculated by approximately 0 05 l s on average while the discharge at outlet 2 is underestimated by about 0 1 l s fig 10 as for test series 1 and 2 fig 2a b the downstream boundary conditions should be adapted to obtain a more correct distribution however it should be noted that changing critical flow to free outflow at outlets 1 and 2 in which the flow is partly supercritical did not change the outflow distribution the discharges at the outlets for the steady flow case of test series 3 exhibit a slightly increasing trend with increasing porosity in outlet 2 and rather constant values besides ϕ 0 in the other outlets fig 10 for the unsteady flows the peak outflow discharge in outlet 1 is consistently higher than the peak discharges in the other outlets for every tested hydrograph and porosity value as for the respective steady flow test fig 10 the outflow in outlet 1 becomes the highest when the block has no porosity ϕ 0 while it reaches a plateau for each flow case when the block has porosity for the unsteady cases model 1 predicts accurately the peak discharge in outlet 1 for the non porous block for every hydrograph but it overestimates this peak discharge by less than 4 for the porous blocks model 1 performs even better in predicting the peak discharge in outlet 1 in the steady flow case with a slight underestimation of the non porous block case and a few overestimations for the porous block cases the second highest peak outflow discharge occurs in outlet 4 where model 1 overestimates the peak discharge by around 0 085 l s for the non porous block for all flow cases fig 10 the predictive performance of model 1 mostly deteriorates with increasing porosity of the block for all three hydrographs particularly for h sls while this is not observed in the steady flow cases where only a slight overestimation is noted the overestimations in outlets 1 and 4 are partially compensated by some underestimations in the peak outflow discharge in outlet 2 where percentagewise the model predictions deviate from the measurements the most for all flow cases besides the hydrograph h sls finally model 1 predicts accurately the peak outflow discharge in outlet 3 overall for all unsteady cases the average discrepancy between calculations and measurements of the peak discharges at the outlets is 8 6 a comparison between the measured and modelled peak flow depths at the locations p o u t 1 p o u t 4 near the outlets fig 1b is provided in figure s6 in the supplementary material 3 3 3 velocity flow fields as in section 3 2 3 the depth averaged flow velocities modelled with 2d swe are compared to the surface velocities measured with lspiv for areal porosity ϕ 1 in steady flow the flow pattern of the third series is similar to c06 04 with two main nearly symmetrical recirculation zones fig 11 for the unsteady case with the hydrograph h lss after the flow peak the flow pattern remains quite similar for a long time the initial part of this process is reproduced well by model 1 before the flow peak the block is filling and the observed flow pattern comprises four main recirculation zones that are not reproduced by model 1 which instead generates a flow pattern that tends more rapidly to a flow pattern with two main recirculation zones reducing ϕ leads to reduced water volume in the block and an increase in the number of recirculation zones within the porous block which are fairly well reproduced by model 1 fig 12 4 conclusions accurate and fast computational tools for the estimation of urban flood hazard are of vital importance although in such cases the flow can be 3d in parts of the urban layout it is important from a management perspective to understand when these 3d processes are dominant and when the flow can be reliably modelled with 2d shallow water equations in this paper we demonstrated the capacity of two 2d shallow water flow solvers to simulate urban floods involving flow exchanges with the interior of an urban block in eighteen idealized urban layouts the computations were compared against published and new experimental observations in steady and unsteady conditions the tested computational models differed mostly by the turbulence closure used for estimating the eddy viscosity both models reproduced accurately the measured flow depth for all cases the prediction of the discharge distribution and the flow velocity patterns within and around the urban block was in general satisfactory but deteriorated when the flow exchanges between the urban block and the surrounding streets increased and became asymmetrical the average difference between the modelled discharge distributions and the measurements at the outlets was 2 5 and 7 3 for model 1 and model 2 respectively with respect to the flow velocities none of the two models outperformed consistently the other which implies that both tested turbulence closure models are suitable to model the flow patterns within and around an urban block although with different accuracy at different flow patterns for unsteady conditions the difficulties increased because of the occurrence of hydraulic jumps and the sequence of a filling phase and an emptying phase of the block the error thus rose in parameters such as the peak flow depths in the streets and the peak discharges at the outlets which were miscalculated by 6 7 and 8 6 respectively however the influence of the porosity of the urban block was generally simulated in the right way and except during rapid filling of the block the computed velocity pattern inside the block reproduced sufficiently well the main process even if the discharge partition at the outlets is only a little sensitive to a change in the urban block openings local modifications of the flow field can be particularly important for urban planning under climate change scenarios since the building density and the distance between neighboring buildings are the most influential parameters affecting pluvial flooding bruwier et al 2020 the geometric configurations considered here are highly simplified compared to real world urbanized floodplains which have considerably more intricate flowpaths street profiles opening shapes and indoor arrangement of buildings in addition in reality the flow exchanges between the streets and the urban blocks are influenced by obstructions near the openings such as parked cars and street furniture mignot et al 2020 and the interaction of surface flows with surcharging sewers kitsikoudis et al 2021 these aspects highlight the limitations of the present study and need to be investigated in future studies with either large scale experiments or field data to additionally address potential scale effects that affected our results in practice evaluating accurately the flow intrusion into buildings and building blocks would require particularly fine mesh resolution in the near field of the opening or the use of parametrizations such as weir equations such aspects affect the operationality of models for simulating large urban floodplains and need to be investigated the performance of 1d modelling in the streets combined with side discharge equations for the exchanges through building openings could also be investigated in a follow up study declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors from inrae and insa lyon acknowledge the financial support offered by the french national research agency anr for the project deufi under grant anr 18 ce01 0020 the authors gratefully acknowledge msc students yann nicol and eliott crestey who contributed to the numerical computations insightful comments by the editor and three anonymous reviewers have contributed to significantly improve the quality of the manuscript data availability all experimental observations used in this research are available at https doi org 10 57745 ujocj8 mejia morales et al 2023b for the steady flows and at https doi org 10 57745 bfhgo3 for the unsteady flows mejía morales et al 2022 authors contributions the study was designed by a p b d s p and e m who also defined the methodology all laboratory experiments were conducted by m m m under the supervision of s p and e m computations with model 1 were conducted by a p and those with model 2 by students under the guidance of p a b d s e and m p the original draft of the manuscript was prepared by v k with the support of b d a p and m m m it was revised by v k b d e m and s p appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2023 129231 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2098,concerns about whether the reservoirs against climate change will be able to fulfill their missions in the future have revealed the necessity of adapting their operations to changing circumstances on the other hand the fact that ensembles of hydrological climate impact projections contain uncertainties originating from the general circulation models gcms and the representative concentration pathway rcp scenarios is also an unavoidable challenge this study carried out to address these two issues together corroborates a two dimensional hedging model as an adaptive measure for climate change impacts and undertakes an uncertainty investigation over the gordes reservoir in western turkey first the statistically downscaled and bias corrected meteorological data from twelve gcms under two rcps were transformed into inflow projections employing a hydrological modeling procedure for the period 2021 2050 then a parameterization simulation optimization framework imposed on adapting to all scenario gcm variations was developed by integrating the hedging rule with the differential evolution algorithm the results demonstrated that adaptive hedging policies mitigated potentially derived vulnerabilities from standard operating policies ranging from 68 to 97 and kept the sustainability index above 0 75 against climate change induced water deficits finally the decomposition of uncertainty contributions in reservoir operation optimization along the gcm rcp rule curve chain was made by the analysis of variances it is obvious from the findings that the gcm was the foremost source of uncertainty in both predicted releases and key performance indices while the uncertainty from the rcp gcm interaction is of secondary importance but a noteworthy detection is that the contributions of gcms to the total variance for vulnerability and sustainability indices could be reduced by about 20 with a refined ensemble consisting of the outputs from four gcms that better represent the local predictors the interpretation of the uncertainty analysis approach which was previously performed in the literature for runoff quantiles at the reservoir scale as well as the reservoir operation optimization model which has been verified to be viable might be advantageous for water resources management abbreviations gcm general circulation model rcp representative concentration pathway cmip5 coupled model inter comparison project phase 5 sop standard operation policy rls releases room reservoir operation optimization model parsimo parameterization simulation optimization hdg 2d two dimensional hedging rule anova analysis of variance p thiessen weighted precipitation t mean temperature epan pan evaporation vmax maximum operation volume vmin dead volume hist historical scenario ncep national center for environmental prediction ncar national center for atmospheric research sra stepwise regression analysis k tau kendall tau correlation coefficient mi mutual information ann artificial neural network r2 determination coefficient nse nash sutcliffe efficiency rsr ratio of the root mean square error to the standard deviation of measured data qdm quantile delta mapping enet net evaporation bma bayesian model averaging d demand of objective function dea differential evolution algorithm rv volumetric reliability vul vulnerability def water deficit rt time based reliability si reservoir sustainability res resilience keywords climate change reservoir operation two dimensional hedging rule vulnerability sustainability index decomposing uncertainty contributions data availability data will be made available on request 1 introduction over the past 30 years the tendency of anthropogenic stress and climate change to endanger the resilience of reservoirs has become more pronounced ostad ali askari 2022 the fact that the reservoir inflows tend to alter significantly owing to climate change and differ radically from the planning situation has led to difficulties in the reservoir operation whose primary task is to settle on how much water volume will be released or retained for future periods celeste and billib 2009 adeloye and dau 2019 in this context the operating policies of the reservoirs need to be re evaluated to reliably supply the water demand which is expected to increase in the future and to ensure the sustainability of available water resources by taking into account the probable climate change effects ahmadianfar and zamani 2020 in the literature various studies looking into the responses of the reservoirs to climate change conditions through standard operation policy sop which adopts an elementary operation rule that prioritizes instant release up to the targeted demands can be found in these reports the projected hydro meteorological data derived from coupled model inter comparison project phase 5 cmip5 climate models under varied rcp scenarios constitute inputs for the reservoir operations and whether the reservoirs meet the urban or agricultural demands under those effects is interpreted with the help of several performance indices e g okkan and kirdemir 2018 nguyen et al 2020 these studies have reported that disruptive performances might take place due to climate change e g lower reliability higher vulnerabilities water shortages and so on however constructing rule curves specific to the reservoirs provides a sounder framework compared to the sop on account of adaptation to different climatic conditions adeloye and dau 2019 nourani et al 2020 regarding the issue reservoir operation optimization models rooms ranging from implicit optimization models to parameterization simulation optimization parsimo models have been developed celeste and billib 2009 in the implicit optimization which is a more complicated one than the others the deterministic room is primarily solved with dynamic or quadratic programming to find the optimal releases rls and those rls series can be conditioned on the initial storage and the reservoir inflow by using various predictive models e g karamouz and houck 1982 chandramouli and raman 2001 mousavi et al 2005 mehta and jain 2009 ji et al 2014 liu et al 2019 on the other hand parsimo starts with a set of randomly assigned rule parameter values and iteratively reaches the ultimate parameter solution that makes a particular objective function within defined constraints optimal by means of an automatic calibration algorithm draper and lund 2004 celeste and billib 2009 as the parsimo is built without exploiting two consecutive optimization processes as existed in the implicit scheme it is hypothesized to be more applicable under climate change projections while the familiar parsimo models involve variants of so called sop the non linear hedging policies enable restriction of the immediate releases at discrete times through calibrated parametric rules to mitigate both probable water shortages and reservoir vulnerabilities during dry periods celeste and billib 2009 bayesteh and azari 2021 on the other side the extraction of optimal reservoir operation policies from these models is a challenging issue due to the non linear and large scale nature of the parameters involved relevant to the subject there have been various studies in the literature on the integration of hedging models with optimization tools such as genetic algorithms beshavard et al 2022 modern meta heuristics abdollahi and ahmadianfar 2021 and multiphase hybrid algorithms mostaghimzadeh et al 2022 most of the studies dealing with the rooms parameterized with single or multi objective optimization algorithms have been performed with observed or stochastically generated synthetic inflow series that represent the historical period rather than any gcm data utilization e g draper and lund 2004 celeste and billib 2009 kumar and kasthurirengan 2018 men et al 2019 bayesteh and azari 2021 in addition there are few published papers that evaluate the climate change effects as a precursor to the development of adaptation strategies and the rooms connected with cmip5 projections e g adeloye and dau 2019 ahmadianfar and zamani 2020 nourani et al 2020 li et al 2021 thomas et al 2021 lai et al 2022 moghaddasi et al 2022 thiha et al 2022 moreover the realization that the uncertainties arising from the usage of gcms and rcp scenarios are reflected in the ensemble of hydrological climate impact projections should not be disregarded more specifically quantitative uncertainty levels gradually rise as any stage of a climate change impact assessment builds on the previous one therefore when assessing the relevant process it is necessary to examine the uncertainty holistically kim et al 2019 however to the best of our knowledge no study has attempted to deeply probe the reservoir operation policies in the context of climate change uncertainty and quantify these uncertainties with a chained multi model framework despite several works highlighting the climate change uncertainties propagated to the reservoir operation process e g raje and mujumdar 2010 karami and dariane 2018 quantifying uncertainty in projected releases from multi model climate ensembles and decomposing total uncertainty into individual and interactional contributions is still a matter of curiosity even the studies quantifying the uncertainty contribution of multiple sources in hydro meteorological projections are rather limited e g yip et al 2011 bosshard et al 2013 vetter et al 2017 wang et al 2020 and decomposing the uncertainties in the predicted reservoir releases and corresponding reservoir performances will provide decision makers with additional information about the search for potential precautions this study has been reported on the grounds that the importance of hedging based policies as a climate change adaptation measure has been stressed in a scant number of papers and that there is a research gap regarding quantifying uncertainties in the projected reservoir outcomes i e storage releases and various performance indices in this respect we used a two dimensional hedging rule termed hdg 2d integrated with a robust version of the differential evolution algorithm dea and then performed the analysis of variance anova which allows for decomposing uncertainty variances along a modeling chain combination consisting of gcms rcps and rule curves such research on both the adaptation of a reservoir to climate change under various situations and the quantification of potential factors that lead to uncertainty in adaptive operation policies has not been addressed so far the answers to the following questions have been sought over the case of a reservoir operated in western turkey what are the discrepancies in the results obtained from hdg 2d parameterized according to the historical conditions and rcp scenarios of different gcms how effective is the adaptive hedging policy in tempering potential vulnerabilities obtained through sop and ensuring sustainability what is the dominant factor that causes uncertainty in the relative changes of the projected releases and the performance indices obtained after hedging modeling to what extent do individual and interactional contributions to total ensemble uncertainty differ for gcms that better represent past climate we expect the study whose objectives are listed above to yield significant insights into the effects of different sources of uncertainty on reservoir operation optimization and help elucidate the role of gcm selection on overall ensemble uncertainty 2 data 2 1 study region and observation data the gordes watershed which is located in the western part of turkey and has a drainage area of 1045 km2 within the gediz basin was chosen as the study area fig 1 gordes reservoir is designed to provide an additional water supply of 63 mm3 year to the city of izmir and 72 mm3 year to the surrounding irrigation associations thiessen weighted precipitation p areal mean temperature t and pan evaporation epan data were compiled for the 1981 2010 water year period and the related monthly average values are given in fig 2 a epan values could be derived from the locally calibrated kharrufa equation using only the monthly mean temperature input accordingly the mean annual p t and epan regimes for the watershed where a semi arid climatic zone is found to be prevailing are around 575 mm 14 c and 1500 mm respectively in addition the reservoir inflows were obtained from the data of flow gauging station d05a028 the demands calculated according to the population projection of the network to be fed from the gordes system for the year 2050 together with the monthly average inflows are indicated in fig 2b the demand for the irrigation season with an average temperature of 23 c accounts for nearly 70 of the total demand all the data including the area volume curve of the reservoir were collected from the general directorate of state hydraulic works and the maximum operation volume vmax and dead volume vmin of the reservoir were reported as 448 46 and 18 3 mm3 respectively since kirdemir et al 2022 stressed that semi arid climatic conditions in the gediz basin may intensify over time due to the increasing radiative forcing trend it is crucial to alleviate the vulnerability of the reservoirs operated at the basin during dry periods 2 2 selection of potential predictors and gcms in the study the outputs of cmip5 models operated under rcp4 5 a stabilization scenario where the total radiative forcing stabilizes at 4 5 w m2 after year 2100 and rcp8 5 described by aggravating greenhouse gas emissions over time giving rise to a radiative forcing of 8 5 w m2 in 2100 covering the period 2021 2050 were employed besides historical scenario hereafter hist outputs of these models representing 1981 2005 past emission conditions were evaluated the gcms whose hist outputs were relatively inconsistent with local observations were eliminated and twelve of those listed in table 1 were utilized prior to the statistical downscaling of gcm outputs selecting explanatory predictors among a subset of the national center for environmental prediction and national center for atmospheric research ncep ncar reanalysis data set was made through stepwise regression analysis sra which is reasonably straightforward see yang et al 2017 when the predictant variable is p six predictors namely large scale precipitation pr the temperature at atmospheric pressure levels of 200 500 and 850 hpa i e ta200 ta500 ta850 and geopotential height at levels of 200 and 850 hpa i e zg200 zg850 are explanatory also the sra exerted on t suggests using four potential predictors namely the temperature at both the surface tas and at the pressure levels of 850 and 200 hpa as well as the geopotential height at 200 hpa in making the selection of gcms a modified version of a framework proposed by nourani et al 2019 was considered as schematized in fig 3 the kendall tau correlation coefficient k tau and mutual information mi scores were calculated between the dominant ncep ncar reanalysis predictors and the corresponding hist scenario outputs of the gcms the algorithm mentioned in bowden et al 2005 was regarded while operating mi which has the feature of capturing the nonlinear dependence between two variables for each predictor the gcms were separately ranked according to k tau and mi scores as exemplified in the second step in fig 3 herein the measures are rounded to two decimal places before being sorted in descending order when two or more rank numbers for the same set of values overlap the highest rank was returned the two dimensional result set made up of the median values of the rank numbers assigned to the selected predictors for each gcm was then analyzed with the complete linkage algorithm which is a hierarchical clustering technique as it is clear from the third step in fig 3 if the number of clusters is set to three csiro mk3 6 hadgem2 es cesm1 cam5 and miroc esm fall within the same cluster these four gcms gave lower rank values compared to the rest so their outputs were employed while building a proper multi model ensemble referred to as ens4 in this study to figure out the role of gcm selection on the modeling chain another variation that evaluates all gcms in table 1 was included in the content and was termed ens12 2 3 downscaled data in this section it was targeted to statistically downscale gcm outputs with a strategy based upon artificial neural networks ann which can act as a transfer function between ncep ncar data and local scale predictants despite the various ann topologies documented the typical feed forward neural network can be favored because of its robust performance ostad ali askari et al 2017 in ann modeling compiled data were separated into two equal parts for training the water period of 1981 1995 and testing the water period of 1996 2010 following the split sample procedure data standardization activation functions employed and training algorithm are the same as in fistikoglu and okkan 2011 calibration of each network whose number of hidden layer neurons nhn varied from 2 to 20 was conducted minimizing the root mean squared error rmse of the training period however in order to avoid overtraining the number of iterations was fixed at 50 and the configuration providing the lowest rmse and highest nse for the testing period data was preferred the models were graded pursuant to two quantitative statistics such as nash sutcliffe efficiency nse and the ratio of the root mean square error to the standard deviation of measured data rsr just as moriasi et al 2007 have recommended according to table s1 the downscaling model established for t is rated as very good during the training and testing periods while the model established for p has provided good simulation this seems reasonable given the complexity of simulating this variable following the downscaling of gcm outputs through the ann structures that provided sufficient simulation accuracy systematic distributional biases in those outputs were corrected by the quantile delta mapping qdm algorithm which explicitly preserves relative changes in both precipitation and temperature quantiles without inflating relative trends cannon et al 2015 with such approaches it is possible to narrow both the gcm and gcm scenario interaction uncertainties along the modeling chain e g miao et al 2016 however it can be clearly seen in fig 4 that these uncertainties remain largely embedded and that the gcms offer divergent anomalies it has been determined from fig 4a that temperature anomalies vary between 0 7 and 2 0 c under the rcp4 5 scenario and between 0 95 and 2 6 c under the rcp8 5 scenario it is evident that the gcm uncertainty is rather dramatic even with the temperature variable whose downscaling modeling performance is much better on the other hand the plurality of gcms used gave anomalies above about 1 c and these changes were found to be statistically significant according to the t test the significance level of 0 10 was preferred it is also obvious that the anomalies foreseen for precipitation display more variability and likewise chaoticity in terms of amount and trend direction fig 4b while some gcms including giss e2 r giss e2 h and mri cgcm3 hereinafter referred to as wet gcms predicted a non significant increase in precipitation in both rcps decreases of up to 20 were projected for nine of the gcms operated under the rcp8 5 however it has been observed that the mean values of precipitation projections derived for almost all rcp and gcm variations do not differ significantly from those of hist in this respect our results are also in line with the studies conducted with regional climate models in basins having mediterranean climate characteristics see mesta et al 2022 moreover the projected changes in precipitation and temperature were naturally reflected in the net evaporation loss enet regime the projected increases for this variable under rcp4 5 were found non significant apart from those obtained from gfdl cm3 hadgem2 es and miroc esm whilst the increases foreseen under rcp8 5 were all significant except those obtained from csiro mk3 6 gfdl esm2m and wet gcms 2 4 reservoir inflow projections in the study the five parameter versions of two lumped rainfall runoff models abcde and the dynamic water budget model dynwbm were evaluated while simulating reservoir inflows since they are attributed parametrically as parsimonious and perform well for neighboring watersheds with similar climatic conditions e g okkan and kirdemir 2020 further details about these models can be accessed elsewhere thomas 1981 zhang et al 2008 okkan and kirdemir 2020 okkan et al 2021 both models using only monthly total precipitation and temperature based potential evaporation data as inputs were calibrated with half of the observations covering the water period of 1981 1995 and then tested with the remaining part of the data to get more reliable probabilistic hydrologic predictions from two competing prediction sets the outputs of those models were weighted by the bayesian model averaging bma approach incorporating their probabilistic likelihood measures duan et al 2007 while estimating the weights the calibration data were subjected to the expectation maximization algorithm and the predictions of the bma slightly outperformed those of individual rainfall runoff models in both the calibration and testing periods table s2 the unexplained variance below 15 is indicative of the gain in median simulation accuracy achieved by harnessing this post processing scheme the downscaled and bias corrected data were then converted into reservoir inflow projections using this parameterized procedure which attributes a very good modeling and thus gives confidence in runoff simulation as can be seen in fig 4b the deviation between anomalies derived from different gcms is even more apparent for projected reservoir inflow since they are likely to have been further inflated by the interplay of variabilities over precipitation and evaporation projections besides with 8 gcms assessed under the rcp4 5 the relative changes indicated a decreasing trend while ens4 projected a 14 5 decline in mean inflow range 19 6 to 9 0 under the rcp8 5 however the gcms tended to decrease in mean inflows more as temperature anomalies were therein more distinctive for ens4 23 4 range 33 0 to 10 5 ens12 on the other hand showed up more optimistic because the decreases foreseen under this variation were non significant 3 methodology the methods i e ann based statistical downscaling conceptual rainfall runoff modeling qdm and bma used in producing hydro meteorological projections are essentially familiar and have already been briefly mentioned in the previous section therefore this section introduces the methodology that forms the core of the study the developed parsimo model and how its optimization is made are provided in sections 3 1 and 3 2 respectively the several indices used to measure how the reservoir behaves after generating hedging policies under different variations are addressed in section 3 3 the approach used when analyzing the potential uncertainties in the projected releases and performance indices obtained by the room is detailed in section 3 4 3 1 hedging rule based parsimo model in parsimo fig 5 a the simulating model is responsible for the monthly water balance based reservoir operation guided by the predefined rule curve parameters and the continuity equation for the system is applied as follows 1 v t 1 v t q t r l s t e t t in which t is the month index rls t which is functionalized by rule parameters is release volume in month t v t is the reservoir storage at the beginning of the month t v t 1 is the reservoir storage at the end of the month t q t and e t are respectively inflow and net evaporation loss volumes during month t e t is calculated iteratively as the average volume corresponding to the relevant mean surface area is not known prior to the operation the physical limitations of the reservoir system define the lower and upper bounds of releases and storage volumes as follows 2 0 r l s t d t t 3 v min v t 1 v max t where d t is demand in month t in the optimization procedure a set of predefined rule parameters is transferred from optimization to the simulation stage to get an objective function of value then a metaheuristic strategy is implemented to rearrange these parameters and it is iteratively run until the of no longer improves celeste and billib 2009 in this study a robust variant of the dea was preferred as the optimization model in that it does not need any derivatives of an objective function to search for the optimum solution and can converge fast this method and the assumed objective function are described in the following subsection as for the parsimo model a two dimensional hedging rule termed hdg 2d was exploited since it has been praised in some eminent studies e g celeste and billib 2009 ahmadianfar and zamani 2020 bayesteh and azari 2021 abdollahi and ahmadianfar 2021 the model schematically presented in fig 5b adopts a rule that applies the hedging when h t a combination of active storage and inflow is below a certain hdg τ parameter in the model m τ is also a parameter that determines the shape of the hedging region celeste and billib 2009 thus the hdg 2d model has 24 parameters to be calibrated for each month τ 1 12 and the constraints to these parameters are denoted in fig 5c while the parametric rules obtained by calibrating this model under the hist scenario outputs of any gcm are referred to as rule1 readapting the parameterization for each rcp in the same gcm is referred to as rule2 the steps regarding the implementation of hdg 2d are stated in fig 5c 3 2 optimization process in the calibration of the hdg 2d model stated above it is expected that the releases are close to the demands and at this stage the left hand side of eq 4 is usually minimized 4 of t 1 n d t r l s t d t 2 m a x 0 v min v t 1 v min 2 m a x 0 v t 1 v max v max 2 where n is the total operating horizon in months in order to mitigate the deficits in cases where storage falls below the dead volume and to keep storage levels as high as possible two penalty functions are included in the above equation in accordance with celeste et al 2008 ehteram et al 2018 and nourani et al 2020 as noted earlier the optimization algorithm used in the minimization of the objective function is a modified version of dea that introduces one competitive kind of population based evolutionary algorithm the first operator in the flowchart of a typical dea is the mutation which is performed to make random perturbations in the npop sized population storn and price 1997 despite the simple and applicable nature of the original dea it has been demonstrated in the study of leon and xiong 2014 that it can be improved with alternative mutation schemes the findings from their experiments over various benchmark functions e g different unimodal and multimodal functions have pointed out a robust mutation strategy termed current to best mutation in this approach the vector made by the difference between the current best solution xbest and the current individual and another difference vector created by two random individuals chosen from the population are summed the weighting of these difference vectors is controlled by the f1 and f2 scaling factors respectively finally the weighted difference vectors are added to the current individual to get the mutant vector gong and cai 2013 leon and xiong 2014 5 mv i g x i g f 1 x best g x i g f 2 x r 1 g x r 2 g where g is the generation index xi and xr1 xr2 represent the current individual and any randomly chosen individual respectively i r1 r2 1 2 npop while the above procedure is performed for each population member the crossover and selection operators are consecutively applied as suggested in storn and price 1997 in pursuit of several experiments f1 and f2 were fixed to 0 3 and 0 7 respectively and the crossover constant was chosen as 0 85 in the algorithm runs np and the maximum number of iterations were taken as 30 and 500 respectively 3 3 indicators of reservoir performance reservoir key performance indices are evaluated at the end of the hedging modeling and deriving releases series under various conditions the indices namely volumetric reliability and vulnerability which are directly linked to the water deficit def are defined as follows mcmahon et al 2006 sandoval solis et al 2011 6 r v t 1 n r l s t t 1 n d t 7 vul t 1 n d e f t no of times def 0 o c c u r e d m e a n o f d e m a n d s def 0 occured if the number of months of operation at dead volume is ascribed to the failure period fp the time based reliability can be expressed as follows jain 2010 8 r t 1 f p n f p n there are also attempts to combine the concepts of the indices above to produce a singular metric according to sandoval solis et al 2011 the geometric mean of several indices may give an idea about the reservoir sustainability 9 s i r t r v 1 v u l 1 3 indeed there is also a metric such as resilience res that defines how quickly a reservoir can recover from a failure however in a study conducted by mcmahon et al 2006 under historical and stochastic sequences the complementary nature of the relationship between vul and res was indicated they stated that assuming a high negative correlation between the two metrics only vul can be preferred alongside the reliability indices when determining si and this index is more tangible in quantifying water shortages as well 3 4 uncertainty decomposition a three way anova approach was used in the study to decompose total ensemble uncertainty into contributions from three members of the chain i e gcm rcp and operating rule and interactions between them first the relative changes of the projected releases compared to the hist outputs were analyzed the total sum of squares stt which is then partitioned into the several effects is estimated as follows bosshard et al 2013 10 stt t i 1 n rule j 1 n gcm k 1 n rcp y ijk t y ooo t 2 t 2021 2050 where yijk t is the release anomaly corresponding to rule curve i gcm j and rcp scenario k respectively and is equal to rls ijk t rls j h i s t rls j h i s t herein rlsj hist is the mean value of the annual releases compiled for any gcm under hist besides yooo t is the overall mean value obtained from all variations for any year t nrule and nrcp are both 2 while ngcm is 12 and 4 for ens12 and ens4 respectively as seen in eq 11 stt is partitioned into three main effects ssrule ssgcm and ssrcp pertaining to rule curves gcms and rcps respectively there are also three first order interaction terms ssrule gcm ssrule rcp and ssgcm rcp and one second order interaction term ssrule gcm rcp 11 s t t t s s rule t s s gcm t s s rcp t effects f r o m m a i n f a c t o r s s s rule g c m t s s rule r c p t s s gcm r c p t s s rule g c m r c p t i n t e r a c t i o n t e r m s as an example the calculations for the ssgcm ssgcm rcp and total interaction i t are given below 12 s s gcm t n rule n rcp j 1 n gcm y ojo t y ooo t 2 13 s s gcm r c p t n rule j 1 n gcm k 1 n rcp y ojk t y ojo t y ook t y ooo t 2 14 i t s s rule g c m t s s rule r c p t s s gcm r c p t s s rule g c m r c p t where the token denotes averaging over a certain factor or factors for instance yojo is the mean of the related anomalies compiled from all rule curves and rcp variants for the gcm j it should also be noted that the above ss terms are expressed in terms of the variance by dividing them by the total number of combinations nrule ngcm nrcp and that the uncertainty decomposition strategy could be performed for the projected reservoir performance indices regardless of time t 4 results 4 1 evaluation of release predictions as mentioned in the last paragraph of section 3 1 it is targeted to parameterize two operating rules for any gcm and produce rule1 and rule2 based releases and storage predictions the modified dea which governs the optimization step of parsimo was run 10 times given the minimization of the fitness function specified in eq 4 and the global best solutions extracted from those simulations were stored the parameter estimates of the hdg 2d model calibrated under different variations are not shown since they would take up redundant space in the text in the study it was encountered that the majority of gcms had significant drought intensities during their hist scenario periods and the water restriction characterization of the hedging model trained under these conditions and hist based adjusted parameters were found adaptable for their rcp simulations as well so this circumstance was likely to result in a strong linear dependence between rule1 and rule2 outputs as a representative example the results for the hadgem2 es are given in fig 6 besides the fact that the gcms predict decreases in inflows and increases in evaporative losses for the future period has triggered hedging to be applicated in higher zones compared to that of hist throughout their reservoir operation optimization hence the releases produced with rule2 are inclined to be slightly smaller than those of rule1 since the extent to which rule based differences contribute to uncertainty was investigated in section 4 3 we worked with the rule type independent release series under this subsection when sop based reservoir operations under hist were taken as a baseline the anomalies with regard to the averaged releases for each variation were denoted in fig 7 accordingly for the gcms such as cesm1 cam5 hadgem2 es and miroc esm that predict noticeable reductions in the runoff the sop offers significant decreases in releases under both scenarios it is immediately clear from fig 7 that hedging applied not only to these gcms but also to ens4 has mitigated the reductions in releases for example in the rcp4 5 ens4 variant the release of 3 8 more water volume through hdg 2d compared to sop incorporating no hedging has led to closing deficit of 21 during the projection period moreover the sop behaved more sensitively to the rcp8 5 whereas the deficit curbing attainment of hdg 2d was more prominent in pessimistic conditions for rcp8 5 ens4 this approach resulted in the release of 6 2 more supply compared to the sop and prevented a 27 deficit in addition it seems usual that hdg 2d is not efficient under increasing inflow conditions which were provided by several gcms including wet gcms and with sufficient releases even with sop in order to exhibit the inter annual variabilities in the operation optimization during 2021 2050 the quartiles of projected releases from all variants constituting ens4 were denoted in fig 8 this figure also showed the ensemble averages regarding the hist scenario releases that were derived from hdg 2d this may address the query of how the reservoir would have been affected by merely climate change if hedging had been put into action in the past as is seen in fig 8 while the expected values of the releases in the early years of the projection period increased compared to that of hist in the last 20 years they showed a significant decrease notwithstanding the application of hedging this was detected in all quartiles not just in the mean values besides the fluctuations of the median lines marked in the same figure are rather volatile and the non parametric mann whitney u test with a significance level of 0 10 also verifies that the relevant distributions along those continuous sequences do not match that of hist not shown furthermore the fact that the interquartile ranges were conspicuously larger than that of hist in almost all years prompted us to conduct an uncertainty analysis as elaborated later in section 4 3 4 2 reservoir performance indices based assessment the investigation of reservoir performance indices is also required to demonstrate the acknowledged effect of the hedging policy utilized for a rule type independent rendition the reservoir performance indices derived from rule1 and rule2 were averaged for each gcm rcp variation and the alterations achieved compared to the sop based estimation for the related variation were revealed in fig 9 the indices recorded for ens4 by both operating approaches are also shown in table 2 without hedging vulnerability is high even with some gcms that provide benign inflow conditions and it intensifies to approximately 0 81 for ens4 under both scenarios at this point the most fascinating aspect of hdg 2d is that it curbed the excessive vulnerabilities offered by sop under the effects of climate change and enabled the acquisition of vul values below the 0 25 tolerable threshold suggested by adeloye and dau 2019 since the hedging is mainly deployed to limit single period shortages by deliberately allocating the potential deficits to the entire operating period with specific rule parameters these results seem reasonable although the hedging intentionally adds more failures i e situations of insufficient supply to fully meet demand and brings about volumetric reliability to deteriorate the hdg 2d gave a larger rv in many gcm rcp variations compared to the sop yet the differences between them are indistinguishable and this is an indication that the developed strategy would conserve the system s performance in meeting the total demand even with ens4 that predicted a significant reduction in inflows as for rt we focused on examining the frequency of periods operated at dead volume using the form of eq 8 rather than its routine usage that takes account of any positive deficit even a small one according to fig 9 the hdg 2d appeared to set forth greater improvement for the gcms with dry characterization in time based reliability but the improvements of up to 16 can be deemed marginal all these results show that the effect of the hedging on the two different reliability indices is not as great as in vul and thus the improvements made for vul drastically contribute to the sustainability of the system as a result the built model has tempered the huge vulnerability associated with the pessimistic conditions and enhanced modest sustainability indeed for all the gcm rcp variations the si has been raised to above 0 75 for example the si of around 0 5 given by the sop for rcp8 5 ens4 has increased by 70 with the hdg 2d this result is notable given that only 16 of the demanded volume was withheld for this variant it can be deduced from table 2 how the indices for ens4 might change over the 2021 2050 period assuming that hedging is dynamically deployed in all scenarios including hist the results that put emphasis on solely climate change effects in a reservoir system where sustainability is sought suggest that the deterioration in both reliability indices is less than 11 whereas there seemed to be a much more dramatic impact on the vul nonetheless as the vul values are within acceptable limits by virtue of hedging in all scenarios the si has deteriorated only marginally 4 3 evaluation of uncertainties while it has already been explained in the previous subsections that hdg 2d can be responsible for executing the adaptation process in response to climate change a quantitative revelation of which sources dominate the uncertainty containing process has been made in this section the temporal results of the anova exerted on the release anomalies during 2021 2050 are shown in fig 10 from the determinations obtained for both ens12 and ens4 it is clear that both gcm and total interaction uncertainties have an increasing trend over time in ens12 the individual gcm uncertainty over the last 15 years has displayed a relatively large leap compared to other sources whilst this source has accounted for 60 of the total cumulative uncertainty besides the rule based uncertainty tends to augment over time in both cases but it is overwhelmed under the gcm or even the rcp whose contribution to the total ensemble uncertainty is nearly 2 for the ens12 even though the gcm uncertainty has decreased by 38 for the ens4 and the related trend line has become more stationary the uncertainty over the reservoir operation optimization process continues to be dominated by the gcm as typically highlighted in streamflow projection studies e g vetter et al 2017 bosshard et al 2013 kim et al 2019 the uncertainties stemming from the usage of the operation rule and rcp as well as their interactions are somewhat more noticeable for ens4 but their fractional contribution to the total uncertainty is still marginal even if the rcp usage contributes 10 more variance in the ens4 option compared to that of ens12 the total uncertainty derived from all sources within the chain is comparatively low in ens4 i e what is mentioned is roughly a 15 reduction in both mean and interquartile range statistics for total uncertainty additionally in the ens4 from about half of the projection period the frequency of occasions in which the contribution of total interaction uncertainty is greater than the contribution directly provided by gcm has risen this is due to the fact that the mean temperature is predicted to be 0 4 0 5 c higher with ens4 in specific years i e 2039 and 2043 compared to those in ens12 hence the whisker above the 75th percentile has relatively enlarged for the respective gcm rcp interaction not shown furthermore fig 11 depicts the variance decomposition results for the performance indices under both ens12 and ens4 the gcm originated uncertainty for all indices is the most obvious and has also been alleviated by opting for ens4 this inference is inherently consistent with the information from fig 10 the fact that the gcm based uncertainty of the ens12 was more prominent for the vul made the overall uncertainty regarding this index vastly higher this is because the vulnerability derived from wet gcms which provide a non significant rise in mean inflows is essentially minor in comparison to the rest and their presence has led to an increase in the variability amongst gcms as for ens12 furthermore because some gcms such as hadgem2 es and miroc esm have projected inflows with dense critical periods they behaved more sensitively to the operation rule and thus caused the rule based uncertainty contribution on the rv and vul indices in ens4 of which they are a member to increase by 12 compared to those of ens12 since the radiative forcing tendencies of the two rcps during the available projection period are similar meinshausen et al 2011 the influence of scenario uncertainty on rv and vul typically remains marginal just as the differences in the emission conditions of the scenarios after 2050 are reflected in the uncertainties in the temperature forecasts e g jiao and yuan 2019 the rcp based uncertainty in the reservoir performance indices will inevitably become more apparent with the use of longer term projections moreover fig 11 clearly reveals that the contribution of rcp uncertainty to rt under ens4 is comparatively greater than that under ens12 the primary reason for this is that the time based reliabilities from cesm1 cam5 and miroc esm under rcp8 5 are approximately 10 smaller than the values under rcp4 5 this is less apparent in other gcms and is mainly linked to the relatively high frequencies of low inflows values below the first quartile produced by these gcms under rcp8 5 as well as the fact that the amount demanded from the reservoir during the irrigation season constitutes an important part of the annual demand as to the uncertainties in the si the purpose of which is to facilitate the measurement and identification of policies that improve reservoir planning when there are trade offs between criteria it has been determined that the gcm uncertainty and total variance can be lessened by 59 and 44 respectively with the ens4 preference in contrast the contribution of scenario induced uncertainty to the si has slightly risen and scenario gcm interaction uncertainty is now of secondary importance following gcm uncertainty the fact that selecting the appropriate set of gcms gives the opportunity to decrease the overall amount of uncertainty is nonetheless promising 5 discussion and conclusions in the study a hedging policy which provides an arguably simpler way than implicit models was developed to carry out the process of adapting a reservoir to uncertain climate change projections to produce hydro meteorological projections gcm sets mismatched with locally observed variables such as precipitation and temperature were excluded and 12 gcm data sets from the cmip5 archive were dissected then based on the findings in fig 3 a more refined ensemble set of four gcms was adopted recognizing the relevance of local predictors the degradation of the climatic signal and trend structure reported in conventional quantile mapping based bias correction was not detected in our study thanks to the efficient algorithm of qdm although the virtue of qdm is to narrow the gcm scenario uncertainty it is quite unequivocal that different gcms predict highly varied anomalies for example the hydrological modeling procedure whose simulation performance was validated offered inflow anomalies ranging from 33 to 19 despite being run with bias corrected data afterward we optimized the hdg 2d governed by two rules by means of dea and compared its performance with that of an elementary sop with a view to inspecting the adaptive capabilities of the proposed strategy under climate change induced deficits our results suggest that without hedging reservoir performance could aggravate being contingent on pessimistic inflow conditions whereas the hedging model resulted in markedly reduced vulnerabilities keeping them below the 25 threshold recommended by adeloye and dau 2019 and not causing a discernible decline in rv all this indicates that the gordes reservoir if operated with hedging policies has a buffering capacity and can ensure its sustainability reasonably well even in drier climatic conditions on the other hand the likelihood that releases would cut back in the last 20 years of the projected period compared to the past has pointed out that despite a hedging policy the reservoir may exhibit responsiveness to the effects of climate change furthermore the conclusion made by wang et al 2020 that fewer gcms would contribute more to the total uncertainty was not verified at this reservoir scale as the contrary was shown in ens4 of course in such a case the process continues to be heavily affected by gcm uncertainty and its reduction is not related to the number of gcms but to their qualifications it has also been designated that with the inclusion of ens4 the rcp and operating rule uncertainties have increased slightly but their contribution to the overall uncertainty is negligible as it has also been previously demonstrated by the observed data that the rule parameter uncertainty is lower than the inflow prediction uncertainty see liu et al 2019 it might not come as a surprise that the gcm uncertainty outweighs the rule curve uncertainty apart from all these concluding remarks several issues that are open to discussion and that we intend to deal with in the future are listed below although it has been documented that gcm performances vary in terms of spatial resolution and variables analyzed no attempt has been made to weight the relevant gcm outputs in analyses conducted with ens12 and ens4 wang et al 2019 emphasized in their work that when downscaling and bias correction are consecutively implemented the unequal weighting methods have not brought evident influences on the ensembles of hydrological climate impact projections nonetheless to what extent different weighting approaches can affect the uncertainty of both the gcm and the sources interacting with the gcm will be further investigated in this context there are various methods ranging from multi criteria weighting to performance based weighting and it will be beneficial to examine their performance with out of sample testing wang et al 2019 on the other side the employed downscaling strategy is statistically based and regarded as viable only if specific assumptions are met schoof 2013 in the study even if it is considered that the local observations are compatible with potential predictors and that the calibration and testing of the statistical downscaling models were graded as good or very good it should be noted that more credible results might be obtained with the regional climate models which reflect the physical geography characteristics jiao and yuan 2019 another limited aspect of the study is that inflow simulations have been made exclusively through lumped type hydrological models mandal and simonovic 2017 and velazquez et al 2013 evaluated both the lumped and distributed hydrological models and stated that the uncertainty arising from hydrological modeling over long term mean streamflow is relatively inconsiderable whereas the type of model and even its parameterization contributed dramatically to the low flow predictions as opposed to that of the gcm this situation is likely to affect the operational practices and is worth scrutinizing especially given indices such as rt and vul that are sensitive to low inflow sequences in the projected data since the a priori selection of a suitable optimization technique for a given hedging model and the hydro climatological condition is not straightforward our assessment resorts to hedging modeling under gcm rcp variants and decomposing uncertainty in the modeling chain however it should be noted that developing optimization techniques is a worthy endeavor for setting operational rules and that some recent meta heuristics could outperform other optimizers in terms of solution accuracy and convergence rate as abdollahi and ahmadianfar 2021 have mentioned hence questioning the extent to which different meta heuristics add uncertainty to a similar process is also worth examining in a future study flow variability is a crucial determinant of a healthy river environment therefore discharging a certain volume of water might potentially mitigate the negative impact of reservoirs on riverine ecosystems in this context suwal et al 2020 have formulated a multi objective reservoir optimization in order to maximize power generation while minimizing the deficiency of environmental flows e flows utilizing a non dominated sorting genetic algorithm moreover a rare number of studies have examined the impact of flow regime on both e flows and energy outputs in the case of run of river hydropower plants kuriqi et al 2019a 2019b and the trade offs between hydropower production and ecosystem protection under different e flow scenarios bejarano et al 2019 kuriqi et al 2017 one of our upcoming projects will include conducting such research in perspective of the uncertainties introduced by gcms and rcps there are also other studies devoted to decomposing uncertainties for example lee et al 2017 have proposed a method based on the principle of maximum entropy to illustrate how uncertainty is propagated at each stage of the projection the situation in which the sum of the uncertainties at each stage is not equal to the total uncertainty peculiar to their method is not encountered with the anova which is more theoretically sound a comparative examination with emerging methods such as uncertainty decomposition via the bayesian perspective would be of great value credit authorship contribution statement umut okkan funding acquisition conceptualization supervision methodology software data curation formal analysis writing original draft visualization investigation writing review editing okan fistikoglu conceptualization supervision methodology investigation writing review editing zeynep beril ersoy software formal analysis data curation writing original draft writing review editing visualization investigation ahmad tamim noori data curation formal analysis visualization declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests umut okkan reports financial support was provided by scientific and technological research council of turkey acknowledgments this study reported herein was funded by the scientific and technological research council of turkey under grant no 121y037 four reviewers and editors provided insightful feedback that significantly strengthened our paper for which we are extremely thankful appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2023 129286 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2098,concerns about whether the reservoirs against climate change will be able to fulfill their missions in the future have revealed the necessity of adapting their operations to changing circumstances on the other hand the fact that ensembles of hydrological climate impact projections contain uncertainties originating from the general circulation models gcms and the representative concentration pathway rcp scenarios is also an unavoidable challenge this study carried out to address these two issues together corroborates a two dimensional hedging model as an adaptive measure for climate change impacts and undertakes an uncertainty investigation over the gordes reservoir in western turkey first the statistically downscaled and bias corrected meteorological data from twelve gcms under two rcps were transformed into inflow projections employing a hydrological modeling procedure for the period 2021 2050 then a parameterization simulation optimization framework imposed on adapting to all scenario gcm variations was developed by integrating the hedging rule with the differential evolution algorithm the results demonstrated that adaptive hedging policies mitigated potentially derived vulnerabilities from standard operating policies ranging from 68 to 97 and kept the sustainability index above 0 75 against climate change induced water deficits finally the decomposition of uncertainty contributions in reservoir operation optimization along the gcm rcp rule curve chain was made by the analysis of variances it is obvious from the findings that the gcm was the foremost source of uncertainty in both predicted releases and key performance indices while the uncertainty from the rcp gcm interaction is of secondary importance but a noteworthy detection is that the contributions of gcms to the total variance for vulnerability and sustainability indices could be reduced by about 20 with a refined ensemble consisting of the outputs from four gcms that better represent the local predictors the interpretation of the uncertainty analysis approach which was previously performed in the literature for runoff quantiles at the reservoir scale as well as the reservoir operation optimization model which has been verified to be viable might be advantageous for water resources management abbreviations gcm general circulation model rcp representative concentration pathway cmip5 coupled model inter comparison project phase 5 sop standard operation policy rls releases room reservoir operation optimization model parsimo parameterization simulation optimization hdg 2d two dimensional hedging rule anova analysis of variance p thiessen weighted precipitation t mean temperature epan pan evaporation vmax maximum operation volume vmin dead volume hist historical scenario ncep national center for environmental prediction ncar national center for atmospheric research sra stepwise regression analysis k tau kendall tau correlation coefficient mi mutual information ann artificial neural network r2 determination coefficient nse nash sutcliffe efficiency rsr ratio of the root mean square error to the standard deviation of measured data qdm quantile delta mapping enet net evaporation bma bayesian model averaging d demand of objective function dea differential evolution algorithm rv volumetric reliability vul vulnerability def water deficit rt time based reliability si reservoir sustainability res resilience keywords climate change reservoir operation two dimensional hedging rule vulnerability sustainability index decomposing uncertainty contributions data availability data will be made available on request 1 introduction over the past 30 years the tendency of anthropogenic stress and climate change to endanger the resilience of reservoirs has become more pronounced ostad ali askari 2022 the fact that the reservoir inflows tend to alter significantly owing to climate change and differ radically from the planning situation has led to difficulties in the reservoir operation whose primary task is to settle on how much water volume will be released or retained for future periods celeste and billib 2009 adeloye and dau 2019 in this context the operating policies of the reservoirs need to be re evaluated to reliably supply the water demand which is expected to increase in the future and to ensure the sustainability of available water resources by taking into account the probable climate change effects ahmadianfar and zamani 2020 in the literature various studies looking into the responses of the reservoirs to climate change conditions through standard operation policy sop which adopts an elementary operation rule that prioritizes instant release up to the targeted demands can be found in these reports the projected hydro meteorological data derived from coupled model inter comparison project phase 5 cmip5 climate models under varied rcp scenarios constitute inputs for the reservoir operations and whether the reservoirs meet the urban or agricultural demands under those effects is interpreted with the help of several performance indices e g okkan and kirdemir 2018 nguyen et al 2020 these studies have reported that disruptive performances might take place due to climate change e g lower reliability higher vulnerabilities water shortages and so on however constructing rule curves specific to the reservoirs provides a sounder framework compared to the sop on account of adaptation to different climatic conditions adeloye and dau 2019 nourani et al 2020 regarding the issue reservoir operation optimization models rooms ranging from implicit optimization models to parameterization simulation optimization parsimo models have been developed celeste and billib 2009 in the implicit optimization which is a more complicated one than the others the deterministic room is primarily solved with dynamic or quadratic programming to find the optimal releases rls and those rls series can be conditioned on the initial storage and the reservoir inflow by using various predictive models e g karamouz and houck 1982 chandramouli and raman 2001 mousavi et al 2005 mehta and jain 2009 ji et al 2014 liu et al 2019 on the other hand parsimo starts with a set of randomly assigned rule parameter values and iteratively reaches the ultimate parameter solution that makes a particular objective function within defined constraints optimal by means of an automatic calibration algorithm draper and lund 2004 celeste and billib 2009 as the parsimo is built without exploiting two consecutive optimization processes as existed in the implicit scheme it is hypothesized to be more applicable under climate change projections while the familiar parsimo models involve variants of so called sop the non linear hedging policies enable restriction of the immediate releases at discrete times through calibrated parametric rules to mitigate both probable water shortages and reservoir vulnerabilities during dry periods celeste and billib 2009 bayesteh and azari 2021 on the other side the extraction of optimal reservoir operation policies from these models is a challenging issue due to the non linear and large scale nature of the parameters involved relevant to the subject there have been various studies in the literature on the integration of hedging models with optimization tools such as genetic algorithms beshavard et al 2022 modern meta heuristics abdollahi and ahmadianfar 2021 and multiphase hybrid algorithms mostaghimzadeh et al 2022 most of the studies dealing with the rooms parameterized with single or multi objective optimization algorithms have been performed with observed or stochastically generated synthetic inflow series that represent the historical period rather than any gcm data utilization e g draper and lund 2004 celeste and billib 2009 kumar and kasthurirengan 2018 men et al 2019 bayesteh and azari 2021 in addition there are few published papers that evaluate the climate change effects as a precursor to the development of adaptation strategies and the rooms connected with cmip5 projections e g adeloye and dau 2019 ahmadianfar and zamani 2020 nourani et al 2020 li et al 2021 thomas et al 2021 lai et al 2022 moghaddasi et al 2022 thiha et al 2022 moreover the realization that the uncertainties arising from the usage of gcms and rcp scenarios are reflected in the ensemble of hydrological climate impact projections should not be disregarded more specifically quantitative uncertainty levels gradually rise as any stage of a climate change impact assessment builds on the previous one therefore when assessing the relevant process it is necessary to examine the uncertainty holistically kim et al 2019 however to the best of our knowledge no study has attempted to deeply probe the reservoir operation policies in the context of climate change uncertainty and quantify these uncertainties with a chained multi model framework despite several works highlighting the climate change uncertainties propagated to the reservoir operation process e g raje and mujumdar 2010 karami and dariane 2018 quantifying uncertainty in projected releases from multi model climate ensembles and decomposing total uncertainty into individual and interactional contributions is still a matter of curiosity even the studies quantifying the uncertainty contribution of multiple sources in hydro meteorological projections are rather limited e g yip et al 2011 bosshard et al 2013 vetter et al 2017 wang et al 2020 and decomposing the uncertainties in the predicted reservoir releases and corresponding reservoir performances will provide decision makers with additional information about the search for potential precautions this study has been reported on the grounds that the importance of hedging based policies as a climate change adaptation measure has been stressed in a scant number of papers and that there is a research gap regarding quantifying uncertainties in the projected reservoir outcomes i e storage releases and various performance indices in this respect we used a two dimensional hedging rule termed hdg 2d integrated with a robust version of the differential evolution algorithm dea and then performed the analysis of variance anova which allows for decomposing uncertainty variances along a modeling chain combination consisting of gcms rcps and rule curves such research on both the adaptation of a reservoir to climate change under various situations and the quantification of potential factors that lead to uncertainty in adaptive operation policies has not been addressed so far the answers to the following questions have been sought over the case of a reservoir operated in western turkey what are the discrepancies in the results obtained from hdg 2d parameterized according to the historical conditions and rcp scenarios of different gcms how effective is the adaptive hedging policy in tempering potential vulnerabilities obtained through sop and ensuring sustainability what is the dominant factor that causes uncertainty in the relative changes of the projected releases and the performance indices obtained after hedging modeling to what extent do individual and interactional contributions to total ensemble uncertainty differ for gcms that better represent past climate we expect the study whose objectives are listed above to yield significant insights into the effects of different sources of uncertainty on reservoir operation optimization and help elucidate the role of gcm selection on overall ensemble uncertainty 2 data 2 1 study region and observation data the gordes watershed which is located in the western part of turkey and has a drainage area of 1045 km2 within the gediz basin was chosen as the study area fig 1 gordes reservoir is designed to provide an additional water supply of 63 mm3 year to the city of izmir and 72 mm3 year to the surrounding irrigation associations thiessen weighted precipitation p areal mean temperature t and pan evaporation epan data were compiled for the 1981 2010 water year period and the related monthly average values are given in fig 2 a epan values could be derived from the locally calibrated kharrufa equation using only the monthly mean temperature input accordingly the mean annual p t and epan regimes for the watershed where a semi arid climatic zone is found to be prevailing are around 575 mm 14 c and 1500 mm respectively in addition the reservoir inflows were obtained from the data of flow gauging station d05a028 the demands calculated according to the population projection of the network to be fed from the gordes system for the year 2050 together with the monthly average inflows are indicated in fig 2b the demand for the irrigation season with an average temperature of 23 c accounts for nearly 70 of the total demand all the data including the area volume curve of the reservoir were collected from the general directorate of state hydraulic works and the maximum operation volume vmax and dead volume vmin of the reservoir were reported as 448 46 and 18 3 mm3 respectively since kirdemir et al 2022 stressed that semi arid climatic conditions in the gediz basin may intensify over time due to the increasing radiative forcing trend it is crucial to alleviate the vulnerability of the reservoirs operated at the basin during dry periods 2 2 selection of potential predictors and gcms in the study the outputs of cmip5 models operated under rcp4 5 a stabilization scenario where the total radiative forcing stabilizes at 4 5 w m2 after year 2100 and rcp8 5 described by aggravating greenhouse gas emissions over time giving rise to a radiative forcing of 8 5 w m2 in 2100 covering the period 2021 2050 were employed besides historical scenario hereafter hist outputs of these models representing 1981 2005 past emission conditions were evaluated the gcms whose hist outputs were relatively inconsistent with local observations were eliminated and twelve of those listed in table 1 were utilized prior to the statistical downscaling of gcm outputs selecting explanatory predictors among a subset of the national center for environmental prediction and national center for atmospheric research ncep ncar reanalysis data set was made through stepwise regression analysis sra which is reasonably straightforward see yang et al 2017 when the predictant variable is p six predictors namely large scale precipitation pr the temperature at atmospheric pressure levels of 200 500 and 850 hpa i e ta200 ta500 ta850 and geopotential height at levels of 200 and 850 hpa i e zg200 zg850 are explanatory also the sra exerted on t suggests using four potential predictors namely the temperature at both the surface tas and at the pressure levels of 850 and 200 hpa as well as the geopotential height at 200 hpa in making the selection of gcms a modified version of a framework proposed by nourani et al 2019 was considered as schematized in fig 3 the kendall tau correlation coefficient k tau and mutual information mi scores were calculated between the dominant ncep ncar reanalysis predictors and the corresponding hist scenario outputs of the gcms the algorithm mentioned in bowden et al 2005 was regarded while operating mi which has the feature of capturing the nonlinear dependence between two variables for each predictor the gcms were separately ranked according to k tau and mi scores as exemplified in the second step in fig 3 herein the measures are rounded to two decimal places before being sorted in descending order when two or more rank numbers for the same set of values overlap the highest rank was returned the two dimensional result set made up of the median values of the rank numbers assigned to the selected predictors for each gcm was then analyzed with the complete linkage algorithm which is a hierarchical clustering technique as it is clear from the third step in fig 3 if the number of clusters is set to three csiro mk3 6 hadgem2 es cesm1 cam5 and miroc esm fall within the same cluster these four gcms gave lower rank values compared to the rest so their outputs were employed while building a proper multi model ensemble referred to as ens4 in this study to figure out the role of gcm selection on the modeling chain another variation that evaluates all gcms in table 1 was included in the content and was termed ens12 2 3 downscaled data in this section it was targeted to statistically downscale gcm outputs with a strategy based upon artificial neural networks ann which can act as a transfer function between ncep ncar data and local scale predictants despite the various ann topologies documented the typical feed forward neural network can be favored because of its robust performance ostad ali askari et al 2017 in ann modeling compiled data were separated into two equal parts for training the water period of 1981 1995 and testing the water period of 1996 2010 following the split sample procedure data standardization activation functions employed and training algorithm are the same as in fistikoglu and okkan 2011 calibration of each network whose number of hidden layer neurons nhn varied from 2 to 20 was conducted minimizing the root mean squared error rmse of the training period however in order to avoid overtraining the number of iterations was fixed at 50 and the configuration providing the lowest rmse and highest nse for the testing period data was preferred the models were graded pursuant to two quantitative statistics such as nash sutcliffe efficiency nse and the ratio of the root mean square error to the standard deviation of measured data rsr just as moriasi et al 2007 have recommended according to table s1 the downscaling model established for t is rated as very good during the training and testing periods while the model established for p has provided good simulation this seems reasonable given the complexity of simulating this variable following the downscaling of gcm outputs through the ann structures that provided sufficient simulation accuracy systematic distributional biases in those outputs were corrected by the quantile delta mapping qdm algorithm which explicitly preserves relative changes in both precipitation and temperature quantiles without inflating relative trends cannon et al 2015 with such approaches it is possible to narrow both the gcm and gcm scenario interaction uncertainties along the modeling chain e g miao et al 2016 however it can be clearly seen in fig 4 that these uncertainties remain largely embedded and that the gcms offer divergent anomalies it has been determined from fig 4a that temperature anomalies vary between 0 7 and 2 0 c under the rcp4 5 scenario and between 0 95 and 2 6 c under the rcp8 5 scenario it is evident that the gcm uncertainty is rather dramatic even with the temperature variable whose downscaling modeling performance is much better on the other hand the plurality of gcms used gave anomalies above about 1 c and these changes were found to be statistically significant according to the t test the significance level of 0 10 was preferred it is also obvious that the anomalies foreseen for precipitation display more variability and likewise chaoticity in terms of amount and trend direction fig 4b while some gcms including giss e2 r giss e2 h and mri cgcm3 hereinafter referred to as wet gcms predicted a non significant increase in precipitation in both rcps decreases of up to 20 were projected for nine of the gcms operated under the rcp8 5 however it has been observed that the mean values of precipitation projections derived for almost all rcp and gcm variations do not differ significantly from those of hist in this respect our results are also in line with the studies conducted with regional climate models in basins having mediterranean climate characteristics see mesta et al 2022 moreover the projected changes in precipitation and temperature were naturally reflected in the net evaporation loss enet regime the projected increases for this variable under rcp4 5 were found non significant apart from those obtained from gfdl cm3 hadgem2 es and miroc esm whilst the increases foreseen under rcp8 5 were all significant except those obtained from csiro mk3 6 gfdl esm2m and wet gcms 2 4 reservoir inflow projections in the study the five parameter versions of two lumped rainfall runoff models abcde and the dynamic water budget model dynwbm were evaluated while simulating reservoir inflows since they are attributed parametrically as parsimonious and perform well for neighboring watersheds with similar climatic conditions e g okkan and kirdemir 2020 further details about these models can be accessed elsewhere thomas 1981 zhang et al 2008 okkan and kirdemir 2020 okkan et al 2021 both models using only monthly total precipitation and temperature based potential evaporation data as inputs were calibrated with half of the observations covering the water period of 1981 1995 and then tested with the remaining part of the data to get more reliable probabilistic hydrologic predictions from two competing prediction sets the outputs of those models were weighted by the bayesian model averaging bma approach incorporating their probabilistic likelihood measures duan et al 2007 while estimating the weights the calibration data were subjected to the expectation maximization algorithm and the predictions of the bma slightly outperformed those of individual rainfall runoff models in both the calibration and testing periods table s2 the unexplained variance below 15 is indicative of the gain in median simulation accuracy achieved by harnessing this post processing scheme the downscaled and bias corrected data were then converted into reservoir inflow projections using this parameterized procedure which attributes a very good modeling and thus gives confidence in runoff simulation as can be seen in fig 4b the deviation between anomalies derived from different gcms is even more apparent for projected reservoir inflow since they are likely to have been further inflated by the interplay of variabilities over precipitation and evaporation projections besides with 8 gcms assessed under the rcp4 5 the relative changes indicated a decreasing trend while ens4 projected a 14 5 decline in mean inflow range 19 6 to 9 0 under the rcp8 5 however the gcms tended to decrease in mean inflows more as temperature anomalies were therein more distinctive for ens4 23 4 range 33 0 to 10 5 ens12 on the other hand showed up more optimistic because the decreases foreseen under this variation were non significant 3 methodology the methods i e ann based statistical downscaling conceptual rainfall runoff modeling qdm and bma used in producing hydro meteorological projections are essentially familiar and have already been briefly mentioned in the previous section therefore this section introduces the methodology that forms the core of the study the developed parsimo model and how its optimization is made are provided in sections 3 1 and 3 2 respectively the several indices used to measure how the reservoir behaves after generating hedging policies under different variations are addressed in section 3 3 the approach used when analyzing the potential uncertainties in the projected releases and performance indices obtained by the room is detailed in section 3 4 3 1 hedging rule based parsimo model in parsimo fig 5 a the simulating model is responsible for the monthly water balance based reservoir operation guided by the predefined rule curve parameters and the continuity equation for the system is applied as follows 1 v t 1 v t q t r l s t e t t in which t is the month index rls t which is functionalized by rule parameters is release volume in month t v t is the reservoir storage at the beginning of the month t v t 1 is the reservoir storage at the end of the month t q t and e t are respectively inflow and net evaporation loss volumes during month t e t is calculated iteratively as the average volume corresponding to the relevant mean surface area is not known prior to the operation the physical limitations of the reservoir system define the lower and upper bounds of releases and storage volumes as follows 2 0 r l s t d t t 3 v min v t 1 v max t where d t is demand in month t in the optimization procedure a set of predefined rule parameters is transferred from optimization to the simulation stage to get an objective function of value then a metaheuristic strategy is implemented to rearrange these parameters and it is iteratively run until the of no longer improves celeste and billib 2009 in this study a robust variant of the dea was preferred as the optimization model in that it does not need any derivatives of an objective function to search for the optimum solution and can converge fast this method and the assumed objective function are described in the following subsection as for the parsimo model a two dimensional hedging rule termed hdg 2d was exploited since it has been praised in some eminent studies e g celeste and billib 2009 ahmadianfar and zamani 2020 bayesteh and azari 2021 abdollahi and ahmadianfar 2021 the model schematically presented in fig 5b adopts a rule that applies the hedging when h t a combination of active storage and inflow is below a certain hdg τ parameter in the model m τ is also a parameter that determines the shape of the hedging region celeste and billib 2009 thus the hdg 2d model has 24 parameters to be calibrated for each month τ 1 12 and the constraints to these parameters are denoted in fig 5c while the parametric rules obtained by calibrating this model under the hist scenario outputs of any gcm are referred to as rule1 readapting the parameterization for each rcp in the same gcm is referred to as rule2 the steps regarding the implementation of hdg 2d are stated in fig 5c 3 2 optimization process in the calibration of the hdg 2d model stated above it is expected that the releases are close to the demands and at this stage the left hand side of eq 4 is usually minimized 4 of t 1 n d t r l s t d t 2 m a x 0 v min v t 1 v min 2 m a x 0 v t 1 v max v max 2 where n is the total operating horizon in months in order to mitigate the deficits in cases where storage falls below the dead volume and to keep storage levels as high as possible two penalty functions are included in the above equation in accordance with celeste et al 2008 ehteram et al 2018 and nourani et al 2020 as noted earlier the optimization algorithm used in the minimization of the objective function is a modified version of dea that introduces one competitive kind of population based evolutionary algorithm the first operator in the flowchart of a typical dea is the mutation which is performed to make random perturbations in the npop sized population storn and price 1997 despite the simple and applicable nature of the original dea it has been demonstrated in the study of leon and xiong 2014 that it can be improved with alternative mutation schemes the findings from their experiments over various benchmark functions e g different unimodal and multimodal functions have pointed out a robust mutation strategy termed current to best mutation in this approach the vector made by the difference between the current best solution xbest and the current individual and another difference vector created by two random individuals chosen from the population are summed the weighting of these difference vectors is controlled by the f1 and f2 scaling factors respectively finally the weighted difference vectors are added to the current individual to get the mutant vector gong and cai 2013 leon and xiong 2014 5 mv i g x i g f 1 x best g x i g f 2 x r 1 g x r 2 g where g is the generation index xi and xr1 xr2 represent the current individual and any randomly chosen individual respectively i r1 r2 1 2 npop while the above procedure is performed for each population member the crossover and selection operators are consecutively applied as suggested in storn and price 1997 in pursuit of several experiments f1 and f2 were fixed to 0 3 and 0 7 respectively and the crossover constant was chosen as 0 85 in the algorithm runs np and the maximum number of iterations were taken as 30 and 500 respectively 3 3 indicators of reservoir performance reservoir key performance indices are evaluated at the end of the hedging modeling and deriving releases series under various conditions the indices namely volumetric reliability and vulnerability which are directly linked to the water deficit def are defined as follows mcmahon et al 2006 sandoval solis et al 2011 6 r v t 1 n r l s t t 1 n d t 7 vul t 1 n d e f t no of times def 0 o c c u r e d m e a n o f d e m a n d s def 0 occured if the number of months of operation at dead volume is ascribed to the failure period fp the time based reliability can be expressed as follows jain 2010 8 r t 1 f p n f p n there are also attempts to combine the concepts of the indices above to produce a singular metric according to sandoval solis et al 2011 the geometric mean of several indices may give an idea about the reservoir sustainability 9 s i r t r v 1 v u l 1 3 indeed there is also a metric such as resilience res that defines how quickly a reservoir can recover from a failure however in a study conducted by mcmahon et al 2006 under historical and stochastic sequences the complementary nature of the relationship between vul and res was indicated they stated that assuming a high negative correlation between the two metrics only vul can be preferred alongside the reliability indices when determining si and this index is more tangible in quantifying water shortages as well 3 4 uncertainty decomposition a three way anova approach was used in the study to decompose total ensemble uncertainty into contributions from three members of the chain i e gcm rcp and operating rule and interactions between them first the relative changes of the projected releases compared to the hist outputs were analyzed the total sum of squares stt which is then partitioned into the several effects is estimated as follows bosshard et al 2013 10 stt t i 1 n rule j 1 n gcm k 1 n rcp y ijk t y ooo t 2 t 2021 2050 where yijk t is the release anomaly corresponding to rule curve i gcm j and rcp scenario k respectively and is equal to rls ijk t rls j h i s t rls j h i s t herein rlsj hist is the mean value of the annual releases compiled for any gcm under hist besides yooo t is the overall mean value obtained from all variations for any year t nrule and nrcp are both 2 while ngcm is 12 and 4 for ens12 and ens4 respectively as seen in eq 11 stt is partitioned into three main effects ssrule ssgcm and ssrcp pertaining to rule curves gcms and rcps respectively there are also three first order interaction terms ssrule gcm ssrule rcp and ssgcm rcp and one second order interaction term ssrule gcm rcp 11 s t t t s s rule t s s gcm t s s rcp t effects f r o m m a i n f a c t o r s s s rule g c m t s s rule r c p t s s gcm r c p t s s rule g c m r c p t i n t e r a c t i o n t e r m s as an example the calculations for the ssgcm ssgcm rcp and total interaction i t are given below 12 s s gcm t n rule n rcp j 1 n gcm y ojo t y ooo t 2 13 s s gcm r c p t n rule j 1 n gcm k 1 n rcp y ojk t y ojo t y ook t y ooo t 2 14 i t s s rule g c m t s s rule r c p t s s gcm r c p t s s rule g c m r c p t where the token denotes averaging over a certain factor or factors for instance yojo is the mean of the related anomalies compiled from all rule curves and rcp variants for the gcm j it should also be noted that the above ss terms are expressed in terms of the variance by dividing them by the total number of combinations nrule ngcm nrcp and that the uncertainty decomposition strategy could be performed for the projected reservoir performance indices regardless of time t 4 results 4 1 evaluation of release predictions as mentioned in the last paragraph of section 3 1 it is targeted to parameterize two operating rules for any gcm and produce rule1 and rule2 based releases and storage predictions the modified dea which governs the optimization step of parsimo was run 10 times given the minimization of the fitness function specified in eq 4 and the global best solutions extracted from those simulations were stored the parameter estimates of the hdg 2d model calibrated under different variations are not shown since they would take up redundant space in the text in the study it was encountered that the majority of gcms had significant drought intensities during their hist scenario periods and the water restriction characterization of the hedging model trained under these conditions and hist based adjusted parameters were found adaptable for their rcp simulations as well so this circumstance was likely to result in a strong linear dependence between rule1 and rule2 outputs as a representative example the results for the hadgem2 es are given in fig 6 besides the fact that the gcms predict decreases in inflows and increases in evaporative losses for the future period has triggered hedging to be applicated in higher zones compared to that of hist throughout their reservoir operation optimization hence the releases produced with rule2 are inclined to be slightly smaller than those of rule1 since the extent to which rule based differences contribute to uncertainty was investigated in section 4 3 we worked with the rule type independent release series under this subsection when sop based reservoir operations under hist were taken as a baseline the anomalies with regard to the averaged releases for each variation were denoted in fig 7 accordingly for the gcms such as cesm1 cam5 hadgem2 es and miroc esm that predict noticeable reductions in the runoff the sop offers significant decreases in releases under both scenarios it is immediately clear from fig 7 that hedging applied not only to these gcms but also to ens4 has mitigated the reductions in releases for example in the rcp4 5 ens4 variant the release of 3 8 more water volume through hdg 2d compared to sop incorporating no hedging has led to closing deficit of 21 during the projection period moreover the sop behaved more sensitively to the rcp8 5 whereas the deficit curbing attainment of hdg 2d was more prominent in pessimistic conditions for rcp8 5 ens4 this approach resulted in the release of 6 2 more supply compared to the sop and prevented a 27 deficit in addition it seems usual that hdg 2d is not efficient under increasing inflow conditions which were provided by several gcms including wet gcms and with sufficient releases even with sop in order to exhibit the inter annual variabilities in the operation optimization during 2021 2050 the quartiles of projected releases from all variants constituting ens4 were denoted in fig 8 this figure also showed the ensemble averages regarding the hist scenario releases that were derived from hdg 2d this may address the query of how the reservoir would have been affected by merely climate change if hedging had been put into action in the past as is seen in fig 8 while the expected values of the releases in the early years of the projection period increased compared to that of hist in the last 20 years they showed a significant decrease notwithstanding the application of hedging this was detected in all quartiles not just in the mean values besides the fluctuations of the median lines marked in the same figure are rather volatile and the non parametric mann whitney u test with a significance level of 0 10 also verifies that the relevant distributions along those continuous sequences do not match that of hist not shown furthermore the fact that the interquartile ranges were conspicuously larger than that of hist in almost all years prompted us to conduct an uncertainty analysis as elaborated later in section 4 3 4 2 reservoir performance indices based assessment the investigation of reservoir performance indices is also required to demonstrate the acknowledged effect of the hedging policy utilized for a rule type independent rendition the reservoir performance indices derived from rule1 and rule2 were averaged for each gcm rcp variation and the alterations achieved compared to the sop based estimation for the related variation were revealed in fig 9 the indices recorded for ens4 by both operating approaches are also shown in table 2 without hedging vulnerability is high even with some gcms that provide benign inflow conditions and it intensifies to approximately 0 81 for ens4 under both scenarios at this point the most fascinating aspect of hdg 2d is that it curbed the excessive vulnerabilities offered by sop under the effects of climate change and enabled the acquisition of vul values below the 0 25 tolerable threshold suggested by adeloye and dau 2019 since the hedging is mainly deployed to limit single period shortages by deliberately allocating the potential deficits to the entire operating period with specific rule parameters these results seem reasonable although the hedging intentionally adds more failures i e situations of insufficient supply to fully meet demand and brings about volumetric reliability to deteriorate the hdg 2d gave a larger rv in many gcm rcp variations compared to the sop yet the differences between them are indistinguishable and this is an indication that the developed strategy would conserve the system s performance in meeting the total demand even with ens4 that predicted a significant reduction in inflows as for rt we focused on examining the frequency of periods operated at dead volume using the form of eq 8 rather than its routine usage that takes account of any positive deficit even a small one according to fig 9 the hdg 2d appeared to set forth greater improvement for the gcms with dry characterization in time based reliability but the improvements of up to 16 can be deemed marginal all these results show that the effect of the hedging on the two different reliability indices is not as great as in vul and thus the improvements made for vul drastically contribute to the sustainability of the system as a result the built model has tempered the huge vulnerability associated with the pessimistic conditions and enhanced modest sustainability indeed for all the gcm rcp variations the si has been raised to above 0 75 for example the si of around 0 5 given by the sop for rcp8 5 ens4 has increased by 70 with the hdg 2d this result is notable given that only 16 of the demanded volume was withheld for this variant it can be deduced from table 2 how the indices for ens4 might change over the 2021 2050 period assuming that hedging is dynamically deployed in all scenarios including hist the results that put emphasis on solely climate change effects in a reservoir system where sustainability is sought suggest that the deterioration in both reliability indices is less than 11 whereas there seemed to be a much more dramatic impact on the vul nonetheless as the vul values are within acceptable limits by virtue of hedging in all scenarios the si has deteriorated only marginally 4 3 evaluation of uncertainties while it has already been explained in the previous subsections that hdg 2d can be responsible for executing the adaptation process in response to climate change a quantitative revelation of which sources dominate the uncertainty containing process has been made in this section the temporal results of the anova exerted on the release anomalies during 2021 2050 are shown in fig 10 from the determinations obtained for both ens12 and ens4 it is clear that both gcm and total interaction uncertainties have an increasing trend over time in ens12 the individual gcm uncertainty over the last 15 years has displayed a relatively large leap compared to other sources whilst this source has accounted for 60 of the total cumulative uncertainty besides the rule based uncertainty tends to augment over time in both cases but it is overwhelmed under the gcm or even the rcp whose contribution to the total ensemble uncertainty is nearly 2 for the ens12 even though the gcm uncertainty has decreased by 38 for the ens4 and the related trend line has become more stationary the uncertainty over the reservoir operation optimization process continues to be dominated by the gcm as typically highlighted in streamflow projection studies e g vetter et al 2017 bosshard et al 2013 kim et al 2019 the uncertainties stemming from the usage of the operation rule and rcp as well as their interactions are somewhat more noticeable for ens4 but their fractional contribution to the total uncertainty is still marginal even if the rcp usage contributes 10 more variance in the ens4 option compared to that of ens12 the total uncertainty derived from all sources within the chain is comparatively low in ens4 i e what is mentioned is roughly a 15 reduction in both mean and interquartile range statistics for total uncertainty additionally in the ens4 from about half of the projection period the frequency of occasions in which the contribution of total interaction uncertainty is greater than the contribution directly provided by gcm has risen this is due to the fact that the mean temperature is predicted to be 0 4 0 5 c higher with ens4 in specific years i e 2039 and 2043 compared to those in ens12 hence the whisker above the 75th percentile has relatively enlarged for the respective gcm rcp interaction not shown furthermore fig 11 depicts the variance decomposition results for the performance indices under both ens12 and ens4 the gcm originated uncertainty for all indices is the most obvious and has also been alleviated by opting for ens4 this inference is inherently consistent with the information from fig 10 the fact that the gcm based uncertainty of the ens12 was more prominent for the vul made the overall uncertainty regarding this index vastly higher this is because the vulnerability derived from wet gcms which provide a non significant rise in mean inflows is essentially minor in comparison to the rest and their presence has led to an increase in the variability amongst gcms as for ens12 furthermore because some gcms such as hadgem2 es and miroc esm have projected inflows with dense critical periods they behaved more sensitively to the operation rule and thus caused the rule based uncertainty contribution on the rv and vul indices in ens4 of which they are a member to increase by 12 compared to those of ens12 since the radiative forcing tendencies of the two rcps during the available projection period are similar meinshausen et al 2011 the influence of scenario uncertainty on rv and vul typically remains marginal just as the differences in the emission conditions of the scenarios after 2050 are reflected in the uncertainties in the temperature forecasts e g jiao and yuan 2019 the rcp based uncertainty in the reservoir performance indices will inevitably become more apparent with the use of longer term projections moreover fig 11 clearly reveals that the contribution of rcp uncertainty to rt under ens4 is comparatively greater than that under ens12 the primary reason for this is that the time based reliabilities from cesm1 cam5 and miroc esm under rcp8 5 are approximately 10 smaller than the values under rcp4 5 this is less apparent in other gcms and is mainly linked to the relatively high frequencies of low inflows values below the first quartile produced by these gcms under rcp8 5 as well as the fact that the amount demanded from the reservoir during the irrigation season constitutes an important part of the annual demand as to the uncertainties in the si the purpose of which is to facilitate the measurement and identification of policies that improve reservoir planning when there are trade offs between criteria it has been determined that the gcm uncertainty and total variance can be lessened by 59 and 44 respectively with the ens4 preference in contrast the contribution of scenario induced uncertainty to the si has slightly risen and scenario gcm interaction uncertainty is now of secondary importance following gcm uncertainty the fact that selecting the appropriate set of gcms gives the opportunity to decrease the overall amount of uncertainty is nonetheless promising 5 discussion and conclusions in the study a hedging policy which provides an arguably simpler way than implicit models was developed to carry out the process of adapting a reservoir to uncertain climate change projections to produce hydro meteorological projections gcm sets mismatched with locally observed variables such as precipitation and temperature were excluded and 12 gcm data sets from the cmip5 archive were dissected then based on the findings in fig 3 a more refined ensemble set of four gcms was adopted recognizing the relevance of local predictors the degradation of the climatic signal and trend structure reported in conventional quantile mapping based bias correction was not detected in our study thanks to the efficient algorithm of qdm although the virtue of qdm is to narrow the gcm scenario uncertainty it is quite unequivocal that different gcms predict highly varied anomalies for example the hydrological modeling procedure whose simulation performance was validated offered inflow anomalies ranging from 33 to 19 despite being run with bias corrected data afterward we optimized the hdg 2d governed by two rules by means of dea and compared its performance with that of an elementary sop with a view to inspecting the adaptive capabilities of the proposed strategy under climate change induced deficits our results suggest that without hedging reservoir performance could aggravate being contingent on pessimistic inflow conditions whereas the hedging model resulted in markedly reduced vulnerabilities keeping them below the 25 threshold recommended by adeloye and dau 2019 and not causing a discernible decline in rv all this indicates that the gordes reservoir if operated with hedging policies has a buffering capacity and can ensure its sustainability reasonably well even in drier climatic conditions on the other hand the likelihood that releases would cut back in the last 20 years of the projected period compared to the past has pointed out that despite a hedging policy the reservoir may exhibit responsiveness to the effects of climate change furthermore the conclusion made by wang et al 2020 that fewer gcms would contribute more to the total uncertainty was not verified at this reservoir scale as the contrary was shown in ens4 of course in such a case the process continues to be heavily affected by gcm uncertainty and its reduction is not related to the number of gcms but to their qualifications it has also been designated that with the inclusion of ens4 the rcp and operating rule uncertainties have increased slightly but their contribution to the overall uncertainty is negligible as it has also been previously demonstrated by the observed data that the rule parameter uncertainty is lower than the inflow prediction uncertainty see liu et al 2019 it might not come as a surprise that the gcm uncertainty outweighs the rule curve uncertainty apart from all these concluding remarks several issues that are open to discussion and that we intend to deal with in the future are listed below although it has been documented that gcm performances vary in terms of spatial resolution and variables analyzed no attempt has been made to weight the relevant gcm outputs in analyses conducted with ens12 and ens4 wang et al 2019 emphasized in their work that when downscaling and bias correction are consecutively implemented the unequal weighting methods have not brought evident influences on the ensembles of hydrological climate impact projections nonetheless to what extent different weighting approaches can affect the uncertainty of both the gcm and the sources interacting with the gcm will be further investigated in this context there are various methods ranging from multi criteria weighting to performance based weighting and it will be beneficial to examine their performance with out of sample testing wang et al 2019 on the other side the employed downscaling strategy is statistically based and regarded as viable only if specific assumptions are met schoof 2013 in the study even if it is considered that the local observations are compatible with potential predictors and that the calibration and testing of the statistical downscaling models were graded as good or very good it should be noted that more credible results might be obtained with the regional climate models which reflect the physical geography characteristics jiao and yuan 2019 another limited aspect of the study is that inflow simulations have been made exclusively through lumped type hydrological models mandal and simonovic 2017 and velazquez et al 2013 evaluated both the lumped and distributed hydrological models and stated that the uncertainty arising from hydrological modeling over long term mean streamflow is relatively inconsiderable whereas the type of model and even its parameterization contributed dramatically to the low flow predictions as opposed to that of the gcm this situation is likely to affect the operational practices and is worth scrutinizing especially given indices such as rt and vul that are sensitive to low inflow sequences in the projected data since the a priori selection of a suitable optimization technique for a given hedging model and the hydro climatological condition is not straightforward our assessment resorts to hedging modeling under gcm rcp variants and decomposing uncertainty in the modeling chain however it should be noted that developing optimization techniques is a worthy endeavor for setting operational rules and that some recent meta heuristics could outperform other optimizers in terms of solution accuracy and convergence rate as abdollahi and ahmadianfar 2021 have mentioned hence questioning the extent to which different meta heuristics add uncertainty to a similar process is also worth examining in a future study flow variability is a crucial determinant of a healthy river environment therefore discharging a certain volume of water might potentially mitigate the negative impact of reservoirs on riverine ecosystems in this context suwal et al 2020 have formulated a multi objective reservoir optimization in order to maximize power generation while minimizing the deficiency of environmental flows e flows utilizing a non dominated sorting genetic algorithm moreover a rare number of studies have examined the impact of flow regime on both e flows and energy outputs in the case of run of river hydropower plants kuriqi et al 2019a 2019b and the trade offs between hydropower production and ecosystem protection under different e flow scenarios bejarano et al 2019 kuriqi et al 2017 one of our upcoming projects will include conducting such research in perspective of the uncertainties introduced by gcms and rcps there are also other studies devoted to decomposing uncertainties for example lee et al 2017 have proposed a method based on the principle of maximum entropy to illustrate how uncertainty is propagated at each stage of the projection the situation in which the sum of the uncertainties at each stage is not equal to the total uncertainty peculiar to their method is not encountered with the anova which is more theoretically sound a comparative examination with emerging methods such as uncertainty decomposition via the bayesian perspective would be of great value credit authorship contribution statement umut okkan funding acquisition conceptualization supervision methodology software data curation formal analysis writing original draft visualization investigation writing review editing okan fistikoglu conceptualization supervision methodology investigation writing review editing zeynep beril ersoy software formal analysis data curation writing original draft writing review editing visualization investigation ahmad tamim noori data curation formal analysis visualization declaration of competing interest the authors declare the following financial interests personal relationships which may be considered as potential competing interests umut okkan reports financial support was provided by scientific and technological research council of turkey acknowledgments this study reported herein was funded by the scientific and technological research council of turkey under grant no 121y037 four reviewers and editors provided insightful feedback that significantly strengthened our paper for which we are extremely thankful appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2023 129286 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2099,more than 80 percent of the bridges in the united states are built over waterways the support systems of the structures crossing waterways are subjected to scour during their service life owing to the flowing water induced bed shear stresses resulting in scour work herein is focused on characterizing the error associated with three abutment scour prediction models included in the hydraulic engineering circular no 18 an abutment scour database is utilized to quantify the predicted versus the measured scour depth relationship abutment scour prediction models are assessed in terms of two statistical parameters termed herein mean absolute percentage error mape as a measure of accuracy of the prediction and level of conservatism defined as percentage of cases for which the predicted scour exceeded the measured scour for scour associated with vertical wall and spill through abutments responses to long abutment and intermediate abutment are examined separately for vertical wall abutments conservatism ranged from 4 76 to 100 and mape ranged from 44 to 201 for spill through abutments conservatism ranged from 0 to 100 and mape ranged from 10 3 to 347 comprehension of the accuracy and conservatism of the deterministic models considered herein contributes to understanding the limitation of the scour depth prediction models keywords abutment scour mape conservatism hec 18 vertical wall abutment spill through abutment data availability the data has been cited in the in text citation sturm 2004 all data models and code generated or used during the study appear in the submitted article 1 introduction records from the national bridge inventory nbi suggest that more than 80 of a total of 583 000 bridges in the united states are built over waterways the support systems of structures crossing the waterways are subjected to scour during their design life owing to the flowing water induced bed shear stresses scour around existing bridge foundation has been a primary cause of bridge collapse in the united states and worldwide melville and coleman 2000 wardhana and hadipriono 2003 liang et al 2015 qi et al 2016 cardoso and bettess 1999 and azamathulla 2012 demonstrated that an appropriate estimation of abutment scour and its disruptive effects on the foundation system is important to design an infrastructure constructed on waterway as overestimating or underestimating of the scour depth will result in higher construction cost and abutment failure respectively two types of local scour processes are predominantly observed viz pier scour and abutment scour abutment scour occurs when the abutment and roadway embankment obstruct the flow the flow obstructed by the abutment and roadway embankment accelerates and forms vortices at the upstream side of the embankment that runs through the toe of the abutment followed by wake vortex formation at the downstream end of the abutment e g ettema et al 2010 arneson et al 2012 kothyari et al 1992 arneson et al 2012 and ettema et al 2017 investigated the vortex formation mechanism and its effect on the scour magnitude the national bridge inspection standards nbis regulation suggests that bridge owners identify bridges that are scour susceptible and further suggests preparing the plan of action to address potential deficiencies despite such stringent regulations in a period of 10 years from 2001 to 2011 the percentage of scour critical bridges has been reduced by merely 0 5 from 5 2 to 4 7 bridge scour evaluation program in 2011 reported that there are 23 034 scour critical bridges in the united states arneson et al 2012 ettema et al 2011 explained that the state of the art of abutment scour research is less advanced than pier scour research pier scour studies have covered deterministic e g melville 1997 arneson et al 2012 probabilistic e g lagasse et al 2013 shahriar et al 2021a shahriar et al 2021b shahriar 2022 and observational e g govindasamy et al 2013 aspects while abutment scour analyses are still deterministic in nature in the bridge design philosophy considering load and resistance factor design lrfd approach shahriar et al 2023 reported that an inconsistency is posed by adopting a reliability index based approach for the design of superstructure components in contrast to the deterministic approach for estimating the scour magnitude at the foundation system in order to mitigate the concerns reliability index based scour assessment approach has been proposed by shahriar et al 2023 which can be used to design pile foundation under bridge piers however the reliability based aspects e g bias uncertainty of abutment scour have not been investigated in the present state of the art furthermore for nearly 15 abutment scour prediction models available in the literature no specific information on their inherent degree of conservatism un conservatism is available in recent times researchers e g guven and gunal 2008 najafzadeh et al 2013 khosravi et al 2021 employed artificial intelligence ai to accurately predict the scour depth although deterministic scour prediction models and ai based models seem easy to apply ettema et al 2011 and arneson et al 2012 suggested that abutment scour prediction models do not take into account the physical processes involved therefore there is a necessity to comprehend the inherent conservatism un conservatism of the deterministic models used to estimate abutment scour depth so that a reliability based framework to assess abutment scour in the context of lrfd can be developed work herein is focused on comparing the predicted versus the measured abutment scour depths using a database collected from literature three abutment scour prediction models suggested by hydraulic engineering circular hec no 18 arneson et al 2012 are used herein namely these are froehlich 1989 model highway in the river environment hire federal highway administration 2001 model and national cooperative highway research program 2010 model the database documented by sturm 2004 is utilized herein these comparative analyses are conducted for vertical wall abutment and spill through abutment using two statistical parameters termed as accuracy and level of conservatism accuracy being the measure of accuracy of the scour prediction models considered while level of conservatism is the measure of inherent conservatism of the scour prediction models considered 2 abutment scour process flow through a bridge waterway is narrowed by the presence of bridge abutment and its embankment the flow width narrows and the flow accelerates through the contraction generating macro turbulence structures e g eddies and vortices the nature of shedding and dispersion of macro turbulence structures within the flow is dependent on the shape of the channel ettema et al 2010 melville et al 2011 fig 1 shows the flow structures generated due to the floodplain main channel flow interaction in a typical compound channel melville et al 2011 reported that in the abutment scouring process two contraction features e g flow contraction and large scale turbulence are difficult to separate the extent of domination of either of the flow features is dependent on the extent of flow contraction type of abutment and its foundation ettema et al 2010 melville et al 2011 for situations when the abutment is well set back on a flood plain deepest scour occurs where the flow contraction is the greatest melville et al 2006a melville et al 2006b melville et al 2011 reported that for spill through abutments with erodible embankment abutment flow field is controlled by the flow contraction the geometry of the scour hole and bridge waterway changes once the scouring begins nevertheless ettema et al 2010 based on flume testing on the spill through and vertical wall abutments concluded that the deepest scour occurs at the intersection of axis of the abutment and the bank of the main channel they further reported that as the flow constriction increases the scour hole becomes more uniform across the main channel bed local scour at bridge abutment can occur under clear water or live bed conditions flow intensity expressed as the ratio of upstream mean flow velocity v to sediment critical velocity v c determines whether grain motion occurs for both uniform and non uniform sediment clear water condition occurs when v v c 1 under clear water conditions there is no supply of sediment to the scour hole from upstream such conditions are encountered on the bed of floodplain of a compound river channel melville 1997 live bed condition occurs when v v c 1 under live bed conditions sediment is transported from the upstream to the scour hole and continues until there is an equilibrium of sediment supply and that transported out of the scour hole if the geometric standard deviation of the bed particle is 1 3 the sediment can be classified as uniform for non uniform sediment armoring occurs both in the channel bed and in the scour hole armoring process reduces the scour depth progression raudkivi and ettema 1977 1983 garde et al 1961 gill 1972 kandasamy 1989 and melville 1997 reported that for clear water conditions as the flow intensity increases the scour depth at bridge abutment increases however melville 1992 presented that beyond a flow intensity of 1 the scour depth does not increase melville 1997 based on the analyses of the data from dongol 1994 described that under live bed conditions scour depth initially decreases with the increasing flow intensity and then increases again to a second maximum the second maximum is also referred to as live bed peak the magnitude being dependent on the size and steepness of the bed features chee 1982 chiew 1984 melville and sutherland 1988 ettema et al 2010 corroborated similar observation for the clear water condition observing that over the range of 0 5 1 0 the scour depth increased and reached maximum for v v c 1 although for live bed condition the scour depth dipped to 70 of the peak scour magnitude with uncertainties associated with the bed form presence was also noticed 3 scour prediction models for estimating the equilibrium scour depth at bridge abutments hec 18 suggested use of three models namely froehlich 1989 model hire federal highway administration 2001 model and national cooperative highway research program 2010 model froehlich 1989 model shown in equation 1 table 1 was developed based on 170 abutment scour measurements in laboratory flumes froehlich 1989 considered test results reported in liu et al 1961 garde et al 1961 and gill 1972 melville et al 2011 noted that froehlich model should be applied to estimate the live bed scour depth at bridge abutments hire federal highway administration 2001 model was developed based on field data of scour at the end of spurs in the mississippi river hec 18 suggested that the hire federal highway administration 2001 model shown in equation 2 table 1 is applicable mostly for abutments where the abutment length l to the flow depth y ratio is greater than 25 the abutment scour assessment procedure developed by ettema et al 2010 as a part of nchrp 24 20 2010 argues the importance of considering how abutments are built all the abutment scour prediction formulae except for ettema et al 2010 consider the abutment as a pier like structure extending as solid forms deeply into the bed ettema et al 2010 suggested that the maximum scour depth that is attainable at an abutment is limited by the geotechnical stability of the embankment at the abutment ettema et al 2010 developed the abutment scour model for a range of abutment types abutment locations and sediment transport conditions one of the basic differences between ettema et al 2010 model and the other models available for abutment scour calculation is that ettema et al 2010 considered the potential maximum scour depth near an abutment can be expressed in terms of an amplified contraction scour ettema et al 2010 also identified three scour conditions e g scour conditions a b and c depending on the developed flow field at an abutment scour condition a is used when the abutment is in or close to the main channel scour condition b is considered when the abutment is set back from the main channel and scour condition c is selected when the approach embankment is breached specific site conditions necessary to identify the scour condition can be found in ettema et al 2010 and arneson et al 2012 the ettema et al 2010 model can be expressed as shown in equation 3 through equation 6 table 1 the model is applicable to site conditions where 0 2 l b f 2 and 0 23 b f 0 5 b m 1 l b f and b m designate length of the embankment width of the floodplain and width of the main channel respectively melville et al 2011 classified the factors affecting abutment scour into five categories table 2 shows the parameter group and associated parameter names as suggested by melville et al 2011 the various deterministic scour prediction models were focused on different datasets thereby the performance of a given model to a common set of data will be different table 3 presents the models suggested by hec 18 and the list of parameters considered in developing those models the hec 18 suggested models are considered herein to estimate the equilibrium scour depth 4 database for analyses melville 1992 suggested that abutment scour estimation process can be classified into three categories based on the abutment length l to flow depth y ratio these are a long abutment l y 25 where scour depth is proportional to flow depth b intermediate abutment 1 l y 25 where scour depth is dependent on both flow depth and embankment length and c short abutment l y 1 where scour depth is proportional to embankment length table 4 shows a summary of the laboratory data sources available in literature it is apparent that researchers investigated the abutment scour development process in sandy bed d 50 ranges from 0 29 mm to 3 30 mm under clear water and live bed upstream flow condition it is important to note that the list of laboratory data sources presented in table 4 is not exhaustive all the data sources presented in table 4 are from testing conducted in a rectangular channel the tests performed by sturm 2004 focused however on compound channel hydraulics a more representative scenario compared to the rectangular channel hydraulic as such test data by sturm 2004 are utilized herein sturm 2004 conducted the experiments in a 4 2 m wide by 24 4 m long flume of a fixed slope fig 2 shows the compound channel configuration used in scour experiments by sturm 2004 scour depths were measured as a function of discharge sediment size and abutment shape and length for two different compound channel configurations constructed table 5 shows the ranges of some key variables e g embankment length discharge median grain size reported scour depth and time to reach equilibrium for the sturm 2004 database the flume tests reported in the database were conducted until the equilibrium scour depth is achieved for vertical wall and spill through abutments the time to reach equilibrium varied from 9 5 h to 65 6 h depending on bed condition and abutment category the sturm 2004 database was classified and analyzed separately for long intermediate and short abutments 5 analyses 5 1 predicted measured scour depth relation 5 1 1 vertical wall abutment fig 3 a b c d e f g shows the relationship between the predicted and the measured scour depth for the vertical wall vw abutments using the three models considered herein fig 3 a c d e suggests that in general for long abutments froehlich 1989 model provides a more conservative estimate of scour depth compared to hire federal highway administration 2001 model although froehlich 1989 model did not underpredict the measured scour depth the hire federal highway administration 2001 model underpredicted the measured scour depth in two occasions the underpredicted two occasions were corresponding to long abutment in clear water condition shown in fig 3d hec 18 suggested that hire federal highway administration 2001 model be only used for estimating scour of long abutments accordingly the scour depth estimations were compared only for long abutments the application of national cooperative highway research program 2010 model is based on the location of embankment in relation to the floodplain ettema et al 2010 suggested that if the projected embankment length l is 75 percent or greater than the floodplain width b f live bed conditions prevail while for the cases when l b f 75 clear water conditions exist data in fig 3 f suggest that for l b f 75 cases national cooperative highway research program 2010 model under predicted the scour depth for 2 out of 10 data points observation of fig 3 g suggest that for the l b f 75 cases national cooperative highway research program 2010 model significantly under predicts the scour depth referring to fig 3 g on an average the predicted scour depth is 0 45 times the measured scour depth for l b f 75 case comparison of fig 3 f and 3 g suggest that most data points 80 are in l b f 75 category while the rest 20 sites have l b f 75 fig 4 shows the key variables that affect the abutment scour magnitude and the discrepancy between the predicted and the measured scour depth for the vertical wall abutment type a positive quantitative discrepancy is indicative of overprediction of measured value while a negative discrepancy means underprediction fig 4 a c presents that the prediction discrepancy for froehlich 1989 model and hire federal highway administration 2001 model is not dependent on f r or l y froehlich 1989 model is an envelope curve developed to envelope 98 of the field data investigated as such it is understandable that froehlich 1989 model overpredicted the measured scour depth for most occasions the national cooperative highway research program 2010 model uses the unit discharge ratio q 2 q 1 to determine an amplification factor necessary to be applied to the estimated contraction scour in order to determine the abutment scour magnitude it appears that for clear water condition all the test cases underpredicted and q 2 q 1 ranged from 1 6 to 2 3 while for live bed condition 3 out of 10 cases underpredicted and for the overpredicted cases q 2 q 1 ranged from 4 29 to 7 64 the unit discharge chart provided by national cooperative highway research program 2010 model had a maximum q 2 q 1 value of 3 thereby a constant value of amplification factor was considered for q 2 q 1 greater than 3 refer to fig 4 c all the clear water data points had f r greater than 0 274 and for all these tests the scour depth was underpredicted fig 4 e further suggests that the live bed tests of sturm 2004 database was conducted at low f r 0 25 any dependence of the prediction discrepancy on l y and l d 50 was not discerned 5 1 2 spill through abutment fig 5 shows the relationship between the predicted and the measured scour depth for the spill through st abutments using the three models considered herein in the case of long abutment in clear water condition froehlich 1989 model over predicted the measured scour depth on the average by a factor of 1 35 in comparison for long abutment in live bed condition fig 5b froehlich 1989 model under predicted the measured scour depth on the average by a factor of 0 89 times on average hire federal highway administration 2001 model under predicted the measured scour depth by a factor of 0 97 and 0 59 for clear water and live bed conditions fig 5 c d respectively fig 5 e shows that for l b f 75 cases national cooperative highway research program 2010 model slightly over predicts the measured scour depth with the predicted scour depth being on the average 1 13 times the measured scour depth observation of fig 5 f suggest that for l b f 75 cases national cooperative highway research program 2010 model significantly under predicts the scour depth referring to fig 5 f on an average the predicted scour depth is 0 53 times the measured scour depth for l b f 75 case fig 6 shows the relationship between key variables affecting the scour prediction models and the quantitative discrepancy between the predicted and the measured scour depth for spill through abutment type similar to fig 4 a positive quantitative discrepancy suggests overprediction of measured value while a negative discrepancy means underprediction it is apparent that in general as the froude number f r increases the prediction discrepancy increases it is the case that the live bed tests were run at low f r 0 19 0 24 compared to the f r for the clear water tests 0 29 0 47 for hire federal highway administration 2001 model data in fig 6 c indicate that the live bed tests were conducted at a low f r 1 19 0 25 which resulted in an underprediction of scour depth for all the test cases the clear water tests were conducted at a higher f r which resulted in a prediction discrepancy ranging from 3 9 cm to 18 4 cm data in fig 6 d suggest that for clear water tests q 2 q 1 was in the range of 1 38 2 29 while for the live bed tests q 2 q 1 was in the range of 4 29 7 64 for the tests conducted at a q 2 q 1 greater than 6 overprediction of the measured scour depth was obtained using the national cooperative highway research program 2010 model it means that for higher constriction the deterministic national cooperative highway research program 2010 model predicts a higher scour depth represented by higher overprediction 0 2 to 22 cm compared to the measured scour depth in relation to f r and l y no specific trend of under or overprediction was noticed from data in fig 6 e f fig 6 g shows that l d 50 ranged from 354 to 718 for clear water cases while for the live bed cases l d 50 ranged from 798 to 1110 a higher l d 50 is representative of smaller particle size around the abutment compared to the length of embankment there are 7 cases with l d 50 more than 1000 and overprediction of scour depth was noticed for all those cases benedict 2014 observed that the magnitude of overprediction from the national cooperative highway research program 2010 model diminishes as the d 50 increases which is in congruent with the relationship observed herein 5 2 accuracy and conservatism analyses a perfectly accurate scour prediction model would be the one that yields a predicted scour depth that is equal to the measured scour depth as the scour processes depend on multiple factors it is unlikely that a deterministic model would exactly predict the measured scour depth for varying hydraulic structural and geotechnical conditions a perfectly conservative scour prediction model would be the one that never yields a predicted scour depth that is less than the measured scour depth tan and duncan 1991 adopted similar logic while assessing the performance of the deterministic models to estimate the settlement of footings on sand while shahriar et al 2022 utilized accuracy and level of conservatism to assess the performance of pier scour prediction models in this study accuracy was measured in terms of mean absolute percentage error mape for each of the data sets the absolute percentage error was calculated and the mean value of the error was reported as the mape associated with a specific deterministic model conservatism was defined as the ratio of the number of cases the calculated scour depth is more than the measured scour depth expressed as percentage of the total number of data points fig 7 shows the relationship between the level of conservatism and mean absolute percentage error for vertical wall vw and spill through st abutment using the three models considered herein for vw abutment conservatism ranges from 4 76 to 100 and mape ranges from 44 to 201 froehlich 1989 model provided a 100 conservative estimate of the scour depth for both clear water and live bed conditions while hire federal highway administration 2001 model presented a 100 conservative estimate of the scour depth for long abutments the least conservative estimate was obtained using national cooperative highway research program 2010 model in clear water conditions the least mape was presented by hire federal highway administration 2001 model under live bed condition 44 for long abutment case the highest mape was demonstrated by froehlich 1989 model under clear water condition 201 for intermediate abutment case referring to fig 7 b for st abutment conservatism ranges from 0 to 100 and mape ranges from 10 3 to 347 froehlich 1989 model provided a 100 conservative estimate of the scour depth for long abutments in clear water condition hire federal highway administration 2001 model under clear water condition provided the least conservative estimate 0 meaning all measured data were under predicted of the scour depths froehlich 1989 model for long abutments in live bed condition provided the least mape 10 3 while froehlich 1989 model for long abutments in clear water condition provided the highest mape 347 table 6 summarizes the results of abutment scour analyses it is apparent that except for the spill through abutment in live bed condition froehlich 1989 model provided the most conservative scour estimate with the highest mape associated national cooperative highway research program 2010 model provided the least conservative scour estimate for vw abutment and st abutment in clear water condition however national cooperative highway research program 2010 model predicted most conservative scour estimate for st abutment in live bed condition 6 conclusion work herein is focused on comparing the predicted and the measured abutment scour depth by comparing computed values from the froehlich 1989 model the highway in the river environment hire federal highway administration 2001 model and the national cooperative highway research program national cooperative highway research program 2010 model with measured scour depths from flume testing reported by sturm 2004 the comparative results were synthesized in terms of accuracy and conservatism accuracy was defined in terms of mean absolute percentage error conservatism was defined as the ratio of the number of cases the calculated scour depth is more than the measured scour depth expressed as percentage of the total number of data points based on the results presented herein the following conclusions are advanced 1 for vertical wall abutments conservatism ranges from 4 76 to 100 for clear water conditions the froehlich 1989 model provides the most conservative 100 scour estimate while the national cooperative highway research program 2010 model provides the least conservative 4 76 scour estimate for live bed conditions the froehlich 1989 model and the hire federal highway administration 2001 model provide the most conservative 100 scour estimate while the national cooperative highway research program 2010 model provides the least conservative 70 scour estimate 2 for vertical wall abutments mape ranges from 44 to 201 for clear water conditions the froehlich 1989 model provides the highest mape 201 while the hire federal highway administration 2001 model provides the least mape 61 for live bed conditions the froehlich 1989 model provides the highest mape 82 while the hire federal highway administration 2001 model provides the least mape 44 3 for spill through abutments conservatism ranges from 0 to 100 for clear water conditions the froehlich 1989 model provides the most conservative 100 scour estimate while the national cooperative highway research program 2010 model provides the least conservative 41 67 scour estimate for live bed conditions the national cooperative highway research program 2010 model provides the most conservative 70 scour estimate while the hire federal highway administration 2001 model provides the least conservative 0 scour estimate 4 for spill through abutments mape ranges from 10 3 to 347 for clear water conditions the froehlich 1989 model provides the highest mape 347 while the national cooperative highway research program 2010 model provides the least mape 72 for live bed conditions the hire federal highway administration 2001 model provides the highest mape 41 while the froehlich 1989 model provides the least mape 10 3 5 for vertical wall abutment under clear water long abutment condition the froehlich 1989 model provides the most conservative scour depth estimate and the hire federal highway administration 2001 model provides the least mape under live bed long abutment condition the froehlich 1989 model provides the most conservative scour depth estimate and the national cooperative highway research program 2010 model provides the least mape 6 for spill through abutment under clear water long abutment condition the froehlich 1989 model provides the most conservative scour depth estimate and the national cooperative highway research program 2010 model provides the least mape under live bed long abutment condition the national cooperative highway research program 2010 model provides the most conservative scour depth estimate and the froehlich 1989 model provides the least mape in general the relationship between the conservatism and accuracy varies depending on the abutment type vertical wall spill through deterministic model being used froehlich hire nchrp and bed condition clear water live bed the developed methodology herein provides the profession with information on the limitations of the models used in predicting abutment scour in terms of level of conservatism and accuracy declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors appreciate the funding from research need proposal 2020 067 of north carolina department of transportation ncdot any conclusions findings opinions and recommendations expressed in this article are those of the authors and do not necessarily reflect the views of ncdot 
2099,more than 80 percent of the bridges in the united states are built over waterways the support systems of the structures crossing waterways are subjected to scour during their service life owing to the flowing water induced bed shear stresses resulting in scour work herein is focused on characterizing the error associated with three abutment scour prediction models included in the hydraulic engineering circular no 18 an abutment scour database is utilized to quantify the predicted versus the measured scour depth relationship abutment scour prediction models are assessed in terms of two statistical parameters termed herein mean absolute percentage error mape as a measure of accuracy of the prediction and level of conservatism defined as percentage of cases for which the predicted scour exceeded the measured scour for scour associated with vertical wall and spill through abutments responses to long abutment and intermediate abutment are examined separately for vertical wall abutments conservatism ranged from 4 76 to 100 and mape ranged from 44 to 201 for spill through abutments conservatism ranged from 0 to 100 and mape ranged from 10 3 to 347 comprehension of the accuracy and conservatism of the deterministic models considered herein contributes to understanding the limitation of the scour depth prediction models keywords abutment scour mape conservatism hec 18 vertical wall abutment spill through abutment data availability the data has been cited in the in text citation sturm 2004 all data models and code generated or used during the study appear in the submitted article 1 introduction records from the national bridge inventory nbi suggest that more than 80 of a total of 583 000 bridges in the united states are built over waterways the support systems of structures crossing the waterways are subjected to scour during their design life owing to the flowing water induced bed shear stresses scour around existing bridge foundation has been a primary cause of bridge collapse in the united states and worldwide melville and coleman 2000 wardhana and hadipriono 2003 liang et al 2015 qi et al 2016 cardoso and bettess 1999 and azamathulla 2012 demonstrated that an appropriate estimation of abutment scour and its disruptive effects on the foundation system is important to design an infrastructure constructed on waterway as overestimating or underestimating of the scour depth will result in higher construction cost and abutment failure respectively two types of local scour processes are predominantly observed viz pier scour and abutment scour abutment scour occurs when the abutment and roadway embankment obstruct the flow the flow obstructed by the abutment and roadway embankment accelerates and forms vortices at the upstream side of the embankment that runs through the toe of the abutment followed by wake vortex formation at the downstream end of the abutment e g ettema et al 2010 arneson et al 2012 kothyari et al 1992 arneson et al 2012 and ettema et al 2017 investigated the vortex formation mechanism and its effect on the scour magnitude the national bridge inspection standards nbis regulation suggests that bridge owners identify bridges that are scour susceptible and further suggests preparing the plan of action to address potential deficiencies despite such stringent regulations in a period of 10 years from 2001 to 2011 the percentage of scour critical bridges has been reduced by merely 0 5 from 5 2 to 4 7 bridge scour evaluation program in 2011 reported that there are 23 034 scour critical bridges in the united states arneson et al 2012 ettema et al 2011 explained that the state of the art of abutment scour research is less advanced than pier scour research pier scour studies have covered deterministic e g melville 1997 arneson et al 2012 probabilistic e g lagasse et al 2013 shahriar et al 2021a shahriar et al 2021b shahriar 2022 and observational e g govindasamy et al 2013 aspects while abutment scour analyses are still deterministic in nature in the bridge design philosophy considering load and resistance factor design lrfd approach shahriar et al 2023 reported that an inconsistency is posed by adopting a reliability index based approach for the design of superstructure components in contrast to the deterministic approach for estimating the scour magnitude at the foundation system in order to mitigate the concerns reliability index based scour assessment approach has been proposed by shahriar et al 2023 which can be used to design pile foundation under bridge piers however the reliability based aspects e g bias uncertainty of abutment scour have not been investigated in the present state of the art furthermore for nearly 15 abutment scour prediction models available in the literature no specific information on their inherent degree of conservatism un conservatism is available in recent times researchers e g guven and gunal 2008 najafzadeh et al 2013 khosravi et al 2021 employed artificial intelligence ai to accurately predict the scour depth although deterministic scour prediction models and ai based models seem easy to apply ettema et al 2011 and arneson et al 2012 suggested that abutment scour prediction models do not take into account the physical processes involved therefore there is a necessity to comprehend the inherent conservatism un conservatism of the deterministic models used to estimate abutment scour depth so that a reliability based framework to assess abutment scour in the context of lrfd can be developed work herein is focused on comparing the predicted versus the measured abutment scour depths using a database collected from literature three abutment scour prediction models suggested by hydraulic engineering circular hec no 18 arneson et al 2012 are used herein namely these are froehlich 1989 model highway in the river environment hire federal highway administration 2001 model and national cooperative highway research program 2010 model the database documented by sturm 2004 is utilized herein these comparative analyses are conducted for vertical wall abutment and spill through abutment using two statistical parameters termed as accuracy and level of conservatism accuracy being the measure of accuracy of the scour prediction models considered while level of conservatism is the measure of inherent conservatism of the scour prediction models considered 2 abutment scour process flow through a bridge waterway is narrowed by the presence of bridge abutment and its embankment the flow width narrows and the flow accelerates through the contraction generating macro turbulence structures e g eddies and vortices the nature of shedding and dispersion of macro turbulence structures within the flow is dependent on the shape of the channel ettema et al 2010 melville et al 2011 fig 1 shows the flow structures generated due to the floodplain main channel flow interaction in a typical compound channel melville et al 2011 reported that in the abutment scouring process two contraction features e g flow contraction and large scale turbulence are difficult to separate the extent of domination of either of the flow features is dependent on the extent of flow contraction type of abutment and its foundation ettema et al 2010 melville et al 2011 for situations when the abutment is well set back on a flood plain deepest scour occurs where the flow contraction is the greatest melville et al 2006a melville et al 2006b melville et al 2011 reported that for spill through abutments with erodible embankment abutment flow field is controlled by the flow contraction the geometry of the scour hole and bridge waterway changes once the scouring begins nevertheless ettema et al 2010 based on flume testing on the spill through and vertical wall abutments concluded that the deepest scour occurs at the intersection of axis of the abutment and the bank of the main channel they further reported that as the flow constriction increases the scour hole becomes more uniform across the main channel bed local scour at bridge abutment can occur under clear water or live bed conditions flow intensity expressed as the ratio of upstream mean flow velocity v to sediment critical velocity v c determines whether grain motion occurs for both uniform and non uniform sediment clear water condition occurs when v v c 1 under clear water conditions there is no supply of sediment to the scour hole from upstream such conditions are encountered on the bed of floodplain of a compound river channel melville 1997 live bed condition occurs when v v c 1 under live bed conditions sediment is transported from the upstream to the scour hole and continues until there is an equilibrium of sediment supply and that transported out of the scour hole if the geometric standard deviation of the bed particle is 1 3 the sediment can be classified as uniform for non uniform sediment armoring occurs both in the channel bed and in the scour hole armoring process reduces the scour depth progression raudkivi and ettema 1977 1983 garde et al 1961 gill 1972 kandasamy 1989 and melville 1997 reported that for clear water conditions as the flow intensity increases the scour depth at bridge abutment increases however melville 1992 presented that beyond a flow intensity of 1 the scour depth does not increase melville 1997 based on the analyses of the data from dongol 1994 described that under live bed conditions scour depth initially decreases with the increasing flow intensity and then increases again to a second maximum the second maximum is also referred to as live bed peak the magnitude being dependent on the size and steepness of the bed features chee 1982 chiew 1984 melville and sutherland 1988 ettema et al 2010 corroborated similar observation for the clear water condition observing that over the range of 0 5 1 0 the scour depth increased and reached maximum for v v c 1 although for live bed condition the scour depth dipped to 70 of the peak scour magnitude with uncertainties associated with the bed form presence was also noticed 3 scour prediction models for estimating the equilibrium scour depth at bridge abutments hec 18 suggested use of three models namely froehlich 1989 model hire federal highway administration 2001 model and national cooperative highway research program 2010 model froehlich 1989 model shown in equation 1 table 1 was developed based on 170 abutment scour measurements in laboratory flumes froehlich 1989 considered test results reported in liu et al 1961 garde et al 1961 and gill 1972 melville et al 2011 noted that froehlich model should be applied to estimate the live bed scour depth at bridge abutments hire federal highway administration 2001 model was developed based on field data of scour at the end of spurs in the mississippi river hec 18 suggested that the hire federal highway administration 2001 model shown in equation 2 table 1 is applicable mostly for abutments where the abutment length l to the flow depth y ratio is greater than 25 the abutment scour assessment procedure developed by ettema et al 2010 as a part of nchrp 24 20 2010 argues the importance of considering how abutments are built all the abutment scour prediction formulae except for ettema et al 2010 consider the abutment as a pier like structure extending as solid forms deeply into the bed ettema et al 2010 suggested that the maximum scour depth that is attainable at an abutment is limited by the geotechnical stability of the embankment at the abutment ettema et al 2010 developed the abutment scour model for a range of abutment types abutment locations and sediment transport conditions one of the basic differences between ettema et al 2010 model and the other models available for abutment scour calculation is that ettema et al 2010 considered the potential maximum scour depth near an abutment can be expressed in terms of an amplified contraction scour ettema et al 2010 also identified three scour conditions e g scour conditions a b and c depending on the developed flow field at an abutment scour condition a is used when the abutment is in or close to the main channel scour condition b is considered when the abutment is set back from the main channel and scour condition c is selected when the approach embankment is breached specific site conditions necessary to identify the scour condition can be found in ettema et al 2010 and arneson et al 2012 the ettema et al 2010 model can be expressed as shown in equation 3 through equation 6 table 1 the model is applicable to site conditions where 0 2 l b f 2 and 0 23 b f 0 5 b m 1 l b f and b m designate length of the embankment width of the floodplain and width of the main channel respectively melville et al 2011 classified the factors affecting abutment scour into five categories table 2 shows the parameter group and associated parameter names as suggested by melville et al 2011 the various deterministic scour prediction models were focused on different datasets thereby the performance of a given model to a common set of data will be different table 3 presents the models suggested by hec 18 and the list of parameters considered in developing those models the hec 18 suggested models are considered herein to estimate the equilibrium scour depth 4 database for analyses melville 1992 suggested that abutment scour estimation process can be classified into three categories based on the abutment length l to flow depth y ratio these are a long abutment l y 25 where scour depth is proportional to flow depth b intermediate abutment 1 l y 25 where scour depth is dependent on both flow depth and embankment length and c short abutment l y 1 where scour depth is proportional to embankment length table 4 shows a summary of the laboratory data sources available in literature it is apparent that researchers investigated the abutment scour development process in sandy bed d 50 ranges from 0 29 mm to 3 30 mm under clear water and live bed upstream flow condition it is important to note that the list of laboratory data sources presented in table 4 is not exhaustive all the data sources presented in table 4 are from testing conducted in a rectangular channel the tests performed by sturm 2004 focused however on compound channel hydraulics a more representative scenario compared to the rectangular channel hydraulic as such test data by sturm 2004 are utilized herein sturm 2004 conducted the experiments in a 4 2 m wide by 24 4 m long flume of a fixed slope fig 2 shows the compound channel configuration used in scour experiments by sturm 2004 scour depths were measured as a function of discharge sediment size and abutment shape and length for two different compound channel configurations constructed table 5 shows the ranges of some key variables e g embankment length discharge median grain size reported scour depth and time to reach equilibrium for the sturm 2004 database the flume tests reported in the database were conducted until the equilibrium scour depth is achieved for vertical wall and spill through abutments the time to reach equilibrium varied from 9 5 h to 65 6 h depending on bed condition and abutment category the sturm 2004 database was classified and analyzed separately for long intermediate and short abutments 5 analyses 5 1 predicted measured scour depth relation 5 1 1 vertical wall abutment fig 3 a b c d e f g shows the relationship between the predicted and the measured scour depth for the vertical wall vw abutments using the three models considered herein fig 3 a c d e suggests that in general for long abutments froehlich 1989 model provides a more conservative estimate of scour depth compared to hire federal highway administration 2001 model although froehlich 1989 model did not underpredict the measured scour depth the hire federal highway administration 2001 model underpredicted the measured scour depth in two occasions the underpredicted two occasions were corresponding to long abutment in clear water condition shown in fig 3d hec 18 suggested that hire federal highway administration 2001 model be only used for estimating scour of long abutments accordingly the scour depth estimations were compared only for long abutments the application of national cooperative highway research program 2010 model is based on the location of embankment in relation to the floodplain ettema et al 2010 suggested that if the projected embankment length l is 75 percent or greater than the floodplain width b f live bed conditions prevail while for the cases when l b f 75 clear water conditions exist data in fig 3 f suggest that for l b f 75 cases national cooperative highway research program 2010 model under predicted the scour depth for 2 out of 10 data points observation of fig 3 g suggest that for the l b f 75 cases national cooperative highway research program 2010 model significantly under predicts the scour depth referring to fig 3 g on an average the predicted scour depth is 0 45 times the measured scour depth for l b f 75 case comparison of fig 3 f and 3 g suggest that most data points 80 are in l b f 75 category while the rest 20 sites have l b f 75 fig 4 shows the key variables that affect the abutment scour magnitude and the discrepancy between the predicted and the measured scour depth for the vertical wall abutment type a positive quantitative discrepancy is indicative of overprediction of measured value while a negative discrepancy means underprediction fig 4 a c presents that the prediction discrepancy for froehlich 1989 model and hire federal highway administration 2001 model is not dependent on f r or l y froehlich 1989 model is an envelope curve developed to envelope 98 of the field data investigated as such it is understandable that froehlich 1989 model overpredicted the measured scour depth for most occasions the national cooperative highway research program 2010 model uses the unit discharge ratio q 2 q 1 to determine an amplification factor necessary to be applied to the estimated contraction scour in order to determine the abutment scour magnitude it appears that for clear water condition all the test cases underpredicted and q 2 q 1 ranged from 1 6 to 2 3 while for live bed condition 3 out of 10 cases underpredicted and for the overpredicted cases q 2 q 1 ranged from 4 29 to 7 64 the unit discharge chart provided by national cooperative highway research program 2010 model had a maximum q 2 q 1 value of 3 thereby a constant value of amplification factor was considered for q 2 q 1 greater than 3 refer to fig 4 c all the clear water data points had f r greater than 0 274 and for all these tests the scour depth was underpredicted fig 4 e further suggests that the live bed tests of sturm 2004 database was conducted at low f r 0 25 any dependence of the prediction discrepancy on l y and l d 50 was not discerned 5 1 2 spill through abutment fig 5 shows the relationship between the predicted and the measured scour depth for the spill through st abutments using the three models considered herein in the case of long abutment in clear water condition froehlich 1989 model over predicted the measured scour depth on the average by a factor of 1 35 in comparison for long abutment in live bed condition fig 5b froehlich 1989 model under predicted the measured scour depth on the average by a factor of 0 89 times on average hire federal highway administration 2001 model under predicted the measured scour depth by a factor of 0 97 and 0 59 for clear water and live bed conditions fig 5 c d respectively fig 5 e shows that for l b f 75 cases national cooperative highway research program 2010 model slightly over predicts the measured scour depth with the predicted scour depth being on the average 1 13 times the measured scour depth observation of fig 5 f suggest that for l b f 75 cases national cooperative highway research program 2010 model significantly under predicts the scour depth referring to fig 5 f on an average the predicted scour depth is 0 53 times the measured scour depth for l b f 75 case fig 6 shows the relationship between key variables affecting the scour prediction models and the quantitative discrepancy between the predicted and the measured scour depth for spill through abutment type similar to fig 4 a positive quantitative discrepancy suggests overprediction of measured value while a negative discrepancy means underprediction it is apparent that in general as the froude number f r increases the prediction discrepancy increases it is the case that the live bed tests were run at low f r 0 19 0 24 compared to the f r for the clear water tests 0 29 0 47 for hire federal highway administration 2001 model data in fig 6 c indicate that the live bed tests were conducted at a low f r 1 19 0 25 which resulted in an underprediction of scour depth for all the test cases the clear water tests were conducted at a higher f r which resulted in a prediction discrepancy ranging from 3 9 cm to 18 4 cm data in fig 6 d suggest that for clear water tests q 2 q 1 was in the range of 1 38 2 29 while for the live bed tests q 2 q 1 was in the range of 4 29 7 64 for the tests conducted at a q 2 q 1 greater than 6 overprediction of the measured scour depth was obtained using the national cooperative highway research program 2010 model it means that for higher constriction the deterministic national cooperative highway research program 2010 model predicts a higher scour depth represented by higher overprediction 0 2 to 22 cm compared to the measured scour depth in relation to f r and l y no specific trend of under or overprediction was noticed from data in fig 6 e f fig 6 g shows that l d 50 ranged from 354 to 718 for clear water cases while for the live bed cases l d 50 ranged from 798 to 1110 a higher l d 50 is representative of smaller particle size around the abutment compared to the length of embankment there are 7 cases with l d 50 more than 1000 and overprediction of scour depth was noticed for all those cases benedict 2014 observed that the magnitude of overprediction from the national cooperative highway research program 2010 model diminishes as the d 50 increases which is in congruent with the relationship observed herein 5 2 accuracy and conservatism analyses a perfectly accurate scour prediction model would be the one that yields a predicted scour depth that is equal to the measured scour depth as the scour processes depend on multiple factors it is unlikely that a deterministic model would exactly predict the measured scour depth for varying hydraulic structural and geotechnical conditions a perfectly conservative scour prediction model would be the one that never yields a predicted scour depth that is less than the measured scour depth tan and duncan 1991 adopted similar logic while assessing the performance of the deterministic models to estimate the settlement of footings on sand while shahriar et al 2022 utilized accuracy and level of conservatism to assess the performance of pier scour prediction models in this study accuracy was measured in terms of mean absolute percentage error mape for each of the data sets the absolute percentage error was calculated and the mean value of the error was reported as the mape associated with a specific deterministic model conservatism was defined as the ratio of the number of cases the calculated scour depth is more than the measured scour depth expressed as percentage of the total number of data points fig 7 shows the relationship between the level of conservatism and mean absolute percentage error for vertical wall vw and spill through st abutment using the three models considered herein for vw abutment conservatism ranges from 4 76 to 100 and mape ranges from 44 to 201 froehlich 1989 model provided a 100 conservative estimate of the scour depth for both clear water and live bed conditions while hire federal highway administration 2001 model presented a 100 conservative estimate of the scour depth for long abutments the least conservative estimate was obtained using national cooperative highway research program 2010 model in clear water conditions the least mape was presented by hire federal highway administration 2001 model under live bed condition 44 for long abutment case the highest mape was demonstrated by froehlich 1989 model under clear water condition 201 for intermediate abutment case referring to fig 7 b for st abutment conservatism ranges from 0 to 100 and mape ranges from 10 3 to 347 froehlich 1989 model provided a 100 conservative estimate of the scour depth for long abutments in clear water condition hire federal highway administration 2001 model under clear water condition provided the least conservative estimate 0 meaning all measured data were under predicted of the scour depths froehlich 1989 model for long abutments in live bed condition provided the least mape 10 3 while froehlich 1989 model for long abutments in clear water condition provided the highest mape 347 table 6 summarizes the results of abutment scour analyses it is apparent that except for the spill through abutment in live bed condition froehlich 1989 model provided the most conservative scour estimate with the highest mape associated national cooperative highway research program 2010 model provided the least conservative scour estimate for vw abutment and st abutment in clear water condition however national cooperative highway research program 2010 model predicted most conservative scour estimate for st abutment in live bed condition 6 conclusion work herein is focused on comparing the predicted and the measured abutment scour depth by comparing computed values from the froehlich 1989 model the highway in the river environment hire federal highway administration 2001 model and the national cooperative highway research program national cooperative highway research program 2010 model with measured scour depths from flume testing reported by sturm 2004 the comparative results were synthesized in terms of accuracy and conservatism accuracy was defined in terms of mean absolute percentage error conservatism was defined as the ratio of the number of cases the calculated scour depth is more than the measured scour depth expressed as percentage of the total number of data points based on the results presented herein the following conclusions are advanced 1 for vertical wall abutments conservatism ranges from 4 76 to 100 for clear water conditions the froehlich 1989 model provides the most conservative 100 scour estimate while the national cooperative highway research program 2010 model provides the least conservative 4 76 scour estimate for live bed conditions the froehlich 1989 model and the hire federal highway administration 2001 model provide the most conservative 100 scour estimate while the national cooperative highway research program 2010 model provides the least conservative 70 scour estimate 2 for vertical wall abutments mape ranges from 44 to 201 for clear water conditions the froehlich 1989 model provides the highest mape 201 while the hire federal highway administration 2001 model provides the least mape 61 for live bed conditions the froehlich 1989 model provides the highest mape 82 while the hire federal highway administration 2001 model provides the least mape 44 3 for spill through abutments conservatism ranges from 0 to 100 for clear water conditions the froehlich 1989 model provides the most conservative 100 scour estimate while the national cooperative highway research program 2010 model provides the least conservative 41 67 scour estimate for live bed conditions the national cooperative highway research program 2010 model provides the most conservative 70 scour estimate while the hire federal highway administration 2001 model provides the least conservative 0 scour estimate 4 for spill through abutments mape ranges from 10 3 to 347 for clear water conditions the froehlich 1989 model provides the highest mape 347 while the national cooperative highway research program 2010 model provides the least mape 72 for live bed conditions the hire federal highway administration 2001 model provides the highest mape 41 while the froehlich 1989 model provides the least mape 10 3 5 for vertical wall abutment under clear water long abutment condition the froehlich 1989 model provides the most conservative scour depth estimate and the hire federal highway administration 2001 model provides the least mape under live bed long abutment condition the froehlich 1989 model provides the most conservative scour depth estimate and the national cooperative highway research program 2010 model provides the least mape 6 for spill through abutment under clear water long abutment condition the froehlich 1989 model provides the most conservative scour depth estimate and the national cooperative highway research program 2010 model provides the least mape under live bed long abutment condition the national cooperative highway research program 2010 model provides the most conservative scour depth estimate and the froehlich 1989 model provides the least mape in general the relationship between the conservatism and accuracy varies depending on the abutment type vertical wall spill through deterministic model being used froehlich hire nchrp and bed condition clear water live bed the developed methodology herein provides the profession with information on the limitations of the models used in predicting abutment scour in terms of level of conservatism and accuracy declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors appreciate the funding from research need proposal 2020 067 of north carolina department of transportation ncdot any conclusions findings opinions and recommendations expressed in this article are those of the authors and do not necessarily reflect the views of ncdot 
