index,text
6260,situational awareness in sanitary sewer systems requires accurate flow information at different spatial locations in a city it is especially desirable to predict flows across a wastewater network in response to heavy rainfall events in addition to regular consumption patterns typically complicated hydraulic models that suffer from difficulties in parameter identification and high computational burden are used for flow prediction recently data driven approaches have been employed for flow prediction in this paper we first design and then compare the performance of three data driven methods to predict flow 1 artificial neural network ann 2 long short term memory lstm 3 least absolute shrinkage and selection operator lasso we test the performance of these machine and statistical learning techniques using data gathered from the city of springfield while all three data driven methodologies provide acceptable prediction performance we observe that lstm outperforms ann due to the inherent memory integrated with a feedback structure the statistical learning approach i e lasso regression not only offers good prediction performance but also helps identify the key spatial and temporal features that influence flow at any specific location this added information could aid in remediation activities another key contribution of this paper is that we quantify the value of groundwater data in flow prediction specifically including groundwater data as an additional input enhances flow prediction performance in all three methods lastly to better predict flows corresponding to rare events e g 50 or 100 year rainfall events we use a resampling approach known as smoter to modify the training dataset simulation results indicate that the resampling technique is effective in improving prediction performance keywords sewer system flow prediction neural network lstm lasso resampling 1 introduction cso combined sewer overflows ann artificial neural network rnn recurrent neural network lstm long short term memory lasso least absolute shrinkage and selection operator smote synthetic minority over sampling technique Ïƒ c activation function hadamard product in any city sewer systems are considered essential as they play an important role in sanitation and disease prevention flow control in sewer systems is critical as improper management can lead to overflows and flooding in some parts and high influent flows at wastewater treatment plants wwtp however understanding of flows in the sewer system in real time is not trivial due to varying capacities of sewer pipes differential water consumption patterns and weather events we cannot expect similar flows at different locations within a city for example during heavy rainfall events overflows may occur at some parts of the city whereas other parts may not encounter any flooding therefore cities have significant interest in implementing flow control strategies to prevent catastrophic events darsono and labadie 2007 presents an optimal control algorithm to decrease the hydraulic load of the wwtp in combined sewer systems this control strategy minimizes the pollution impacts of untreated combined sewer overflows cso and is cost effective in terms of practical implementation darsono and labadie 2007 while this approach could efficiently reduce infrastructure investments it requires a correct control design and adequate modeling capabilities garofalo et al 2017 obviously an accurate control depends on precise information regarding flows in the sewer system not only for the current time instant but also for future times instances chen et al 2014 liu et al 2016 this fact highlights the importance of flow prediction and situational awareness while designing flow control strategies in prior efforts many flow prediction approaches have been explored flow prediction using a hydraulic model of the sewer system is an efficient approach when rainfall patterns and structure of the sewer system is adequately identified autixier et al 2014 lucas and sample 2015 in spite of their superior performance hydraulic models are extremely sensitive to the accuracy of the sewer system information additionally hydraulic model based prediction techniques suffer from high computational complexity and typically a large number of model parameters need to be identified for the prediction to be effective furthermore while majority of the hydraulic models are presented in a deterministic framework the dynamics of many of the underlying parameters are stochastic in nature this makes the system identification process quite challenging therefore although hydraulic models capture the hydraulic aspects of a sewer system accurately the disadvantages discussed above act as deterrents to its use in practical flow prediction el din and smith 2002 as an alternative to the hydraulic models black box models may be able to capture the relationships between measurable inputs and outputs of dynamical systems even without detailed internal description of the sewer system one of the first efforts to use black box models for flow prediction in wwtp was presented in carstensen et al 1998 which compares three prediction models with different complexities among a simple regression model a gray box model and a complex hydraulic model carstensen et al 1998 shows that the first two models outperform the complex hydraulic model for flow prediction another black box model that could be a qualified candidate is artificial neural network ann which is implemented for wastewater flow prediction in el din and smith 2002 based on data from edmonton canada the architecture of the ann and the impact of the various parameters involved in its design are discussed in el din and smith 2002 in wei et al 2012 the authors study a multilayer perceptron neural network algorithm to predict flow at the treatment plant at des moines wastewater reclamation facility wrf in iowa usa in addition to the current rainfall data they also use radar reflectivity data to achieve an improved flow prediction for different short time horizons in a recent work flow is predicted using different recurrent neural networks for drammen city norway zhang et al 2018b for control purposes jean et al 2018 evaluates the effects of three rainfall data selection methods on the estimation of cso volume thresholds different neural networks are applied for predicting the water level of a cso structure in zhang et al 2018a using data collected from an internet of things monitoring cso structure in chen et al 2014 real time future flow prediction is studied based on autoregressive moving average arma models and multistep iterative prediction then online control of chemical dosing is implemented using the predicted flow another control application is presented in liu et al 2016 where an event driven model predictive control empc methodology is proposed to control the flows of sewage streams containing the dosed chemical in li et al 2019 autoregressive with exogenous inputs arx models are employed to reduce the delays with rainfall data used as model inputs to control chemical dosing in sewer systems using a hydraulic model and lstm zhang et al 2017 presents a new inter catchment wastewater transfer method which can mitigate sewer overflows 1 1 contribution in this paper for the first time we present a comprehensive comparison of the performances of various flow prediction methods that are based on black box models while we consider previously proposed black box models such as ann and long short term memory lstm network we also custom design a statistical learning based prediction algorithm built on the least absolute shrinkage and selection operator lasso specifically we use these three strategies to predict flows at the wastewater treatment plant located in the city of springfield missouri usa we show that while the ann provides acceptable flow predictions lstm outperforms ann in terms of accuracy of prediction the lasso approach in addition to offering a prediction performance that is comparable to ann and lstm is able to determine the most effective spatial and temporal features that influence flow to the best knowledge of the authors groundwater level has not been considered as a feature input for the machine learning methods used for flow prediction in this paper we quantify the value of using groundwater levels as inputs by highlighting the resulting improvement in flow prediction in all three methods one of the important goals emphasized in many prior efforts is the effective prediction of flow during storm events el din and smith 2002 zhang et al 2018b however training datasets rarely contains such storm cases leading to poor prediction accuracy for storm events to alleviate the issue of scarce training data related to rare events we employ a resampling approach known as smoter in order to balance the dataset used for training smoter consists of an oversampling and an undersampling processes wherein the oversampling process generates new data similar to the rare cases and the undersampling process randomly removes some of the normal samples from the training dataset simulation results illustrate the efficiency of the applied resampling approach while highlighting the effects of the degree of oversampling and undersampling on the prediction performance 2 data description the area of interest in our work is the city of springfield missouri usa coordinates 37 12 55 n 93 17 54 w this is the third largest city in the state of missouri with 167 376 residents and an area of 213 18 km 2 a map for the sanitary sewer network of springfield can be viewed in esri 2019 fig 1 a provides an overview of the entire drainage area and the tributary wastewater paths to the treatment plants as it is discerned by the two colors in fig 1a the overall sewer system is divided into two separate subsystems each with a different treatment plant i e the north and south wwtp fig 1b presents a magnified view of the north subsystem which contains the locations of rain gauges groundwater wells long term and short term flow meters north wastewater treatment plant and sewer paths to the plant for flow prediction we consider the following data sources flow meter data after 2011 flow meter data is available every 15 min for all the permanent meter locations indicated by the red bullets in fig 1b in addition to the permanent sites some temporary flow meters are installed to provide additional information indicated by yellow and orange bullets in fig 1b the temporary flow meters are not operational at all times and measure flows for specific time periods similar to the permanent flow meters the time interval between two subsequent measurements of the temporary flow meters is 15 min rainfall data indicated via a drop symbol in fig 1b rainfall gauges have been installed in locations throughout the city to record rainfall amounts starting in 2012 similar to the flow data rainfall amounts are recorded reported every 15 min obviously these rain gauges are not necessarily co located with the flow meters however we can approximate the amount of rainfall at every flow meter location using weighted combination of the nearest rain gauge measurements groundwater data levels of water in the groundwater wells are provided every 5 min from april 2016 to october 2017 in fig 1b green bullets indicate locations of the groundwater wells time information for all the above variables flow rain groundwater data collected includes time information therefore we use the time index as an input feature for flow prediction this is reasonable as water consumption behavior of springfield residents should vary based on the time of day 3 prediction techniques in any sewer system the exact relationship between inputs e g rainfall time etc and output e g flow is not easily captured relying on precise but complex hydraulic models present challenges related to parameter identification this is because the available data is typically noisy and the error associated with the data significantly degrades the performance of any system identification approach therefore we confine ourselves to black box modeling techniques which can capture the complexity of the system even with noisy data the rest of this section provides general descriptions of three prediction techniques and one data resampling approach that we apply in this paper additionally the strengths and weaknesses of the considered methods are also highlighted 3 1 artificial neural network artificial neural network ann is a computational model which is inspired by the structure and function of biological neural networks consider that we have an input output data set then aided by some training techniques ann can play the role of a black box that relates the inputs to the outputs in other words anns are nonlinear data modeling tools that can find complex relationships and patterns between inputs and outputs as shown in fig 2 anns consist of three main layers that are interconnected the first layer contains input neurons that are transferred to the next layer by some weights and a bias the second layer known as a hidden layer passes the weighted data through some activation functions then the calculated values from hidden layer are conveyed to the output by new weights and a bias it should be noted that the hidden layer can be divided into more connected layers after deciding the structure of the ann e g number of hidden layers and number of hidden neurons at each layer we need to estimate the values of the weights and biases using training techniques the training process is accomplished via different algorithms such as gradient descent methods gene expression programming evolutionary methods particle swarm optimization etc in addition to the exact training procedure used the number of epochs for training is an important factor chosen by the algorithm designer if the training iterations are not adequate the neural network may miss some characteristics of the system on the other hand if too many training epochs are used over training could lead to a poor performance when dealing with test datasets that have characteristics that are changed from the training set in our problem empirical observations suggest a strong dependency of flow on variables such as groundwater rainfall and residential water consumption however such a relationship between flow and the mentioned variables is quite complex therefore an ann can serve as a good candidate to capture the complex relationship between flow and variables such as rainfall amounts groundwater levels etc 3 2 lstm various forms of rnns have been effective in modeling dynamic behavior and time series data zhang et al 2018b hu et al 2018 equipped with feedback connections an rnn is designed to memorize recent information and apply it as inputs to the activation functions used in a neural network typically rnns consist of consecutive units with each unit corresponding to a certain time t there are various architectures of rnns that result from different connections between the units however if the units exhibit long term correlation with the past standard rnns perform poorly lecun et al 2015 to address this problem hochreiter and schmidhuber introduced lstm neural networks hochreiter and schmidhuber 1997 which inherently store long term system states lstm networks are well suited to classifying processing and making predictions based on time series data since there can be lags of unknown duration between important events in a time series a typical lstm unit consists of a memory cell forget gate input gate and output gate the memory cell keeps track of the relationship between the elements in the input data the forget gate determines whether a piece of information should be retained or forgotten in the cell the input gate specifies the extent of new information flowing into the cell the output gate as the name suggests controls the information output from the cell which in turn determines the output of the lstm unit the proposed rnn based spe forecasting will rely on multiple lstm units connecting to one another in series fig 3 shows a unit of lstm for time t where x t r d is the input vector to the lstm unit h t r h is the output vector of the lstm unit and c t r h is the cell state vector also f t r h i t r h and o t r h denote forget gate s activation vector input gate s activation vector and output gate s activation vector respectively eqs 1 5 define the relationships between different variables of an lstm unit at time t hochreiter and schmidhuber 1997 1 f t Ïƒ g w f x t u f h t 1 b f 2 i t Ïƒ g w i x t u i h t 1 b i 3 o t Ïƒ g w o x t u o h t 1 b o 4 c t f t c t 1 i t Ïƒ c w c x t u c h t 1 b c 5 h t o t Ïƒ h c t here Ïƒ g Ïƒ c and Ïƒ h denote activation functions in this work we choose Ïƒ g to be a sigmoid function while Ïƒ c and Ïƒ h are hyperbolic tangent functions denotes hadamard product element wise product w u and b are weight matrices and bias vectors that should be identified during the training process an iterative gradient descent e g backpropagation through time has been shown to be an effective approach to train the lstm units recently lstm has been shown to be effective in prediction of wastewater flow zhang et al 2018b and rainfall runoff hu et al 2018 in sewer systems flow at current time is not only a function of current time features but also a function of the previous rainfall and flow values such a phenomenon implies that our chosen prediction technique must use prior information for current time prediction since the lstm stores previous states of the underlying system it is a viable candidate for wastewater flow prediction 3 3 lasso regression as a regression analysis method least absolute shrinkage and selection operator lasso aims to minimize sum of the squared errors while including an l 1 norm penalty for the number of predictors used tibshirani 1996 consider the regression model 6 y i Î² 0 u 1 i Î² 1 u p i Î² p i i 1 2 n where y i is the i th sample of target output value and i is the error term u i u 1 i u p i t is i th predictor feature input vector and Î² Î² 1 Î² p t is the parameter vector the objective of lasso is to 7 min Î² 0 Î² 1 n i 1 n y i Î² 0 u i t Î² Î» Î² 1 where Î» is a tuning parameter that controls regularization term that determines the level of sparsity that one would like to impose on Î² vector so if a high value is chosen for this parameter most of the elements in Î² are set to zero or close to zero in other words lasso determines the most significant variables in the regression model 6 by assigning small values to the less important factors then if we just retain the important predictors and throw away the less significant predictors we can still achieve an acceptable performance of prediction consequently one can predict flows using the lasso regression model with fewer input features than those used in ann or lstm however it should be noted that if we excessively increase Î» and emphasize sparsity some of the important predictors may be neglected the computation of the lasso solutions is a quadratic programming problem and can be tackled by standard numerical analysis algorithms however the least angle regression procedure outperforms the existing approach this algorithm exploits the special structure of the lasso problem and provides an efficient way to compute the solutions in this paper the lasso strategy enables us to determine the most influential factors in flow prediction understanding the most important predictors could be very useful for sewer system designers and managers more precisely a designer can manipulate the sewer system structure to refine flow at a certain location since the designer knows the most influential factors a priori furthermore removing unnecessary predictors can help to achieve a better performance as we avoid noise accumulation 3 4 resampling all the mentioned techniques in this section are black box data based approaches where the role of data in achieving well trained models is critical in many applications we are typically only interested in the prediction classification of certain rare events however these rare events may be sparsely represented in the training data set this in turn leads to the designed models missing lacking important characteristics of the underlying systems one approach to tackle the problem of imbalanced data is the undersampling and synthetic minority over sampling technique smote introduced in chawla et al 2002 for classification applications smote combines undersampling of the major classes with oversampling of the minor class in torgo et al 2013 authors generalize smote for regression tasks in order to accurately predict the rare cases algorithm 1 and algorithm 2 summarize smote for regression models also known as smoter in this algorithm x and y are samples of the inputs and outputs respectively d denotes the entire data set for training Ï• y is a score function that maps the outputs to a value between 0 and 1 choice of this function depends on how we define rare cases but in general the extreme cases must receive a high relevance score a threshold value t h should also be determined to decide whether the selected pair x y belongs to a rare cases or not y is the median of target values o and u are the percentages of oversampling and undersampling respectively k is the number of nearest neighbors in the k nearest neighbors knn step of algorithm 2 extensive discussions on choosing the parameters and relative function Ï• can be found in torgo et al 2013 and moniz et al 2016 after setting the parameters the data is divided between rare and normal cases in step 1 also the rare cases are divided based on the data which are higher or lower than the median step 2 corresponds to the oversampling process which is performed by the algorithm 2 in step 3 we randomly remove u percent of the normal cases in step 4 we combine all the undersampled and oversampled data as a new dataset algorithm 2 oversamples the rare cases that are determined by step 1 in algorithm 2 in algorithm 2 for each sample of dataset d k nearest neighbors are selected from the same dataset then if the attribute of the selected sample is numerical a new attribute is generated that is an interpolation of the values of the two original cases if the attribute is not numerical we randomly select the attribute of the original sample or attribute of the selected neighbor z then a new target is generated by a weighted average of the original sample and the selected neighbor the new target and the new attributes are combined and are considered as a new data sample depending on the rate of oversampling o this process is repeated algorithm 1 the main smoter algorithm torgo et al 2013 set the parameters o u y t h and k determine the function Ï• t h the threshold for relevance of the target variable values o u percentages of oversampling and undersampling k the number of neighbors used in case generation step 1 divide the data according to following rarel x y d Ï• y t h y y y is the median of target values rareh x y d Ï• y t h y y norm d rarel rareh step 2 generate new synthetic cases for rare cases using algorithm 2 and save it into the following sets newcasesl gensynth rarel o k newcasesh gensynth rareh o k step 3 undersample the normal cases by newnorm remove uofnorm step 4 making the new dataset by newdataset newnorm newcasesl newcasesh algorithm 2 generate synthetic cases torgo et al 2013 set the dataset to d function gensynth d o k set ng o 100 for case d do nns knn k case d case k nearest neighbours of case for i 1 to ng do choose one of the nns as z for all a attributes do if a is a nummeric value then diff case a z a new a case a random 0 1 diff else new a randomly select among case a and z a endif endfor set d 1 to the distance between new and case set d 2 to the distance between new and z new target d 2 case target d 1 z target d 1 d 2 newcases newcases new end for end for return newcases in the city of springfield sewer system flow values are typically low across the network due to the prevailing drought conditions however city planners are interested in understanding the impact of rare and intense rainfall events on the flows in the network in addition to an imbalanced dataset the available dataset is not large therefore lack of data and imbalance in the dataset motivate the use of resampling techniques to the best knowledge of authors resampling techniques have not been employed in area of flow prediction 4 results and discussion in this section we present the flow prediction performances of the three data driven approaches to quantify performance of the prediction methods we use the coefficient of determination r 2 which is a statistical indicator that compares the accuracy of the model to the accuracy of a trivial benchmark model the coefficient of determination corresponds to 8 r 2 1 i 1 n y i y i 2 i 1 n y i y i 2 where y i is the actual output value y i is the predicted value y i is the mean of y i values and n is the total number of data samples r 2 can take a value between and 1 where an r 2 value of 1 indicates a perfect prediction while r 2 implies a very poor fit in general we can not compare the r 2 values reported in other efforts with the obtained r 2 values for our system as the data sets and the underlying factors can be quite different however as a rule of thumb we can clarify a prediction method as successful if it returns a r 2 value above 0 5 zhang et al 2018b 4 1 data preparation black and veatch corporation has installed several meters throughout the city the installed hardware are ads triton meters these are categorized in contact meters which means there are area velocity probes installed in the pipe the topside control units have cellular sim cards that transmit the data collected to flowview which is an ads s web based site in order to store the data this data is in 15 min intervals then different scripts are run to process the raw data data scrubbing process this includes removing erroneous spikes due to debris build up or drops due to dead batteries therefore the available data is not always conductive to analysis for example data gathered from flow meters can sometimes be missing additionally spurious measurements noise or other factors can lead to outliers in the data set such erroneous data must be cleaned prior to design and training of the prediction techniques we replace the outliers and fill in the missed data with similar data from the previous hours 4 2 input output configuration the inputs or predictor variables in all the data driven methods include the following hour index we use 24 inputs corresponding to 24 h of a day at each time instant only one of the 24 inputs that corresponds to the current hour is set to 1 and rest of the inputs are set to 0 for example if time is 4 p m the 16 th input is 1 and the other 23 inputs are 0 rainfall amounts in order to estimate current flow we must include the effects of previous rainfall amounts one approach is to consider the average of rainfall values for a specific horizon of time as a predictor variable however our simulations show that a moving window including actual rainfall amounts as inputs is more effective than using the averaged rainfall as a predictor the size of this moving window is also a tuning parameter that should be selected carefully groundwater levels we have access to data from two groundwater wells located in the north plant area and one groundwater well for the south area since our goal is to predict flow at various locations of the sewer system flow plays the role of the output in all three methods any dataset must contain all the mentioned inputs and output i e dataset must have rainfall amounts groundwater levels and flow meters to this end we consider the dataset starting from 4 1 2016 to 9 1 2017 first half of this dataset is dedicated to train the models while the second half plays the role of testing data therefore in the following subsections we provide the test results based on the last 5832 data samples we have developed our own code to implement the overall data analysis task this code is written in matlab and leverages the neural network toolbox the lasso method also within matlab solves the l 1 minimization problems in 7 using the cvx package grant and boyd 2014 the code was run on intel core i7 pc with clock speed 3 4 ghz and 32 0 gb ram remark when the new data becomes available the parameters of all the methods can be updated to enhance the accuracy of future predictions 4 3 ann as we mentioned earlier parameters of the ann should be tuned to achieve an acceptable performance in this study we apply an ann with only one hidden layer numbers of hidden neurons however can be varied in our experiments we observed that with 8 to 25 hidden neurons the prediction performance was quite similar the activation function is selected to be a sigmoid function fig 4 shows the predicted flow versus the actual flow for location pr18 in the north part of the city it is clear that the estimated value of flow is approximately close to the actual value obviously after a rainfall event depicted by the red curve the flow increases and the estimated flow responds to the rainfall amount for this prediction the coefficient of determination r 2 is 0 7822 which falls in the acceptable interval of prediction accuracy table 1 presents the performances of ann for different locations the first three locations are in the northern part and the last four are in the southern part obviously all the r 2 values are above 0 5 which shows that the ann approach to flow prediction is effective it should be mentioned that the two sites with relatively lower r 2 values sb02 and sc21 have small amounts of flow the ann performs well for sites with large amounts of flow such as nwtp sc17 sc21a and sc19 the results validate that ann can be a reliable method for flow prediction while its implementation can be improved by manipulation tuning parameters and refining techniques for ann 4 3 1 groundwater effect in previous efforts groundwater levels have not been considered as inputs to the data driven models however our experiments demonstrate that using groundwater data as inputs dramatically improves the prediction results for example r 2 for ann without using groundwater to predict flow of nwtp is 0 6374 while ann aided by groundwater provides an r 2 0 7341 the value of using groundwater for flow prediction is further highlighted by observing the correlation between groundwater levels and flow for example the correlation coefficient lee rodgers and nicewander 1988 between flow at nwtp and groundwater in well 5 is 0 7044 furthermore fig 5 depicts two groundwater well levels at the north part of springfield compared to the flow of nwtp it is obvious that groundwater level increases with the spikes of flow it implies that a strong correlation exists between flows and groundwater levels therefore it is quite prudent to use the groundwater data to estimate flow at various locations 4 4 lstm similar to ann lstm also needs to be tuned for different situations since lstm intrinsically stores prior information we do not need to consider a moving window as large as the moving window applied for ann our simulations have shown that a moving window size of 10 results in the best performance another important tuning parameter is the number of hidden neurons corresponding to each lstm unit it is important to remember that the training process for lstm is longer than the ann therefore increasing the number of training epochs may not be feasible furthermore increasing the number of the epochs may lead to overfitting and consequently the trained lstm network may not generalize in other words its performance may degrade for another dataset that does not share the same characteristics of the training set therefore there should be a trade off between the training time and overfitting table 2 summarizes r 2 values for flow prediction of nwtp as a function of all these parameters and their impact is discussed below moving window for ann and lasso discussed in the next section we use a moving window with length of 150 however our simulation shows that if the other parameters are fixed we can achieve better performance by decreasing the moving window length this observation can be explained by the fact that lstm internally stores prior information therefore we do not need to consider a large moving window on the other hand very small moving windows may lead to poor performances according to our simulations the best value for moving window is 10 for our case study hidden layer length of the hidden layer in the ann is chosen as 23 which can not be generalized for our lstm implementation by looking at the table 2 increasing the hidden layer neurons improves the results however such an increase has some drawbacks first it considerably increases training time second increasing the hidden neurons prevents generalization of the lstm implementation similar to the other parameters the lstm designer should decide if the achieved results are acceptable while considering lower number of neurons in the hidden layer training epochs training epochs is an important factor during training process we have to be certain that the selected number of epochs is sufficient enough for training our model however increasing the training epochs beyond a certain value does not improve the results in our experiments we determined that a training epoch of 400 is adequate for our lstm training as we shown by our results and similar works such as hu et al 2018 lstm is a highly qualified candidate for flow prediction this phenomenon is more highlighted when the dynamic of system is changing lstm is capable of train the network as we get the new data this capability becomes more interesting when geographical and environmental conditions of an area is varying over time 4 5 lasso as we mentioned earlier the lasso technique can be beneficial to identify the most influential factors in flow prediction we implement the lasso approach for a flow location in the south area of the city labeled as sc 19 in the lasso method we select the inputs predictors or u i k as eight different rainfall meters hour index groundwater levels and flow at the previous time instant more precisely in the regression model represented by eq 6 y i is flow at current time the first predictor is flow at the previous time instant i e u 1 i y i 1 furthermore u 2 i u 25 i are hour indicators u 26 i u 1225 i are rainfall values with moving window of 150 and u 1226 i is a groundwater level table 3 shows the results of lasso with different Î» values if there is no penalty on choosing the weights i e Î» 0 lasso regression represents a simple regression and its performance is close to the simple ann result provided in table 1 however as Î» grows r 2 decreases since we put a penalty on choosing more predictors obviously from the optimization the problem in 7 higher values of Î» enforces sparsity in prediction in other words in addition to error minimization the optimizer is also concerned about minimizing the number of nonzero weights in order to find the most significant predictors features we put a constraint on choosing the weights i e Î» 0 here we observe that Î» 10 provides an acceptable performance now it is desirable to look at the weights corresponding to the different flow gauges first we define a measure to quantify influence of each location on the prediction assume that Î² g Î² j Î² j w t are the corresponding coefficients to the rainfall values of the gauge g where w is the moving window represented by e g in eq 9 we consider second norm of these coefficients as a measure 9 e g Î² g 2 l j j w Î² l 2 1 2 definitely smaller e g means that the gauge g has a negligible effect on the flow of the selected location e g s for lasso coefficients of sc19 flow prediction are provided in table 4 these rain locations are spread along the south part of the city however it is clear that some of them are more significant predictors e g 1 than the other locations e g 0 4 now our aim is to remove some of these predictors features in the regression model therefore we do not incorporate some of the rain locations as predictors features for our systems in two scenarios scenario 1 we only consider the rainfall values at sc19 because the result in table 4 suggests sc19 is the most significant predictor feature among the selected inputs since e sc 19 is a large value after this removal we still achieve an acceptable performance with r 2 0 7089 while considering all predictors results in r 2 0 7315 scenario 2 to see effects of the least important locations we do not consider battlefield jr07 and pc06 although three locations 450 predictors are removed we still obtain a good prediction i e r 2 is only 0 004 smaller than the case when we consider all predictors in general depending on our desired accuracy of prediction we can remove the less important predictors features just as some of the sites might be considered as less important for predicting flows some of the time samples do not notably contribute to flow prediction recall that we consider a moving window with length of 150 intuitively the samples relevant to a very long time ago are not as important as the recent samples in order to evaluate the effect of past times we apply lasso technique with a penalty Î» 1 then we remove a group of predictions with the smallest coefficients for this purpose we set a threshold Î· that the coefficients with absolute value smaller than the threshold are taken out of the coefficient vector in other words if Î² j Î· holds we set Î² j 0 which means the corresponding predictor u j is ignored during the prediction process the first row of table 5 shows the threshold values for removing the coefficients second row gives the number of removed coefficients and third row returns r 2 values when we remove the detected coefficients from this table we can see the improvement in performance using this predictor refinement process however such an improvement stops after Î· 0 07 this means that considering the coefficients above Î· 0 07 is necessary for flow prediction in this case if we apply the coefficient removal approach with a threshold Î· 0 0684 we ignore 62 6 of the predictors while we achieve even a better performance compared to the case when all prediction are retained this is due to the fact that predictors can be correlated to each other and using a large number of such correlated predictors can lead to noise accumulation in general the lasso results are totally reliable while they return the most effective factors in prediction of flow for a certain location this property enables the designers to have a better insight about the sewer network and effective features 4 6 resampling one of our goals is to predict flows during storm events rather than normal days without rain given the drought conditions in springfield the storm events are rare to show this phenomenon graphically fig 6 demonstrates the histogram of flow for pr18 meter during 518 days this plot shows the number of samples corresponding to a certain range of flow values from april 2016 to october 2017 flow during normal days does not exceed 4 5 mgd marked by red line in fig 6 obviously most of the data belong to normal days in other words the numbers of samples for rainy days and drought days are not balanced however we are concerned about the prediction of storm events which are less presented in the given dataset consequently training dataset to this end we split the data between normal and rare cases using a threshold more precisely if the flow is smaller than the determined threshold we consider the corresponding data as a normal case otherwise the data is categorized as a rare case it should be noted that the threshold value changes for different flow locations since otherwise a normal case for one location may be categorized as a rare case for another one using algorithm 1 and algorithm 2 we oversample the rare cases and undersample the normal cases by rates of o and u respectively table 6 summarizes the results of ann and lasso with various oversampling and undersampling rates second column of this table is the percentage of undersampling u presented in algorithm 1 third column is the oversampling rate where if it is equal to o o percent of the rare cases is generated during the resampling process fourth and fifth columns of table 6 show performances of the ann and lasso techniques respectively training and testing dataset for both ann and lasso are the same additionally all the configurations and parameters for both ann and lasso are set to be the same as the previous simulations given in this section we set Î» 0 5 for lasso regression in general resampling technique improves the results obtained relative to the original dataset experiment 1 for both ann and lasso the experiments 3 to 7 demonstrate that undersampling improves the original results all the other experiments confirm that oversampling of the training dataset leads to improved prediction performance to see the mentioned effects fig 7 a and fig 7b depict two zoomed in snapshots of actual flow at nwtp with three different predictions based on experiments 1 9 and 10 fig 7a and fig 7b show two examples of rare events and normal cases receptively fig 7a shows flow at the highest amount among all the times where obviously experiment 10 outperforms experiment 1 since it has more rare samples for its training processes on the other hand as seen in fig 7b experiment 10 has less accuracy compared to experiment 9 and experiment 1 this is because the undersampling rate of experiment 10 is 50 and consequently it has less data corresponding to the normal cases during the training process in general there is a trade off between accuracy and oversampling rate and it varies for different locations as a general result of our experiments we can assert that for the locations which the data are scarce and imbalanced the resampling technique is a good candidate for improving accuracy of flow prediction 5 conclusion in this paper we address the problem of flow prediction in the sewer network of the city of springfield missouri to this end we use three data driven models 1 artificial neural network ann 2 long short term memory lstm 3 least absolute shrinkage and selection operator lasso all three techniques perform well in terms of accuracy while the lstm achieves a better performance among the other methods it has the most complexity furthermore the lasso method is able to identify the most important spatial and temporal features that affect flow of wastewater to enhance accuracy of flow prediction we add groundwater level data as a new feature simulation results demonstrate that groundwater data is considerably helpful in improving flow prediction since we are interested in prediction of rare events the imbalance in the data set is alleviated using a resampling technique smoter that generates new cases similar to the rare events while ignoring some data relevant to normal events simulation results show prediction performance is improved using the resampling technique funding sources this work was supported by the city of springfield missouri though a grant from black and veatch inc declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
6260,situational awareness in sanitary sewer systems requires accurate flow information at different spatial locations in a city it is especially desirable to predict flows across a wastewater network in response to heavy rainfall events in addition to regular consumption patterns typically complicated hydraulic models that suffer from difficulties in parameter identification and high computational burden are used for flow prediction recently data driven approaches have been employed for flow prediction in this paper we first design and then compare the performance of three data driven methods to predict flow 1 artificial neural network ann 2 long short term memory lstm 3 least absolute shrinkage and selection operator lasso we test the performance of these machine and statistical learning techniques using data gathered from the city of springfield while all three data driven methodologies provide acceptable prediction performance we observe that lstm outperforms ann due to the inherent memory integrated with a feedback structure the statistical learning approach i e lasso regression not only offers good prediction performance but also helps identify the key spatial and temporal features that influence flow at any specific location this added information could aid in remediation activities another key contribution of this paper is that we quantify the value of groundwater data in flow prediction specifically including groundwater data as an additional input enhances flow prediction performance in all three methods lastly to better predict flows corresponding to rare events e g 50 or 100 year rainfall events we use a resampling approach known as smoter to modify the training dataset simulation results indicate that the resampling technique is effective in improving prediction performance keywords sewer system flow prediction neural network lstm lasso resampling 1 introduction cso combined sewer overflows ann artificial neural network rnn recurrent neural network lstm long short term memory lasso least absolute shrinkage and selection operator smote synthetic minority over sampling technique Ïƒ c activation function hadamard product in any city sewer systems are considered essential as they play an important role in sanitation and disease prevention flow control in sewer systems is critical as improper management can lead to overflows and flooding in some parts and high influent flows at wastewater treatment plants wwtp however understanding of flows in the sewer system in real time is not trivial due to varying capacities of sewer pipes differential water consumption patterns and weather events we cannot expect similar flows at different locations within a city for example during heavy rainfall events overflows may occur at some parts of the city whereas other parts may not encounter any flooding therefore cities have significant interest in implementing flow control strategies to prevent catastrophic events darsono and labadie 2007 presents an optimal control algorithm to decrease the hydraulic load of the wwtp in combined sewer systems this control strategy minimizes the pollution impacts of untreated combined sewer overflows cso and is cost effective in terms of practical implementation darsono and labadie 2007 while this approach could efficiently reduce infrastructure investments it requires a correct control design and adequate modeling capabilities garofalo et al 2017 obviously an accurate control depends on precise information regarding flows in the sewer system not only for the current time instant but also for future times instances chen et al 2014 liu et al 2016 this fact highlights the importance of flow prediction and situational awareness while designing flow control strategies in prior efforts many flow prediction approaches have been explored flow prediction using a hydraulic model of the sewer system is an efficient approach when rainfall patterns and structure of the sewer system is adequately identified autixier et al 2014 lucas and sample 2015 in spite of their superior performance hydraulic models are extremely sensitive to the accuracy of the sewer system information additionally hydraulic model based prediction techniques suffer from high computational complexity and typically a large number of model parameters need to be identified for the prediction to be effective furthermore while majority of the hydraulic models are presented in a deterministic framework the dynamics of many of the underlying parameters are stochastic in nature this makes the system identification process quite challenging therefore although hydraulic models capture the hydraulic aspects of a sewer system accurately the disadvantages discussed above act as deterrents to its use in practical flow prediction el din and smith 2002 as an alternative to the hydraulic models black box models may be able to capture the relationships between measurable inputs and outputs of dynamical systems even without detailed internal description of the sewer system one of the first efforts to use black box models for flow prediction in wwtp was presented in carstensen et al 1998 which compares three prediction models with different complexities among a simple regression model a gray box model and a complex hydraulic model carstensen et al 1998 shows that the first two models outperform the complex hydraulic model for flow prediction another black box model that could be a qualified candidate is artificial neural network ann which is implemented for wastewater flow prediction in el din and smith 2002 based on data from edmonton canada the architecture of the ann and the impact of the various parameters involved in its design are discussed in el din and smith 2002 in wei et al 2012 the authors study a multilayer perceptron neural network algorithm to predict flow at the treatment plant at des moines wastewater reclamation facility wrf in iowa usa in addition to the current rainfall data they also use radar reflectivity data to achieve an improved flow prediction for different short time horizons in a recent work flow is predicted using different recurrent neural networks for drammen city norway zhang et al 2018b for control purposes jean et al 2018 evaluates the effects of three rainfall data selection methods on the estimation of cso volume thresholds different neural networks are applied for predicting the water level of a cso structure in zhang et al 2018a using data collected from an internet of things monitoring cso structure in chen et al 2014 real time future flow prediction is studied based on autoregressive moving average arma models and multistep iterative prediction then online control of chemical dosing is implemented using the predicted flow another control application is presented in liu et al 2016 where an event driven model predictive control empc methodology is proposed to control the flows of sewage streams containing the dosed chemical in li et al 2019 autoregressive with exogenous inputs arx models are employed to reduce the delays with rainfall data used as model inputs to control chemical dosing in sewer systems using a hydraulic model and lstm zhang et al 2017 presents a new inter catchment wastewater transfer method which can mitigate sewer overflows 1 1 contribution in this paper for the first time we present a comprehensive comparison of the performances of various flow prediction methods that are based on black box models while we consider previously proposed black box models such as ann and long short term memory lstm network we also custom design a statistical learning based prediction algorithm built on the least absolute shrinkage and selection operator lasso specifically we use these three strategies to predict flows at the wastewater treatment plant located in the city of springfield missouri usa we show that while the ann provides acceptable flow predictions lstm outperforms ann in terms of accuracy of prediction the lasso approach in addition to offering a prediction performance that is comparable to ann and lstm is able to determine the most effective spatial and temporal features that influence flow to the best knowledge of the authors groundwater level has not been considered as a feature input for the machine learning methods used for flow prediction in this paper we quantify the value of using groundwater levels as inputs by highlighting the resulting improvement in flow prediction in all three methods one of the important goals emphasized in many prior efforts is the effective prediction of flow during storm events el din and smith 2002 zhang et al 2018b however training datasets rarely contains such storm cases leading to poor prediction accuracy for storm events to alleviate the issue of scarce training data related to rare events we employ a resampling approach known as smoter in order to balance the dataset used for training smoter consists of an oversampling and an undersampling processes wherein the oversampling process generates new data similar to the rare cases and the undersampling process randomly removes some of the normal samples from the training dataset simulation results illustrate the efficiency of the applied resampling approach while highlighting the effects of the degree of oversampling and undersampling on the prediction performance 2 data description the area of interest in our work is the city of springfield missouri usa coordinates 37 12 55 n 93 17 54 w this is the third largest city in the state of missouri with 167 376 residents and an area of 213 18 km 2 a map for the sanitary sewer network of springfield can be viewed in esri 2019 fig 1 a provides an overview of the entire drainage area and the tributary wastewater paths to the treatment plants as it is discerned by the two colors in fig 1a the overall sewer system is divided into two separate subsystems each with a different treatment plant i e the north and south wwtp fig 1b presents a magnified view of the north subsystem which contains the locations of rain gauges groundwater wells long term and short term flow meters north wastewater treatment plant and sewer paths to the plant for flow prediction we consider the following data sources flow meter data after 2011 flow meter data is available every 15 min for all the permanent meter locations indicated by the red bullets in fig 1b in addition to the permanent sites some temporary flow meters are installed to provide additional information indicated by yellow and orange bullets in fig 1b the temporary flow meters are not operational at all times and measure flows for specific time periods similar to the permanent flow meters the time interval between two subsequent measurements of the temporary flow meters is 15 min rainfall data indicated via a drop symbol in fig 1b rainfall gauges have been installed in locations throughout the city to record rainfall amounts starting in 2012 similar to the flow data rainfall amounts are recorded reported every 15 min obviously these rain gauges are not necessarily co located with the flow meters however we can approximate the amount of rainfall at every flow meter location using weighted combination of the nearest rain gauge measurements groundwater data levels of water in the groundwater wells are provided every 5 min from april 2016 to october 2017 in fig 1b green bullets indicate locations of the groundwater wells time information for all the above variables flow rain groundwater data collected includes time information therefore we use the time index as an input feature for flow prediction this is reasonable as water consumption behavior of springfield residents should vary based on the time of day 3 prediction techniques in any sewer system the exact relationship between inputs e g rainfall time etc and output e g flow is not easily captured relying on precise but complex hydraulic models present challenges related to parameter identification this is because the available data is typically noisy and the error associated with the data significantly degrades the performance of any system identification approach therefore we confine ourselves to black box modeling techniques which can capture the complexity of the system even with noisy data the rest of this section provides general descriptions of three prediction techniques and one data resampling approach that we apply in this paper additionally the strengths and weaknesses of the considered methods are also highlighted 3 1 artificial neural network artificial neural network ann is a computational model which is inspired by the structure and function of biological neural networks consider that we have an input output data set then aided by some training techniques ann can play the role of a black box that relates the inputs to the outputs in other words anns are nonlinear data modeling tools that can find complex relationships and patterns between inputs and outputs as shown in fig 2 anns consist of three main layers that are interconnected the first layer contains input neurons that are transferred to the next layer by some weights and a bias the second layer known as a hidden layer passes the weighted data through some activation functions then the calculated values from hidden layer are conveyed to the output by new weights and a bias it should be noted that the hidden layer can be divided into more connected layers after deciding the structure of the ann e g number of hidden layers and number of hidden neurons at each layer we need to estimate the values of the weights and biases using training techniques the training process is accomplished via different algorithms such as gradient descent methods gene expression programming evolutionary methods particle swarm optimization etc in addition to the exact training procedure used the number of epochs for training is an important factor chosen by the algorithm designer if the training iterations are not adequate the neural network may miss some characteristics of the system on the other hand if too many training epochs are used over training could lead to a poor performance when dealing with test datasets that have characteristics that are changed from the training set in our problem empirical observations suggest a strong dependency of flow on variables such as groundwater rainfall and residential water consumption however such a relationship between flow and the mentioned variables is quite complex therefore an ann can serve as a good candidate to capture the complex relationship between flow and variables such as rainfall amounts groundwater levels etc 3 2 lstm various forms of rnns have been effective in modeling dynamic behavior and time series data zhang et al 2018b hu et al 2018 equipped with feedback connections an rnn is designed to memorize recent information and apply it as inputs to the activation functions used in a neural network typically rnns consist of consecutive units with each unit corresponding to a certain time t there are various architectures of rnns that result from different connections between the units however if the units exhibit long term correlation with the past standard rnns perform poorly lecun et al 2015 to address this problem hochreiter and schmidhuber introduced lstm neural networks hochreiter and schmidhuber 1997 which inherently store long term system states lstm networks are well suited to classifying processing and making predictions based on time series data since there can be lags of unknown duration between important events in a time series a typical lstm unit consists of a memory cell forget gate input gate and output gate the memory cell keeps track of the relationship between the elements in the input data the forget gate determines whether a piece of information should be retained or forgotten in the cell the input gate specifies the extent of new information flowing into the cell the output gate as the name suggests controls the information output from the cell which in turn determines the output of the lstm unit the proposed rnn based spe forecasting will rely on multiple lstm units connecting to one another in series fig 3 shows a unit of lstm for time t where x t r d is the input vector to the lstm unit h t r h is the output vector of the lstm unit and c t r h is the cell state vector also f t r h i t r h and o t r h denote forget gate s activation vector input gate s activation vector and output gate s activation vector respectively eqs 1 5 define the relationships between different variables of an lstm unit at time t hochreiter and schmidhuber 1997 1 f t Ïƒ g w f x t u f h t 1 b f 2 i t Ïƒ g w i x t u i h t 1 b i 3 o t Ïƒ g w o x t u o h t 1 b o 4 c t f t c t 1 i t Ïƒ c w c x t u c h t 1 b c 5 h t o t Ïƒ h c t here Ïƒ g Ïƒ c and Ïƒ h denote activation functions in this work we choose Ïƒ g to be a sigmoid function while Ïƒ c and Ïƒ h are hyperbolic tangent functions denotes hadamard product element wise product w u and b are weight matrices and bias vectors that should be identified during the training process an iterative gradient descent e g backpropagation through time has been shown to be an effective approach to train the lstm units recently lstm has been shown to be effective in prediction of wastewater flow zhang et al 2018b and rainfall runoff hu et al 2018 in sewer systems flow at current time is not only a function of current time features but also a function of the previous rainfall and flow values such a phenomenon implies that our chosen prediction technique must use prior information for current time prediction since the lstm stores previous states of the underlying system it is a viable candidate for wastewater flow prediction 3 3 lasso regression as a regression analysis method least absolute shrinkage and selection operator lasso aims to minimize sum of the squared errors while including an l 1 norm penalty for the number of predictors used tibshirani 1996 consider the regression model 6 y i Î² 0 u 1 i Î² 1 u p i Î² p i i 1 2 n where y i is the i th sample of target output value and i is the error term u i u 1 i u p i t is i th predictor feature input vector and Î² Î² 1 Î² p t is the parameter vector the objective of lasso is to 7 min Î² 0 Î² 1 n i 1 n y i Î² 0 u i t Î² Î» Î² 1 where Î» is a tuning parameter that controls regularization term that determines the level of sparsity that one would like to impose on Î² vector so if a high value is chosen for this parameter most of the elements in Î² are set to zero or close to zero in other words lasso determines the most significant variables in the regression model 6 by assigning small values to the less important factors then if we just retain the important predictors and throw away the less significant predictors we can still achieve an acceptable performance of prediction consequently one can predict flows using the lasso regression model with fewer input features than those used in ann or lstm however it should be noted that if we excessively increase Î» and emphasize sparsity some of the important predictors may be neglected the computation of the lasso solutions is a quadratic programming problem and can be tackled by standard numerical analysis algorithms however the least angle regression procedure outperforms the existing approach this algorithm exploits the special structure of the lasso problem and provides an efficient way to compute the solutions in this paper the lasso strategy enables us to determine the most influential factors in flow prediction understanding the most important predictors could be very useful for sewer system designers and managers more precisely a designer can manipulate the sewer system structure to refine flow at a certain location since the designer knows the most influential factors a priori furthermore removing unnecessary predictors can help to achieve a better performance as we avoid noise accumulation 3 4 resampling all the mentioned techniques in this section are black box data based approaches where the role of data in achieving well trained models is critical in many applications we are typically only interested in the prediction classification of certain rare events however these rare events may be sparsely represented in the training data set this in turn leads to the designed models missing lacking important characteristics of the underlying systems one approach to tackle the problem of imbalanced data is the undersampling and synthetic minority over sampling technique smote introduced in chawla et al 2002 for classification applications smote combines undersampling of the major classes with oversampling of the minor class in torgo et al 2013 authors generalize smote for regression tasks in order to accurately predict the rare cases algorithm 1 and algorithm 2 summarize smote for regression models also known as smoter in this algorithm x and y are samples of the inputs and outputs respectively d denotes the entire data set for training Ï• y is a score function that maps the outputs to a value between 0 and 1 choice of this function depends on how we define rare cases but in general the extreme cases must receive a high relevance score a threshold value t h should also be determined to decide whether the selected pair x y belongs to a rare cases or not y is the median of target values o and u are the percentages of oversampling and undersampling respectively k is the number of nearest neighbors in the k nearest neighbors knn step of algorithm 2 extensive discussions on choosing the parameters and relative function Ï• can be found in torgo et al 2013 and moniz et al 2016 after setting the parameters the data is divided between rare and normal cases in step 1 also the rare cases are divided based on the data which are higher or lower than the median step 2 corresponds to the oversampling process which is performed by the algorithm 2 in step 3 we randomly remove u percent of the normal cases in step 4 we combine all the undersampled and oversampled data as a new dataset algorithm 2 oversamples the rare cases that are determined by step 1 in algorithm 2 in algorithm 2 for each sample of dataset d k nearest neighbors are selected from the same dataset then if the attribute of the selected sample is numerical a new attribute is generated that is an interpolation of the values of the two original cases if the attribute is not numerical we randomly select the attribute of the original sample or attribute of the selected neighbor z then a new target is generated by a weighted average of the original sample and the selected neighbor the new target and the new attributes are combined and are considered as a new data sample depending on the rate of oversampling o this process is repeated algorithm 1 the main smoter algorithm torgo et al 2013 set the parameters o u y t h and k determine the function Ï• t h the threshold for relevance of the target variable values o u percentages of oversampling and undersampling k the number of neighbors used in case generation step 1 divide the data according to following rarel x y d Ï• y t h y y y is the median of target values rareh x y d Ï• y t h y y norm d rarel rareh step 2 generate new synthetic cases for rare cases using algorithm 2 and save it into the following sets newcasesl gensynth rarel o k newcasesh gensynth rareh o k step 3 undersample the normal cases by newnorm remove uofnorm step 4 making the new dataset by newdataset newnorm newcasesl newcasesh algorithm 2 generate synthetic cases torgo et al 2013 set the dataset to d function gensynth d o k set ng o 100 for case d do nns knn k case d case k nearest neighbours of case for i 1 to ng do choose one of the nns as z for all a attributes do if a is a nummeric value then diff case a z a new a case a random 0 1 diff else new a randomly select among case a and z a endif endfor set d 1 to the distance between new and case set d 2 to the distance between new and z new target d 2 case target d 1 z target d 1 d 2 newcases newcases new end for end for return newcases in the city of springfield sewer system flow values are typically low across the network due to the prevailing drought conditions however city planners are interested in understanding the impact of rare and intense rainfall events on the flows in the network in addition to an imbalanced dataset the available dataset is not large therefore lack of data and imbalance in the dataset motivate the use of resampling techniques to the best knowledge of authors resampling techniques have not been employed in area of flow prediction 4 results and discussion in this section we present the flow prediction performances of the three data driven approaches to quantify performance of the prediction methods we use the coefficient of determination r 2 which is a statistical indicator that compares the accuracy of the model to the accuracy of a trivial benchmark model the coefficient of determination corresponds to 8 r 2 1 i 1 n y i y i 2 i 1 n y i y i 2 where y i is the actual output value y i is the predicted value y i is the mean of y i values and n is the total number of data samples r 2 can take a value between and 1 where an r 2 value of 1 indicates a perfect prediction while r 2 implies a very poor fit in general we can not compare the r 2 values reported in other efforts with the obtained r 2 values for our system as the data sets and the underlying factors can be quite different however as a rule of thumb we can clarify a prediction method as successful if it returns a r 2 value above 0 5 zhang et al 2018b 4 1 data preparation black and veatch corporation has installed several meters throughout the city the installed hardware are ads triton meters these are categorized in contact meters which means there are area velocity probes installed in the pipe the topside control units have cellular sim cards that transmit the data collected to flowview which is an ads s web based site in order to store the data this data is in 15 min intervals then different scripts are run to process the raw data data scrubbing process this includes removing erroneous spikes due to debris build up or drops due to dead batteries therefore the available data is not always conductive to analysis for example data gathered from flow meters can sometimes be missing additionally spurious measurements noise or other factors can lead to outliers in the data set such erroneous data must be cleaned prior to design and training of the prediction techniques we replace the outliers and fill in the missed data with similar data from the previous hours 4 2 input output configuration the inputs or predictor variables in all the data driven methods include the following hour index we use 24 inputs corresponding to 24 h of a day at each time instant only one of the 24 inputs that corresponds to the current hour is set to 1 and rest of the inputs are set to 0 for example if time is 4 p m the 16 th input is 1 and the other 23 inputs are 0 rainfall amounts in order to estimate current flow we must include the effects of previous rainfall amounts one approach is to consider the average of rainfall values for a specific horizon of time as a predictor variable however our simulations show that a moving window including actual rainfall amounts as inputs is more effective than using the averaged rainfall as a predictor the size of this moving window is also a tuning parameter that should be selected carefully groundwater levels we have access to data from two groundwater wells located in the north plant area and one groundwater well for the south area since our goal is to predict flow at various locations of the sewer system flow plays the role of the output in all three methods any dataset must contain all the mentioned inputs and output i e dataset must have rainfall amounts groundwater levels and flow meters to this end we consider the dataset starting from 4 1 2016 to 9 1 2017 first half of this dataset is dedicated to train the models while the second half plays the role of testing data therefore in the following subsections we provide the test results based on the last 5832 data samples we have developed our own code to implement the overall data analysis task this code is written in matlab and leverages the neural network toolbox the lasso method also within matlab solves the l 1 minimization problems in 7 using the cvx package grant and boyd 2014 the code was run on intel core i7 pc with clock speed 3 4 ghz and 32 0 gb ram remark when the new data becomes available the parameters of all the methods can be updated to enhance the accuracy of future predictions 4 3 ann as we mentioned earlier parameters of the ann should be tuned to achieve an acceptable performance in this study we apply an ann with only one hidden layer numbers of hidden neurons however can be varied in our experiments we observed that with 8 to 25 hidden neurons the prediction performance was quite similar the activation function is selected to be a sigmoid function fig 4 shows the predicted flow versus the actual flow for location pr18 in the north part of the city it is clear that the estimated value of flow is approximately close to the actual value obviously after a rainfall event depicted by the red curve the flow increases and the estimated flow responds to the rainfall amount for this prediction the coefficient of determination r 2 is 0 7822 which falls in the acceptable interval of prediction accuracy table 1 presents the performances of ann for different locations the first three locations are in the northern part and the last four are in the southern part obviously all the r 2 values are above 0 5 which shows that the ann approach to flow prediction is effective it should be mentioned that the two sites with relatively lower r 2 values sb02 and sc21 have small amounts of flow the ann performs well for sites with large amounts of flow such as nwtp sc17 sc21a and sc19 the results validate that ann can be a reliable method for flow prediction while its implementation can be improved by manipulation tuning parameters and refining techniques for ann 4 3 1 groundwater effect in previous efforts groundwater levels have not been considered as inputs to the data driven models however our experiments demonstrate that using groundwater data as inputs dramatically improves the prediction results for example r 2 for ann without using groundwater to predict flow of nwtp is 0 6374 while ann aided by groundwater provides an r 2 0 7341 the value of using groundwater for flow prediction is further highlighted by observing the correlation between groundwater levels and flow for example the correlation coefficient lee rodgers and nicewander 1988 between flow at nwtp and groundwater in well 5 is 0 7044 furthermore fig 5 depicts two groundwater well levels at the north part of springfield compared to the flow of nwtp it is obvious that groundwater level increases with the spikes of flow it implies that a strong correlation exists between flows and groundwater levels therefore it is quite prudent to use the groundwater data to estimate flow at various locations 4 4 lstm similar to ann lstm also needs to be tuned for different situations since lstm intrinsically stores prior information we do not need to consider a moving window as large as the moving window applied for ann our simulations have shown that a moving window size of 10 results in the best performance another important tuning parameter is the number of hidden neurons corresponding to each lstm unit it is important to remember that the training process for lstm is longer than the ann therefore increasing the number of training epochs may not be feasible furthermore increasing the number of the epochs may lead to overfitting and consequently the trained lstm network may not generalize in other words its performance may degrade for another dataset that does not share the same characteristics of the training set therefore there should be a trade off between the training time and overfitting table 2 summarizes r 2 values for flow prediction of nwtp as a function of all these parameters and their impact is discussed below moving window for ann and lasso discussed in the next section we use a moving window with length of 150 however our simulation shows that if the other parameters are fixed we can achieve better performance by decreasing the moving window length this observation can be explained by the fact that lstm internally stores prior information therefore we do not need to consider a large moving window on the other hand very small moving windows may lead to poor performances according to our simulations the best value for moving window is 10 for our case study hidden layer length of the hidden layer in the ann is chosen as 23 which can not be generalized for our lstm implementation by looking at the table 2 increasing the hidden layer neurons improves the results however such an increase has some drawbacks first it considerably increases training time second increasing the hidden neurons prevents generalization of the lstm implementation similar to the other parameters the lstm designer should decide if the achieved results are acceptable while considering lower number of neurons in the hidden layer training epochs training epochs is an important factor during training process we have to be certain that the selected number of epochs is sufficient enough for training our model however increasing the training epochs beyond a certain value does not improve the results in our experiments we determined that a training epoch of 400 is adequate for our lstm training as we shown by our results and similar works such as hu et al 2018 lstm is a highly qualified candidate for flow prediction this phenomenon is more highlighted when the dynamic of system is changing lstm is capable of train the network as we get the new data this capability becomes more interesting when geographical and environmental conditions of an area is varying over time 4 5 lasso as we mentioned earlier the lasso technique can be beneficial to identify the most influential factors in flow prediction we implement the lasso approach for a flow location in the south area of the city labeled as sc 19 in the lasso method we select the inputs predictors or u i k as eight different rainfall meters hour index groundwater levels and flow at the previous time instant more precisely in the regression model represented by eq 6 y i is flow at current time the first predictor is flow at the previous time instant i e u 1 i y i 1 furthermore u 2 i u 25 i are hour indicators u 26 i u 1225 i are rainfall values with moving window of 150 and u 1226 i is a groundwater level table 3 shows the results of lasso with different Î» values if there is no penalty on choosing the weights i e Î» 0 lasso regression represents a simple regression and its performance is close to the simple ann result provided in table 1 however as Î» grows r 2 decreases since we put a penalty on choosing more predictors obviously from the optimization the problem in 7 higher values of Î» enforces sparsity in prediction in other words in addition to error minimization the optimizer is also concerned about minimizing the number of nonzero weights in order to find the most significant predictors features we put a constraint on choosing the weights i e Î» 0 here we observe that Î» 10 provides an acceptable performance now it is desirable to look at the weights corresponding to the different flow gauges first we define a measure to quantify influence of each location on the prediction assume that Î² g Î² j Î² j w t are the corresponding coefficients to the rainfall values of the gauge g where w is the moving window represented by e g in eq 9 we consider second norm of these coefficients as a measure 9 e g Î² g 2 l j j w Î² l 2 1 2 definitely smaller e g means that the gauge g has a negligible effect on the flow of the selected location e g s for lasso coefficients of sc19 flow prediction are provided in table 4 these rain locations are spread along the south part of the city however it is clear that some of them are more significant predictors e g 1 than the other locations e g 0 4 now our aim is to remove some of these predictors features in the regression model therefore we do not incorporate some of the rain locations as predictors features for our systems in two scenarios scenario 1 we only consider the rainfall values at sc19 because the result in table 4 suggests sc19 is the most significant predictor feature among the selected inputs since e sc 19 is a large value after this removal we still achieve an acceptable performance with r 2 0 7089 while considering all predictors results in r 2 0 7315 scenario 2 to see effects of the least important locations we do not consider battlefield jr07 and pc06 although three locations 450 predictors are removed we still obtain a good prediction i e r 2 is only 0 004 smaller than the case when we consider all predictors in general depending on our desired accuracy of prediction we can remove the less important predictors features just as some of the sites might be considered as less important for predicting flows some of the time samples do not notably contribute to flow prediction recall that we consider a moving window with length of 150 intuitively the samples relevant to a very long time ago are not as important as the recent samples in order to evaluate the effect of past times we apply lasso technique with a penalty Î» 1 then we remove a group of predictions with the smallest coefficients for this purpose we set a threshold Î· that the coefficients with absolute value smaller than the threshold are taken out of the coefficient vector in other words if Î² j Î· holds we set Î² j 0 which means the corresponding predictor u j is ignored during the prediction process the first row of table 5 shows the threshold values for removing the coefficients second row gives the number of removed coefficients and third row returns r 2 values when we remove the detected coefficients from this table we can see the improvement in performance using this predictor refinement process however such an improvement stops after Î· 0 07 this means that considering the coefficients above Î· 0 07 is necessary for flow prediction in this case if we apply the coefficient removal approach with a threshold Î· 0 0684 we ignore 62 6 of the predictors while we achieve even a better performance compared to the case when all prediction are retained this is due to the fact that predictors can be correlated to each other and using a large number of such correlated predictors can lead to noise accumulation in general the lasso results are totally reliable while they return the most effective factors in prediction of flow for a certain location this property enables the designers to have a better insight about the sewer network and effective features 4 6 resampling one of our goals is to predict flows during storm events rather than normal days without rain given the drought conditions in springfield the storm events are rare to show this phenomenon graphically fig 6 demonstrates the histogram of flow for pr18 meter during 518 days this plot shows the number of samples corresponding to a certain range of flow values from april 2016 to october 2017 flow during normal days does not exceed 4 5 mgd marked by red line in fig 6 obviously most of the data belong to normal days in other words the numbers of samples for rainy days and drought days are not balanced however we are concerned about the prediction of storm events which are less presented in the given dataset consequently training dataset to this end we split the data between normal and rare cases using a threshold more precisely if the flow is smaller than the determined threshold we consider the corresponding data as a normal case otherwise the data is categorized as a rare case it should be noted that the threshold value changes for different flow locations since otherwise a normal case for one location may be categorized as a rare case for another one using algorithm 1 and algorithm 2 we oversample the rare cases and undersample the normal cases by rates of o and u respectively table 6 summarizes the results of ann and lasso with various oversampling and undersampling rates second column of this table is the percentage of undersampling u presented in algorithm 1 third column is the oversampling rate where if it is equal to o o percent of the rare cases is generated during the resampling process fourth and fifth columns of table 6 show performances of the ann and lasso techniques respectively training and testing dataset for both ann and lasso are the same additionally all the configurations and parameters for both ann and lasso are set to be the same as the previous simulations given in this section we set Î» 0 5 for lasso regression in general resampling technique improves the results obtained relative to the original dataset experiment 1 for both ann and lasso the experiments 3 to 7 demonstrate that undersampling improves the original results all the other experiments confirm that oversampling of the training dataset leads to improved prediction performance to see the mentioned effects fig 7 a and fig 7b depict two zoomed in snapshots of actual flow at nwtp with three different predictions based on experiments 1 9 and 10 fig 7a and fig 7b show two examples of rare events and normal cases receptively fig 7a shows flow at the highest amount among all the times where obviously experiment 10 outperforms experiment 1 since it has more rare samples for its training processes on the other hand as seen in fig 7b experiment 10 has less accuracy compared to experiment 9 and experiment 1 this is because the undersampling rate of experiment 10 is 50 and consequently it has less data corresponding to the normal cases during the training process in general there is a trade off between accuracy and oversampling rate and it varies for different locations as a general result of our experiments we can assert that for the locations which the data are scarce and imbalanced the resampling technique is a good candidate for improving accuracy of flow prediction 5 conclusion in this paper we address the problem of flow prediction in the sewer network of the city of springfield missouri to this end we use three data driven models 1 artificial neural network ann 2 long short term memory lstm 3 least absolute shrinkage and selection operator lasso all three techniques perform well in terms of accuracy while the lstm achieves a better performance among the other methods it has the most complexity furthermore the lasso method is able to identify the most important spatial and temporal features that affect flow of wastewater to enhance accuracy of flow prediction we add groundwater level data as a new feature simulation results demonstrate that groundwater data is considerably helpful in improving flow prediction since we are interested in prediction of rare events the imbalance in the data set is alleviated using a resampling technique smoter that generates new cases similar to the rare events while ignoring some data relevant to normal events simulation results show prediction performance is improved using the resampling technique funding sources this work was supported by the city of springfield missouri though a grant from black and veatch inc declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
6261,increasing city resilience to floods under climate change has become one of the major challenges for decision makers urban planners and engineering practitioners around the world accurate prediction of urban floods under heavy precipitation is critically important to address such a challenge as it can help understand the vulnerability of a city to future climate change and simulate the effectiveness of various sustainable engineering techniques in reducing urban flooding risks in real urban settings here we propose a new model for urban flood prediction under heavy precipitation the model divides an irregular urban area into many grid cells with no limitation on the spatial resolution as long as the dem data of the same resolution are available it is capable of reflecting the frequent inflow or outflow interactions among grid cells and capturing the rapid generation of surface runoff in urban areas during heavy rainfall the model also accounts for typical characteristics of urban areas such as large scale impermeable surfaces and urban drainage systems in order to simulate urban floods more realistically in addition the model uses both surface elevation and instantaneous surface water depth of all grid cells to dynamically determine the directions of horizontal inflow and outflow during each time step of model simulation this enables the model to capture the reverse flow phenomenon which is commonly seen in flat urban areas during heavy storms by applying the proposed model for reproducing the 2016 flood in lafayette parish louisiana we demonstrate its effectiveness in predicting real world flood events keywords urban flood prediction climate change heavy precipitation reverse flow flooding risk flood map urban resilience 1 introduction as a consequence of global warming the hydrological cycle has been amplified in the form of more frequent and intense precipitation events allan and soden 2008 held and soden 2006 wang et al 2014 this becomes a serious challenge for most of the urban areas around the world where unprecedented flooding events due to heavy precipitation have been more frequently observed in recent years garner et al 2017 jha et al 2012 wang et al 2016 wilby and keenan 2012 for example the 2016 louisiana flood was caused by a severe weather system which brought prolonged rainfall in about 72 h to the southern parts of louisiana resulting in widespread and catastrophic flooding this flood damaged more than 140 000 houses required evacuation of more than 20 000 people and led to at least 13 deaths and 10 billion monetary losses noaa 2016 vahedifard et al 2016 watson et al 2017 it has been regarded as an historic and unprecedented flood event in louisiana and one of the worst natural disasters in united states severe floods have also frequently struck several major cities in canada in recent years and caused billions of dollars in damage particularly the 2013 floods in calgary and toronto have been recorded as the largest natural disasters in alberta s and ontario s history respectively the insurance damages resulted from these two events have constituted the first and third largest natural insured catastrophes in canadian history tanner and arvai 2018 thistlethwaite and henstra 2017 wang et al 2014 recent studies suggest that future global warming will lead to significant changes in the intensity and frequency of precipitation extremes which are very likely to be associated with higher risks of urban flooding alfieri et al 2016 schiermeier 2011 wang et al 2015 hence how to increase city resilience to floods under climate change has now become one of the major challenges for decision makers urban planners and engineering practitioners around the world hammond et al 2015 kim et al 2017 accurate prediction of urban flooding under heavy precipitation is critically important to address such a challenge because it can help understand the vulnerability of a city to future climate change from a long term perspective and simulate the effectiveness of various sustainable engineering techniques in reducing urban flooding risks in a real urban setting jha et al 2012 although most of conventional hydrological models perform generally well in simulating surface runoffs or river streamflow in natural watersheds they are unable to accurately simulate urban flooding because of the following reasons 1 most hydrological models require a predefined watershed in order to determine model simulation boundary arnold et al 2012 moriasi et al 2007 wolock and mccabe jr 1995 however it is difficult to identify a watershed since the natural watershed has been largely altered by intense human activities in modern urban areas 2 some grid based land surface models e g vic are able to simulate large scale runoffs by dividing the study domain into many grid cells with a spatial resolution greater than 1 km gao et al 2010 liang et al 1994 but they are still unsuitable for urban flood modeling because they simulate vertical water flow and horizontal runoff separately for example vic assumes that water can only enter a grid cell via the atmosphere i e no horizontal inflow or outflow interactions among grid cells during the first stage for vertical water balance simulation in the second stage for horizontal runoff simulation it simply uses dem based river network to connect individual grid cells in order to determine the streamflow of watershed outlet liang et al 1994 such an assumption apparently does not hold for urban flooding which usually occurs quickly and is characterized with frequent inflow or outflow interactions among grid cells besides the finest resolution of vic is 1 km which is too coarse to capture the rapid generation of surface runoff in urban areas during heavy rainfall 3 during flooding events surface runoffs or river flows can be reversed in some relatively flat urban areas because of the uneven increases to surface water depths at local scales that means the direction of surface runoff in each grid cell can be constantly changing while the flood is quickly developing due to heavy rainfall however all conventional hydrological models use dem to determine flow direction before initiating a model simulation callow et al 2007 wise 2007 the determined flow directions remain unchanged during the entire model simulation apparently none of the latest hydrological models is capable of simulating the reverse flow phenomenon in many urban areas given the increasing attention to building climate resilient urban infrastructures costa et al 2016 prasad et al 2008 it is in urgent need to develop new methods or approaches in support of accurate prediction of urban flooding under heavy precipitation a large number of computer models for flood simulation have been proposed in recent years gires et al 2015 kauffeldt et al 2016 praskievicz and chang 2009 salvadore et al 2015 sood and smakhtin 2015 teng et al 2017 but these models are always based on various assumptions which make them only suitable for simulating some specific types of flood in general floods can be categorized into four types based on their location of occurrence and causes including river flood coastal flood flash flood and urban flood ali 2018 river flood occurs when a river channel is filled with too much water that is more than its handling capacity thus the surplus water overflows the river banks and runs into the adjacent low lying lands coastal flood occurs when tropical storms earthquake or volcanic activities drive an unusually high amount of ocean water towards the inland flash flood occurs when a large amount of water is discharged within short period of time e g a sudden release of water from a dam and urban flood is mainly caused by rapid runoff over the large scale impermeable surfaces under heavy rainfall conventional hydrological models e g vic swat and topmodel are designed to simulate river streamflow which might be used as inputs for river flood simulation but they cannot be applied directly for flood simulation because they are incapable of estimating flood extent and depth current flood models such as hec ras loi et al 2018 tuflow goodall et al 2017 csiro tvd teng et al 2015 autorapid follum et al 2017 and hand zheng et al 2018 are essentially flood inundation models which assume that a large amount of water will first enter into the river channel and lead to a rapid increase in water level then the adjacent low lying areas will be flooded only after the river bank is overtopped shown in fig 1 this assumption usually holds for river coastal and flash floods because a large amount of water must be first transported to the to be flooded regions through river channels before any floods occur however it does not hold for urban floods which are typically caused by heavy rain storms and have been commonly observed in many cities in recent years particularly when a large amount of water is dumped down to an urban area during a heavy precipitation event surface runoff will be generated immediately due to the almost zero infiltration of impermeable surfaces any locally low lying areas i e pits will be easily flooded because of the quick accumulation of surface runoff furthermore the accumulated surface runoff can also flow into the river channels and raise their water levels shown in fig 1 apparently these dynamics cannot be represented by flood inundation models although many previous studies have applied flood inundation models for urban flooding simulation teng et al 2017 it is not surprising to see that their real world applications are always focused on cities being passed through by a river in general flood inundation models first estimate the total water volume using the information of rainfall depth and total area of the study region all water will then be assumed to be dumped down into the river channel leading to a rapid increase in its water level thus low lying regions close to the river bank will always be flooded first while other low lying regions located far enough from the river will be inundated later or might be not inundated at all if they are blocked by some high elevated lands but this is clearly not true in an urban setting where a large amount of water can be poured down to every corner of a city during a rain storm for example an empty swimming pool at the top of a mountain will also be filled with water at the same time when the river s water level is quickly rising during heavy rainfall however flood inundation models will result in no water in the swimming pool as long as the elevation of the mountain is higher than the water elevation in the flooded river hence flood inundation models are not suitable for urban flood simulations under heavy precipitation because of their inappropriateness in reflecting the spatial and temporal dynamics of urban flooding process therefore in this study we will propose a new model for urban flooding prediction under extreme precipitation events particularly we focus on heavy rain as it is the main cause for urban flooding according to the glossary of meteorological terms by american meteorological society see http glossary ametsoc org wiki heavy rain heavy rain is defined as rain with a rate of accumulation exceeding a specific value that is geographically dependent however this definition is somehow too general to be used as a quantitative criterion there are many other quantitative criteria used to define heavy rain for example the meteorological service of new zealand defines heavy rain as 50 mm or more in a 6 hour period or 100 mm or more in a 24 hour period see https www metservice com warnings weather warning criteria chart in general heavy rain can be regarded as a large amount of rainfall during a relative short period of time in order to reflect the rapid development of urban floods under heavy rainfall the new model divides the study domain into many grid cells without any limitation on the spatial resolution of the grid cell as long as the dem data of the same resolution are available it can simulate both horizontal surface runoff and vertical water flow simultaneously in each time step of model simulation this makes the model capable of reflecting the frequent inflow or outflow interactions among grid cells and thus capturing the rapid generation of surface runoff in urban areas during heavy rainfall the model can also account for typical characteristics of urban areas such as large scale impermeable surfaces and urban drainage systems more importantly the model uses both the surface elevation and instantaneous surface water depth to dynamically determine the directions of horizontal inflow and outflow for each grid cell during each time step of model simulation this enables the model to capture the reverse flow phenomenon which is commonly seen in flat urban areas during heavy storms the proposed model will then be used to reproduce the 2016 flood in lafayette parish louisiana to demonstrate its effectiveness in predicting both flood extent and depth during real world flooding events 2 model development during a heavy precipitation event water often accumulates quickly in urban areas and becomes surface runoff the areas in the downstream direction of surface runoff are likely to be flooded therefore it is important to determine the direction of surface runoff before predicting which areas are expected to be flooded 2 1 direction of surface runoff in order to simulate the complexity of earth s surface we first convert the continuous surface to many square grid cells the size of the grid cell i e the length of one side of the cell denoted as r indicates the spatial resolution of our model which can range from a few meters to a few kilometers depending on the geographical extent of the heavy precipitation event of interest for a given time t the instantaneous height of a grid cell denoted as p is defined as the sum of its elevation l and surface water depth h as illustrated in fig 2 in the case of no surface water the instantaneous height of a grid cell is equal to its elevation and no surface runoff or outflow from this grid cell is generated if a grid cell is covered by surface water due to a heavy rainfall or the inflow from its immediate neighbors surface runoff is likely to be generated in this grid cell and the outflow direction is determined by the gradient of instantaneous height as described in the following as illustrated in fig 3 a given grid cell labeled as 0 can have at most one outflow direction while its inflows may come from at most all of its immediate neighbors for example if its instantaneous height is the lowest among all its immediate neighbors there will be no outflow but water in its immediate neighbors might all flow into this grid cell otherwise there must be at least one immediate neighbor with its instantaneous height lower than that of the focal grid cell the surface water in the grid cell will thus flow into the neighbor with the lowest instantaneous height for simplicity here we use numbers from 1 to 8 to label the outflow directions of all grid cells within the domain e g an outflow direction of 1 indicates that the water flows into the east neighbor while an outflow direction of 4 means that the water flows toward the southwest neighbor as shown in the 3d view of the gridded domain in fig 3 among all immediate neighbors of grid cell 0 grid cell 2 has the lowest instantaneous height thus we can use 2 to represent the outflow direction of grid cell 0 meanwhile grid cell 0 is expected to receive water from three of its immediate neighbors i e 6 7 and 8 thus its inflow directions should consist of 6 7 and 8 for a grid cell located in the edge or corner of the domain its outflow direction can be any one from 1 to 8 however if the outflow direction faces toward one of out domain neighbors the outflow from this grid cell is completely or partially regarded as a discharge of the domain to its surrounding area for example if the outflow direction of the edge grid cell shown in fig 3 is 5 the outflow will completely contribute to the domain s discharge however if the outflow direction is 6 a small portion of the outflow of this grid cell might enter into grid cell 7 while the majority of the outflow will contribute to the domain s discharge in contrast the inflows to an edge or corner grid cell are assumed to be only originated from its immediate neighbors within the domain although the out domain neighbors sometimes can also contribute the inflows to the grid cell e g in some mountainous regions as shown in fig 4 their contributions cannot be explicitly accounted for as their instantaneous heights are unknown instead we may estimate the inflows from these out domain neighbors based on the local hydrograph or the latest observations meanwhile we can adjust the original domain size to avoid placing the boundary of the domain on some uphill areas as illustrated in fig 4 with the purpose of minimizing the effects of inaccurate edge inflows on the water mass balance within the domain 2 2 velocity of surface runoff we use the momentum equation of saint venant equations to estimate the outflow velocity of a grid cell once its outflow direction is determined the saint venant equations are originally developed to calculate the momentum budget in a river channel chalfen and niemiec 1986 garcia and kahawita 1986 strelkoff 1970 as follows 1 u t u u x g h x g s c d u 2 h where u is the water velocity unit m s and it varies with time and location the temporal and spatial variations of u are represented by u t and u x respectively h unit m is the water depth at deepest point for a slice of river channel defined by dx unit m and it varies with location represented by h x g is the gravity acceleration with a rate of 9 8 m s2 s sin Î¸ represents the bottom slope and Î¸ unit is the bottom slope angle for a slice of river channel defined by dx and cd is a drag coefficient in the river channel and varies with the shape and roughness of the channel bed when applying the above equation to calculate the velocity of surface runoff within the gridded domain a number of approximations should be made as follows a given a small grid size denoted as r unit m and a short time step it is reasonable to assume that the surface water flow within the grid cell and to its neighbor grid cells is steady and uniform specifically steady means no temporal variation in water velocity i e u t 0 while uniform means no spatial variation in water velocity i e u x 0 therefore the left hand side of eq 1 becomes zero and we can rewrite it as 2 g s h x c d u 2 h as shown in fig 5 the elevations of the upstream and downstream cells are denoted as l 1 and l 2 and their surface water depths are represented by h 1 and h 2 respectively for convenience we classify the outflow directions into two cases parallel directions 1 3 5 and 7 and diagonal directions 2 4 6 and 8 for parallel outflow directions see case 1 in fig 5 the bottom slope s in eq 2 can be estimated by 3 s l 1 l 2 l 1 l 2 2 r 2 and the change in water depth along the outflow direction can be calculated by 4 h x h 2 h 1 r for diagonal outflow directions see case 2 in fig 5 the bottom slope s in eq 2 can be estimated by 5 s l 1 l 2 l 1 l 2 2 2 r 2 and the change in water depth along the outflow direction can be calculated by 6 h x h 2 h 1 2 r b as shown in eq 2 h is defined as the water depth at deepest point for a slice of river channel defined by dx here we discretize the continuous surface into small grid cells and we assume that the water above one grid cell can only flow into one of its eight neighbors the slice of river channel defined by dx is thus always within the current grid cell and its downstream neighbor therefore the deepest water depth over dx should be the maximum between h 1 and h 2 and can be expressed as 7 h m a x h 1 h 2 c as the shape of the surface water tank above a grid cell is assumed to be fixed while it moves toward the downstream grid cell by a small distance dx during a short period dt it is reasonable to assume that the drag coefficient cd in eq 2 is dominated by the roughness of the grid cell which varies with different land cover use types for example a grid cell covered by trees and shrubs often has a higher drag coefficient than a grid cell used for urban development in general the drag coefficient for grid cells covered by vegetation should be adjusted according to the area density and type of vegetation the effects of these factors on the drag coefficient have been widely investigated in the literature arcement and schneider 1989 cushman roisin and beckers 2011 fischenich and dudley 1999 hui et al 2010 wohl 1998 the value of cd usually varies between 0 003 and 0 2 based on the recommendations in the literature arcement and schneider 1989 hui et al 2010 a predefined value of cd is assigned to each category of land cover use here we refer to the national land cover classification system issued by the united states geological survey usgs website https www mrlc gov nlcd11 leg php to classify land cover use into 20 categories the values of cd for these categories are listed in table 1 based on the above approximations we can rewrite eq 2 to derive the outflow velocity u as follows 8 u g s h x h c d once the outflow direction is determined all the variables in the right hand side of eq 8 can be explicitly calculated according to eqs 3 7 in detail if the outflow direction is labeled as 1 3 5 or 7 the outflow velocity can be estimated by 9 u g l 1 l 2 l 1 l 2 2 r 2 h 2 h 1 r m a x h 1 h 2 c d if the outflow direction is labeled as 2 4 6 or 8 the outflow velocity can be estimated by 10 u g l 1 l 2 l 1 l 2 2 2 r 2 h 2 h 1 2 r m a x h 1 h 2 c d here the instantaneous height for the upstream grid cell is p 1 l 1 h 1 and that for the downstream grid cell is p 2 l 2 h 2 respectively since the surface water always flows from the upstream grid cell to the downstream grid we must have the following relationship 11 p 1 l 1 h 1 p 2 l 2 h 2 while the elevation of each grid cell is usually constant during a short period of dt the surface water depth can vary significantly with several factors e g inflows from its neighbors outflow to the lowest neighbor infiltration to the underlying soil and evapotranspiration to the atmosphere thus the relationship between h 1 and h 2 can also change over time this means that the value of m a x h 1 h 2 in eqs 9 and 10 should be determined separately for each time step to illustrate how to determine the value of m a x h 1 h 2 we consider three cases for the relationship between h 1 and h 2 as illustrated in fig 6 since determining the value for m a x h 1 h 2 is not affected by the outflow direction we will only consider parallel outflow directions i e 1 3 5 and 7 for illustration purposes i e eq 9 should be used note that the determination rules for diagonal outflow directions of 2 4 6 and 8 i e eq 10 should be used are exactly the same case 1 h 1 h 2 we can choose either h 1 or h 2 to represent m a x h 1 h 2 in order to meet the requirement defined by eq 11 we must have l 1 l 2 meanwhile h x in eq 2 will be 0 thus eq 9 can be rewritten as 12 u g l 1 l 2 l 1 l 2 2 r 2 h 1 c d in this case the outflow velocity of the upstream grid cell will be solely affected by the elevation gradient case 2 h 1 h 2 we should choose h 2 for m a x h 1 h 2 in order to meet the requirement defined by eq 11 we must have l 1 l 2 equation 9 can be rewritten as 13 u g l 1 l 2 l 1 l 2 2 r 2 h 2 h 1 r h 2 c d apparently h 2 h 1 r in eq 13 is positive indicating that the deeper water depth in the downstream grid cell tends to restrain the water coming from the upstream grid cell furthermore this may imply that the downstream grid cell is receiving more water from its other neighbors leading to a faster increase in its surface water depth than the upstream grid cell after a certain time the instantaneous height of the downstream grid cell may become higher than that of the upstream grid cell i e p 1 p 2 the flow direction can thus be reversed even though the elevation gradient still exists case 3 h 1 h 2 we should choose h 1 for m a x h 1 h 2 in this case the relationship between l 1 and l 2 can be arbitrary refer to the three scenarios under case 3 in fig 5 as long as the instantaneous heights of two grid cells meet the requirement defined by eq 11 eq 9 can be written as 14 u g l 1 l 2 l 1 l 2 2 r 2 h 2 h 1 r h 1 c d note that h 2 h 1 r in eq 14 is negative in this case as subtracting a negative value is equivalent to adding a positive value this indicates that the higher water depth in the upstream grid cell tends to speed up its outflow particularly the 3rd scenario in this case i e l 1 l 2 see fig 6 may be regarded as the next stage result from case 2 while the outflow direction is reversed due to the greater and faster inflows into the initial downstream grid cell in other words the initial upstream grid cell with a higher elevation can sometimes be turned into a downstream grid cell more importantly this implies that the flow direction of surface runoff can sometimes be reversed due to the uneven increases to the surface water depth at local scales 2 3 volume of surface runoff the depth of the surface runoff is determined by the gradient of instantaneous height between the upstream grid cell and the downstream one i e p 1 p 2 here we assume that the surface runoff above the upstream grid cell is shaped as a square water tank and the shape of this water tank will not change over a very short period of dt in detail the floorage of this water tank is the same as the area of the grid cell and its initial height is equal to the gradient of instantaneous height i e p 1 p 2 apparently the height of the outflow water tank of the upstream grid cell should be non negative and must be not greater than its initial surface water depth h 1 once the depth of the outflow water tank is determined the outflow volume during dt is proportional to the moving out floorage of the water tank from the grid cell which is further determined by outflow direction and velocity as the shape of the water tank remains unchanged here we should note that for parallel outflow directions the water from the upstream grid cell will only flow into the downstream one however for diagonal outflow directions the upstream water may also partially flow into the other two side neighbors as illustrated in case 2 of fig 7 as long as the instantaneous heights of these two neighbors are lower than that of the upstream grid cell the moving out floorage of the water tank during dt is denoted as da and its calculation varies with the outflow direction specifically the outflow velocity u should be first calculated according to eqs 9 or 10 once the outflow direction is determined next the moving out floorage of the water tank can be calculated as follows see also fig 7 case 1 parallel outflow directions 1 3 5 and 7 the water tank will only move into the downstream grid cell and the moving out floorage can be calculated as 15 da r u d t case 2 diagonal outflow directions 2 4 6 and 8 the water tank may also need to pass through the other two side neighbors when moving downward to the downstream grid cell the expected floorage moving into each side neighbor can be calculated as 16 1 2 r u d t 1 2 u d t 2 and the expected floorage moving into the downstream grid cell is computed as 17 1 2 u d t 2 thus the total moving out floorage of the water tank in this case should be the sum of moving in floorage of the downstream grid cell and the other two side neighbors it can be calculated as 18 da 2 r u d t 1 2 u d t 2 after the moving out floorage of the water tank da is computed with eqs 15 or 18 the outflow volume unit m3 s of the upstream grid cell during dt can be calculated as 19 v dt d a p 1 p 2 thus the remaining volume of the water tank in the upstream grid cell is calculated as 20 v dt r 2 d a p 1 p 2 here we should note that eqs 15 20 hold only if some surface water is present in the upstream grid cell i e h 1 0 otherwise there will be no surface runoff i e v dt 0 and v dt 0 as shown in fig 3 the upstream grid cell of interest might also serve as a downstream grid cell to one or more of its immediate neighbors because it is allowed to receive inflows from multiple grid cells the outflows from its upstream neighbors during a short time of dt can also be calculated separately by eqs 15 17 as all these outflows from the upstream neighbors will become the inflow to the grid cell of interest we can sum them up to compute its total inflow during dt denoted as v dt unit m3 s note that the outflows from its upstream grid cells to their two side neighbors in case 2 of fig 7 should be determined by their instantaneous heights in a case by case fashion for example if the instantaneous heights of both side neighbors are lower than that of the upstream grid cell then they both will receive the same portion of the outflow as expressed by eq 16 if their instantaneous heights are both greater than that of the upstream grid cell then they will receive no water from the upstream grid cell i e the expected outflows to these side neighbors will be added to the outflow to the downstream grid cell if one neighbor is higher than the upstream grid cell and the other is lower then the lower neighbor will receive its portion of the outflow as expressed by eq 16 and the expected portion to the higher neighbor will be added to the outflow to the downstream grid cell 2 4 mass balance of surface water in order to calculate the water mass balance within a grid cell we usually need to consider the entire column of the grid cell as illustrated in case 1 of fig 8 however here our focus is on the surface water during flooding season which is typically generated during a short time period because of heavy precipitation as the surface water must pass through the unsaturated soil to recharge groundwater and this process usually takes a longer time period it is reasonable to assume that the interaction between surface water and groundwater is negligible during a heavy precipitation event perhaps the only exception for the above assumption is rivers or lakes where groundwater recharges surface water by providing the base flow in rivers or the base water level in lakes however this recharge process is too slow to contribute to the commonly seen floods which usually develop in a short time period thus in the following analyses we will only consider the column above the normal aquifer within a grid cell to calculate the mass balance of surface water see case 2 of fig 8 as for the unsaturated soil above the aquifer we assume that water within this layer only flows downward i e there is no horizontal water flow furthermore the water loss due to evapotranspiration during a flooding event is often very small compared to the large amount of precipitation it thus can be assumed to be negligible in the calculation of surface water mass balance during a heavy precipitation event therefore the water mass balance equation in case 2 of fig 8 can be further simplified as 21 water m a s s g a i n p r e c i p i t a t i o n s u r f a c e i n f l o w s u r f a c e o u t f l o w i n f i l t r a t i o n the above equation indicates that the surface water mass change in a grid cell is expressed as the tradeoff between the total mass inputs contributed by precipitation and surface inflow and the total mass outputs caused by surface outflow and infiltration here the mass of surface inflow and outflow can be directly calculated by multiplying the total volumes of surface inflow and outflow i e v dt and v dt by the average water density during dt the total water mass from precipitation during dt can be estimated with the observations from weather stations and radars or the simulations from weather prediction models the calculation of infiltration should account for various soil textures and land cover use and is described in detail as follows for simplicity we assume that all water inputs and outputs have the same water density thus the calculation of water mass balance is equivalent to the calculation of water volume balance for a given time t the infiltration rate of a given grid cell is denoted as k varying from 0 to 100 the filtration rate here means how much of the total surface water above this grid cell will be infiltrated into the soil apparently the infiltration rate is significantly influenced by the initial soil moisture content denoted as m ranging from 0 to 100 the effects of initial soil moisture content on infiltration rate have been studied a long time ago horton 1933 houser 2005 morin and benyamini 1977 philip 1957 stÃ¤hli et al 1999 according to gray and norum 1967 the infiltration rate decreases exponentially as the initial soil moisture content increases here we use an exponential function to represent their relationship as follows 22 k e Î² m where Î² is a damping coefficient describing the decreasing rate of infiltration due to the increase in soil moisture and its values usually vary between 5 and 10 in general if the surface soil is completely saturated i e the initial moisture content reaches its maximum m 100 then no surface water will be infiltrated i e the infiltration rate decreases to zero k 0 it is worth to mention that the theoretical infiltration rate in this case might not decrease to exactly zero this is because the hydraulic conductivity of saturated soil is not completely zero see table 2 however since the typical values of saturated hydraulic conductivity are ranging between 1 03 10 6 and 1 76 10 4 m s clapp and hornberger 1978 implying that the movement of water in saturated soil is too slow to release sufficient space for the surface water to be infiltrated during a short time period this is especially true when the internal time step of our model integration is very small typically in the order of a few seconds therefore it is reasonable to treat the infiltration rate as zero in practice to meet this requirement the minimum value of Î² should be used in eq 22 in detail if we put m 100 and Î² 5 into the right hand side of eq 22 the calculated value of k is 0 67 which can be reasonably regarded as 0 however if we further decrease the value of Î² the value of k becomes too large to be deemed to be zero therefore the minimum value of Î² should be 5 we also should note that for some types of soil with very small particles e g silty clay although they are not fully saturated i e m 100 the infiltration rate can still be as low as zero in this case we may increase the value of Î² to damp the infiltration particularly if we increase Î² to 10 even though the surface soil is only half saturated i e m 50 the infiltration rate will drop to 0 67 which can be deemed to be 0 this is reasonable enough to represent the damped infiltration of silty clay soil which has the smallest particles among all types of soil therefore the maximum value of Î² is assigned 10 the exponential relationships between infiltration rate and initial soil moisture content with the minimum and maximum damping coefficients are shown in fig 9 in general the damping coefficient Î² can be determined by soil texture smaller values of Î² should be used for coarse textured soils e g sand to reflect high infiltration while bigger values should be selected for fine textured soils e g clay to indicate low infiltration in order to find an appropriate value of the damping coefficient from 5 to 10 for each type of soil texture we need to determine the saturated hydraulic conductivity denoted as ks unit m s within the soil as it also correlates with soil texture in general high hydraulic conductivity means that water can move quickly within the soil leading to more space in the soil to allow more surface water to be infiltrated during a certain time period this corresponds to small damping effects on infiltration and smaller values of Î² should be chosen to reflect reduced damping effects similarly high values of Î² should be selected to indicate increased damping effects for low hydraulic conductivity in other words the damping coefficient Î² is inversely proportional to the saturated hydraulic conductivity ks here we consider 12 classes of soil texture as defined in the united states department of agriculture soil texture triangle usda 2017 clapp and hornberger 1978 have identified the key soil hydraulic properties including saturated hydraulic conductivity and porosity based on 1 845 soil samples which are listed in table 2 it has been found that sand has the highest saturated hydraulic conductivity i e 1 76 10 4 m s while silty clay has the lowest i e 1 03 10 6 m s accordingly sand should have the smallest damping coefficient i e Î² 5 and silty clay should have the biggest damping coefficient i e Î² 10 we assume that the damping coefficient and the saturated hydraulic conductivity are linearly and inversely correlated the damping coefficients for other soil textures can be calculated as follows 23 Î² 10 k s 1 03 10 6 1 76 10 4 1 03 10 6 10 5 the calculated damping coefficients for 12 soil texture classes are also listed in table 2 in addition to the damping coefficient Î² the initial soil moisture content m is also required to estimate the infiltration rate according to eq 22 in general the initial soil moisture content can be directly measured through field trips or estimated with historical observations thus the initial infiltration rate can be calculated with eq 22 after an appropriate value of Î² is selected from table 2 generally speaking infiltration is a relatively slow process compared to the rapid surface runoff caused by intense rainfall during a flooding event thus we can assume that the infiltrated water from t to t dt only comes from the remaining surface water at time t dt in other words infiltration only takes place instantaneously at time t dt thus the calculation of the infiltrated water can be broken down into two steps 1 calculate the volume of remaining surface water at time t dt without considering infiltration 2 compute the infiltration rate at time t with eq 22 and apply it to the remaining water obtained in the first step to estimate the total volume of infiltrated water from t and t dt specifically the remaining surface water without consideration of infiltration in the upstream grid cell at time t d t denoted as v t d t must be a non negative value i e 0 and can be expressed as 24 v t d t h 1 r 2 p r 2 d t v dt v dt where p is the precipitation rate above the grid cell at time t unit m s v dt and v dt represent the volumes of inflow and outflow of the grid cell from t to t d t respectively here we should note that using precipitation rate as the input allows us to simulate the rapid evolution of urban floods during heavy precipitation particularly the computational time step i e dt of the proposed model is usually in the order of seconds and is a function of grid cell size and water velocity defined by the well known courant friedrichs lewy cfl condition courant et al 1967 in other words in order to ensure model convergence we need to make sure that a particle of water entering a grid cell would not travel through it until a computation step within the model has been completed for example if the grid size is 100 m and the calculated maximum water velocity with the saint venant equations is 5 m s then the time for the water to pass through one grid cell would be 100 5 20 s therefore we need to choose a time step less than 20 s to ensure the water did not leave the grid cell too soon once the remaining surface water without consideration of infiltration is calculated with eq 24 we can proceed to calculate the infiltrated water volume at time t dt assume that the soil moisture content at time t is denoted as m t the infiltration rate k can be calculated with eq 22 thus the total volume of water expected to be infiltrated into the soil for the given grid cell denoted as v dt can be calculated as 25 v dt k v t d t meanwhile we should note that the actual volume of infiltrated water for the grid cell is limited by the maximum volume of water that the soil can hold while it is completely saturated denoted as v max which is further determined by the depth of unsaturated surface soil i e depth of vadose zone or groundwater level denoted as q unit m and its porosity denoted as n unit specifically v max is calculated by 26 v max r 2 q n where the value of q is usually measured through field trips or estimated with historical observations while the value of n is determined by soil texture see table 2 thus the maximum volume of water that the soil in the grid cell can still infiltrate at time t can be calculated as v max 1 m t here we will consider the following two cases to calculate the actual infiltrated water volume during dt and to update the soil moisture content at time t d t case 1 v dt v max 1 m t the grid cell can only infiltrate a portion of the v dt as the soil will be saturated thus the total volume of surface water left in the grid cell at time t d t is represented by 27 v t d t v t d t v max 1 m t meanwhile the soil moisture content at time t d t is updated as follows 28 m t d t 100 case 2 v dt v max 1 m t all of the surface water to be infiltrated will be ultimately infiltrated into this grid cell thus the total volume of surface water remaining in the grid cell at time t d t can be expressed as 29 v t d t v t d t 1 k meanwhile the soil moisture content at time t d t is updated as follows 30 m t d t m t v dt v max 100 2 5 consideration of urban area in general the calculation of the expected water to be infiltrated as specified by eq 25 can be applied to all natural surfaces however it is not applicable for urban regions which are commonly covered by impermeable materials in this case surface runoff might be generated immediately after rainfall reaches the impermeable surfaces urban drainage or stormwater systems are therefore designed to collect the surface runoff through street drains and discharge it into a detention basin or the nearest receiving water body to minimize the possibility of flooding butler and davies 2003 schmitt et al 2004 yazdanfar and sharma 2015 the maximum capability of an urban drainage system in conveying surface runoff is designed according to the historical rainfall intensity durationfrequency idf curves derived from precipitation observations at weather stations idf curves describe all characteristics of extreme precipitation events with different durations and return periods at a given location and are widely used for guiding the hydraulic design of urban drainage systems and other infrastructures cheng and aghakouchak 2014 sarhadi and soulis 2017 wang et al 2014 here we should note that some urban drainage models allow the consideration of detailed hydraulic structures of an urban stormwater system such as pipe network combined sewer and drop shafts a commonly used model for this is the storm water management model swmm gironÃ¡s et al 2010 however the underground stormwater pipe network in an urban setting is considerably complicated especially when the study domain is large the data for such a complex network are typically not available to the public this makes the application of swmm model practically difficult for many public users more importantly swmm is usually used to simulate how much water can be held by the stormwater pipe network under various engineering design scenarios there are a number of similar models based on different implementations of the saint venant equations i e 1d 2d or their combinations to deal with the interactions between drainage flow and surface flow barnard et al 2007 leandro et al 2009 2011 however we should note that these models are intentionally designed for simulating the water distribution in the stormwater system given that the receiving water volume from surface runoff is within its handling capacity but once the capacity is exceeded these models might lose their effectiveness as the entire pipe network is filled with water this is often the case for urban floods under heavy rain storms where people are more concerned with the regions likely to be flooded and their expected water depths to tackle these challenges our model will only consider the capacity of an urban stormwater system rather than its detailed pipe network specifically if the total runoff per unit time generated over urban surface does not exceed the capacity of the stormwater system no flood is expected otherwise a portion of the runoff which cannot be handled by the stormwater system will be accumulated somewhere above the surface resulting in floods the capacity of an urban stormwater system can be easily estimated with the rainfall idf curves used for designing the system idf curves are readily available through public sources and easier to obtain than the pipe network of urban stormwater system this overcomes the challenge in data availability for swmm model most importantly our model emphasizes the situation of heavy precipitation where stormwater systems are often overloaded and urban floods are very likely to be expected it should be noted that our assumption of using idf curves to determine the capacity of stormwater system in urban areas is based on the condition that the stormwater system is fully functional in other words if the stormwater system is not fully functional using idf curves might underestimate the flood extent or depth in this case some adjustments to the idf estimated capacity of stormwater system should be performed accordingly through a full consideration of the effects of land cover use soil type street drain and open ditch on the horizontal movement and vertical infiltration of surface runoff our model is able to simulate both flood extent and depth effectively in order to calculate the mass balance of surface water in an urban area we simply classify urban grid cells covered by impermeable surfaces into two categories a with a street drain and b without a street drain note that some grid cells in an urban area might be not fully covered by impervious surfaces such as parks and zoos where the infiltration rate is still high for urban grid cells without a street drain we use the percent coverage of impervious surface in each grid cell as indicated by its land cover class in table 1 as a criterion to make adjustments to its infiltration rate see fig 10 specifically we use an infiltration adjustment coefficient denoted as Î± ranging from 0 to 100 to indicate how to make an adjustment to the original infiltration rate k derived according to the soil texture class without considering the impervious surface above the soil the adjusted infiltration rate denoted as k can be calculated as 31 k k Î± if a grid cell is completely covered by impervious materials the value of Î± should be 0 indicating that the infiltration rate will decrease to zero after the adjustment if there is no impervious coverage for a grid cell then the value of Î± should be 100 indicating that the infiltration rate is solely determined by the soil texture this is usually the case for many natural surfaces such as barren forest shrubland herbaceous and planted cultivated regions however we should note that the soils beneath other surfaces e g water and wetlands are always saturated thus the infiltration rates for these two types of land cover should be 0 even though various soil textures may be reported here we apply Î± 0 to lands covered by water and wetlands to indicate that no infiltration occurs for a grid cell partially covered by impervious materials we apply four different values of Î± i e 0 20 50 and 80 corresponding to the four urban land cover classes refer to table 1 for example if the land cover of a grid cell is classified as low intensity develop areas with 20 to 49 impervious coverage the value of Î± should be 50 indicating that the original infiltration rate k will be halved according to eq 25 the total volume of water expected to be infiltrated v dt should be recalculated with the updated infiltration rate k as follows 32 v dt k v t d t similarly eq 29 should be replaced by 33 v t d t v t d t 1 k based on the above adjustments the remaining surface water for urban grid cells can be calculated according to the following two cases case 1 an urban grid cell without a street drain the total volume of surface water remaining in the grid cell at time t d t can be calculated using either eqs 27 or 33 depending on whether the soil will be saturated or not note that the adjusted infiltration rate should be used in this case case 2 an urban grid cell with a street drain all the surface water is supposed to flow into the underground drainage system through the street drain unless the intensity of incoming surface water exceeds the designed capacity of this inlet shown in fig 9 assume that the maximum capacity of a street drain in terms of collecting incoming surface water during a short time period of dt is measured by i unit m s it can be easily derived from the idf curves used to design the drain inlet once the expected duration of the precipitation event is determined thus the total surface water volume that can be accommodated by the drainage system during dt denoted as v d is expressed as 34 v d i d t here the total volume of remaining surface water without consideration of infiltration at t d t is still denoted as v t d t and calculated by eq 24 apparently if v t d t v d all the surface water is supposed to flow into the underground drainage system thus we have v t d t 0 otherwise the total volume of remaining surface water after considering the discharge into drainage system can be calculated by 35 v t d t v t d t v d once the volume of remaining surface water after a short time period of dt i e v t d t is calculated with eqs 27 29 33 or 35 the surface water depth or flood depth above a given grid cell at time t d t can be directly calculated as follows 36 h t d t v t d t r 2 based on eq 36 the flood depth above all grid cells within a given study area during and after the heavy precipitation event can be simulated 3 application 3 1 study area here we will apply the proposed model to simulate the 2016 flood in louisiana united states with the purpose of demonstrating its performance in simulating real world flooding events in august 2016 a severe weather system brought prolonged rainfall in about 72 h to the southern parts of louisiana resulting in widespread and catastrophic flooding the flood damaged more than 140 000 houses required evacuation of more than 20 000 people and led to at least 13 deaths noaa 2016 vahedifard et al 2016 watson et al 2017 monetary losses resulting from various damages to homes businesses and infrastructure were estimated to be 10 billion watson et al 2017 although this disaster was not caused by a named tropical storm it has been regarded as an historic and unprecedented flood event in louisiana and one of the worst natural disasters in united states as one of the severely affected areas in south louisiana during this flooding event lafayette parish is bisected by the vermilion river which has historically been heavily flooded due to the reverse flow phenomenon kinsland 1998 kinsland and wildgen 2006 see also fig 11 during heavy rainfall events parts of the vermilion river in the lafayette area can experience negative discharge since the rise in water level in downstream reaches exceeds the water level in upstream reaches leading to an inversion in the flow direction watson et al 2017 this reverse flow phenomenon is mainly caused by the large area of urbanization and has been well confirmed by the 2016 flood profiles as reported by the united states geological survey watson et al 2017 apparently this phenomenon cannot be handled by conventional hydrological or flooding simulation models as they are all based on an assumption of no reverse flow by contrast the proposed model in this study can deal with the reverse in flow direction caused by the uneven increases to the surface water depths at local scales therefore we will specifically focus on lafayette parish in this application the total area of lafayette parish is 696 7 km2 the proposed model will be used to simulate the flood extent and depth of the 2016 flood in lafayette parish the simulated results will be compared to the observed flooding records to help validate the performance of the proposed model 3 2 data collection the proposed model requires the data for elevations land cover soil texture depth of vadose zone or groundwater level precipitation and initial soil moisture content as inputs to simulate flood extent and depth for the 2016 flood in lafayette parish here the elevation data for lafayette parish are collected from the louisiana statewide lidar project which provides a high resolution 5 m digital elevation model dem dataset for the entire state of louisiana cunningham et al 2004 the dataset is available for public access through the louisiana atlas gis platform website https maps ga lsu edu lidar2000 the digital elevation map for lafayette parish is shown in fig 11 land cover data for lafayette parish are obtained from the national land cover database 2011 nlcd 2011 which is the most recent national land cover product created by the multi resolution land characteristics mrlc consortium coulston et al 2013 homer et al 2015 the nlcd 2011 dataset has a spatial resolution of 30 m and is available for public access at https www mrlc gov nlcd2011 php soil texture data for lafayette parish are collected from the soil survey geographic database ssurgo provided by united states department of agriculture ssurgo 2018 the ssurgo provides high resolution county wide soil data and is available for public access at https websoilsurvey sc egov usda gov app websoilsurvey aspx the data for groundwater level of lafayette parish are derived from the national water information system nwis hosted by united states geological survey usgs nwis 2018 the nwis dataset contains extensive water data for the entire country of united states public access to many of these data is provided via the usgs water data web interface available at https waterdata usgs gov nwis the precipitation data for lafayette parish during the 2016 flood are collected from the national centers for environmental information ncei under the national oceanic and atmospheric administration of united states ncei 2018 particularly the gauged precipitation data at a weather station in the lafayette regional airport is derived from the ncei dataset at https www ncdc noaa gov cdo web the 2016 flood in louisiana occurred between august 12 to 22 as heavy rainfalls poured down during august 12 and 13 as recorded by the weather station in the lafayette regional airport the total precipitation received from august 12 to 13 was as high as 528 1 mm see fig 12 the data for soil moisture content are obtained from the national integrated drought information system nidis under the national oceanic and atmospheric administration of united states nidis 2004 the nidis dataset is available for public access at https www drought gov drought data gallery soil moisture here we should note that the model s spatial resolution is usually finer than that of the nidis soil moisture dataset the nidis dataset needs to be regridded to match the spatial resolution of our model runs besides the soil moisture values in nidis dataset are provided in the form of water depth unit mm and should be converted to percentage in order to provide initial soil moisture input for our model to do so we need to use the information for the depth of unsaturated surface soil i e depth of vadose zone or groundwater level and the soil porosity as mentioned above the data for groundwater level can be obtained from the nwis dataset while the soil porosity can be determined by soil type for example if the soil moisture of a given grid cell is estimated as 600 mm by the nidis dataset the depth of its groundwater level is 5 m or 5000 mm and its main soil type is silt i e its porosity is 0 493 respectively its soil moisture percent can be calculated as 600 mm 5000 mm 0 493 0 243 i e 24 3 in addition to the above required data it is important to provide information about location of street drains in order to allow the model to interact with the underground drainage systems in urban regions however according to the lafayette consolidated government lcg the drainage system in lafayette parish is mostly above 88 built with roadside open ditches and coulees and only a small portion around 12 of the urban areas are designed with subsurface drains which are also eventually connected to the nearest open ditches or coulees lcg 2018 this means that most of the surface runoffs during flooding seasons are likely to flow into the open ditches or coulees as the subsurface drains are only capable of holding surface water for small regions therefore the capability of these subsurface drains in terms of discharging surface water may be negligible as here we focus on the entire parish of lafayette rather than a small region thus we will not consider the street drains connecting to these subsurface drains in this application i e we assume that there are no street drains for all urban grid cells another reason that we do not consider the street drains is that there are no publicly available data for the locations of street drains in lafayette parish we also need to collect observational data for the 2016 flood in lafayette parish to validate the performance of the proposed model as the heavy rainfalls in lafayette parish occurred on august 12 and 13 2016 we will use the proposed model to simulate the flood extent and depth after august 13 we therefore should collect the observed data for flood extent and depth recorded on august 14 2016 due to the limited data availability for the 2016 flood in lafayette parish here we will estimate the flood depths for four flooded regions in lafayette parish based on some aerial videos recorded by private drones on august 13 and 14 see fig 13 for each aerial video we first need to find the border of a flooded region in the video as the water elevation of this region should be the same as the land elevation of any points on the border as the land elevation is already known see fig 11 the water elevation for this flooded region can be obtained then the difference between water elevation and land elevation for any locations within the flooded region can be regarded as the flood depth the areas with positive flood depth can thus be used to represent the flood extent the estimated water elevations for the four flooded regions in lafayette parish are shown in fig 13 for each labeled point in fig 13 we assume that the water elevation of its surrounding area within a radius of 1 km is equal to that of the labeled point as the water appears still in the aerial videos thus we can derive the flood extent and depth within the 1 km circular area of each labeled point in fig 13 and the results are shown in fig 14 note that the circular areas of west pinhook road and rotary point partially overlap the water elevation for the overlapped area is replaced by the average of the water elevations at these two points besides water depths less than 10 mm above ground are usually not regarded as flooded and thus not considered in this study for estimating flood extent it is also worthwhile to mention that the estimated flood extent and depth based on the above assumption might be unable to capture the floods of some regions where local pits exist for example an empty outdoor swimming pool at the top of a mountain can be filled with water during a heavy precipitation event however the above estimation method is unable to reflect this as the mountain s elevation is often higher than the water elevation of observed flooded regions it is thus very important to keep in mind that the flood maps displayed in fig 14 only represent the local flood extent surrounding the reported points in fig 13 in other words some non flooded areas with local pits might also be flooded in reality 3 3 results and discussions in order to test the performance of the proposed model in simulating the flood extent and depth of the 2016 flood in lafayette parish we run the model over the entire parish with four spatial resolutions 1000 m 500 m 100 m and 50 m the parameters related to land cover and soil texture in this case study are kept fixed as given in tables 1 and 2 note that the values for these parameters i e c d Î± and Î² can be adjusted accordingly as long as they are within the corresponding ranges the purpose here is to test whether the proposed model can reasonably reproduce the 2016 flood in lafayette parish with the default parameters while results with different spatial resolutions are used to help find a threshold of the spatial resolution at which the model can generate reasonable simulations according to the cfl condition the computational time steps for these four model resolutions i e 1000 m 500 m 100 m and 50 m are set as 60 s 60 s 30 s and 15 s respectively note that the time steps for the 1000 m and 500 m model runs could be larger as determined by the cfl condition but here we use 60 s to allow better representation of the quick evolution of urban floods under heavy precipitation in general higher spatial resolutions usually result in higher computational requirements thus it is necessary to find a tradeoff between high computation requirements and reliable simulation results this is especially important for real time flooding prediction and warning which requires as much lead time as possible for evacuation preparation the selection of model spatial resolution is usually based on the following factors 1 the ability in capturing the geographical characteristics of the study area 2 the available resolutions of the input data such as dem land cover use soil texture groundwater and soil moisture 3 the computational requirements as determined by the total number of grid cells and the modeling time step the simulations for the 2016 flood in lafayette parish with four spatial resolutions are shown in fig 15 it can be clearly seen that the spatial patterns of flooded regions with water depth higher than 1 m can be well captured when the model resolution increases from 1000 m to 100 m this is especially true for the vermilion river where the flood depths can reach 3 m or even higher however when the model resolution is further improved from 100 m to 50 m we do not see apparent improvements in terms of capturing flooding spatial patterns for the entire parish of lafayette however as the model resolution is changed from 100 m to 50 m the total number of grid cells within the domain will increase by four times thus the required computational time and storage will become at least fourfold this may suggest that 100 m is probably a reasonable spatial resolution for the proposed model to generate acceptable simulations recent studies suggest that model performance increases with decreasing spatial scales i e improved model resolutions ichiba et al 2018 however if we continue to improve the model resolution after a certain point no apparent improvement or even a possible decline in model performance will be expected this is mainly because most of the input data such as land cover and soil type are not available at higher resolutions ichiba et al 2018 in order to further investigate the effects of model spatial resolution we extract the simulated flood depths with four resolutions at the four flooded regions in lafayette parish and compare them to the observed flood depths see fig 16 note that the observed flood depths are estimated with the 5 m lidar based dem dataset thus the spatial resolution of observed flood depths can also be regarded as 5 m as shown in fig 16 the simulations with resolutions of 1000 m and 500 m are unable to capture the spatial variations of flood depths because of their coarse resolutions when the model resolution is refined to 100 m the spatial patterns of flooded areas in the observations are generally reflected in the simulations if the resolution is further increased from 100 m to 50 m the spatial variations of flood depths are represented with more details this is particularly true for some areas which contain local pits low areas and are not directly connected to the river the flood depths for these areas are not displayed in the observations due to the abovementioned limitations of our estimate methods however most of them are well represented by the 50 m simulations in general it is reasonable to believe that model simulations with higher resolutions e g 25 m 10 m or even 5 m should be more capable of reflecting the spatial variations of local flood depths as displayed in the observations however we should note that model simulations with finer resolutions will lead to substantially increased computational requirements meanwhile the performance in local flooding simulation may not be significantly improved it is thus important to seek a balance point between model performance and computational requirements in order to quantify the performance improvement of the 50 m model simulations relative to the 100 m simulations here we compare these two simulations from three aspects 1 the performance in capturing the observed flood extent 2 the ability of reproducing the observed flood depths and 3 the capability of simulating the reversed flow in vermilion river as for the simulated flood extent we only consider the grid cells with water depth greater than 10 mm the simulated flood extent from each resolution setting is first overlapped with the observed flood extent the overlapped area is regarded as the captured flood extent by the given model simulations the percentage of the capture flood extent to the observed flood extent is defined as the capture rate see fig 17 we can thus use the capture rate to determine which model i e 100 m or 50 m has a better performance in capturing the observed flood extent as shown in fig 16 the 100 m model simulations present relatively higher capture rates in all four flooded regions than the 50 m model simulations suggesting that the 100 m model simulations perform better in capturing the observed flood extent for example the capture rates of the 100 m model at three evaluated locations west pinhook road and rotary point kaliste saloom road and middleburg drive are 96 8 89 3 and 83 9 respectively in contrast the capture rates of the 50 m model at these three locations all decrease slightly i e 95 7 84 8 and 78 2 this is because the 100 m model is spatially constructed with bigger grid cells than the 50 m model and is thus likely to generate larger flood extent in general coarse resolutions enable the model to better capture the observed flood extent but degrade its ability of capturing the spatial variations of local flood extent therefore the above comparison only suggests that the 100 m simulations are comparable to the 50 m ones while the slightly improved capture rates in the 100 m simulations can be attributed to their larger grid cells due to the mismatch of spatial resolutions i e 100 m and 50 m simulations and 5 m observations the ability of reproducing the observed flood depths is evaluated through comparisons of the histograms and general statistics mean maximum and standard deviation of flood depths although we could use spatial interpolation methods to regrid the simulations or observations to match the resolution of observations or simulations in order to perform grid by grid comparisons this would inevitably introduce some errors or uncertainties associated with the interpolation methods therefore comparisons at individual grid cells are avoided here besides only those grid cells within the captured areas shown in fig 17 are considered to avoid introducing any errors caused by the abovementioned estimation method for observed flood depth comparisons of flood depth histograms between simulations and observations are shown in fig 18 while the comparisons of general statistics are listed in table 3 it appears that both the 100 m and 50 m simulations are likely to overestimate the flood depths observed in the regions near the main channel of the vermilion river i e west pinhook road and rotary point in contrast both model simulations are likely to underestimate the observed water depths in the regions near the tributaries of the vermilion river i e kaliste saloom road and middleburg drive this is probably because the lidar based dem used to drive our model was collected many years ago during 2000 and 2004 when we use this dem dataset to simulate the 2016 flood in lafayette parish we have to assume that the water levels of all water bodies just prior to the 2016 flood are exactly the same as those recorded by the earlier dem dataset this assumption is unlikely to hold in many locations as their water levels can vary significantly with time due to the combined effects of many factors such as baseflow from groundwater precipitation and evaporation therefore our model s underestimation of the observed flood depths in the main channel of vermilion river may indicate that the water level in the main stream of the vermilion river before the 2016 flood was higher than the water level recorded in the earlier dem dataset while the overestimation of observed flood depths in its tributaries suggests that their water levels before the 2016 flood are lower than the lidar recorded values however we should note that there might be other reasons leading to the modeling error in urban flood simulations such as the spatial mismatch of input data for dem and land cover the errors introduced by spatial interpolation and the mismatch between rainfall input resolution and model resolution ichiba et al 2018 particularly the point observations of rainfall at one weather station are used as input to drive our flood model implying an assumption that there are no spatial or temporal variations in rainfall over the entire study area throughout the two day event this is a limitation of the case study which might somehow contribute to the model bias in simulating the 2016 flood in lafayette parish in order to quantify the abovementioned systematic biases i e underestimation or overestimation we calculate a relative bias defined as a percentage of the difference between simulations and observations to the observed values see table 3 through the comparisons of three general statistics mean maximum and standard deviation between simulated and observed flood depths we notice that the systematic biases in simulating the mean values of flood depths are further enlarged after the spatial resolution is refined from 100 m to 50 m for example the 100 m model shows a positive bias 61 4 in simulating the average flood depth at the location near west pinhook road and rotary point this positive bias is slightly enlarged to 64 1 by the 50 m model in contrast the 100 m model presents negative biases at other two locations i e 10 4 near kaliste saloom road and 6 7 near middleburg drive these negative biases are further amplified to 15 8 and 37 1 by the 50 m model since the land and soil related parameters here are fixed as their default values this suggests that these parameters should be adjusted through effective calibration before the model resolution is further increased otherwise the model biases are likely to be further magnified as mentioned above these systematic biases are very likely caused by the inappropriateness of using the lidar based water levels to estimate the initial water levels before the heavy rainfall therefore more accurate estimations of the initial water levels in the vermilion river and its tributaries should be obtained and used to drive high resolution model simulations in order to minimize the amplification of systematic biases accurate observations or estimations can usually be obtained with unmanned aerial vehicles uavs or field measurement before heavy precipitation is approaching thus the accuracy of predicting the expected flood extend and depth can be improved significantly nevertheless we do notice some improvements after the model resolution is increased from 100 m to 50 m for example the 50 m model performs better in reproducing the standard deviations of observed flood depths in several regions in detail the relative bias for the standard deviation of flood depth at the location near west pinhook road and rotary point is improved from 8 8 to 2 4 while the location near kaliste saloom road presents an improvement from 45 6 to 32 2 this further confirms that model runs with higher spatial resolutions are more capable of capturing the spatial variations of local flood depth in general given that the model parameters are properly calibrated high resolution simulations are expected to produce more reasonable results finally in order to compare the model s capability of simulating the reversed flow in vermilion river we use the simulated outflow direction and velocity from the 100 m and 50 m model runs to construct two vector maps shown in fig 19 as the reverse flow phenomenon in the vermilion river is usually observed near the rotary point see fig 11 here we mainly focus on the comparisons of the reverse flow direction at this location simulated by these two model runs i e 100 m and 50 m as shown in fig 19 both the 100 m and 50 m model runs can reasonably simulate the reverse flow phenomenon in the vermilion river near rotary point however the 50 m model apparently performs better in terms of reproducing the more detailed eddies in the river channel due to the mixing of surface runoff from different directions in the above analyses we have evaluated the performance of our proposed model in simulating the flood depth flood extent and reverse flow phenomenon in lafayette parish during the 2016 louisiana flood it is worth to mention that the usgs has conducted an in depth study for the 2016 louisiana flood with a flood inundation model watson et al 2017 here we should note that the inundation map generated by usgs is only limited to those low lying areas close to the main river channel as all inundation models are intended for simulating river floods in comparison our model can create a flood map for the entire domain with a reasonable representation of all low lying areas no matter whether or not they are close to the main river channel as for the flood depths the usgs reported that the depths of water at those high water marks ranged from 0 7 to 6 8 ft aboveground approximately 213 2073 mm in comparison the simulated flood depths in the vermilion river by our model seem to be slightly higher than the usgs results see figs 15 and 16 however since those high water marks in the usgs results are usually placed in the riverside rather than in the middle of river channel it is reasonable to believe that the flood depths in the vermilion river should be slightly higher the reported range by usgs in other words our model performs reasonably well in simulating the flood depths as reported by usgs in addition the usgs study reported that flow direction of the vermilion river in the lafayette area can reverse because of the large area of urbanization even though their flood inundation model was incapable of reflecting this reverse flow effect in comparison such a reverse flow effect can be successfully captured by our model particularly the reverse flow effect near the rotary point in vermilion river has been widely reported as shown in fig 19 our model can perfectly capture the reverse flow phenomenon at the rotary point 4 conclusions in this study a new numerical model is developed to bridge the research gap in urban flooding prediction under heavy precipitation the developed divides the study domain into many grid cells without any limitation on the spatial resolution of the grid cell as long as the dem data of the same resolution are available it can simulate both horizontal surface runoff and vertical water flow simultaneously in each time step of model simulation this makes the model capable of reflecting the frequent inflow or outflow interactions among grid cells and capturing the rapid generation of surface runoff in urban areas during heavy rainfall the model can also account for typical characteristics of urban areas such as large scale impermeable surfaces and urban drainage systems more importantly the model uses both the surface elevation and instantaneous surface water depth to dynamically determine the directions of horizontal inflow and outflow for each grid cell during each time step of model simulation this enables the model to capture the reverse flow phenomenon which is commonly seen in flat urban areas during heavy storms in order to demonstrate the effectiveness of the proposed model in predicting real world flooding events we have applied it to reproduce the 2016 flood in lafayette parish the model is run at four spatial resolutions to help find a threshold in model resolution at which the model can generate reasonable simulations the results show that the model simulations at 1000 m and 500 m are unable to reproduce the flooded regions and their spatial distributions because of their coarse resolutions this further confirms that conventional hydrological models with spatial resolutions greater than 1 km are not suitable for urban flooding simulation however if the model resolution is refined to 100 m the spatial patterns of flooded areas in the observations start to show up in the simulations suggesting that 100 m is probably a reasonable spatial resolution for the proposed model to generate acceptable simulations when the model resolution is further improved from 100 m to 50 m we do not see apparent improvements in terms of capturing the large scale flooding patterns but our results do show that the spatial variations of flood depths can be better represented with more details this is particularly true for some areas which contain local pits and are not directly connected to river channels in addition the model simulations at 100 m and 50 m are both capable of reproducing the reverse flow phenomenon observed in the study area however the model simulation with a higher resolution apparently performs better in terms of reproducing the more detailed eddies in river channels due to the mixing of surface runoff from different directions in general it is reasonable to believe that model simulations with higher resolutions should be more capable of reflecting the spatial variations of local flood depths as displayed in the observations however we should note that model simulations with finer resolutions will lead to substantially increased computational requirements meanwhile the performance in local flooding simulation may not be significantly improved it is thus important to seek a balance point between model performance and computational requirements in practical applications this is especially important for real time flooding prediction and warning which requires as much lead time as possible for evacuation preparation in addition we should note that default values for model parameters given in tables 1 and 2 related to land cover and soil texture are used in the case study because our purpose is to test whether the proposed model can reasonably reproduce the 2016 flood in lafayette parish with default settings it is a limitation of our case study as a properly calibrated model might produce even better simulations in practice the values for these parameters i e c d Î± and Î² can be adjusted accordingly as long as they are within the corresponding ranges further studies on the model sensitivity to these parameters should be conducted to better facilitate the selection of appropriate values for different applications it also should be noted that although the proposed model in this study uses square grid cells of same size to discretize the continuous land surface the key idea of the model can be potentially applied to other types of grids e g triangle or hexagon to allow for variable grid size however the numerical implementation of most equations in this study is based upon the assumption of square grid cells appropriate modifications to their numerical implementation are thus required for other types of grids last but not least although the proposed model here is intended for urban flooding prediction it can also be applied for modeling floods in rural areas or natural watersheds where no adjustment to infiltration rate is required declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by university of prince edward island and the natural science and engineering research council of canada the 5 m dem data for lafayette parish are available for public access through louisiana atlas gis platform https maps ga lsu edu lidar2000 the land cover data i e nlcd 2011 are available for public access at https www mrlc gov nlcd2011 php the soil texture data i e ssurgo are available for public access at https websoilsurvey sc egov usda gov app websoilsurvey aspx the data for groundwater level of lafayette parish are derived from the national water information system nwis which is accessible via the usgs water data web interface https waterdata usgs gov nwis the precipitation data for lafayette parish during the 2016 flood are available for public access at https www ncdc noaa gov cdo web the data for soil moisture content are obtained from the national integrated drought information system nidis which is available for public access at https www drought gov drought data gallery soil moisture 
6261,increasing city resilience to floods under climate change has become one of the major challenges for decision makers urban planners and engineering practitioners around the world accurate prediction of urban floods under heavy precipitation is critically important to address such a challenge as it can help understand the vulnerability of a city to future climate change and simulate the effectiveness of various sustainable engineering techniques in reducing urban flooding risks in real urban settings here we propose a new model for urban flood prediction under heavy precipitation the model divides an irregular urban area into many grid cells with no limitation on the spatial resolution as long as the dem data of the same resolution are available it is capable of reflecting the frequent inflow or outflow interactions among grid cells and capturing the rapid generation of surface runoff in urban areas during heavy rainfall the model also accounts for typical characteristics of urban areas such as large scale impermeable surfaces and urban drainage systems in order to simulate urban floods more realistically in addition the model uses both surface elevation and instantaneous surface water depth of all grid cells to dynamically determine the directions of horizontal inflow and outflow during each time step of model simulation this enables the model to capture the reverse flow phenomenon which is commonly seen in flat urban areas during heavy storms by applying the proposed model for reproducing the 2016 flood in lafayette parish louisiana we demonstrate its effectiveness in predicting real world flood events keywords urban flood prediction climate change heavy precipitation reverse flow flooding risk flood map urban resilience 1 introduction as a consequence of global warming the hydrological cycle has been amplified in the form of more frequent and intense precipitation events allan and soden 2008 held and soden 2006 wang et al 2014 this becomes a serious challenge for most of the urban areas around the world where unprecedented flooding events due to heavy precipitation have been more frequently observed in recent years garner et al 2017 jha et al 2012 wang et al 2016 wilby and keenan 2012 for example the 2016 louisiana flood was caused by a severe weather system which brought prolonged rainfall in about 72 h to the southern parts of louisiana resulting in widespread and catastrophic flooding this flood damaged more than 140 000 houses required evacuation of more than 20 000 people and led to at least 13 deaths and 10 billion monetary losses noaa 2016 vahedifard et al 2016 watson et al 2017 it has been regarded as an historic and unprecedented flood event in louisiana and one of the worst natural disasters in united states severe floods have also frequently struck several major cities in canada in recent years and caused billions of dollars in damage particularly the 2013 floods in calgary and toronto have been recorded as the largest natural disasters in alberta s and ontario s history respectively the insurance damages resulted from these two events have constituted the first and third largest natural insured catastrophes in canadian history tanner and arvai 2018 thistlethwaite and henstra 2017 wang et al 2014 recent studies suggest that future global warming will lead to significant changes in the intensity and frequency of precipitation extremes which are very likely to be associated with higher risks of urban flooding alfieri et al 2016 schiermeier 2011 wang et al 2015 hence how to increase city resilience to floods under climate change has now become one of the major challenges for decision makers urban planners and engineering practitioners around the world hammond et al 2015 kim et al 2017 accurate prediction of urban flooding under heavy precipitation is critically important to address such a challenge because it can help understand the vulnerability of a city to future climate change from a long term perspective and simulate the effectiveness of various sustainable engineering techniques in reducing urban flooding risks in a real urban setting jha et al 2012 although most of conventional hydrological models perform generally well in simulating surface runoffs or river streamflow in natural watersheds they are unable to accurately simulate urban flooding because of the following reasons 1 most hydrological models require a predefined watershed in order to determine model simulation boundary arnold et al 2012 moriasi et al 2007 wolock and mccabe jr 1995 however it is difficult to identify a watershed since the natural watershed has been largely altered by intense human activities in modern urban areas 2 some grid based land surface models e g vic are able to simulate large scale runoffs by dividing the study domain into many grid cells with a spatial resolution greater than 1 km gao et al 2010 liang et al 1994 but they are still unsuitable for urban flood modeling because they simulate vertical water flow and horizontal runoff separately for example vic assumes that water can only enter a grid cell via the atmosphere i e no horizontal inflow or outflow interactions among grid cells during the first stage for vertical water balance simulation in the second stage for horizontal runoff simulation it simply uses dem based river network to connect individual grid cells in order to determine the streamflow of watershed outlet liang et al 1994 such an assumption apparently does not hold for urban flooding which usually occurs quickly and is characterized with frequent inflow or outflow interactions among grid cells besides the finest resolution of vic is 1 km which is too coarse to capture the rapid generation of surface runoff in urban areas during heavy rainfall 3 during flooding events surface runoffs or river flows can be reversed in some relatively flat urban areas because of the uneven increases to surface water depths at local scales that means the direction of surface runoff in each grid cell can be constantly changing while the flood is quickly developing due to heavy rainfall however all conventional hydrological models use dem to determine flow direction before initiating a model simulation callow et al 2007 wise 2007 the determined flow directions remain unchanged during the entire model simulation apparently none of the latest hydrological models is capable of simulating the reverse flow phenomenon in many urban areas given the increasing attention to building climate resilient urban infrastructures costa et al 2016 prasad et al 2008 it is in urgent need to develop new methods or approaches in support of accurate prediction of urban flooding under heavy precipitation a large number of computer models for flood simulation have been proposed in recent years gires et al 2015 kauffeldt et al 2016 praskievicz and chang 2009 salvadore et al 2015 sood and smakhtin 2015 teng et al 2017 but these models are always based on various assumptions which make them only suitable for simulating some specific types of flood in general floods can be categorized into four types based on their location of occurrence and causes including river flood coastal flood flash flood and urban flood ali 2018 river flood occurs when a river channel is filled with too much water that is more than its handling capacity thus the surplus water overflows the river banks and runs into the adjacent low lying lands coastal flood occurs when tropical storms earthquake or volcanic activities drive an unusually high amount of ocean water towards the inland flash flood occurs when a large amount of water is discharged within short period of time e g a sudden release of water from a dam and urban flood is mainly caused by rapid runoff over the large scale impermeable surfaces under heavy rainfall conventional hydrological models e g vic swat and topmodel are designed to simulate river streamflow which might be used as inputs for river flood simulation but they cannot be applied directly for flood simulation because they are incapable of estimating flood extent and depth current flood models such as hec ras loi et al 2018 tuflow goodall et al 2017 csiro tvd teng et al 2015 autorapid follum et al 2017 and hand zheng et al 2018 are essentially flood inundation models which assume that a large amount of water will first enter into the river channel and lead to a rapid increase in water level then the adjacent low lying areas will be flooded only after the river bank is overtopped shown in fig 1 this assumption usually holds for river coastal and flash floods because a large amount of water must be first transported to the to be flooded regions through river channels before any floods occur however it does not hold for urban floods which are typically caused by heavy rain storms and have been commonly observed in many cities in recent years particularly when a large amount of water is dumped down to an urban area during a heavy precipitation event surface runoff will be generated immediately due to the almost zero infiltration of impermeable surfaces any locally low lying areas i e pits will be easily flooded because of the quick accumulation of surface runoff furthermore the accumulated surface runoff can also flow into the river channels and raise their water levels shown in fig 1 apparently these dynamics cannot be represented by flood inundation models although many previous studies have applied flood inundation models for urban flooding simulation teng et al 2017 it is not surprising to see that their real world applications are always focused on cities being passed through by a river in general flood inundation models first estimate the total water volume using the information of rainfall depth and total area of the study region all water will then be assumed to be dumped down into the river channel leading to a rapid increase in its water level thus low lying regions close to the river bank will always be flooded first while other low lying regions located far enough from the river will be inundated later or might be not inundated at all if they are blocked by some high elevated lands but this is clearly not true in an urban setting where a large amount of water can be poured down to every corner of a city during a rain storm for example an empty swimming pool at the top of a mountain will also be filled with water at the same time when the river s water level is quickly rising during heavy rainfall however flood inundation models will result in no water in the swimming pool as long as the elevation of the mountain is higher than the water elevation in the flooded river hence flood inundation models are not suitable for urban flood simulations under heavy precipitation because of their inappropriateness in reflecting the spatial and temporal dynamics of urban flooding process therefore in this study we will propose a new model for urban flooding prediction under extreme precipitation events particularly we focus on heavy rain as it is the main cause for urban flooding according to the glossary of meteorological terms by american meteorological society see http glossary ametsoc org wiki heavy rain heavy rain is defined as rain with a rate of accumulation exceeding a specific value that is geographically dependent however this definition is somehow too general to be used as a quantitative criterion there are many other quantitative criteria used to define heavy rain for example the meteorological service of new zealand defines heavy rain as 50 mm or more in a 6 hour period or 100 mm or more in a 24 hour period see https www metservice com warnings weather warning criteria chart in general heavy rain can be regarded as a large amount of rainfall during a relative short period of time in order to reflect the rapid development of urban floods under heavy rainfall the new model divides the study domain into many grid cells without any limitation on the spatial resolution of the grid cell as long as the dem data of the same resolution are available it can simulate both horizontal surface runoff and vertical water flow simultaneously in each time step of model simulation this makes the model capable of reflecting the frequent inflow or outflow interactions among grid cells and thus capturing the rapid generation of surface runoff in urban areas during heavy rainfall the model can also account for typical characteristics of urban areas such as large scale impermeable surfaces and urban drainage systems more importantly the model uses both the surface elevation and instantaneous surface water depth to dynamically determine the directions of horizontal inflow and outflow for each grid cell during each time step of model simulation this enables the model to capture the reverse flow phenomenon which is commonly seen in flat urban areas during heavy storms the proposed model will then be used to reproduce the 2016 flood in lafayette parish louisiana to demonstrate its effectiveness in predicting both flood extent and depth during real world flooding events 2 model development during a heavy precipitation event water often accumulates quickly in urban areas and becomes surface runoff the areas in the downstream direction of surface runoff are likely to be flooded therefore it is important to determine the direction of surface runoff before predicting which areas are expected to be flooded 2 1 direction of surface runoff in order to simulate the complexity of earth s surface we first convert the continuous surface to many square grid cells the size of the grid cell i e the length of one side of the cell denoted as r indicates the spatial resolution of our model which can range from a few meters to a few kilometers depending on the geographical extent of the heavy precipitation event of interest for a given time t the instantaneous height of a grid cell denoted as p is defined as the sum of its elevation l and surface water depth h as illustrated in fig 2 in the case of no surface water the instantaneous height of a grid cell is equal to its elevation and no surface runoff or outflow from this grid cell is generated if a grid cell is covered by surface water due to a heavy rainfall or the inflow from its immediate neighbors surface runoff is likely to be generated in this grid cell and the outflow direction is determined by the gradient of instantaneous height as described in the following as illustrated in fig 3 a given grid cell labeled as 0 can have at most one outflow direction while its inflows may come from at most all of its immediate neighbors for example if its instantaneous height is the lowest among all its immediate neighbors there will be no outflow but water in its immediate neighbors might all flow into this grid cell otherwise there must be at least one immediate neighbor with its instantaneous height lower than that of the focal grid cell the surface water in the grid cell will thus flow into the neighbor with the lowest instantaneous height for simplicity here we use numbers from 1 to 8 to label the outflow directions of all grid cells within the domain e g an outflow direction of 1 indicates that the water flows into the east neighbor while an outflow direction of 4 means that the water flows toward the southwest neighbor as shown in the 3d view of the gridded domain in fig 3 among all immediate neighbors of grid cell 0 grid cell 2 has the lowest instantaneous height thus we can use 2 to represent the outflow direction of grid cell 0 meanwhile grid cell 0 is expected to receive water from three of its immediate neighbors i e 6 7 and 8 thus its inflow directions should consist of 6 7 and 8 for a grid cell located in the edge or corner of the domain its outflow direction can be any one from 1 to 8 however if the outflow direction faces toward one of out domain neighbors the outflow from this grid cell is completely or partially regarded as a discharge of the domain to its surrounding area for example if the outflow direction of the edge grid cell shown in fig 3 is 5 the outflow will completely contribute to the domain s discharge however if the outflow direction is 6 a small portion of the outflow of this grid cell might enter into grid cell 7 while the majority of the outflow will contribute to the domain s discharge in contrast the inflows to an edge or corner grid cell are assumed to be only originated from its immediate neighbors within the domain although the out domain neighbors sometimes can also contribute the inflows to the grid cell e g in some mountainous regions as shown in fig 4 their contributions cannot be explicitly accounted for as their instantaneous heights are unknown instead we may estimate the inflows from these out domain neighbors based on the local hydrograph or the latest observations meanwhile we can adjust the original domain size to avoid placing the boundary of the domain on some uphill areas as illustrated in fig 4 with the purpose of minimizing the effects of inaccurate edge inflows on the water mass balance within the domain 2 2 velocity of surface runoff we use the momentum equation of saint venant equations to estimate the outflow velocity of a grid cell once its outflow direction is determined the saint venant equations are originally developed to calculate the momentum budget in a river channel chalfen and niemiec 1986 garcia and kahawita 1986 strelkoff 1970 as follows 1 u t u u x g h x g s c d u 2 h where u is the water velocity unit m s and it varies with time and location the temporal and spatial variations of u are represented by u t and u x respectively h unit m is the water depth at deepest point for a slice of river channel defined by dx unit m and it varies with location represented by h x g is the gravity acceleration with a rate of 9 8 m s2 s sin Î¸ represents the bottom slope and Î¸ unit is the bottom slope angle for a slice of river channel defined by dx and cd is a drag coefficient in the river channel and varies with the shape and roughness of the channel bed when applying the above equation to calculate the velocity of surface runoff within the gridded domain a number of approximations should be made as follows a given a small grid size denoted as r unit m and a short time step it is reasonable to assume that the surface water flow within the grid cell and to its neighbor grid cells is steady and uniform specifically steady means no temporal variation in water velocity i e u t 0 while uniform means no spatial variation in water velocity i e u x 0 therefore the left hand side of eq 1 becomes zero and we can rewrite it as 2 g s h x c d u 2 h as shown in fig 5 the elevations of the upstream and downstream cells are denoted as l 1 and l 2 and their surface water depths are represented by h 1 and h 2 respectively for convenience we classify the outflow directions into two cases parallel directions 1 3 5 and 7 and diagonal directions 2 4 6 and 8 for parallel outflow directions see case 1 in fig 5 the bottom slope s in eq 2 can be estimated by 3 s l 1 l 2 l 1 l 2 2 r 2 and the change in water depth along the outflow direction can be calculated by 4 h x h 2 h 1 r for diagonal outflow directions see case 2 in fig 5 the bottom slope s in eq 2 can be estimated by 5 s l 1 l 2 l 1 l 2 2 2 r 2 and the change in water depth along the outflow direction can be calculated by 6 h x h 2 h 1 2 r b as shown in eq 2 h is defined as the water depth at deepest point for a slice of river channel defined by dx here we discretize the continuous surface into small grid cells and we assume that the water above one grid cell can only flow into one of its eight neighbors the slice of river channel defined by dx is thus always within the current grid cell and its downstream neighbor therefore the deepest water depth over dx should be the maximum between h 1 and h 2 and can be expressed as 7 h m a x h 1 h 2 c as the shape of the surface water tank above a grid cell is assumed to be fixed while it moves toward the downstream grid cell by a small distance dx during a short period dt it is reasonable to assume that the drag coefficient cd in eq 2 is dominated by the roughness of the grid cell which varies with different land cover use types for example a grid cell covered by trees and shrubs often has a higher drag coefficient than a grid cell used for urban development in general the drag coefficient for grid cells covered by vegetation should be adjusted according to the area density and type of vegetation the effects of these factors on the drag coefficient have been widely investigated in the literature arcement and schneider 1989 cushman roisin and beckers 2011 fischenich and dudley 1999 hui et al 2010 wohl 1998 the value of cd usually varies between 0 003 and 0 2 based on the recommendations in the literature arcement and schneider 1989 hui et al 2010 a predefined value of cd is assigned to each category of land cover use here we refer to the national land cover classification system issued by the united states geological survey usgs website https www mrlc gov nlcd11 leg php to classify land cover use into 20 categories the values of cd for these categories are listed in table 1 based on the above approximations we can rewrite eq 2 to derive the outflow velocity u as follows 8 u g s h x h c d once the outflow direction is determined all the variables in the right hand side of eq 8 can be explicitly calculated according to eqs 3 7 in detail if the outflow direction is labeled as 1 3 5 or 7 the outflow velocity can be estimated by 9 u g l 1 l 2 l 1 l 2 2 r 2 h 2 h 1 r m a x h 1 h 2 c d if the outflow direction is labeled as 2 4 6 or 8 the outflow velocity can be estimated by 10 u g l 1 l 2 l 1 l 2 2 2 r 2 h 2 h 1 2 r m a x h 1 h 2 c d here the instantaneous height for the upstream grid cell is p 1 l 1 h 1 and that for the downstream grid cell is p 2 l 2 h 2 respectively since the surface water always flows from the upstream grid cell to the downstream grid we must have the following relationship 11 p 1 l 1 h 1 p 2 l 2 h 2 while the elevation of each grid cell is usually constant during a short period of dt the surface water depth can vary significantly with several factors e g inflows from its neighbors outflow to the lowest neighbor infiltration to the underlying soil and evapotranspiration to the atmosphere thus the relationship between h 1 and h 2 can also change over time this means that the value of m a x h 1 h 2 in eqs 9 and 10 should be determined separately for each time step to illustrate how to determine the value of m a x h 1 h 2 we consider three cases for the relationship between h 1 and h 2 as illustrated in fig 6 since determining the value for m a x h 1 h 2 is not affected by the outflow direction we will only consider parallel outflow directions i e 1 3 5 and 7 for illustration purposes i e eq 9 should be used note that the determination rules for diagonal outflow directions of 2 4 6 and 8 i e eq 10 should be used are exactly the same case 1 h 1 h 2 we can choose either h 1 or h 2 to represent m a x h 1 h 2 in order to meet the requirement defined by eq 11 we must have l 1 l 2 meanwhile h x in eq 2 will be 0 thus eq 9 can be rewritten as 12 u g l 1 l 2 l 1 l 2 2 r 2 h 1 c d in this case the outflow velocity of the upstream grid cell will be solely affected by the elevation gradient case 2 h 1 h 2 we should choose h 2 for m a x h 1 h 2 in order to meet the requirement defined by eq 11 we must have l 1 l 2 equation 9 can be rewritten as 13 u g l 1 l 2 l 1 l 2 2 r 2 h 2 h 1 r h 2 c d apparently h 2 h 1 r in eq 13 is positive indicating that the deeper water depth in the downstream grid cell tends to restrain the water coming from the upstream grid cell furthermore this may imply that the downstream grid cell is receiving more water from its other neighbors leading to a faster increase in its surface water depth than the upstream grid cell after a certain time the instantaneous height of the downstream grid cell may become higher than that of the upstream grid cell i e p 1 p 2 the flow direction can thus be reversed even though the elevation gradient still exists case 3 h 1 h 2 we should choose h 1 for m a x h 1 h 2 in this case the relationship between l 1 and l 2 can be arbitrary refer to the three scenarios under case 3 in fig 5 as long as the instantaneous heights of two grid cells meet the requirement defined by eq 11 eq 9 can be written as 14 u g l 1 l 2 l 1 l 2 2 r 2 h 2 h 1 r h 1 c d note that h 2 h 1 r in eq 14 is negative in this case as subtracting a negative value is equivalent to adding a positive value this indicates that the higher water depth in the upstream grid cell tends to speed up its outflow particularly the 3rd scenario in this case i e l 1 l 2 see fig 6 may be regarded as the next stage result from case 2 while the outflow direction is reversed due to the greater and faster inflows into the initial downstream grid cell in other words the initial upstream grid cell with a higher elevation can sometimes be turned into a downstream grid cell more importantly this implies that the flow direction of surface runoff can sometimes be reversed due to the uneven increases to the surface water depth at local scales 2 3 volume of surface runoff the depth of the surface runoff is determined by the gradient of instantaneous height between the upstream grid cell and the downstream one i e p 1 p 2 here we assume that the surface runoff above the upstream grid cell is shaped as a square water tank and the shape of this water tank will not change over a very short period of dt in detail the floorage of this water tank is the same as the area of the grid cell and its initial height is equal to the gradient of instantaneous height i e p 1 p 2 apparently the height of the outflow water tank of the upstream grid cell should be non negative and must be not greater than its initial surface water depth h 1 once the depth of the outflow water tank is determined the outflow volume during dt is proportional to the moving out floorage of the water tank from the grid cell which is further determined by outflow direction and velocity as the shape of the water tank remains unchanged here we should note that for parallel outflow directions the water from the upstream grid cell will only flow into the downstream one however for diagonal outflow directions the upstream water may also partially flow into the other two side neighbors as illustrated in case 2 of fig 7 as long as the instantaneous heights of these two neighbors are lower than that of the upstream grid cell the moving out floorage of the water tank during dt is denoted as da and its calculation varies with the outflow direction specifically the outflow velocity u should be first calculated according to eqs 9 or 10 once the outflow direction is determined next the moving out floorage of the water tank can be calculated as follows see also fig 7 case 1 parallel outflow directions 1 3 5 and 7 the water tank will only move into the downstream grid cell and the moving out floorage can be calculated as 15 da r u d t case 2 diagonal outflow directions 2 4 6 and 8 the water tank may also need to pass through the other two side neighbors when moving downward to the downstream grid cell the expected floorage moving into each side neighbor can be calculated as 16 1 2 r u d t 1 2 u d t 2 and the expected floorage moving into the downstream grid cell is computed as 17 1 2 u d t 2 thus the total moving out floorage of the water tank in this case should be the sum of moving in floorage of the downstream grid cell and the other two side neighbors it can be calculated as 18 da 2 r u d t 1 2 u d t 2 after the moving out floorage of the water tank da is computed with eqs 15 or 18 the outflow volume unit m3 s of the upstream grid cell during dt can be calculated as 19 v dt d a p 1 p 2 thus the remaining volume of the water tank in the upstream grid cell is calculated as 20 v dt r 2 d a p 1 p 2 here we should note that eqs 15 20 hold only if some surface water is present in the upstream grid cell i e h 1 0 otherwise there will be no surface runoff i e v dt 0 and v dt 0 as shown in fig 3 the upstream grid cell of interest might also serve as a downstream grid cell to one or more of its immediate neighbors because it is allowed to receive inflows from multiple grid cells the outflows from its upstream neighbors during a short time of dt can also be calculated separately by eqs 15 17 as all these outflows from the upstream neighbors will become the inflow to the grid cell of interest we can sum them up to compute its total inflow during dt denoted as v dt unit m3 s note that the outflows from its upstream grid cells to their two side neighbors in case 2 of fig 7 should be determined by their instantaneous heights in a case by case fashion for example if the instantaneous heights of both side neighbors are lower than that of the upstream grid cell then they both will receive the same portion of the outflow as expressed by eq 16 if their instantaneous heights are both greater than that of the upstream grid cell then they will receive no water from the upstream grid cell i e the expected outflows to these side neighbors will be added to the outflow to the downstream grid cell if one neighbor is higher than the upstream grid cell and the other is lower then the lower neighbor will receive its portion of the outflow as expressed by eq 16 and the expected portion to the higher neighbor will be added to the outflow to the downstream grid cell 2 4 mass balance of surface water in order to calculate the water mass balance within a grid cell we usually need to consider the entire column of the grid cell as illustrated in case 1 of fig 8 however here our focus is on the surface water during flooding season which is typically generated during a short time period because of heavy precipitation as the surface water must pass through the unsaturated soil to recharge groundwater and this process usually takes a longer time period it is reasonable to assume that the interaction between surface water and groundwater is negligible during a heavy precipitation event perhaps the only exception for the above assumption is rivers or lakes where groundwater recharges surface water by providing the base flow in rivers or the base water level in lakes however this recharge process is too slow to contribute to the commonly seen floods which usually develop in a short time period thus in the following analyses we will only consider the column above the normal aquifer within a grid cell to calculate the mass balance of surface water see case 2 of fig 8 as for the unsaturated soil above the aquifer we assume that water within this layer only flows downward i e there is no horizontal water flow furthermore the water loss due to evapotranspiration during a flooding event is often very small compared to the large amount of precipitation it thus can be assumed to be negligible in the calculation of surface water mass balance during a heavy precipitation event therefore the water mass balance equation in case 2 of fig 8 can be further simplified as 21 water m a s s g a i n p r e c i p i t a t i o n s u r f a c e i n f l o w s u r f a c e o u t f l o w i n f i l t r a t i o n the above equation indicates that the surface water mass change in a grid cell is expressed as the tradeoff between the total mass inputs contributed by precipitation and surface inflow and the total mass outputs caused by surface outflow and infiltration here the mass of surface inflow and outflow can be directly calculated by multiplying the total volumes of surface inflow and outflow i e v dt and v dt by the average water density during dt the total water mass from precipitation during dt can be estimated with the observations from weather stations and radars or the simulations from weather prediction models the calculation of infiltration should account for various soil textures and land cover use and is described in detail as follows for simplicity we assume that all water inputs and outputs have the same water density thus the calculation of water mass balance is equivalent to the calculation of water volume balance for a given time t the infiltration rate of a given grid cell is denoted as k varying from 0 to 100 the filtration rate here means how much of the total surface water above this grid cell will be infiltrated into the soil apparently the infiltration rate is significantly influenced by the initial soil moisture content denoted as m ranging from 0 to 100 the effects of initial soil moisture content on infiltration rate have been studied a long time ago horton 1933 houser 2005 morin and benyamini 1977 philip 1957 stÃ¤hli et al 1999 according to gray and norum 1967 the infiltration rate decreases exponentially as the initial soil moisture content increases here we use an exponential function to represent their relationship as follows 22 k e Î² m where Î² is a damping coefficient describing the decreasing rate of infiltration due to the increase in soil moisture and its values usually vary between 5 and 10 in general if the surface soil is completely saturated i e the initial moisture content reaches its maximum m 100 then no surface water will be infiltrated i e the infiltration rate decreases to zero k 0 it is worth to mention that the theoretical infiltration rate in this case might not decrease to exactly zero this is because the hydraulic conductivity of saturated soil is not completely zero see table 2 however since the typical values of saturated hydraulic conductivity are ranging between 1 03 10 6 and 1 76 10 4 m s clapp and hornberger 1978 implying that the movement of water in saturated soil is too slow to release sufficient space for the surface water to be infiltrated during a short time period this is especially true when the internal time step of our model integration is very small typically in the order of a few seconds therefore it is reasonable to treat the infiltration rate as zero in practice to meet this requirement the minimum value of Î² should be used in eq 22 in detail if we put m 100 and Î² 5 into the right hand side of eq 22 the calculated value of k is 0 67 which can be reasonably regarded as 0 however if we further decrease the value of Î² the value of k becomes too large to be deemed to be zero therefore the minimum value of Î² should be 5 we also should note that for some types of soil with very small particles e g silty clay although they are not fully saturated i e m 100 the infiltration rate can still be as low as zero in this case we may increase the value of Î² to damp the infiltration particularly if we increase Î² to 10 even though the surface soil is only half saturated i e m 50 the infiltration rate will drop to 0 67 which can be deemed to be 0 this is reasonable enough to represent the damped infiltration of silty clay soil which has the smallest particles among all types of soil therefore the maximum value of Î² is assigned 10 the exponential relationships between infiltration rate and initial soil moisture content with the minimum and maximum damping coefficients are shown in fig 9 in general the damping coefficient Î² can be determined by soil texture smaller values of Î² should be used for coarse textured soils e g sand to reflect high infiltration while bigger values should be selected for fine textured soils e g clay to indicate low infiltration in order to find an appropriate value of the damping coefficient from 5 to 10 for each type of soil texture we need to determine the saturated hydraulic conductivity denoted as ks unit m s within the soil as it also correlates with soil texture in general high hydraulic conductivity means that water can move quickly within the soil leading to more space in the soil to allow more surface water to be infiltrated during a certain time period this corresponds to small damping effects on infiltration and smaller values of Î² should be chosen to reflect reduced damping effects similarly high values of Î² should be selected to indicate increased damping effects for low hydraulic conductivity in other words the damping coefficient Î² is inversely proportional to the saturated hydraulic conductivity ks here we consider 12 classes of soil texture as defined in the united states department of agriculture soil texture triangle usda 2017 clapp and hornberger 1978 have identified the key soil hydraulic properties including saturated hydraulic conductivity and porosity based on 1 845 soil samples which are listed in table 2 it has been found that sand has the highest saturated hydraulic conductivity i e 1 76 10 4 m s while silty clay has the lowest i e 1 03 10 6 m s accordingly sand should have the smallest damping coefficient i e Î² 5 and silty clay should have the biggest damping coefficient i e Î² 10 we assume that the damping coefficient and the saturated hydraulic conductivity are linearly and inversely correlated the damping coefficients for other soil textures can be calculated as follows 23 Î² 10 k s 1 03 10 6 1 76 10 4 1 03 10 6 10 5 the calculated damping coefficients for 12 soil texture classes are also listed in table 2 in addition to the damping coefficient Î² the initial soil moisture content m is also required to estimate the infiltration rate according to eq 22 in general the initial soil moisture content can be directly measured through field trips or estimated with historical observations thus the initial infiltration rate can be calculated with eq 22 after an appropriate value of Î² is selected from table 2 generally speaking infiltration is a relatively slow process compared to the rapid surface runoff caused by intense rainfall during a flooding event thus we can assume that the infiltrated water from t to t dt only comes from the remaining surface water at time t dt in other words infiltration only takes place instantaneously at time t dt thus the calculation of the infiltrated water can be broken down into two steps 1 calculate the volume of remaining surface water at time t dt without considering infiltration 2 compute the infiltration rate at time t with eq 22 and apply it to the remaining water obtained in the first step to estimate the total volume of infiltrated water from t and t dt specifically the remaining surface water without consideration of infiltration in the upstream grid cell at time t d t denoted as v t d t must be a non negative value i e 0 and can be expressed as 24 v t d t h 1 r 2 p r 2 d t v dt v dt where p is the precipitation rate above the grid cell at time t unit m s v dt and v dt represent the volumes of inflow and outflow of the grid cell from t to t d t respectively here we should note that using precipitation rate as the input allows us to simulate the rapid evolution of urban floods during heavy precipitation particularly the computational time step i e dt of the proposed model is usually in the order of seconds and is a function of grid cell size and water velocity defined by the well known courant friedrichs lewy cfl condition courant et al 1967 in other words in order to ensure model convergence we need to make sure that a particle of water entering a grid cell would not travel through it until a computation step within the model has been completed for example if the grid size is 100 m and the calculated maximum water velocity with the saint venant equations is 5 m s then the time for the water to pass through one grid cell would be 100 5 20 s therefore we need to choose a time step less than 20 s to ensure the water did not leave the grid cell too soon once the remaining surface water without consideration of infiltration is calculated with eq 24 we can proceed to calculate the infiltrated water volume at time t dt assume that the soil moisture content at time t is denoted as m t the infiltration rate k can be calculated with eq 22 thus the total volume of water expected to be infiltrated into the soil for the given grid cell denoted as v dt can be calculated as 25 v dt k v t d t meanwhile we should note that the actual volume of infiltrated water for the grid cell is limited by the maximum volume of water that the soil can hold while it is completely saturated denoted as v max which is further determined by the depth of unsaturated surface soil i e depth of vadose zone or groundwater level denoted as q unit m and its porosity denoted as n unit specifically v max is calculated by 26 v max r 2 q n where the value of q is usually measured through field trips or estimated with historical observations while the value of n is determined by soil texture see table 2 thus the maximum volume of water that the soil in the grid cell can still infiltrate at time t can be calculated as v max 1 m t here we will consider the following two cases to calculate the actual infiltrated water volume during dt and to update the soil moisture content at time t d t case 1 v dt v max 1 m t the grid cell can only infiltrate a portion of the v dt as the soil will be saturated thus the total volume of surface water left in the grid cell at time t d t is represented by 27 v t d t v t d t v max 1 m t meanwhile the soil moisture content at time t d t is updated as follows 28 m t d t 100 case 2 v dt v max 1 m t all of the surface water to be infiltrated will be ultimately infiltrated into this grid cell thus the total volume of surface water remaining in the grid cell at time t d t can be expressed as 29 v t d t v t d t 1 k meanwhile the soil moisture content at time t d t is updated as follows 30 m t d t m t v dt v max 100 2 5 consideration of urban area in general the calculation of the expected water to be infiltrated as specified by eq 25 can be applied to all natural surfaces however it is not applicable for urban regions which are commonly covered by impermeable materials in this case surface runoff might be generated immediately after rainfall reaches the impermeable surfaces urban drainage or stormwater systems are therefore designed to collect the surface runoff through street drains and discharge it into a detention basin or the nearest receiving water body to minimize the possibility of flooding butler and davies 2003 schmitt et al 2004 yazdanfar and sharma 2015 the maximum capability of an urban drainage system in conveying surface runoff is designed according to the historical rainfall intensity durationfrequency idf curves derived from precipitation observations at weather stations idf curves describe all characteristics of extreme precipitation events with different durations and return periods at a given location and are widely used for guiding the hydraulic design of urban drainage systems and other infrastructures cheng and aghakouchak 2014 sarhadi and soulis 2017 wang et al 2014 here we should note that some urban drainage models allow the consideration of detailed hydraulic structures of an urban stormwater system such as pipe network combined sewer and drop shafts a commonly used model for this is the storm water management model swmm gironÃ¡s et al 2010 however the underground stormwater pipe network in an urban setting is considerably complicated especially when the study domain is large the data for such a complex network are typically not available to the public this makes the application of swmm model practically difficult for many public users more importantly swmm is usually used to simulate how much water can be held by the stormwater pipe network under various engineering design scenarios there are a number of similar models based on different implementations of the saint venant equations i e 1d 2d or their combinations to deal with the interactions between drainage flow and surface flow barnard et al 2007 leandro et al 2009 2011 however we should note that these models are intentionally designed for simulating the water distribution in the stormwater system given that the receiving water volume from surface runoff is within its handling capacity but once the capacity is exceeded these models might lose their effectiveness as the entire pipe network is filled with water this is often the case for urban floods under heavy rain storms where people are more concerned with the regions likely to be flooded and their expected water depths to tackle these challenges our model will only consider the capacity of an urban stormwater system rather than its detailed pipe network specifically if the total runoff per unit time generated over urban surface does not exceed the capacity of the stormwater system no flood is expected otherwise a portion of the runoff which cannot be handled by the stormwater system will be accumulated somewhere above the surface resulting in floods the capacity of an urban stormwater system can be easily estimated with the rainfall idf curves used for designing the system idf curves are readily available through public sources and easier to obtain than the pipe network of urban stormwater system this overcomes the challenge in data availability for swmm model most importantly our model emphasizes the situation of heavy precipitation where stormwater systems are often overloaded and urban floods are very likely to be expected it should be noted that our assumption of using idf curves to determine the capacity of stormwater system in urban areas is based on the condition that the stormwater system is fully functional in other words if the stormwater system is not fully functional using idf curves might underestimate the flood extent or depth in this case some adjustments to the idf estimated capacity of stormwater system should be performed accordingly through a full consideration of the effects of land cover use soil type street drain and open ditch on the horizontal movement and vertical infiltration of surface runoff our model is able to simulate both flood extent and depth effectively in order to calculate the mass balance of surface water in an urban area we simply classify urban grid cells covered by impermeable surfaces into two categories a with a street drain and b without a street drain note that some grid cells in an urban area might be not fully covered by impervious surfaces such as parks and zoos where the infiltration rate is still high for urban grid cells without a street drain we use the percent coverage of impervious surface in each grid cell as indicated by its land cover class in table 1 as a criterion to make adjustments to its infiltration rate see fig 10 specifically we use an infiltration adjustment coefficient denoted as Î± ranging from 0 to 100 to indicate how to make an adjustment to the original infiltration rate k derived according to the soil texture class without considering the impervious surface above the soil the adjusted infiltration rate denoted as k can be calculated as 31 k k Î± if a grid cell is completely covered by impervious materials the value of Î± should be 0 indicating that the infiltration rate will decrease to zero after the adjustment if there is no impervious coverage for a grid cell then the value of Î± should be 100 indicating that the infiltration rate is solely determined by the soil texture this is usually the case for many natural surfaces such as barren forest shrubland herbaceous and planted cultivated regions however we should note that the soils beneath other surfaces e g water and wetlands are always saturated thus the infiltration rates for these two types of land cover should be 0 even though various soil textures may be reported here we apply Î± 0 to lands covered by water and wetlands to indicate that no infiltration occurs for a grid cell partially covered by impervious materials we apply four different values of Î± i e 0 20 50 and 80 corresponding to the four urban land cover classes refer to table 1 for example if the land cover of a grid cell is classified as low intensity develop areas with 20 to 49 impervious coverage the value of Î± should be 50 indicating that the original infiltration rate k will be halved according to eq 25 the total volume of water expected to be infiltrated v dt should be recalculated with the updated infiltration rate k as follows 32 v dt k v t d t similarly eq 29 should be replaced by 33 v t d t v t d t 1 k based on the above adjustments the remaining surface water for urban grid cells can be calculated according to the following two cases case 1 an urban grid cell without a street drain the total volume of surface water remaining in the grid cell at time t d t can be calculated using either eqs 27 or 33 depending on whether the soil will be saturated or not note that the adjusted infiltration rate should be used in this case case 2 an urban grid cell with a street drain all the surface water is supposed to flow into the underground drainage system through the street drain unless the intensity of incoming surface water exceeds the designed capacity of this inlet shown in fig 9 assume that the maximum capacity of a street drain in terms of collecting incoming surface water during a short time period of dt is measured by i unit m s it can be easily derived from the idf curves used to design the drain inlet once the expected duration of the precipitation event is determined thus the total surface water volume that can be accommodated by the drainage system during dt denoted as v d is expressed as 34 v d i d t here the total volume of remaining surface water without consideration of infiltration at t d t is still denoted as v t d t and calculated by eq 24 apparently if v t d t v d all the surface water is supposed to flow into the underground drainage system thus we have v t d t 0 otherwise the total volume of remaining surface water after considering the discharge into drainage system can be calculated by 35 v t d t v t d t v d once the volume of remaining surface water after a short time period of dt i e v t d t is calculated with eqs 27 29 33 or 35 the surface water depth or flood depth above a given grid cell at time t d t can be directly calculated as follows 36 h t d t v t d t r 2 based on eq 36 the flood depth above all grid cells within a given study area during and after the heavy precipitation event can be simulated 3 application 3 1 study area here we will apply the proposed model to simulate the 2016 flood in louisiana united states with the purpose of demonstrating its performance in simulating real world flooding events in august 2016 a severe weather system brought prolonged rainfall in about 72 h to the southern parts of louisiana resulting in widespread and catastrophic flooding the flood damaged more than 140 000 houses required evacuation of more than 20 000 people and led to at least 13 deaths noaa 2016 vahedifard et al 2016 watson et al 2017 monetary losses resulting from various damages to homes businesses and infrastructure were estimated to be 10 billion watson et al 2017 although this disaster was not caused by a named tropical storm it has been regarded as an historic and unprecedented flood event in louisiana and one of the worst natural disasters in united states as one of the severely affected areas in south louisiana during this flooding event lafayette parish is bisected by the vermilion river which has historically been heavily flooded due to the reverse flow phenomenon kinsland 1998 kinsland and wildgen 2006 see also fig 11 during heavy rainfall events parts of the vermilion river in the lafayette area can experience negative discharge since the rise in water level in downstream reaches exceeds the water level in upstream reaches leading to an inversion in the flow direction watson et al 2017 this reverse flow phenomenon is mainly caused by the large area of urbanization and has been well confirmed by the 2016 flood profiles as reported by the united states geological survey watson et al 2017 apparently this phenomenon cannot be handled by conventional hydrological or flooding simulation models as they are all based on an assumption of no reverse flow by contrast the proposed model in this study can deal with the reverse in flow direction caused by the uneven increases to the surface water depths at local scales therefore we will specifically focus on lafayette parish in this application the total area of lafayette parish is 696 7 km2 the proposed model will be used to simulate the flood extent and depth of the 2016 flood in lafayette parish the simulated results will be compared to the observed flooding records to help validate the performance of the proposed model 3 2 data collection the proposed model requires the data for elevations land cover soil texture depth of vadose zone or groundwater level precipitation and initial soil moisture content as inputs to simulate flood extent and depth for the 2016 flood in lafayette parish here the elevation data for lafayette parish are collected from the louisiana statewide lidar project which provides a high resolution 5 m digital elevation model dem dataset for the entire state of louisiana cunningham et al 2004 the dataset is available for public access through the louisiana atlas gis platform website https maps ga lsu edu lidar2000 the digital elevation map for lafayette parish is shown in fig 11 land cover data for lafayette parish are obtained from the national land cover database 2011 nlcd 2011 which is the most recent national land cover product created by the multi resolution land characteristics mrlc consortium coulston et al 2013 homer et al 2015 the nlcd 2011 dataset has a spatial resolution of 30 m and is available for public access at https www mrlc gov nlcd2011 php soil texture data for lafayette parish are collected from the soil survey geographic database ssurgo provided by united states department of agriculture ssurgo 2018 the ssurgo provides high resolution county wide soil data and is available for public access at https websoilsurvey sc egov usda gov app websoilsurvey aspx the data for groundwater level of lafayette parish are derived from the national water information system nwis hosted by united states geological survey usgs nwis 2018 the nwis dataset contains extensive water data for the entire country of united states public access to many of these data is provided via the usgs water data web interface available at https waterdata usgs gov nwis the precipitation data for lafayette parish during the 2016 flood are collected from the national centers for environmental information ncei under the national oceanic and atmospheric administration of united states ncei 2018 particularly the gauged precipitation data at a weather station in the lafayette regional airport is derived from the ncei dataset at https www ncdc noaa gov cdo web the 2016 flood in louisiana occurred between august 12 to 22 as heavy rainfalls poured down during august 12 and 13 as recorded by the weather station in the lafayette regional airport the total precipitation received from august 12 to 13 was as high as 528 1 mm see fig 12 the data for soil moisture content are obtained from the national integrated drought information system nidis under the national oceanic and atmospheric administration of united states nidis 2004 the nidis dataset is available for public access at https www drought gov drought data gallery soil moisture here we should note that the model s spatial resolution is usually finer than that of the nidis soil moisture dataset the nidis dataset needs to be regridded to match the spatial resolution of our model runs besides the soil moisture values in nidis dataset are provided in the form of water depth unit mm and should be converted to percentage in order to provide initial soil moisture input for our model to do so we need to use the information for the depth of unsaturated surface soil i e depth of vadose zone or groundwater level and the soil porosity as mentioned above the data for groundwater level can be obtained from the nwis dataset while the soil porosity can be determined by soil type for example if the soil moisture of a given grid cell is estimated as 600 mm by the nidis dataset the depth of its groundwater level is 5 m or 5000 mm and its main soil type is silt i e its porosity is 0 493 respectively its soil moisture percent can be calculated as 600 mm 5000 mm 0 493 0 243 i e 24 3 in addition to the above required data it is important to provide information about location of street drains in order to allow the model to interact with the underground drainage systems in urban regions however according to the lafayette consolidated government lcg the drainage system in lafayette parish is mostly above 88 built with roadside open ditches and coulees and only a small portion around 12 of the urban areas are designed with subsurface drains which are also eventually connected to the nearest open ditches or coulees lcg 2018 this means that most of the surface runoffs during flooding seasons are likely to flow into the open ditches or coulees as the subsurface drains are only capable of holding surface water for small regions therefore the capability of these subsurface drains in terms of discharging surface water may be negligible as here we focus on the entire parish of lafayette rather than a small region thus we will not consider the street drains connecting to these subsurface drains in this application i e we assume that there are no street drains for all urban grid cells another reason that we do not consider the street drains is that there are no publicly available data for the locations of street drains in lafayette parish we also need to collect observational data for the 2016 flood in lafayette parish to validate the performance of the proposed model as the heavy rainfalls in lafayette parish occurred on august 12 and 13 2016 we will use the proposed model to simulate the flood extent and depth after august 13 we therefore should collect the observed data for flood extent and depth recorded on august 14 2016 due to the limited data availability for the 2016 flood in lafayette parish here we will estimate the flood depths for four flooded regions in lafayette parish based on some aerial videos recorded by private drones on august 13 and 14 see fig 13 for each aerial video we first need to find the border of a flooded region in the video as the water elevation of this region should be the same as the land elevation of any points on the border as the land elevation is already known see fig 11 the water elevation for this flooded region can be obtained then the difference between water elevation and land elevation for any locations within the flooded region can be regarded as the flood depth the areas with positive flood depth can thus be used to represent the flood extent the estimated water elevations for the four flooded regions in lafayette parish are shown in fig 13 for each labeled point in fig 13 we assume that the water elevation of its surrounding area within a radius of 1 km is equal to that of the labeled point as the water appears still in the aerial videos thus we can derive the flood extent and depth within the 1 km circular area of each labeled point in fig 13 and the results are shown in fig 14 note that the circular areas of west pinhook road and rotary point partially overlap the water elevation for the overlapped area is replaced by the average of the water elevations at these two points besides water depths less than 10 mm above ground are usually not regarded as flooded and thus not considered in this study for estimating flood extent it is also worthwhile to mention that the estimated flood extent and depth based on the above assumption might be unable to capture the floods of some regions where local pits exist for example an empty outdoor swimming pool at the top of a mountain can be filled with water during a heavy precipitation event however the above estimation method is unable to reflect this as the mountain s elevation is often higher than the water elevation of observed flooded regions it is thus very important to keep in mind that the flood maps displayed in fig 14 only represent the local flood extent surrounding the reported points in fig 13 in other words some non flooded areas with local pits might also be flooded in reality 3 3 results and discussions in order to test the performance of the proposed model in simulating the flood extent and depth of the 2016 flood in lafayette parish we run the model over the entire parish with four spatial resolutions 1000 m 500 m 100 m and 50 m the parameters related to land cover and soil texture in this case study are kept fixed as given in tables 1 and 2 note that the values for these parameters i e c d Î± and Î² can be adjusted accordingly as long as they are within the corresponding ranges the purpose here is to test whether the proposed model can reasonably reproduce the 2016 flood in lafayette parish with the default parameters while results with different spatial resolutions are used to help find a threshold of the spatial resolution at which the model can generate reasonable simulations according to the cfl condition the computational time steps for these four model resolutions i e 1000 m 500 m 100 m and 50 m are set as 60 s 60 s 30 s and 15 s respectively note that the time steps for the 1000 m and 500 m model runs could be larger as determined by the cfl condition but here we use 60 s to allow better representation of the quick evolution of urban floods under heavy precipitation in general higher spatial resolutions usually result in higher computational requirements thus it is necessary to find a tradeoff between high computation requirements and reliable simulation results this is especially important for real time flooding prediction and warning which requires as much lead time as possible for evacuation preparation the selection of model spatial resolution is usually based on the following factors 1 the ability in capturing the geographical characteristics of the study area 2 the available resolutions of the input data such as dem land cover use soil texture groundwater and soil moisture 3 the computational requirements as determined by the total number of grid cells and the modeling time step the simulations for the 2016 flood in lafayette parish with four spatial resolutions are shown in fig 15 it can be clearly seen that the spatial patterns of flooded regions with water depth higher than 1 m can be well captured when the model resolution increases from 1000 m to 100 m this is especially true for the vermilion river where the flood depths can reach 3 m or even higher however when the model resolution is further improved from 100 m to 50 m we do not see apparent improvements in terms of capturing flooding spatial patterns for the entire parish of lafayette however as the model resolution is changed from 100 m to 50 m the total number of grid cells within the domain will increase by four times thus the required computational time and storage will become at least fourfold this may suggest that 100 m is probably a reasonable spatial resolution for the proposed model to generate acceptable simulations recent studies suggest that model performance increases with decreasing spatial scales i e improved model resolutions ichiba et al 2018 however if we continue to improve the model resolution after a certain point no apparent improvement or even a possible decline in model performance will be expected this is mainly because most of the input data such as land cover and soil type are not available at higher resolutions ichiba et al 2018 in order to further investigate the effects of model spatial resolution we extract the simulated flood depths with four resolutions at the four flooded regions in lafayette parish and compare them to the observed flood depths see fig 16 note that the observed flood depths are estimated with the 5 m lidar based dem dataset thus the spatial resolution of observed flood depths can also be regarded as 5 m as shown in fig 16 the simulations with resolutions of 1000 m and 500 m are unable to capture the spatial variations of flood depths because of their coarse resolutions when the model resolution is refined to 100 m the spatial patterns of flooded areas in the observations are generally reflected in the simulations if the resolution is further increased from 100 m to 50 m the spatial variations of flood depths are represented with more details this is particularly true for some areas which contain local pits low areas and are not directly connected to the river the flood depths for these areas are not displayed in the observations due to the abovementioned limitations of our estimate methods however most of them are well represented by the 50 m simulations in general it is reasonable to believe that model simulations with higher resolutions e g 25 m 10 m or even 5 m should be more capable of reflecting the spatial variations of local flood depths as displayed in the observations however we should note that model simulations with finer resolutions will lead to substantially increased computational requirements meanwhile the performance in local flooding simulation may not be significantly improved it is thus important to seek a balance point between model performance and computational requirements in order to quantify the performance improvement of the 50 m model simulations relative to the 100 m simulations here we compare these two simulations from three aspects 1 the performance in capturing the observed flood extent 2 the ability of reproducing the observed flood depths and 3 the capability of simulating the reversed flow in vermilion river as for the simulated flood extent we only consider the grid cells with water depth greater than 10 mm the simulated flood extent from each resolution setting is first overlapped with the observed flood extent the overlapped area is regarded as the captured flood extent by the given model simulations the percentage of the capture flood extent to the observed flood extent is defined as the capture rate see fig 17 we can thus use the capture rate to determine which model i e 100 m or 50 m has a better performance in capturing the observed flood extent as shown in fig 16 the 100 m model simulations present relatively higher capture rates in all four flooded regions than the 50 m model simulations suggesting that the 100 m model simulations perform better in capturing the observed flood extent for example the capture rates of the 100 m model at three evaluated locations west pinhook road and rotary point kaliste saloom road and middleburg drive are 96 8 89 3 and 83 9 respectively in contrast the capture rates of the 50 m model at these three locations all decrease slightly i e 95 7 84 8 and 78 2 this is because the 100 m model is spatially constructed with bigger grid cells than the 50 m model and is thus likely to generate larger flood extent in general coarse resolutions enable the model to better capture the observed flood extent but degrade its ability of capturing the spatial variations of local flood extent therefore the above comparison only suggests that the 100 m simulations are comparable to the 50 m ones while the slightly improved capture rates in the 100 m simulations can be attributed to their larger grid cells due to the mismatch of spatial resolutions i e 100 m and 50 m simulations and 5 m observations the ability of reproducing the observed flood depths is evaluated through comparisons of the histograms and general statistics mean maximum and standard deviation of flood depths although we could use spatial interpolation methods to regrid the simulations or observations to match the resolution of observations or simulations in order to perform grid by grid comparisons this would inevitably introduce some errors or uncertainties associated with the interpolation methods therefore comparisons at individual grid cells are avoided here besides only those grid cells within the captured areas shown in fig 17 are considered to avoid introducing any errors caused by the abovementioned estimation method for observed flood depth comparisons of flood depth histograms between simulations and observations are shown in fig 18 while the comparisons of general statistics are listed in table 3 it appears that both the 100 m and 50 m simulations are likely to overestimate the flood depths observed in the regions near the main channel of the vermilion river i e west pinhook road and rotary point in contrast both model simulations are likely to underestimate the observed water depths in the regions near the tributaries of the vermilion river i e kaliste saloom road and middleburg drive this is probably because the lidar based dem used to drive our model was collected many years ago during 2000 and 2004 when we use this dem dataset to simulate the 2016 flood in lafayette parish we have to assume that the water levels of all water bodies just prior to the 2016 flood are exactly the same as those recorded by the earlier dem dataset this assumption is unlikely to hold in many locations as their water levels can vary significantly with time due to the combined effects of many factors such as baseflow from groundwater precipitation and evaporation therefore our model s underestimation of the observed flood depths in the main channel of vermilion river may indicate that the water level in the main stream of the vermilion river before the 2016 flood was higher than the water level recorded in the earlier dem dataset while the overestimation of observed flood depths in its tributaries suggests that their water levels before the 2016 flood are lower than the lidar recorded values however we should note that there might be other reasons leading to the modeling error in urban flood simulations such as the spatial mismatch of input data for dem and land cover the errors introduced by spatial interpolation and the mismatch between rainfall input resolution and model resolution ichiba et al 2018 particularly the point observations of rainfall at one weather station are used as input to drive our flood model implying an assumption that there are no spatial or temporal variations in rainfall over the entire study area throughout the two day event this is a limitation of the case study which might somehow contribute to the model bias in simulating the 2016 flood in lafayette parish in order to quantify the abovementioned systematic biases i e underestimation or overestimation we calculate a relative bias defined as a percentage of the difference between simulations and observations to the observed values see table 3 through the comparisons of three general statistics mean maximum and standard deviation between simulated and observed flood depths we notice that the systematic biases in simulating the mean values of flood depths are further enlarged after the spatial resolution is refined from 100 m to 50 m for example the 100 m model shows a positive bias 61 4 in simulating the average flood depth at the location near west pinhook road and rotary point this positive bias is slightly enlarged to 64 1 by the 50 m model in contrast the 100 m model presents negative biases at other two locations i e 10 4 near kaliste saloom road and 6 7 near middleburg drive these negative biases are further amplified to 15 8 and 37 1 by the 50 m model since the land and soil related parameters here are fixed as their default values this suggests that these parameters should be adjusted through effective calibration before the model resolution is further increased otherwise the model biases are likely to be further magnified as mentioned above these systematic biases are very likely caused by the inappropriateness of using the lidar based water levels to estimate the initial water levels before the heavy rainfall therefore more accurate estimations of the initial water levels in the vermilion river and its tributaries should be obtained and used to drive high resolution model simulations in order to minimize the amplification of systematic biases accurate observations or estimations can usually be obtained with unmanned aerial vehicles uavs or field measurement before heavy precipitation is approaching thus the accuracy of predicting the expected flood extend and depth can be improved significantly nevertheless we do notice some improvements after the model resolution is increased from 100 m to 50 m for example the 50 m model performs better in reproducing the standard deviations of observed flood depths in several regions in detail the relative bias for the standard deviation of flood depth at the location near west pinhook road and rotary point is improved from 8 8 to 2 4 while the location near kaliste saloom road presents an improvement from 45 6 to 32 2 this further confirms that model runs with higher spatial resolutions are more capable of capturing the spatial variations of local flood depth in general given that the model parameters are properly calibrated high resolution simulations are expected to produce more reasonable results finally in order to compare the model s capability of simulating the reversed flow in vermilion river we use the simulated outflow direction and velocity from the 100 m and 50 m model runs to construct two vector maps shown in fig 19 as the reverse flow phenomenon in the vermilion river is usually observed near the rotary point see fig 11 here we mainly focus on the comparisons of the reverse flow direction at this location simulated by these two model runs i e 100 m and 50 m as shown in fig 19 both the 100 m and 50 m model runs can reasonably simulate the reverse flow phenomenon in the vermilion river near rotary point however the 50 m model apparently performs better in terms of reproducing the more detailed eddies in the river channel due to the mixing of surface runoff from different directions in the above analyses we have evaluated the performance of our proposed model in simulating the flood depth flood extent and reverse flow phenomenon in lafayette parish during the 2016 louisiana flood it is worth to mention that the usgs has conducted an in depth study for the 2016 louisiana flood with a flood inundation model watson et al 2017 here we should note that the inundation map generated by usgs is only limited to those low lying areas close to the main river channel as all inundation models are intended for simulating river floods in comparison our model can create a flood map for the entire domain with a reasonable representation of all low lying areas no matter whether or not they are close to the main river channel as for the flood depths the usgs reported that the depths of water at those high water marks ranged from 0 7 to 6 8 ft aboveground approximately 213 2073 mm in comparison the simulated flood depths in the vermilion river by our model seem to be slightly higher than the usgs results see figs 15 and 16 however since those high water marks in the usgs results are usually placed in the riverside rather than in the middle of river channel it is reasonable to believe that the flood depths in the vermilion river should be slightly higher the reported range by usgs in other words our model performs reasonably well in simulating the flood depths as reported by usgs in addition the usgs study reported that flow direction of the vermilion river in the lafayette area can reverse because of the large area of urbanization even though their flood inundation model was incapable of reflecting this reverse flow effect in comparison such a reverse flow effect can be successfully captured by our model particularly the reverse flow effect near the rotary point in vermilion river has been widely reported as shown in fig 19 our model can perfectly capture the reverse flow phenomenon at the rotary point 4 conclusions in this study a new numerical model is developed to bridge the research gap in urban flooding prediction under heavy precipitation the developed divides the study domain into many grid cells without any limitation on the spatial resolution of the grid cell as long as the dem data of the same resolution are available it can simulate both horizontal surface runoff and vertical water flow simultaneously in each time step of model simulation this makes the model capable of reflecting the frequent inflow or outflow interactions among grid cells and capturing the rapid generation of surface runoff in urban areas during heavy rainfall the model can also account for typical characteristics of urban areas such as large scale impermeable surfaces and urban drainage systems more importantly the model uses both the surface elevation and instantaneous surface water depth to dynamically determine the directions of horizontal inflow and outflow for each grid cell during each time step of model simulation this enables the model to capture the reverse flow phenomenon which is commonly seen in flat urban areas during heavy storms in order to demonstrate the effectiveness of the proposed model in predicting real world flooding events we have applied it to reproduce the 2016 flood in lafayette parish the model is run at four spatial resolutions to help find a threshold in model resolution at which the model can generate reasonable simulations the results show that the model simulations at 1000 m and 500 m are unable to reproduce the flooded regions and their spatial distributions because of their coarse resolutions this further confirms that conventional hydrological models with spatial resolutions greater than 1 km are not suitable for urban flooding simulation however if the model resolution is refined to 100 m the spatial patterns of flooded areas in the observations start to show up in the simulations suggesting that 100 m is probably a reasonable spatial resolution for the proposed model to generate acceptable simulations when the model resolution is further improved from 100 m to 50 m we do not see apparent improvements in terms of capturing the large scale flooding patterns but our results do show that the spatial variations of flood depths can be better represented with more details this is particularly true for some areas which contain local pits and are not directly connected to river channels in addition the model simulations at 100 m and 50 m are both capable of reproducing the reverse flow phenomenon observed in the study area however the model simulation with a higher resolution apparently performs better in terms of reproducing the more detailed eddies in river channels due to the mixing of surface runoff from different directions in general it is reasonable to believe that model simulations with higher resolutions should be more capable of reflecting the spatial variations of local flood depths as displayed in the observations however we should note that model simulations with finer resolutions will lead to substantially increased computational requirements meanwhile the performance in local flooding simulation may not be significantly improved it is thus important to seek a balance point between model performance and computational requirements in practical applications this is especially important for real time flooding prediction and warning which requires as much lead time as possible for evacuation preparation in addition we should note that default values for model parameters given in tables 1 and 2 related to land cover and soil texture are used in the case study because our purpose is to test whether the proposed model can reasonably reproduce the 2016 flood in lafayette parish with default settings it is a limitation of our case study as a properly calibrated model might produce even better simulations in practice the values for these parameters i e c d Î± and Î² can be adjusted accordingly as long as they are within the corresponding ranges further studies on the model sensitivity to these parameters should be conducted to better facilitate the selection of appropriate values for different applications it also should be noted that although the proposed model in this study uses square grid cells of same size to discretize the continuous land surface the key idea of the model can be potentially applied to other types of grids e g triangle or hexagon to allow for variable grid size however the numerical implementation of most equations in this study is based upon the assumption of square grid cells appropriate modifications to their numerical implementation are thus required for other types of grids last but not least although the proposed model here is intended for urban flooding prediction it can also be applied for modeling floods in rural areas or natural watersheds where no adjustment to infiltration rate is required declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by university of prince edward island and the natural science and engineering research council of canada the 5 m dem data for lafayette parish are available for public access through louisiana atlas gis platform https maps ga lsu edu lidar2000 the land cover data i e nlcd 2011 are available for public access at https www mrlc gov nlcd2011 php the soil texture data i e ssurgo are available for public access at https websoilsurvey sc egov usda gov app websoilsurvey aspx the data for groundwater level of lafayette parish are derived from the national water information system nwis which is accessible via the usgs water data web interface https waterdata usgs gov nwis the precipitation data for lafayette parish during the 2016 flood are available for public access at https www ncdc noaa gov cdo web the data for soil moisture content are obtained from the national integrated drought information system nidis which is available for public access at https www drought gov drought data gallery soil moisture 
6262,a framework for joint verification of river flow and precipitation ensembles is developed and demonstrated over britain for eventual use in an operational flood forecasting setting the river flow ensembles are obtained from a distributed hydrological model the g2g model using an ensemble of 15 min precipitation accumulations as input on a 1 km grid the precipitation ensemble consists of operational numerical weather prediction nwp forecasts from the met office unified model both hourly and daily precipitation accumulations are verified and the relevance of different accumulation periods discussed in the context of timing errors and hydrological response the implications of precipitation observation error are investigated by comparing verification results from raingauge and radar derived precipitation estimates challenges of verification using only a limited record of precipitation ensembles from a system only relatively recently made operational are addressed methods of obtaining more robust verification statistics given the available ensembles are presented and demonstrated for an example period in december 2015 for precipitation percentile thresholds are used to ensure a given number of threshold crossing events for analysis using a contingency table and derived skill scores for river flow percentiles thresholds are of less relevance to operational flood guidance instead exceedance of a flow threshold of given rarity return period is used as a surrogate measure of flood severity at the regional scale both river flow and precipitation verification analyses are found to be dependent on the locations considered this is linked to variations in precipitation amount for river flows catchment properties and in particular catchment size are found to be a key influence on verification it is demonstrated how such behaviour can be used to obtain more robust river flow verification statistics at sub regional scales keywords verification precipitation river flow ensemble uncertainty operational 1 background and introduction the development of hydrological ensemble systems is an active area of research including investigations into the sources and drivers of uncertainty e g zappa et al 2011 brown et al 2014a b he et al 2015 observation uncertainties e g caseri et al 2016 cecinati et al 2017 and ensemble formulation marty et al 2013 davolio et al 2013 along with a prediction of the expected hydrological flows these ensembles give a measure of the forecast uncertainty allowing the forecasts to be interpreted probabilistically alfieri et al 2011 hardy et al 2016 alfonson et al 2016 the hydrologic ensemble prediction experiment hepex initiated in 2004 is a community of researchers and hydrological practitioners seeking to advance the science and practice of hydrological ensemble prediction e g schaake et al 2007 thielen et al 2008 coupled river flow and numerical weather prediction nwp ensembles are now used operationally across the world e g cloke and pappenberger 2009 and references therein alfieri et al 2014 demargne et al 2014 to provide forecasts guidance and warnings of flooding over britain coupled with nwp ensemble precipitation forecasts from the met office the grid to grid g2g distributed hydrological model moore et al 2006 bell et al 2009 cole and moore 2009 configured at national scale forms a key element of the operational flood guidance provided by the flood forecasting centre ffc over england wales price et al 2012 and the scottish flood forecasting service sffs over scotland cranston et al 2012 cranston and tavendale 2012 these hydrological ensemble systems benefit from recent advances in nwp precipitation ensembles which are now run at sufficiently high resolution that convection can be resolved although not fully captured by the model dynamics e g clark et al 2016 these convection permitting nwp ensembles are produced operationally at a number of forecasting centres including the met office baldauf et al 2011 bouttier et al 2012 hagelin et al 2017 to fully benefit from these ensemble systems it is necessary to understand their performance behaviour and drivers several recent studies have focussed on the verification of short to medium range hydrological ensemble forecasts using example river basins addor et al 2011 zappa et al 2013 brown et al 2014a b probabilistic forecasts from the european flood awareness system have also been recently assessed more generally verifying against a reference simulation with observed fields as input alfieri et al 2014 at the ffc the performance of the overall end to end ensemble flood forecasting system is currently not verified routinely this paper reports first steps towards filling this operational critical knowledge gap the aim is to develop a holistic end to end joint river flow and precipitation ensemble verification framework relevant to the ensembles use in a flood forecasting context an operationally useful subset of existing metrics and methods are selected and discussed along with the definition of thresholds accumulation periods spatial scales and pooling methods relevant in this context to this end the focus here is on flood producing thresholds when evaluating the river flow ensemble and on precipitation thresholds that select the tail of the precipitation distribution when evaluating precipitation ensembles developing a verification framework appropriate for and relevant to flood forecasting is considered a necessary and important avenue of scientific investigation to enable the best use to be made of recent developments in verification metrics namely to pull through the science to the forecasting bench of course many scientific challenges remain including the most appropriate way of pairing precipitation events with the corresponding river flow events both in terms of magnitude and time scale the development of the verification framework is first presented followed by a brief demonstration of its application with results analysis and discussion a 32 day example period going beyond the often presented case study approach is used to put these considerations in context and to demonstrate some of the overarching scientific statistical and technical challenges in this area such an analysis is an important precursor to a long time period verification assessing the end to end forecasting system aligning with the operational setup at the ffc a total of 898 catchments with gauged river flows within england and wales are included in the analysis here omitting 225 catchments over scotland for brevity of presentation the paper is structured as follows first the verification metrics models and data are introduced in section 2 next key verification considerations including the use of thresholds accumulation periods and spatial scales are discussed in the context of a short range operational flood forecasting system in section 3 section 4 demonstrates the verification methods using december 2015 as an example study period further discussion in section 5 is followed by the key conclusions in section 6 2 framework for verification metrics models and data 2 1 verification metrics considered to provide an overview of ensemble performance a range of well established verification metrics and diagrams were selected and applied to verify both the continuous probabilistic forecasts and the binary forecasts of event occurrence in an operational flood forecasting context the use of thresholds on river flow and on precipitation to define the binary forecasts of event occurrence is discussed in section 3 2 the selected metrics and diagrams summarised below for ease of reference give an overview of the ensemble forecast accuracy of course with an increasingly large selection of forecast verification metrics detailed in the hydrometeorological literature other choices could have been made this work focussed on using well established metrics giving a measure of the key ensemble forecast attributes error of the full ensemble distribution probability error reliability resolution potential calibrated skill discrimination and economic value each attribute is defined as the selected metrics are introduced some forecast attributes are evaluated by more than one metric allowing the relative sensitivities of different metrics to precipitation observation uncertainty to be assessed to obtain a measure of the forecast skill in relative terms a metric calculated from the ensemble forecasts can be compared to that calculated for a reference benchmark forecast this gives the skill score for that metric the choice of benchmark forecast depends on the aim of the verification and must be considered in the interpretation of the skill score results for example see pappenberger et al 2015 common benchmarks include climatology persistence and a random forecast 2 1 1 evaluation of continuous probabilistic forecasts to assess the error of the full ensemble distribution a continuous version of the brier score brier 1950 the continuous ranked probability score crps was applied hersbach 2000 the crps expressed in the same units as the observed variable measures the difference between the cumulative distribution estimated by the ensemble forecast and the step function cumulative density function of the observation as indicated in hersbach 2000 the crps is commonly averaged over a number of cases here the crps is averaged over all forecasts and all time periods contained within the forecast lead time range being considered a dimensionless skill score the continuous ranked probability skill score crpss was formed by comparing to the crps calculated from the sample climatology this benchmark was selected to be consistent with that used for the brier score and the evaluation of longer lead time river flow forecasts not presented here at best the crpss takes a value of one and values less than zero indicate the forecast performs worse than the reference the rank histogram talagrand et al 1997 hamill 2001 was used to assess the reliability of the ensemble that is whether or not the ensemble and observations have been drawn from the same distribution a flat rank histogram suggests that the ensemble spread is an appropriate representation of the forecast uncertainty whilst u and domed shaped rank histograms indicate that the ensemble spread is too small and large overall respectively an asymmetric rank histogram indicates that the ensemble is biased 2 1 2 evaluation of forecasts of binary events the brier skill score bss was used to assess the probabilistic forecast skill the bss measures the proportional improvement in mean square probability error as defined by the brier score brier 1950 with respect to a reference forecast in a standard manner see wilks 2011 here the reference forecast is taken as the sample climatology of events occurring over the threshold of interest in this context of binary events the use of a sample climatology is preferred to a benchmark based on persistence at best bss takes a value of one and values less than zero indicate the forecast performs worse than the reference the reliability or attributes diagram wilks 2011 plotting the forecast probability against the probability of the observation given the forecast allows the reliability and resolution of the probability forecasts to be visually assessed resolution is an indication of how much the forecast deviates from the reference with the forecast being more useful if resolution is larger sharper with smaller forecast spread provided the ensemble is reliable a forecast with no resolution has an observed relative frequency equal to the sample climatology plotted as a horizontal line on the reliability diagram in general it is considered good practice to include a sharpness histogram with the reliability diagram showing the sample size for each probability bin when this is the case it is referred to as an attributes diagram typically for a high threshold precipitation or river flow the low probability bins will be populated orders of magnitude more than the higher probability bins this affects and can skew the interpretation the relative operating characteristic roc diagram see for example jolliffe and stephenson 2012 plots for a given threshold paired values of probability of detection pod and false alarm rate f of ensemble forecasts for different probabilities of exceedance the roc diagram measures the discrimination of the forecasts the ability of the forecasts to distinguish between observed events and non events this diagram was also used to assess the potential skill of the ensemble that is the ensemble skill if forecast probabilities were well calibrated a skill score based on the area under the roc curve the roc skill score rocss is defined as the area under the roc curve auc normalised with reference to a random forecast with no skill an auc equal to 0 5 this reference was used to relate directly to the roc diagram a rocss of one indicates a perfect forecast and if above zero the forecast has a skill better than a random forecast the economic benefit of a forecasting system depends on the cost loss ratio of a particular user to assess the economic value of forecasts the relative economic value rev statistic murphy 1977 richardson 2000 wilks 2001 zhu et al 2002 was used the rev is widely used in the verification of both hydrological and meteorological forecasts e g roulin 2006 magnusson et al 2014 and uses information derived from a contingency table e g wilks 2011 to calculate the economic value relative to a forecast based on climatological information a cost loss decision making model is assumed to define the cost for taking action irrespective of whether or not the event occurs and the loss incurred when the event occurs but no action was taken the rev diagram presents the rev for different cost loss ratios the rev has a maximum value of one for a perfect forecasting system a value of zero for forecasts having the same value as climatological information only and is negative for forecasts which have less value than using only climatological information 2 2 meteorological and hydrological models used to focus on the considerations and strategies for joint river flow and precipitation ensemble verification this paper restricts attention to the first 24 h of river flow ensemble forecasts and the corresponding precipitation ensembles the precipitation ensemble consists of nowcasts from the short term ensemble prediction system steps bowler et al 2006 which merges a radar extrapolation nowcast with a spatially downscaled nwp forecast and forecasts from the met office global and regional ensemble prediction system mogreps hagelin et al 2017 bowler et al 2008 run using the operational met office unified model um davies et al 2005 tang et al 2012 steps aims to account for uncertainty in the motion and evolution of radar based precipitation fields mogreps aims to account for uncertainties in meteorological initial and boundary conditions and sub grid processes in the nwp model the river flow ensemble is generated using the g2g distributed hydrological model moore et al 2006 bell et al 2009 cole and moore 2009 2 2 1 best medium range precipitation ensemble best medium range best mr ensemble forecasts of 15 min precipitation accumulations mm per 15 min are produced with uk coverage extending out to over 6 days and issued four times a day these forecasts use the best available precipitation estimates for the lead time period considered in this study the ensemble forecasts are a blend of the 2 km resolution steps extrapolation nowcasting system and the convection permitting 2 2 km mogreps uk for the first 7 h depending on forecast triggering see below and mogreps uk beyond for the operational forecasts considered here december 2015 mogreps uk extends out to 36 h to produce the best mr product all mogreps forecasts are downscaled onto a fixed 2 km grid over the uk the british national grid as used by steps to allow the latest forecast to be available to the ffc the best mr forecasts are triggered based on the time when the required input data from the nwp model are available as opposed to being clock triggered at a fixed time this results in forecast start times which vary by up to three hours to simplify interpretation only best mr forecasts issued at 0100 0700 1300 and 1900 each four hours after the associated mogreps uk run have been used during the study period considered here section 2 4 forecasts issued at these times correspond to around 65 of the total number of forecasts issued the operational best mr forecasts are archived from 25 november 2015 to present this archive reflects the biannual upgrades to the um past forecasts are not re run for a new model version and no hindcast archives are available the best use of such an archive is a key consideration for the operational implementation of an ensemble verification system of flood events and is discussed further throughout this paper 2 2 2 river flow ensemble forecasts using the g2g distributed hydrological model g2g is a physical conceptual distributed hydrological model developed by the centre for ecology hydrology ceh to forecast river flow and surface water flooding moore et al 2006 bell et al 2009 cole and moore 2009 g2g takes account of the effects on grid cell runoff production of land cover and soil geology properties along with antecedent wetness conditions with water flows routed from cell to cell g2g is formulated to represent spatial variability in river flow response to precipitation across a landscape with catchment river basin and countrywide coverage g2g can make full use of spatially distributed precipitation data derived from observation networks of weather radars and raingauges as well as precipitation forecasts from nowcasts nwp models and their combination g2g is in operational use as a countrywide flood forecasting system by both the ffc over england and wales price et al 2012 and by the scottish flood forecasting service sffs across scotland cranston et al 2012 five day outlook forecasts from g2g are used in preparing the flood guidance statements issued by these operational bodies for this study the operational g2g configuration on a 1 km grid and for a 15 min time step is employed price et al 2012 with the period january to march 2008 used for calibration a raingauge based precipitation truth see section 2 3 1 is used in the calibration of g2g and is also used to obtain the initial conditions for each g2g forecast spatial datasets e g terrain soil geology and land cover are used to support its configuration and parameterisation lessening the need for extensive calibration data assimilation of river flow observations helps to maintain realistic model states which are then used to initialise forecasts of river flow flow insertion is applied to correct flows to those observed at gauged river locations and thereby improve the flows propagated downstream a conservative form of empirical state updating uses the observed flow to gradually adjust the g2g water storage upstream of gauged river locations for this work the river flow ensemble forecasts were re run using the operational g2g configuration and the 15 minute accumulation best mr ensemble rainfall forecasts as input instantaneous river flows m3 s 1 were output every 15 min for the 898 river gauging station locations used operationally in g2g over england wales observed river flows are available from the environment agency and natural resources wales for these sites section 2 3 2 no additional uncertainties are incorporated in the river flow ensemble the ensemble only accounts for uncertainty in the input precipitation 2 3 verification data sources 2 3 1 precipitation to consider the effect of observation uncertainties on forecast verification two precipitation truth types are used raingauge based and radar based the raingauge based precipitation truth uses data from the raingauge network across england and wales operated by the environment agency ea and natural resources wales nrw a gridded 1 km raingauge based truth is then calculated by fitting a multiquadric surface with zero offset to the point raingauge observations accumulated to a 15 min interval moore et al 1994 cole and moore 2008 raingauge data have been quality controlled at ceh using the methods presented in howard et al 2012 the radar based precipitation truth is generated using the met office radarnet system harrison et al 2012 which combines 5 min scan data from individual radars and includes data quality control radar data processing includes a raingauge based mean field adjustment constant over the domain of a single radar that uses data from the met office raingauge network applied over a time period dependent on the number of recent raingauge radar pairs available as the radar rainfall data do not relate directly to the 15 minute raingauge data used for the raingauge composite precipitation truth these datasets are considered suitably independent observation sources for verification 2 3 2 river flow data on river flow at 15 min intervals were obtained for the 898 ea and nrw river gauging stations used operationally in g2g over england wales of these catchments around half are less than 100 km2 with 75 less than 250 km2 and 90 less than 700 km2 the catchment response times range from less than an hour to a few days the river flow data have been quality controlled at ceh including visual checks to identify periods of erroneous data consideration of uncertainties in the river flow data in a forecast verification context was beyond the scope of the current study but is recognised as an important avenue of future investigation 2 4 ensemble verification period ensemble verification was undertaken using forecasts starting during the 32 day period from 25 november to 26 december 2015 hereafter referred to as the study period throughout this paper the term sample climatology refers to average values calculated over this period this winter period was very wet as revealed by the december 2015 precipitation anomaly from the 1981 2010 average which exceeded 200 for much of and 300 for parts of wales and northern england fig 3 of mccarthy et al 2016 many record breaking precipitation totals were seen over this period including the highest 24 hour total ever recorded and the second highest rain day rainfall reliably recorded in the british isles burt 2016 flooding associated with two met office and met Ã©ireann named storms occurred over this period 5 to 6 december from storm desmond and 24 to 26 december after storm eva precipitation from these storms fell onto already saturated ground following three storms the previous month resulting in very high river flows flow records for nine catchments held in the national river flow archive set new data era peak flows during this period a number of flow events were assessed to have return periods greater than 100 years and widespread flood damage was suffered across large parts of britain barker et al 2016 marsh et al 2016 3 approach to verification to obtain an overview of ensemble performance national and regional scales must be considered alongside the performance for individual catchments this paper focusses on ensemble verification over england and wales with national scale verification undertaken using data for all 898 catchments for which river flows are used operationally in g2g eight catchment groups defined based on aggregated river drainage basins aligned to wales and environment agency regions over england are used to verify forecasts at the regional scale these catchment groups are shown in fig 1 to facilitate a joint verification of river flow and precipitation it is necessary to match as closely as possible the precipitation and the hydrological response to this end all precipitation verification reported here employs catchment average precipitation calculated from the gridded precipitation ensemble output this differs from the conventional meteorological and currently operational verification of precipitation at either individual observation locations or using a gridded radar product e g mittermaier et al 2013 mittermaier 2014 mittermaier and csima 2017 and is essential for meaningful hydrological comparison one of the primary challenges for joint precipitation and river flow verification is how best to link precipitation events with the corresponding river flow events in terms of both magnitude and time scale in the sections that follow methods of choosing accumulation periods and thresholds for precipitation and river flow are discussed which go some way towards addressing this challenge with the aim of developing a meaningful joint verification framework in an operational flood forecasting context these choices are then discussed further and put into context in section 4 when the verification framework is demonstrated 3 1 thresholds to obtain ensemble verification results that are useful and relevant in an operational flood forecasting context it is necessary to define river flow thresholds which select the flooding events of interest and precipitation thresholds which select relevant precipitation values using these thresholds the observed time series of river flow and precipitation are converted to binary time series where a value of one indicates an observed event and time series of ensemble forecast probabilities whose values indicate the forecast probability of an event occurring these binary and forecast probability time series are then used to calculate the threshold based metrics and diagrams described in section 2 1 namely the bss attributes diagram roc diagram rocss and rev diagram traditionally thresholds for forecast verification are treated differently between hydrological and meteorological communities it could be argued that only precipitation that leads directly to a flood response in the river flow is of interest unfortunately this view is too simplistic as for example the same precipitation totals over the same catchment may not lead to the same river flow response the river flow response is determined by multiple factors which interact non linearly and non systematically thus it is necessary to consider separately the calculation of precipitation and river flow thresholds in this paper we consider two methods of calculating precipitation thresholds and one method for calculating river flow thresholds for the verification of precipitation thresholds are usually specified as either fixed values e g 4 mm h 1 or as percentiles of the total precipitation in the verification domain for example over england and wales at a specific time thus in this method percentiles of the spatial precipitation distribution are used as precipitation thresholds hereafter referred to as spatial percentile thresholds with a new threshold calculated every time the forecasts and observations are compared i e the spatial percentile thresholds vary temporally this method for precipitation thresholds is discussed further in section 3 1 2 an alternative method appropriate at the catchment scale takes percentiles of the temporal distribution of catchment average precipitation values for each catchment thus this second method uses percentiles of the sample climatology for each catchment as time invariant thresholds that vary from catchment to catchment this thresholding methodology denoted temporal percentile thresholds is discussed further in section 3 1 3 the method used for defining river flow thresholds based on return periods is discussed first in section 3 1 1 in addition to the need for different methods of calculating thresholds for river flow and precipitation it is also necessary to consider how best to apply those thresholds for precipitation where high but not necessarily increasing values are relevant for hydrological response it is appropriate to consider threshold exceedance any precipitation values over the threshold are considered to be events however for river flow flood forecasting interest is focussed on the rising flows the start of a potential flood event to focus on these times upward threshold crossings i e the point at which rising river flows first cross a given threshold value are used in this paper to define hydrological events this is discussed further in section 3 1 1 3 1 1 return period based thresholds for river flow here a hydrological threshold is selected as the river flow corresponding to a specific return period where a return period of n years means that there is on average a 1 in n chance of a flood of at least that magnitude occurring in one particular year thus higher return periods correspond to more extreme events and higher flow thresholds return period threshold values from the flood estimate handbook feh institute of hydrology 1999 were scaled to match the g2g median flood equal to q 2 the flow q of return period 2 years calculated over the water years 2007 to 2015 as done operationally for g2g forecasts it is noted that the median flood has a close association with the bankfull discharge for natural rivers for flood guidance purposes the ffc use a 1 km grid of q t values for a range of t values return period in years as a nationally consistent indicator of flood severity when referenced against the g2g flows this approach complements flood thresholds for specific sites associated with actual flooding and used with local models in this study a forecast hydrological event is defined for each ensemble member as at least one upward crossing of a threshold upward threshold crossing occurring anywhere within the forecast lead time range of 0 24 h upward threshold crossings are calculated from the instantaneous river flow data m3 s 1 at two consecutive time steps i e separated by 15 min note that timing uncertainties up to 24 h in the river flow forecasts are tolerated in this analysis for flood guidance purposes there is interest in a given threshold being crossed at any time within the next 24 h this approach is preferred to verifying 24 hour river flow accumulations as it retains the shape and magnitude of the 15 min time step flood hydrograph accommodating timing uncertainties can be particularly important in this analysis where instantaneous river flows are verified section 2 2 2 hydrological forecast probabilities are calculated by taking the average number of events forecast across the ensemble an observed hydrological event is defined as an upward threshold crossing occurring anywhere within the corresponding 24 h observation period an upward threshold crossing is used in preference to a threshold exceedance for river flow to focus on the start of a potential flooding event and accommodating timing errors within a 24 hour period using a threshold exceedance would be particularly inappropriate for less extreme thresholds which may be exceeded for a number of days in an operational verification system several verification thresholds would be used spanning a range of return periods this would allow the ensemble performance at different points in the flood hydrograph to be monitored for meaningful verification statistics it is necessary to consider a large number of events this becomes difficult for thresholds of high return period for this reason the focus here is on one return period threshold Â½q 2 this corresponds to half the river flow of the bankfull level which can be related to the q 2 threshold and is considered to be the minimum threshold of interest in a flood forecasting context it will be shown in section 4 that even for this low threshold sampling uncertainties impact on the verification results 3 1 2 spatial percentile thresholds for precipitation as discussed above spatial percentile precipitation thresholds are calculated from the spatial distribution of all precipitation in the verification area at a particular time for either observations or an individual ensemble member for this study where catchment average precipitation values are used the spatial distribution is formed of the catchment average precipitation values of all catchments in the verification area thus the spatial percentile thresholds focus on the tail of the precipitation distribution for every forecast of course percentile based results are dominated by the use of more modest precipitation accumulation thresholds but at least they allow for higher thresholds to be included when they do occur e g mittermaier et al 2013 spatial precipitation thresholds also allow for a consistent interpretation across the different catchments within the verification area by definition spatial percentile thresholds only select a small number of events leading to large sampling uncertainties however this compromise is necessary to focus on the catchment average precipitation relevant in a hydrological context for the study period used here the 95th percentile was found to be a good compromise given these considerations and is used for all spatial percentile precipitation threshold results thus after application of this threshold to a particular ensemble member precipitation forecast or to the precipitation observations 5 of the catchments considered will be denoted as having an event and allocated the value one and the remaining catchments will have no event allocated a value of zero in this paper the group of catchments used to calculate the spatial percentile thresholds depends on the scale of the analysis being conducted for the national scale analysis all 898 catchments in england wales are used for the regional analysis only catchments within the region of interest are used thus for a particular time the regional analysis will use a different threshold value for each region in fig 1 note that these regional analysis spatial percentile threshold values will also differ from that used for the national scale analysis at this time for analysis at the catchment scale spatial percentiles thresholds calculated from the regional analysis are applied fig 2 shows how the 95th percentile threshold regional analysis values relate to catchment average precipitation values for two example regions the north west region of england predominantly upland shows very heavy precipitation at the 95th percentile up to 10 mm h 1 with four days having over 100 mm of precipitation this highlights the unusual nature of the selected verification period and why this period is relevant for understanding model performance in a flood warning context sample size tends to restrict the computation of verification statistics for fixed thresholds exceeding 4 mm h 1 unless computed over a very long time period the other example region in fig 2 anglian in lowland eastern england shows much lower precipitation thresholds and is much more representative of conditions more generally 3 1 3 temporal percentile thresholds for precipitation as discussed above temporal percentile thresholds are calculated separately for each catchment by taking percentiles of the temporal distribution of catchment average precipitation values thus unlike the spatial percentile threshold approach discussed in section 3 1 1 results using temporal percentile thresholds cannot be consistently compared across different geographical areas in this study the use of temporal percentile precipitation thresholds is limited to that at the catchment scale of course with knowledge of the precipitation values corresponding to each catchment threshold useful insight can still be gained when comparing individual catchment performance maps of the time invariant temporal percentile threshold values calculated from the full 32 day study period 25 november to 26 december 2015 see section 2 4 are shown in fig 3 consistent with the spatial percentile thresholds the 95th percentile value is used here maps are shown for both the raingauge and radar data centre and right and also for an example ensemble member other members lead to similar conclusions the spatial distribution of thresholds is similar for all three agreeing with the results of fig 2 fig 3 shows lower precipitation thresholds to the southeast and higher precipitation values in wales and the northwest of england however the most extreme values shown in fig 2 are lost from the analysis when using these sample climatology thresholds with maximum totals of 7 9 mm h 1 and 129 mm d 1 occurring for the radar data the values for raingauge and ensemble precipitation are slightly lower thus the chances of capturing and evaluating the characteristics of the very highest precipitation totals isolated in time and localised in space are reduced 3 2 accumulation periods from a nwp perspective it is desirable to consider different precipitation accumulation periods as longer accumulation periods have higher forecast skill duc et al 2013 the intensity duration relationship is highly non linear longer accumulation periods do not necessarily imply that it rained for longer periods but larger time windows have the ability of blurring or mitigating against the impact of timing errors the precise definition of the accumulation window can be important though as from a hydrological perspective it can affect the time delay in any catchment flow response to precipitation for example the river flow at the outlet of a large slow response catchment will be linked to precipitation falling further in the past than for a small rapid response catchment here both 24 hour and one hour daily and hourly precipitation accumulations are considered for verification for all metrics considered i e both threshold based and non threshold based thus for each forecast origin there is one comparison of the forecasts and observations when considering a 24 hour accumulation with units mm d 1 or 24 comparisons when considering one hour accumulations with units mm h 1 of course it is still desirable to consider directly the precipitation at the temporal resolution that is used as input to the g2g model although this like for like correspondence between river flow and precipitation time interval is desirable it is considered more important that the precipitation verification should be appropriate to catchment response and meaningful from a flood forecasting perspective future work will extend this study to consider 15 min precipitation accumulations this is a very stringent timing test for the forecasts timing errors are likely to contribute significantly to the overall precipitation forecast error e g mass et al 2002 particularly during winter when frontal systems dominate of course timing and spatial errors in the precipitation field are directly linked e g mittermaier and roberts 2010 dey et al 2016 as discussed in section 3 1 an event for river flow threshold based verification metrics is defined for each ensemble member as at least one upward threshold crossing occurring anywhere within the forecast lead time range of 0 to 24 h thus although the threshold crossings are evaluated using the 15 minute river flow data units m3 s 1 the consideration of timing uncertainties is comparable to that of the analysis of daily precipitation accumulations for the calculation of non threshold based metrics e g crpss rank histogram two methods are applied to account for timing uncertainty in the river flows firstly taking the mean flow over the 24 hour period units m3 s 1 and secondly taking the maximum value over the 24 hour period units m3 s 1 the former evaluates the 24 hour river flow volume and the latter focusses on the highest points in the hydrograph of interest in a flood forecasting context additionally for completeness the 15 min river flows are evaluated directly 3 3 summary of verification approach when comparing any of the methods for precipitation forecast verification with those used for the hydrological ensemble forecasts there are some key differences deserving of further discussion in particular the following points are noted for hydrological forecasts upward threshold crossings are used as this is what is of operational interest for flood guidance and warning systems for precipitation forecasts threshold exceedance is used instead as the hydrological response is determined by high but not necessarily rising precipitation values for hydrological forecasts events are defined when a threshold is crossed at any 15 minute time step in the forecast period of interest here 24 h whereas for precipitation each accumulation period is treated separately thus in terms of the time period considered the performance of the daily precipitation accumulation ensemble links more directly with the river flow ensemble performance however the performance of the hourly precipitation accumulation ensemble relates more directly to the catchment runoff response as the hydrological ensemble is driven by 15 min precipitation accumulations and run at a 15 min time step for hydrological forecasts return period thresholds are used to select flooding events of interest for precipitation two methods of using percentile thresholds are used spatial percentile thresholds calculated from the spatial distribution of catchment average precipitation values varying in time and temporal percentile thresholds time invariant and calculated separately from the temporal distribution of precipitation values for each catchment for metrics calculated using the full ensemble distribution not threshold based e g crpss rank histogram both daily and hourly precipitation accumulations are evaluated for these metrics the 15 min river flow data are evaluated alongside the daily mean and daily maximum river flows this allows the effect of timing uncertainties to be investigated and links made between these metrics and those using thresholds 4 demonstration of verification framework results for example period 4 1 overall analyses to give an overview of ensemble performance forecasts from all catchments in england wales are first considered together for the calculation of verification statistics and associated diagrams fig 4 shows the reliability roc and rev diagrams with bootstrap confidence intervals at the 75th 90th and 99th percentile in grey shading and rank histograms for this overall verification considering the reliability diagrams and associated sharpness histograms showing the sample size for each probability bin it can be seen that the river flow ensemble is over forecasting the probabilities are too high and also over confident larger probabilities are more over forecast this is also seen for the hourly precipitation accumulation ensemble suggesting that the over confidence in the input precipitation ensemble is contributing to the over confidence in the river flow ensemble in contrast the daily precipitation accumulation ensemble shows good reliability for forecast probabilities up to 0 8 by considering daily accumulations the effects of timing errors are reduced this gives an upper band on the ensemble performance for probabilities above 0 8 both the hourly and daily precipitation accumulation ensembles show an increased over confidence thus the raingauge based precipitation used as truth for fig 4 is not capturing the highest precipitation values as frequently as they are forecast possibly due to the extreme precipitation values not occurring at raingauge locations signalling observation error in the form of raingauge representativity all three sharpness histograms show a higher forecast relative frequency for low forecast probabilities as expected there is generally a low chance of the threshold being crossed river flow or exceeded precipitation for river flow there is also a slight increase in the forecast relative frequency for the highest sharpness histogram bin thus at times when it is likely that a threshold will be crossed it is more common for the majority of ensemble members to predict this event than for the ensemble to be split between members that do and do not capture the event it is possible that for the short and abnormally wet study period considered here section 2 4 the river flow sharpness histogram is influenced by flooding events with flows rising much higher than the Â½q 2 threshold note that as the reliability diagram shows these high probability forecasts to be forecast too frequently the ensemble is over confident and over spread the observed increase in sharpness does not lead to higher forecast accuracy the effects of sampling uncertainty can be seen in both the river flow and the daily precipitation accumulation results with the larger bootstrap uncertainties seen for 24 h accumulations this difference is thought to be due to the different approaches to thresholding the river flow and precipitation accumulations in particular the choice of absolute value river flow thresholds indicating possible flood events results in a much smaller sample of river flow threshold crossings and higher sampling uncertainties it may perhaps be surprising to see that the daily precipitation confidence intervals are wider than those for hourly precipitation this is primarily because daily precipitation accumulations can span a wider range of values mm d 1 the range of hourly precipitation accumulation values mm h 1 is generally much less the exception being precipitation that leads to flash floods on short time scales the larger sample size available for analysing hourly precipitation accumulations will also contribute to the narrower uncertainty bands like those for river flow precipitation probabilities tend to be over confident especially for larger probabilities the roc diagrams indicate high potential skill for both the river flow and precipitation ensembles agreeing with the reliability diagrams the highest potential skill is seen for the daily precipitation accumulations the river flow ensemble shows higher potential skill than the hourly precipitation accumulation ensemble suggesting that re calibration of the river flow forecast probabilities in particular could lead to improved performance for all three ensembles the rev diagrams show positive rev over a range of different cost loss ratios with higher probability thresholds having lower rev values but over a larger range of cost loss ratios comparing the river flow and precipitation rev diagrams it is seen that overall the river flow ensemble has a narrower envelope of cost loss ratios with positive rev than for precipitation the Â½q 2 threshold river flow ensemble forecasts have comparable economic value to the daily 95th percentile threshold precipitation forecasts though the daily precipitation forecasts show somewhat higher rev for high cost loss ratios the hourly precipitation forecasts show a smaller envelope of positive rev with a lower peak and for a smaller range of cost loss ratios interestingly this difference was not seen when comparing the roc curves it occurs only when considering the cost loss ratio the lower potential skill indicated by the roc diagram and lower rev for hourly accumulations is tied to the spatial constraints applied in this analysis where the precipitation is expected to occur in the right place catchment at the right time even though the precipitation forecast is an ensemble any mismatches in space and or time are accentuated for shorter accumulation periods for higher precipitation thresholds not shown the rev curves are more similar to those for the Â½q 2 river flow threshold suggesting that these differences may also be due to differences in the thresholding methods this highlights the need for a thorough understanding of both river flow and precipitation verification methods for a meaningful joint verification thresholding differences do not exist for the rank histograms which are calculated from the full ensemble distribution rank histograms show larger differences between river flow and precipitation ensemble performance the river flow ensemble rank histograms show the observations falling in the lowest bin the ensemble over predicting river flow over 40 of the time and into the highest bin the ensemble under predicting river flow around 30 of the time when instantaneous 15 minute river flows are used similar results are obtained using the daily maximum and daily mean instantaneous river flows shown by the hashed bars in fig 4 although the relative population of the highest and lowest bins changes slightly this suggests that timing uncertainties from the use of instantaneous river flows are not causing the strong under dispersion seen in the river flow rank histograms instead the under dispersion is thought to relate to several different factors as discussed in section 2 2 2 the river flow ensemble only takes account of rainfall uncertainty other forms of uncertainty such as model uncertainty in representing the hydrological processes may be important to accurately capture the ensemble dispersion e g brown et al 2014b it is possible that the unusual nature of the verification period section 2 4 also acts to highlight the river flow ensemble under dispersion additionally although the ensemble is not reproducing the range of observed values and so is in an overall sense under spread a similar effect could also be caused by conditional biases in the ensemble forecasts for example if an ensemble had a high bias for half of the forecasts evaluated and a low bias for the other half the rank histogram from all evaluated forecasts would show higher populations for both the lowest bin from the high biased forecasts and the highest bin from the low biased forecasts it is interesting that the ensemble under and over predicts the flow values given that the forecast probabilities were seen from the reliability diagrams to be overestimated only this suggests that although the high flows irrespective of absolute magnitude are overestimated by the ensemble giving an over confidence in predicting threshold crossings the low flows are underestimated that is the ensemble flows are too peaky of course the overall quality of the hydrological simulation also impacts the ensemble performance as a physically based model which conserves water balance g2g does not contain a bias correction term and the ensemble forecasts for some sites will have high low bias this would also show in the rank histograms as a conditional bias contributing to a u shaped rank histogram future work will investigate the use of post processing to bias correct the river flow ensemble members even when river flow data assimilation is used and the input rainfall ensemble is reliable recent studies e g bourgin et al 2014 have shown that post processing is needed to obtain reliable river flow ensembles although more uniform than the river flow rank histograms those for precipitation also show observations falling too frequently in the ensemble extremities at the high end of the ensemble for hourly accumulations and at the low end of the ensemble for daily accumulations however these differences in precipitation rank histograms were found to be highly sensitive to the precipitation observation type and should hence be treated with caution rank histograms in particular are known for being sensitive to observation uncertainty e g hamill 2001 but other diagnostics which can be related to the distribution can also be affected fig 5 shows the equivalent precipitation ensemble verification diagrams as fig 4 but using a radar based precipitation truth as the river flow results are not evaluated for different precipitation truths they are unchanged from fig 4 and are not repeated in fig 5 for brevity of presentation overall the radar based reliability diagrams give a similar message to those with a raingauge truth the ensembles are over forecasting and overconfident however there are differences particularly for the daily precipitation accumulations which show much poorer reliability when a radar based truth is used for a radar based truth performance is similar across the full range of probabilities for both truth types the relationship between hourly and daily precipitation reliability diagrams is similar only subtle differences are seen in the roc and rev the radar based daily precipitation accumulation rank histograms in fig 5 suggest the smallest accumulations occur more frequently compared to the raingauge based rank histogram in fig 4 though both are suggesting a dry bias and insufficient spread for the hourly accumulations the shape of the rank histogram changes more dramatically looking fairly well spread based on the radar rainfall accumulations in fig 5 whilst for gauge rainfall in fig 4 it shows that observations fall in the largest accumulation bin more frequently that is the same forecast against a different observation has a wet bias under forecasting lighter precipitation according to the raingauge observations this suggests an interesting dynamic between hourly and daily precipitation the differences may simply be due to the temporal granularity as an hourly accumulation may not be a good fit when it comes to defining events whereas on the daily time scale events are generally less susceptible to timing errors unless it is a very long duration event that is less likely to straddle adjacent time periods with detrimental impact these results highlight both the benefit of considering multiple verification diagrams and also the importance of considering observation uncertainty 4 2 regional scale verification the ensemble performance is found to vary considerably at the regional scale fig 6 shows reliability and roc diagrams stratified by region for river flow and hourly precipitation accumulation with shading showing bootstrap confidence intervals at the 99th percentile daily precipitation results were found to be similar to those for hourly accumulations and are not included here overall higher reliability is seen for precipitation than for river flow although it is expected that 15 min precipitation accumulations not considered here would have lower reliability than daily or hourly ones it is not thought that this would fully account for these differences for river flow there is more regional variation in reliability than for precipitation with regions to the south east of the country having much lower reliability when considering river flow this is partly explained by the large sampling uncertainties seen for these regions as shown by the bootstrap confidence intervals for these regions there are so few river flow threshold crossings even for the Â½q 2 threshold that the range of possible reliability goes from very poor to good hydrological differences between the regions are also thought to contribute to the greater regional variation for the river flow ensemble performance comparing the individual region performance there is not a direct correspondence between the river flow and precipitation reliability for example the north east and north west of england regions perform best for river flow but midlands and wales perform best for precipitation anglian performs worst for both river flow and precipitation these differences highlight again the dependence of the river flow ensemble behaviour on hydrological processes controlling runoff production water storage and translation the river flow ensemble is not a simple transformation of the rainfall ensemble members and the river flow ensemble performance cannot be directly estimated in a simple manner from that of the rainfall ensemble the roc diagrams stratified by region show higher potential skill for river flow than for hourly rainfall accumulations across the majority of regions agreeing with the national scale analyses section 4 1 exceptions to this are the thames and anglian regions where the river flow analyses are dominated by sampling uncertainties with rivers being generally less responsive to precipitation a similar regional dependence is seen in the roc and reliability diagrams 4 3 verification at the catchment scale the crpss as fewer catchments are used to calculate threshold based verification statistics the effective sample size decreases and sampling uncertainties increase in this section the spatial distribution of the crpss scores calculated for individual sites are considered as the crpss is calculated from the full ensemble distribution it is less targeted to the flood forecasting context than the threshold based metrics however a brief analysis of the crpss serves as a useful complement to the threshold based scores through giving a better understanding of the ensemble performance as a whole additionally as the crpss is calculated from the full ensemble distribution for all forecasts in the verification period the crpss is less influenced by sampling issues and the catchment scale performance can be better evaluated this is important in an operational context where flooding cases may cover only a small number of catchments fig 7 shows maps of crpss for instantaneous 15 minute river flows and for hourly and daily precipitation accumulations results for daily mean and daily maximum river flows are very similar to those for the instantaneous river flows leading to the same discussion and conclusions and are not included here for brevity of presentation all crpss values were formed by comparing crps calculated from all ensemble forecasts in the sampling period to the crps calculated from the sample climatology as defined in section 2 1 1 for the majority of catchments the ensemble forecasts are more skilful than the sample climatology with positive crpss values there are a few exceptions for river flow predominantly in the midlands region fig 1 and corresponding to catchments with unnatural flow regimes artificial influences such as abstractions discharges and reservoirs which are not represented in detail in g2g for river flow little consistent variation is seen in crpss values across england wales this suggests that the influence of non location specific catchment properties such as catchment size are influencing the crpss more than locally consistent catchment properties in contrast the precipitation crpss values show clear spatial variations which can be linked to the distribution of precipitation accumulations e g poorer skill in the south and east of england where smaller accumulations were experienced during the winter period and there was less deviation from the sample climatological values thus the spatial variations seen in the precipitation crpss scores is related to the use of a reference based on the sample climatology this relationship will be investigated further when the verification framework is applied to longer study periods using longer climatological references overall precipitation skill is spatially more coherent purely because the atmosphere is a continuum and inhomogeneous catchment properties aside from relief remain irrelevant until the precipitation has reached the ground overall similar results are obtained from raingauge and radar based truths any differences in the northwest and southwest corners of both england and of wales can be linked to the extent and quality of radar coverage in these areas without differences in thresholding methods a more direct comparison can also be made between river flow and precipitation ensemble crpss values the river flow crpss values are calculated from g2g modelled flows using 15 min precipitation data as input ideally these would be compared with 15 min accumulation precipitation crpss values however this was not possible in this study due to data processing constraints instead the river flow crpss values are compared to those calculated from both precipitation hourly and daily accumulations this gives an indication of the dependence of the precipitation crpss on the temporal resolution used and allows for an informed comparison with the river flow verification from scatter plots of the crpss for river flow against raingauge based precipitation shown in fig 8 it is seen that a smaller range of crpss values are obtained for precipitation than for river flow thus the river flow ensemble is being influenced by hydrological effects in addition to the precipitation uncertainty input through the precipitation ensemble moving from daily to hourly accumulations the standard deviation of the precipitation crpss values decreases slightly this narrowing of the range of crpss precipitation scores for the shorter accumulation period is initially somewhat unexpected but is related to the crps being in the same units as the variable that is being verified daily precipitation totals have a larger range compared to hourly ones giving a larger range of crpss magnitudes as the crps calculated from the ensemble forecasts will vary more between catchments than the crps of the sample climatology reference this larger precipitation range for the daily accumulations will also result in a larger range of crpss values the hourly scores are lower which is expected given that timing errors will have a larger impact at this temporal granularity also shown in fig 8 are the q q quantile quantile plots of the crpss for river flow against that for raingauge based precipitation by considering only quantiles of the crpss distributions attention is focused at the regional scale the relationship between individual catchments is no longer preserved the scores are presented in an ordered or ranked fashion and show the range of values for a region for both the precipitation and the river flow the q q plots show particularly for higher crpss values indicating higher skill an almost linear relationship between the river flow and precipitation crpss values for lower crpss values this relationship is less clear a range of river flow crpss values are seen for a given precipitation crpss value similar results not shown are obtained when comparing against a radar based precipitation truth 4 4 pooling of river flow data by catchment size given the dependence of the river flow crpss on non location specific catchment properties as shown in fig 7 and discussed above the relationship between various catchment properties e g catchment size terrain slope and sub catchment properties and threshold based verification scores was investigated fig 9 shows river flow reliability and roc diagrams calculated using all catchments in england and wales pooled by catchment size 5 pooling groups each containing around 180 catchments this catchment property was found to relate directly to the threshold based verification scores in particular the reliability diagram shows a clear trend of reliability decreasing with decreasing catchment size for forecast probabilities up to 0 4 these differences are larger than the 90th percentile bootstrap sampling uncertainties this trend agrees with that found in alfieri et al 2014 a similar trend is seen in the roc diagram for the four largest catchment size groups however the group of smallest catchments does not follow this trend and shows performance similar to the group of largest catchments similar conclusions were found using more catchment groups best performance was seen on the roc diagram for the smallest and largest catchments with middle sized catchments performing worse the rev curves partitioned by catchment size not shown also lead to similar conclusions this feature is unexpected generally larger catchments are expected to perform better than smaller ones and may be related to the unusual verification period considered here with many cases of large scale heavy precipitation and extreme flood events section 2 4 future work using a longer verification period will aim to disentangle this 4 5 verification at the catchment scale threshold based scores sampling uncertainties at individual sites are expected to be large particularly for river flows using flood producing thresholds hence in addition to considering threshold based river flow results at individual sites the relationship between catchment size and river flow ensemble performance section 4 4 is exploited by pooling the data from several catchments within a given geographic region the aim is to exploit the clear relationship between river flow ensemble performance and catchment area shown in fig 9 to reduce the river flow sampling uncertainties for the calculation of threshold based verification metrics at sub regional scales as similarly sized catchments have similar ensemble performance for river flow data from similarly sized catchments can be sensibly combined to calculate verification statistics thus reducing sampling uncertainty as the river flow ensemble takes precipitation input which varies coherently across geographic regions as discussed in section 4 3 with reference to the precipitation crpss maps it is also important to retain some regional variability in the river flow verification statistics here the aggregated river drainage basins regions shown in fig 1 are used use of regions based on hydro climate or broad scale landscape features are other options for regional pooling that might be considered in future work within a given region the catchments are ranked by catchment size for each catchment within that region data from a fixed number of other catchments with closely ranking areas are also used to calculate the verification scores thus a moving catchment size bin is used centred upon the size of the catchment of interest the width of this moving bin is defined based on a fixed number of catchments for example 15 sites larger and 15 sites smaller than the catchment of interest irrespective of the distribution of catchment sizes this option is used here as it ensures the same amount of data is used for each calculation another option would be to fix the range of catchment sizes to include in the moving bin for example to consider all sites within 200 km2 of the catchment of interest due to the long tailed distribution of catchment sizes this would result in a large number of catchments falling within the moving catchment bin for small catchments and only one or two catchments falling within the moving catchment bin for large catchments hence this is not a sensible option to use the number of sites to use for each pool of data is a compromise between retaining all the catchment scale information but not reducing sampling uncertainty at all and losing all the catchment scale information i e evaluating at the regional scale but reducing the sampling uncertainty to investigate the effects of pool size moving bins consisting of 11 21 31 41 and 61 sites were considered by comparing the rocss and bss values for all these options it was found that large differences were seen when moving from a 1 to 11 11 to 21 and 21 to 31 site pool however when increasing the pool size further the bss and rocss results converged and smaller differences were seen hence a pool of 31 sites was considered appropriate in fig 10 river flow bss and rocss results are mapped both for individual sites and using a moving catchment size pool of 31 sites within each region note that for precipitation accumulations local properties dominate and pooling by catchment size is not sensible instead it is necessary to consider individual site scores in the context of the scores obtained for neighbouring sites from fig 10 it can be seen that when using individual site data only it is not possible to calculate marked by hatching the rocss and bss at a large number of sites due to a small sample size by applying an area based pooling the values are more consistent between neighbouring locations note that the pooling does not affect the overall interpretation of the maps at national and regional scales for example differences seen between the rocss and bss maps are consistent between the no pooling and pooling results in fig 10 this is also true when a different number of catchments e g 11 21 are used in the catchment size pool not shown for brevity of presentation of course area based pooling only has an effect if there are threshold crossings at some sites within a given area pool hence although pooling has been found to be a useful method of obtaining more meaningful information from a small sample of hydrological events it is still reliant on having some hydrological events in this study the bss and rocss were calculated at all sites or pools of sites where any events occurred i e when there was at least one event in the verification period this method was used to retain the maximum possible amount of information from as many sites as possible of course it also results in some scores being calculated from a very small number of events particularly in regions to the southeast of england in future work with the benefit of a longer verification period the use of a higher sample size cut off for scores calculation will be investigated precipitation accumulation bss and rocss results calculated using spatial percentile thresholds for daily and hourly accumulations are shown in fig 11 both raingauge and radar based precipitation are considered as truth sampling effects are responsible for the lack of scores for many catchments as indicated by the hatching as discussed in section 3 1 2 this resulted from the thresholds being calculated from the catchment average values in a region such that by definition only the catchments with mean precipitation totals in the top 5 are verified the exceptionally wet period meant that these thresholds can be locally very high however on a larger multi catchment scale the precipitation lacked the spatial uniformity of intensity to exceed such thresholds over multiple catchments the net result is that many catchments did not receive the required amount of precipitation to have a sufficient sample for verification the bss is overwhelmingly negative skill worse than sample climatology with a scattering of catchments with small positive skill scores the rocss score shows more regions with skill with higher levels of skill for the daily precipitation accumulations overall there are strong similarities in the signals from daily and hourly accumulations for day 1 with the daily showing some additional skill results between radar and gauge rainfall accumulations are surprisingly similar in terms of spatial patterns with some notable exceptions e g mid wales bss and rocss results using the second thresholding methodology for precipitation accumulations at the catchment scale using temporal percentile precipitation thresholds section 3 1 3 are shown in fig 12 here as the number of threshold exceedances is fixed for each catchment the effects of sampling uncertainty are less and scores can be calculated for all catchments both the bss and rocss are seen to vary smoothly across the country with more skill higher scores seen to the northwest and lower scores to the southeast scores are much higher than those using spatial percentile thresholds fig 11 reflecting the less extreme values of precipitation that are being verified thus for lower precipitation thresholds further from the tail of the precipitation distribution the precipitation ensemble performs better however to understand the precipitation ensemble performance in a flood forecasting context it is necessary to focus on the tail of the precipitation distribution thus despite the presence of sampling uncertainties it is necessary to use other methods such as the spatial percentile thresholds used in fig 11 in this context of course to reduce sampling uncertainties verification should be performed over a longer verification period however given the extreme nature of precipitation and river flow events of interest in a flood forecasting context it is likely that sampling uncertainty and methods of its reduction will remain an important consideration in ensemble verification for both river flow and precipitation using either thresholding methodology higher values of rocss than bss are obtained suggesting that biases in the forecast probabilities are reducing the overall ensemble skill it is therefore conceivable that some post processing of the precipitation totals and probabilities could be of benefit in general the bss values for river flow are more similar to those for precipitation when calculated with spatial percentile thresholds for catchments where the scores could be calculated than with sample climatology based thresholds this is expected as the spatial threshold method evaluates more directly situations where flooding is more likely to occur for the rocss this is less clear with the river flow rocss values generally sitting between those calculated for the two precipitation thresholding methodologies higher than those for spatial percentile thresholds but lower than for temporal percentile thresholds 5 discussion using a sample one month case study period december 2015 this paper has demonstrated important considerations for a joint verification framework for hydrological and meteorological ensemble forecasting systems the sample represents a very short and atypical period good for getting enhanced sampling of flood producing rain but not generally representative the focus has been on verification information relevant in an operational flood forecasting context to gather meaningful statistics that are more representative of all possible scenarios the verification will need to be run over extended operational periods before overall conclusions on true skill can be drawn due to the infrequency of flood events sampling uncertainties are inherent in the verification of flood forecasting ensembles for example as discussed by cloke and pappenberger 2009 by considering lower thresholds more robust statistics can be obtained but such results may be of lesser practical relevance to those using flood forecasting ensembles for operational decision making individual case studies of flooding events for specific catchments can help to provide some confidence in forecast performance however in a probabilistic sense no meaningful evaluation of individual case study performance is possible in this paper a short verification period of 32 days was used moving beyond the individual case study approach this very wet period was selected so as to contain a number of flood events and thus to enable ensemble verification to be examined in a flood forecasting context of course by definition this means that it is not a representative period and model performance over this period will not be indicative of the model performance in less extreme cases results were presented for the lowest threshold considered appropriate in a flood forecasting context equivalent to half the flow of the two year flood event methods of increasing the sampling size were discussed throughout this paper in particular the consideration of a larger number of catchments grouped nationally and regionally the use of percentile thresholds for precipitation and the pooling of river flow results based on catchment size were presented as possible methods of reducing sampling uncertainties to a manageable level the performance of precipitation forecasts is very strongly dependent on the choice of thresholds used in the calculation of skill spatial percentile thresholds can often lead to skill metrics being dominated by non extreme events they pick up what happens in a particular accumulation period which most of the time is not extreme based on the roc and reliability diagrams and skill scores used here the skill and performance of the precipitation ensemble is good however as the exceptionally wet period used in this paper shows when the sampling does fall in the tails of the distribution the performance of the precipitation forecasts shows certain weaknesses for the shorter accumulation periods the lack of skill could well be dominated by timing errors which are not accounted for here therefore various sources of sampling uncertainty will always feature as a key consideration when interpreting verification results from flood forecasting ensemble systems this is particularly true for operational nwp ensemble systems such as the mogreps where long term hindcasts are not produced and weather models are in continuous development for forecast performance to be better understood it is vital that the associated sampling uncertainties are fully and comprehensively conveyed to operational forecasters and decision makers some form of post processing of the precipitation accumulations bias correction in the first instance and potentially also a subsequent probability calibration could add benefit however post processing precipitation can be challenging due to the non gaussian nature of its distribution e g scheuerer and hamill 2015 ben bouallÃ¨gue 2013 bentzien and friederichs 2012 for flood forecasting to benefit from a joint hydrological meteorological ensemble verification relevant and physically meaningful time scales must be considered and sources of uncertainty identified to this end precipitation results were presented using both hourly and daily accumulations to encompass the effects of catchments with different hydrological response times each precipitation accumulation period was considered separately in the verification for hydrological ensembles to focus on hydrological threshold crossings in a flood warning context hydrological events were defined when a river flow threshold was crossed at any 15 minute time step in the 24 hour forecast period of interest for precipitation the two truth types raingauge based and radar based were used to quantify the effects of precipitation observation uncertainty on the verification results the rank histogram was found to be particularly sensitive to this observation uncertainty reliability diagrams also showed sensitivity to the precipitation truth type with a raingauge truth suggesting high probabilities were particularly underestimated although not seen for the hourly precipitation results daily accumulations appeared to be much more reliable when comparing to raingauge data as only one set of river flow observations are available consideration of river flow observation uncertainty is a more involved process beyond the scope of this current study but an important topic for future work for the short verification period used in this study probabilistic forecasts derived from both the river flow and the precipitation accumulation ensembles tended to be over confident with over confidence increasing with forecast probability this is related to the lack of reliability in the forecast probabilities the river flow ensemble was found to be more severely under spread than the precipitation accumulation ensemble according to the rank histogram this suggests that unaccounted for uncertainties in the hydrological modelling process the river flow ensemble considers precipitation uncertainty only may be important for forecast accuracy in agreement with the conclusions of brown et al 2014b hydrological uncertainties will be considered in future work 6 conclusions from the investigations and analyses presented above the following key conclusions can be drawn for the full evaluation of operational flood forecasting ensembles it is necessary to consider both precipitation and river flow ensembles in a joined up manner differences in the physical nature of precipitation and river flow require consideration and lead to different verification solutions and interpretations examples have been presented of pooling river flow verification analyses but not precipitation analyses based on catchment properties and using percentile thresholds when verifying precipitation accumulations different spatial scales e g national regional and sub regional where possible should be considered to give informed and physically relevant information about the ensemble s performance this study has highlighted the varying effects of sampling uncertainty at different scales and the information that can be gained from a multi scale analysis to obtain a representative and unbiased view of ensemble performance it is necessary to use a range of metrics for both river flow and precipitation verification this is particularly important given the differing sensitivities of verification metrics to observation error in this study sensitivity to precipitation observation error was exemplified by the rank histogram these conclusions will form the basis of future work seeking an end to end ensemble verification framework relevant to operational flood forecasting ensembles in particular the results of this initial detailed case study over a 32 day period will allow appropriate choices to be made when considering longer datasets and different hydrological ensemble systems declaration of competing interest none declared acknowledgements the authors thank the two anonymous reviewers and associate editor maria helena ramos for their detailed comments that helped improve the quality and clarity of this paper the work reported on here formed part of the rainfall and river flow ensemble verification project commissioned by the flood forecasting centre on behalf of the environment agency scottish environment protection agency and natural resources wales preparation of this paper was made possible through centre for ecology hydrology and met office science funding 
6262,a framework for joint verification of river flow and precipitation ensembles is developed and demonstrated over britain for eventual use in an operational flood forecasting setting the river flow ensembles are obtained from a distributed hydrological model the g2g model using an ensemble of 15 min precipitation accumulations as input on a 1 km grid the precipitation ensemble consists of operational numerical weather prediction nwp forecasts from the met office unified model both hourly and daily precipitation accumulations are verified and the relevance of different accumulation periods discussed in the context of timing errors and hydrological response the implications of precipitation observation error are investigated by comparing verification results from raingauge and radar derived precipitation estimates challenges of verification using only a limited record of precipitation ensembles from a system only relatively recently made operational are addressed methods of obtaining more robust verification statistics given the available ensembles are presented and demonstrated for an example period in december 2015 for precipitation percentile thresholds are used to ensure a given number of threshold crossing events for analysis using a contingency table and derived skill scores for river flow percentiles thresholds are of less relevance to operational flood guidance instead exceedance of a flow threshold of given rarity return period is used as a surrogate measure of flood severity at the regional scale both river flow and precipitation verification analyses are found to be dependent on the locations considered this is linked to variations in precipitation amount for river flows catchment properties and in particular catchment size are found to be a key influence on verification it is demonstrated how such behaviour can be used to obtain more robust river flow verification statistics at sub regional scales keywords verification precipitation river flow ensemble uncertainty operational 1 background and introduction the development of hydrological ensemble systems is an active area of research including investigations into the sources and drivers of uncertainty e g zappa et al 2011 brown et al 2014a b he et al 2015 observation uncertainties e g caseri et al 2016 cecinati et al 2017 and ensemble formulation marty et al 2013 davolio et al 2013 along with a prediction of the expected hydrological flows these ensembles give a measure of the forecast uncertainty allowing the forecasts to be interpreted probabilistically alfieri et al 2011 hardy et al 2016 alfonson et al 2016 the hydrologic ensemble prediction experiment hepex initiated in 2004 is a community of researchers and hydrological practitioners seeking to advance the science and practice of hydrological ensemble prediction e g schaake et al 2007 thielen et al 2008 coupled river flow and numerical weather prediction nwp ensembles are now used operationally across the world e g cloke and pappenberger 2009 and references therein alfieri et al 2014 demargne et al 2014 to provide forecasts guidance and warnings of flooding over britain coupled with nwp ensemble precipitation forecasts from the met office the grid to grid g2g distributed hydrological model moore et al 2006 bell et al 2009 cole and moore 2009 configured at national scale forms a key element of the operational flood guidance provided by the flood forecasting centre ffc over england wales price et al 2012 and the scottish flood forecasting service sffs over scotland cranston et al 2012 cranston and tavendale 2012 these hydrological ensemble systems benefit from recent advances in nwp precipitation ensembles which are now run at sufficiently high resolution that convection can be resolved although not fully captured by the model dynamics e g clark et al 2016 these convection permitting nwp ensembles are produced operationally at a number of forecasting centres including the met office baldauf et al 2011 bouttier et al 2012 hagelin et al 2017 to fully benefit from these ensemble systems it is necessary to understand their performance behaviour and drivers several recent studies have focussed on the verification of short to medium range hydrological ensemble forecasts using example river basins addor et al 2011 zappa et al 2013 brown et al 2014a b probabilistic forecasts from the european flood awareness system have also been recently assessed more generally verifying against a reference simulation with observed fields as input alfieri et al 2014 at the ffc the performance of the overall end to end ensemble flood forecasting system is currently not verified routinely this paper reports first steps towards filling this operational critical knowledge gap the aim is to develop a holistic end to end joint river flow and precipitation ensemble verification framework relevant to the ensembles use in a flood forecasting context an operationally useful subset of existing metrics and methods are selected and discussed along with the definition of thresholds accumulation periods spatial scales and pooling methods relevant in this context to this end the focus here is on flood producing thresholds when evaluating the river flow ensemble and on precipitation thresholds that select the tail of the precipitation distribution when evaluating precipitation ensembles developing a verification framework appropriate for and relevant to flood forecasting is considered a necessary and important avenue of scientific investigation to enable the best use to be made of recent developments in verification metrics namely to pull through the science to the forecasting bench of course many scientific challenges remain including the most appropriate way of pairing precipitation events with the corresponding river flow events both in terms of magnitude and time scale the development of the verification framework is first presented followed by a brief demonstration of its application with results analysis and discussion a 32 day example period going beyond the often presented case study approach is used to put these considerations in context and to demonstrate some of the overarching scientific statistical and technical challenges in this area such an analysis is an important precursor to a long time period verification assessing the end to end forecasting system aligning with the operational setup at the ffc a total of 898 catchments with gauged river flows within england and wales are included in the analysis here omitting 225 catchments over scotland for brevity of presentation the paper is structured as follows first the verification metrics models and data are introduced in section 2 next key verification considerations including the use of thresholds accumulation periods and spatial scales are discussed in the context of a short range operational flood forecasting system in section 3 section 4 demonstrates the verification methods using december 2015 as an example study period further discussion in section 5 is followed by the key conclusions in section 6 2 framework for verification metrics models and data 2 1 verification metrics considered to provide an overview of ensemble performance a range of well established verification metrics and diagrams were selected and applied to verify both the continuous probabilistic forecasts and the binary forecasts of event occurrence in an operational flood forecasting context the use of thresholds on river flow and on precipitation to define the binary forecasts of event occurrence is discussed in section 3 2 the selected metrics and diagrams summarised below for ease of reference give an overview of the ensemble forecast accuracy of course with an increasingly large selection of forecast verification metrics detailed in the hydrometeorological literature other choices could have been made this work focussed on using well established metrics giving a measure of the key ensemble forecast attributes error of the full ensemble distribution probability error reliability resolution potential calibrated skill discrimination and economic value each attribute is defined as the selected metrics are introduced some forecast attributes are evaluated by more than one metric allowing the relative sensitivities of different metrics to precipitation observation uncertainty to be assessed to obtain a measure of the forecast skill in relative terms a metric calculated from the ensemble forecasts can be compared to that calculated for a reference benchmark forecast this gives the skill score for that metric the choice of benchmark forecast depends on the aim of the verification and must be considered in the interpretation of the skill score results for example see pappenberger et al 2015 common benchmarks include climatology persistence and a random forecast 2 1 1 evaluation of continuous probabilistic forecasts to assess the error of the full ensemble distribution a continuous version of the brier score brier 1950 the continuous ranked probability score crps was applied hersbach 2000 the crps expressed in the same units as the observed variable measures the difference between the cumulative distribution estimated by the ensemble forecast and the step function cumulative density function of the observation as indicated in hersbach 2000 the crps is commonly averaged over a number of cases here the crps is averaged over all forecasts and all time periods contained within the forecast lead time range being considered a dimensionless skill score the continuous ranked probability skill score crpss was formed by comparing to the crps calculated from the sample climatology this benchmark was selected to be consistent with that used for the brier score and the evaluation of longer lead time river flow forecasts not presented here at best the crpss takes a value of one and values less than zero indicate the forecast performs worse than the reference the rank histogram talagrand et al 1997 hamill 2001 was used to assess the reliability of the ensemble that is whether or not the ensemble and observations have been drawn from the same distribution a flat rank histogram suggests that the ensemble spread is an appropriate representation of the forecast uncertainty whilst u and domed shaped rank histograms indicate that the ensemble spread is too small and large overall respectively an asymmetric rank histogram indicates that the ensemble is biased 2 1 2 evaluation of forecasts of binary events the brier skill score bss was used to assess the probabilistic forecast skill the bss measures the proportional improvement in mean square probability error as defined by the brier score brier 1950 with respect to a reference forecast in a standard manner see wilks 2011 here the reference forecast is taken as the sample climatology of events occurring over the threshold of interest in this context of binary events the use of a sample climatology is preferred to a benchmark based on persistence at best bss takes a value of one and values less than zero indicate the forecast performs worse than the reference the reliability or attributes diagram wilks 2011 plotting the forecast probability against the probability of the observation given the forecast allows the reliability and resolution of the probability forecasts to be visually assessed resolution is an indication of how much the forecast deviates from the reference with the forecast being more useful if resolution is larger sharper with smaller forecast spread provided the ensemble is reliable a forecast with no resolution has an observed relative frequency equal to the sample climatology plotted as a horizontal line on the reliability diagram in general it is considered good practice to include a sharpness histogram with the reliability diagram showing the sample size for each probability bin when this is the case it is referred to as an attributes diagram typically for a high threshold precipitation or river flow the low probability bins will be populated orders of magnitude more than the higher probability bins this affects and can skew the interpretation the relative operating characteristic roc diagram see for example jolliffe and stephenson 2012 plots for a given threshold paired values of probability of detection pod and false alarm rate f of ensemble forecasts for different probabilities of exceedance the roc diagram measures the discrimination of the forecasts the ability of the forecasts to distinguish between observed events and non events this diagram was also used to assess the potential skill of the ensemble that is the ensemble skill if forecast probabilities were well calibrated a skill score based on the area under the roc curve the roc skill score rocss is defined as the area under the roc curve auc normalised with reference to a random forecast with no skill an auc equal to 0 5 this reference was used to relate directly to the roc diagram a rocss of one indicates a perfect forecast and if above zero the forecast has a skill better than a random forecast the economic benefit of a forecasting system depends on the cost loss ratio of a particular user to assess the economic value of forecasts the relative economic value rev statistic murphy 1977 richardson 2000 wilks 2001 zhu et al 2002 was used the rev is widely used in the verification of both hydrological and meteorological forecasts e g roulin 2006 magnusson et al 2014 and uses information derived from a contingency table e g wilks 2011 to calculate the economic value relative to a forecast based on climatological information a cost loss decision making model is assumed to define the cost for taking action irrespective of whether or not the event occurs and the loss incurred when the event occurs but no action was taken the rev diagram presents the rev for different cost loss ratios the rev has a maximum value of one for a perfect forecasting system a value of zero for forecasts having the same value as climatological information only and is negative for forecasts which have less value than using only climatological information 2 2 meteorological and hydrological models used to focus on the considerations and strategies for joint river flow and precipitation ensemble verification this paper restricts attention to the first 24 h of river flow ensemble forecasts and the corresponding precipitation ensembles the precipitation ensemble consists of nowcasts from the short term ensemble prediction system steps bowler et al 2006 which merges a radar extrapolation nowcast with a spatially downscaled nwp forecast and forecasts from the met office global and regional ensemble prediction system mogreps hagelin et al 2017 bowler et al 2008 run using the operational met office unified model um davies et al 2005 tang et al 2012 steps aims to account for uncertainty in the motion and evolution of radar based precipitation fields mogreps aims to account for uncertainties in meteorological initial and boundary conditions and sub grid processes in the nwp model the river flow ensemble is generated using the g2g distributed hydrological model moore et al 2006 bell et al 2009 cole and moore 2009 2 2 1 best medium range precipitation ensemble best medium range best mr ensemble forecasts of 15 min precipitation accumulations mm per 15 min are produced with uk coverage extending out to over 6 days and issued four times a day these forecasts use the best available precipitation estimates for the lead time period considered in this study the ensemble forecasts are a blend of the 2 km resolution steps extrapolation nowcasting system and the convection permitting 2 2 km mogreps uk for the first 7 h depending on forecast triggering see below and mogreps uk beyond for the operational forecasts considered here december 2015 mogreps uk extends out to 36 h to produce the best mr product all mogreps forecasts are downscaled onto a fixed 2 km grid over the uk the british national grid as used by steps to allow the latest forecast to be available to the ffc the best mr forecasts are triggered based on the time when the required input data from the nwp model are available as opposed to being clock triggered at a fixed time this results in forecast start times which vary by up to three hours to simplify interpretation only best mr forecasts issued at 0100 0700 1300 and 1900 each four hours after the associated mogreps uk run have been used during the study period considered here section 2 4 forecasts issued at these times correspond to around 65 of the total number of forecasts issued the operational best mr forecasts are archived from 25 november 2015 to present this archive reflects the biannual upgrades to the um past forecasts are not re run for a new model version and no hindcast archives are available the best use of such an archive is a key consideration for the operational implementation of an ensemble verification system of flood events and is discussed further throughout this paper 2 2 2 river flow ensemble forecasts using the g2g distributed hydrological model g2g is a physical conceptual distributed hydrological model developed by the centre for ecology hydrology ceh to forecast river flow and surface water flooding moore et al 2006 bell et al 2009 cole and moore 2009 g2g takes account of the effects on grid cell runoff production of land cover and soil geology properties along with antecedent wetness conditions with water flows routed from cell to cell g2g is formulated to represent spatial variability in river flow response to precipitation across a landscape with catchment river basin and countrywide coverage g2g can make full use of spatially distributed precipitation data derived from observation networks of weather radars and raingauges as well as precipitation forecasts from nowcasts nwp models and their combination g2g is in operational use as a countrywide flood forecasting system by both the ffc over england and wales price et al 2012 and by the scottish flood forecasting service sffs across scotland cranston et al 2012 five day outlook forecasts from g2g are used in preparing the flood guidance statements issued by these operational bodies for this study the operational g2g configuration on a 1 km grid and for a 15 min time step is employed price et al 2012 with the period january to march 2008 used for calibration a raingauge based precipitation truth see section 2 3 1 is used in the calibration of g2g and is also used to obtain the initial conditions for each g2g forecast spatial datasets e g terrain soil geology and land cover are used to support its configuration and parameterisation lessening the need for extensive calibration data assimilation of river flow observations helps to maintain realistic model states which are then used to initialise forecasts of river flow flow insertion is applied to correct flows to those observed at gauged river locations and thereby improve the flows propagated downstream a conservative form of empirical state updating uses the observed flow to gradually adjust the g2g water storage upstream of gauged river locations for this work the river flow ensemble forecasts were re run using the operational g2g configuration and the 15 minute accumulation best mr ensemble rainfall forecasts as input instantaneous river flows m3 s 1 were output every 15 min for the 898 river gauging station locations used operationally in g2g over england wales observed river flows are available from the environment agency and natural resources wales for these sites section 2 3 2 no additional uncertainties are incorporated in the river flow ensemble the ensemble only accounts for uncertainty in the input precipitation 2 3 verification data sources 2 3 1 precipitation to consider the effect of observation uncertainties on forecast verification two precipitation truth types are used raingauge based and radar based the raingauge based precipitation truth uses data from the raingauge network across england and wales operated by the environment agency ea and natural resources wales nrw a gridded 1 km raingauge based truth is then calculated by fitting a multiquadric surface with zero offset to the point raingauge observations accumulated to a 15 min interval moore et al 1994 cole and moore 2008 raingauge data have been quality controlled at ceh using the methods presented in howard et al 2012 the radar based precipitation truth is generated using the met office radarnet system harrison et al 2012 which combines 5 min scan data from individual radars and includes data quality control radar data processing includes a raingauge based mean field adjustment constant over the domain of a single radar that uses data from the met office raingauge network applied over a time period dependent on the number of recent raingauge radar pairs available as the radar rainfall data do not relate directly to the 15 minute raingauge data used for the raingauge composite precipitation truth these datasets are considered suitably independent observation sources for verification 2 3 2 river flow data on river flow at 15 min intervals were obtained for the 898 ea and nrw river gauging stations used operationally in g2g over england wales of these catchments around half are less than 100 km2 with 75 less than 250 km2 and 90 less than 700 km2 the catchment response times range from less than an hour to a few days the river flow data have been quality controlled at ceh including visual checks to identify periods of erroneous data consideration of uncertainties in the river flow data in a forecast verification context was beyond the scope of the current study but is recognised as an important avenue of future investigation 2 4 ensemble verification period ensemble verification was undertaken using forecasts starting during the 32 day period from 25 november to 26 december 2015 hereafter referred to as the study period throughout this paper the term sample climatology refers to average values calculated over this period this winter period was very wet as revealed by the december 2015 precipitation anomaly from the 1981 2010 average which exceeded 200 for much of and 300 for parts of wales and northern england fig 3 of mccarthy et al 2016 many record breaking precipitation totals were seen over this period including the highest 24 hour total ever recorded and the second highest rain day rainfall reliably recorded in the british isles burt 2016 flooding associated with two met office and met Ã©ireann named storms occurred over this period 5 to 6 december from storm desmond and 24 to 26 december after storm eva precipitation from these storms fell onto already saturated ground following three storms the previous month resulting in very high river flows flow records for nine catchments held in the national river flow archive set new data era peak flows during this period a number of flow events were assessed to have return periods greater than 100 years and widespread flood damage was suffered across large parts of britain barker et al 2016 marsh et al 2016 3 approach to verification to obtain an overview of ensemble performance national and regional scales must be considered alongside the performance for individual catchments this paper focusses on ensemble verification over england and wales with national scale verification undertaken using data for all 898 catchments for which river flows are used operationally in g2g eight catchment groups defined based on aggregated river drainage basins aligned to wales and environment agency regions over england are used to verify forecasts at the regional scale these catchment groups are shown in fig 1 to facilitate a joint verification of river flow and precipitation it is necessary to match as closely as possible the precipitation and the hydrological response to this end all precipitation verification reported here employs catchment average precipitation calculated from the gridded precipitation ensemble output this differs from the conventional meteorological and currently operational verification of precipitation at either individual observation locations or using a gridded radar product e g mittermaier et al 2013 mittermaier 2014 mittermaier and csima 2017 and is essential for meaningful hydrological comparison one of the primary challenges for joint precipitation and river flow verification is how best to link precipitation events with the corresponding river flow events in terms of both magnitude and time scale in the sections that follow methods of choosing accumulation periods and thresholds for precipitation and river flow are discussed which go some way towards addressing this challenge with the aim of developing a meaningful joint verification framework in an operational flood forecasting context these choices are then discussed further and put into context in section 4 when the verification framework is demonstrated 3 1 thresholds to obtain ensemble verification results that are useful and relevant in an operational flood forecasting context it is necessary to define river flow thresholds which select the flooding events of interest and precipitation thresholds which select relevant precipitation values using these thresholds the observed time series of river flow and precipitation are converted to binary time series where a value of one indicates an observed event and time series of ensemble forecast probabilities whose values indicate the forecast probability of an event occurring these binary and forecast probability time series are then used to calculate the threshold based metrics and diagrams described in section 2 1 namely the bss attributes diagram roc diagram rocss and rev diagram traditionally thresholds for forecast verification are treated differently between hydrological and meteorological communities it could be argued that only precipitation that leads directly to a flood response in the river flow is of interest unfortunately this view is too simplistic as for example the same precipitation totals over the same catchment may not lead to the same river flow response the river flow response is determined by multiple factors which interact non linearly and non systematically thus it is necessary to consider separately the calculation of precipitation and river flow thresholds in this paper we consider two methods of calculating precipitation thresholds and one method for calculating river flow thresholds for the verification of precipitation thresholds are usually specified as either fixed values e g 4 mm h 1 or as percentiles of the total precipitation in the verification domain for example over england and wales at a specific time thus in this method percentiles of the spatial precipitation distribution are used as precipitation thresholds hereafter referred to as spatial percentile thresholds with a new threshold calculated every time the forecasts and observations are compared i e the spatial percentile thresholds vary temporally this method for precipitation thresholds is discussed further in section 3 1 2 an alternative method appropriate at the catchment scale takes percentiles of the temporal distribution of catchment average precipitation values for each catchment thus this second method uses percentiles of the sample climatology for each catchment as time invariant thresholds that vary from catchment to catchment this thresholding methodology denoted temporal percentile thresholds is discussed further in section 3 1 3 the method used for defining river flow thresholds based on return periods is discussed first in section 3 1 1 in addition to the need for different methods of calculating thresholds for river flow and precipitation it is also necessary to consider how best to apply those thresholds for precipitation where high but not necessarily increasing values are relevant for hydrological response it is appropriate to consider threshold exceedance any precipitation values over the threshold are considered to be events however for river flow flood forecasting interest is focussed on the rising flows the start of a potential flood event to focus on these times upward threshold crossings i e the point at which rising river flows first cross a given threshold value are used in this paper to define hydrological events this is discussed further in section 3 1 1 3 1 1 return period based thresholds for river flow here a hydrological threshold is selected as the river flow corresponding to a specific return period where a return period of n years means that there is on average a 1 in n chance of a flood of at least that magnitude occurring in one particular year thus higher return periods correspond to more extreme events and higher flow thresholds return period threshold values from the flood estimate handbook feh institute of hydrology 1999 were scaled to match the g2g median flood equal to q 2 the flow q of return period 2 years calculated over the water years 2007 to 2015 as done operationally for g2g forecasts it is noted that the median flood has a close association with the bankfull discharge for natural rivers for flood guidance purposes the ffc use a 1 km grid of q t values for a range of t values return period in years as a nationally consistent indicator of flood severity when referenced against the g2g flows this approach complements flood thresholds for specific sites associated with actual flooding and used with local models in this study a forecast hydrological event is defined for each ensemble member as at least one upward crossing of a threshold upward threshold crossing occurring anywhere within the forecast lead time range of 0 24 h upward threshold crossings are calculated from the instantaneous river flow data m3 s 1 at two consecutive time steps i e separated by 15 min note that timing uncertainties up to 24 h in the river flow forecasts are tolerated in this analysis for flood guidance purposes there is interest in a given threshold being crossed at any time within the next 24 h this approach is preferred to verifying 24 hour river flow accumulations as it retains the shape and magnitude of the 15 min time step flood hydrograph accommodating timing uncertainties can be particularly important in this analysis where instantaneous river flows are verified section 2 2 2 hydrological forecast probabilities are calculated by taking the average number of events forecast across the ensemble an observed hydrological event is defined as an upward threshold crossing occurring anywhere within the corresponding 24 h observation period an upward threshold crossing is used in preference to a threshold exceedance for river flow to focus on the start of a potential flooding event and accommodating timing errors within a 24 hour period using a threshold exceedance would be particularly inappropriate for less extreme thresholds which may be exceeded for a number of days in an operational verification system several verification thresholds would be used spanning a range of return periods this would allow the ensemble performance at different points in the flood hydrograph to be monitored for meaningful verification statistics it is necessary to consider a large number of events this becomes difficult for thresholds of high return period for this reason the focus here is on one return period threshold Â½q 2 this corresponds to half the river flow of the bankfull level which can be related to the q 2 threshold and is considered to be the minimum threshold of interest in a flood forecasting context it will be shown in section 4 that even for this low threshold sampling uncertainties impact on the verification results 3 1 2 spatial percentile thresholds for precipitation as discussed above spatial percentile precipitation thresholds are calculated from the spatial distribution of all precipitation in the verification area at a particular time for either observations or an individual ensemble member for this study where catchment average precipitation values are used the spatial distribution is formed of the catchment average precipitation values of all catchments in the verification area thus the spatial percentile thresholds focus on the tail of the precipitation distribution for every forecast of course percentile based results are dominated by the use of more modest precipitation accumulation thresholds but at least they allow for higher thresholds to be included when they do occur e g mittermaier et al 2013 spatial precipitation thresholds also allow for a consistent interpretation across the different catchments within the verification area by definition spatial percentile thresholds only select a small number of events leading to large sampling uncertainties however this compromise is necessary to focus on the catchment average precipitation relevant in a hydrological context for the study period used here the 95th percentile was found to be a good compromise given these considerations and is used for all spatial percentile precipitation threshold results thus after application of this threshold to a particular ensemble member precipitation forecast or to the precipitation observations 5 of the catchments considered will be denoted as having an event and allocated the value one and the remaining catchments will have no event allocated a value of zero in this paper the group of catchments used to calculate the spatial percentile thresholds depends on the scale of the analysis being conducted for the national scale analysis all 898 catchments in england wales are used for the regional analysis only catchments within the region of interest are used thus for a particular time the regional analysis will use a different threshold value for each region in fig 1 note that these regional analysis spatial percentile threshold values will also differ from that used for the national scale analysis at this time for analysis at the catchment scale spatial percentiles thresholds calculated from the regional analysis are applied fig 2 shows how the 95th percentile threshold regional analysis values relate to catchment average precipitation values for two example regions the north west region of england predominantly upland shows very heavy precipitation at the 95th percentile up to 10 mm h 1 with four days having over 100 mm of precipitation this highlights the unusual nature of the selected verification period and why this period is relevant for understanding model performance in a flood warning context sample size tends to restrict the computation of verification statistics for fixed thresholds exceeding 4 mm h 1 unless computed over a very long time period the other example region in fig 2 anglian in lowland eastern england shows much lower precipitation thresholds and is much more representative of conditions more generally 3 1 3 temporal percentile thresholds for precipitation as discussed above temporal percentile thresholds are calculated separately for each catchment by taking percentiles of the temporal distribution of catchment average precipitation values thus unlike the spatial percentile threshold approach discussed in section 3 1 1 results using temporal percentile thresholds cannot be consistently compared across different geographical areas in this study the use of temporal percentile precipitation thresholds is limited to that at the catchment scale of course with knowledge of the precipitation values corresponding to each catchment threshold useful insight can still be gained when comparing individual catchment performance maps of the time invariant temporal percentile threshold values calculated from the full 32 day study period 25 november to 26 december 2015 see section 2 4 are shown in fig 3 consistent with the spatial percentile thresholds the 95th percentile value is used here maps are shown for both the raingauge and radar data centre and right and also for an example ensemble member other members lead to similar conclusions the spatial distribution of thresholds is similar for all three agreeing with the results of fig 2 fig 3 shows lower precipitation thresholds to the southeast and higher precipitation values in wales and the northwest of england however the most extreme values shown in fig 2 are lost from the analysis when using these sample climatology thresholds with maximum totals of 7 9 mm h 1 and 129 mm d 1 occurring for the radar data the values for raingauge and ensemble precipitation are slightly lower thus the chances of capturing and evaluating the characteristics of the very highest precipitation totals isolated in time and localised in space are reduced 3 2 accumulation periods from a nwp perspective it is desirable to consider different precipitation accumulation periods as longer accumulation periods have higher forecast skill duc et al 2013 the intensity duration relationship is highly non linear longer accumulation periods do not necessarily imply that it rained for longer periods but larger time windows have the ability of blurring or mitigating against the impact of timing errors the precise definition of the accumulation window can be important though as from a hydrological perspective it can affect the time delay in any catchment flow response to precipitation for example the river flow at the outlet of a large slow response catchment will be linked to precipitation falling further in the past than for a small rapid response catchment here both 24 hour and one hour daily and hourly precipitation accumulations are considered for verification for all metrics considered i e both threshold based and non threshold based thus for each forecast origin there is one comparison of the forecasts and observations when considering a 24 hour accumulation with units mm d 1 or 24 comparisons when considering one hour accumulations with units mm h 1 of course it is still desirable to consider directly the precipitation at the temporal resolution that is used as input to the g2g model although this like for like correspondence between river flow and precipitation time interval is desirable it is considered more important that the precipitation verification should be appropriate to catchment response and meaningful from a flood forecasting perspective future work will extend this study to consider 15 min precipitation accumulations this is a very stringent timing test for the forecasts timing errors are likely to contribute significantly to the overall precipitation forecast error e g mass et al 2002 particularly during winter when frontal systems dominate of course timing and spatial errors in the precipitation field are directly linked e g mittermaier and roberts 2010 dey et al 2016 as discussed in section 3 1 an event for river flow threshold based verification metrics is defined for each ensemble member as at least one upward threshold crossing occurring anywhere within the forecast lead time range of 0 to 24 h thus although the threshold crossings are evaluated using the 15 minute river flow data units m3 s 1 the consideration of timing uncertainties is comparable to that of the analysis of daily precipitation accumulations for the calculation of non threshold based metrics e g crpss rank histogram two methods are applied to account for timing uncertainty in the river flows firstly taking the mean flow over the 24 hour period units m3 s 1 and secondly taking the maximum value over the 24 hour period units m3 s 1 the former evaluates the 24 hour river flow volume and the latter focusses on the highest points in the hydrograph of interest in a flood forecasting context additionally for completeness the 15 min river flows are evaluated directly 3 3 summary of verification approach when comparing any of the methods for precipitation forecast verification with those used for the hydrological ensemble forecasts there are some key differences deserving of further discussion in particular the following points are noted for hydrological forecasts upward threshold crossings are used as this is what is of operational interest for flood guidance and warning systems for precipitation forecasts threshold exceedance is used instead as the hydrological response is determined by high but not necessarily rising precipitation values for hydrological forecasts events are defined when a threshold is crossed at any 15 minute time step in the forecast period of interest here 24 h whereas for precipitation each accumulation period is treated separately thus in terms of the time period considered the performance of the daily precipitation accumulation ensemble links more directly with the river flow ensemble performance however the performance of the hourly precipitation accumulation ensemble relates more directly to the catchment runoff response as the hydrological ensemble is driven by 15 min precipitation accumulations and run at a 15 min time step for hydrological forecasts return period thresholds are used to select flooding events of interest for precipitation two methods of using percentile thresholds are used spatial percentile thresholds calculated from the spatial distribution of catchment average precipitation values varying in time and temporal percentile thresholds time invariant and calculated separately from the temporal distribution of precipitation values for each catchment for metrics calculated using the full ensemble distribution not threshold based e g crpss rank histogram both daily and hourly precipitation accumulations are evaluated for these metrics the 15 min river flow data are evaluated alongside the daily mean and daily maximum river flows this allows the effect of timing uncertainties to be investigated and links made between these metrics and those using thresholds 4 demonstration of verification framework results for example period 4 1 overall analyses to give an overview of ensemble performance forecasts from all catchments in england wales are first considered together for the calculation of verification statistics and associated diagrams fig 4 shows the reliability roc and rev diagrams with bootstrap confidence intervals at the 75th 90th and 99th percentile in grey shading and rank histograms for this overall verification considering the reliability diagrams and associated sharpness histograms showing the sample size for each probability bin it can be seen that the river flow ensemble is over forecasting the probabilities are too high and also over confident larger probabilities are more over forecast this is also seen for the hourly precipitation accumulation ensemble suggesting that the over confidence in the input precipitation ensemble is contributing to the over confidence in the river flow ensemble in contrast the daily precipitation accumulation ensemble shows good reliability for forecast probabilities up to 0 8 by considering daily accumulations the effects of timing errors are reduced this gives an upper band on the ensemble performance for probabilities above 0 8 both the hourly and daily precipitation accumulation ensembles show an increased over confidence thus the raingauge based precipitation used as truth for fig 4 is not capturing the highest precipitation values as frequently as they are forecast possibly due to the extreme precipitation values not occurring at raingauge locations signalling observation error in the form of raingauge representativity all three sharpness histograms show a higher forecast relative frequency for low forecast probabilities as expected there is generally a low chance of the threshold being crossed river flow or exceeded precipitation for river flow there is also a slight increase in the forecast relative frequency for the highest sharpness histogram bin thus at times when it is likely that a threshold will be crossed it is more common for the majority of ensemble members to predict this event than for the ensemble to be split between members that do and do not capture the event it is possible that for the short and abnormally wet study period considered here section 2 4 the river flow sharpness histogram is influenced by flooding events with flows rising much higher than the Â½q 2 threshold note that as the reliability diagram shows these high probability forecasts to be forecast too frequently the ensemble is over confident and over spread the observed increase in sharpness does not lead to higher forecast accuracy the effects of sampling uncertainty can be seen in both the river flow and the daily precipitation accumulation results with the larger bootstrap uncertainties seen for 24 h accumulations this difference is thought to be due to the different approaches to thresholding the river flow and precipitation accumulations in particular the choice of absolute value river flow thresholds indicating possible flood events results in a much smaller sample of river flow threshold crossings and higher sampling uncertainties it may perhaps be surprising to see that the daily precipitation confidence intervals are wider than those for hourly precipitation this is primarily because daily precipitation accumulations can span a wider range of values mm d 1 the range of hourly precipitation accumulation values mm h 1 is generally much less the exception being precipitation that leads to flash floods on short time scales the larger sample size available for analysing hourly precipitation accumulations will also contribute to the narrower uncertainty bands like those for river flow precipitation probabilities tend to be over confident especially for larger probabilities the roc diagrams indicate high potential skill for both the river flow and precipitation ensembles agreeing with the reliability diagrams the highest potential skill is seen for the daily precipitation accumulations the river flow ensemble shows higher potential skill than the hourly precipitation accumulation ensemble suggesting that re calibration of the river flow forecast probabilities in particular could lead to improved performance for all three ensembles the rev diagrams show positive rev over a range of different cost loss ratios with higher probability thresholds having lower rev values but over a larger range of cost loss ratios comparing the river flow and precipitation rev diagrams it is seen that overall the river flow ensemble has a narrower envelope of cost loss ratios with positive rev than for precipitation the Â½q 2 threshold river flow ensemble forecasts have comparable economic value to the daily 95th percentile threshold precipitation forecasts though the daily precipitation forecasts show somewhat higher rev for high cost loss ratios the hourly precipitation forecasts show a smaller envelope of positive rev with a lower peak and for a smaller range of cost loss ratios interestingly this difference was not seen when comparing the roc curves it occurs only when considering the cost loss ratio the lower potential skill indicated by the roc diagram and lower rev for hourly accumulations is tied to the spatial constraints applied in this analysis where the precipitation is expected to occur in the right place catchment at the right time even though the precipitation forecast is an ensemble any mismatches in space and or time are accentuated for shorter accumulation periods for higher precipitation thresholds not shown the rev curves are more similar to those for the Â½q 2 river flow threshold suggesting that these differences may also be due to differences in the thresholding methods this highlights the need for a thorough understanding of both river flow and precipitation verification methods for a meaningful joint verification thresholding differences do not exist for the rank histograms which are calculated from the full ensemble distribution rank histograms show larger differences between river flow and precipitation ensemble performance the river flow ensemble rank histograms show the observations falling in the lowest bin the ensemble over predicting river flow over 40 of the time and into the highest bin the ensemble under predicting river flow around 30 of the time when instantaneous 15 minute river flows are used similar results are obtained using the daily maximum and daily mean instantaneous river flows shown by the hashed bars in fig 4 although the relative population of the highest and lowest bins changes slightly this suggests that timing uncertainties from the use of instantaneous river flows are not causing the strong under dispersion seen in the river flow rank histograms instead the under dispersion is thought to relate to several different factors as discussed in section 2 2 2 the river flow ensemble only takes account of rainfall uncertainty other forms of uncertainty such as model uncertainty in representing the hydrological processes may be important to accurately capture the ensemble dispersion e g brown et al 2014b it is possible that the unusual nature of the verification period section 2 4 also acts to highlight the river flow ensemble under dispersion additionally although the ensemble is not reproducing the range of observed values and so is in an overall sense under spread a similar effect could also be caused by conditional biases in the ensemble forecasts for example if an ensemble had a high bias for half of the forecasts evaluated and a low bias for the other half the rank histogram from all evaluated forecasts would show higher populations for both the lowest bin from the high biased forecasts and the highest bin from the low biased forecasts it is interesting that the ensemble under and over predicts the flow values given that the forecast probabilities were seen from the reliability diagrams to be overestimated only this suggests that although the high flows irrespective of absolute magnitude are overestimated by the ensemble giving an over confidence in predicting threshold crossings the low flows are underestimated that is the ensemble flows are too peaky of course the overall quality of the hydrological simulation also impacts the ensemble performance as a physically based model which conserves water balance g2g does not contain a bias correction term and the ensemble forecasts for some sites will have high low bias this would also show in the rank histograms as a conditional bias contributing to a u shaped rank histogram future work will investigate the use of post processing to bias correct the river flow ensemble members even when river flow data assimilation is used and the input rainfall ensemble is reliable recent studies e g bourgin et al 2014 have shown that post processing is needed to obtain reliable river flow ensembles although more uniform than the river flow rank histograms those for precipitation also show observations falling too frequently in the ensemble extremities at the high end of the ensemble for hourly accumulations and at the low end of the ensemble for daily accumulations however these differences in precipitation rank histograms were found to be highly sensitive to the precipitation observation type and should hence be treated with caution rank histograms in particular are known for being sensitive to observation uncertainty e g hamill 2001 but other diagnostics which can be related to the distribution can also be affected fig 5 shows the equivalent precipitation ensemble verification diagrams as fig 4 but using a radar based precipitation truth as the river flow results are not evaluated for different precipitation truths they are unchanged from fig 4 and are not repeated in fig 5 for brevity of presentation overall the radar based reliability diagrams give a similar message to those with a raingauge truth the ensembles are over forecasting and overconfident however there are differences particularly for the daily precipitation accumulations which show much poorer reliability when a radar based truth is used for a radar based truth performance is similar across the full range of probabilities for both truth types the relationship between hourly and daily precipitation reliability diagrams is similar only subtle differences are seen in the roc and rev the radar based daily precipitation accumulation rank histograms in fig 5 suggest the smallest accumulations occur more frequently compared to the raingauge based rank histogram in fig 4 though both are suggesting a dry bias and insufficient spread for the hourly accumulations the shape of the rank histogram changes more dramatically looking fairly well spread based on the radar rainfall accumulations in fig 5 whilst for gauge rainfall in fig 4 it shows that observations fall in the largest accumulation bin more frequently that is the same forecast against a different observation has a wet bias under forecasting lighter precipitation according to the raingauge observations this suggests an interesting dynamic between hourly and daily precipitation the differences may simply be due to the temporal granularity as an hourly accumulation may not be a good fit when it comes to defining events whereas on the daily time scale events are generally less susceptible to timing errors unless it is a very long duration event that is less likely to straddle adjacent time periods with detrimental impact these results highlight both the benefit of considering multiple verification diagrams and also the importance of considering observation uncertainty 4 2 regional scale verification the ensemble performance is found to vary considerably at the regional scale fig 6 shows reliability and roc diagrams stratified by region for river flow and hourly precipitation accumulation with shading showing bootstrap confidence intervals at the 99th percentile daily precipitation results were found to be similar to those for hourly accumulations and are not included here overall higher reliability is seen for precipitation than for river flow although it is expected that 15 min precipitation accumulations not considered here would have lower reliability than daily or hourly ones it is not thought that this would fully account for these differences for river flow there is more regional variation in reliability than for precipitation with regions to the south east of the country having much lower reliability when considering river flow this is partly explained by the large sampling uncertainties seen for these regions as shown by the bootstrap confidence intervals for these regions there are so few river flow threshold crossings even for the Â½q 2 threshold that the range of possible reliability goes from very poor to good hydrological differences between the regions are also thought to contribute to the greater regional variation for the river flow ensemble performance comparing the individual region performance there is not a direct correspondence between the river flow and precipitation reliability for example the north east and north west of england regions perform best for river flow but midlands and wales perform best for precipitation anglian performs worst for both river flow and precipitation these differences highlight again the dependence of the river flow ensemble behaviour on hydrological processes controlling runoff production water storage and translation the river flow ensemble is not a simple transformation of the rainfall ensemble members and the river flow ensemble performance cannot be directly estimated in a simple manner from that of the rainfall ensemble the roc diagrams stratified by region show higher potential skill for river flow than for hourly rainfall accumulations across the majority of regions agreeing with the national scale analyses section 4 1 exceptions to this are the thames and anglian regions where the river flow analyses are dominated by sampling uncertainties with rivers being generally less responsive to precipitation a similar regional dependence is seen in the roc and reliability diagrams 4 3 verification at the catchment scale the crpss as fewer catchments are used to calculate threshold based verification statistics the effective sample size decreases and sampling uncertainties increase in this section the spatial distribution of the crpss scores calculated for individual sites are considered as the crpss is calculated from the full ensemble distribution it is less targeted to the flood forecasting context than the threshold based metrics however a brief analysis of the crpss serves as a useful complement to the threshold based scores through giving a better understanding of the ensemble performance as a whole additionally as the crpss is calculated from the full ensemble distribution for all forecasts in the verification period the crpss is less influenced by sampling issues and the catchment scale performance can be better evaluated this is important in an operational context where flooding cases may cover only a small number of catchments fig 7 shows maps of crpss for instantaneous 15 minute river flows and for hourly and daily precipitation accumulations results for daily mean and daily maximum river flows are very similar to those for the instantaneous river flows leading to the same discussion and conclusions and are not included here for brevity of presentation all crpss values were formed by comparing crps calculated from all ensemble forecasts in the sampling period to the crps calculated from the sample climatology as defined in section 2 1 1 for the majority of catchments the ensemble forecasts are more skilful than the sample climatology with positive crpss values there are a few exceptions for river flow predominantly in the midlands region fig 1 and corresponding to catchments with unnatural flow regimes artificial influences such as abstractions discharges and reservoirs which are not represented in detail in g2g for river flow little consistent variation is seen in crpss values across england wales this suggests that the influence of non location specific catchment properties such as catchment size are influencing the crpss more than locally consistent catchment properties in contrast the precipitation crpss values show clear spatial variations which can be linked to the distribution of precipitation accumulations e g poorer skill in the south and east of england where smaller accumulations were experienced during the winter period and there was less deviation from the sample climatological values thus the spatial variations seen in the precipitation crpss scores is related to the use of a reference based on the sample climatology this relationship will be investigated further when the verification framework is applied to longer study periods using longer climatological references overall precipitation skill is spatially more coherent purely because the atmosphere is a continuum and inhomogeneous catchment properties aside from relief remain irrelevant until the precipitation has reached the ground overall similar results are obtained from raingauge and radar based truths any differences in the northwest and southwest corners of both england and of wales can be linked to the extent and quality of radar coverage in these areas without differences in thresholding methods a more direct comparison can also be made between river flow and precipitation ensemble crpss values the river flow crpss values are calculated from g2g modelled flows using 15 min precipitation data as input ideally these would be compared with 15 min accumulation precipitation crpss values however this was not possible in this study due to data processing constraints instead the river flow crpss values are compared to those calculated from both precipitation hourly and daily accumulations this gives an indication of the dependence of the precipitation crpss on the temporal resolution used and allows for an informed comparison with the river flow verification from scatter plots of the crpss for river flow against raingauge based precipitation shown in fig 8 it is seen that a smaller range of crpss values are obtained for precipitation than for river flow thus the river flow ensemble is being influenced by hydrological effects in addition to the precipitation uncertainty input through the precipitation ensemble moving from daily to hourly accumulations the standard deviation of the precipitation crpss values decreases slightly this narrowing of the range of crpss precipitation scores for the shorter accumulation period is initially somewhat unexpected but is related to the crps being in the same units as the variable that is being verified daily precipitation totals have a larger range compared to hourly ones giving a larger range of crpss magnitudes as the crps calculated from the ensemble forecasts will vary more between catchments than the crps of the sample climatology reference this larger precipitation range for the daily accumulations will also result in a larger range of crpss values the hourly scores are lower which is expected given that timing errors will have a larger impact at this temporal granularity also shown in fig 8 are the q q quantile quantile plots of the crpss for river flow against that for raingauge based precipitation by considering only quantiles of the crpss distributions attention is focused at the regional scale the relationship between individual catchments is no longer preserved the scores are presented in an ordered or ranked fashion and show the range of values for a region for both the precipitation and the river flow the q q plots show particularly for higher crpss values indicating higher skill an almost linear relationship between the river flow and precipitation crpss values for lower crpss values this relationship is less clear a range of river flow crpss values are seen for a given precipitation crpss value similar results not shown are obtained when comparing against a radar based precipitation truth 4 4 pooling of river flow data by catchment size given the dependence of the river flow crpss on non location specific catchment properties as shown in fig 7 and discussed above the relationship between various catchment properties e g catchment size terrain slope and sub catchment properties and threshold based verification scores was investigated fig 9 shows river flow reliability and roc diagrams calculated using all catchments in england and wales pooled by catchment size 5 pooling groups each containing around 180 catchments this catchment property was found to relate directly to the threshold based verification scores in particular the reliability diagram shows a clear trend of reliability decreasing with decreasing catchment size for forecast probabilities up to 0 4 these differences are larger than the 90th percentile bootstrap sampling uncertainties this trend agrees with that found in alfieri et al 2014 a similar trend is seen in the roc diagram for the four largest catchment size groups however the group of smallest catchments does not follow this trend and shows performance similar to the group of largest catchments similar conclusions were found using more catchment groups best performance was seen on the roc diagram for the smallest and largest catchments with middle sized catchments performing worse the rev curves partitioned by catchment size not shown also lead to similar conclusions this feature is unexpected generally larger catchments are expected to perform better than smaller ones and may be related to the unusual verification period considered here with many cases of large scale heavy precipitation and extreme flood events section 2 4 future work using a longer verification period will aim to disentangle this 4 5 verification at the catchment scale threshold based scores sampling uncertainties at individual sites are expected to be large particularly for river flows using flood producing thresholds hence in addition to considering threshold based river flow results at individual sites the relationship between catchment size and river flow ensemble performance section 4 4 is exploited by pooling the data from several catchments within a given geographic region the aim is to exploit the clear relationship between river flow ensemble performance and catchment area shown in fig 9 to reduce the river flow sampling uncertainties for the calculation of threshold based verification metrics at sub regional scales as similarly sized catchments have similar ensemble performance for river flow data from similarly sized catchments can be sensibly combined to calculate verification statistics thus reducing sampling uncertainty as the river flow ensemble takes precipitation input which varies coherently across geographic regions as discussed in section 4 3 with reference to the precipitation crpss maps it is also important to retain some regional variability in the river flow verification statistics here the aggregated river drainage basins regions shown in fig 1 are used use of regions based on hydro climate or broad scale landscape features are other options for regional pooling that might be considered in future work within a given region the catchments are ranked by catchment size for each catchment within that region data from a fixed number of other catchments with closely ranking areas are also used to calculate the verification scores thus a moving catchment size bin is used centred upon the size of the catchment of interest the width of this moving bin is defined based on a fixed number of catchments for example 15 sites larger and 15 sites smaller than the catchment of interest irrespective of the distribution of catchment sizes this option is used here as it ensures the same amount of data is used for each calculation another option would be to fix the range of catchment sizes to include in the moving bin for example to consider all sites within 200 km2 of the catchment of interest due to the long tailed distribution of catchment sizes this would result in a large number of catchments falling within the moving catchment bin for small catchments and only one or two catchments falling within the moving catchment bin for large catchments hence this is not a sensible option to use the number of sites to use for each pool of data is a compromise between retaining all the catchment scale information but not reducing sampling uncertainty at all and losing all the catchment scale information i e evaluating at the regional scale but reducing the sampling uncertainty to investigate the effects of pool size moving bins consisting of 11 21 31 41 and 61 sites were considered by comparing the rocss and bss values for all these options it was found that large differences were seen when moving from a 1 to 11 11 to 21 and 21 to 31 site pool however when increasing the pool size further the bss and rocss results converged and smaller differences were seen hence a pool of 31 sites was considered appropriate in fig 10 river flow bss and rocss results are mapped both for individual sites and using a moving catchment size pool of 31 sites within each region note that for precipitation accumulations local properties dominate and pooling by catchment size is not sensible instead it is necessary to consider individual site scores in the context of the scores obtained for neighbouring sites from fig 10 it can be seen that when using individual site data only it is not possible to calculate marked by hatching the rocss and bss at a large number of sites due to a small sample size by applying an area based pooling the values are more consistent between neighbouring locations note that the pooling does not affect the overall interpretation of the maps at national and regional scales for example differences seen between the rocss and bss maps are consistent between the no pooling and pooling results in fig 10 this is also true when a different number of catchments e g 11 21 are used in the catchment size pool not shown for brevity of presentation of course area based pooling only has an effect if there are threshold crossings at some sites within a given area pool hence although pooling has been found to be a useful method of obtaining more meaningful information from a small sample of hydrological events it is still reliant on having some hydrological events in this study the bss and rocss were calculated at all sites or pools of sites where any events occurred i e when there was at least one event in the verification period this method was used to retain the maximum possible amount of information from as many sites as possible of course it also results in some scores being calculated from a very small number of events particularly in regions to the southeast of england in future work with the benefit of a longer verification period the use of a higher sample size cut off for scores calculation will be investigated precipitation accumulation bss and rocss results calculated using spatial percentile thresholds for daily and hourly accumulations are shown in fig 11 both raingauge and radar based precipitation are considered as truth sampling effects are responsible for the lack of scores for many catchments as indicated by the hatching as discussed in section 3 1 2 this resulted from the thresholds being calculated from the catchment average values in a region such that by definition only the catchments with mean precipitation totals in the top 5 are verified the exceptionally wet period meant that these thresholds can be locally very high however on a larger multi catchment scale the precipitation lacked the spatial uniformity of intensity to exceed such thresholds over multiple catchments the net result is that many catchments did not receive the required amount of precipitation to have a sufficient sample for verification the bss is overwhelmingly negative skill worse than sample climatology with a scattering of catchments with small positive skill scores the rocss score shows more regions with skill with higher levels of skill for the daily precipitation accumulations overall there are strong similarities in the signals from daily and hourly accumulations for day 1 with the daily showing some additional skill results between radar and gauge rainfall accumulations are surprisingly similar in terms of spatial patterns with some notable exceptions e g mid wales bss and rocss results using the second thresholding methodology for precipitation accumulations at the catchment scale using temporal percentile precipitation thresholds section 3 1 3 are shown in fig 12 here as the number of threshold exceedances is fixed for each catchment the effects of sampling uncertainty are less and scores can be calculated for all catchments both the bss and rocss are seen to vary smoothly across the country with more skill higher scores seen to the northwest and lower scores to the southeast scores are much higher than those using spatial percentile thresholds fig 11 reflecting the less extreme values of precipitation that are being verified thus for lower precipitation thresholds further from the tail of the precipitation distribution the precipitation ensemble performs better however to understand the precipitation ensemble performance in a flood forecasting context it is necessary to focus on the tail of the precipitation distribution thus despite the presence of sampling uncertainties it is necessary to use other methods such as the spatial percentile thresholds used in fig 11 in this context of course to reduce sampling uncertainties verification should be performed over a longer verification period however given the extreme nature of precipitation and river flow events of interest in a flood forecasting context it is likely that sampling uncertainty and methods of its reduction will remain an important consideration in ensemble verification for both river flow and precipitation using either thresholding methodology higher values of rocss than bss are obtained suggesting that biases in the forecast probabilities are reducing the overall ensemble skill it is therefore conceivable that some post processing of the precipitation totals and probabilities could be of benefit in general the bss values for river flow are more similar to those for precipitation when calculated with spatial percentile thresholds for catchments where the scores could be calculated than with sample climatology based thresholds this is expected as the spatial threshold method evaluates more directly situations where flooding is more likely to occur for the rocss this is less clear with the river flow rocss values generally sitting between those calculated for the two precipitation thresholding methodologies higher than those for spatial percentile thresholds but lower than for temporal percentile thresholds 5 discussion using a sample one month case study period december 2015 this paper has demonstrated important considerations for a joint verification framework for hydrological and meteorological ensemble forecasting systems the sample represents a very short and atypical period good for getting enhanced sampling of flood producing rain but not generally representative the focus has been on verification information relevant in an operational flood forecasting context to gather meaningful statistics that are more representative of all possible scenarios the verification will need to be run over extended operational periods before overall conclusions on true skill can be drawn due to the infrequency of flood events sampling uncertainties are inherent in the verification of flood forecasting ensembles for example as discussed by cloke and pappenberger 2009 by considering lower thresholds more robust statistics can be obtained but such results may be of lesser practical relevance to those using flood forecasting ensembles for operational decision making individual case studies of flooding events for specific catchments can help to provide some confidence in forecast performance however in a probabilistic sense no meaningful evaluation of individual case study performance is possible in this paper a short verification period of 32 days was used moving beyond the individual case study approach this very wet period was selected so as to contain a number of flood events and thus to enable ensemble verification to be examined in a flood forecasting context of course by definition this means that it is not a representative period and model performance over this period will not be indicative of the model performance in less extreme cases results were presented for the lowest threshold considered appropriate in a flood forecasting context equivalent to half the flow of the two year flood event methods of increasing the sampling size were discussed throughout this paper in particular the consideration of a larger number of catchments grouped nationally and regionally the use of percentile thresholds for precipitation and the pooling of river flow results based on catchment size were presented as possible methods of reducing sampling uncertainties to a manageable level the performance of precipitation forecasts is very strongly dependent on the choice of thresholds used in the calculation of skill spatial percentile thresholds can often lead to skill metrics being dominated by non extreme events they pick up what happens in a particular accumulation period which most of the time is not extreme based on the roc and reliability diagrams and skill scores used here the skill and performance of the precipitation ensemble is good however as the exceptionally wet period used in this paper shows when the sampling does fall in the tails of the distribution the performance of the precipitation forecasts shows certain weaknesses for the shorter accumulation periods the lack of skill could well be dominated by timing errors which are not accounted for here therefore various sources of sampling uncertainty will always feature as a key consideration when interpreting verification results from flood forecasting ensemble systems this is particularly true for operational nwp ensemble systems such as the mogreps where long term hindcasts are not produced and weather models are in continuous development for forecast performance to be better understood it is vital that the associated sampling uncertainties are fully and comprehensively conveyed to operational forecasters and decision makers some form of post processing of the precipitation accumulations bias correction in the first instance and potentially also a subsequent probability calibration could add benefit however post processing precipitation can be challenging due to the non gaussian nature of its distribution e g scheuerer and hamill 2015 ben bouallÃ¨gue 2013 bentzien and friederichs 2012 for flood forecasting to benefit from a joint hydrological meteorological ensemble verification relevant and physically meaningful time scales must be considered and sources of uncertainty identified to this end precipitation results were presented using both hourly and daily accumulations to encompass the effects of catchments with different hydrological response times each precipitation accumulation period was considered separately in the verification for hydrological ensembles to focus on hydrological threshold crossings in a flood warning context hydrological events were defined when a river flow threshold was crossed at any 15 minute time step in the 24 hour forecast period of interest for precipitation the two truth types raingauge based and radar based were used to quantify the effects of precipitation observation uncertainty on the verification results the rank histogram was found to be particularly sensitive to this observation uncertainty reliability diagrams also showed sensitivity to the precipitation truth type with a raingauge truth suggesting high probabilities were particularly underestimated although not seen for the hourly precipitation results daily accumulations appeared to be much more reliable when comparing to raingauge data as only one set of river flow observations are available consideration of river flow observation uncertainty is a more involved process beyond the scope of this current study but an important topic for future work for the short verification period used in this study probabilistic forecasts derived from both the river flow and the precipitation accumulation ensembles tended to be over confident with over confidence increasing with forecast probability this is related to the lack of reliability in the forecast probabilities the river flow ensemble was found to be more severely under spread than the precipitation accumulation ensemble according to the rank histogram this suggests that unaccounted for uncertainties in the hydrological modelling process the river flow ensemble considers precipitation uncertainty only may be important for forecast accuracy in agreement with the conclusions of brown et al 2014b hydrological uncertainties will be considered in future work 6 conclusions from the investigations and analyses presented above the following key conclusions can be drawn for the full evaluation of operational flood forecasting ensembles it is necessary to consider both precipitation and river flow ensembles in a joined up manner differences in the physical nature of precipitation and river flow require consideration and lead to different verification solutions and interpretations examples have been presented of pooling river flow verification analyses but not precipitation analyses based on catchment properties and using percentile thresholds when verifying precipitation accumulations different spatial scales e g national regional and sub regional where possible should be considered to give informed and physically relevant information about the ensemble s performance this study has highlighted the varying effects of sampling uncertainty at different scales and the information that can be gained from a multi scale analysis to obtain a representative and unbiased view of ensemble performance it is necessary to use a range of metrics for both river flow and precipitation verification this is particularly important given the differing sensitivities of verification metrics to observation error in this study sensitivity to precipitation observation error was exemplified by the rank histogram these conclusions will form the basis of future work seeking an end to end ensemble verification framework relevant to operational flood forecasting ensembles in particular the results of this initial detailed case study over a 32 day period will allow appropriate choices to be made when considering longer datasets and different hydrological ensemble systems declaration of competing interest none declared acknowledgements the authors thank the two anonymous reviewers and associate editor maria helena ramos for their detailed comments that helped improve the quality and clarity of this paper the work reported on here formed part of the rainfall and river flow ensemble verification project commissioned by the flood forecasting centre on behalf of the environment agency scottish environment protection agency and natural resources wales preparation of this paper was made possible through centre for ecology hydrology and met office science funding 
6263,post processing of hydrological model simulations using machine learning algorithms can be applied to quantify the uncertainty of hydrological predictions combining multiple diverse machine learning algorithms referred to as base learners using stacked generalization stacking i e a type of ensemble learning is considered to improve predictions relative to the base learners here we propose stacking of quantile regression and quantile regression forests stacking is performed by minimising the interval score of the quantile predictions provided by the ensemble learner which is a linear combination of quantile regression and quantile regression forests the proposed ensemble learner post processes simulations of the gr4j hydrological model for 511 basins in the contiguous us we illustrate its significantly improved performance relative to the base learners used and a less prominent improvement relative to the hard to beat in practice equal weight combiner keywords combining probabilistic forecasts ensemble learning hydrological uncertainty interval score quantile regression quantile regression forests 1 introduction an important objective of hydrological models is to predict a variable of interest e g river discharge or runoff volume usually referred to as predictand using the terminology of krzysztofowicz 1999 as a response to other hydrological variables temperature precipitation etc see e g lidÃ©n and harlin 2000 mouelhi et al 2006a b das et al 2008 kaleris and langousis 2017 in this context hydrological models can be classified into three broad categories i e physically based conceptual and data driven see e g solomatine and wagener 2011 the output of the physically based and conceptual models is point predictions of hydrologic quantities which do not allow for direct quantification of predictive uncertainties to account for the latter within the general framework of probabilistic prediction see e g krzysztofowicz and kelly 2000 krzysztofowicz 2001 2002 kavetski et al 2002 montanari and brath 2004 kuczera et al 2006 todini 2007 montanari and grossi 2008 weijs et al 2010 montanari and koutsoyiannis 2012 hernÃ¡ndez lÃ³pez and francÃ©s 2017 tyralis and koutsoyiannis 2017 one needs to estimate the probability distribution function pdf of the predictand variable or the joint probability distribution function of all predictand variables of interest corresponding to different uncertainty sources see e g the detailed review on global uncertainty estimation by montanari 2011 one way to do so is to post process hydrological model outputs using conditional distribution based models regression based methods or other algorithmic approaches see e g li et al 2017 here we are interested in the case where post processing of hydrological predictions is conducted using quantile regression based models for a detailed review of the general framework of regression schemes in the context of hydrometeorological post processing the reader is referred to li et al 2017 examples of relevant algorithms include see e g messner 2018 for a detailed list a quantile regression see e g koenker and bassett 1978 koenker 2005 on the methodological framework ouali et al 2016 for an application on regional frequency analysis in hydrology and weerts et al 2011 lÃ³pez lÃ³pez et al 2014 dogulu et al 2015 for applications on hydrological post processing and section 2 5 1 b quantile regression neural networks qrnn where artificial neural networks are used to quantify the relationship between predictor variables and conditional quantiles of dependent variables see e g taylor 2000 and bogner et al 2016 for an application on flood forecasting systems within the broader class of regression schemes one can also consider a autoregressive models with exogenous variables arx see e g reinsel 1979 hannan et al 1980 box et al 2015 and seo et al 2006 for an application b vector autoregressive models with exogenous variables varx see e g hannan et al 1980 on the methodological framework and bogner and pappenberger 2011 for an application c use of ensemble kalman filtering techniques see e g kalman 1960 evensen 1994 and vrugt and robinson 2007 for an application d generalized additive models gamlss where the distribution parameters of dependent variables are modelled using regression algorithms see e g rigby and stasinopoulos 2005 and yan et al 2014 for an application on river storage forecasts in an effort to improve the accuracy of hydrological predictions methods to combine probabilistic forecasts originating from the application of algorithmic schemes to the outputs of hydrological models hereafter referred to as base learners see e g alpaydin 2014 p 487 have started gaining prominence these include bayesian model averaging bma see e g min and zellner 1993 raftery et al 1997 2005 non homogenous gaussian regression ngr see e g gneiting et al 2005 and the beta transformed linear pool blp see e g ranjan and gneiting 2010 gneiting and ranjan 2013 among other see e g the reviews in bogner et al 2017 baran and lerch 2018 and wang et al 2019 most regression models belong to the families of statistical learning sl see e g hastie et al 2009 james et al 2013 or machine learning ml algorithms with the distinction between the two terms being primarily a matter of scientific debate see e g bzdok et al 2018 for brevity in what follows we use the term machine learning ml for the algorithms and general methodological framework and skip the alternative term machine learning algorithms belong to the class of nonparametric methods thus not providing explicit expressions for the pdfs of the obtained forecasts the latter need to be estimated independently in the context of each specific application and hydrological model used to be properly combined using methods such as bma blp ngr etc see above recognizing the need to combine probabilistic predictions without obtaining explicit expressions for the pdfs of the base learners wang et al 2019 proposed the constrained quantile regression averaging cqra method to directly combine quantile forecasts and predict electricity demand cqra is based on the minimization of the quantile score qs see e g koenker and machado 1999 friederichs and hense 2007 bentzien and friederichs 2014 referred to as pinball loss in hong et al 2016 wang et al 2019 over all targeted quantiles and forecast horizons using linear programming to estimate optimal weights for all individual probabilistic forecasts the method is capable of combining probabilistic forecasts independent of whether their predictive pdfs exhibit closed forms e g as in tyralis and koutsoyiannis 2014 note that qs has been consistently used in hydrological post processing to characterize the distribution properties of predictand variables bogner et al 2016 2017 as well as the quality reliability sharpness level of calibration etc of the predictions the aim of this study is to propose a novel method to improve probabilistic predictions provided by single quantile regression algorithms by combining probabilistic hydrological forecasts in the absence of explicit expressions for the pdfs of the base learners we are interested in obtaining central prediction intervals therefore the method is based on the minimization of the interval score is also referred to as winkler score gneiting and raftery 2007 and combines base learners using stacked generalization stacking wolpert 1992 following the cqra method stacking focuses on the performance of the combination of the algorithms in contrast to the widely used in hydrology bayesian model averaging which may produce largely inaccurate results as proved by yao et al 2018 furthermore it has been suggested that combining quantile forecasts as e g in the cqra method should be preferred compared to combining distribution forecasts e g in the context of simple averaging lichtendahl et al 2013 we introduce the method with the aim to improve probabilistic predictions when post processing the outputs of hydrological models we assess the proposed methodology by applying it to 511 basins in the contiguous us conus using temperature precipitation and streamflow data sourced from camels catchment attributes and meteorology for large sample studies dataset two experiments are conducted in the 511 basins i e a one step ahead prediction see e g evin et al 2014 and b post processing of hydrological model simulations the assessment is of large scale see e g the review in beck et al 2017 and therefore it can effectively serve for validation of the introduced method large scale assessments are increasingly used in hydrological modelling and forecasting see e g perrin et al 2001 mouelhi et al 2006a b bourgin et al 2015 langousis et al 2016 beck et al 2017 tyralis and papacharalampous 2017 2018 bock et al 2018 papacharalampous et al 2018a b c 2019a c tyralis et al 2018 xu et al 2018 as their results are more general than those of case studies while only few large scale studies currently appear in the literature of hydrological post processing see e g pagano et al 2013 in sections 2 and 3 3 we introduce the proposed general framework and its technical aspects in section 4 we apply the suggested approach within the concept of hydrological post processing for 511 basins as outlined above and illustrate its improved performance relative to the base learners used sections 4 1 and 5 discuss the obtained results as well as general concepts regarding the application of the method 2 methods the definitions and nomenclature for the variables sets and methods used hereafter are detailed in appendix a appendix b outlines the software packages used to implement the presented methods and illustrations 2 1 general introduction stacked generalization is a type of ensemble learning alpaydin 2014 pp 487 515 introduced by wolpert 1992 where the base learners are combined using another learner usually referred to as the combiner learner see e g alpaydin 2014 p 504 a note to be made here is that ensemble learning of ml algorithms should not be confused with the general concept of ensemble forecasting in hydrology which implies that the estimation variance of hydrological quantities can be obtained from the spread of the ensemble member forecasts originating from different hydrological models see e g gneiting et al 2005 in the context of probabilistic forecasts ensemble learning stands for the use of multiple ml algorithms to obtain individual probabilistic forecasts and their subsequent combination through a combiner learner to obtain prediction intervals for example the cqra method wang et al 2019 relies on weighted averaging of predictive quantiles of the base learners the base learners used herein are quantile regression qr and quantile regression forests qrf meinshausen 2006 see section 2 5 for details qrf are based on random forests rf breiman 2001 and they have been used for hydrometeorological post processing by taillardat et al 2016 as well as in other hydrological applications see e g bhuiyan et al 2018 here qrf are introduced in the context of hydrological post processing for combiner learner we use the weighted sum of the predictive quantiles with estimated weights that minimize the is we produce probabilistic streamflow predictions at daily timescale by post processing streamflow simulations the latter are obtained via the gr4j gÃ©nie rural Ã  4 paramÃ¨tres journalier lumped conceptual hydrological model introduced by perrin et al 2003 other hydrological models can be also used however our focus here is on the post processing procedure by considering two experiments a experiment 1 one step ahead predictions as e g in evin et al 2014 where at each time step of the prediction period the base learners use observed streamflow information from the previous day and the same day hydrological model output b experiment 2 post processing of hydrological model simulations where at each time step the base learners use hydrological model outputs for the current and two previous days the proposed framework can also be applied by selecting different predictor variables for the base learners as e g ye et al 2014 and or used to obtain probabilistic predictions at multiple steps ahead we run the calibrated hydrological model in simulation mode i e we obtain the streamflow simulations by using recorded temperature and precipitation as input data klemeÅ¡ 1986 see e g vrugt and robinson 2007 montanari and grossi 2008 zhao et al 2011 evin et al 2014 ye et al 2014 dogulu et al 2015 in this way we assess the performance of ml algorithms in post processing hydrological model outputs avoiding possible influences imposed by the accuracy of weather forecasts for the proposed methodology to be used for forecasting purposes one needs to run the hydrological model in forecast mode i e to use temperature and precipitation forecasts instead of recorded quantities klemeÅ¡ 1986 in this case the pdfs of the predictand variables are expected to be wider hemri 2018 reflecting an increase in the uncertainty of the predictions the latter is imposed by the intrinsically uncertain character of the weather forecasts an alternative way to use post processing approaches in forecasting is to train the post processor assuming no uncertainty in the inputs and then combine input uncertainty and post processing see e g krzysztofowicz 1999 pagano et al 2013 2 2 the ensemble learner in this section we present the general framework of the proposed methodology brief descriptions of its specific components are given in section 2 5 we define the interval score of base learner n at time t for a prediction interval 1 a 0 a 1 as gneiting and raftery 2007 1 l n t a y n t a 2 y n t 1 a 2 y t y n t 1 a 2 y n t a 2 2 a y n t a 2 y t 1 y t y n t a 2 2 a y t y n t 1 a 2 1 y t y n t 1 a 2 is is a proper scoring rule to assess the properties of prediction intervals see e g gneiting and raftery 2007 which traces back to dunsmore 1968 and winkler 1972 see e g gneiting and raftery 2007 and has been used to assess the quality of hydrometeorological forecasts see e g hamill and wilks 1995 and hydrological predictions see e g bock et al 2018 papacharalampous et al 2019b c the reliability score which is related to is has been used to assess the performance of algorithms for hydrological post processing see e g ye et al 2014 also let t 1 n 1 n 2 n 3 where the period t with available observations has been divided into three consecutive subperiods t 1 t 2 and t 3 containing n 1 n 2 and n 3 values respectively the stacked algorithm is trained in the period t 1 t 2 whereas period t 3 i e an independent period with data not used for training is used to test the stacked algorithm in what follows we outline the algorithmic steps used to combine the probabilistic predictions for a specific prediction interval 1 a see also fig 1 for an illustration step 1 train the base learners in subperiod t 1 each of the n base learners fn q q a 2 1 a 2 is trained independently in subperiod t 1 using x t as predictor variables and yt as dependent variables where t t 1 step 2 use the base learners to obtain predictions in subperiod t 2 the trained base learners of step 1 are used to predict yn t q t t 2 n n q a 2 1 a 2 where x t are used as predictor variables of the trained base learners i e yn t q fn q x t step 3 stacked generalization the quantity t lt a yt a 2 yt 1 a 2 yt is minimized in subperiod t 2 where yt q n wn a yn t q q a 2 1 a 2 subject to the constraints n wn a 1 and wn a 0 1 n n the aim is to obtain proper weights wn a that minimize the total loss over different times t i e t lt a yt a 2 yt 1 a 2 yt step 4 retrain the base learners using the whole training period t 1 t 2 each of the n base learners fn q q a 2 1 a 2 is trained independently again in the period t 1 t 2 using x t as predictor variables and yt as dependent variables t t 1 t 2 step 5 obtain predictions in test period t 3 the predictive quantile yt q q a 2 1 a 2 at time t t 3 for a given predictor variable x t is calculated as yt q fe q x t where fe q denotes the weighted sum with weights estimated in step 3 of the quantiles obtained from the base learners trained in period t 1 t 2 to calculate the weights of the ensemble learner we employ two alternative approaches in the first approach termed as ensemble learning method 1 for each value of a steps 1 5 are applied leading to weight combinations for the base learners that differ for each prediction interval 1 a in the second approach termed as ensemble learning method 2 step 3 is modified to minimize the quantity a t lt a yt a 2 yt 1 a 2 yt i e the total loss over several prediction intervals and times instead of t lt a yt a 2 yt 1 a 2 yt hence in the second approach the obtained weight combinations for the base learners are invariant with respect to the prediction interval 1 a i e wn are estimated instead of wn a 2 3 comparison to other methods the ensemble learners 1 and 2 are compared to a the simple averaging approach which assigns equal weights i e in our case Â½ for two base learners to all quantile forecasts simple averaging is an important benchmark as it corresponds to an equally weighted opinion pool that is hard to beat in practice see e g lichtendahl et al 2013 simple averaging has been exploited in papacharalampous et al 2019b c in a different context i e by averaging multiple quantile predictions on the order of hundreds obtained using simulations from a single hydrological model in this case simple averaging was selected as an alternative to weight optimization which may result in prohibitive computational requirements due to the large number of applied weights b ensemble learners 3 and 4 which correspond to ensemble learners 1 and 2 see previous section respectively with the difference that step 4 of the algorithm i e retraining of the base learners in period t 1 t 2 is omitted thus prediction is made using the trained base learners of step 1 this comparison allows quantification of the information gain when retraining the base learners in a longer period i e t 1 t 2 c the qr and qrf base learners used to form the ensemble learners 2 4 specific remarks on the proposed algorithm 2 4 1 fundamental concepts the proposed algorithm borrows concepts from the fields of hydrology machine learning and statistics the first basic concept originating from the field of statistics is use of the interval score is defined in eq 1 use of is is substantiated by theoretical arguments see e g gneiting and raftery 2007 with lower values indicating better performance of the base learners is penalizes wider prediction intervals through component yn t 1 a 2 yn t a 2 in eq 1 as well as intervals that do not contain observations i e through the component of eq 1 that remains after subtraction of the interval width the latter penalty hereafter referred to simply as penalty increases with the distance of the observations outside the prediction interval and although more general it is implicitly linked to the reliability score rs which is defined here for base learner n as 2 rs n a t 1 y n t y n t a 2 y n t 1 a 2 t an optimal rs should have value equal to 1 a i e 1 a of the observed values should fall inside the 1 a prediction interval ranking of methods can be conducted by averaging the implemented scores over a fixed set of forecasts see e g gneiting and raftery 2007 with better performing methods exhibiting lower scores a second concept borrowed from the field of machine learning is implementation of stacked generalization see step 3 above stacked generalization or stacking is a type of ensemble learning introduced by wolpert 1992 see e g alpaydin 2014 p 504 for a comprehensive description of the algorithm where a combiner learner is used to improve the predictions of the base learners see e g breiman 1996a smyth and wolpert 1999 with the latter been used as input under this setting the base learners and the combiner learner need to be trained over different sets here this is achieved by splitting the training period into two subperiods t 1 and t 2 simultaneous fitting of the ml algorithms i e step 1 above and estimation of the weights i e steps 2 and 3 above using the whole t 1 t 2 period is generally not recommended as ml algorithms tend to overfit leading to superior algorithmic performances in the training set relative to independent test sets this has been verified also in the context of the present study where we found that qrf completely dominated qr other ensemble learning methods also exist see e g the review by sagi and rokach 2018 two of the most widely used are bagging breiman 1996b and boosting friedman 2001 see also the reviews by natekin and knoll 2013 mayr et al 2014 bagging averages multiple weak learners i e learners with low performance or unstable learners while in boosting new weak base learners are progressively introduced and trained to minimize the error of the ensemble learner following an iterative procedure thus new models are progressively added to the ensemble instead stacking which is a meta learning method uses diverse base learners to gain in performance sagi and rokach 2018 the overall performance of the ensemble learner formed by the combiner learner and the base learners depends on the efficiency of the combiner learner to properly weigh the base learners within a given test set and depends on the effectiveness of its calibration in the training period t 2 as well as potential similarities between periods t 1 and t 2 the splitting problem of a set into training and validation periods is common to all areas of hydrology and machine learning and addressing it goes beyond the scopes of the present study here the training set is partitioned into two subperiods i e t 1 and t 2 of almost equal lengths i e 8 and 6 years respectively whereas the test set i e subperiod t 3 includes 30 of the available data i e 6 out of 20 years see section 3 2 for details similar relative lengths for the corresponding training and test periods have been used in other ml studies see e g antal et al 2003 yu and xu 2008 papacharalampous et al 2019a the overall results can be considered reliable as the length of the available data allows examination of various patterns of low and high flows as well as other statistical attributes of the data 2 4 2 differences from existing frameworks the proposed algorithm borrows concepts from wang et al 2019 and trapero et al 2019 who used qs as a loss function and yao et al 2018 who combined closed expressions of probabilistic forecasts in the former two studies the weights of the base learners were estimated by minimizing the qs across all targeted quantiles and forecast horizons here we are interested in estimating optimal prediction intervals i e pairs of quantiles in the form of prediction ranges and therefore minimization of is is more suitable than minimization of qs the latter would lead to doubling the number of the applied weights i e one weight per bound in qs vs one weight per interval in is thus increasing the uncertainty of the resulting predictions we also note that existing applications of qs and is concepts are limited to fields outside hydrology additional advantages when using is minimization to combine probabilistic forecasts relative to other methods e g bma blp ngr see introduction are that a the weight search can be formulated as a linear programming problem with considerable increase of accuracy and computational efficiency of the algorithm and b quantile crossing issues are minimized as the obtained weights do not depend on posterior distributions that may present multi modal features for an extensive discussion on the merits of stacked generalization relative to bma the reader is referred to wang et al 2019 further advantages of the method inherit from the properties of stacked generalization which is a general methodology with deep theoretical background for details see wolpert 1992 and the fact that the method is simple straightforward to use computationally efficient i e it takes approximately 45 min to process 511 basins with 30 years of data each including hydrological model simulations on a regular pc and practical due to its full automation trapero et al 2019 2 5 base learners general guidelines for the selection of base learners are presented in alpaydin 2014 pp 488 491 in brief the base learners should be simple accurate and diverse so they complement each other here we use qr and qrf as base learners but the method can combine more than two quantile regression base learners the ensemble learner can also include different base learners which originate from the same ml algorithm e g qrf implemented with different parameters or predictor variables two reviews on quantile regression algorithms detailing recent progress in the field can be found in koenker 2017 and waldmann 2018 in general quantile regression algorithms model the conditional quantiles of dependent variables as functions of predictor variables while a detailed presentation of the implemented ml algorithms goes beyond the scopes of the present study brief descriptions of the methods and software packages used for their implementation are presented below 2 5 1 quantile regression linear in parameters quantile regression qr was introduced by koenker and bassett 1978 while an extended treatment of the method can be found in koenker 2005 the method uses similar techniques to linear regression to estimate the quantiles of a dependent variable conditional on predictor variables its main difference relative to linear regression is that minimization is conducted in terms of conditional quantiles whereas linear regression considers the conditional mean of the response variable an intuitive explanation of qr is that it fits a linear model and bisects the data so that 100 q lie below the predicted values of the fitted model practically this is done by fitting a linear model to the data and minimizing the average qs the method is suitable for modelling heteroscedasticity koenker 2005 p 25 we apply the method using the rq r function of the quantreg r package koenker 2018 which implements the fitting algorithm proposed by koenker and d orey 1987 1994 2 5 2 quantile regression forests quantile regression forests qrf were introduced by meinshausen 2006 the algorithm is based on random forests rf breimanm 2001 see also biau and scornet 2016 with interest being on conditional quantiles rather the conditional mean rf is a very accurate algorithm as proved by its performance in practical problems and competitions examples include successful use of rf in hydrology see e g nikolopoulos et al 2018 papacharalampous and tyralis 2018 point time series forecasting in hydrometeorological applications see e g tyralis and papacharalampous 2017 papacharalampous et al 2018a 2019a and spatial interpolation of hydrological quantities see e g tyralis et al 2018 2019b an extensive review on the use of rf in water sciences can be found in tyralis et al 2019a and a detailed description of the algorithm can be found in hastie et al 2009 pp 587 604 in a regression setting random forests average an ensemble of decision trees the ensemble is created by bagging abbreviation for bootstrap aggregation breiman 1996b regression trees in addition to bagging the splitting at the nodes of the regression tree is conducted by randomly selecting a fixed number of predictor variables thus inducing an additional degree of randomization which increases accuracy of the algorithm similar to random forests which approximate conditional means quantile regression forests approximate conditional quantiles this is done by averaging the indicator functions of the events exhibiting decision tree outcomes in the test set lower than a predefined quantile level here we apply qrf using the quantile forest r function of the grf r package tibshirani et al 2018 which emulates meinshausen s 2006 algorithm see also athey et al 2019 the corresponding algorithm is straightforward and very simple to use with a few parameters to tune while the default values in the software implementation are near optimal see e g the discussion in verikas et al 2011 oshiro et al 2012 scornet et al 2015 biau and scornet 2016 probst and boulesteix 2018 tyralis et al 2019a therefore optimization of the algorithm is omitted considering also that interest is in the relative improvement of the combiner learner with respect to the base learners used other properties of random forests are that they demonstrate high predictive performance they are non linear and non parametric they are fast compared to other machine learning algorithms and they are stable and robust to the inclusion of noisy predictor variables while they do not extrapolate outside the training range within the test set see e g biau and scornet 2016 tyralis et al 2019a 3 data and models 3 1 data a detailed presentation of camels dataset used in the present study can be found in addor et al 2017a b newman et al 2014 2015 2017 and thornton et al 2014 the dataset comprises of daily hydrometeorological and streamflow data from 671 small to medium sized basins in conus for each basin the daily minimum and maximum temperatures and precipitation have been obtained by processing the daily dataset of thornton et al 2014 changes in the basins due to human influences are minimal therefore use of ml algorithms for uncertainty characterization is an acceptable option see e g solomatine and wagener 2011 regarding the requirements for statistical similarity between subperiods when applying ml methods and koutsoyiannis and montanari 2015 regarding the appropriateness of the assumption of stationarity when changes cannot be explained deductively here we focus on the 34 year period 1980 2013 and exclude basins with missing data or other inconsistencies the final sample consists of 511 basins representing most climate types over conus see fig 2 for each of the 511 basins we estimate the mean daily temperature as the average of the respective minimum and maximum daily temperatures the daily potential evapotranspiration pet is estimated by implementing oudin s formula oudin et al 2005 for the latter we use the pedaily oudin r function of the airgr r package for details see coron et al 2017 2019 with the daily mean temperature as input 3 2 hydrological model the gr4j model constitutes an improvement of the gr3j gÃ©nie rural Ã  3 paramÃ¨tres journalier model by edijatno et al 1999 and comprises of four parameters while its precursor i e gr3j comprises of three parameters perrin et al 2003 the use of this small number of parameters is fully justified in perrin et al 2001 the hydrological model is herein calibrated in a non adaptive way i e the calibration is performed once for each basin and the hydrological model is thereafter applied with fixed parameter values see e g toth et al 1999 although feasible we do not perform adaptive calibration see e g brath and rosso 1993 ye et al 2014 as its benefits are delivered by the base learners in the context of the hydrological post processing framework toth et al 1999 we use the airgr r package to apply the gr4j hydrological model to each basin we simulate daily streamflow with recorded daily precipitation and pet as input the period 1980 1981 is used to warm up the hydrological model while period 1982 1993 is used for model calibration using the calibration michel r function of the airgr r package the latter function implements michel s 1991 optimization algorithm using the nash sutcliffe criterion nash and sutcliffe 1970 to characterize the quality of the hydrological simulations relative to recorded streamflows following the notation presented in section 2 2 we define the periods t 1 1994 01 01 2001 12 31 t 2 2002 01 01 2007 12 31 t 3 2008 01 01 2013 12 31 and use the calibrated hydrological model to simulate daily streamflows for the total period t t 1 t 2 t 3 the simulated streamflow vt at time t is calculated using information until day 1993 12 31 for yt i e the recorded streamflow and until day t for pr t and pet t i e precipitation and potential evapotranspiration respectively the final product consists of 511 simulated streamflow series at a daily resolution in period t with a total of 1 120 112 simulated values in period t 3 where the ensemble learner is tested i e 2192 simulated streamflow values for each of the 511 basins 3 3 technical aspects post processing aims at estimating the uncertainty of the predictand variable conditional on model simulations see introduction in experiment 1 i e one step ahead predictions see section 2 1 the predictor variable is defined as x t yt 1 vt use of the last streamflow observation yt 1 as predictor is supported by numerous relevant examples see e g krzysztofowicz 1987 seo et al 2006 evin et al 2014 bogner et al 2016 due to the high magnitude of dependence between sequential streamflow observations in experiment 2 the predictor variable is defined as x t vt vt 1 vt 2 and corresponds to the case of post processing hydrological model simulations the q th quantiles of the predictand can be obtained by post processing x t through yntq fe q x t when using ml algorithms it is common to pre process the data by applying some transformation with the aim to increase the performance of the model appropriate transformations can be applied to both yt and vt several options are available in the existing literature such as the arcsinh log square root box cox and yeo johnson transformations as well as the normal quantile transformation see e g krzysztofowicz 1997 bogner et al 2012 all aforementioned normalization transformations can be implemented using the bestnormalize r package peterson 2018 the selected transformation should be applied to both simulated and observed streamflows in the training sets and all ml calculations should be performed using transformed quantities the inverse transformation is then applied to the predicted quantiles we tried all previously mentioned transformations and found that the square root transformation was the only one not resulting in unrealistically high quantiles by the qr algorithm in the examined cases when compared to conventional statistical methods waldmann 2018 qr is more robust and less sensitive to the existence of outliers of the dependent variable while rf is invariant to monotonic transformations of the predictor variables dÃ­az uriarte and de andres 2006 the square root transformation has also been used by messner 2018 for the purposes of hydrometeorological post processing other issues which need to be addressed in most post processing applications include heteroscedastic behaviour of the data censoring i e in case the predicted quantiles exhibit negative values and quantile crossing problems regarding heteroscedasticity it can be theoretically addressed by using base learners that can effectively model heteroscedastic behaviour such as the qr used in the present study problems of negative quantiles were minimal in the present application in the case of qrf base learners negative values are by definition not possible as the predicted quantiles constitute subsets of the values found in the training set for the qr base learners the problem of negative quantiles was addressed by censoring them quantile crossing problems were also minimal in the present application and have been addressed by properly adjusting the corresponding quantiles similar to the approach of wang et al 2019 according to the latter if quantile q 1 results to be larger than quantile q 2 with q 1 q 2 then quantile q 2 is set equal to quantile q 1 4 results and discussion for period t 3 fig 3 summarizes information on the simulated and observed streamflows for all basins analysed fig 3 a presents a scatterplot for the same and previous day observed discharges yt and yt 1 respectively while fig 3 b d show scatterplots of yt with respect to the simulated streamflows vt vt 1 and vt 2 respectively regarding fig 3 b one sees that the linear regression line red between the same day hydrological simulations and observations is close to the 45 degree line black indicating that the hydrological model pre processes the data relatively well however there seems to be a moderate negative bias in the estimation of high flows as indicated by the points lying above the 45 degree line also as physically expected the deviation between observed and simulated flows increases with increasing lag times see fig 3 c d fig 3 a illustrates the significant positive correlation of observed streamflows in two sequential days i e yt and yt 1 indicating the appropriateness of using x t yt 1 vt as a predictor variable in hydrological post processing clearly the deviation of the linear regression line in fig 3 a from the 45 degree line is larger than that in fig 3 b indicating the important pre processing role of the hydrological model regarding high flows the respective points in fig 3 a are scattered symmetrically around the 45 degree line this is statistically justifiable as the probabilities of observing higher or lower flows in day t with respect to day t 1 are approximately equal the appropriateness of using x t yt 1 vt as a predictor variable in hydrological post processing is also illustrated in fig 4 which shows histograms of correlations obtained for validation period t 3 and all considered basins between yt and a yt 1 b vt c vt 1 and d vt 2 one sees that the correlations between yt and each of the variables yt 1 and vt are generally higher relative to the correlations between the observed streamflow yt at time t and the simulated streamflows at earlier times i e vt 1 and vt 2 correlation histograms obtained for validation period t 1 t 2 are similar to those in fig 4 not shown here for brevity fig 5 shows an example of post processed hydrological simulations in the context of experiments 1 and 2 at an arbitrary basin the 0 025 and 0 975 quantiles of the base learners qr and qrf and ensemble learners 1 and 2 are also presented visual inspection of the post processed simulations indicates that qr qrf and ensemble learners 1 and 2 produce intervals that in general include yt in experiment 2 the prediction intervals are wider due to the larger degree of uncertainty induced by the absence of the previous day observed streamflow yt 1 as predictor variable in the next section we quantitatively assess the performance of each method including ensemble learners 3 and 4 as well as simple averaging using proper metrics 4 1 performance assessment for brevity and without loss of generality in what follows we centre the discussion to experiment 1 the results of experiment 2 are presented through comparison of performances relative to experiment 1 for all basins analysed we assess the predictive performance of ensemble learners 1 4 and the simple averaging method in period t 3 the assessment is made by estimating the relative improvement ri introduced with respect to each of the base learners for instance the relative improvement ri of the interval score of learner i with respect to the n th base learner used for benchmarking is defined as 3 ri l n a l i a l n a similarly by substituting the interval score by its components i e interval widths and penalty see section 2 4 1 one can obtain their relative improvements as well see section 4 2 for a detailed analysis and presentation of findings regarding experiment 1 fig 6 a shows the mean ri over all basins of ensemble learners 1 4 and simple averaging with respect to qr for different prediction intervals 1 a 20 40 60 80 90 95 fig 6 b presents similar results to fig 6 a but for experiment 2 a positive value of ri indicates that the examined learner improves over the benchmark learner values equal to 0 indicate that the examined and benchmark learners are identical regarding experiment 1 when compared to qr ensemble learners 1 and 2 improve more than 10 at prediction intervals below 80 while the relative improvement decreases to 5 at higher prediction intervals when compared to qrf the relative improvement is 1 2 at low prediction intervals and increases to more than 5 at higher prediction intervals lower prediction intervals are representative of the median values of the streamflow whereas higher prediction intervals can be used to predict low and high flows the diverse properties of the two base learners with respect to the magnitude of the prediction interval are also presented in fig 6 the performance of the qrf base learner is 9 better relative to qr at low prediction intervals and it decreases at higher prediction intervals a probable reason for this is that by construction qrf cannot predict beyond the range of observed flows in the training set whereas the qr algorithm is regression based allowing for extrapolation beyond this range at all prediction intervals considered the performances of ensemble learners 3 and 4 are approximately 2 lower compared to ensemble learners 1 and 2 respectively consequently retraining of the algorithm in period t 2 step 4 is beneficial and should be preferred the relative improvement of ensemble learner 1 over simple averaging is approximately 2 at prediction intervals below 80 with the two methods sharing similarly good performances at higher prediction intervals regarding experiment 2 see fig 6 b one sees that the ri curves are shifted downwards relative to fig 6 a indicating lower overall performances associated with the larger degree of uncertainty relative to experiment 1 induced by the absence of the previous day observed streamflow as predictor variable in addition while ensemble learners 1 and 2 perform better than the base learners simple averaging performs equally well to both ensemble learners 1 and 2 at all prediction intervals this important result indicates that the outcome of optimal weight selection is strongly influenced by the uncertainty of the predictor predictand relationship more precisely as the level of uncertainty increases e g experiment 2 relative to experiment 1 weight optimization may not lead to significant improvements relative to simple averaging i e a uniform weighting scheme that assigns equal weights to all base learners when averaged over all prediction intervals the relative improvement of the interval score of ensemble learner 1 in experiment 1 is 8 84 with respect to qr and 4 43 with respect to qrf the corresponding improvements introduced by ensemble learner 2 are 8 55 with respect to qr and 4 18 with respect to qrf and by simple averaging are 7 90 and 3 60 respectively the slight improvement of ensemble learner 1 in experiment 1 should be attributed to its higher flexibility not compromised by overfitting clearly the two ensemble learners are able to exploit the diverse properties of the base learners and improve uniformly over them demonstrating improved performance relative to simple averaging by approximately 1 also it follows from the discussion above that the first ensemble learner is approximately 0 5 more efficient relative to the second one the reason for this is that the first learner uses a combiner algorithm that allows for additional degrees of freedom as the weights applied to the base learners may vary with the prediction interval 1 a see section 2 2 note that the aforementioned increase of predictive performances over the base learners is significant especially due to the size of the test set i e 511 time series each one consisting of 34 years of daily streamflow observations for example in their study wang et al 2019 indicate 4 39 average relative improvement of the quantile score with respect to the three base learners used based on eight daily time series of electricity consumption each one consisting of four years of data although smaller due to the larger uncertainty of the predictor predictand relationship see above improvements of ensemble learners 1 and 2 over the base learners are also observed in experiment 2 see fig 6 b in addition both ensemble learners appear to be overall equivalent to simple averaging indicating that weight optimization does not lead to significant improvements relative to the uniform weighting scheme of simple averaging fig 7 presents histograms of the relative improvements of the is see eq 3 introduced by the two ensemble learners in experiment 1 for all considered basins relative to the two base learners each histogram consists of 3066 values which correspond to six values i e one per prediction interval 1 a 20 40 60 80 90 95 per basin in all cases the improvements are mostly positive and well dispersed indicating that the results presented in fig 6 i e the mean relative improvement of each ensemble learner relative to the two base learners are not dominated by exceptional performances of the ensemble learners over a limited set of basins figs 8 and 9 present boxplots of the average interval scores is in experiment 1 and experiment 2 respectively for period t 3 one sees that a independent of the experiment and method used is increases with increasing 1 a b in both experiments ensemble learners 1 4 and simple averaging improve over the base learners and c is in experiment 1 are generally lower than those in experiment 2 thus confirming that yt 1 i e used as predictor variable in experiment 1 is more informative than vt 1 and vt 2 combined i e used as predictor variables in experiment 2 4 2 components of the interval score to gain further insight regarding the performance of each method in the testing period t 3 fig 10 presents for both experiments the ensemble mean over all basins of the absolute differences between the reliability scores see section 2 4 1 and the corresponding nominal prediction intervals one can see that in experiment 1 qr performs better than qrf at prediction intervals below 60 whereas the performances are reversed at higher prediction intervals the differences are less pronounced in experiment 2 with qr performing better than qrf at all prediction intervals in both experiments ensemble learners 3 and 4 demonstrate limited performance relative to ensemble learners 1 and 2 with the latter two exhibiting similar performances to simple averaging balancing those of qr and qrf base learners fig 11 presents the median relative improvements i e with respect to qr of the performances of base learner qrf ensemble learners 1 4 and simple averaging in terms of prediction interval widths median values are preferred over mean values to avoid influences by very low i e near zero prediction intervals while the performances of all methods are comparable in experiment 2 see fig 11 b due to the higher level of uncertainty induced by the absence of the previous day observed streamflow as predictor variable in experiment 1 see fig 11 a qr uniformly dominates qrf and all ensemble methods used fig 12 presents the ensemble mean over all basins of the relative improvements with respect to qr of penalties associated with intervals that do not contain observations see section 2 4 1 the general pattern is similar to that of interval scores in fig 6 indicating that penalties are an important contributor to the interval score 4 3 weights to gain insight on how the weights of the ensemble learners see section 2 2 are affected by the performances of the base learners fig 13 shows for ensemble learner 1 scatterplots of the weights assigned to the qr base learner as a function of the relative improvement of the average interval score of qrf relative to qr for different prediction intervals 1 a as expected one sees that independent of the experiment i e 1 fig 13 a or 2 fig 13 b the weights assigned to the qr base learner tend to decrease when the relative improvement of qrf relative to qr increases for ensemble learner 1 and experiment 1 fig 14 presents histograms of the weights assigned to the qr base learner for varying prediction intervals 1 a when the prediction interval 1 a increases the weights increase as well this should be expected because the relative gain in performance when referring to the interval score of qr with respect to qrf increases for higher prediction intervals spikes at the edges of the histograms correspond to cases where the qr base learner completely dominates qrf or vice versa resulting in weights equal to 1 or 0 respectively 5 concluding remarks ensemble learning of base learners can result in improved performance of probabilistic predictions the few existing methods require formal definition of the likelihoods of the base learners which is too restrictive as most base learners cannot provide explicit expressions for the pdfs of the obtained forecasts in this study we borrowed concepts from wang et al 2019 to propose an ensemble learner which uses stacked generalization to linearly combine the quantile predictions of non parametric base learners i e quantile regression and quantile regression forests algorithms using weights that minimize the interval score of the resulting prediction the method was tested using a large dataset consisting of 511 basins the conducted tests focused in delivering one step ahead predictions experiment 1 as well as in post processing simulations of a conceptual hydrological model experiment 2 it was found that the ensemble learners improve over the performance of the best base learner by 1 5 depending on the experiment and the prediction interval the suggested method was also found to outperform simple averaging i e a uniform weighting scheme that assigns equal weights to all base learners or to be sharing the first place with it in all examined cases with the maximum obtained improvement over this tough benchmark being approximately equal to 2 the results are considered significant especially given the length of the sample the algorithm has been tested i e post processing of 1 120 112 hydrological predictions from 511 time series and the fact that simple averaging is hard to beat in practice see e g lichtendahl et al 2013 the latter general observation indicates that when the uncertainty of the predictor predictand relationship increases the effectiveness of weight optimization tends to decrease approaching that of simple averaging i e a uniform weighting scheme with equal weights assigned to all base learners to the best of our knowledge no similar study has been conducted in the hydrological literature with the closest work being that of wang et al 2019 in electricity forecasting the latter is based on minimization of the quantile score qs see introduction indicating 4 39 average relative improvement with respect to the three base learners used whereas in the present study ensemble learning is conducted by minimizing the interval score is resulting e g in experiment 1 to approximately 6 5 average relative improvement over the 2 base learners i e 8 98 4 44 8 66 4 18 4 see section 4 1 also note that application of the constrained quantile regression averaging cqra method of wang et al 2019 was based on eight daily time series of electricity consumption each one consisting of four years of data one should consider the convenience of using the proposed method over other combination methods e g bayesian model averaging as well as theoretical studies that support a stacking against bayesian model averaging and b working with quantile forecasts instead of probability distributions see also section 1 the extended use of machine algorithms in hydrological post processing should also be considered machine learning algorithms are accurate they have been tested extensively in practice as well as in forecasting competitions they are easy to apply due to their open software implementation and they are optimally programmed resulting in considerable decrease of computation times the computations of the present study including fitting of the hydrological model required approximately 45 min on a regular pc thus allowing large scale implementations based on the aforementioned findings we recommend use of the proposed ensemble learners for improving the probabilistic predictions of base learners future research could focus on defining optimal splitting points of the training set used inclusion of more base learners testing the method using forecasts of daily temperature and precipitation as input as well as assessing the performance of stacked generalization when metrics scores other than is see e g gneiting and raftery 2007 shastri et al 2017 are minimized to optimally combine probabilistic forecasts further uses of the method are also possible spanning from hydrological forecasting using data driven models to water demand forecasting to water science problems and beyond e g in electricity load forecasting and more declaration of competing interest the authors declare no conflict of interest acknowledgements we are grateful to the editor associate editor and the reviewers for their constructive comments and suggestions which helped us to improve the manuscript appendix a nomenclature indices 1 a central prediction interval n index of base learners t index of time periods q index of quantiles sets n set of base learners t set of time periods q set of quantiles functions 1 the indicator function a cardinality of a set a fn q the n th base learner for the q th quantile fe q the ensemble learner for the q th quantile fn the n th base learner variables x t inputs of a model at time t yt actual streamflow at time t pr t observed daily precipitation at time t pet t observed daily potential evapotranspiration at time t v t simulated daily streamflow at time t yn t q the forecasted q th quantile of the n th base learner at time t yt q the forecasted q th quantile of the ensemble learner at time t ln t a interval score of the n th base learner at time t for the 1 a prediction interval lt a interval score of the weighted average of the n methods at time t for the 1 a prediction interval appendix b used software all computations and visualizations were conducted in r programming language r core team 2019 using the following packages airgr coron et al 2017 2019 bestnormalize peterson 2018 data table dowle and srinivasan 2019 devtools wickham et al 2019c doparallel microsoft and weston 2017 dplyr wickham et al 2019b foreach microsoft corporation and weston 2018 gdata warnes et al 2017 ggplot2 wickham 2016 wickham et al 2019a grf tibshirani et al 2018 knitr xie 2014 2015 2019 quantreg koenker 2018 readr wickham et al 2018 reshape2 wickham 2007 2017 rmarkdown allaire et al 2019 stringi gagolewski 2019 stringr wickham 2019 
6263,post processing of hydrological model simulations using machine learning algorithms can be applied to quantify the uncertainty of hydrological predictions combining multiple diverse machine learning algorithms referred to as base learners using stacked generalization stacking i e a type of ensemble learning is considered to improve predictions relative to the base learners here we propose stacking of quantile regression and quantile regression forests stacking is performed by minimising the interval score of the quantile predictions provided by the ensemble learner which is a linear combination of quantile regression and quantile regression forests the proposed ensemble learner post processes simulations of the gr4j hydrological model for 511 basins in the contiguous us we illustrate its significantly improved performance relative to the base learners used and a less prominent improvement relative to the hard to beat in practice equal weight combiner keywords combining probabilistic forecasts ensemble learning hydrological uncertainty interval score quantile regression quantile regression forests 1 introduction an important objective of hydrological models is to predict a variable of interest e g river discharge or runoff volume usually referred to as predictand using the terminology of krzysztofowicz 1999 as a response to other hydrological variables temperature precipitation etc see e g lidÃ©n and harlin 2000 mouelhi et al 2006a b das et al 2008 kaleris and langousis 2017 in this context hydrological models can be classified into three broad categories i e physically based conceptual and data driven see e g solomatine and wagener 2011 the output of the physically based and conceptual models is point predictions of hydrologic quantities which do not allow for direct quantification of predictive uncertainties to account for the latter within the general framework of probabilistic prediction see e g krzysztofowicz and kelly 2000 krzysztofowicz 2001 2002 kavetski et al 2002 montanari and brath 2004 kuczera et al 2006 todini 2007 montanari and grossi 2008 weijs et al 2010 montanari and koutsoyiannis 2012 hernÃ¡ndez lÃ³pez and francÃ©s 2017 tyralis and koutsoyiannis 2017 one needs to estimate the probability distribution function pdf of the predictand variable or the joint probability distribution function of all predictand variables of interest corresponding to different uncertainty sources see e g the detailed review on global uncertainty estimation by montanari 2011 one way to do so is to post process hydrological model outputs using conditional distribution based models regression based methods or other algorithmic approaches see e g li et al 2017 here we are interested in the case where post processing of hydrological predictions is conducted using quantile regression based models for a detailed review of the general framework of regression schemes in the context of hydrometeorological post processing the reader is referred to li et al 2017 examples of relevant algorithms include see e g messner 2018 for a detailed list a quantile regression see e g koenker and bassett 1978 koenker 2005 on the methodological framework ouali et al 2016 for an application on regional frequency analysis in hydrology and weerts et al 2011 lÃ³pez lÃ³pez et al 2014 dogulu et al 2015 for applications on hydrological post processing and section 2 5 1 b quantile regression neural networks qrnn where artificial neural networks are used to quantify the relationship between predictor variables and conditional quantiles of dependent variables see e g taylor 2000 and bogner et al 2016 for an application on flood forecasting systems within the broader class of regression schemes one can also consider a autoregressive models with exogenous variables arx see e g reinsel 1979 hannan et al 1980 box et al 2015 and seo et al 2006 for an application b vector autoregressive models with exogenous variables varx see e g hannan et al 1980 on the methodological framework and bogner and pappenberger 2011 for an application c use of ensemble kalman filtering techniques see e g kalman 1960 evensen 1994 and vrugt and robinson 2007 for an application d generalized additive models gamlss where the distribution parameters of dependent variables are modelled using regression algorithms see e g rigby and stasinopoulos 2005 and yan et al 2014 for an application on river storage forecasts in an effort to improve the accuracy of hydrological predictions methods to combine probabilistic forecasts originating from the application of algorithmic schemes to the outputs of hydrological models hereafter referred to as base learners see e g alpaydin 2014 p 487 have started gaining prominence these include bayesian model averaging bma see e g min and zellner 1993 raftery et al 1997 2005 non homogenous gaussian regression ngr see e g gneiting et al 2005 and the beta transformed linear pool blp see e g ranjan and gneiting 2010 gneiting and ranjan 2013 among other see e g the reviews in bogner et al 2017 baran and lerch 2018 and wang et al 2019 most regression models belong to the families of statistical learning sl see e g hastie et al 2009 james et al 2013 or machine learning ml algorithms with the distinction between the two terms being primarily a matter of scientific debate see e g bzdok et al 2018 for brevity in what follows we use the term machine learning ml for the algorithms and general methodological framework and skip the alternative term machine learning algorithms belong to the class of nonparametric methods thus not providing explicit expressions for the pdfs of the obtained forecasts the latter need to be estimated independently in the context of each specific application and hydrological model used to be properly combined using methods such as bma blp ngr etc see above recognizing the need to combine probabilistic predictions without obtaining explicit expressions for the pdfs of the base learners wang et al 2019 proposed the constrained quantile regression averaging cqra method to directly combine quantile forecasts and predict electricity demand cqra is based on the minimization of the quantile score qs see e g koenker and machado 1999 friederichs and hense 2007 bentzien and friederichs 2014 referred to as pinball loss in hong et al 2016 wang et al 2019 over all targeted quantiles and forecast horizons using linear programming to estimate optimal weights for all individual probabilistic forecasts the method is capable of combining probabilistic forecasts independent of whether their predictive pdfs exhibit closed forms e g as in tyralis and koutsoyiannis 2014 note that qs has been consistently used in hydrological post processing to characterize the distribution properties of predictand variables bogner et al 2016 2017 as well as the quality reliability sharpness level of calibration etc of the predictions the aim of this study is to propose a novel method to improve probabilistic predictions provided by single quantile regression algorithms by combining probabilistic hydrological forecasts in the absence of explicit expressions for the pdfs of the base learners we are interested in obtaining central prediction intervals therefore the method is based on the minimization of the interval score is also referred to as winkler score gneiting and raftery 2007 and combines base learners using stacked generalization stacking wolpert 1992 following the cqra method stacking focuses on the performance of the combination of the algorithms in contrast to the widely used in hydrology bayesian model averaging which may produce largely inaccurate results as proved by yao et al 2018 furthermore it has been suggested that combining quantile forecasts as e g in the cqra method should be preferred compared to combining distribution forecasts e g in the context of simple averaging lichtendahl et al 2013 we introduce the method with the aim to improve probabilistic predictions when post processing the outputs of hydrological models we assess the proposed methodology by applying it to 511 basins in the contiguous us conus using temperature precipitation and streamflow data sourced from camels catchment attributes and meteorology for large sample studies dataset two experiments are conducted in the 511 basins i e a one step ahead prediction see e g evin et al 2014 and b post processing of hydrological model simulations the assessment is of large scale see e g the review in beck et al 2017 and therefore it can effectively serve for validation of the introduced method large scale assessments are increasingly used in hydrological modelling and forecasting see e g perrin et al 2001 mouelhi et al 2006a b bourgin et al 2015 langousis et al 2016 beck et al 2017 tyralis and papacharalampous 2017 2018 bock et al 2018 papacharalampous et al 2018a b c 2019a c tyralis et al 2018 xu et al 2018 as their results are more general than those of case studies while only few large scale studies currently appear in the literature of hydrological post processing see e g pagano et al 2013 in sections 2 and 3 3 we introduce the proposed general framework and its technical aspects in section 4 we apply the suggested approach within the concept of hydrological post processing for 511 basins as outlined above and illustrate its improved performance relative to the base learners used sections 4 1 and 5 discuss the obtained results as well as general concepts regarding the application of the method 2 methods the definitions and nomenclature for the variables sets and methods used hereafter are detailed in appendix a appendix b outlines the software packages used to implement the presented methods and illustrations 2 1 general introduction stacked generalization is a type of ensemble learning alpaydin 2014 pp 487 515 introduced by wolpert 1992 where the base learners are combined using another learner usually referred to as the combiner learner see e g alpaydin 2014 p 504 a note to be made here is that ensemble learning of ml algorithms should not be confused with the general concept of ensemble forecasting in hydrology which implies that the estimation variance of hydrological quantities can be obtained from the spread of the ensemble member forecasts originating from different hydrological models see e g gneiting et al 2005 in the context of probabilistic forecasts ensemble learning stands for the use of multiple ml algorithms to obtain individual probabilistic forecasts and their subsequent combination through a combiner learner to obtain prediction intervals for example the cqra method wang et al 2019 relies on weighted averaging of predictive quantiles of the base learners the base learners used herein are quantile regression qr and quantile regression forests qrf meinshausen 2006 see section 2 5 for details qrf are based on random forests rf breiman 2001 and they have been used for hydrometeorological post processing by taillardat et al 2016 as well as in other hydrological applications see e g bhuiyan et al 2018 here qrf are introduced in the context of hydrological post processing for combiner learner we use the weighted sum of the predictive quantiles with estimated weights that minimize the is we produce probabilistic streamflow predictions at daily timescale by post processing streamflow simulations the latter are obtained via the gr4j gÃ©nie rural Ã  4 paramÃ¨tres journalier lumped conceptual hydrological model introduced by perrin et al 2003 other hydrological models can be also used however our focus here is on the post processing procedure by considering two experiments a experiment 1 one step ahead predictions as e g in evin et al 2014 where at each time step of the prediction period the base learners use observed streamflow information from the previous day and the same day hydrological model output b experiment 2 post processing of hydrological model simulations where at each time step the base learners use hydrological model outputs for the current and two previous days the proposed framework can also be applied by selecting different predictor variables for the base learners as e g ye et al 2014 and or used to obtain probabilistic predictions at multiple steps ahead we run the calibrated hydrological model in simulation mode i e we obtain the streamflow simulations by using recorded temperature and precipitation as input data klemeÅ¡ 1986 see e g vrugt and robinson 2007 montanari and grossi 2008 zhao et al 2011 evin et al 2014 ye et al 2014 dogulu et al 2015 in this way we assess the performance of ml algorithms in post processing hydrological model outputs avoiding possible influences imposed by the accuracy of weather forecasts for the proposed methodology to be used for forecasting purposes one needs to run the hydrological model in forecast mode i e to use temperature and precipitation forecasts instead of recorded quantities klemeÅ¡ 1986 in this case the pdfs of the predictand variables are expected to be wider hemri 2018 reflecting an increase in the uncertainty of the predictions the latter is imposed by the intrinsically uncertain character of the weather forecasts an alternative way to use post processing approaches in forecasting is to train the post processor assuming no uncertainty in the inputs and then combine input uncertainty and post processing see e g krzysztofowicz 1999 pagano et al 2013 2 2 the ensemble learner in this section we present the general framework of the proposed methodology brief descriptions of its specific components are given in section 2 5 we define the interval score of base learner n at time t for a prediction interval 1 a 0 a 1 as gneiting and raftery 2007 1 l n t a y n t a 2 y n t 1 a 2 y t y n t 1 a 2 y n t a 2 2 a y n t a 2 y t 1 y t y n t a 2 2 a y t y n t 1 a 2 1 y t y n t 1 a 2 is is a proper scoring rule to assess the properties of prediction intervals see e g gneiting and raftery 2007 which traces back to dunsmore 1968 and winkler 1972 see e g gneiting and raftery 2007 and has been used to assess the quality of hydrometeorological forecasts see e g hamill and wilks 1995 and hydrological predictions see e g bock et al 2018 papacharalampous et al 2019b c the reliability score which is related to is has been used to assess the performance of algorithms for hydrological post processing see e g ye et al 2014 also let t 1 n 1 n 2 n 3 where the period t with available observations has been divided into three consecutive subperiods t 1 t 2 and t 3 containing n 1 n 2 and n 3 values respectively the stacked algorithm is trained in the period t 1 t 2 whereas period t 3 i e an independent period with data not used for training is used to test the stacked algorithm in what follows we outline the algorithmic steps used to combine the probabilistic predictions for a specific prediction interval 1 a see also fig 1 for an illustration step 1 train the base learners in subperiod t 1 each of the n base learners fn q q a 2 1 a 2 is trained independently in subperiod t 1 using x t as predictor variables and yt as dependent variables where t t 1 step 2 use the base learners to obtain predictions in subperiod t 2 the trained base learners of step 1 are used to predict yn t q t t 2 n n q a 2 1 a 2 where x t are used as predictor variables of the trained base learners i e yn t q fn q x t step 3 stacked generalization the quantity t lt a yt a 2 yt 1 a 2 yt is minimized in subperiod t 2 where yt q n wn a yn t q q a 2 1 a 2 subject to the constraints n wn a 1 and wn a 0 1 n n the aim is to obtain proper weights wn a that minimize the total loss over different times t i e t lt a yt a 2 yt 1 a 2 yt step 4 retrain the base learners using the whole training period t 1 t 2 each of the n base learners fn q q a 2 1 a 2 is trained independently again in the period t 1 t 2 using x t as predictor variables and yt as dependent variables t t 1 t 2 step 5 obtain predictions in test period t 3 the predictive quantile yt q q a 2 1 a 2 at time t t 3 for a given predictor variable x t is calculated as yt q fe q x t where fe q denotes the weighted sum with weights estimated in step 3 of the quantiles obtained from the base learners trained in period t 1 t 2 to calculate the weights of the ensemble learner we employ two alternative approaches in the first approach termed as ensemble learning method 1 for each value of a steps 1 5 are applied leading to weight combinations for the base learners that differ for each prediction interval 1 a in the second approach termed as ensemble learning method 2 step 3 is modified to minimize the quantity a t lt a yt a 2 yt 1 a 2 yt i e the total loss over several prediction intervals and times instead of t lt a yt a 2 yt 1 a 2 yt hence in the second approach the obtained weight combinations for the base learners are invariant with respect to the prediction interval 1 a i e wn are estimated instead of wn a 2 3 comparison to other methods the ensemble learners 1 and 2 are compared to a the simple averaging approach which assigns equal weights i e in our case Â½ for two base learners to all quantile forecasts simple averaging is an important benchmark as it corresponds to an equally weighted opinion pool that is hard to beat in practice see e g lichtendahl et al 2013 simple averaging has been exploited in papacharalampous et al 2019b c in a different context i e by averaging multiple quantile predictions on the order of hundreds obtained using simulations from a single hydrological model in this case simple averaging was selected as an alternative to weight optimization which may result in prohibitive computational requirements due to the large number of applied weights b ensemble learners 3 and 4 which correspond to ensemble learners 1 and 2 see previous section respectively with the difference that step 4 of the algorithm i e retraining of the base learners in period t 1 t 2 is omitted thus prediction is made using the trained base learners of step 1 this comparison allows quantification of the information gain when retraining the base learners in a longer period i e t 1 t 2 c the qr and qrf base learners used to form the ensemble learners 2 4 specific remarks on the proposed algorithm 2 4 1 fundamental concepts the proposed algorithm borrows concepts from the fields of hydrology machine learning and statistics the first basic concept originating from the field of statistics is use of the interval score is defined in eq 1 use of is is substantiated by theoretical arguments see e g gneiting and raftery 2007 with lower values indicating better performance of the base learners is penalizes wider prediction intervals through component yn t 1 a 2 yn t a 2 in eq 1 as well as intervals that do not contain observations i e through the component of eq 1 that remains after subtraction of the interval width the latter penalty hereafter referred to simply as penalty increases with the distance of the observations outside the prediction interval and although more general it is implicitly linked to the reliability score rs which is defined here for base learner n as 2 rs n a t 1 y n t y n t a 2 y n t 1 a 2 t an optimal rs should have value equal to 1 a i e 1 a of the observed values should fall inside the 1 a prediction interval ranking of methods can be conducted by averaging the implemented scores over a fixed set of forecasts see e g gneiting and raftery 2007 with better performing methods exhibiting lower scores a second concept borrowed from the field of machine learning is implementation of stacked generalization see step 3 above stacked generalization or stacking is a type of ensemble learning introduced by wolpert 1992 see e g alpaydin 2014 p 504 for a comprehensive description of the algorithm where a combiner learner is used to improve the predictions of the base learners see e g breiman 1996a smyth and wolpert 1999 with the latter been used as input under this setting the base learners and the combiner learner need to be trained over different sets here this is achieved by splitting the training period into two subperiods t 1 and t 2 simultaneous fitting of the ml algorithms i e step 1 above and estimation of the weights i e steps 2 and 3 above using the whole t 1 t 2 period is generally not recommended as ml algorithms tend to overfit leading to superior algorithmic performances in the training set relative to independent test sets this has been verified also in the context of the present study where we found that qrf completely dominated qr other ensemble learning methods also exist see e g the review by sagi and rokach 2018 two of the most widely used are bagging breiman 1996b and boosting friedman 2001 see also the reviews by natekin and knoll 2013 mayr et al 2014 bagging averages multiple weak learners i e learners with low performance or unstable learners while in boosting new weak base learners are progressively introduced and trained to minimize the error of the ensemble learner following an iterative procedure thus new models are progressively added to the ensemble instead stacking which is a meta learning method uses diverse base learners to gain in performance sagi and rokach 2018 the overall performance of the ensemble learner formed by the combiner learner and the base learners depends on the efficiency of the combiner learner to properly weigh the base learners within a given test set and depends on the effectiveness of its calibration in the training period t 2 as well as potential similarities between periods t 1 and t 2 the splitting problem of a set into training and validation periods is common to all areas of hydrology and machine learning and addressing it goes beyond the scopes of the present study here the training set is partitioned into two subperiods i e t 1 and t 2 of almost equal lengths i e 8 and 6 years respectively whereas the test set i e subperiod t 3 includes 30 of the available data i e 6 out of 20 years see section 3 2 for details similar relative lengths for the corresponding training and test periods have been used in other ml studies see e g antal et al 2003 yu and xu 2008 papacharalampous et al 2019a the overall results can be considered reliable as the length of the available data allows examination of various patterns of low and high flows as well as other statistical attributes of the data 2 4 2 differences from existing frameworks the proposed algorithm borrows concepts from wang et al 2019 and trapero et al 2019 who used qs as a loss function and yao et al 2018 who combined closed expressions of probabilistic forecasts in the former two studies the weights of the base learners were estimated by minimizing the qs across all targeted quantiles and forecast horizons here we are interested in estimating optimal prediction intervals i e pairs of quantiles in the form of prediction ranges and therefore minimization of is is more suitable than minimization of qs the latter would lead to doubling the number of the applied weights i e one weight per bound in qs vs one weight per interval in is thus increasing the uncertainty of the resulting predictions we also note that existing applications of qs and is concepts are limited to fields outside hydrology additional advantages when using is minimization to combine probabilistic forecasts relative to other methods e g bma blp ngr see introduction are that a the weight search can be formulated as a linear programming problem with considerable increase of accuracy and computational efficiency of the algorithm and b quantile crossing issues are minimized as the obtained weights do not depend on posterior distributions that may present multi modal features for an extensive discussion on the merits of stacked generalization relative to bma the reader is referred to wang et al 2019 further advantages of the method inherit from the properties of stacked generalization which is a general methodology with deep theoretical background for details see wolpert 1992 and the fact that the method is simple straightforward to use computationally efficient i e it takes approximately 45 min to process 511 basins with 30 years of data each including hydrological model simulations on a regular pc and practical due to its full automation trapero et al 2019 2 5 base learners general guidelines for the selection of base learners are presented in alpaydin 2014 pp 488 491 in brief the base learners should be simple accurate and diverse so they complement each other here we use qr and qrf as base learners but the method can combine more than two quantile regression base learners the ensemble learner can also include different base learners which originate from the same ml algorithm e g qrf implemented with different parameters or predictor variables two reviews on quantile regression algorithms detailing recent progress in the field can be found in koenker 2017 and waldmann 2018 in general quantile regression algorithms model the conditional quantiles of dependent variables as functions of predictor variables while a detailed presentation of the implemented ml algorithms goes beyond the scopes of the present study brief descriptions of the methods and software packages used for their implementation are presented below 2 5 1 quantile regression linear in parameters quantile regression qr was introduced by koenker and bassett 1978 while an extended treatment of the method can be found in koenker 2005 the method uses similar techniques to linear regression to estimate the quantiles of a dependent variable conditional on predictor variables its main difference relative to linear regression is that minimization is conducted in terms of conditional quantiles whereas linear regression considers the conditional mean of the response variable an intuitive explanation of qr is that it fits a linear model and bisects the data so that 100 q lie below the predicted values of the fitted model practically this is done by fitting a linear model to the data and minimizing the average qs the method is suitable for modelling heteroscedasticity koenker 2005 p 25 we apply the method using the rq r function of the quantreg r package koenker 2018 which implements the fitting algorithm proposed by koenker and d orey 1987 1994 2 5 2 quantile regression forests quantile regression forests qrf were introduced by meinshausen 2006 the algorithm is based on random forests rf breimanm 2001 see also biau and scornet 2016 with interest being on conditional quantiles rather the conditional mean rf is a very accurate algorithm as proved by its performance in practical problems and competitions examples include successful use of rf in hydrology see e g nikolopoulos et al 2018 papacharalampous and tyralis 2018 point time series forecasting in hydrometeorological applications see e g tyralis and papacharalampous 2017 papacharalampous et al 2018a 2019a and spatial interpolation of hydrological quantities see e g tyralis et al 2018 2019b an extensive review on the use of rf in water sciences can be found in tyralis et al 2019a and a detailed description of the algorithm can be found in hastie et al 2009 pp 587 604 in a regression setting random forests average an ensemble of decision trees the ensemble is created by bagging abbreviation for bootstrap aggregation breiman 1996b regression trees in addition to bagging the splitting at the nodes of the regression tree is conducted by randomly selecting a fixed number of predictor variables thus inducing an additional degree of randomization which increases accuracy of the algorithm similar to random forests which approximate conditional means quantile regression forests approximate conditional quantiles this is done by averaging the indicator functions of the events exhibiting decision tree outcomes in the test set lower than a predefined quantile level here we apply qrf using the quantile forest r function of the grf r package tibshirani et al 2018 which emulates meinshausen s 2006 algorithm see also athey et al 2019 the corresponding algorithm is straightforward and very simple to use with a few parameters to tune while the default values in the software implementation are near optimal see e g the discussion in verikas et al 2011 oshiro et al 2012 scornet et al 2015 biau and scornet 2016 probst and boulesteix 2018 tyralis et al 2019a therefore optimization of the algorithm is omitted considering also that interest is in the relative improvement of the combiner learner with respect to the base learners used other properties of random forests are that they demonstrate high predictive performance they are non linear and non parametric they are fast compared to other machine learning algorithms and they are stable and robust to the inclusion of noisy predictor variables while they do not extrapolate outside the training range within the test set see e g biau and scornet 2016 tyralis et al 2019a 3 data and models 3 1 data a detailed presentation of camels dataset used in the present study can be found in addor et al 2017a b newman et al 2014 2015 2017 and thornton et al 2014 the dataset comprises of daily hydrometeorological and streamflow data from 671 small to medium sized basins in conus for each basin the daily minimum and maximum temperatures and precipitation have been obtained by processing the daily dataset of thornton et al 2014 changes in the basins due to human influences are minimal therefore use of ml algorithms for uncertainty characterization is an acceptable option see e g solomatine and wagener 2011 regarding the requirements for statistical similarity between subperiods when applying ml methods and koutsoyiannis and montanari 2015 regarding the appropriateness of the assumption of stationarity when changes cannot be explained deductively here we focus on the 34 year period 1980 2013 and exclude basins with missing data or other inconsistencies the final sample consists of 511 basins representing most climate types over conus see fig 2 for each of the 511 basins we estimate the mean daily temperature as the average of the respective minimum and maximum daily temperatures the daily potential evapotranspiration pet is estimated by implementing oudin s formula oudin et al 2005 for the latter we use the pedaily oudin r function of the airgr r package for details see coron et al 2017 2019 with the daily mean temperature as input 3 2 hydrological model the gr4j model constitutes an improvement of the gr3j gÃ©nie rural Ã  3 paramÃ¨tres journalier model by edijatno et al 1999 and comprises of four parameters while its precursor i e gr3j comprises of three parameters perrin et al 2003 the use of this small number of parameters is fully justified in perrin et al 2001 the hydrological model is herein calibrated in a non adaptive way i e the calibration is performed once for each basin and the hydrological model is thereafter applied with fixed parameter values see e g toth et al 1999 although feasible we do not perform adaptive calibration see e g brath and rosso 1993 ye et al 2014 as its benefits are delivered by the base learners in the context of the hydrological post processing framework toth et al 1999 we use the airgr r package to apply the gr4j hydrological model to each basin we simulate daily streamflow with recorded daily precipitation and pet as input the period 1980 1981 is used to warm up the hydrological model while period 1982 1993 is used for model calibration using the calibration michel r function of the airgr r package the latter function implements michel s 1991 optimization algorithm using the nash sutcliffe criterion nash and sutcliffe 1970 to characterize the quality of the hydrological simulations relative to recorded streamflows following the notation presented in section 2 2 we define the periods t 1 1994 01 01 2001 12 31 t 2 2002 01 01 2007 12 31 t 3 2008 01 01 2013 12 31 and use the calibrated hydrological model to simulate daily streamflows for the total period t t 1 t 2 t 3 the simulated streamflow vt at time t is calculated using information until day 1993 12 31 for yt i e the recorded streamflow and until day t for pr t and pet t i e precipitation and potential evapotranspiration respectively the final product consists of 511 simulated streamflow series at a daily resolution in period t with a total of 1 120 112 simulated values in period t 3 where the ensemble learner is tested i e 2192 simulated streamflow values for each of the 511 basins 3 3 technical aspects post processing aims at estimating the uncertainty of the predictand variable conditional on model simulations see introduction in experiment 1 i e one step ahead predictions see section 2 1 the predictor variable is defined as x t yt 1 vt use of the last streamflow observation yt 1 as predictor is supported by numerous relevant examples see e g krzysztofowicz 1987 seo et al 2006 evin et al 2014 bogner et al 2016 due to the high magnitude of dependence between sequential streamflow observations in experiment 2 the predictor variable is defined as x t vt vt 1 vt 2 and corresponds to the case of post processing hydrological model simulations the q th quantiles of the predictand can be obtained by post processing x t through yntq fe q x t when using ml algorithms it is common to pre process the data by applying some transformation with the aim to increase the performance of the model appropriate transformations can be applied to both yt and vt several options are available in the existing literature such as the arcsinh log square root box cox and yeo johnson transformations as well as the normal quantile transformation see e g krzysztofowicz 1997 bogner et al 2012 all aforementioned normalization transformations can be implemented using the bestnormalize r package peterson 2018 the selected transformation should be applied to both simulated and observed streamflows in the training sets and all ml calculations should be performed using transformed quantities the inverse transformation is then applied to the predicted quantiles we tried all previously mentioned transformations and found that the square root transformation was the only one not resulting in unrealistically high quantiles by the qr algorithm in the examined cases when compared to conventional statistical methods waldmann 2018 qr is more robust and less sensitive to the existence of outliers of the dependent variable while rf is invariant to monotonic transformations of the predictor variables dÃ­az uriarte and de andres 2006 the square root transformation has also been used by messner 2018 for the purposes of hydrometeorological post processing other issues which need to be addressed in most post processing applications include heteroscedastic behaviour of the data censoring i e in case the predicted quantiles exhibit negative values and quantile crossing problems regarding heteroscedasticity it can be theoretically addressed by using base learners that can effectively model heteroscedastic behaviour such as the qr used in the present study problems of negative quantiles were minimal in the present application in the case of qrf base learners negative values are by definition not possible as the predicted quantiles constitute subsets of the values found in the training set for the qr base learners the problem of negative quantiles was addressed by censoring them quantile crossing problems were also minimal in the present application and have been addressed by properly adjusting the corresponding quantiles similar to the approach of wang et al 2019 according to the latter if quantile q 1 results to be larger than quantile q 2 with q 1 q 2 then quantile q 2 is set equal to quantile q 1 4 results and discussion for period t 3 fig 3 summarizes information on the simulated and observed streamflows for all basins analysed fig 3 a presents a scatterplot for the same and previous day observed discharges yt and yt 1 respectively while fig 3 b d show scatterplots of yt with respect to the simulated streamflows vt vt 1 and vt 2 respectively regarding fig 3 b one sees that the linear regression line red between the same day hydrological simulations and observations is close to the 45 degree line black indicating that the hydrological model pre processes the data relatively well however there seems to be a moderate negative bias in the estimation of high flows as indicated by the points lying above the 45 degree line also as physically expected the deviation between observed and simulated flows increases with increasing lag times see fig 3 c d fig 3 a illustrates the significant positive correlation of observed streamflows in two sequential days i e yt and yt 1 indicating the appropriateness of using x t yt 1 vt as a predictor variable in hydrological post processing clearly the deviation of the linear regression line in fig 3 a from the 45 degree line is larger than that in fig 3 b indicating the important pre processing role of the hydrological model regarding high flows the respective points in fig 3 a are scattered symmetrically around the 45 degree line this is statistically justifiable as the probabilities of observing higher or lower flows in day t with respect to day t 1 are approximately equal the appropriateness of using x t yt 1 vt as a predictor variable in hydrological post processing is also illustrated in fig 4 which shows histograms of correlations obtained for validation period t 3 and all considered basins between yt and a yt 1 b vt c vt 1 and d vt 2 one sees that the correlations between yt and each of the variables yt 1 and vt are generally higher relative to the correlations between the observed streamflow yt at time t and the simulated streamflows at earlier times i e vt 1 and vt 2 correlation histograms obtained for validation period t 1 t 2 are similar to those in fig 4 not shown here for brevity fig 5 shows an example of post processed hydrological simulations in the context of experiments 1 and 2 at an arbitrary basin the 0 025 and 0 975 quantiles of the base learners qr and qrf and ensemble learners 1 and 2 are also presented visual inspection of the post processed simulations indicates that qr qrf and ensemble learners 1 and 2 produce intervals that in general include yt in experiment 2 the prediction intervals are wider due to the larger degree of uncertainty induced by the absence of the previous day observed streamflow yt 1 as predictor variable in the next section we quantitatively assess the performance of each method including ensemble learners 3 and 4 as well as simple averaging using proper metrics 4 1 performance assessment for brevity and without loss of generality in what follows we centre the discussion to experiment 1 the results of experiment 2 are presented through comparison of performances relative to experiment 1 for all basins analysed we assess the predictive performance of ensemble learners 1 4 and the simple averaging method in period t 3 the assessment is made by estimating the relative improvement ri introduced with respect to each of the base learners for instance the relative improvement ri of the interval score of learner i with respect to the n th base learner used for benchmarking is defined as 3 ri l n a l i a l n a similarly by substituting the interval score by its components i e interval widths and penalty see section 2 4 1 one can obtain their relative improvements as well see section 4 2 for a detailed analysis and presentation of findings regarding experiment 1 fig 6 a shows the mean ri over all basins of ensemble learners 1 4 and simple averaging with respect to qr for different prediction intervals 1 a 20 40 60 80 90 95 fig 6 b presents similar results to fig 6 a but for experiment 2 a positive value of ri indicates that the examined learner improves over the benchmark learner values equal to 0 indicate that the examined and benchmark learners are identical regarding experiment 1 when compared to qr ensemble learners 1 and 2 improve more than 10 at prediction intervals below 80 while the relative improvement decreases to 5 at higher prediction intervals when compared to qrf the relative improvement is 1 2 at low prediction intervals and increases to more than 5 at higher prediction intervals lower prediction intervals are representative of the median values of the streamflow whereas higher prediction intervals can be used to predict low and high flows the diverse properties of the two base learners with respect to the magnitude of the prediction interval are also presented in fig 6 the performance of the qrf base learner is 9 better relative to qr at low prediction intervals and it decreases at higher prediction intervals a probable reason for this is that by construction qrf cannot predict beyond the range of observed flows in the training set whereas the qr algorithm is regression based allowing for extrapolation beyond this range at all prediction intervals considered the performances of ensemble learners 3 and 4 are approximately 2 lower compared to ensemble learners 1 and 2 respectively consequently retraining of the algorithm in period t 2 step 4 is beneficial and should be preferred the relative improvement of ensemble learner 1 over simple averaging is approximately 2 at prediction intervals below 80 with the two methods sharing similarly good performances at higher prediction intervals regarding experiment 2 see fig 6 b one sees that the ri curves are shifted downwards relative to fig 6 a indicating lower overall performances associated with the larger degree of uncertainty relative to experiment 1 induced by the absence of the previous day observed streamflow as predictor variable in addition while ensemble learners 1 and 2 perform better than the base learners simple averaging performs equally well to both ensemble learners 1 and 2 at all prediction intervals this important result indicates that the outcome of optimal weight selection is strongly influenced by the uncertainty of the predictor predictand relationship more precisely as the level of uncertainty increases e g experiment 2 relative to experiment 1 weight optimization may not lead to significant improvements relative to simple averaging i e a uniform weighting scheme that assigns equal weights to all base learners when averaged over all prediction intervals the relative improvement of the interval score of ensemble learner 1 in experiment 1 is 8 84 with respect to qr and 4 43 with respect to qrf the corresponding improvements introduced by ensemble learner 2 are 8 55 with respect to qr and 4 18 with respect to qrf and by simple averaging are 7 90 and 3 60 respectively the slight improvement of ensemble learner 1 in experiment 1 should be attributed to its higher flexibility not compromised by overfitting clearly the two ensemble learners are able to exploit the diverse properties of the base learners and improve uniformly over them demonstrating improved performance relative to simple averaging by approximately 1 also it follows from the discussion above that the first ensemble learner is approximately 0 5 more efficient relative to the second one the reason for this is that the first learner uses a combiner algorithm that allows for additional degrees of freedom as the weights applied to the base learners may vary with the prediction interval 1 a see section 2 2 note that the aforementioned increase of predictive performances over the base learners is significant especially due to the size of the test set i e 511 time series each one consisting of 34 years of daily streamflow observations for example in their study wang et al 2019 indicate 4 39 average relative improvement of the quantile score with respect to the three base learners used based on eight daily time series of electricity consumption each one consisting of four years of data although smaller due to the larger uncertainty of the predictor predictand relationship see above improvements of ensemble learners 1 and 2 over the base learners are also observed in experiment 2 see fig 6 b in addition both ensemble learners appear to be overall equivalent to simple averaging indicating that weight optimization does not lead to significant improvements relative to the uniform weighting scheme of simple averaging fig 7 presents histograms of the relative improvements of the is see eq 3 introduced by the two ensemble learners in experiment 1 for all considered basins relative to the two base learners each histogram consists of 3066 values which correspond to six values i e one per prediction interval 1 a 20 40 60 80 90 95 per basin in all cases the improvements are mostly positive and well dispersed indicating that the results presented in fig 6 i e the mean relative improvement of each ensemble learner relative to the two base learners are not dominated by exceptional performances of the ensemble learners over a limited set of basins figs 8 and 9 present boxplots of the average interval scores is in experiment 1 and experiment 2 respectively for period t 3 one sees that a independent of the experiment and method used is increases with increasing 1 a b in both experiments ensemble learners 1 4 and simple averaging improve over the base learners and c is in experiment 1 are generally lower than those in experiment 2 thus confirming that yt 1 i e used as predictor variable in experiment 1 is more informative than vt 1 and vt 2 combined i e used as predictor variables in experiment 2 4 2 components of the interval score to gain further insight regarding the performance of each method in the testing period t 3 fig 10 presents for both experiments the ensemble mean over all basins of the absolute differences between the reliability scores see section 2 4 1 and the corresponding nominal prediction intervals one can see that in experiment 1 qr performs better than qrf at prediction intervals below 60 whereas the performances are reversed at higher prediction intervals the differences are less pronounced in experiment 2 with qr performing better than qrf at all prediction intervals in both experiments ensemble learners 3 and 4 demonstrate limited performance relative to ensemble learners 1 and 2 with the latter two exhibiting similar performances to simple averaging balancing those of qr and qrf base learners fig 11 presents the median relative improvements i e with respect to qr of the performances of base learner qrf ensemble learners 1 4 and simple averaging in terms of prediction interval widths median values are preferred over mean values to avoid influences by very low i e near zero prediction intervals while the performances of all methods are comparable in experiment 2 see fig 11 b due to the higher level of uncertainty induced by the absence of the previous day observed streamflow as predictor variable in experiment 1 see fig 11 a qr uniformly dominates qrf and all ensemble methods used fig 12 presents the ensemble mean over all basins of the relative improvements with respect to qr of penalties associated with intervals that do not contain observations see section 2 4 1 the general pattern is similar to that of interval scores in fig 6 indicating that penalties are an important contributor to the interval score 4 3 weights to gain insight on how the weights of the ensemble learners see section 2 2 are affected by the performances of the base learners fig 13 shows for ensemble learner 1 scatterplots of the weights assigned to the qr base learner as a function of the relative improvement of the average interval score of qrf relative to qr for different prediction intervals 1 a as expected one sees that independent of the experiment i e 1 fig 13 a or 2 fig 13 b the weights assigned to the qr base learner tend to decrease when the relative improvement of qrf relative to qr increases for ensemble learner 1 and experiment 1 fig 14 presents histograms of the weights assigned to the qr base learner for varying prediction intervals 1 a when the prediction interval 1 a increases the weights increase as well this should be expected because the relative gain in performance when referring to the interval score of qr with respect to qrf increases for higher prediction intervals spikes at the edges of the histograms correspond to cases where the qr base learner completely dominates qrf or vice versa resulting in weights equal to 1 or 0 respectively 5 concluding remarks ensemble learning of base learners can result in improved performance of probabilistic predictions the few existing methods require formal definition of the likelihoods of the base learners which is too restrictive as most base learners cannot provide explicit expressions for the pdfs of the obtained forecasts in this study we borrowed concepts from wang et al 2019 to propose an ensemble learner which uses stacked generalization to linearly combine the quantile predictions of non parametric base learners i e quantile regression and quantile regression forests algorithms using weights that minimize the interval score of the resulting prediction the method was tested using a large dataset consisting of 511 basins the conducted tests focused in delivering one step ahead predictions experiment 1 as well as in post processing simulations of a conceptual hydrological model experiment 2 it was found that the ensemble learners improve over the performance of the best base learner by 1 5 depending on the experiment and the prediction interval the suggested method was also found to outperform simple averaging i e a uniform weighting scheme that assigns equal weights to all base learners or to be sharing the first place with it in all examined cases with the maximum obtained improvement over this tough benchmark being approximately equal to 2 the results are considered significant especially given the length of the sample the algorithm has been tested i e post processing of 1 120 112 hydrological predictions from 511 time series and the fact that simple averaging is hard to beat in practice see e g lichtendahl et al 2013 the latter general observation indicates that when the uncertainty of the predictor predictand relationship increases the effectiveness of weight optimization tends to decrease approaching that of simple averaging i e a uniform weighting scheme with equal weights assigned to all base learners to the best of our knowledge no similar study has been conducted in the hydrological literature with the closest work being that of wang et al 2019 in electricity forecasting the latter is based on minimization of the quantile score qs see introduction indicating 4 39 average relative improvement with respect to the three base learners used whereas in the present study ensemble learning is conducted by minimizing the interval score is resulting e g in experiment 1 to approximately 6 5 average relative improvement over the 2 base learners i e 8 98 4 44 8 66 4 18 4 see section 4 1 also note that application of the constrained quantile regression averaging cqra method of wang et al 2019 was based on eight daily time series of electricity consumption each one consisting of four years of data one should consider the convenience of using the proposed method over other combination methods e g bayesian model averaging as well as theoretical studies that support a stacking against bayesian model averaging and b working with quantile forecasts instead of probability distributions see also section 1 the extended use of machine algorithms in hydrological post processing should also be considered machine learning algorithms are accurate they have been tested extensively in practice as well as in forecasting competitions they are easy to apply due to their open software implementation and they are optimally programmed resulting in considerable decrease of computation times the computations of the present study including fitting of the hydrological model required approximately 45 min on a regular pc thus allowing large scale implementations based on the aforementioned findings we recommend use of the proposed ensemble learners for improving the probabilistic predictions of base learners future research could focus on defining optimal splitting points of the training set used inclusion of more base learners testing the method using forecasts of daily temperature and precipitation as input as well as assessing the performance of stacked generalization when metrics scores other than is see e g gneiting and raftery 2007 shastri et al 2017 are minimized to optimally combine probabilistic forecasts further uses of the method are also possible spanning from hydrological forecasting using data driven models to water demand forecasting to water science problems and beyond e g in electricity load forecasting and more declaration of competing interest the authors declare no conflict of interest acknowledgements we are grateful to the editor associate editor and the reviewers for their constructive comments and suggestions which helped us to improve the manuscript appendix a nomenclature indices 1 a central prediction interval n index of base learners t index of time periods q index of quantiles sets n set of base learners t set of time periods q set of quantiles functions 1 the indicator function a cardinality of a set a fn q the n th base learner for the q th quantile fe q the ensemble learner for the q th quantile fn the n th base learner variables x t inputs of a model at time t yt actual streamflow at time t pr t observed daily precipitation at time t pet t observed daily potential evapotranspiration at time t v t simulated daily streamflow at time t yn t q the forecasted q th quantile of the n th base learner at time t yt q the forecasted q th quantile of the ensemble learner at time t ln t a interval score of the n th base learner at time t for the 1 a prediction interval lt a interval score of the weighted average of the n methods at time t for the 1 a prediction interval appendix b used software all computations and visualizations were conducted in r programming language r core team 2019 using the following packages airgr coron et al 2017 2019 bestnormalize peterson 2018 data table dowle and srinivasan 2019 devtools wickham et al 2019c doparallel microsoft and weston 2017 dplyr wickham et al 2019b foreach microsoft corporation and weston 2018 gdata warnes et al 2017 ggplot2 wickham 2016 wickham et al 2019a grf tibshirani et al 2018 knitr xie 2014 2015 2019 quantreg koenker 2018 readr wickham et al 2018 reshape2 wickham 2007 2017 rmarkdown allaire et al 2019 stringi gagolewski 2019 stringr wickham 2019 
6264,the hydrological regimes of downstream reservoirs have been significantly altered due to the operation and regulation of upstream cascade reservoirs the original design flood quantiles namely design flood in construction period do not consider anthropogenic impacts in reservoir operation period and have led to enormous conflicts between flood control and conservation in this study the design flood and flood limited water level in operation period are defined for practical application we establish a general framework to measure the spatiotemporal pattern of streamflow and to estimate design floods of cascade reservoirs in operation period the multivariate t copula and a genetic algorithm strategy are proposed to solve the curse of dimensionality encountered in the derivation of most likely regional composition the jinsha river and yalong river cascade reservoir system in china which consists of 13 large reservoirs with the total storage capacity of 74 06 billion m3 and hydropower capacity of 71 47 gw is selected as a case study results indicate that 1 the curse of dimensionality can be well addressed by applying multivariate t copula to build high dimensional joint distribution and using the genetic algorithm to achieve the most likely regional composition 2 compared with the design floods in construction period the design floods of downstream reservoirs in operation period have been significantly reduced due to the upstream reservoir regulation the 1000 year design peak flood discharge 3 day 7 day and 30 day flood volumes of xiangjiaba reservoir decrease by 38 7 37 4 34 2 and 13 8 respectively 3 the flood limited water level of these reservoirs can be raised without increasing flood control risks in operation period the cascade reservoirs in the jinsha river and yalong river can generate 3 28 billion kw h more hydropower or increase 4 3 annually during flood season keywords cascade reservoirs design flood most likely regional composition flood limited water level multivariate t copula genetic algorithm 1 introduction reservoirs are one of the most efficient infrastructure components for integrated water resources management guo et al 2004 liu et al 2015 in recent decades more and more reservoirs have been built for flood control and hydropower generation purposes and to meet the social economic development forming a large quantity of cascade reservoir systems throughout the world zhao et al 2016 liu et al 2016 yazdi and moridi 2018 the function of reservoirs in terms of flood water utilization has become increasingly important li et al 2010 ouyang et al 2015 reservoirs have significant impact on hydrological characteristics especially the temporal variation of maximum stream flows gao et al 2019 with the formation of substantial numbers of cascade reservoir systems the hydrological regimes of downstream section have been significantly altered the statistical properties e g mean variance skewness of the extreme flood series of downstream reservoir in operation period are considerably different from the annual maximum data series adopted for design flood in construction period liang et al 2018 therefore the investigation on design flood in operation period is of great significance in terms of scientific reservoir management the flood limited water level flwl in construction period which is determined by the reservoir regulation of the annual design flood hydrographs for given return period serves as the most vital parameter of tradeoff between flood control and conservation yun and singh 2008 li et al 2010 during the flood season the reservoir water level is not allowed to exceed the flwl in order to offer adequate storage for possible incidences of large floods liu et al 2015 the flwl in construction period gives too much priority to low probability floods which makes it a great challenge for reservoirs to produce the level of benefits that provides the economic justification for their development liu et al 2015 with the formation of cascade reservoir systems the application of flwl in construction period has led to greater conflicts between flood control and water resources utilization for example the downstream reservoirs in a cascade reservoir system are often unable to refill to the normal water level by the end of refilling period under circumstances where all cascade reservoirs compete to impound water zhou et al 2015 which largely limits the comprehensive benefits of these reservoirs therefore it is valuable to find the appropriate flwl in operation period for raising water resources comprehensive utilization rate recognizing the drawbacks of design flood and flwl in construction period the design flood and flwl in operation period are defined for practical application in this study yet only a handful of studies made preliminary attempts to study the reservoir impacts on downstream design floods which can be classified into three categories the first option is based on the non stationary flood frequency analysis framework with the reservoir index to represent the impacts of reservoir lÃ³pez and francÃ©s 2013 zhang et al 2015 wang et al 2017 the reservoir index can be taken into account in a generalized addictive model as an effective covariate to study the reservoir impacts nevertheless all the variables in a reservoir index series are constants in a specific period regardless of reservoir regulation strategies su and chen 2019 therefore the reservoir index may not be capable of accounting for the complex joint operation strategies of a cascade reservoir system the second option is to derive flood frequency distributions resulting from reservoir impacts by applying linear and nonlinear reservoir models gao et al 2019 however the linear and nonlinear models are both the simplification of reservoir operation rules basha 1994 and thus remain non ignorable biases from actual situation kim and georgakakos 2014 these differences could be further significantly amplified due to the high dimensional and nonlinear features of cascade reservoir system the third option is to study the reservoir impacts based on the flood regional composition frc framework guo et al 2018 the frc framework aims at analyzing the flood generation mechanism of the investigated reservoir the inflow of the investigated downstream reservoir comprises the inflow of upstream reservoir and all intermediate basin inflows between reservoirs the upstream reservoir inflow can be transformed into outflow according to man made reservoir operation rules the design flood of the investigated downstream reservoir in operation period is influenced by 1 the characteristics of upstream reservoirs flood control storage operation rules etc 2 the frc of the reservoir therefore the methods based on the frc framework aim to seek an appropriate frc from all possible compositions that satisfy water balance equation guo et al 2018 derived theoretical formula to achieve the most likely regional composition mlrc method for triple cascade reservoirs and evaluated the reservoir operation impacts based on the mlrc results the mlrc assumes that frcs may differ in terms of their probabilities of occurrence which can be measured by the value of joint probability density function of floods occurring at all sub basins and the frc with the largest occurrence probability should be selected based on this assumption guo et al 2018 adopted the asymmetric archimedean copula to build the joint distribution of floods occurring at all sub basins and used the newton iteration method to seek for the optimal solution of the mlrc guo s method with strong statistical basis is likely to be implemented for practical use but the barriers are twofold for derivation of mlrc as the number of cascade reservoirs increases 1 the asymmetric archimedean copula adopted to build the joint distributions in guo s study will bring large modelling errors triggered by the uncertainty of nesting structure and inability to be fully nested in high dimension serinaldi and grimaldi 2007 hofert and pham 2013 2 the newton iteration method adopted in guo s study is not capable of obtaining the optimal solution of the formula in high dimension due to the local convergence of the method dembo et al 1982 pilanci and wainwright 2017 bottou et al 2018 therefore the curse of dimensionality has become the main problem to be addressed in the literature it is essential to conduct in depth research with state of the art techniques to conquer these challenges in this study we focus on overcoming the curse of dimensionality based on guo s method due to its ease of implementation and potentiality in practical application the abovementioned two bottlenecks caused by high dimensionality in deriving mlrc are addressed by selecting the multivariate t copula to build the joint distribution and by applying a genetic algorithm ga based method the multivariate t copula belongs to the meta elliptical class and can be easier extended to higher dimension compared with archimedean copulas serinaldi et al 2009 moreover the empirical fit of the t copula is often good and is almost always superior to that of the gaussian copula mashal et al 2003 breymann et al 2003 the t copula has been given more attention recently due to its favorable empirical fit and ease of implementation e g zhang et al 2016 rivieccio and de luca 2016 hao et al 2017 he et al 2017 and therefore is considered in this study the derivation of mlrc in higher dimension typically involves highly nonlinear and nonconvex problems as opposed to the current formula based technique adopted by guo et al 2018 we develop a ga based method to obtain the robust solution of mlrc in higher dimension the ga proposed by holland 1975 has the ability to mimic processes observed in natural evolution and is prevalently utilized to address various optimization problems in hydrological practice e g yin et al 2017 ehteram et al 2018 zhou et al 2019 based on the derivation of mlrc in high dimension we propose a general framework for deriving design floods for cascade reservoir system and determining the flwl in operation period to ensure that the results of mlrc are reasonable and can be adopted for practical use the standard frc method recommended by mwr 2006 i e equivalent frequency regional composition efrc method is also applied and compared in this study this study is therefore explored with three foci 1 introducing the design flood and flwl in operation period to practical application in hydrology and 2 addressing the curse of dimensionality encountered in the derivation of mlrc by applying the multivariate t copula and a ga based strategy 3 developing a general framework for determining the design flood of cascade reservoirs in operation period the present paper is structured as follows section 2 describes the methodology used in this study section 3 briefly introduces the study area and the material in sections 4 a mix cascade reservoir system in the jinsha river and yalong river in china is selected as a case study the impact of upstream reservoir regulation on design floods as well as the flwls of downstream reservoirs are analyzed and discussed the last two sections are devoted to the discussion and conclusion of this study respectively 2 methodology 2 1 design flood in operation period the similarities and differences between design flood in construction period and in operation period are compared in table 1 the purpose of design flood in construction period is to provide vital information for the design of the reservoir storage spillway size etc while the design flood in operation period considers the impact of upstream reservoir operation and thus is capable of adapting to the variation of upstream hydrological regime and its main purpose is to counterbalance the conflicts between flood control and conservation in terms of property the design flood in construction period and in operation period are both static values which are unchanged over time and therefore can be easily implemented for practical use by applying flwl in construction period the reservoir can regulate low frequency design floods but the comprehensive benefits of reservoir storage cannot be fully utilized by applying flwl in operation period the reservoir can withstand the same frequency floods regulated by upstream reservoirs and gain more comprehensive benefits during the flood season to summarize the main difference between the design flood in construction period and in operation period is that the latter considers the impact of upstream reservoir regulation and tends to be more rational than the former in terms of flood water utilization 2 2 derivation of design flood in operation period 2 2 1 framework the main goal of this study aims at exploring a general framework for estimating design flood for a cascade reservoir system in operation period in which the curse of dimensionality is the main task to be addressed the architecture of the proposed framework is shown in fig 1 which comprises three modules namely 1 design flood in construction period module 2 flood regional composition frc module and 3 design flood in operation period module the first module is responsible for calculating the design flood peak volumes and design flood hydrographs in construction period for the second and third modules respectively in this study the design floods both in operation and construction periods are analyzed for the given same return periods and flood prevention standards the second module is responsible for obtaining the frc of the investigated reservoir in this module the multivariate t copula is proposed to build high dimensional joint distributions and a ga based method is proposed to derive the mlrc the efrc method suggested by mwr 2006 is also applied in this module for comparative study finally the design flood and flwl in operation period is calculated in the last module based on the design flood hydrographs in construction period obtained in the first module and the frc results obtained in the second module by regulating the design flood hydrographs in construction period with the consideration of the operation rules of upstream cascade reservoirs the complex impact of reservoir regulation on downstream reservoir design flood can be numerically evaluated in this module the main procedures and detailed explanations are given as follows 2 2 2 design flood in construction period module the general methodology of estimating design flood in construction period is to fit the flow data such as the annual maximum am data using a theoretical probabilistic distribution and derive the exceedance probability based on the selected distribution xiong et al 2018 the pearson type 3 p3 distribution and curve fitting estimation method is recommended by ministry of water resources mwr 2006 as the standard hydrological frequency analysis procedure in china and thus is adopted in this study compared with commonly used methods such as l moments and maximum likelihood method the curve fitting method can easily incorporate historical and paleo flood information while guarantee satisfactory statistical properties of the estimation results hu 1987 hua 1987 the kolmogorov smirnov ks test and anderson darling ad test are selected as the goodness of fit statistics to determine whether the candidate variables follow the p3 distribution zhang and singh 2007 huang et al 2018b from the perspective of engineering safety the flood which may lead to serious damage would be preferred when selecting typical flood hydrograph for reservoir design purpose mwr 2006 xiong et al 2019 severe floods occurred in both the mainstream and main tributaries of jinsha river in 1966 flood season therefore the 1966 flood hydrograph is selected as the typical flood hydrograph in this study 2 2 3 flood regional composition module 2 2 3 1 flood regional composition for cascade reservoirs the flood regional composition frc has been recommended by mwr 2006 to solve a wide range of problems for the following reasons 1 methods based on the frc are easy to implement lu et al 2012 2 methods based on frc can provide numerical solutions even for highly complex problems such as calculating the flood control effects of reservoir on downstream section and analyzing the joint operation strategies of cascade reservoirs or reservoir group which is essential for the engineering practice guo et al 2018 the sketch diagram of cascade reservoirs is given in fig 2 in general a more complex hybrid reservoir group can be decomposed into several cascade reservoirs based on large scale system theories chen et al 2017 let a a1 a2 a n b b1 b2 b n and c denote the reservoirs intermediate basins and downstream reservoir site of interest respectively and let random variables x 1 x 2 xn y 1 y 2 yn and z represent the natural flood volumes of reservoirs a1 a2 a n intermediate basins b1 b2 b n and downstream reservoir c with the corresponding values x 1 x 2 xn y 1 y 2 yn and z respectively we focus on the a1 a2 sub system to further demonstrate the frc concept the design flood of downstream reservoir a2 is impacted by the regulation of upstream reservoir a1 the inflow of downstream reservoir a2 is composed of inflow of upstream reservoir a1 and intermediate basin inflow b1 thus x 1 y 1 is the frc of x 2 in which x 1 can be transformed into outflow based on the operation rules of a1 therefore the statistical behavior of design flood of downstream reservoir a2 in operation period is deterministically related to the frc of a2 i e x 1 y 1 for the cascade reservoir system according to the principle of water balance equation all the frc x 1 y 1 y 2 yn z should be subjected to the following equations 1 x 1 y 1 x 2 x 1 y 1 y 2 x 3 x 1 y 1 y 2 y n z due to the inherent stochastic nature of flood generation mechanism there are countless compositions of x 1 y 1 y 2 yn z aligning with the principle of water balance in order to search for a representative frc which is consistent with the spatiotemporal pattern of streamflow two frc methods based on different assumptions are considered in this study and are introduced hereafter 2 2 3 2 equivalent frequency regional composition efrc method the efrc method assumes that the return period of flood occurring at one of the subareas reservoir or the intermediate basin is the same as the downstream reservoir though easy to implement the efrc suffers from two main drawbacks guo et al 2018 1 it assumes perfect correlation between floods occur at one sub basin and downstream site which does not always conform to the reality 2 for the composition of cascade reservoirs the number of efrcs 2 n 1 increases exponentially with the increase of the number of cascade reservoirs n in this study only one representative efrc for cascade reservoirs namely efrc 1 is considered for the purpose of comparison for a given return period the floods occurring at all reservoirs x 1 x 2 xn z can be determined by their respective fitted distributions the floods occurring at all intermediate basins y 1 y 2 yn can be calculated using the water balance equation efrc x 1 y 1 y 2 yn can thus be determined readers can refer to liang et al 2017 and guo et al 2018 for more details about the efrc method 2 2 3 3 most likely regional composition mlrc method to overcome the drawbacks of efrc guo et al 2018 proposed the mlrc based on copulas they indicated that 1 the mlrc method with stronger statistical basis can better capture the actual spatial correlation of flood events occurring at different sub basins 2 the composition of mlrc method is unique and thus is easy to implement for large cascade reservoir system the mlrc method assumes that frcs may differ in terms of their probability of occurrence which can be measured by the value of joint probability density function of random variables x 1 x 2 xn z i e f x 1 x 2 xn z according to sklar s theorem nelsen 2006 the joint probability density function can be expressed in terms of its marginal distributions and the associated dependence function i e copula function as follows salvadori and de michele 2004 salvadori et al 2016 2 f x 1 x 2 x n 1 x n z c u 1 u 2 u n 1 u n v i 1 n f x i x i f z z where c is the pdf of copula function and u 1 u 2 un v are the corresponding empirical frequencies of x 1 x 2 xn z f x i x i i 1 2 n and f z z are the marginal distributions of floods occurring at reservoirs a1 a2 a n and downstream reservoir c respectively the composition x 1 x 2 xn z is more likely to occur when the value of density function f x 1 x 2 xn z increases salvadori et al 2011 in order to search for the mlrc the f x 1 x 2 xn z is maximized by subjecting water balance constraint in eq 1 for the given z zp 3 max f x 1 x 2 x n 1 x n z c u 1 u 2 u n 1 u n v i 1 n f x i x i f z z s t x 1 y 1 x 2 x 1 y 1 y 2 x 3 x 1 y 1 y 2 y n z z p where zp is the design flood volume for a given return period of the investigated reservoir obtained from the design flood in construction period module the joint probability density f x 1 x 2 xn z is maximized when its first order derivative equals zero and the following equation should be satisfied 4 f x 1 x 2 x n 1 x n z x 1 0 f x 1 x 2 x n 1 x n z x 2 0 f x 1 x 2 x n 1 x n z z 0 s t x 1 y 1 x 2 x 1 y 1 y 2 x 3 x 1 y 1 y 2 y n z z p after the composition x 1 x 2 xn z is derived from eq 4 mlrc x 1 y 1 y 2 yn can be determined using the water balance equation as indicated previously guo s method suffers from curse of dimensionality as the number of cascade reservoirs increase in this study the curse of dimensionality is addressed by the following flow chart as shown in fig 3 and the main procedures are described as follows 1 determine the marginal distributions and joint distribution according to the sampled am data series the vine copula and meta elliptical copula are often used to establish a higher dimensional joint distribution the vine copula embraces a large quantity of pair copula decompositions for high dimensional variables which makes the selection of suitable vine composition considerably complex aas et al 2009 on the other hand the meta elliptical copula can model arbitrary pairwise dependence structures through a correlation matrix and is easy to implement huang et al 2018a considering the advantage of meta elliptical copula and the drawback of vine copula the meta elliptical copula is selected in this paper gaussian copula and t copula are the commonly used meta elliptical copulas mashal et al 2003 and breymann et al 2003 have shown that the goodness of fit of the t copula is almost always superior to that of the gaussian copula serinaldi et al 2009 and chen et al 2016 also indicated that t copula is adequate to construct high dimensional joint distribution hence the t copula is employed to establish the joint distribution of the flood volumes of different sub basins in this study the maximum likelihood method is adopted to estimate parameters of t copula huang et al 2018a the goodness of fit of t copula is assessed with the ks test and cramer von mises cm test genest and rÃ©millard 2008 genest et al 2009 the degree of freedom of t copula is selected under the criteria of the root mean square error rmse and akaike information criterion aic xiong et al 2018 zhong et al 2018 the smaller rmse and aic represents the better performance of the candidate model 2 in order to find the optimal composition of flood volumes x 1 x 2 xn that maximizes the joint probability density function the negative value of joint probability density is set as the fitness function and the flood volumes of the upstream reservoirs x 1 x 2 xn is set as the parameters to be optimized considering the water balance equations should always be satisfied in frc the water balance equations are employed as constraints to determine the initial value and threshold of the parameters the fitness function and constraints are expressed as follows 5 min f x 1 x 2 x n 1 x n z c u 1 u 2 u n 1 u n v i 1 n f x i x i f z z s t x 1 y 1 x 2 x 1 y 1 y 2 x 3 x 1 y 1 y 2 y n z z p where the corresponding empirical frequencies u 1 u 2 un can be expressed with respect to their marginal distributions the formula based derivation of mlrc for an n dimensional cascade reservoir system adopted by guo et al 2018 is substantially equivalent to solving a set of n dimensional nonlinear and nonconvex equations and the optimal solution can be hard to achieve by the newton iteration method due to its inherent drawbacks of local convergence and susceptibility to initial solution the ga based method in this study however can effectively achieve the computational complexity reduction by transforming the derivation process into an optimization process the ga technique has proved to be capable of coping with highly complex optimization problems chang et al 2003 ghareb et al 2016 attaining the mlrc of n dimensional cascade reservoir system corresponds to an optimization problem with n parameters to be optimized and currently the number of reservoirs n is usually less than 20 moreover different initial conditions have been considered to test whether the optimal solution is robust results indicate that when using different initial seeds the obtained optimal solutions show no significant difference therefore the ga based method can be effectively applied to existing cascade reservoir systems 2 2 3 4 comparison of efrc and mlrc the basic cascade reservoir system see sub system a1 a2 with synthetic inflow time series are used to compare the efrc and mlrc methods since both efrc and mlrc methods focus on the analysis of am data series we adopt the following procedures to generate am flood series of a1 and a2 with different correlations for comparison firstly the margins of a1 and a2 are assumed to be p3 distribution and the location scale shape parameters of p3 distribution for a1 a2 reservoir are assumed to be 30 50 7 5 12 5 and 4 4 respectively secondly two sets of uniform variables over 0 1 are randomly generated from a bivariate t copula with sample size m 60 using r package copula demarta and mcneil 2005 yan 2007 which are considered as the empirical frequencies of the am flood series of the two reservoirs and the correlation coefficients between the two sets of uniform variables are set to be 0 01 0 3 0 6 and 0 99 respectively to demonstrate am flood series with different correlations finally the am flood series of the two reservoirs can be generated by applying the predetermined p3 quantile functions to the obtained uniform variables with the consideration of noise term the efrc and mlrc methods are used to analyze the frc of 1000 year design flood of a2 inflow and results are summarized in table 2 it can be seen from table 2 that when the correlation coefficient between the am floods of a1 and a2 is extremely high 0 99 the results obtained by mlrc and efrc are nearly the same however when the correlation is very weak 0 01 the differences between results of mlrc and efrc are significant i e the equivalent frequency assumption adopted by efrc is unreasonable for weak correlated floods mwr 2006 thus the mlrc method enabling to consider the actual correlation between am floods may offer more reasonable results compared with efrc 2 2 4 design flood in operation period module the main procedures of this module are described as follows 1 derive the corresponding design flood hydrographs of each sub basin using the peak and volume amplitude pva method zhong et al 2017 yin et al 2018 based on the frc results and design flood hydrographs in construction period the advantage of pva method is that it ensures that both the flood peak and volume are equal to the assigned values without modifying flood duration yin et al 2018 2 obtain the design flood hydrograph at the downstream site c based on river channel flood routing and reservoir flood regulation the commonly used muskingum model is considered for the river channel flood routing franchini et al 2011 guo et al 2018 then the design peak flood discharge and volumes are directly determined from the design flood hydrograph in operation period to offer a numerical estimation of design flood 3 determine the flwl in operation period using the iterative calculation method xiao et al 2009 zhou et al 2015 according to the derived design flood hydrograph and the reservoir operation rules the flood prevention standard cannot be lowered down under the iterative calculation approach zhou et al 2015 3 study area and materials jinsha river is the upper reach of yangtze river and has a total length of 3481 km with a drainage area of 502 000 km2 accounting for 26 of the drainage area of the yangtze river the jinsha river is divided into upper middle and lower reaches with respective lengths of 965 km 1220 km and 1296 km yalong river is the largest tributary of jinsha river with a drainage area of 136 000 km2 during the past decades a series of dams have been built along the main jinsha river and yalong river for the purpose of flood control and hydropower generation among them the liyuan ly ahai ah jinanqiao jaq longkaikou lkk ludila ldl and guanyinyan gyy reservoirs are located in the midstream jinsha river while the wudongde wdd baihetan bht xiluodu xld and xiangjiaba xjb reservoirs are located in the downstream jinsha river along the yalong river there are lianghekou lhk jinping jp ertan et cascade reservoirs the investigated area and sketch map of cascade reservoirs in the jinsha river and yalong river are shown in fig 4 the basic information of these reservoirs is listed in table 3 to promote water resources utilization and hydropower generation these 13 mega cascade reservoirs have been jointly operated to achieve a total reservoir storage of 74 06 billion m3 21 82 billion m3 for flood control and a total installed hydropower capacity of 71 47 gw the current operation rules standard operation policy of these reservoirs are provided by the changjiang yangtze river water resources commission cwrc ministry of water resource cwrc 2018 the data sets used in this study are natural daily streamflow data of the investigated dam sites with record lengths all longer than 50 years the data sets are provided by the cwrc and have been widely used for the design and management of these reservoirs 4 result analysis 4 1 the design flood in construction period according to the regulation characteristics of cascade reservoirs in the jinsha river and yalong river annual maximum 7 day and 30 day denoted as am 7d and am 30d flood volume series are selected for flood frequency analysis the p values of ks and ad tests of each variable are calculated for both of the tests when p value is larger than 0 05 the assumption that the am series follows the p3 distribution cannot be rejected at a 5 significance level for illustration only the cumulative distributions of am 7d flood volumes series fitted by p3 distributions for ah lkk gyy et bht and xjb reservoirs are plotted in fig 5 the figure shows a good correspondence between empirical and theoretical frequencies and the assumption that the variables follow the p3 distribution cannot be rejected 4 2 design flood of cascade reservoirs in the midstream jinsha river and yalong river 4 2 1 flood regional composition there are no intermediate inflows from tributaries between cascade reservoirs in the midstream jinsha river and yalong river fig 4 and the frc for these reservoirs can be directly analyzed by the general form fig 2 the frc of am 7d and 30d flood volumes of each reservoir in the midstream jinsha river and yalong river are analyzed taking the frc of gyy reservoir as an example the flood volume of gyy reservoir x 6 is decomposed to ly reservoir x 1 l a inter basin y 1 a j inter basin y 2 j l inter basin y 3 l l inter basin y 4 and l g inter basin y 5 taking the establishment of joint distribution for the mlrc of gyy reservoir as an illustration the p values of ks and cm tests rmse and aic values of t copulas with different degrees of freedom are calculated and listed in table 4 it can be seen that all p values are greater than 0 05 indicating that the variables following t copula cannot be rejected at a 5 significance level table 4 indicates that the t copula with v 3 degrees of freedom exhibits the smallest rmse and aic and thus should be selected the adequacy of the t copula is further evaluated with the p p plots by plotting the empirical copula against theoretical copula for illustration only fig 6 shows the p p plots of joint distributions for the mlrc method of ah lkk gyy and et reservoirs respectively the p values obtained from ks and cm tests are also shown it can be seen from fig 6 that all p values of all t copulas are greater than 0 05 and no strong departure from the expected distribution can be observed from the p p plots therefore the t copula can be effectively implemented for establishing joint distributions in high dimension following the procedures described in section 2 2 3 the efrc and mlrc methods of am 7d and 30d flood volumes of cascade reservoirs are obtained taking the frc of am 7d flood volumes of gyy reservoir as an example it can be observed from table 5 that the results obtained by the efrc and mlrc methods show no significant difference in this case study the similarity between the results of efrc and mlrc is due to the fact that there exist strongly positive correlations between the flood volumes of downstream reservoir and its upstream reservoirs and this will be further discussed in section 5 1 regarding the curse of dimensionality guo s method is compared with the proposed method for deriving mlrc with different numbers of cascade reservoirs taking the derivation of mlrc of ah 2 reservoirs and ldl reservoirs 5 reservoirs as illustrations the mlrc derived by guo s method and the proposed method are compared in table 6 as for the derivation of mlrc for ah reservoir the results obtained by the two methods are nearly the same this indicates that the solutions of the proposed method are reliable as for the derivation of mlrc for ldl reservoir guo s method cannot obtain the solution of mlrc while the proposed method can effectively achieve the mlrc of ldl reservoir this demonstrates that guo s method can be subject to the curse of dimensionality as the number of cascade reservoirs increases which can be well addressed by the proposed method 4 2 2 reservoir design flood in operation period the impact of upstream reservoir operation can be analyzed following the procedures described in section 2 2 4 the original design flood frequencies of cascade reservoirs in the midstream jinsha river and yalong river are 0 2 return period is 500 year and 0 1 return period is 1000 year respectively the design frequency floods in construction period and in operation period are compared in table 7 results indicate that 1 the estimated design peak flood discharge q max 3 day w 3 7 day w 7 and 30 day w 30 flood volumes of downstream reservoirs decrease due to the upstream reservoir operation for instance the estimated q max w 3 w 7 and w 30 of gyy reservoir in operation period decrease by 11 5 9 7 7 3 and 0 4 respectively compared with the design values in construction period 2 the reduction rates of downstream reservoirs are greater than that of the upstream reservoirs this is as expected that when the available flood control storage is larger the flood can be better regulated 3 the flwl of downstream reservoirs in operation period is higher than that of the designed values in construction period the flwls of ah jaq lkk ldl gyy jp and et reservoirs in operation period in construction period are 1493 7 1493 3 m 1410 8 1410 m 1290 3 1289 m 1212 8 1212 m 1123 5 1122 3 m 1862 4 1859 m and 1191 6 1190 m respectively the less required flood control storage accounts for the raise of flwls in operation period 4 under the flwl scheme in operation period the ah jaq lkk ldl gyy jp and et reservoirs increase the hydropower generation by 0 45 0 72 2 1 1 1 2 1 6 and 0 6 respectively it can be summarized from table 7 that the cascade reservoirs in the midstream jinsha river and yalong river can increase annual hydropower generation hg by 126 million kw h in flood season in operation period 4 3 design flood of cascade reservoirs in the downstream jinsha river 4 3 1 flood regional composition the inflows of cascade reservoirs in the downstream jinsha river are impacted by both the cascade reservoirs in the midstream jinsha river and yalong river see fig 4 and it would not be possible to carry out frc analysis without decomposition chen et al 2017 the frc of wdd bht xld and xjb reservoirs follows a three step procedure described as below taking the frc of xjb reservoir as an illustration firstly the flood volume of xjb x 13 is decomposed to wdd reservoir x 10 w b inter basin y 9 b x inter basin y 10 and x x inter basin y 11 secondly the flood volume of wdd reservoir x 10 is decomposed to gyy reservoir x 6 et reservoir x 9 and g e w inter basin y 8 finally the flood volume of gyy reservoir x 6 is decomposed to ly reservoir x 1 l a inter basin y 1 a j inter basin y 2 j l inter basin y 3 l l inter basin y 4 and l g inter basin y 5 and the flood volume of et reservoir x 9 is decomposed to lhk reservoir x 7 l j inter basin y 6 and j e inter basin y 7 the frc of gyy and et reservoirs has been achieved in section 4 2 the efrc and mlrc are adopted to obtain the frc of these reservoirs following the decomposition procedure fig 7 shows the goodness of fit of the selected t copula table 8 lists the results of frc using the xjb reservoir as an example results from fig 7 and table 8 reveal again that 1 the t copula can be effectively implemented to establish the joint distributions for mlrc of the investigated reservoir 2 the mlrc method can be effectively implemented for the complex hybrid reservoir system 4 3 2 design flood in operation period the design flood return periods of cascade reservoirs in the downstream jinsha river are all 1000 year the 1000 year design flood hydrographs of wdd bht xld and xjb reservoirs in construction period and in operation period are compared in fig 8 the statistics of the 1000 year design floods in construction period and the estimated design floods in operation period are summarized in table 9 results show that 1 the 1000 year design flood hydrographs of the four reservoirs alter considerably in operation period with smaller flood peaks and gentler flood processes 2 the 1000 year design q max w 3 w 7 and w 30 of the four reservoirs decrease significantly due to the upstream reservoir operation for instance the 1000 year design q max w 3 w 7 and w 30 of xjb reservoir decrease by 38 7 37 4 34 2 13 8 respectively in operation period compared with the cascade reservoirs in the midstream jinsha river and yalong river greater significant reduction rates are observed 3 due to the significant decrease in design floods the flwl of the four reservoirs in operation period can be largely raised the flwls of wdd bht xld and xjb reservoirs in operation period in construction period are 958 01 952 m 793 6 785 m 572 71 5 6 0 m and 372 09 370 m respectively 4 under the flwl scheme in operation period the wdd bht xld and xjb reservoirs can increase the hydropower generation hg by 4 7 4 8 7 2 and 2 2 respectively it can be summarized from table 9 that the cascade reservoirs in the downstream jinsha river can increase annual hydropower generation by 3 15 billion kw h during flood season in reservoir operation period 5 discussion 5 1 analysis of efrc and mlrc results as can be seen from table 5 the results of efrc and mlrc of gyy reservoir are analogous in this case study this is because the inflow discharges are highly correlated as demonstrated in section 2 2 3 4 the kendall s tau correlation coefficients between the am 7d flood volume series of cascade reservoirs in the midstream jinsha river are calculated and listed in table 10 it is indicated from table 10 that the correlations are considerably strong with the kendall s tau correlation coefficients all greater than 0 7 the main reasons for the strong correlation are summarized as follows 1 the drainage areas of these reservoirs are quite similar table 3 and there are no big tributaries between these reservoirs fig 4 2 they belong to the same rainfall zone and yield analogous climatic factors consequently the equivalent frequency floods at each reservoir site are likely to occur in this case study and the efrc method can be adopted for such catchment with high correlated inflows across all sub basins while the mlrc method which can be applied to both high and low correlated sub basin inflows is more rational than the efrc method on the other hand as noted before different frcs may differ in terms of their occurrence probabilities which can be measured by the value of joint probability density function of floods occurring at all sub basins guo et al 2018 the larger joint probability density function denotes the greater occurrence probability of the frc in practical application the frc which is more likely to occur usually gains more attention therefore the occurrence probabilities of efrc and mlrc for design flood with different return periods are compared in fig 9 taking the lkk and gyy reservoirs as an example it can be seen that the occurrence probabilities of efrc mlrc for design flood of lkk reservoir with return periods of 1000 500 and 100 year are 2 32 5 64 10 7 3 48 12 6 10 7 and 9 42 22 04 10 7 respectively and the occurrence probabilities of efrc mlrc for design flood of gyy reservoir with return periods of 1000 500 and 100 year are 1 35 4 17 10 8 2 76 15 75 10 8 and 14 15 56 6 10 8 respectively when the floods with return periods of 1000 500 and 100 year occur at the lkk gyy reservoir the occurrence probabilities of mlrc are 2 43 3 09 3 62 5 49 and 2 34 4 0 times as large as that of efrc respectively this explicitly implies that the mlrc offers a meaningful flood regional composition scheme with high occurrence probability in terms of occurrence probability the mlrc performs much better than efrc method 5 2 relation between the reduction rates and available flood control storage the relation between the reduction rates of q max w 3 w 7 and w 30 and available flood control storage are investigated and plotted in fig 10 in recognition that the available flood control storage can largely influence the reduction rates it can be observed from fig 10 that 1 the q max and w 30 are the most and least impacted by the available flood control storage respectively this is mainly because a reservoir will store water to reduce flood peak when a flood occurs however not for a long period such as 30 days in flood season 2 the reduction rates of designed q max w 3 w 7 and w 30 increase with the increase of available flood control storage while the increasing rate shows a decreasing trend the available flood control storages of the cascade reservoirs in the downstream jinsha river are considerably larger than that in the midstream jinsha river and yalong river and thus the reduction rates are also larger the decreasing trend in increasing rate is mainly because when the flood control storage is sufficient for the flood control purpose the flood can be well regulated and the reduction rate will no longer increase with the increase of flood control storage 5 3 practical application of flwl in operation period in this study it is shown that the formation of cascade reservoir system has resulted in considerable attenuation effects on design floods of downstream reservoirs with advancements in meteorological and hydrological forecasting capabilities the operational efficiency of existing reservoir has been improved significantly which creates the possibility for the practical application of flwl in operation period for example on basis of real time flood forecasting a pre release strategy can be adopted to bring substantial economic benefits without increasing flood control risks li et al 2010 based on the application experience a pre release strategy combined with hydrological forecasts can be adopted for practical application of flwl in operation period when recent hydrological forecasts indicate that no extreme flood e g return period greater than 100 year will occur the reservoir can be operated within the flwl in operation period when extreme floods are forecasted to occur within the forecast horizon reservoir can pre release flood water to operate within the flwl in construction period to ensure the safety of dam as well as downstream sections this pre release strategy has been widely used to better compromise between flood control and conservation and it will be quantitatively investigated for practical application of flwl in operation period in future work 6 conclusions in this study the design flood and flwl in operation period are defined for practical application a general framework enabling to measure the spatiotemporal pattern of streamflow for deriving the design floods in operation period for cascade reservoirs is established the multivariate t copula and ga based strategy are applied to solve the curse of dimensionality involved in the derivation of mlrc the hybrid reservoir system in jinsha and yalong river is selected as the case study the main conclusions are summarized as follows 1 the multivariate t copula can be effectively implemented for establishing high dimensional joint distributions and the ga based method can be adopted to achieve the mlrc in high dimension the proposed framework can be effectively implemented for a complex cascade reservoir system 2 compared with the design floods in construction period design floods of the downstream reservoirs in operation period have been reduced significantly due to the regulation of upstream reservoirs when the available flood control storage is larger the reduction rates of design floods are even greater for instance the 1000 year design q max w 3 w 7 and w 30 of xjb reservoir decrease by 38 7 37 4 34 2 13 8 respectively in operation period hence the impact of upstream reservoir regulation on downstream reservoir cannot be ignored and the investigated reservoir operation policy formulated based on the design flood in operation period should be adopted 3 the flwl in operation period can be raised compared with the design value in construction period the flwls of ah jaq lkk ldl gyy jp et wdd bht xld and xjb reservoirs in operation period in construction period are 1493 7 1493 3 m 1410 8 1410 m 1290 3 1289 m 1212 8 1212 m 1123 5 1122 3 m 1862 4 1859 m 1191 6 1190 m 958 01 9 5 2 m 793 6 7 8 5 m 572 71 5 6 0 m and 372 09 3 7 0 m respectively the economic benefits obtained from the new flwl scheme are enormous as can be summarized from tables 5 and 7 the 13 reservoirs in the jinsha river and yalong river can increase the annual hydropower generation by 3 28 billion kw h or increase 4 3 during flood season without increasing flood control risks declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was financially supported by the national natural science foundation of china 51879192 the national key research and development plan of china 2018yfc1508001 the 111 project fund of china b18037 and the research council of norway frinatek project 274310 the authors would like to thank the editors and anonymous reviewers for their constructive comments which have led to significant improvement on the presentation and quality of the paper 
6264,the hydrological regimes of downstream reservoirs have been significantly altered due to the operation and regulation of upstream cascade reservoirs the original design flood quantiles namely design flood in construction period do not consider anthropogenic impacts in reservoir operation period and have led to enormous conflicts between flood control and conservation in this study the design flood and flood limited water level in operation period are defined for practical application we establish a general framework to measure the spatiotemporal pattern of streamflow and to estimate design floods of cascade reservoirs in operation period the multivariate t copula and a genetic algorithm strategy are proposed to solve the curse of dimensionality encountered in the derivation of most likely regional composition the jinsha river and yalong river cascade reservoir system in china which consists of 13 large reservoirs with the total storage capacity of 74 06 billion m3 and hydropower capacity of 71 47 gw is selected as a case study results indicate that 1 the curse of dimensionality can be well addressed by applying multivariate t copula to build high dimensional joint distribution and using the genetic algorithm to achieve the most likely regional composition 2 compared with the design floods in construction period the design floods of downstream reservoirs in operation period have been significantly reduced due to the upstream reservoir regulation the 1000 year design peak flood discharge 3 day 7 day and 30 day flood volumes of xiangjiaba reservoir decrease by 38 7 37 4 34 2 and 13 8 respectively 3 the flood limited water level of these reservoirs can be raised without increasing flood control risks in operation period the cascade reservoirs in the jinsha river and yalong river can generate 3 28 billion kw h more hydropower or increase 4 3 annually during flood season keywords cascade reservoirs design flood most likely regional composition flood limited water level multivariate t copula genetic algorithm 1 introduction reservoirs are one of the most efficient infrastructure components for integrated water resources management guo et al 2004 liu et al 2015 in recent decades more and more reservoirs have been built for flood control and hydropower generation purposes and to meet the social economic development forming a large quantity of cascade reservoir systems throughout the world zhao et al 2016 liu et al 2016 yazdi and moridi 2018 the function of reservoirs in terms of flood water utilization has become increasingly important li et al 2010 ouyang et al 2015 reservoirs have significant impact on hydrological characteristics especially the temporal variation of maximum stream flows gao et al 2019 with the formation of substantial numbers of cascade reservoir systems the hydrological regimes of downstream section have been significantly altered the statistical properties e g mean variance skewness of the extreme flood series of downstream reservoir in operation period are considerably different from the annual maximum data series adopted for design flood in construction period liang et al 2018 therefore the investigation on design flood in operation period is of great significance in terms of scientific reservoir management the flood limited water level flwl in construction period which is determined by the reservoir regulation of the annual design flood hydrographs for given return period serves as the most vital parameter of tradeoff between flood control and conservation yun and singh 2008 li et al 2010 during the flood season the reservoir water level is not allowed to exceed the flwl in order to offer adequate storage for possible incidences of large floods liu et al 2015 the flwl in construction period gives too much priority to low probability floods which makes it a great challenge for reservoirs to produce the level of benefits that provides the economic justification for their development liu et al 2015 with the formation of cascade reservoir systems the application of flwl in construction period has led to greater conflicts between flood control and water resources utilization for example the downstream reservoirs in a cascade reservoir system are often unable to refill to the normal water level by the end of refilling period under circumstances where all cascade reservoirs compete to impound water zhou et al 2015 which largely limits the comprehensive benefits of these reservoirs therefore it is valuable to find the appropriate flwl in operation period for raising water resources comprehensive utilization rate recognizing the drawbacks of design flood and flwl in construction period the design flood and flwl in operation period are defined for practical application in this study yet only a handful of studies made preliminary attempts to study the reservoir impacts on downstream design floods which can be classified into three categories the first option is based on the non stationary flood frequency analysis framework with the reservoir index to represent the impacts of reservoir lÃ³pez and francÃ©s 2013 zhang et al 2015 wang et al 2017 the reservoir index can be taken into account in a generalized addictive model as an effective covariate to study the reservoir impacts nevertheless all the variables in a reservoir index series are constants in a specific period regardless of reservoir regulation strategies su and chen 2019 therefore the reservoir index may not be capable of accounting for the complex joint operation strategies of a cascade reservoir system the second option is to derive flood frequency distributions resulting from reservoir impacts by applying linear and nonlinear reservoir models gao et al 2019 however the linear and nonlinear models are both the simplification of reservoir operation rules basha 1994 and thus remain non ignorable biases from actual situation kim and georgakakos 2014 these differences could be further significantly amplified due to the high dimensional and nonlinear features of cascade reservoir system the third option is to study the reservoir impacts based on the flood regional composition frc framework guo et al 2018 the frc framework aims at analyzing the flood generation mechanism of the investigated reservoir the inflow of the investigated downstream reservoir comprises the inflow of upstream reservoir and all intermediate basin inflows between reservoirs the upstream reservoir inflow can be transformed into outflow according to man made reservoir operation rules the design flood of the investigated downstream reservoir in operation period is influenced by 1 the characteristics of upstream reservoirs flood control storage operation rules etc 2 the frc of the reservoir therefore the methods based on the frc framework aim to seek an appropriate frc from all possible compositions that satisfy water balance equation guo et al 2018 derived theoretical formula to achieve the most likely regional composition mlrc method for triple cascade reservoirs and evaluated the reservoir operation impacts based on the mlrc results the mlrc assumes that frcs may differ in terms of their probabilities of occurrence which can be measured by the value of joint probability density function of floods occurring at all sub basins and the frc with the largest occurrence probability should be selected based on this assumption guo et al 2018 adopted the asymmetric archimedean copula to build the joint distribution of floods occurring at all sub basins and used the newton iteration method to seek for the optimal solution of the mlrc guo s method with strong statistical basis is likely to be implemented for practical use but the barriers are twofold for derivation of mlrc as the number of cascade reservoirs increases 1 the asymmetric archimedean copula adopted to build the joint distributions in guo s study will bring large modelling errors triggered by the uncertainty of nesting structure and inability to be fully nested in high dimension serinaldi and grimaldi 2007 hofert and pham 2013 2 the newton iteration method adopted in guo s study is not capable of obtaining the optimal solution of the formula in high dimension due to the local convergence of the method dembo et al 1982 pilanci and wainwright 2017 bottou et al 2018 therefore the curse of dimensionality has become the main problem to be addressed in the literature it is essential to conduct in depth research with state of the art techniques to conquer these challenges in this study we focus on overcoming the curse of dimensionality based on guo s method due to its ease of implementation and potentiality in practical application the abovementioned two bottlenecks caused by high dimensionality in deriving mlrc are addressed by selecting the multivariate t copula to build the joint distribution and by applying a genetic algorithm ga based method the multivariate t copula belongs to the meta elliptical class and can be easier extended to higher dimension compared with archimedean copulas serinaldi et al 2009 moreover the empirical fit of the t copula is often good and is almost always superior to that of the gaussian copula mashal et al 2003 breymann et al 2003 the t copula has been given more attention recently due to its favorable empirical fit and ease of implementation e g zhang et al 2016 rivieccio and de luca 2016 hao et al 2017 he et al 2017 and therefore is considered in this study the derivation of mlrc in higher dimension typically involves highly nonlinear and nonconvex problems as opposed to the current formula based technique adopted by guo et al 2018 we develop a ga based method to obtain the robust solution of mlrc in higher dimension the ga proposed by holland 1975 has the ability to mimic processes observed in natural evolution and is prevalently utilized to address various optimization problems in hydrological practice e g yin et al 2017 ehteram et al 2018 zhou et al 2019 based on the derivation of mlrc in high dimension we propose a general framework for deriving design floods for cascade reservoir system and determining the flwl in operation period to ensure that the results of mlrc are reasonable and can be adopted for practical use the standard frc method recommended by mwr 2006 i e equivalent frequency regional composition efrc method is also applied and compared in this study this study is therefore explored with three foci 1 introducing the design flood and flwl in operation period to practical application in hydrology and 2 addressing the curse of dimensionality encountered in the derivation of mlrc by applying the multivariate t copula and a ga based strategy 3 developing a general framework for determining the design flood of cascade reservoirs in operation period the present paper is structured as follows section 2 describes the methodology used in this study section 3 briefly introduces the study area and the material in sections 4 a mix cascade reservoir system in the jinsha river and yalong river in china is selected as a case study the impact of upstream reservoir regulation on design floods as well as the flwls of downstream reservoirs are analyzed and discussed the last two sections are devoted to the discussion and conclusion of this study respectively 2 methodology 2 1 design flood in operation period the similarities and differences between design flood in construction period and in operation period are compared in table 1 the purpose of design flood in construction period is to provide vital information for the design of the reservoir storage spillway size etc while the design flood in operation period considers the impact of upstream reservoir operation and thus is capable of adapting to the variation of upstream hydrological regime and its main purpose is to counterbalance the conflicts between flood control and conservation in terms of property the design flood in construction period and in operation period are both static values which are unchanged over time and therefore can be easily implemented for practical use by applying flwl in construction period the reservoir can regulate low frequency design floods but the comprehensive benefits of reservoir storage cannot be fully utilized by applying flwl in operation period the reservoir can withstand the same frequency floods regulated by upstream reservoirs and gain more comprehensive benefits during the flood season to summarize the main difference between the design flood in construction period and in operation period is that the latter considers the impact of upstream reservoir regulation and tends to be more rational than the former in terms of flood water utilization 2 2 derivation of design flood in operation period 2 2 1 framework the main goal of this study aims at exploring a general framework for estimating design flood for a cascade reservoir system in operation period in which the curse of dimensionality is the main task to be addressed the architecture of the proposed framework is shown in fig 1 which comprises three modules namely 1 design flood in construction period module 2 flood regional composition frc module and 3 design flood in operation period module the first module is responsible for calculating the design flood peak volumes and design flood hydrographs in construction period for the second and third modules respectively in this study the design floods both in operation and construction periods are analyzed for the given same return periods and flood prevention standards the second module is responsible for obtaining the frc of the investigated reservoir in this module the multivariate t copula is proposed to build high dimensional joint distributions and a ga based method is proposed to derive the mlrc the efrc method suggested by mwr 2006 is also applied in this module for comparative study finally the design flood and flwl in operation period is calculated in the last module based on the design flood hydrographs in construction period obtained in the first module and the frc results obtained in the second module by regulating the design flood hydrographs in construction period with the consideration of the operation rules of upstream cascade reservoirs the complex impact of reservoir regulation on downstream reservoir design flood can be numerically evaluated in this module the main procedures and detailed explanations are given as follows 2 2 2 design flood in construction period module the general methodology of estimating design flood in construction period is to fit the flow data such as the annual maximum am data using a theoretical probabilistic distribution and derive the exceedance probability based on the selected distribution xiong et al 2018 the pearson type 3 p3 distribution and curve fitting estimation method is recommended by ministry of water resources mwr 2006 as the standard hydrological frequency analysis procedure in china and thus is adopted in this study compared with commonly used methods such as l moments and maximum likelihood method the curve fitting method can easily incorporate historical and paleo flood information while guarantee satisfactory statistical properties of the estimation results hu 1987 hua 1987 the kolmogorov smirnov ks test and anderson darling ad test are selected as the goodness of fit statistics to determine whether the candidate variables follow the p3 distribution zhang and singh 2007 huang et al 2018b from the perspective of engineering safety the flood which may lead to serious damage would be preferred when selecting typical flood hydrograph for reservoir design purpose mwr 2006 xiong et al 2019 severe floods occurred in both the mainstream and main tributaries of jinsha river in 1966 flood season therefore the 1966 flood hydrograph is selected as the typical flood hydrograph in this study 2 2 3 flood regional composition module 2 2 3 1 flood regional composition for cascade reservoirs the flood regional composition frc has been recommended by mwr 2006 to solve a wide range of problems for the following reasons 1 methods based on the frc are easy to implement lu et al 2012 2 methods based on frc can provide numerical solutions even for highly complex problems such as calculating the flood control effects of reservoir on downstream section and analyzing the joint operation strategies of cascade reservoirs or reservoir group which is essential for the engineering practice guo et al 2018 the sketch diagram of cascade reservoirs is given in fig 2 in general a more complex hybrid reservoir group can be decomposed into several cascade reservoirs based on large scale system theories chen et al 2017 let a a1 a2 a n b b1 b2 b n and c denote the reservoirs intermediate basins and downstream reservoir site of interest respectively and let random variables x 1 x 2 xn y 1 y 2 yn and z represent the natural flood volumes of reservoirs a1 a2 a n intermediate basins b1 b2 b n and downstream reservoir c with the corresponding values x 1 x 2 xn y 1 y 2 yn and z respectively we focus on the a1 a2 sub system to further demonstrate the frc concept the design flood of downstream reservoir a2 is impacted by the regulation of upstream reservoir a1 the inflow of downstream reservoir a2 is composed of inflow of upstream reservoir a1 and intermediate basin inflow b1 thus x 1 y 1 is the frc of x 2 in which x 1 can be transformed into outflow based on the operation rules of a1 therefore the statistical behavior of design flood of downstream reservoir a2 in operation period is deterministically related to the frc of a2 i e x 1 y 1 for the cascade reservoir system according to the principle of water balance equation all the frc x 1 y 1 y 2 yn z should be subjected to the following equations 1 x 1 y 1 x 2 x 1 y 1 y 2 x 3 x 1 y 1 y 2 y n z due to the inherent stochastic nature of flood generation mechanism there are countless compositions of x 1 y 1 y 2 yn z aligning with the principle of water balance in order to search for a representative frc which is consistent with the spatiotemporal pattern of streamflow two frc methods based on different assumptions are considered in this study and are introduced hereafter 2 2 3 2 equivalent frequency regional composition efrc method the efrc method assumes that the return period of flood occurring at one of the subareas reservoir or the intermediate basin is the same as the downstream reservoir though easy to implement the efrc suffers from two main drawbacks guo et al 2018 1 it assumes perfect correlation between floods occur at one sub basin and downstream site which does not always conform to the reality 2 for the composition of cascade reservoirs the number of efrcs 2 n 1 increases exponentially with the increase of the number of cascade reservoirs n in this study only one representative efrc for cascade reservoirs namely efrc 1 is considered for the purpose of comparison for a given return period the floods occurring at all reservoirs x 1 x 2 xn z can be determined by their respective fitted distributions the floods occurring at all intermediate basins y 1 y 2 yn can be calculated using the water balance equation efrc x 1 y 1 y 2 yn can thus be determined readers can refer to liang et al 2017 and guo et al 2018 for more details about the efrc method 2 2 3 3 most likely regional composition mlrc method to overcome the drawbacks of efrc guo et al 2018 proposed the mlrc based on copulas they indicated that 1 the mlrc method with stronger statistical basis can better capture the actual spatial correlation of flood events occurring at different sub basins 2 the composition of mlrc method is unique and thus is easy to implement for large cascade reservoir system the mlrc method assumes that frcs may differ in terms of their probability of occurrence which can be measured by the value of joint probability density function of random variables x 1 x 2 xn z i e f x 1 x 2 xn z according to sklar s theorem nelsen 2006 the joint probability density function can be expressed in terms of its marginal distributions and the associated dependence function i e copula function as follows salvadori and de michele 2004 salvadori et al 2016 2 f x 1 x 2 x n 1 x n z c u 1 u 2 u n 1 u n v i 1 n f x i x i f z z where c is the pdf of copula function and u 1 u 2 un v are the corresponding empirical frequencies of x 1 x 2 xn z f x i x i i 1 2 n and f z z are the marginal distributions of floods occurring at reservoirs a1 a2 a n and downstream reservoir c respectively the composition x 1 x 2 xn z is more likely to occur when the value of density function f x 1 x 2 xn z increases salvadori et al 2011 in order to search for the mlrc the f x 1 x 2 xn z is maximized by subjecting water balance constraint in eq 1 for the given z zp 3 max f x 1 x 2 x n 1 x n z c u 1 u 2 u n 1 u n v i 1 n f x i x i f z z s t x 1 y 1 x 2 x 1 y 1 y 2 x 3 x 1 y 1 y 2 y n z z p where zp is the design flood volume for a given return period of the investigated reservoir obtained from the design flood in construction period module the joint probability density f x 1 x 2 xn z is maximized when its first order derivative equals zero and the following equation should be satisfied 4 f x 1 x 2 x n 1 x n z x 1 0 f x 1 x 2 x n 1 x n z x 2 0 f x 1 x 2 x n 1 x n z z 0 s t x 1 y 1 x 2 x 1 y 1 y 2 x 3 x 1 y 1 y 2 y n z z p after the composition x 1 x 2 xn z is derived from eq 4 mlrc x 1 y 1 y 2 yn can be determined using the water balance equation as indicated previously guo s method suffers from curse of dimensionality as the number of cascade reservoirs increase in this study the curse of dimensionality is addressed by the following flow chart as shown in fig 3 and the main procedures are described as follows 1 determine the marginal distributions and joint distribution according to the sampled am data series the vine copula and meta elliptical copula are often used to establish a higher dimensional joint distribution the vine copula embraces a large quantity of pair copula decompositions for high dimensional variables which makes the selection of suitable vine composition considerably complex aas et al 2009 on the other hand the meta elliptical copula can model arbitrary pairwise dependence structures through a correlation matrix and is easy to implement huang et al 2018a considering the advantage of meta elliptical copula and the drawback of vine copula the meta elliptical copula is selected in this paper gaussian copula and t copula are the commonly used meta elliptical copulas mashal et al 2003 and breymann et al 2003 have shown that the goodness of fit of the t copula is almost always superior to that of the gaussian copula serinaldi et al 2009 and chen et al 2016 also indicated that t copula is adequate to construct high dimensional joint distribution hence the t copula is employed to establish the joint distribution of the flood volumes of different sub basins in this study the maximum likelihood method is adopted to estimate parameters of t copula huang et al 2018a the goodness of fit of t copula is assessed with the ks test and cramer von mises cm test genest and rÃ©millard 2008 genest et al 2009 the degree of freedom of t copula is selected under the criteria of the root mean square error rmse and akaike information criterion aic xiong et al 2018 zhong et al 2018 the smaller rmse and aic represents the better performance of the candidate model 2 in order to find the optimal composition of flood volumes x 1 x 2 xn that maximizes the joint probability density function the negative value of joint probability density is set as the fitness function and the flood volumes of the upstream reservoirs x 1 x 2 xn is set as the parameters to be optimized considering the water balance equations should always be satisfied in frc the water balance equations are employed as constraints to determine the initial value and threshold of the parameters the fitness function and constraints are expressed as follows 5 min f x 1 x 2 x n 1 x n z c u 1 u 2 u n 1 u n v i 1 n f x i x i f z z s t x 1 y 1 x 2 x 1 y 1 y 2 x 3 x 1 y 1 y 2 y n z z p where the corresponding empirical frequencies u 1 u 2 un can be expressed with respect to their marginal distributions the formula based derivation of mlrc for an n dimensional cascade reservoir system adopted by guo et al 2018 is substantially equivalent to solving a set of n dimensional nonlinear and nonconvex equations and the optimal solution can be hard to achieve by the newton iteration method due to its inherent drawbacks of local convergence and susceptibility to initial solution the ga based method in this study however can effectively achieve the computational complexity reduction by transforming the derivation process into an optimization process the ga technique has proved to be capable of coping with highly complex optimization problems chang et al 2003 ghareb et al 2016 attaining the mlrc of n dimensional cascade reservoir system corresponds to an optimization problem with n parameters to be optimized and currently the number of reservoirs n is usually less than 20 moreover different initial conditions have been considered to test whether the optimal solution is robust results indicate that when using different initial seeds the obtained optimal solutions show no significant difference therefore the ga based method can be effectively applied to existing cascade reservoir systems 2 2 3 4 comparison of efrc and mlrc the basic cascade reservoir system see sub system a1 a2 with synthetic inflow time series are used to compare the efrc and mlrc methods since both efrc and mlrc methods focus on the analysis of am data series we adopt the following procedures to generate am flood series of a1 and a2 with different correlations for comparison firstly the margins of a1 and a2 are assumed to be p3 distribution and the location scale shape parameters of p3 distribution for a1 a2 reservoir are assumed to be 30 50 7 5 12 5 and 4 4 respectively secondly two sets of uniform variables over 0 1 are randomly generated from a bivariate t copula with sample size m 60 using r package copula demarta and mcneil 2005 yan 2007 which are considered as the empirical frequencies of the am flood series of the two reservoirs and the correlation coefficients between the two sets of uniform variables are set to be 0 01 0 3 0 6 and 0 99 respectively to demonstrate am flood series with different correlations finally the am flood series of the two reservoirs can be generated by applying the predetermined p3 quantile functions to the obtained uniform variables with the consideration of noise term the efrc and mlrc methods are used to analyze the frc of 1000 year design flood of a2 inflow and results are summarized in table 2 it can be seen from table 2 that when the correlation coefficient between the am floods of a1 and a2 is extremely high 0 99 the results obtained by mlrc and efrc are nearly the same however when the correlation is very weak 0 01 the differences between results of mlrc and efrc are significant i e the equivalent frequency assumption adopted by efrc is unreasonable for weak correlated floods mwr 2006 thus the mlrc method enabling to consider the actual correlation between am floods may offer more reasonable results compared with efrc 2 2 4 design flood in operation period module the main procedures of this module are described as follows 1 derive the corresponding design flood hydrographs of each sub basin using the peak and volume amplitude pva method zhong et al 2017 yin et al 2018 based on the frc results and design flood hydrographs in construction period the advantage of pva method is that it ensures that both the flood peak and volume are equal to the assigned values without modifying flood duration yin et al 2018 2 obtain the design flood hydrograph at the downstream site c based on river channel flood routing and reservoir flood regulation the commonly used muskingum model is considered for the river channel flood routing franchini et al 2011 guo et al 2018 then the design peak flood discharge and volumes are directly determined from the design flood hydrograph in operation period to offer a numerical estimation of design flood 3 determine the flwl in operation period using the iterative calculation method xiao et al 2009 zhou et al 2015 according to the derived design flood hydrograph and the reservoir operation rules the flood prevention standard cannot be lowered down under the iterative calculation approach zhou et al 2015 3 study area and materials jinsha river is the upper reach of yangtze river and has a total length of 3481 km with a drainage area of 502 000 km2 accounting for 26 of the drainage area of the yangtze river the jinsha river is divided into upper middle and lower reaches with respective lengths of 965 km 1220 km and 1296 km yalong river is the largest tributary of jinsha river with a drainage area of 136 000 km2 during the past decades a series of dams have been built along the main jinsha river and yalong river for the purpose of flood control and hydropower generation among them the liyuan ly ahai ah jinanqiao jaq longkaikou lkk ludila ldl and guanyinyan gyy reservoirs are located in the midstream jinsha river while the wudongde wdd baihetan bht xiluodu xld and xiangjiaba xjb reservoirs are located in the downstream jinsha river along the yalong river there are lianghekou lhk jinping jp ertan et cascade reservoirs the investigated area and sketch map of cascade reservoirs in the jinsha river and yalong river are shown in fig 4 the basic information of these reservoirs is listed in table 3 to promote water resources utilization and hydropower generation these 13 mega cascade reservoirs have been jointly operated to achieve a total reservoir storage of 74 06 billion m3 21 82 billion m3 for flood control and a total installed hydropower capacity of 71 47 gw the current operation rules standard operation policy of these reservoirs are provided by the changjiang yangtze river water resources commission cwrc ministry of water resource cwrc 2018 the data sets used in this study are natural daily streamflow data of the investigated dam sites with record lengths all longer than 50 years the data sets are provided by the cwrc and have been widely used for the design and management of these reservoirs 4 result analysis 4 1 the design flood in construction period according to the regulation characteristics of cascade reservoirs in the jinsha river and yalong river annual maximum 7 day and 30 day denoted as am 7d and am 30d flood volume series are selected for flood frequency analysis the p values of ks and ad tests of each variable are calculated for both of the tests when p value is larger than 0 05 the assumption that the am series follows the p3 distribution cannot be rejected at a 5 significance level for illustration only the cumulative distributions of am 7d flood volumes series fitted by p3 distributions for ah lkk gyy et bht and xjb reservoirs are plotted in fig 5 the figure shows a good correspondence between empirical and theoretical frequencies and the assumption that the variables follow the p3 distribution cannot be rejected 4 2 design flood of cascade reservoirs in the midstream jinsha river and yalong river 4 2 1 flood regional composition there are no intermediate inflows from tributaries between cascade reservoirs in the midstream jinsha river and yalong river fig 4 and the frc for these reservoirs can be directly analyzed by the general form fig 2 the frc of am 7d and 30d flood volumes of each reservoir in the midstream jinsha river and yalong river are analyzed taking the frc of gyy reservoir as an example the flood volume of gyy reservoir x 6 is decomposed to ly reservoir x 1 l a inter basin y 1 a j inter basin y 2 j l inter basin y 3 l l inter basin y 4 and l g inter basin y 5 taking the establishment of joint distribution for the mlrc of gyy reservoir as an illustration the p values of ks and cm tests rmse and aic values of t copulas with different degrees of freedom are calculated and listed in table 4 it can be seen that all p values are greater than 0 05 indicating that the variables following t copula cannot be rejected at a 5 significance level table 4 indicates that the t copula with v 3 degrees of freedom exhibits the smallest rmse and aic and thus should be selected the adequacy of the t copula is further evaluated with the p p plots by plotting the empirical copula against theoretical copula for illustration only fig 6 shows the p p plots of joint distributions for the mlrc method of ah lkk gyy and et reservoirs respectively the p values obtained from ks and cm tests are also shown it can be seen from fig 6 that all p values of all t copulas are greater than 0 05 and no strong departure from the expected distribution can be observed from the p p plots therefore the t copula can be effectively implemented for establishing joint distributions in high dimension following the procedures described in section 2 2 3 the efrc and mlrc methods of am 7d and 30d flood volumes of cascade reservoirs are obtained taking the frc of am 7d flood volumes of gyy reservoir as an example it can be observed from table 5 that the results obtained by the efrc and mlrc methods show no significant difference in this case study the similarity between the results of efrc and mlrc is due to the fact that there exist strongly positive correlations between the flood volumes of downstream reservoir and its upstream reservoirs and this will be further discussed in section 5 1 regarding the curse of dimensionality guo s method is compared with the proposed method for deriving mlrc with different numbers of cascade reservoirs taking the derivation of mlrc of ah 2 reservoirs and ldl reservoirs 5 reservoirs as illustrations the mlrc derived by guo s method and the proposed method are compared in table 6 as for the derivation of mlrc for ah reservoir the results obtained by the two methods are nearly the same this indicates that the solutions of the proposed method are reliable as for the derivation of mlrc for ldl reservoir guo s method cannot obtain the solution of mlrc while the proposed method can effectively achieve the mlrc of ldl reservoir this demonstrates that guo s method can be subject to the curse of dimensionality as the number of cascade reservoirs increases which can be well addressed by the proposed method 4 2 2 reservoir design flood in operation period the impact of upstream reservoir operation can be analyzed following the procedures described in section 2 2 4 the original design flood frequencies of cascade reservoirs in the midstream jinsha river and yalong river are 0 2 return period is 500 year and 0 1 return period is 1000 year respectively the design frequency floods in construction period and in operation period are compared in table 7 results indicate that 1 the estimated design peak flood discharge q max 3 day w 3 7 day w 7 and 30 day w 30 flood volumes of downstream reservoirs decrease due to the upstream reservoir operation for instance the estimated q max w 3 w 7 and w 30 of gyy reservoir in operation period decrease by 11 5 9 7 7 3 and 0 4 respectively compared with the design values in construction period 2 the reduction rates of downstream reservoirs are greater than that of the upstream reservoirs this is as expected that when the available flood control storage is larger the flood can be better regulated 3 the flwl of downstream reservoirs in operation period is higher than that of the designed values in construction period the flwls of ah jaq lkk ldl gyy jp and et reservoirs in operation period in construction period are 1493 7 1493 3 m 1410 8 1410 m 1290 3 1289 m 1212 8 1212 m 1123 5 1122 3 m 1862 4 1859 m and 1191 6 1190 m respectively the less required flood control storage accounts for the raise of flwls in operation period 4 under the flwl scheme in operation period the ah jaq lkk ldl gyy jp and et reservoirs increase the hydropower generation by 0 45 0 72 2 1 1 1 2 1 6 and 0 6 respectively it can be summarized from table 7 that the cascade reservoirs in the midstream jinsha river and yalong river can increase annual hydropower generation hg by 126 million kw h in flood season in operation period 4 3 design flood of cascade reservoirs in the downstream jinsha river 4 3 1 flood regional composition the inflows of cascade reservoirs in the downstream jinsha river are impacted by both the cascade reservoirs in the midstream jinsha river and yalong river see fig 4 and it would not be possible to carry out frc analysis without decomposition chen et al 2017 the frc of wdd bht xld and xjb reservoirs follows a three step procedure described as below taking the frc of xjb reservoir as an illustration firstly the flood volume of xjb x 13 is decomposed to wdd reservoir x 10 w b inter basin y 9 b x inter basin y 10 and x x inter basin y 11 secondly the flood volume of wdd reservoir x 10 is decomposed to gyy reservoir x 6 et reservoir x 9 and g e w inter basin y 8 finally the flood volume of gyy reservoir x 6 is decomposed to ly reservoir x 1 l a inter basin y 1 a j inter basin y 2 j l inter basin y 3 l l inter basin y 4 and l g inter basin y 5 and the flood volume of et reservoir x 9 is decomposed to lhk reservoir x 7 l j inter basin y 6 and j e inter basin y 7 the frc of gyy and et reservoirs has been achieved in section 4 2 the efrc and mlrc are adopted to obtain the frc of these reservoirs following the decomposition procedure fig 7 shows the goodness of fit of the selected t copula table 8 lists the results of frc using the xjb reservoir as an example results from fig 7 and table 8 reveal again that 1 the t copula can be effectively implemented to establish the joint distributions for mlrc of the investigated reservoir 2 the mlrc method can be effectively implemented for the complex hybrid reservoir system 4 3 2 design flood in operation period the design flood return periods of cascade reservoirs in the downstream jinsha river are all 1000 year the 1000 year design flood hydrographs of wdd bht xld and xjb reservoirs in construction period and in operation period are compared in fig 8 the statistics of the 1000 year design floods in construction period and the estimated design floods in operation period are summarized in table 9 results show that 1 the 1000 year design flood hydrographs of the four reservoirs alter considerably in operation period with smaller flood peaks and gentler flood processes 2 the 1000 year design q max w 3 w 7 and w 30 of the four reservoirs decrease significantly due to the upstream reservoir operation for instance the 1000 year design q max w 3 w 7 and w 30 of xjb reservoir decrease by 38 7 37 4 34 2 13 8 respectively in operation period compared with the cascade reservoirs in the midstream jinsha river and yalong river greater significant reduction rates are observed 3 due to the significant decrease in design floods the flwl of the four reservoirs in operation period can be largely raised the flwls of wdd bht xld and xjb reservoirs in operation period in construction period are 958 01 952 m 793 6 785 m 572 71 5 6 0 m and 372 09 370 m respectively 4 under the flwl scheme in operation period the wdd bht xld and xjb reservoirs can increase the hydropower generation hg by 4 7 4 8 7 2 and 2 2 respectively it can be summarized from table 9 that the cascade reservoirs in the downstream jinsha river can increase annual hydropower generation by 3 15 billion kw h during flood season in reservoir operation period 5 discussion 5 1 analysis of efrc and mlrc results as can be seen from table 5 the results of efrc and mlrc of gyy reservoir are analogous in this case study this is because the inflow discharges are highly correlated as demonstrated in section 2 2 3 4 the kendall s tau correlation coefficients between the am 7d flood volume series of cascade reservoirs in the midstream jinsha river are calculated and listed in table 10 it is indicated from table 10 that the correlations are considerably strong with the kendall s tau correlation coefficients all greater than 0 7 the main reasons for the strong correlation are summarized as follows 1 the drainage areas of these reservoirs are quite similar table 3 and there are no big tributaries between these reservoirs fig 4 2 they belong to the same rainfall zone and yield analogous climatic factors consequently the equivalent frequency floods at each reservoir site are likely to occur in this case study and the efrc method can be adopted for such catchment with high correlated inflows across all sub basins while the mlrc method which can be applied to both high and low correlated sub basin inflows is more rational than the efrc method on the other hand as noted before different frcs may differ in terms of their occurrence probabilities which can be measured by the value of joint probability density function of floods occurring at all sub basins guo et al 2018 the larger joint probability density function denotes the greater occurrence probability of the frc in practical application the frc which is more likely to occur usually gains more attention therefore the occurrence probabilities of efrc and mlrc for design flood with different return periods are compared in fig 9 taking the lkk and gyy reservoirs as an example it can be seen that the occurrence probabilities of efrc mlrc for design flood of lkk reservoir with return periods of 1000 500 and 100 year are 2 32 5 64 10 7 3 48 12 6 10 7 and 9 42 22 04 10 7 respectively and the occurrence probabilities of efrc mlrc for design flood of gyy reservoir with return periods of 1000 500 and 100 year are 1 35 4 17 10 8 2 76 15 75 10 8 and 14 15 56 6 10 8 respectively when the floods with return periods of 1000 500 and 100 year occur at the lkk gyy reservoir the occurrence probabilities of mlrc are 2 43 3 09 3 62 5 49 and 2 34 4 0 times as large as that of efrc respectively this explicitly implies that the mlrc offers a meaningful flood regional composition scheme with high occurrence probability in terms of occurrence probability the mlrc performs much better than efrc method 5 2 relation between the reduction rates and available flood control storage the relation between the reduction rates of q max w 3 w 7 and w 30 and available flood control storage are investigated and plotted in fig 10 in recognition that the available flood control storage can largely influence the reduction rates it can be observed from fig 10 that 1 the q max and w 30 are the most and least impacted by the available flood control storage respectively this is mainly because a reservoir will store water to reduce flood peak when a flood occurs however not for a long period such as 30 days in flood season 2 the reduction rates of designed q max w 3 w 7 and w 30 increase with the increase of available flood control storage while the increasing rate shows a decreasing trend the available flood control storages of the cascade reservoirs in the downstream jinsha river are considerably larger than that in the midstream jinsha river and yalong river and thus the reduction rates are also larger the decreasing trend in increasing rate is mainly because when the flood control storage is sufficient for the flood control purpose the flood can be well regulated and the reduction rate will no longer increase with the increase of flood control storage 5 3 practical application of flwl in operation period in this study it is shown that the formation of cascade reservoir system has resulted in considerable attenuation effects on design floods of downstream reservoirs with advancements in meteorological and hydrological forecasting capabilities the operational efficiency of existing reservoir has been improved significantly which creates the possibility for the practical application of flwl in operation period for example on basis of real time flood forecasting a pre release strategy can be adopted to bring substantial economic benefits without increasing flood control risks li et al 2010 based on the application experience a pre release strategy combined with hydrological forecasts can be adopted for practical application of flwl in operation period when recent hydrological forecasts indicate that no extreme flood e g return period greater than 100 year will occur the reservoir can be operated within the flwl in operation period when extreme floods are forecasted to occur within the forecast horizon reservoir can pre release flood water to operate within the flwl in construction period to ensure the safety of dam as well as downstream sections this pre release strategy has been widely used to better compromise between flood control and conservation and it will be quantitatively investigated for practical application of flwl in operation period in future work 6 conclusions in this study the design flood and flwl in operation period are defined for practical application a general framework enabling to measure the spatiotemporal pattern of streamflow for deriving the design floods in operation period for cascade reservoirs is established the multivariate t copula and ga based strategy are applied to solve the curse of dimensionality involved in the derivation of mlrc the hybrid reservoir system in jinsha and yalong river is selected as the case study the main conclusions are summarized as follows 1 the multivariate t copula can be effectively implemented for establishing high dimensional joint distributions and the ga based method can be adopted to achieve the mlrc in high dimension the proposed framework can be effectively implemented for a complex cascade reservoir system 2 compared with the design floods in construction period design floods of the downstream reservoirs in operation period have been reduced significantly due to the regulation of upstream reservoirs when the available flood control storage is larger the reduction rates of design floods are even greater for instance the 1000 year design q max w 3 w 7 and w 30 of xjb reservoir decrease by 38 7 37 4 34 2 13 8 respectively in operation period hence the impact of upstream reservoir regulation on downstream reservoir cannot be ignored and the investigated reservoir operation policy formulated based on the design flood in operation period should be adopted 3 the flwl in operation period can be raised compared with the design value in construction period the flwls of ah jaq lkk ldl gyy jp et wdd bht xld and xjb reservoirs in operation period in construction period are 1493 7 1493 3 m 1410 8 1410 m 1290 3 1289 m 1212 8 1212 m 1123 5 1122 3 m 1862 4 1859 m 1191 6 1190 m 958 01 9 5 2 m 793 6 7 8 5 m 572 71 5 6 0 m and 372 09 3 7 0 m respectively the economic benefits obtained from the new flwl scheme are enormous as can be summarized from tables 5 and 7 the 13 reservoirs in the jinsha river and yalong river can increase the annual hydropower generation by 3 28 billion kw h or increase 4 3 during flood season without increasing flood control risks declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was financially supported by the national natural science foundation of china 51879192 the national key research and development plan of china 2018yfc1508001 the 111 project fund of china b18037 and the research council of norway frinatek project 274310 the authors would like to thank the editors and anonymous reviewers for their constructive comments which have led to significant improvement on the presentation and quality of the paper 
