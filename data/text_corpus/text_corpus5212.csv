index,text
26060,macroscale hydrological and land surface models increasingly rely on novel streamflow routing algorithms that explicitly account for the presence of engineered water storages which largely affect river basin dynamics in this short communication we contribute to this growing field by presenting vic resopt a software package for the representation and optimization of water reservoirs in the routing model commonly used as a post processor with the variable infiltration capacity vic hydrologic model vic resopt comprises tools for simulating different reservoir operating approaches optimizing operations and exploring the sensitivity of the model output with respect to the parameterization of the operating rules the package relies on automated scripts and optimization engines thereby requiring minimal user effort we demonstrate some of its functionalities in the upper portion of the chao phraya river basin thailand which is regulated by two large dams keywords vic hydrologic model optimal reservoir operations water resources management global and regional studies earth system models software availability name of software vic resopt developer thanh duc dang contact email thanhiwer gmail com year first available 2019 programming languages fortran python dependencies platypus a framework for evolutionary computing in python safe toolbox sensitivity analysis for everybody tested platforms linux ubuntu 14 04 lts trusty tahr and 16 04 xenial xerus available from https github com thanhiwer vicresopt 1 introduction water reservoir operations is a pivotal component of integrated water resources management properly functioning reservoir systems can meet multiple often conflicting objectives such as protecting urban areas from floods supporting agriculture or producing hydropower since these benefits largely depend on the intelligence used to support management decisions researchers have developed a broad spectrum of optimization approaches and algorithms that can effectively aid water managers dobson et al 2019 over the years these algorithms have been implemented in decision support systems that allow users to solve multiple management problems for example hec resprm hydrologic engineering center prescriptive reservoir model program and modsim dss a generic river basin management decision support system use network flow optimization to determine water release strategies for a given set of objectives and inflow time series usace 2003 labadie 2006 riverware adopts a preemptive optimization approach to meet a given set of objectives in order of priority zagona et al 2001 other popular decision support systems powered by optimization algorithms are twole soncini sessa et al 2003 weap yates et al 2005 and mike hydro basin dhi 2017 researchers can also apply the most recent advances in reservoir systems optimization through r and matlab packages such as reservoir turner and galelli 2016 and m3o giuliani et al 2016b importantly reservoir operations has long transcended the domain of integrated water resources management becoming an essential component of regional and global studies on the impact of climate change on water and energy resources in these studies the aforementioned software is often used to post process the streamflow or runoff data simulated by macroscale hydrological models so as to account for the presence of engineered water storages turner et al 2017 hoang et al 2019 an alternative and perhaps more suitable modelling approach stands in the direct representation of water reservoirs within hydrological models an option that has been embraced only recently by the land surface modelling community nazemi and wheater 2015a b at this stage there is indeed only a handful of hydrological and land surface models relying on such approach e g matsiro pokhrel et al 2012 dhsvm zhao et al 2016 leaf hydro flood shin et al 2019 and mesh yassin et al 2019 all these models are capable of simulating the mass balance and release of reservoirs but cannot optimize their operations a missing feature that limits the scope of hydrological modelling studies for example these models could be used to evaluate the effect of climate change on the hydropower production of a river basin zhou et al 2018 but not to optimize the system performance for different hydrological scenarios or evaluate the impact of operating rules aimed at reducing the environmental impacts of dams here we attempt to bridge the gap between the optimization of reservoir system operations and large scale hydrological modelling by presenting vic resopt a software package for the representation and optimization of water storages in the variable infiltration capacity vic model a large scale semi distributed hydrologic model that simulates the water and energy balances at the land surface liang et al 1994 together with the flow routing model introduced by lohmann et al 1996 1998 vic has been adopted in hundreds of studies at the catchment regional and global scale see melsen et al 2016 bohn and vivoni 2019 and references therein in our work vic resopt builds on a variant of lohmann s routing model that accounts for the storage dynamics of water reservoirs dang et al 2020 and enhances it with a number of functionalities borrowed from the field of reservoir optimization among these we note the capability of 1 simulating different operating approaches i e rule curves operating rules and pre defined release time series 2 optimizing reservoir operations and 3 exploring the sensitivity of vic s output with respect to the parameterization of the operating approach the package comes with visual analytics to help users explore how different management solutions affect the hydrological and hydraulic processes of a river basin 2 background on vic vic organizes its spatial domain into a number of computational cells and for each one simulates the water and energy fluxes governing the terrestrial water cycle liang et al 1994 specifically vic divides each cell into one vegetation and two or three soil layers for which it calculates evapotranspiration infiltration runoff and baseflow at a daily or sub daily time step water can only enter a cell through the atmosphere meaning that non channel flow between grid cells is not modelled in addition the model assumes that water reaching the channel network cannot flow back into the soil ibidem the main input data are climate forcings that is precipitation temperature and wind additional data required by vic are land use and soil texture maps vegetation phenology variables i e leaf area index and albedo and a digital elevation model dem the routing process is performed separately from the land surface simulation using the routing model of lohmann et al 1996 1998 the model gathers the gridded surface runoff and baseflow to simulate discharge throughout the river network routing is based on a flow direction matrix created from the dem and a linearized version of the saint venant equations the model first creates the impulse response functions irfs for each grid cell and then simulates the flow convolution by aggregating the flow contribution from all upstream cells at each time step lagged according to the irfs in addition to the flow direction matrix the model requires data on the flow velocity and diffusion station location to export the output data and grid unit hydrograph over the course of the years vic has undergone key modifications coordinated by the uw hydro computational hydrology group at the university of washington the latest available version is vic 5 hamman et al 2018 this version does not add or modify the representation of physical processes compared to vic 4 2 d the last release of the vic version 4 development track instead the development of vic 5 focussed on other aspects such as a reconfiguration of the legacy vic source code the use of netcdf files and the development of a dedicated github repository a similar comment applies to the routing model the most recent version is rvic hamman et al 2017 niemeyer et al 2018 which differs from the original lohmann s routing model in the development of the irfs and the adoption of various infrastructure improvements another difference between rvic and the original model stands in the adoption of python instead of fortran 3 vic resopt software characteristics and capabilities the development of vic resopt is based on vic 4 2 d and the original routing model which are still very popular in the scientific community in particular vic resopt builds on an extensive modification of the fortran code of lohmann s routing model fig 1 here we first describe the solution adopted to represent water reservoirs and their operations in the routing model sections 3 1 and 3 2 and then proceed to describe the main functionalities of the package namely optimization of reservoir operations section 3 3 sensitivity analysis automatic calibration and output visualization section 3 4 3 1 representation of water reservoirs a key factor concerning the representation of water reservoirs is the location of dams and upstream impoundments to store this information vic resopt leverages the organization of the modelling domain into a number of computational cells specifically vic resopt uses a two dimensional array reservoirlocation txt that has the same number of rows and columns of the flow direction matrix as illustrated in fig 2 each cell of the reservoir location matrix can take an integer value belonging to the set 0 9999 values ranging from 1 to 9998 are used to denote cells containing a dam along with the corresponding dam id while the value 9999 denotes cells belonging to upstream impoundments 0 refers to other land uses naturally such representation of dams and upstream impoundments requires a high spatial resolution say below 0 1 degrees so as to avoid allocating multiple dams within one cell the specifications of each reservoir are contained in dedicated files whose name depends on the reservoir id for example res1 txt for the reservoir with id 1 the specifications include both design aspects e g critical water level and parameters characterizing the reservoir operations e g infiltration rate operating approach the complete list of these specifications is provided in table 1 for each reservoir vic resopt aggregates the mass balance computed daily in the dam cell the balance depends on five processes namely inflow seepage infiltration evaporation and release the reader is referred to the supplementary material for a complete mathematical description of the reservoir mass balance to calculate inflow vic resopt determines which cells contribute to a reservoir by processing the information contained in the flow direction matrix this information is stored in a series of uh files e g res1 uh for the reservoir with id 1 water seepage and infiltration are modelled through user determined rates see table 1 the volume of water leaving the reservoir through seepage via dam body foundation or abutment contributes to the downstream discharge while the water that infiltrates into the underground aquifers is considered lost evaporation losses from the reservoir surface are modelled with the penman equation for all cells belonging to the impoundment the release from the reservoir can be modelled and optimized through a number of operating approaches the total volume of water leaving the reservoir through seepage controlled discharge and spillways is finally routed to the immediate downstream cell from where it can flow to other reservoirs the routing process thus represents both single and multi reservoir systems and is implemented in the file reservoirs f in addition to the output files provided by vic vic resopt generates output files containing the daily operational data of each reservoir such as inflow storage and release e g reservoir 1 day 3 2 reservoir operations release decisions from water reservoirs are typically supported by either rule curves or operating rules soncini sessa et al 2007 both operating approaches have the ultimate goal of recommending release decisions on a periodic basis but they achieve such decisions differently as illustrated in fig 3 a b a rule curve is represented by the target water level namely the ideal level the reservoir should reach in each day of a typical year as explained next the release is then determined on the basis of the actual and target levels an operating rule is a function mapping the actual level or storage into a release decision fig 3 c in vic resopt the parameters characterizing the rule curve or operating rule of each reservoir are provided in the file containing the reservoir specifications see section 3 1 in addition users have the option to replace either of the two approaches with a pre defined release time series fig 3 d namely the volume of water released daily over a given simulation horizon overall the available operating approaches and corresponding parameters can be summarized as follows in its simplest configuration a rule curve depends on four parameters namely the minimum and maximum water levels that a reservoir should reach within a year h 1 and h 2 and the time at which the two levels should be reached t 1 and t 2 see fig 3 a note that h 1 and h 2 cannot exceed the dead and critical water levels h min and h max this rule curve represents a scenario in which the drawdown and refill take place subsequently to include scenarios in which the target water level should remain constant over prolonged periods of time vic resopt includes a more complex rule curve that depends on twelve parameters i e the monthly target water levels h 1 h 12 fig 3 b in both configurations the rule curves divide the storage into four zones each associated to different release decisions if the level falls in zone 1 below the dead water level water is not released if the level is in zone 2 between dead and target levels vic resopt uses information on the incoming daily inflow to calculate whether the water level is expected to exceed the target one by the end of the day if that is the case vic resopt discharges the amount of water needed to keep the actual level as close as possible to the target otherwise water is not discharged in zone 3 between the target and critical levels water is released using the dam design discharge so as to quickly reach the target level when the level reaches zone 4 beyond the critical level spillways are activated so as to avoid overtopping an operating rule is represented through a piecewise linear function which depends on the parameters x1 x2 x3 and x4 the value of these parameters denotes three stages namely hedging normal and flood operating conditions hedging is commonly adopted when limited resources are available in such case operators prefer to reduce supply and retain water for potential future use draper and lund 2004 hedging is abandoned when more resources are available and operators can meet the downstream demand for hydropower irrigation or other purposes the third stage is active during flood conditions when a larger amount of water may need to be discharged a risk adverse operator for example could implement an aggressive hedging strategy by increasing the value of x2 thereby meeting the downstream demand only when a large fraction of the storage is available note that one could adopt time varying or periodic parameters so as adapt the operating rules to the inflow conditions or the downstream demand which can vary in the course of a year this option implemented in vic resopt provides more flexibility but comes at the cost of greater modelling complexity the adoption of monthly rule curves for example requires the user to specify the value of 48 instead of 4 parameters the use of a pre defined release time series is particularly useful to evaluate the effect of a given set of release decisions such as a release trajectory that has been observed in the past to implement it users must supply the discharge data and corresponding dates in a specific file e g releasedata1 txt for the reservoir with id 1 note that the effective implementation of the release time series is constrained automatically by vic resopt for example if the pre defined release is unfeasible in a certain day vic resopt releases only the water available in storage additional details about the mathematical formulation of rulecurves operating rules and pre defined release time series are provided in the supplementary material 3 3 optimization of reservoir operations users can optimize the value of the parameters characterizing rule curves i e h 1 h 2 t 1 t 2 or h 1 h 12 and operating rules i e x1 x2 x3 and x 4 with the aid of platypus https github com project platypus platypus a framework for evolutionary computing in python supporting a variety of multi objective evolutionary algorithms moeas in other words our package adopts a simulation optimization approach in which the rule curves and or operating rules are designed by a moea on the basis of the performance attained via simulation over a given scenario this approach is also known as evolutionary multi objective direct policy search giuliani et al 2016a mathematically this means solving the following multi objective optimization problem 1 θ arg min θ j θ where θ represents the decision variables i e the parameters characterizing rule curves or operating rules and j a vector of objective functions evaluated via simulation with vic resopt to ease the modelling exercise vic resopt includes a number of pre defined objective functions representing a broad number of goals typically encountered in reservoir operations table 2 note that the number of parameters to be optimized depends on the problem at hand for example determining the operation of a four reservoir system controlled with simple rule curves requires optimizing sixteen parameters since eq 1 represents a multi objective problem its solution θ is not a single optimal parameterization but rather a set of pareto efficient parameters to setup an optimization problem users need to 1 choose the reservoirs for which the operations must be optimized 2 set the moea s parameters e g number of function evaluations 3 determine the length of simulation and spinning periods and 4 choose the objective functions the value of the objectives to be maximized is automatically multiplied by 1 during the optimization process so as to guarantee the correct verse of optimization all these options are set by editing the file reservoiroptimization txt the optimization is then run through the script optimization py note that the simulation optimization requires to couple only the routing model with a moea a solution that reduces the computational requirements the results of the optimization process are stored in the files optimization objectives txt and optimization variables txt 3 4 other functionalities vic resopt provides three additional functionalities sensitivity analysis to determine the extent to which the parameterization of some rule curves or operating rules affects the model output we coupled vic resopt with the safe sensitivity analysis for everybody toolbox pianosi et al 2015 our package allows users to run two algorithms namely the elementary effect test eet and extended fourier amplitude sensitivity test efast which differ in intended use complexity and computational requirements the former can be seen as a screening tool to discard non influential variables ibid while the latter is a global sensitivity analysis algorithm that accounts for the direct effect of each parameter on the model output as well as its interactions with the other parameters saltelli et al 1999 similarly to the reservoir optimization users need to setup the sensitivity analysis exercise through a number of dedicated files eetparameters txt and reservoireet txt or efastparameters txt and reservoirefast txt note that the analysis is not necessarily limited to the parameters of the operating rules users can also explore the sensitivity of the model output to the parameters of the rainfall runoff and routing models the sensitivity analysis is carried out by either eet analysis py or efast analysis py the former creates the files minse txt and mitrmse txt the latter generates the files sinse txt and sitrmse txt automatic calibration the rainfall runoff and routing models include a broad number of parameters whose values can be set manually or by means of automatic routines wi et al 2017 to support the second option we implemented an automatic calibration routine that leverages the soft link between vic resopt and platypus in this case the decision variables are not the parameters of the rule curves or operating rules but rather the user selected parameters of the rainfall runoff and routing models e g thickness of the soil layers as for the objective functions users can choose among the nash sutcliffe efficiency nse box cox transformed root mean squared error trmse runoff coefficient error roce and mean squared derivative error msde which capture different aspects of a model accuracy dawson et al 2010 reed et al 2013 for this functionality users must edit the configuration file reservoircalibration txt and provide the observed discharge time series used to calculate the various measures of performance the time series is contained in the file observeddischarge csv note that our package supports only single site calibration all operations are finally executed by the script autocalibration py the results are stored in the files calibration objectives txt and calibration variables txt visualization the package is equipped with several functions to visualize the output of vic resopt to ease their use we developed one script for each main kind of analysis that is optimization of reservoir operations visual optimization py sensitivity analysis visual eet py and visual efast py and automatic calibration visual calibration py 4 example application 4 1 study site vic resopt functionalities are demonstrated on the upper portion of the chao phraya river basin thailand the basin upstream of nakon sawan gauging station has a drainage area of 110 000 km 2 about 20 of thailand s surface area fig 4 the river flow is heavily regulated by bhumibol and sirikit dams which create two large impoundments with a live capacity of 9762 and 6668 mm 3 their role is to support irrigation in the downstream reaches produce hydropower and protect bangkok metropolitan area from floods komori et al 2012 the majority of the annual rainfall is concentrated in the monsoon season whose intensity is largely affected by global climate phenomena such as the el niño southern oscillation nguyen and galelli 2018 to setup vic resopt we adopted a spatial resolution of 1 16th of a degree roughly seven km at this latitude daily rainfall data were taken from aphrodite asian precipitation highly resolved observational data integration towards evaluation a gridded dataset built on a dense network of rain gauge data for asia and south east asia yatagai et al 2012 maximum and minimum temperature data were derived from the climate forecast system reanalysis dataset saha et al 2014 while information on land use and soil was retrieved from the global land cover characterization https www usgs gov centers eros science and harmonized world soil databases http www fao org soils portal soil survey soil maps and databases harmonized world soil database v12 en the flow direction map was generated by processing the data contained in the global 30 arc second elevation dem https rda ucar edu datasets ds758 0 the specifications of bhumibol and sirikit dams were retrieved from the global reservoir and dam database http globaldamwatch org grand 4 2 methods and results in this specific example we want to find rule curves that strike a reasonable balance between irrigation water supply annual hydropower production and flood protection i e objectives 1 2 and 4 table 2 we adopt rule curves characterized by four parameters each see fig 3 a meaning that the vector θ contains eight parameters to optimize we set the number of individuals and function evaluations equal to 20 and 500 and adopt a 5 year simulation horizon 2009 2013 these tasks are executed by appropriately editing the reservoiroptimization txt file and running the following command in the script optimization py all data and step by step instructions on how to run the model are provided in the online repository the output of the optimization process is a set of pareto efficient rule curves whose performance is illustrated in fig 5 a note the strong conflict between hydropower production and irrigation increasing the hydropower production leads to a considerable increase in the water deficit these two objectives are also in conflict with flood protection rule curves that minimize the flood peak lead to large water deficits and limited power production to select two balanced rule curves one for each reservoir we use the utopia point criterion according to which the selected rule curves are the ones nearest to the utopia point in terms of euclidean distance in their respective set this is a common selection criterion in multi objective optimization soncini sessa et al 2007 the performance of these rule curves is further tested via simulation on the hydro climatological conditions observed in 2008 and 2011 two years characterized by contrasting dry and wet conditions as illustrated in fig 5 b the optimized rule curves yield a substantial reduction of the flood peak at nakon sawan gauging station c 2 with respect to the existing management system this is achieved through the coordination of storage operations as shown in fig 5 c the optimization process de synchronizes the operations of bhumibol and sirikit by setting different days in which the minimum and maximum water levels must be reached by doing so we can effectively explore the massive storage capacity of both reservoirs overall examples like this one demonstrate the key role played by management decisions in controlling the performance of hydraulic infrastructures 5 outlook vic resopt is specifically conceived to support the broad vic modelling community with a tool for regional and global studies involving the operations of water reservoirs vic resopt requires minimal user effort to represent reservoirs within lohmann s routing model and to analyse and optimize their operations we expect that this feature will encourage external contributions and user feedbacks on the basis of which we will further improve the package some of the imminent updates may include 1 nonlinear operating rules which make the release functions more flexible giuliani et al 2016a 2 impoundment or filling strategies zhang et al 2016 and 3 capability of simulating the installation of new dams over a given simulation horizon finally a major update to the package would be its porting to python so as to make it compatible with vic 5 and rvic declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research is supported by singapore s ministry of education moe through the tier 2 project linking water availability to hydropower supply an engineering systems approach award no moe2017 t2 1 143 we thank the three reviewers for the positive and constructive comments that helped us improve both manuscript and software package appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2020 104673 appendix a supplementary data the following is the supplementary material related to this article mmc s1 
26060,macroscale hydrological and land surface models increasingly rely on novel streamflow routing algorithms that explicitly account for the presence of engineered water storages which largely affect river basin dynamics in this short communication we contribute to this growing field by presenting vic resopt a software package for the representation and optimization of water reservoirs in the routing model commonly used as a post processor with the variable infiltration capacity vic hydrologic model vic resopt comprises tools for simulating different reservoir operating approaches optimizing operations and exploring the sensitivity of the model output with respect to the parameterization of the operating rules the package relies on automated scripts and optimization engines thereby requiring minimal user effort we demonstrate some of its functionalities in the upper portion of the chao phraya river basin thailand which is regulated by two large dams keywords vic hydrologic model optimal reservoir operations water resources management global and regional studies earth system models software availability name of software vic resopt developer thanh duc dang contact email thanhiwer gmail com year first available 2019 programming languages fortran python dependencies platypus a framework for evolutionary computing in python safe toolbox sensitivity analysis for everybody tested platforms linux ubuntu 14 04 lts trusty tahr and 16 04 xenial xerus available from https github com thanhiwer vicresopt 1 introduction water reservoir operations is a pivotal component of integrated water resources management properly functioning reservoir systems can meet multiple often conflicting objectives such as protecting urban areas from floods supporting agriculture or producing hydropower since these benefits largely depend on the intelligence used to support management decisions researchers have developed a broad spectrum of optimization approaches and algorithms that can effectively aid water managers dobson et al 2019 over the years these algorithms have been implemented in decision support systems that allow users to solve multiple management problems for example hec resprm hydrologic engineering center prescriptive reservoir model program and modsim dss a generic river basin management decision support system use network flow optimization to determine water release strategies for a given set of objectives and inflow time series usace 2003 labadie 2006 riverware adopts a preemptive optimization approach to meet a given set of objectives in order of priority zagona et al 2001 other popular decision support systems powered by optimization algorithms are twole soncini sessa et al 2003 weap yates et al 2005 and mike hydro basin dhi 2017 researchers can also apply the most recent advances in reservoir systems optimization through r and matlab packages such as reservoir turner and galelli 2016 and m3o giuliani et al 2016b importantly reservoir operations has long transcended the domain of integrated water resources management becoming an essential component of regional and global studies on the impact of climate change on water and energy resources in these studies the aforementioned software is often used to post process the streamflow or runoff data simulated by macroscale hydrological models so as to account for the presence of engineered water storages turner et al 2017 hoang et al 2019 an alternative and perhaps more suitable modelling approach stands in the direct representation of water reservoirs within hydrological models an option that has been embraced only recently by the land surface modelling community nazemi and wheater 2015a b at this stage there is indeed only a handful of hydrological and land surface models relying on such approach e g matsiro pokhrel et al 2012 dhsvm zhao et al 2016 leaf hydro flood shin et al 2019 and mesh yassin et al 2019 all these models are capable of simulating the mass balance and release of reservoirs but cannot optimize their operations a missing feature that limits the scope of hydrological modelling studies for example these models could be used to evaluate the effect of climate change on the hydropower production of a river basin zhou et al 2018 but not to optimize the system performance for different hydrological scenarios or evaluate the impact of operating rules aimed at reducing the environmental impacts of dams here we attempt to bridge the gap between the optimization of reservoir system operations and large scale hydrological modelling by presenting vic resopt a software package for the representation and optimization of water storages in the variable infiltration capacity vic model a large scale semi distributed hydrologic model that simulates the water and energy balances at the land surface liang et al 1994 together with the flow routing model introduced by lohmann et al 1996 1998 vic has been adopted in hundreds of studies at the catchment regional and global scale see melsen et al 2016 bohn and vivoni 2019 and references therein in our work vic resopt builds on a variant of lohmann s routing model that accounts for the storage dynamics of water reservoirs dang et al 2020 and enhances it with a number of functionalities borrowed from the field of reservoir optimization among these we note the capability of 1 simulating different operating approaches i e rule curves operating rules and pre defined release time series 2 optimizing reservoir operations and 3 exploring the sensitivity of vic s output with respect to the parameterization of the operating approach the package comes with visual analytics to help users explore how different management solutions affect the hydrological and hydraulic processes of a river basin 2 background on vic vic organizes its spatial domain into a number of computational cells and for each one simulates the water and energy fluxes governing the terrestrial water cycle liang et al 1994 specifically vic divides each cell into one vegetation and two or three soil layers for which it calculates evapotranspiration infiltration runoff and baseflow at a daily or sub daily time step water can only enter a cell through the atmosphere meaning that non channel flow between grid cells is not modelled in addition the model assumes that water reaching the channel network cannot flow back into the soil ibidem the main input data are climate forcings that is precipitation temperature and wind additional data required by vic are land use and soil texture maps vegetation phenology variables i e leaf area index and albedo and a digital elevation model dem the routing process is performed separately from the land surface simulation using the routing model of lohmann et al 1996 1998 the model gathers the gridded surface runoff and baseflow to simulate discharge throughout the river network routing is based on a flow direction matrix created from the dem and a linearized version of the saint venant equations the model first creates the impulse response functions irfs for each grid cell and then simulates the flow convolution by aggregating the flow contribution from all upstream cells at each time step lagged according to the irfs in addition to the flow direction matrix the model requires data on the flow velocity and diffusion station location to export the output data and grid unit hydrograph over the course of the years vic has undergone key modifications coordinated by the uw hydro computational hydrology group at the university of washington the latest available version is vic 5 hamman et al 2018 this version does not add or modify the representation of physical processes compared to vic 4 2 d the last release of the vic version 4 development track instead the development of vic 5 focussed on other aspects such as a reconfiguration of the legacy vic source code the use of netcdf files and the development of a dedicated github repository a similar comment applies to the routing model the most recent version is rvic hamman et al 2017 niemeyer et al 2018 which differs from the original lohmann s routing model in the development of the irfs and the adoption of various infrastructure improvements another difference between rvic and the original model stands in the adoption of python instead of fortran 3 vic resopt software characteristics and capabilities the development of vic resopt is based on vic 4 2 d and the original routing model which are still very popular in the scientific community in particular vic resopt builds on an extensive modification of the fortran code of lohmann s routing model fig 1 here we first describe the solution adopted to represent water reservoirs and their operations in the routing model sections 3 1 and 3 2 and then proceed to describe the main functionalities of the package namely optimization of reservoir operations section 3 3 sensitivity analysis automatic calibration and output visualization section 3 4 3 1 representation of water reservoirs a key factor concerning the representation of water reservoirs is the location of dams and upstream impoundments to store this information vic resopt leverages the organization of the modelling domain into a number of computational cells specifically vic resopt uses a two dimensional array reservoirlocation txt that has the same number of rows and columns of the flow direction matrix as illustrated in fig 2 each cell of the reservoir location matrix can take an integer value belonging to the set 0 9999 values ranging from 1 to 9998 are used to denote cells containing a dam along with the corresponding dam id while the value 9999 denotes cells belonging to upstream impoundments 0 refers to other land uses naturally such representation of dams and upstream impoundments requires a high spatial resolution say below 0 1 degrees so as to avoid allocating multiple dams within one cell the specifications of each reservoir are contained in dedicated files whose name depends on the reservoir id for example res1 txt for the reservoir with id 1 the specifications include both design aspects e g critical water level and parameters characterizing the reservoir operations e g infiltration rate operating approach the complete list of these specifications is provided in table 1 for each reservoir vic resopt aggregates the mass balance computed daily in the dam cell the balance depends on five processes namely inflow seepage infiltration evaporation and release the reader is referred to the supplementary material for a complete mathematical description of the reservoir mass balance to calculate inflow vic resopt determines which cells contribute to a reservoir by processing the information contained in the flow direction matrix this information is stored in a series of uh files e g res1 uh for the reservoir with id 1 water seepage and infiltration are modelled through user determined rates see table 1 the volume of water leaving the reservoir through seepage via dam body foundation or abutment contributes to the downstream discharge while the water that infiltrates into the underground aquifers is considered lost evaporation losses from the reservoir surface are modelled with the penman equation for all cells belonging to the impoundment the release from the reservoir can be modelled and optimized through a number of operating approaches the total volume of water leaving the reservoir through seepage controlled discharge and spillways is finally routed to the immediate downstream cell from where it can flow to other reservoirs the routing process thus represents both single and multi reservoir systems and is implemented in the file reservoirs f in addition to the output files provided by vic vic resopt generates output files containing the daily operational data of each reservoir such as inflow storage and release e g reservoir 1 day 3 2 reservoir operations release decisions from water reservoirs are typically supported by either rule curves or operating rules soncini sessa et al 2007 both operating approaches have the ultimate goal of recommending release decisions on a periodic basis but they achieve such decisions differently as illustrated in fig 3 a b a rule curve is represented by the target water level namely the ideal level the reservoir should reach in each day of a typical year as explained next the release is then determined on the basis of the actual and target levels an operating rule is a function mapping the actual level or storage into a release decision fig 3 c in vic resopt the parameters characterizing the rule curve or operating rule of each reservoir are provided in the file containing the reservoir specifications see section 3 1 in addition users have the option to replace either of the two approaches with a pre defined release time series fig 3 d namely the volume of water released daily over a given simulation horizon overall the available operating approaches and corresponding parameters can be summarized as follows in its simplest configuration a rule curve depends on four parameters namely the minimum and maximum water levels that a reservoir should reach within a year h 1 and h 2 and the time at which the two levels should be reached t 1 and t 2 see fig 3 a note that h 1 and h 2 cannot exceed the dead and critical water levels h min and h max this rule curve represents a scenario in which the drawdown and refill take place subsequently to include scenarios in which the target water level should remain constant over prolonged periods of time vic resopt includes a more complex rule curve that depends on twelve parameters i e the monthly target water levels h 1 h 12 fig 3 b in both configurations the rule curves divide the storage into four zones each associated to different release decisions if the level falls in zone 1 below the dead water level water is not released if the level is in zone 2 between dead and target levels vic resopt uses information on the incoming daily inflow to calculate whether the water level is expected to exceed the target one by the end of the day if that is the case vic resopt discharges the amount of water needed to keep the actual level as close as possible to the target otherwise water is not discharged in zone 3 between the target and critical levels water is released using the dam design discharge so as to quickly reach the target level when the level reaches zone 4 beyond the critical level spillways are activated so as to avoid overtopping an operating rule is represented through a piecewise linear function which depends on the parameters x1 x2 x3 and x4 the value of these parameters denotes three stages namely hedging normal and flood operating conditions hedging is commonly adopted when limited resources are available in such case operators prefer to reduce supply and retain water for potential future use draper and lund 2004 hedging is abandoned when more resources are available and operators can meet the downstream demand for hydropower irrigation or other purposes the third stage is active during flood conditions when a larger amount of water may need to be discharged a risk adverse operator for example could implement an aggressive hedging strategy by increasing the value of x2 thereby meeting the downstream demand only when a large fraction of the storage is available note that one could adopt time varying or periodic parameters so as adapt the operating rules to the inflow conditions or the downstream demand which can vary in the course of a year this option implemented in vic resopt provides more flexibility but comes at the cost of greater modelling complexity the adoption of monthly rule curves for example requires the user to specify the value of 48 instead of 4 parameters the use of a pre defined release time series is particularly useful to evaluate the effect of a given set of release decisions such as a release trajectory that has been observed in the past to implement it users must supply the discharge data and corresponding dates in a specific file e g releasedata1 txt for the reservoir with id 1 note that the effective implementation of the release time series is constrained automatically by vic resopt for example if the pre defined release is unfeasible in a certain day vic resopt releases only the water available in storage additional details about the mathematical formulation of rulecurves operating rules and pre defined release time series are provided in the supplementary material 3 3 optimization of reservoir operations users can optimize the value of the parameters characterizing rule curves i e h 1 h 2 t 1 t 2 or h 1 h 12 and operating rules i e x1 x2 x3 and x 4 with the aid of platypus https github com project platypus platypus a framework for evolutionary computing in python supporting a variety of multi objective evolutionary algorithms moeas in other words our package adopts a simulation optimization approach in which the rule curves and or operating rules are designed by a moea on the basis of the performance attained via simulation over a given scenario this approach is also known as evolutionary multi objective direct policy search giuliani et al 2016a mathematically this means solving the following multi objective optimization problem 1 θ arg min θ j θ where θ represents the decision variables i e the parameters characterizing rule curves or operating rules and j a vector of objective functions evaluated via simulation with vic resopt to ease the modelling exercise vic resopt includes a number of pre defined objective functions representing a broad number of goals typically encountered in reservoir operations table 2 note that the number of parameters to be optimized depends on the problem at hand for example determining the operation of a four reservoir system controlled with simple rule curves requires optimizing sixteen parameters since eq 1 represents a multi objective problem its solution θ is not a single optimal parameterization but rather a set of pareto efficient parameters to setup an optimization problem users need to 1 choose the reservoirs for which the operations must be optimized 2 set the moea s parameters e g number of function evaluations 3 determine the length of simulation and spinning periods and 4 choose the objective functions the value of the objectives to be maximized is automatically multiplied by 1 during the optimization process so as to guarantee the correct verse of optimization all these options are set by editing the file reservoiroptimization txt the optimization is then run through the script optimization py note that the simulation optimization requires to couple only the routing model with a moea a solution that reduces the computational requirements the results of the optimization process are stored in the files optimization objectives txt and optimization variables txt 3 4 other functionalities vic resopt provides three additional functionalities sensitivity analysis to determine the extent to which the parameterization of some rule curves or operating rules affects the model output we coupled vic resopt with the safe sensitivity analysis for everybody toolbox pianosi et al 2015 our package allows users to run two algorithms namely the elementary effect test eet and extended fourier amplitude sensitivity test efast which differ in intended use complexity and computational requirements the former can be seen as a screening tool to discard non influential variables ibid while the latter is a global sensitivity analysis algorithm that accounts for the direct effect of each parameter on the model output as well as its interactions with the other parameters saltelli et al 1999 similarly to the reservoir optimization users need to setup the sensitivity analysis exercise through a number of dedicated files eetparameters txt and reservoireet txt or efastparameters txt and reservoirefast txt note that the analysis is not necessarily limited to the parameters of the operating rules users can also explore the sensitivity of the model output to the parameters of the rainfall runoff and routing models the sensitivity analysis is carried out by either eet analysis py or efast analysis py the former creates the files minse txt and mitrmse txt the latter generates the files sinse txt and sitrmse txt automatic calibration the rainfall runoff and routing models include a broad number of parameters whose values can be set manually or by means of automatic routines wi et al 2017 to support the second option we implemented an automatic calibration routine that leverages the soft link between vic resopt and platypus in this case the decision variables are not the parameters of the rule curves or operating rules but rather the user selected parameters of the rainfall runoff and routing models e g thickness of the soil layers as for the objective functions users can choose among the nash sutcliffe efficiency nse box cox transformed root mean squared error trmse runoff coefficient error roce and mean squared derivative error msde which capture different aspects of a model accuracy dawson et al 2010 reed et al 2013 for this functionality users must edit the configuration file reservoircalibration txt and provide the observed discharge time series used to calculate the various measures of performance the time series is contained in the file observeddischarge csv note that our package supports only single site calibration all operations are finally executed by the script autocalibration py the results are stored in the files calibration objectives txt and calibration variables txt visualization the package is equipped with several functions to visualize the output of vic resopt to ease their use we developed one script for each main kind of analysis that is optimization of reservoir operations visual optimization py sensitivity analysis visual eet py and visual efast py and automatic calibration visual calibration py 4 example application 4 1 study site vic resopt functionalities are demonstrated on the upper portion of the chao phraya river basin thailand the basin upstream of nakon sawan gauging station has a drainage area of 110 000 km 2 about 20 of thailand s surface area fig 4 the river flow is heavily regulated by bhumibol and sirikit dams which create two large impoundments with a live capacity of 9762 and 6668 mm 3 their role is to support irrigation in the downstream reaches produce hydropower and protect bangkok metropolitan area from floods komori et al 2012 the majority of the annual rainfall is concentrated in the monsoon season whose intensity is largely affected by global climate phenomena such as the el niño southern oscillation nguyen and galelli 2018 to setup vic resopt we adopted a spatial resolution of 1 16th of a degree roughly seven km at this latitude daily rainfall data were taken from aphrodite asian precipitation highly resolved observational data integration towards evaluation a gridded dataset built on a dense network of rain gauge data for asia and south east asia yatagai et al 2012 maximum and minimum temperature data were derived from the climate forecast system reanalysis dataset saha et al 2014 while information on land use and soil was retrieved from the global land cover characterization https www usgs gov centers eros science and harmonized world soil databases http www fao org soils portal soil survey soil maps and databases harmonized world soil database v12 en the flow direction map was generated by processing the data contained in the global 30 arc second elevation dem https rda ucar edu datasets ds758 0 the specifications of bhumibol and sirikit dams were retrieved from the global reservoir and dam database http globaldamwatch org grand 4 2 methods and results in this specific example we want to find rule curves that strike a reasonable balance between irrigation water supply annual hydropower production and flood protection i e objectives 1 2 and 4 table 2 we adopt rule curves characterized by four parameters each see fig 3 a meaning that the vector θ contains eight parameters to optimize we set the number of individuals and function evaluations equal to 20 and 500 and adopt a 5 year simulation horizon 2009 2013 these tasks are executed by appropriately editing the reservoiroptimization txt file and running the following command in the script optimization py all data and step by step instructions on how to run the model are provided in the online repository the output of the optimization process is a set of pareto efficient rule curves whose performance is illustrated in fig 5 a note the strong conflict between hydropower production and irrigation increasing the hydropower production leads to a considerable increase in the water deficit these two objectives are also in conflict with flood protection rule curves that minimize the flood peak lead to large water deficits and limited power production to select two balanced rule curves one for each reservoir we use the utopia point criterion according to which the selected rule curves are the ones nearest to the utopia point in terms of euclidean distance in their respective set this is a common selection criterion in multi objective optimization soncini sessa et al 2007 the performance of these rule curves is further tested via simulation on the hydro climatological conditions observed in 2008 and 2011 two years characterized by contrasting dry and wet conditions as illustrated in fig 5 b the optimized rule curves yield a substantial reduction of the flood peak at nakon sawan gauging station c 2 with respect to the existing management system this is achieved through the coordination of storage operations as shown in fig 5 c the optimization process de synchronizes the operations of bhumibol and sirikit by setting different days in which the minimum and maximum water levels must be reached by doing so we can effectively explore the massive storage capacity of both reservoirs overall examples like this one demonstrate the key role played by management decisions in controlling the performance of hydraulic infrastructures 5 outlook vic resopt is specifically conceived to support the broad vic modelling community with a tool for regional and global studies involving the operations of water reservoirs vic resopt requires minimal user effort to represent reservoirs within lohmann s routing model and to analyse and optimize their operations we expect that this feature will encourage external contributions and user feedbacks on the basis of which we will further improve the package some of the imminent updates may include 1 nonlinear operating rules which make the release functions more flexible giuliani et al 2016a 2 impoundment or filling strategies zhang et al 2016 and 3 capability of simulating the installation of new dams over a given simulation horizon finally a major update to the package would be its porting to python so as to make it compatible with vic 5 and rvic declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research is supported by singapore s ministry of education moe through the tier 2 project linking water availability to hydropower supply an engineering systems approach award no moe2017 t2 1 143 we thank the three reviewers for the positive and constructive comments that helped us improve both manuscript and software package appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2020 104673 appendix a supplementary data the following is the supplementary material related to this article mmc s1 
26061,streamflow forecasts are essential for water resources management although there are many methods for forecasting streamflow real time forecasts remain challenging this study evaluates streamflow forecasts using a process based model soil and water assessment tool variable source area model swat vsa a stochastic model artificial neural network ann an auto regressive moving average arma model and a bayesian ensemble model that utilizes the swat vsa ann and arma results streamflow is forecast from 1 to 8 d forced with quantitative precipitation forecasts from the us national weather service of the individual models swat vsa and the ann provide better predictions of total streamflow nse 0 60 0 70 and peak flow but underpredicted low flows during the forecast period the ann had the highest predictive power nse 0 44 0 64 however all three models underpredicted peak flow the bayesian ensemble forecast streamflow with the most skill for all forecast lead times nse 0 49 0 67 and provided a quantification of prediction uncertainty keywords swat vsa anns arma forecasting stochastic model process based model bayesian model 1 introduction streamflow forecasting is essential for hydrologists water resource planners water system managers emergency response providers and policymakers to respond to ever increasing water demand and greater variability however since streamflow is largely driven by stochastic processes rainfall temperature seasonal variability and complex nonlinear watershed responses shao et al 2009 londhe and charhate 2010 it is challenging to precisely forecast streamflow particularly as forecast lead times increase challenges to accurate streamflow forecasting as outlined by pagano et al 2014 include automating real time data assimilation for model forcing that rainfall runoff models are simplifications of real world processes and that precipitation forecasts are inherently uncertain there are two broad rainfall runoff modeling approaches used to forecast streamflow physical process based models and empirical statistical models masselot et al 2016 process based models attempt to incorporate the relevant physical laws controlling watershed response and streamflow generation generally requiring substantial effort to build parameterize and calibrate and often utilize an extensive amount of observed data however process based models can provide the more reliable streamflow forecast compared to other forecasting techniques zealand et al 1999 for instance demirel et al 2009 compared the performance of a process based model the soil and water assessment tool swat arnold et al 1998 and an empirical artificial neural network ann model in forecasting streamflow and found that while anns were better at forecasting peak flow swat performed better overall similarly hapuarachchi et al 2011 found that process based models provide more credible results in predicting flash flooding than empirical models while process based models can often forecast streamflow with adequate skill there can also be tremendous uncertainty in the model predictions because of the uncertainty in input data model structure and the numerical techniques employed unfortunately this uncertainty is often not included quantitatively in deterministic forecasts but is often incorporated into the final forecast product qualitatively using post processing tools and forecaster experience empirical models attempt to reproduce the relationships between inputs e g precipitation and outputs streamflow with no assumption of internal process understanding empirical models are often more easily developed and can provide reliable forecasts when the models are trained on robust representative data several different types of empirical models have been used to forecast streamflow including time series models such as the auto regressive moving average arma procedure babu and reddy 2014 mohammad 2015 anns khashei and bijari 2010 kalteh 2013 functional regressions masselot et al 2016 wavelet support vectors kisi and cimen 2011 or decision trees bhattacharya and solomatine 2005 time series models also known as auto regressive models can approximate complex non linear hydrologic processes by using the relationship between observed values of streamflow and precipitation shao et al 2009 auto regressive models assume that the present condition depends on the past conditions often adjusted by an error function and a seasonality function yurekli et al 2005 time series models require a large amount of observed data to train and test the model yurekli et al 2005 khashei et al 2009 and often perform poorly when applied outside of the data range for which they were developed guo et al 2011 however there have been many successful applications of auto regressive models used in hydrologic forecasting for instance toth et al 2000 showed that using an arma model for real time flood forecasting provided better predictive ability than simple rainfall runoff models emphasizing that empirical models can often outperform overly simplistic process based models collischonn et al 2007 used an arma model to forecast medium range reservoir inflow based on quantitative precipitation forecasts qpf and showed that prediction errors of an arma model could be reduced using improved rainfall forecasts another type of empirical model commonly used to forecast streamflow is an ann anns are non linear data driven self adaptive type models that can learn and generalize trends based on functional relationships in the data coulibaly et al 2000 zhang et al 2001 the ann method has been applied in many different sectors including traffic control srinivasan et al 2006 image processing kulkarni 1994 stock forecasting guresen et al 2011 handwriting analysis nasrabadi 2007 and language recognition graves et al 2013 in the water resources and environmental engineering fields anns have been employed for rainfall forecasting french et al 1992 luk et al 2001 rainfall runoff modeling shamseldin 1997 tokar and johnson 1999 reservoir operation jain et al 1999 rani and moreira 2010 and water quality forecasting palani et al 2008 singh et al 2009 although anns have been widely used in water resource fields there remain challenges including the initial effort to build the model computational power to run the model and data required to train anns additionally it can be difficult to identify important input variables parameterize the number of ann layers and identify which transfer functions best describe the system zhang et al 2001 one of the major shortcomings with anns is that they cannot work with major changes to the system they must be re trained to the new system state after major disturbances sudheer et al 2003 one other streamflow forecasting technique that is gaining popularity leverages the skill from multiple models to create an ensemble streamflow forecast one significant advantage of the multiple model ensemble mme approach is the ability to evaluate model uncertainty weigel et al 2008 cloke and pappenberger 2009 provide a review of studies employing mme approaches to exploit the diversity of skillful predictions from different models for river flood forecasting and enumerate the challenges related to the ensemble approach such as computing power resolution of input data quantification and analysis pf total uncertainty communicating uncertainty and probabilistic forecasting one such mme approach is the bayesian generalized multilevel modeling bgmm method which integrates multiple probability distributions by using prior knowledge about parameter distributions allows predictor variables to be linear or nonlinear and incorporates information from all predictors models the method also offers a more complete quantification of uncertainty rather than simply attempting to improve forecasting skill wagena et al 2019 this study compares the performance of a process based model swat vsa with a modified arma time series model an ann model and a bgmm application in a real time forecast of streamflow we applied each model in the us department of agriculture we 38 experimental watershed in east central pennsylvania us and forecast streamflow from 1 to 8 days into the future using a hindcast procedure precipitation data from the us national weather service qpf was used to force each model for each forecast day the bgmm was developed by ensembling the outputs from the swat vsa arma and ann models for each forecast day 2 materials and methods 2 1 watershed description the we 38 experimental watershed is a sub watershed of the mahantango creek watershed in east central pennsylvania which drains to the susquehanna river fig 1 the watershed has an area of 7 3 km2 and has been extensively studied as a united states department of agriculture agricultural research service usda ars experimental watershed since 1966 and contains a wealth of data to evaluate model performance the watershed is characterized by complex terrain with multiple runoff generating mechanisms several crop types and variable climate snow dominated winters humid temperate summers runoff generation and shallow subsurface flow through the vadose are controlled by highly fractured shallow bedrock layers and runoff occurs primarily on variable saturated areas underlain by low permeability fragipan layers bryant et al 2011 as part of the appalachian valley and ridge province the watershed is underlain by fine siltstone supporting soils prone to variable source area vsa runoff generation the uplands feature well drained soils with high infiltration rates while the lower landscape positions are more poorly drained soil with features that seasonally perch water and result in runoff generation by saturation excess processes lu et al 2015 this combination of factors make we38 an ideal location to test the suitability of models to forecast hydrologic responses across a highly variable watershed the climate of we 38 is temperate humid with a mean temperature of 10 1oc mean precipitation of 1080 mm yr 1 and mean streamflow equal to 46 of precipitation buda et al 2011 lu et al 2015 elevation ranges from 220 to 510 m and the land use of the watershed consists of agriculture 44 5 forest 33 8 and pasture 3 5 collick et al 2015 2 2 swat model description the swat model is a process based semi distributed watershed scale model developed to assess the impact of land management practices on water availability and water quality arnold et al 1998 swat requires meteorological precipitation min and max temperature solar radiation wind speed and humidity soil land cover and land management data to simulate surface and subsurface hydrology and various chemical nutrient and sediment fluxes swat vsa re conceptualizes swat to account for areas of the landscape subject to variable saturation dynamics in swat vsa the area of each hydrological response unit hru is defined by the coincidence of land use and wetness index class which is determined from a topographic index ti to differentiate areas of the landscape with respect to their moisture storage and saturation runoff potential easton et al 2008 2011 swat vsa was selected because it has been shown to provide better predictions of soil moisture runoff generation and nutrient export than the standard swat model in we 38 and similar watersheds easton et al 2008 collick et al 2015 wagena et al 2018 it is also capable of predicting these responses at a sub field levels which is important information for land managers and agricultural producers seeking to reduce the water quality impacts of human activities 2 3 watershed model initialization and input data swat vsa was initialized with a 10 m resolution digital elevation model resampled from 0 5 m lidar data obtained from canaan valley institute 2007 using arcswat 2012 and toposwat available from https dx doi org 10 6084 m9 figshare 1342823 developed by fuka et al 2016 toposwat automates the swat vsa initialization process by creating the ti data and then overlaying the soil and ti data to develop the required database for model initialization soils data utilized by topo swat are based on the food and agriculture organization fao soils database fao 2007 this methodology downscales the fao soils data distributes the soil properties across ti classes and has been shown to provide a more accurate representation of soil properties than ssurgo fuka et al 2016 the land use characterization of we 38 was derived from previous studies gburek et al 2002 2006 needelman et al 2004 veith et al 2008 buda et al 2009 2013 the model was initialized using measured precipitation temperature min and max relative humidity wind speed and solar radiation from 1987 to 2010 from land based stations in we 38 wagena et al 2018 2 4 quantitative precipitation forecasts qpfs the qpf is a forecast product developed by the us national weather service weather prediction center the qpf is the expected amount of accumulated liquid precipitation defined as the expected areal average on a 20 20 km grid in a given amount of time the output is then post processed and downscaled to 5 km in final form to send to end users the qpf data are generated in utc not in local time the qpf data are adjusted for topographic effect additionally forecasters adjust qpf data based on their experience to minimize the uncertainty of the forecast it is also important to note that qpf data are provided in 6 hr accumulations so sub 6 hr intensity variability is not provided 2 5 model calibration and evaluation the swat vsa model was calibrated using swat cup swat calibration and uncertainty procedure arnold et al 2012 and sufi2 sequential uncertainty fitting as optimization algorithms with the objective function which was set to the nash sutcliffe efficiency coefficient nse the swat vsa model performance was evaluated based on three metrics percent bias pbias root mean square error rmse and the nse these three metrics were assessed using historical observations from two time periods 1989 to 1998 for model calibration and 1999 to 2007 for model evaluation the nse is an indicator of the predictive power of the model and ranges from to 1 where 1 is a perfect fit between modeled and observed data and an nse of 0 indicates that the observed mean provides a better estimate of the data then the model krause et al 2005 the pbias is a statistical metric that provides an estimate of overprediction pbias 0 or underprediction pbias 0 of the model while rmse is a measure of the spread of observed values about the predicted values 2 6 swat vsa streamflow forecast quantitative precipitation forecasts for 1 to 8 day lead times were used as input to force the swat vsa model in a forecast mode since qpf only contains precipitation data the remainder of the required weather variables such as relative humidity solar radiation wind speed and temperature were simulated using the swat weather generator sharpley and williams 1990 the daily values for temperature maximum and minimum and solar radiation were generated using a weekly stationary process described by matalas 1967 relative humidity was generated using a triangular distribution from monthly averages sharpley and williams 1990 and wind speed was generated using a modified exponential equation sharpley and williams 1990 two distinct sets of meteorological data were used in this study the first was historical data used for model calibration and evaluation the second referred to as archived forecasts was used to evaluate the model s capability to forecast without waiting months to collect future qpf forecast data after model calibration and evaluation using the historic data the qpf data set prepared for the hindcast procedure was used to initialize unique swat vsa model runs for each day in the hindcast period january 1 2017 to february 28 2018 444 days recreating the live forecast procedure that the model was to perform prior to using the qpf forecast data to force the swat vsa model the data were converted from utc to local time during conversion of qpf data from utc to local time there was missing data of 7 8 h depending on the forecast day for the first forecast day in order to ensure that this bias was minimized and did not propagate to the remaining forecasts the missing data were replaced with observed precipitation data in cases where no observed data were available other sources of precipitation data such as satellite or gridded precipitation data sources e g cfsr data could be used to fill missing data each unique model run was forced with 12 years of observed meteorological data with the last eight days of the run defined by the corresponding forecast day i e 24 h 48 h 72 h up to 192 h from the qpf data the remaining input was defined by the meteorological data prepared for calibration evaluation just as it would be in a live forecast the last eight days of each swat vsa daily watershed discharge time series output representing the hydrologic forecasts was saved and separated into eight unique time series by forecast level thus the streamflow hindcast datasets were generated as vectors of data points x i j one vector for each of the eight qpf lead times where x i j is the forecast of streamflow generated by model run i 1 y for a specific lead time of the qpf forecast 1 8 days j i j is equal to the day number within the hindcast period plus one 1 y 1 for example the 24 hr archived forecasts vector is expressed as x 1 1 x 2 1 x 3 1 x y 1 and the 72 hr archived forecasts vector as x 1 1 x 1 2 x 1 3 x 2 3 x 3 3 x y 2 3 this procedure produced time series for all eight hindcast days that could be directly compared to measured volumetric flow at the watershed outlet 2 7 time series models including exogenous covariates arma models typically consider the previous condition data lag and some error term to forecast future conditions babu and reddy 2014 as is commonly used in reservoir forecasting however given the stochastic component of rainfall and its influence on streamflow generation we modify the standard arma model form to include exogenous covariates specifically precipitation the general arma time series model is described by eq 1 1 y t 1 y t 1 2 y t 2 p y t p β x t a t θ 1 a t 1 θ 2 a t 2 θ q a t q fitting eq 1 for the historical arma time series results in 2 y t 1 y t 1 2 y t 2 3 y t 3 β x t a t θ 1 a t 1 θ 2 a t 2 θ 3 a t 3 θ 4 a t 4 the general arma forecast equation 3 y t m 1 y t 1 m 2 y t 2 m p y t 3 m β x t a t m θ 1 a t 1 m θ 2 a t 2 m θ q a t 3 m θ q a t 4 m the fitted arma forecast equation for lead time one day m 1 day 4 y t 1 1 y t 2 y t 1 p y t 2 β x t a t 1 θ 1 a t θ 2 a t 1 θ q a t 2 θ q a t 3 where y t is the predicted streamflow m3 s at time t d y t 1 a n d y t p are previous data values for a given time t 1 and t p a t is the error term at time t a t 1 a n d a t q are previous error terms for time t 1 and t q 1 t p θ 1 a n d θ t q are parameters to be fitted using observed data p and q are the order of the autoregressive and moving average components respectively x t is a row vector containing the values of precipitation covariates at time t β is a column vector containing the coefficients related to those covariates e g precipitation and m is the forecast lead time d the covariate is precipitation which is included to enhance the forecast ability of arma time series model arma time series models have three components the autoregressive ar the moving average ma components and the exogenous covariate to fit the regression the observed streamflow dataset was split into two datasets which corresponded to the swat vsa calibration and evaluation periods prior to the fitting of the arma model stationarity of the observed streamflow data was checked using the augmented dickey fuller method cheung and lai 1995 and the trend and seasonality of the streamflow data were checked using the decomposition method in r by checking for increasing or decreasing values in the time series for trend and by checking for patterns that repeat seasonally all were found to be acceptable the arma model with exogenous covariates was first fit to the calibration dataset from 1989 to 1998 using eq 1 during fitting of the arma model the time series model orders p q were derived from the autocorrelation functions acf and partial autocorrelation functions pacf by plotting acf and pacf and checking whether there were significant autocorrelations with time lags in the flow data time series using r the acf is the linear dependence of a variable with itself at two points in time and is useful in determining whether a time series is stationary the pacf is the autocorrelation between variables after removing linear dependence berryman turchin 2001 after fitting the model using the calibration dataset the best model was selected by checking the acf and pacf residuals of the fitted model and determining whether the residuals of fitted model were normally distributed using shapiro wilk test and quantile quantile q q plot the arma model performance was then evaluated using separate streamflow and precipitation datasets from 1999 to 2007 period finally the fitted arma time series model with the qpf forecast data as covariates eq 3 was used to forecast streamflow for 1 to 8 day lead times for the 01 01 2017 through 02 28 2018 forecast period eight time series of forecast streamflow data one for each forecast day were extracted and compared with the observed streamflow data 2 8 artificial neural networks anns the ann model developed in this study was a deep feed forward neural network composed of an input layer hidden layers and an output layer schmidhuber 2015 layers are made up of nodes called neurons which produce real valued activations from a number of weighed connections to neurons from the previous layer schmidhuber 2015 the first layer of an ann model is made up of input neurons which hold the value of the independent predictor variable one variable per node fig 2 this process is repeated through all layers and nodes and a final computation is done to calculate the values for the dependent prediction variables constituting the output layer schmidhuber 2015 training the ann model involved incrementally adjusting the weights eq 5 assigned to each connection neuron between nodes to optimize a loss function the backpropagation algorithm implemented in this study eq 6 is a comparatively fast and efficient training algorithm and currently very common among modern ann code libraries lecun et al 2015 5 f j w j i x i 6 o u t p u t f f j where f j is the weighted sum of inputs w are the weights x are the inputs and f is the activation function j and i are indices for weights that links inputs hidden and output layers the most critical inputs to train the ann were found to be the lag streamflow and observed precipitation to train the networks the input variables were selected by a trial and error method from a combination of different input variables e g flow and precipitation until weights converge by increasing the number of iterations and changing the transfer functions after which performance was evaluated by comparing the fitted ann output to observed streamflow the number of hidden layers four was iteratively determined based on the performance of fitted ann output to implement the forecast in the ann a similar procedure to swat vsa forecasting was developed with the additional variable of one day lag streamflow and precipitation from qpf used as input to the fitted network the forecasted streamflow for the eight day forecast time series was compared with the observed data to evaluate the performance of the anns 2 9 bayesian generalized non linear multilevel models bgmm the bgmm procedure a statistical post processing method was used to ensemble the streamflow forecasts of the three models for each forecast period the bgmm was fitted using the stan package in r buerkner 2016 carpenter et al 2016 wagena et al 2019 model predictions of streamflow for each forecast day from swat vsa ann and arma were used as bgmm input the bgmm predicts a response y using a combination of predictors three model outputs transformed by an inverse link function assuming a certain distribution d using eqs 7 and 8 7 y i d f η i θ 8 η β x z v where y i is the response at time i obtained through the linear combination of η of predictors transformed by inverse function f and assuming a certain distribution d β and u are fitted coefficients at the population level and group level respectively and x and z are the corresponding design matrices the model parameters β u and θ were estimated using the hamiltonian monte carlo nuts algorithm buerkner 2016 by defining the prior distributions and x or x y and z respectively the weights placed on each of the model predictions were obtained by summing the absolute value of the fitted coefficient values β and then dividing each coefficient by the sum of the absolute value of the coefficients obtained during the training period eq 9 9 w e i g h t s i β i i β where the sum of all weights across i is equal to one and coefficient β are fitted absolute coefficient values of each predictor finally to determine if the models were able to maintain a component mass balance we compared estimates of baseflow and runoff from each model against measured baseflow and runoff in the watershed to separate baseflow and runoff for the arma ann bgmm and measured data we employed a simple signal filter luo et al 2012 and for swat vsa we used the model predictions of baseflow and runoff directly from the model 3 results 3 1 historical model performance assessment according to performance criteria recommended by moriasi et al 2007 2015 a model simulation can be judged as satisfactory if nse 0 5 and pbias 25 for streamflow based on these criteria the results indicate that all three models predicted the observed flow well or moderately well during the calibration and evaluation periods table 1 with a low pbias and good nse and rmse performance metrics all models slightly over predicted the overall streamflow mass balance during calibration and evaluation periods as indicated by the pbias metric table 1 although peak flows were slightly underestimated fig 3 fig 3 shows observed and predicted daily streamflow for each model during the calibration and evaluation periods all models simulated the daily low flows moderately well during both periods table 1 fig 3 overall swat vsa and the ann model provided the most accurate prediction of daily streamflow with nse of 0 60 or greater during both the training and testing periods table 1 all models tended to under predict peak flows during both the training and testing periods although less so for the ann fig 3 for baseflow the arma model performed best followed by the swat vsa and ann models fig 3 3 2 forecast performance evaluation the short term streamflow forecasts for the three models were compared to measured data to assess the forecast ability of the models the forecast results show all models were generally able to forecast streamflow well or moderately well during most lead times table 2 figs 4 7 as would be expected all models exhibited better predictive performance during shorter lead times 24 72 h compared to longer times table 2 all individual models generally underpredicted streamflow during all lead times and exhibited increasing prediction bias at greater lead times the bgmm overpredicted during all lead time periods except during 72 120hr forecast periods when it underpredicted streamflow the bgmm consistently exhibited the greatest nse across all forecast lead times although the ann produced similar forecast results fig 4 compares the streamflow forecast by swat vsa to the observed streamflow for each forecast period swat vsa closely captured the trend of daily forecasted streamflow but tended to miss some peak flow events particularly during the 120 192hr lead times fig 4 swat vsa performed better in capturing peaks during the 24 96hr lead times although there was still a tendency to underpredict high flows while high flows were slightly underpredicted the timing of events was well captured fig 4 however as indicated by fig 8 swat vsa correctly partitioned total streamflow between baseflow and runoff fractions both during the calibration and evaluation and for the most part for the forecast period the notable exception was for the 120 192hr forecasts where swat vsa underpredicted runoff also corroborated in fig 4 and table 2 the majority of this underprediction is due to the bias in qpf forecast precipitation falling well below the observed precipitation during the 120 192 h periods fig 9 the ann model forecasted streamflow well across all lead times fig 5 the ann performed well in predicting both base and peak streamflow and the trend of the observed streamflow across all lead times although there was a slight tendency to under predict peak flow particularly during spring snowmelt march may in fig 5 one of the strengths of the ann model is its significant performance skill in capturing baseflow during all lead times figs 5 and 8 however baseflow separation of the ann predicted streamflow showed it underpredicted runoff during forecast periods except for the 72hr forecast period fig 8 interestingly the systematic underprediction of qpf precipitation during the 120 192hr forecasts fig 9 did not impact the ann predictions nearly as dramatically as the process based swat vsa model fig 6 shows the time series performance of the arma model for the forecast period the arma model tended to underpredict peak flows and overpredict baseflows indeed the mass balance in fig 8 indicates that the model overpredicted baseflow and underpredicted runoff there is no apparent forecast lead time or time of year during which the arma model performed better or worse the multi model bgmm forecasted streamflow well during all lead times compared to the individual models although the ann produced similar results fig 7 shows the time series performance of bgmm during the forecast period the bgmm captured both baseflow and peak flow with skills similar to the ann table 2 and the mass balance in fig 8 corroborates this also shown in fig 7 are the 95 credible intervals for the bgmm model predictions which indicate the probability that the observed streamflow lies within the credible interval 95 of the time as is clear from fig 7 and table 3 the majority of the observed streamflow fell within the 95 credible interval of the bgmm with only 3 1 4 2 of the observed data depending on forecast hr not contained by the interval moreover it was the larger peaks that tended to fall outside of this interval 4 discussion the performance variability among the models in simulating daily streamflow baseflow and runoff is of interest to explore process based models with representative model forcing data adequate physical process representation and good structure that are well calibrated generally predict streamflow well compared to empirical models due to the process based model s representation of internal processes wang et al 2011 however well defined and trained empirical models like anns have the ability to forecast streamflow with comparable or superior skill to process based models though the performance of anns is highly dependent on the number of layers and the transfer functions used to train the model maier et al 2010 the availability of data is also crucial to generalize the performance of anns dawson and wilby 1998 anns are nonlinear in nature and with important variables and associated data can be trained to learn and generalize complex problems in contrast fully empirical time series models like the arma are linear in nature and therefore often fail to capture complex non linear processes results of this study run counter to some of the literature that shows a process based model like swat generally outperforming empirical models in forecasting streamflow demirel et al 2009 however the inherent uncertainty stemming from model parameters structure mathematical process representations inputs and initial conditions can also cause process based models to perform poorly compared to less parametrized empirical models during forecast periods block et al 2009 indeed in this application the ann model tended to produce the most accurate predictions table 2 fig 5 the ann for we38 watershed was trained with adequate data and a sufficient number of layers to simulate baseflow and runoff well during the calibration and evaluation periods with predictive power similar to or better than swat vsa table 1 fig 3c d the arma model also performed well in predicting the trend in streamflow baseflow and runoff when exogenous covariates were used table 1 fig 3e d however while the arma model forecast streamflow moderately well it more severely overpredicted baseflow and underpredicted runoff during all periods compared to the other two models fig 8 swat vsa simulated daily streamflow very well during the calibration and evaluation periods table 1 fig 3a b but like most other process based models missed capturing some of the peaks due to the stochastic nature of streamflow in contrast the swat vsa performance declined substantially during the forecast period from its performance during calibration and evaluation its performance was lower than the other two individual models table 1 table 2 it is important to emphasize that the swat vsa model was developed by considering the relevant processes that can be predicted at the landscape field level using hydrologic response units modeling multiple complex processes inherently introduces more sources of uncertainty than modeling a single output variable as in the ann and arma models in addition the swat model is known to simulate landscape processes quite well but not always subsurface processes e g baseflow luo et al 2012 which may account for its relatively high bias during calibration and evaluation table 1 and during the forecast period table 2 particularly for longer qpf lead times despite these shortcomings the utility of swat vsa goes well beyond streamflow simulation and forecasting one of the benefits of swat vsa over empirical models is its ability to make spatial predictions within the watershed for processes such as soil moisture runoff generation or nutrient export for watershed management concerns beyond water supply and flooding such as selection and targeting of best management practices swat vsa has high potential to enhance decision making wagena and easton 2018 however the process modeling that provides useful information for decision making particularly in agroecosystem management e g plant growth evapotranspiration and soil moisture content also affects the water balance and can cause the uncertainty in each of these processes to cascade through the model and impact streamflow estimation as a result swat vsa is likely to be more sensitive to bias in the qpf data e g fig 9 fig 9 compares the observed precipitation to the qpf predictions and demonstrates that the qpfs had significant bias systematically under predicting precipitation for all lead times most acutely beyond 120hr interestingly the bias in the qpf fig 9 forecast did not impact the ann or arma predictions in any systematic manner they both provided approximately equivalent forecast predictions across all lead times as evidenced in table 2 this at first inspection seems counter intuitive however because both the ann and arma models include a lag observed streamflow term the models both are able to essentially bias correct the qpf data while running the forecast to overcome the shortcomings of any individual model the bgmm leveraged the skill of three individual models and resulted in good performance of streamflow forecasting during all forecast periods the method considers the strengths and weaknesses of each model s performance in forecasting streamflow based on evaluating individual model performance and providing the 95 credible interval consequently the ensemble of the three models outputs using the bgmm approach generally outperformed the individual models in forecasting streamflow although the ann results were similar table 2 fig 7 in addition to the improved forecast skill exhibited by the bgmm approach the uncertainty associated with the streamflow forecast is also provided which can assist in risk based decision making block et al 2009 since there are no perfect forecast models block et al 2009 the bgmm approach could improve forecast skill with respect to runoff baseflow and associated cumulative uncertainty indeed bgmm forecasted baseflow and runoff compared favorably to the individual models during all forecast periods by leveraging each of the model s forecasting abilities fig 8 5 conclusions using forecasted streamflow from both process based and empirical models helps water resources planning and management efforts streamflow forecasting specifically has an essential role in water resources and environmental engineering for example hydrologic forecasting is used by farmers to assess soil moisture condition and enable them to identify areas prone to runoff and erosion sommerlot et al 2016 reducing flood damage by providing a warning period where actions can be taken to mitigate the impact and by reservoir operators to ensure adequate water supplies shiri and kisi 2010 overall our results show that there is no single superior model that can be used to forecast streamflow and each model has its own strengths and weaknesses the ann was able to learn stochastic complex nonlinear problems although they still missed capturing some peak flows while the process based model did not always perform better than the empirical models it does have the ability to provide spatial responses in the watershed and predict water quality both at the watershed outlet and at given locations along the stream network arma models require relatively little data but are limited in their ability to simulate complex processes declaration of competing interest the authors have no conflicts of interest acknowledgments we would like to acknowledge high performance computing support from yellowstone http n2t net ark 85065 d7wd3xhc provided by ncar s computational and information systems laboratory support from the national science foundation under award numbers 1360415 and 1343802 and funding support from the usda under project number 2012 67019 19434 all data and methods used in this manuscript are available upon request 
26061,streamflow forecasts are essential for water resources management although there are many methods for forecasting streamflow real time forecasts remain challenging this study evaluates streamflow forecasts using a process based model soil and water assessment tool variable source area model swat vsa a stochastic model artificial neural network ann an auto regressive moving average arma model and a bayesian ensemble model that utilizes the swat vsa ann and arma results streamflow is forecast from 1 to 8 d forced with quantitative precipitation forecasts from the us national weather service of the individual models swat vsa and the ann provide better predictions of total streamflow nse 0 60 0 70 and peak flow but underpredicted low flows during the forecast period the ann had the highest predictive power nse 0 44 0 64 however all three models underpredicted peak flow the bayesian ensemble forecast streamflow with the most skill for all forecast lead times nse 0 49 0 67 and provided a quantification of prediction uncertainty keywords swat vsa anns arma forecasting stochastic model process based model bayesian model 1 introduction streamflow forecasting is essential for hydrologists water resource planners water system managers emergency response providers and policymakers to respond to ever increasing water demand and greater variability however since streamflow is largely driven by stochastic processes rainfall temperature seasonal variability and complex nonlinear watershed responses shao et al 2009 londhe and charhate 2010 it is challenging to precisely forecast streamflow particularly as forecast lead times increase challenges to accurate streamflow forecasting as outlined by pagano et al 2014 include automating real time data assimilation for model forcing that rainfall runoff models are simplifications of real world processes and that precipitation forecasts are inherently uncertain there are two broad rainfall runoff modeling approaches used to forecast streamflow physical process based models and empirical statistical models masselot et al 2016 process based models attempt to incorporate the relevant physical laws controlling watershed response and streamflow generation generally requiring substantial effort to build parameterize and calibrate and often utilize an extensive amount of observed data however process based models can provide the more reliable streamflow forecast compared to other forecasting techniques zealand et al 1999 for instance demirel et al 2009 compared the performance of a process based model the soil and water assessment tool swat arnold et al 1998 and an empirical artificial neural network ann model in forecasting streamflow and found that while anns were better at forecasting peak flow swat performed better overall similarly hapuarachchi et al 2011 found that process based models provide more credible results in predicting flash flooding than empirical models while process based models can often forecast streamflow with adequate skill there can also be tremendous uncertainty in the model predictions because of the uncertainty in input data model structure and the numerical techniques employed unfortunately this uncertainty is often not included quantitatively in deterministic forecasts but is often incorporated into the final forecast product qualitatively using post processing tools and forecaster experience empirical models attempt to reproduce the relationships between inputs e g precipitation and outputs streamflow with no assumption of internal process understanding empirical models are often more easily developed and can provide reliable forecasts when the models are trained on robust representative data several different types of empirical models have been used to forecast streamflow including time series models such as the auto regressive moving average arma procedure babu and reddy 2014 mohammad 2015 anns khashei and bijari 2010 kalteh 2013 functional regressions masselot et al 2016 wavelet support vectors kisi and cimen 2011 or decision trees bhattacharya and solomatine 2005 time series models also known as auto regressive models can approximate complex non linear hydrologic processes by using the relationship between observed values of streamflow and precipitation shao et al 2009 auto regressive models assume that the present condition depends on the past conditions often adjusted by an error function and a seasonality function yurekli et al 2005 time series models require a large amount of observed data to train and test the model yurekli et al 2005 khashei et al 2009 and often perform poorly when applied outside of the data range for which they were developed guo et al 2011 however there have been many successful applications of auto regressive models used in hydrologic forecasting for instance toth et al 2000 showed that using an arma model for real time flood forecasting provided better predictive ability than simple rainfall runoff models emphasizing that empirical models can often outperform overly simplistic process based models collischonn et al 2007 used an arma model to forecast medium range reservoir inflow based on quantitative precipitation forecasts qpf and showed that prediction errors of an arma model could be reduced using improved rainfall forecasts another type of empirical model commonly used to forecast streamflow is an ann anns are non linear data driven self adaptive type models that can learn and generalize trends based on functional relationships in the data coulibaly et al 2000 zhang et al 2001 the ann method has been applied in many different sectors including traffic control srinivasan et al 2006 image processing kulkarni 1994 stock forecasting guresen et al 2011 handwriting analysis nasrabadi 2007 and language recognition graves et al 2013 in the water resources and environmental engineering fields anns have been employed for rainfall forecasting french et al 1992 luk et al 2001 rainfall runoff modeling shamseldin 1997 tokar and johnson 1999 reservoir operation jain et al 1999 rani and moreira 2010 and water quality forecasting palani et al 2008 singh et al 2009 although anns have been widely used in water resource fields there remain challenges including the initial effort to build the model computational power to run the model and data required to train anns additionally it can be difficult to identify important input variables parameterize the number of ann layers and identify which transfer functions best describe the system zhang et al 2001 one of the major shortcomings with anns is that they cannot work with major changes to the system they must be re trained to the new system state after major disturbances sudheer et al 2003 one other streamflow forecasting technique that is gaining popularity leverages the skill from multiple models to create an ensemble streamflow forecast one significant advantage of the multiple model ensemble mme approach is the ability to evaluate model uncertainty weigel et al 2008 cloke and pappenberger 2009 provide a review of studies employing mme approaches to exploit the diversity of skillful predictions from different models for river flood forecasting and enumerate the challenges related to the ensemble approach such as computing power resolution of input data quantification and analysis pf total uncertainty communicating uncertainty and probabilistic forecasting one such mme approach is the bayesian generalized multilevel modeling bgmm method which integrates multiple probability distributions by using prior knowledge about parameter distributions allows predictor variables to be linear or nonlinear and incorporates information from all predictors models the method also offers a more complete quantification of uncertainty rather than simply attempting to improve forecasting skill wagena et al 2019 this study compares the performance of a process based model swat vsa with a modified arma time series model an ann model and a bgmm application in a real time forecast of streamflow we applied each model in the us department of agriculture we 38 experimental watershed in east central pennsylvania us and forecast streamflow from 1 to 8 days into the future using a hindcast procedure precipitation data from the us national weather service qpf was used to force each model for each forecast day the bgmm was developed by ensembling the outputs from the swat vsa arma and ann models for each forecast day 2 materials and methods 2 1 watershed description the we 38 experimental watershed is a sub watershed of the mahantango creek watershed in east central pennsylvania which drains to the susquehanna river fig 1 the watershed has an area of 7 3 km2 and has been extensively studied as a united states department of agriculture agricultural research service usda ars experimental watershed since 1966 and contains a wealth of data to evaluate model performance the watershed is characterized by complex terrain with multiple runoff generating mechanisms several crop types and variable climate snow dominated winters humid temperate summers runoff generation and shallow subsurface flow through the vadose are controlled by highly fractured shallow bedrock layers and runoff occurs primarily on variable saturated areas underlain by low permeability fragipan layers bryant et al 2011 as part of the appalachian valley and ridge province the watershed is underlain by fine siltstone supporting soils prone to variable source area vsa runoff generation the uplands feature well drained soils with high infiltration rates while the lower landscape positions are more poorly drained soil with features that seasonally perch water and result in runoff generation by saturation excess processes lu et al 2015 this combination of factors make we38 an ideal location to test the suitability of models to forecast hydrologic responses across a highly variable watershed the climate of we 38 is temperate humid with a mean temperature of 10 1oc mean precipitation of 1080 mm yr 1 and mean streamflow equal to 46 of precipitation buda et al 2011 lu et al 2015 elevation ranges from 220 to 510 m and the land use of the watershed consists of agriculture 44 5 forest 33 8 and pasture 3 5 collick et al 2015 2 2 swat model description the swat model is a process based semi distributed watershed scale model developed to assess the impact of land management practices on water availability and water quality arnold et al 1998 swat requires meteorological precipitation min and max temperature solar radiation wind speed and humidity soil land cover and land management data to simulate surface and subsurface hydrology and various chemical nutrient and sediment fluxes swat vsa re conceptualizes swat to account for areas of the landscape subject to variable saturation dynamics in swat vsa the area of each hydrological response unit hru is defined by the coincidence of land use and wetness index class which is determined from a topographic index ti to differentiate areas of the landscape with respect to their moisture storage and saturation runoff potential easton et al 2008 2011 swat vsa was selected because it has been shown to provide better predictions of soil moisture runoff generation and nutrient export than the standard swat model in we 38 and similar watersheds easton et al 2008 collick et al 2015 wagena et al 2018 it is also capable of predicting these responses at a sub field levels which is important information for land managers and agricultural producers seeking to reduce the water quality impacts of human activities 2 3 watershed model initialization and input data swat vsa was initialized with a 10 m resolution digital elevation model resampled from 0 5 m lidar data obtained from canaan valley institute 2007 using arcswat 2012 and toposwat available from https dx doi org 10 6084 m9 figshare 1342823 developed by fuka et al 2016 toposwat automates the swat vsa initialization process by creating the ti data and then overlaying the soil and ti data to develop the required database for model initialization soils data utilized by topo swat are based on the food and agriculture organization fao soils database fao 2007 this methodology downscales the fao soils data distributes the soil properties across ti classes and has been shown to provide a more accurate representation of soil properties than ssurgo fuka et al 2016 the land use characterization of we 38 was derived from previous studies gburek et al 2002 2006 needelman et al 2004 veith et al 2008 buda et al 2009 2013 the model was initialized using measured precipitation temperature min and max relative humidity wind speed and solar radiation from 1987 to 2010 from land based stations in we 38 wagena et al 2018 2 4 quantitative precipitation forecasts qpfs the qpf is a forecast product developed by the us national weather service weather prediction center the qpf is the expected amount of accumulated liquid precipitation defined as the expected areal average on a 20 20 km grid in a given amount of time the output is then post processed and downscaled to 5 km in final form to send to end users the qpf data are generated in utc not in local time the qpf data are adjusted for topographic effect additionally forecasters adjust qpf data based on their experience to minimize the uncertainty of the forecast it is also important to note that qpf data are provided in 6 hr accumulations so sub 6 hr intensity variability is not provided 2 5 model calibration and evaluation the swat vsa model was calibrated using swat cup swat calibration and uncertainty procedure arnold et al 2012 and sufi2 sequential uncertainty fitting as optimization algorithms with the objective function which was set to the nash sutcliffe efficiency coefficient nse the swat vsa model performance was evaluated based on three metrics percent bias pbias root mean square error rmse and the nse these three metrics were assessed using historical observations from two time periods 1989 to 1998 for model calibration and 1999 to 2007 for model evaluation the nse is an indicator of the predictive power of the model and ranges from to 1 where 1 is a perfect fit between modeled and observed data and an nse of 0 indicates that the observed mean provides a better estimate of the data then the model krause et al 2005 the pbias is a statistical metric that provides an estimate of overprediction pbias 0 or underprediction pbias 0 of the model while rmse is a measure of the spread of observed values about the predicted values 2 6 swat vsa streamflow forecast quantitative precipitation forecasts for 1 to 8 day lead times were used as input to force the swat vsa model in a forecast mode since qpf only contains precipitation data the remainder of the required weather variables such as relative humidity solar radiation wind speed and temperature were simulated using the swat weather generator sharpley and williams 1990 the daily values for temperature maximum and minimum and solar radiation were generated using a weekly stationary process described by matalas 1967 relative humidity was generated using a triangular distribution from monthly averages sharpley and williams 1990 and wind speed was generated using a modified exponential equation sharpley and williams 1990 two distinct sets of meteorological data were used in this study the first was historical data used for model calibration and evaluation the second referred to as archived forecasts was used to evaluate the model s capability to forecast without waiting months to collect future qpf forecast data after model calibration and evaluation using the historic data the qpf data set prepared for the hindcast procedure was used to initialize unique swat vsa model runs for each day in the hindcast period january 1 2017 to february 28 2018 444 days recreating the live forecast procedure that the model was to perform prior to using the qpf forecast data to force the swat vsa model the data were converted from utc to local time during conversion of qpf data from utc to local time there was missing data of 7 8 h depending on the forecast day for the first forecast day in order to ensure that this bias was minimized and did not propagate to the remaining forecasts the missing data were replaced with observed precipitation data in cases where no observed data were available other sources of precipitation data such as satellite or gridded precipitation data sources e g cfsr data could be used to fill missing data each unique model run was forced with 12 years of observed meteorological data with the last eight days of the run defined by the corresponding forecast day i e 24 h 48 h 72 h up to 192 h from the qpf data the remaining input was defined by the meteorological data prepared for calibration evaluation just as it would be in a live forecast the last eight days of each swat vsa daily watershed discharge time series output representing the hydrologic forecasts was saved and separated into eight unique time series by forecast level thus the streamflow hindcast datasets were generated as vectors of data points x i j one vector for each of the eight qpf lead times where x i j is the forecast of streamflow generated by model run i 1 y for a specific lead time of the qpf forecast 1 8 days j i j is equal to the day number within the hindcast period plus one 1 y 1 for example the 24 hr archived forecasts vector is expressed as x 1 1 x 2 1 x 3 1 x y 1 and the 72 hr archived forecasts vector as x 1 1 x 1 2 x 1 3 x 2 3 x 3 3 x y 2 3 this procedure produced time series for all eight hindcast days that could be directly compared to measured volumetric flow at the watershed outlet 2 7 time series models including exogenous covariates arma models typically consider the previous condition data lag and some error term to forecast future conditions babu and reddy 2014 as is commonly used in reservoir forecasting however given the stochastic component of rainfall and its influence on streamflow generation we modify the standard arma model form to include exogenous covariates specifically precipitation the general arma time series model is described by eq 1 1 y t 1 y t 1 2 y t 2 p y t p β x t a t θ 1 a t 1 θ 2 a t 2 θ q a t q fitting eq 1 for the historical arma time series results in 2 y t 1 y t 1 2 y t 2 3 y t 3 β x t a t θ 1 a t 1 θ 2 a t 2 θ 3 a t 3 θ 4 a t 4 the general arma forecast equation 3 y t m 1 y t 1 m 2 y t 2 m p y t 3 m β x t a t m θ 1 a t 1 m θ 2 a t 2 m θ q a t 3 m θ q a t 4 m the fitted arma forecast equation for lead time one day m 1 day 4 y t 1 1 y t 2 y t 1 p y t 2 β x t a t 1 θ 1 a t θ 2 a t 1 θ q a t 2 θ q a t 3 where y t is the predicted streamflow m3 s at time t d y t 1 a n d y t p are previous data values for a given time t 1 and t p a t is the error term at time t a t 1 a n d a t q are previous error terms for time t 1 and t q 1 t p θ 1 a n d θ t q are parameters to be fitted using observed data p and q are the order of the autoregressive and moving average components respectively x t is a row vector containing the values of precipitation covariates at time t β is a column vector containing the coefficients related to those covariates e g precipitation and m is the forecast lead time d the covariate is precipitation which is included to enhance the forecast ability of arma time series model arma time series models have three components the autoregressive ar the moving average ma components and the exogenous covariate to fit the regression the observed streamflow dataset was split into two datasets which corresponded to the swat vsa calibration and evaluation periods prior to the fitting of the arma model stationarity of the observed streamflow data was checked using the augmented dickey fuller method cheung and lai 1995 and the trend and seasonality of the streamflow data were checked using the decomposition method in r by checking for increasing or decreasing values in the time series for trend and by checking for patterns that repeat seasonally all were found to be acceptable the arma model with exogenous covariates was first fit to the calibration dataset from 1989 to 1998 using eq 1 during fitting of the arma model the time series model orders p q were derived from the autocorrelation functions acf and partial autocorrelation functions pacf by plotting acf and pacf and checking whether there were significant autocorrelations with time lags in the flow data time series using r the acf is the linear dependence of a variable with itself at two points in time and is useful in determining whether a time series is stationary the pacf is the autocorrelation between variables after removing linear dependence berryman turchin 2001 after fitting the model using the calibration dataset the best model was selected by checking the acf and pacf residuals of the fitted model and determining whether the residuals of fitted model were normally distributed using shapiro wilk test and quantile quantile q q plot the arma model performance was then evaluated using separate streamflow and precipitation datasets from 1999 to 2007 period finally the fitted arma time series model with the qpf forecast data as covariates eq 3 was used to forecast streamflow for 1 to 8 day lead times for the 01 01 2017 through 02 28 2018 forecast period eight time series of forecast streamflow data one for each forecast day were extracted and compared with the observed streamflow data 2 8 artificial neural networks anns the ann model developed in this study was a deep feed forward neural network composed of an input layer hidden layers and an output layer schmidhuber 2015 layers are made up of nodes called neurons which produce real valued activations from a number of weighed connections to neurons from the previous layer schmidhuber 2015 the first layer of an ann model is made up of input neurons which hold the value of the independent predictor variable one variable per node fig 2 this process is repeated through all layers and nodes and a final computation is done to calculate the values for the dependent prediction variables constituting the output layer schmidhuber 2015 training the ann model involved incrementally adjusting the weights eq 5 assigned to each connection neuron between nodes to optimize a loss function the backpropagation algorithm implemented in this study eq 6 is a comparatively fast and efficient training algorithm and currently very common among modern ann code libraries lecun et al 2015 5 f j w j i x i 6 o u t p u t f f j where f j is the weighted sum of inputs w are the weights x are the inputs and f is the activation function j and i are indices for weights that links inputs hidden and output layers the most critical inputs to train the ann were found to be the lag streamflow and observed precipitation to train the networks the input variables were selected by a trial and error method from a combination of different input variables e g flow and precipitation until weights converge by increasing the number of iterations and changing the transfer functions after which performance was evaluated by comparing the fitted ann output to observed streamflow the number of hidden layers four was iteratively determined based on the performance of fitted ann output to implement the forecast in the ann a similar procedure to swat vsa forecasting was developed with the additional variable of one day lag streamflow and precipitation from qpf used as input to the fitted network the forecasted streamflow for the eight day forecast time series was compared with the observed data to evaluate the performance of the anns 2 9 bayesian generalized non linear multilevel models bgmm the bgmm procedure a statistical post processing method was used to ensemble the streamflow forecasts of the three models for each forecast period the bgmm was fitted using the stan package in r buerkner 2016 carpenter et al 2016 wagena et al 2019 model predictions of streamflow for each forecast day from swat vsa ann and arma were used as bgmm input the bgmm predicts a response y using a combination of predictors three model outputs transformed by an inverse link function assuming a certain distribution d using eqs 7 and 8 7 y i d f η i θ 8 η β x z v where y i is the response at time i obtained through the linear combination of η of predictors transformed by inverse function f and assuming a certain distribution d β and u are fitted coefficients at the population level and group level respectively and x and z are the corresponding design matrices the model parameters β u and θ were estimated using the hamiltonian monte carlo nuts algorithm buerkner 2016 by defining the prior distributions and x or x y and z respectively the weights placed on each of the model predictions were obtained by summing the absolute value of the fitted coefficient values β and then dividing each coefficient by the sum of the absolute value of the coefficients obtained during the training period eq 9 9 w e i g h t s i β i i β where the sum of all weights across i is equal to one and coefficient β are fitted absolute coefficient values of each predictor finally to determine if the models were able to maintain a component mass balance we compared estimates of baseflow and runoff from each model against measured baseflow and runoff in the watershed to separate baseflow and runoff for the arma ann bgmm and measured data we employed a simple signal filter luo et al 2012 and for swat vsa we used the model predictions of baseflow and runoff directly from the model 3 results 3 1 historical model performance assessment according to performance criteria recommended by moriasi et al 2007 2015 a model simulation can be judged as satisfactory if nse 0 5 and pbias 25 for streamflow based on these criteria the results indicate that all three models predicted the observed flow well or moderately well during the calibration and evaluation periods table 1 with a low pbias and good nse and rmse performance metrics all models slightly over predicted the overall streamflow mass balance during calibration and evaluation periods as indicated by the pbias metric table 1 although peak flows were slightly underestimated fig 3 fig 3 shows observed and predicted daily streamflow for each model during the calibration and evaluation periods all models simulated the daily low flows moderately well during both periods table 1 fig 3 overall swat vsa and the ann model provided the most accurate prediction of daily streamflow with nse of 0 60 or greater during both the training and testing periods table 1 all models tended to under predict peak flows during both the training and testing periods although less so for the ann fig 3 for baseflow the arma model performed best followed by the swat vsa and ann models fig 3 3 2 forecast performance evaluation the short term streamflow forecasts for the three models were compared to measured data to assess the forecast ability of the models the forecast results show all models were generally able to forecast streamflow well or moderately well during most lead times table 2 figs 4 7 as would be expected all models exhibited better predictive performance during shorter lead times 24 72 h compared to longer times table 2 all individual models generally underpredicted streamflow during all lead times and exhibited increasing prediction bias at greater lead times the bgmm overpredicted during all lead time periods except during 72 120hr forecast periods when it underpredicted streamflow the bgmm consistently exhibited the greatest nse across all forecast lead times although the ann produced similar forecast results fig 4 compares the streamflow forecast by swat vsa to the observed streamflow for each forecast period swat vsa closely captured the trend of daily forecasted streamflow but tended to miss some peak flow events particularly during the 120 192hr lead times fig 4 swat vsa performed better in capturing peaks during the 24 96hr lead times although there was still a tendency to underpredict high flows while high flows were slightly underpredicted the timing of events was well captured fig 4 however as indicated by fig 8 swat vsa correctly partitioned total streamflow between baseflow and runoff fractions both during the calibration and evaluation and for the most part for the forecast period the notable exception was for the 120 192hr forecasts where swat vsa underpredicted runoff also corroborated in fig 4 and table 2 the majority of this underprediction is due to the bias in qpf forecast precipitation falling well below the observed precipitation during the 120 192 h periods fig 9 the ann model forecasted streamflow well across all lead times fig 5 the ann performed well in predicting both base and peak streamflow and the trend of the observed streamflow across all lead times although there was a slight tendency to under predict peak flow particularly during spring snowmelt march may in fig 5 one of the strengths of the ann model is its significant performance skill in capturing baseflow during all lead times figs 5 and 8 however baseflow separation of the ann predicted streamflow showed it underpredicted runoff during forecast periods except for the 72hr forecast period fig 8 interestingly the systematic underprediction of qpf precipitation during the 120 192hr forecasts fig 9 did not impact the ann predictions nearly as dramatically as the process based swat vsa model fig 6 shows the time series performance of the arma model for the forecast period the arma model tended to underpredict peak flows and overpredict baseflows indeed the mass balance in fig 8 indicates that the model overpredicted baseflow and underpredicted runoff there is no apparent forecast lead time or time of year during which the arma model performed better or worse the multi model bgmm forecasted streamflow well during all lead times compared to the individual models although the ann produced similar results fig 7 shows the time series performance of bgmm during the forecast period the bgmm captured both baseflow and peak flow with skills similar to the ann table 2 and the mass balance in fig 8 corroborates this also shown in fig 7 are the 95 credible intervals for the bgmm model predictions which indicate the probability that the observed streamflow lies within the credible interval 95 of the time as is clear from fig 7 and table 3 the majority of the observed streamflow fell within the 95 credible interval of the bgmm with only 3 1 4 2 of the observed data depending on forecast hr not contained by the interval moreover it was the larger peaks that tended to fall outside of this interval 4 discussion the performance variability among the models in simulating daily streamflow baseflow and runoff is of interest to explore process based models with representative model forcing data adequate physical process representation and good structure that are well calibrated generally predict streamflow well compared to empirical models due to the process based model s representation of internal processes wang et al 2011 however well defined and trained empirical models like anns have the ability to forecast streamflow with comparable or superior skill to process based models though the performance of anns is highly dependent on the number of layers and the transfer functions used to train the model maier et al 2010 the availability of data is also crucial to generalize the performance of anns dawson and wilby 1998 anns are nonlinear in nature and with important variables and associated data can be trained to learn and generalize complex problems in contrast fully empirical time series models like the arma are linear in nature and therefore often fail to capture complex non linear processes results of this study run counter to some of the literature that shows a process based model like swat generally outperforming empirical models in forecasting streamflow demirel et al 2009 however the inherent uncertainty stemming from model parameters structure mathematical process representations inputs and initial conditions can also cause process based models to perform poorly compared to less parametrized empirical models during forecast periods block et al 2009 indeed in this application the ann model tended to produce the most accurate predictions table 2 fig 5 the ann for we38 watershed was trained with adequate data and a sufficient number of layers to simulate baseflow and runoff well during the calibration and evaluation periods with predictive power similar to or better than swat vsa table 1 fig 3c d the arma model also performed well in predicting the trend in streamflow baseflow and runoff when exogenous covariates were used table 1 fig 3e d however while the arma model forecast streamflow moderately well it more severely overpredicted baseflow and underpredicted runoff during all periods compared to the other two models fig 8 swat vsa simulated daily streamflow very well during the calibration and evaluation periods table 1 fig 3a b but like most other process based models missed capturing some of the peaks due to the stochastic nature of streamflow in contrast the swat vsa performance declined substantially during the forecast period from its performance during calibration and evaluation its performance was lower than the other two individual models table 1 table 2 it is important to emphasize that the swat vsa model was developed by considering the relevant processes that can be predicted at the landscape field level using hydrologic response units modeling multiple complex processes inherently introduces more sources of uncertainty than modeling a single output variable as in the ann and arma models in addition the swat model is known to simulate landscape processes quite well but not always subsurface processes e g baseflow luo et al 2012 which may account for its relatively high bias during calibration and evaluation table 1 and during the forecast period table 2 particularly for longer qpf lead times despite these shortcomings the utility of swat vsa goes well beyond streamflow simulation and forecasting one of the benefits of swat vsa over empirical models is its ability to make spatial predictions within the watershed for processes such as soil moisture runoff generation or nutrient export for watershed management concerns beyond water supply and flooding such as selection and targeting of best management practices swat vsa has high potential to enhance decision making wagena and easton 2018 however the process modeling that provides useful information for decision making particularly in agroecosystem management e g plant growth evapotranspiration and soil moisture content also affects the water balance and can cause the uncertainty in each of these processes to cascade through the model and impact streamflow estimation as a result swat vsa is likely to be more sensitive to bias in the qpf data e g fig 9 fig 9 compares the observed precipitation to the qpf predictions and demonstrates that the qpfs had significant bias systematically under predicting precipitation for all lead times most acutely beyond 120hr interestingly the bias in the qpf fig 9 forecast did not impact the ann or arma predictions in any systematic manner they both provided approximately equivalent forecast predictions across all lead times as evidenced in table 2 this at first inspection seems counter intuitive however because both the ann and arma models include a lag observed streamflow term the models both are able to essentially bias correct the qpf data while running the forecast to overcome the shortcomings of any individual model the bgmm leveraged the skill of three individual models and resulted in good performance of streamflow forecasting during all forecast periods the method considers the strengths and weaknesses of each model s performance in forecasting streamflow based on evaluating individual model performance and providing the 95 credible interval consequently the ensemble of the three models outputs using the bgmm approach generally outperformed the individual models in forecasting streamflow although the ann results were similar table 2 fig 7 in addition to the improved forecast skill exhibited by the bgmm approach the uncertainty associated with the streamflow forecast is also provided which can assist in risk based decision making block et al 2009 since there are no perfect forecast models block et al 2009 the bgmm approach could improve forecast skill with respect to runoff baseflow and associated cumulative uncertainty indeed bgmm forecasted baseflow and runoff compared favorably to the individual models during all forecast periods by leveraging each of the model s forecasting abilities fig 8 5 conclusions using forecasted streamflow from both process based and empirical models helps water resources planning and management efforts streamflow forecasting specifically has an essential role in water resources and environmental engineering for example hydrologic forecasting is used by farmers to assess soil moisture condition and enable them to identify areas prone to runoff and erosion sommerlot et al 2016 reducing flood damage by providing a warning period where actions can be taken to mitigate the impact and by reservoir operators to ensure adequate water supplies shiri and kisi 2010 overall our results show that there is no single superior model that can be used to forecast streamflow and each model has its own strengths and weaknesses the ann was able to learn stochastic complex nonlinear problems although they still missed capturing some peak flows while the process based model did not always perform better than the empirical models it does have the ability to provide spatial responses in the watershed and predict water quality both at the watershed outlet and at given locations along the stream network arma models require relatively little data but are limited in their ability to simulate complex processes declaration of competing interest the authors have no conflicts of interest acknowledgments we would like to acknowledge high performance computing support from yellowstone http n2t net ark 85065 d7wd3xhc provided by ncar s computational and information systems laboratory support from the national science foundation under award numbers 1360415 and 1343802 and funding support from the usda under project number 2012 67019 19434 all data and methods used in this manuscript are available upon request 
26062,use of physically motivated numerical models like groundwater flow and transport models for probabilistic impact assessments and optimization under uncertainty ouu typically incurs such a computational burdensome that these tools cannot be used during decision making the computational challenges associated with these models can be addressed through emulation in the land use water quality context the linear relation between nitrate loading and surface water groundwater nitrate concentrations presents an opportunity for employing an efficient model emulator through the application of impulse response matrices when paired with first order second moment techniques the emulation strategy gives rise to the stochastic impulse response emulator sire sire is shown to facilitate non intrusive near real time and risk based evaluation of nitrate loading change scenarios as well as nitrate loading ouu subject to surface water groundwater concentration constraints in high decision variable and parameter dimensions two case studies are used to demonstrate sire in the nitrate loading context keywords emulation under uncertainty groundwater modeling nitrate decision support optimization under uncertainty 1 introduction a sound understanding of how land use change impacts water quality for water resource and ecosystem sustainability is critically important for effective land use management the impact of land use based nitrate loading on groundwater and surface water water quality in particular has been the focus of many studies over the last few decades e g moosburner and wood 1980 spalding and exner 1993 schilling and libra 2000 mclay et al 2001 belhouchette et al 2011 green et al 2018 green et al 2016 garcía díaz 2011 ayub et al linked hydrologic flow and transport numerical models hereinafter referred to as linked hydrologic models are often used to simulate the movement and fate of nitrate within a hydrologic system for the purposes of i evaluating the outcome of potential land use change scenarios in terms of changes in surface water groundwater nitrate concentrations e g morgan et al 2007 and ii optimizing the spatial distribution of changes in nitrate loading given maximum allowable surface water and groundwater concentrations e g williams and hann 1978 a common linked hydrological model workflow arises from the need to manage land use practices and development such that the optimal balance between development maximized economic utility and nitrate concentrations at important locations in the hydrologic system can be achieved evaluating the relation between nitrate loading and concentration in the decision support setting typically involves manually modifying the linked hydrologic model inputs to represent a potential nitrate loading change scenario running the model forward to yield simulated surface water and or groundwater concentrations at locations of management or ecological interest as well as post processing model simulation results into a format for use in the decision support context this approach is inefficient because it requires an additional model run per scenario typically taking hours to days and affords additional opportunities for user error beyond these problems if decision makers are interested in the optimal spatial distribution of nitrate loading subject to simulated water quality constraints then hundreds to tens of thousands of model evaluations may be required for a single formal management optimization analysis furthermore if the model is being used to support risk based decision making meaning that uncertainty in the model outputs of interest should be quantified for the given nitrate loading scenario then the number of model evaluations needed may number again in the hundreds to tens of thousands or more for a single nitrate loading scenario these inefficiencies can hamper or even preclude the use of a linked hydrologic model in the decision making process this is an unfortunate outcome as the model is the best available tool to guide decision making in this context doherty 2015 one way in which the computational challenges of the traditional modeling workflow have been addressed is through model emulation strategies which involve deployment of simpler fast running models that emulate the relationship between inputs and outputs of the complex physically motivated linked hydrologic model e g asher et al 2015 for example ahlfeld and mulligan 2000 described the use of response matrices constructed on the basis of simulation model perturbations for solving a variety of groundwater optimization problems siade et al 1029 used proper orthogonal decomposition to replace the original complex model with a faster executing emulator laloy et al 2013 used polynomial chaos expansion to emulate the relation between parameters and state estimates for conditioning fienen et al 2016 explored statistical metamodels to emulate the source of water to wells while fienen et al 2018a uses similar statistical learning techniques for estimating groundwater age cui et al 2018 used a gaussian process to emulate the relation between parameters and state estimates while these techniques are viable approaches to emulating certain parts of an environmental modeling workflow these techniques do not simultaneously combine non intrusiveness i e model independence the ability to scale to high input dimensionality either algorithmically or in terms of training data requirements and the ability to explicitly propagate model input parameter uncertainty to the emulated model outputs furthermore many model emulation techniques do not facilitate emulating a spatially and or temporally discrete input output relation such as evaluating how altering a specific spatially distributed land use type may change nitrate concentrations at important locations in the hydrologic system e g ransom et al 2017 these qualities are important for an emulation strategy to be used in real world practice for interactive risk based resource management decision making a number of studies have demonstrated that under certain conditions the relationship between nitrate loading and surface water and groundwater nitrate concentrations hereinafter referred to as the loading concentration relation is linear e g spalding and exner 1993 mclay et al 2001 this linearity relates the nitrate load to concentrations throughout the domain given a non changing flowfield even though changes to the flowfield would impart nonlinearity on the response this linearity in the load presents an opportunity to employ an emulation strategy that is efficient scalable and non intrusive for evaluation and optimization of the loading concentration relation indeed many resource decisions are relevant within this limitation when considering the response of concentrations at a compliance point where only loads are managed for example the nitrate load cause by different intensities of cattle grazing may not impact the flowfield but only impacts the input concentration thus only altering the linear part of the relationship herein we present a strategy to exploit this linear relation in combination with a commonly employed uncertainty quantification technique to yield a highly efficient scalable and non intrusive approach to emulation under uncertainty euu and optimization under uncertainty ouu first the theories and previous works supporting our approach to euu and ouu are presented and summarized then the utility of emulating the loading concentration relation under uncertainty to support near real time decision making is shown finally we demonstrate the efficiency of ouu when employing the same emulation strategy to provide near real time optimal risk based spatial nitrate loading patterns 2 background and theory the work presented herein is premised on the efficiency gained through exploiting the linearity of the loading concentration relation we also employ an additional linearity assumption via first order second moment techniques to estimate uncertainty in the emulated surface water and groundwater concentrations arising from high dimensional model input uncertainty combining these two elements provides a basis for the development of tools that can deliver non intrusive near real time risk based management scenario outcomes to support decision making related to land use nitrate loading 2 1 terminology it is recognized that a number of concepts related to model emulation optimization and uncertainty quantification span different scientific disciplines and utilize different terminologies to avoid confusion definitions as they pertain to the current study are now provided parameters model inputs any numeric quantity used by the model that are uncertain and thus nominated for adjustment during uncertainty analysis and or history matching herein parameters include horizontal and vertical hydraulic conductivity porosity surface water conductance and nitrate first order reaction rate among others decision variables model inputs related to a resource management action whose values are the subject of management scenarios and formal optimization herein decision variables are the changes in nitrate loading rates to the groundwater system and are spatially distributed so that each active surficial model node is a decision variable model outputs simulated states from the model that are of interest in the decision making process herein the model outputs of interest are surface water and groundwater nitrate concentrations for each active reach and model node respectively these quantities are inherently uncertain because the inputs to the model are uncertain constraints conditions that must be satisfied by the optimization process constraints can take two forms i those directly related to decision variables e g minimum and maximum values for nitrate loading rate and ii those derived from model outputs e g simulated surface water and groundwater concentrations objective function functional comprised of linear combinations of decision variables that is either minimized or maximized during the optimization process herein the functional is the sum of the nitrate loading rate changes scaled by an economic factor to convert mass of nitrate introduced to the groundwater system into monetary units 2 2 linearity in the loading concentration relation the relation between nitrate loading changes decision variables and resulting surface water and groundwater concentration changes model outputs has been the topic of many investigations a large volume of literature demonstrates that this relation takes a linear form readers are referred to gorelick 1982 mclay et al 2001 spalding and exner 1993 johnes 1996 reckhow and simpson 1980 among others for more detailed discussion and validation of linearity of the loading concentration relation these works show that the concentration evaluated at any point in space or time is a linear function of the initial concentration herein the nitrate loading provided that a concentration loading does not change the flow field concentrations of interest can be calculated simply by linearly scaling the concentration resulting from loading see discussion for details on circumstances under which this assumption may be violated it should be noted that simulation of a first order reaction rate to represent denitrification almasri and kaluarachchi 2007 weymann et al 2010 wilson et al 2018 hojberg et al 2017 tan et al 2004 carroll et al 2009 does not effect this linear relation 2 3 uncertainty quantification first order second moment fosm e g tarantola 2005 doherty 2015 techniques are adopted to propagate uncertainty from parameters to model outputs constraints the uncertainty for a given model output of interest can be calculated as 1 σ s 2 y t σ θ y where σ θ is a parameter covariance matrix u y is the vector of partial first derivatives relating changes in parameters to changes in the model output constraint s i e a row of j in equation 2 and σ s 2 is the prior variance of model output constraint s readers are referred to menke 1989 tarantola 2005 and doherty 2015 for more detail on the concepts surrounding fosm based uncertainty estimation in environmental modeling herein we employ only prior parameter covariance matrices and therefore are using only the prior model output constraint uncertainty fosm based posterior model output constraint uncertainty can be estimated with equation 1 by using a posterior parameter covariance matrix a follow on study will employ posterior uncertainties in the euu and ouu context the sensitivity vector y for each model output constraint of interest can be obtained in a non intrusive way by populating a jacobian sensitivity matrix j with finite difference approximate partial first derivatives 2 j c o n c e n t r a t i o n i p a r a m e t e r j c o n c e n t r a t i o n i p a r a m e t e r j δ c o n c e n t r a t i o n i δ p a r a m e t e r j where c o n c e n t r a t i o n i is the simulated concentration for a surface water reach or groundwater model node and p a r a m e t e r j is an uncertain parameter each row of j is a y vector 2 4 chance constraints once the uncertainty for a given model output constraint of interest has been estimated via equation 1 the technique of chance constraints wagner and gorelick 1987 white et al 2018 is adopted to yield deterministic yet risk based values for these simulated concentrations this is achieved by moving along the implied gaussian cumulative distribution function in accordance with a user specified risk value ranging from 0 0 to 1 0 in this way we can form a deterministic optimal nitrate loading change distribution or deterministically emulate the loading concentration relationship with uncertainty implicitly expressed therein i e perform ouu and euu respectively see wagner and gorelick 1987 white et al 2018 and references cited therein for more detailed descriptions of chance constraints fig 1 of white et al 2018 provides a graphical depiction of the chance constraint concept 2 5 stochastic impulse response emulator sire by applying the concepts related to linearity of the loading concentration relation and fosm techniques we present a stochastic impulse response emulator sire sire is a very efficient both in terms of the number of training model evaluations and execution time non intrusive and scalable means to achieve risk based evaluation of potential nitrate loading change scenarios in terms of both groundwater and surface water nitrate concentration changes sire can be used for risk based emulation of a given nitrate loading concentration change outcome or used in an ouu context to seek the optimal spatial distribution of changes in nitrate loading while respecting constraints of maximum allowable increase in surface water and or groundwater concentrations at the core of sire is a response matrix a e g ahlfeld and mulligan 2000 this matrix maps changes in decision variables to changes in model outputs constraints of interest and is analogous to the jacobian matrix from the parameter estimation literature e g doherty 2015 a defined as 3 a c o n c e n t r a t i o n i l o a d i n g j c o n c e n t r a t i o n i l o a d i n g j δ c o n c e n t r a t i o n i δ l o a d i n g j where l o a d i n g j is the nitrate loading rate in a model node j and c o n c e n t r a t i o n i is the concentration at a surface water or groundwater location at a specific time given the linear loading concentration relation a is independent of the current initial decision variable values as shown by gorelick and remson 1982 as long as the velocity field is not influenced by the introduction of nitrate the linear relation between nitrate loading into the hydrologic system and the resulting groundwater and or surface water concentration is linear in the context of formal sensitivity analysis the response matrix can be thought of as a local first order sensitivity analysis rakovec et al 2014 given the linear relation between land use nitrate loading and surface water and or groundwater concentrations the response matrix a can be used to emulate surface water and or groundwater concentration changes from the linked hydrologic model simply by multiplying a desired loading rate change vector by a following this multiplication the uncertainty of each of the model outputs constraints calculated with equation 1 can be used with a user specified risk to shift the simulated concentration change to the desired risk stance e g risk tolerant risk averse the entire sire emulation process requires a single matrix vector product and simple floating point arithmetic yet scales to incorporate large numbers of decision variables model outputs constraints and uncertain parameters 2 6 chance constrained linear programming the response matrix a in equation 3 can also be used to solve a linear programming lp problem of the form nocedal and wright 2006 4 maximize c t x subject to a x b x 0 where x is a vector of m decision variables herein nitrate loadings c is vector of m objective function coefficients or weights not to be confused with nitrate concentration c a is a response matrix previously defined and b is a vector of n specified constraint values herein changes in surface water and groundwater concentration that should not be exceeded using the lp problem implied by equation 4 in combination with the chance constraints in accordance with a specified level of risk constitutes ouu and is referred to as chance constrained linear programming cclp cclp yields an optimal spatial distribution of nitrate loading change that implicitly accounts for uncertainty constraints arising from parameter uncertainty by adjusting the constraint value to account for a user specified level of risk see white et al 2018 for further discussion 3 implementation to implement either euu or ouu with sire requires filling the response matrix a of equations 3 and 4 and the jacobian matrix j of equation 2 using finite difference approximations to partial first derivatives the model must be run once for each decision variable i e for each column of a and once for each parameter i e for each column of j note there is no computational penalty for including the simulated groundwater concentration at every active model node and the simulated surface water concentration in every active reach as constraints i e rows in a and j multiple simulation output times can also be included if the simulation of nitrate loading is transient herein all simulated output quantities are tracked so that the affect of potential new model output constraint locations can be evaluated without the requirement for populating a new a or j i e requiring re running the linked hydrologic model many times we use pestpp opt white et al 2018 to populate a and j matrices in a non intrusive parallelized fashion and to solve the cclp ouu problem the python van rossum 1995 code supporting modflow and mt3d flopy bakker 2014 was used to construct the linked hydrologic models the python code supporting pest pyemu white et al 2016 was used to construct the pest interface for these models recent modifications to pestpp opt allow the sire ouu problem to be solved without any additional model evaluations once the requisite a and j matrices have been filled the linear nature of the loading concentration relation allows modification of upper and lower bounds on decision variables linear relations between decision variables desired constraint values elements of b in equation 4 and the risk value to be altered and evaluated without requiring any additional model evaluations this allows decision makers the ability to rapidly evaluate the outcomes of possible resource management strategies and identify optimal risk based spatial nitrate loading change distribution in near real time during the decision making process 4 example applications the utility of sire in the context of euu and ouu is demonstrated for two example problems first euu is applied to a regional scale real world model of the hauraki plains new zealand yielding a risk based tool to efficiently evaluate how changes in nitrate loading associated with alternative management scenarios may affect nitrate concentrations in surface water and groundwater second cclp based ouu is applied to a complex synthetic model to demonstrate rapid identification of optimal land use practices 4 1 euu example 4 1 1 hauraki plains model description the sire euu technique was applied to a linked flow and transport model of the hauraki plains in new zealand fig 1 the model is described in white 2018 briefly the model has 7 layers 124 rows and 70 columns and uses a uniformly spaced 1 km grid modflow nwt niswonger et al 2011 was used to simulate steady state flow conditions mt3d usgs bedekar et al 2016 was used to simulate surface water and groundwater nitrate transport and groundwater nitrate first order decay over a period of 10 years while sire is completely general with respect to the choice of transport model time stepping herein we present outputs from a 10 year loading simulation for demonstration purposes the nitrate loading was treated as a spatially distributed mass loading rate in every active model node in the uppermost model layer nitrate is introduced into the groundwater system via a direct mass loading boundary condition this type of boundary condition does not affect the groundwater velocity field which is an important consideration to preserve the linear relation between land use loading and resulting groundwater surface water concentrations e g gorelick and remson 1982 denitrification was simulated using the rct package bedekar et al 2016 with a spatially variable first order decay rate based on the work of wilson et al 2018 the parameters used to represent model parameter uncertainty and hence comprise the σ θ matrix of equation 1 include horizontal and vertical hydraulic conductivity first order nitrate decay rate porosity initial nitrate concentration recharge rate groundwater abstraction rate conductance between surface water and groundwater spatially distributed properties and forcings are parameterized at the scale of the geologic model of white et al 2015 as zones of constant parameter values the inclusion of initial nitrate concentrations as parameters allows uncertainty in the legacy nitrate stored in the groundwater system to be propagated to the sire outputs a summary of the parameterization is presented in the supplementary material 4 1 2 sire demonstration a total of 3160 decision variables corresponding to nitrate loading rate in every active model node in uppermost layer are considered therefore the a matrix includes 3160 columns and 4703 rows one for each active surface water reach and groundwater node in the top layer at the end of the 10 year simulation period for the purposes of a demonstrating a sire based decision support tool the 3160 decision variables are grouped according to the spatial distribution of the five primary land use categories in the hauraki plains fig 1 during sire execution the simulated loading change for each active model node is scaled by the proportion of each land use category in the node in this way the decision making process can simulate effects of broad economic sector scale changes in nitrate loading from current conditions which are spatially distributed and vary at the node scale the hauraki plains sire was implemented in a jupyter notebook jupyter 2016 in this way decision makers and stakeholders can easily and efficiently evaluate the efficacy of a given sector scale nitrate loading change without needing to install any specialized software the sire implementation for the hauraki plains model executes in less than 10 s most of which is spent in the visualization of the results so the risk based surface water and groundwater nitrate concentration changes for a given sector scale nitrate loading change scenario can be rapidly evaluated compare this to the time and effort a practitioner must spend constructing datasets executing the forward model which takes approximately 20 min to run and then post process results into form for decision maker consumption figs 2 and 3 show images of input controls and resulting changes in input land use nitrate loading and emulated changes in surface water and groundwater nitrate concentrations using the sire demonstration notebook 4 1 3 sire verification the robustness of our implementation of sire was verified by comparing sire emulated concentrations to those from the linked hydrologic model that it emulates to this end a randomly generated node by node nitrate loading change scenario was constructed and evaluated with both the sire emulator and the modflow nwt mt3d usgs hauraki model to verify the deterministic portion of the sire process the concentration change outcomes yielded by sire the percent changes in nitrate loading inputs were generated from a multi variate uniform distribution u 0 8 1 2 3 160 this simulated change in nitrate loading represents a relatively large change in land use conditions compared to the range of changes allowed in the notebook implementation this test was designed to explore the extent of the linearity assumption in the emulation of loading concentration relation in a decision making context and also the verify performance of the sire implementation fig 4 presents the realized nitrate loading change scenario as well as the error in the emulated groundwater and surface water concentration changes in general the deterministic emulation agrees well with the simulated equivalent with the maximum error less than 0 6 percent over the range of simulated change in surface water and groundwater concentrations 2 55 mg l and 15 61 mg l respectively 4 2 ouu example 4 2 1 synthetic model description sire can also be used to optimize under uncertainty the spatial distribution of nitrate loading with respect to economic utility while meeting surface water concentration constraints we demonstrate this use of sire with a complex synthetic model the model comprises 3 layers 200 rows and 144 columns with a uniform horizontal grid discretization of 250 m modflow nwt niswonger et al 2011 was used to simulate steady state unsaturated groundwater and surface water flow conditions mt3d usgs bedekar et al 2016 was subsequently used to simulate nitrate transport over a 10 year period a schematic of the synthetic model is given in fig 5 spatially distributed nitrate inputs were time invariant and represented using a specified groundwater recharge concentration boundary condition fig 5 changes in land use nitrate loading were simulated by changing the concentration of the recharge flux nitrate transport was simulated explicitly within the groundwater domain through the surface water network and within the unsaturated zone a spatially variable first order denitrification rate was specified using the rct package 4 2 2 chance constrained linear programming problem definition the cclp ouu problem is characterized by the following decision variables nitrate concentration of groundwater recharge at every active uppermost layer model node within dairy land use areas this gives rise to 14 383 decision variables m the response matrix a therefore contains 14 383 columns relating nitrate concentration decision variables kg m3 to economic value is achieved using objective function coefficients see below direct constraints nitrate concentration decision variable values are allowed to vary between 0 and 0 04 kg n m3 40 mg l the upper bound value corresponds to a nitrate load of approximately 200 kg n ha yr which is considered a credible nitrate loading rate e g ledgard et al 1997 model derived constraints simulated surface water nitrate concentrations for all active reaches were tracked during population of a in this way the number and location of surface water concentration constraints used in the ouu solution can be easily changed by decision makers this strategy yields an a matrix with 1184 rows one for each surface water reach at the end of the 10 year simulation period the base case ouu scenario involves four surface water nitrate concentration constraints fig 5 risk neutral initial constraint values are based on the simulated surface water nitrate concentrations using initial maximum a priori parameter values objective function coefficients objective function coefficients comprising c in equation 4 are specified to scale the nitrate concentration based decision variables to monetary units this is achieved through i converting nitrate concentrations kg n m3 to annual nitrate masses kg n yr by multiplying each decision variable values i e recharge nitrate concentration in each node by the annual volumetric recharge rate of the corresponding node and ii subsequently converting the annual nitrate load to monetary values using a highly simplified approximation of the relationship between the nitrate mass applied and arising from a linearized cost abatement curve herein we assume that 1 kg n equates to 30 of cash operating profit national institute of water and atmospheric research 2010 the objective function is therefore given by φ c t x i 1 m w i d x i where w i is the annual volumetric recharge rate for a given decision variable and d is the kg n factor 30 we acknowledge the linearized cost abatement curve assumes the dairy producers are operating under a maximum profit objective and also ignores technological advances that may simultaneously improve the tradeoff between economic and environmental outcomes nevertheless this demonstrates the utility of sire to use a monetary basis for decision support and more complex external relationships linking cost to load can be easily implemented the parameters used to represent model input uncertainty and hence comprise the σ θ matrix of equation 1 are presented in the supplementary material a total of 802 parameters including horizontal and vertical aquifer conductivity surface water groundwater conductance boundary conductance effective porosity and denitrification rate are considered spatially distributed aquifer parameterization is achieved using pilot points doherty 2003 the implementation of the synthetic model sire based ouu like the earlier euu demonstration is presented in a jupyter notebook the sire ouu implementation executes in approximately 15 s this is considered very efficient given the size of this ouu problem which also requires considerable time for input output processes compare this with the approximately 30 min execution time of a single forward run of the synthetic model which then must be multiplied by both the number of nitrate loading decision variables and the number of adjustable parameters for the fosm calculations to yield comparable outputs from the sire process this example sire implementation allows decision makers and stakeholders to easily and quickly identify risk based optimal nitrate loading scenarios without relying on any specialized simulation software furthermore because the simulated surface water concentration for every reach was included in the a matrix constraints can easily be added and or removed and the ouu problem can be repeated without requiring any additional model evaluations 4 2 3 results fig 6 a shows how the optimal objective function varies with risk as expected the optimal objective function value reduces as the level of risk aversion increases in other words less money can be made from nitrate loading when considering a larger probability or chance that the true values of the concentration constraints are satisfied this trend continues until a certain point risk 0 57 where the ouu problem becomes infeasible i e there exists no portion of decision variable space that simultaneously satisfies all constraints at risk values exceeding 0 57 this constitutes a critical finding that it is not possible to be more than 57 confident all surface water concentration constraints are satisfied this infeasibility is attributable to the uncertainty in these simulated quantities estimated by fosm analysis for a strongly risk tolerant stance risk 0 05 the optimal nitrate input distribution corresponds to an objective function of about 78m larger than that achieved by taking a risk neutral stance risk 0 5 that is by considering a 5 chance that surface water concentration constraints are indeed satisfied compared to a risk neutral 50 chance a profit of 78m can be realized we acknowledge that a 5 risk tolerant solution is unlikely to be employed in a real world decision making context however this provides the opportunity for greater understanding of trade offs and also as an end member for stakeholder engagement for a maximally conservative or risk averse stance risk 0 57 the optimal objective function displays a reduction by some 64m compared to that for a risk neutral stance the cost of uncertainty in terms of the objective function arising from optimal nitrate loading is equivalent to 160m i e the objective function range across all feasible risk values in fig 6a fig 6b e shows how the optimal change in the spatial nitrate loading pattern varies with risk as well as with changes to the number of surface water concentration constraints as risk increases fig 6b d there is a significant reduction in the area displaying the maximum change in nitrate concentration value the shrinking of this region is most evident in proximity to and upgradient of constraint locations red triangles fig 6b e fig 6e shows how the optimal nitrate concentration distribution changes when an additional surface water concentration constraint red square fig 6e is introduced and when a risk neutral stance is taken the need to satisfy this additional constraint results in an optimal distribution displaying considerable differences in proximity to the constraint and an overall reduction in nitrate applied as reflected by a reduction in the optimal objective function value by 5m it is important to note that all of the land use ouu experimentation presented above were achieved without the need to re run the linked hydrologic model i e after the a and j matrices are populated each ouu solve took approximately 15 s this is considered extremely efficient given the size of this ouu problem 5 discussion this study has presented an efficient emulator sire that can effectively replace a complex linked hydrologic model that simulates the relation between nitrate loading changes and resulting surface water and groundwater nitrate concentration changes sire explicitly expresses and propagates model parameter uncertainty while also using the concept of risk and chance constraints e g wagner and gorelick 1987 white et al 2018 to yield a single answer as this is typically needed in the decision making context the desired level of risk can be adjusted at minimal computational cost for the purposes of evaluating explicit trade offs between the financial cost of reducing nitrate loading and the ecosystem benefit resulting from lower nitrate concentrations in surface waters the examples presented in this study display a reduction in the typical scenario evaluation and ouu workflow duration from days weeks to seconds these examples therefore demonstrate a highly effective means by which a complex numerical model and its associated model output uncertainty can be used to support real time interactive and spatially explicit decision making and resource management the computational savings afforded by sire ultimately make nitrate loading ouu possible during the course of the decision making process it is expected that the efficiency of sire can also be extended to other environmental management contexts where the relation between decision variables parameters and constraints can be approximated linearly the benefits of sire namely its efficiency scalability and non intrusiveness make it an attractive option for euu and ouu for decision support for example a form of sire has been developed by the authors for the purposes of rapid and risk based surface water depletion assessment and vulnerability mapping e g illangasekare and morel seytoux 1982 fienen et al 2018b additionally the relation between changes in groundwater abstraction and the resulting change in springflow and groundwater levels may also be a candidate relation for sire based emulation another possibility is the relation between groundwater abstraction and land subsidence ultimately further extensions of the sire approach depend on intelligent formulation of the emulated relation to ensure the first order approximation is representative of the simulated relations over the range of decision making interest the application of sire in the context of euu and ouu is demonstrated to scale effectively in both the decision variable space and the parameter space the hauraki plains sire implementation comprised 3160 decision variables and 1507 parameters requiring 4668 model runs to train the emulator the sire assisted ouu analysis for the synthetic model comprised 14 383 decision variables and 802 parameters requiring 15 185 runs for training the ratio between the number of model runs and the number of decision variables for training sire is considered to be few relative to alternative emulation strategies in the literature e g the gaussian process emulator of cui et al 2018 required 4000 model runs to train a deterministic emulator with 38 parameter dimensions as the number of decision variables increases so to does the number of forward simulations required to fill the reponse matrix at the core of the sire approach while the computing resources to evaluate the model hundreds of thousands of times do generally exist the ability to store the resulting response matrix in memory will become problematic in this setting users can formulated the pertubation runs needed to fill the columns of the response matrix as a parametric sweep and store the results as raw simulated outputs which can be written to file in blocks to minimize the memory usage the response coefficients can then be calculated on the fly during post processing the approach to euu and ouu was intentionally placed in the context of changes in nitrate loading and resulting model outputs rather than absolute outcomes for two reasons first providing emulated results in terms of differences cancels out any non linearities in the emulated loading concentration relation arising from non linear groundwater flow field effects this is especially important in transient flow models where seasonal fluctuation may for example affect surface water groundwater exchange patterns in non linear ways by utilizing the same groundwater flow field in both the base and the land use scenario case any non linearity induced in the relation between nitrate land use loading and resulting surface water groundwater concentrations is removed second providing emulated results as differences provides some defense against the ill effects of model error again as a result of cancellation e g white et al 2014 knowling et al 2019 additionally in many decision support settings the change in simulated emulated outcomes resulting from a change in model inputs can be more relevant than absolute simulated outputs as many policy and regulatory decisions are designed to affect change in environmental quality while sire could be applied to absolute outcomes rather than differences users must take care to ensure the linear relation is preserved that is that the groundwater and surface velocity vectors are not changing in time and that these velocity fields are not impacted by the change in land use nitrate loading e g gorelick and remson 1982 the efficiency and scalability of the sire approach is attributable to the use of the linearity assumption in both the relation between nitrate loading and surface water groundwater nitrate concentrations and the relation between parameters and surface water groundwater nitrate concentrations it has been demonstrated that for a relatively large nitrate loading change scenario the resulting maximum error in the context of the simulated range of concentration changes in the emulated surface water and groundwater concentrations is less than 0 6 of simulated range of concentration changes this provides further evidence for the validity of the linearity assumption between nitrate loading and concentrations i e in addition to that provided previously by e g gorelick and remson 1982 moosburner and wood 1980 spalding and exner 1993 mclay et al 2001 others have previously investigated the validity of the fosm assumptions used for uncertainty propagation in the context of linked hydrologic models e g dausman et al 2010 herckenrath et al 2011 these studies have shown that in general the results yielded by fosm are robust in a relative sense while it is recognized that the assumptions inherent to fosm analyses may make the calculated concentration uncertainties only estimates the ability to efficiently provide risk based euu and ouu outcomes resulting from high dimensional parameter space uncertainties far outweighs the approximate nature of the fosm estimates for the ouu analysis presented herein a linear relationship between nitrate loading mass and economic return was assumed based on work previously undertaken within the waikato region of new zealand national institute of water and atmospheric research 2010 this is however a gross simplification of complex dynamic feedbacks that exist within an economy a recent follow on work used ouu applied to a coupled hydrological model partial equilibrium farm systems model and wider computable general equilibrium cge model knowling et al 2020 this integrated approach investigated the risk based trade off between maximizing economic welfare across an entire economy and minimizing environmental impact as well as the validity or otherwise of the assumed linear model of economic value of nitrate there is an obvious reliance on the appropriateness of the underlying complex numerical model as to the robustness or otherwise of the sire emulated change in state variables therefore it is important to represent the underlying physical and biochemical processes appropriately for the purpose of the model as well as to implement appropriate and robust data assimilation history matching analyses 6 conclusions this study demonstrates sire a scalable risk based and non intrusive model emulation and optimization under uncertainty strategy combining two key simplifications firstly it exploits the linear relation between nitrate loading and surface water groundwater concentrations through the application of impulse response matrices in place of complex nutrient transport numerical models secondly fosm uncertainty estimation techniques are used to map uncertainty from model parameters to simulated surface water groundwater concentrations two integrated surface water groundwater nutrient transport models were used to demonstrate the utility of sire in euu and ouu contexts a complex synthetic model with 14 000 decision variables and a real world model hauraki plains new zealand with 3000 decision variables the emulated outputs from sire for the real world example were shown to be commensurate with the simulated outputs for a randomly generated nitrate loading scenario while taking less than 1 s to execute the synthetic ouu example demonstrated highly efficient exploration of risk as it relates to the optimal spatially distributed nitrate loading patterns subject to arbitrary and adjustable constraint locations and values both demonstrations were encoded into open source and user friendly interfaces serving as an example of how these euu and ouu strategies can be used to support decision making and for stakeholder engagement ultimately the sire approach facilitates risk based environmental resource management in near real time enabling decision makers to employ complex physically motivated models in ways not previously possible it is hoped that sire will provide a basis for better understanding of the relation between land use and environmental quality and ultimately better resource management decisions software availability the data files scripts and software used in the hauraki plains sire analysis are available at https doi org 10 5281 zenodo 3594085 and also at https github com jtwhite79 sire the data files scripts and software used in the synthetic model ouu are available at https doi org 10 5281 zenodo 3594091 and also at https github com mjknowling sire ouu the hauraki modflow nwt and mt3d usgs model files are available from waikato regional council upon request declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments and data availability we wish to acknowledge joon hwan kim market economics for assistance in formulating the linearized cost abatement curve we would also like to acknowledge brioch hemmings and zara rawlinson gns science for help building the hauraki plains model and john hadfield bevan jenkins and sung soo koh waikato regional council for providing several of the datasets for the hauraki plains model this research was performed as part of the smart models for aquifer management programme funded by the ministry of business innovation and employment new zealand as well as by a commercial contract with waikato regional council any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104657 
26062,use of physically motivated numerical models like groundwater flow and transport models for probabilistic impact assessments and optimization under uncertainty ouu typically incurs such a computational burdensome that these tools cannot be used during decision making the computational challenges associated with these models can be addressed through emulation in the land use water quality context the linear relation between nitrate loading and surface water groundwater nitrate concentrations presents an opportunity for employing an efficient model emulator through the application of impulse response matrices when paired with first order second moment techniques the emulation strategy gives rise to the stochastic impulse response emulator sire sire is shown to facilitate non intrusive near real time and risk based evaluation of nitrate loading change scenarios as well as nitrate loading ouu subject to surface water groundwater concentration constraints in high decision variable and parameter dimensions two case studies are used to demonstrate sire in the nitrate loading context keywords emulation under uncertainty groundwater modeling nitrate decision support optimization under uncertainty 1 introduction a sound understanding of how land use change impacts water quality for water resource and ecosystem sustainability is critically important for effective land use management the impact of land use based nitrate loading on groundwater and surface water water quality in particular has been the focus of many studies over the last few decades e g moosburner and wood 1980 spalding and exner 1993 schilling and libra 2000 mclay et al 2001 belhouchette et al 2011 green et al 2018 green et al 2016 garcía díaz 2011 ayub et al linked hydrologic flow and transport numerical models hereinafter referred to as linked hydrologic models are often used to simulate the movement and fate of nitrate within a hydrologic system for the purposes of i evaluating the outcome of potential land use change scenarios in terms of changes in surface water groundwater nitrate concentrations e g morgan et al 2007 and ii optimizing the spatial distribution of changes in nitrate loading given maximum allowable surface water and groundwater concentrations e g williams and hann 1978 a common linked hydrological model workflow arises from the need to manage land use practices and development such that the optimal balance between development maximized economic utility and nitrate concentrations at important locations in the hydrologic system can be achieved evaluating the relation between nitrate loading and concentration in the decision support setting typically involves manually modifying the linked hydrologic model inputs to represent a potential nitrate loading change scenario running the model forward to yield simulated surface water and or groundwater concentrations at locations of management or ecological interest as well as post processing model simulation results into a format for use in the decision support context this approach is inefficient because it requires an additional model run per scenario typically taking hours to days and affords additional opportunities for user error beyond these problems if decision makers are interested in the optimal spatial distribution of nitrate loading subject to simulated water quality constraints then hundreds to tens of thousands of model evaluations may be required for a single formal management optimization analysis furthermore if the model is being used to support risk based decision making meaning that uncertainty in the model outputs of interest should be quantified for the given nitrate loading scenario then the number of model evaluations needed may number again in the hundreds to tens of thousands or more for a single nitrate loading scenario these inefficiencies can hamper or even preclude the use of a linked hydrologic model in the decision making process this is an unfortunate outcome as the model is the best available tool to guide decision making in this context doherty 2015 one way in which the computational challenges of the traditional modeling workflow have been addressed is through model emulation strategies which involve deployment of simpler fast running models that emulate the relationship between inputs and outputs of the complex physically motivated linked hydrologic model e g asher et al 2015 for example ahlfeld and mulligan 2000 described the use of response matrices constructed on the basis of simulation model perturbations for solving a variety of groundwater optimization problems siade et al 1029 used proper orthogonal decomposition to replace the original complex model with a faster executing emulator laloy et al 2013 used polynomial chaos expansion to emulate the relation between parameters and state estimates for conditioning fienen et al 2016 explored statistical metamodels to emulate the source of water to wells while fienen et al 2018a uses similar statistical learning techniques for estimating groundwater age cui et al 2018 used a gaussian process to emulate the relation between parameters and state estimates while these techniques are viable approaches to emulating certain parts of an environmental modeling workflow these techniques do not simultaneously combine non intrusiveness i e model independence the ability to scale to high input dimensionality either algorithmically or in terms of training data requirements and the ability to explicitly propagate model input parameter uncertainty to the emulated model outputs furthermore many model emulation techniques do not facilitate emulating a spatially and or temporally discrete input output relation such as evaluating how altering a specific spatially distributed land use type may change nitrate concentrations at important locations in the hydrologic system e g ransom et al 2017 these qualities are important for an emulation strategy to be used in real world practice for interactive risk based resource management decision making a number of studies have demonstrated that under certain conditions the relationship between nitrate loading and surface water and groundwater nitrate concentrations hereinafter referred to as the loading concentration relation is linear e g spalding and exner 1993 mclay et al 2001 this linearity relates the nitrate load to concentrations throughout the domain given a non changing flowfield even though changes to the flowfield would impart nonlinearity on the response this linearity in the load presents an opportunity to employ an emulation strategy that is efficient scalable and non intrusive for evaluation and optimization of the loading concentration relation indeed many resource decisions are relevant within this limitation when considering the response of concentrations at a compliance point where only loads are managed for example the nitrate load cause by different intensities of cattle grazing may not impact the flowfield but only impacts the input concentration thus only altering the linear part of the relationship herein we present a strategy to exploit this linear relation in combination with a commonly employed uncertainty quantification technique to yield a highly efficient scalable and non intrusive approach to emulation under uncertainty euu and optimization under uncertainty ouu first the theories and previous works supporting our approach to euu and ouu are presented and summarized then the utility of emulating the loading concentration relation under uncertainty to support near real time decision making is shown finally we demonstrate the efficiency of ouu when employing the same emulation strategy to provide near real time optimal risk based spatial nitrate loading patterns 2 background and theory the work presented herein is premised on the efficiency gained through exploiting the linearity of the loading concentration relation we also employ an additional linearity assumption via first order second moment techniques to estimate uncertainty in the emulated surface water and groundwater concentrations arising from high dimensional model input uncertainty combining these two elements provides a basis for the development of tools that can deliver non intrusive near real time risk based management scenario outcomes to support decision making related to land use nitrate loading 2 1 terminology it is recognized that a number of concepts related to model emulation optimization and uncertainty quantification span different scientific disciplines and utilize different terminologies to avoid confusion definitions as they pertain to the current study are now provided parameters model inputs any numeric quantity used by the model that are uncertain and thus nominated for adjustment during uncertainty analysis and or history matching herein parameters include horizontal and vertical hydraulic conductivity porosity surface water conductance and nitrate first order reaction rate among others decision variables model inputs related to a resource management action whose values are the subject of management scenarios and formal optimization herein decision variables are the changes in nitrate loading rates to the groundwater system and are spatially distributed so that each active surficial model node is a decision variable model outputs simulated states from the model that are of interest in the decision making process herein the model outputs of interest are surface water and groundwater nitrate concentrations for each active reach and model node respectively these quantities are inherently uncertain because the inputs to the model are uncertain constraints conditions that must be satisfied by the optimization process constraints can take two forms i those directly related to decision variables e g minimum and maximum values for nitrate loading rate and ii those derived from model outputs e g simulated surface water and groundwater concentrations objective function functional comprised of linear combinations of decision variables that is either minimized or maximized during the optimization process herein the functional is the sum of the nitrate loading rate changes scaled by an economic factor to convert mass of nitrate introduced to the groundwater system into monetary units 2 2 linearity in the loading concentration relation the relation between nitrate loading changes decision variables and resulting surface water and groundwater concentration changes model outputs has been the topic of many investigations a large volume of literature demonstrates that this relation takes a linear form readers are referred to gorelick 1982 mclay et al 2001 spalding and exner 1993 johnes 1996 reckhow and simpson 1980 among others for more detailed discussion and validation of linearity of the loading concentration relation these works show that the concentration evaluated at any point in space or time is a linear function of the initial concentration herein the nitrate loading provided that a concentration loading does not change the flow field concentrations of interest can be calculated simply by linearly scaling the concentration resulting from loading see discussion for details on circumstances under which this assumption may be violated it should be noted that simulation of a first order reaction rate to represent denitrification almasri and kaluarachchi 2007 weymann et al 2010 wilson et al 2018 hojberg et al 2017 tan et al 2004 carroll et al 2009 does not effect this linear relation 2 3 uncertainty quantification first order second moment fosm e g tarantola 2005 doherty 2015 techniques are adopted to propagate uncertainty from parameters to model outputs constraints the uncertainty for a given model output of interest can be calculated as 1 σ s 2 y t σ θ y where σ θ is a parameter covariance matrix u y is the vector of partial first derivatives relating changes in parameters to changes in the model output constraint s i e a row of j in equation 2 and σ s 2 is the prior variance of model output constraint s readers are referred to menke 1989 tarantola 2005 and doherty 2015 for more detail on the concepts surrounding fosm based uncertainty estimation in environmental modeling herein we employ only prior parameter covariance matrices and therefore are using only the prior model output constraint uncertainty fosm based posterior model output constraint uncertainty can be estimated with equation 1 by using a posterior parameter covariance matrix a follow on study will employ posterior uncertainties in the euu and ouu context the sensitivity vector y for each model output constraint of interest can be obtained in a non intrusive way by populating a jacobian sensitivity matrix j with finite difference approximate partial first derivatives 2 j c o n c e n t r a t i o n i p a r a m e t e r j c o n c e n t r a t i o n i p a r a m e t e r j δ c o n c e n t r a t i o n i δ p a r a m e t e r j where c o n c e n t r a t i o n i is the simulated concentration for a surface water reach or groundwater model node and p a r a m e t e r j is an uncertain parameter each row of j is a y vector 2 4 chance constraints once the uncertainty for a given model output constraint of interest has been estimated via equation 1 the technique of chance constraints wagner and gorelick 1987 white et al 2018 is adopted to yield deterministic yet risk based values for these simulated concentrations this is achieved by moving along the implied gaussian cumulative distribution function in accordance with a user specified risk value ranging from 0 0 to 1 0 in this way we can form a deterministic optimal nitrate loading change distribution or deterministically emulate the loading concentration relationship with uncertainty implicitly expressed therein i e perform ouu and euu respectively see wagner and gorelick 1987 white et al 2018 and references cited therein for more detailed descriptions of chance constraints fig 1 of white et al 2018 provides a graphical depiction of the chance constraint concept 2 5 stochastic impulse response emulator sire by applying the concepts related to linearity of the loading concentration relation and fosm techniques we present a stochastic impulse response emulator sire sire is a very efficient both in terms of the number of training model evaluations and execution time non intrusive and scalable means to achieve risk based evaluation of potential nitrate loading change scenarios in terms of both groundwater and surface water nitrate concentration changes sire can be used for risk based emulation of a given nitrate loading concentration change outcome or used in an ouu context to seek the optimal spatial distribution of changes in nitrate loading while respecting constraints of maximum allowable increase in surface water and or groundwater concentrations at the core of sire is a response matrix a e g ahlfeld and mulligan 2000 this matrix maps changes in decision variables to changes in model outputs constraints of interest and is analogous to the jacobian matrix from the parameter estimation literature e g doherty 2015 a defined as 3 a c o n c e n t r a t i o n i l o a d i n g j c o n c e n t r a t i o n i l o a d i n g j δ c o n c e n t r a t i o n i δ l o a d i n g j where l o a d i n g j is the nitrate loading rate in a model node j and c o n c e n t r a t i o n i is the concentration at a surface water or groundwater location at a specific time given the linear loading concentration relation a is independent of the current initial decision variable values as shown by gorelick and remson 1982 as long as the velocity field is not influenced by the introduction of nitrate the linear relation between nitrate loading into the hydrologic system and the resulting groundwater and or surface water concentration is linear in the context of formal sensitivity analysis the response matrix can be thought of as a local first order sensitivity analysis rakovec et al 2014 given the linear relation between land use nitrate loading and surface water and or groundwater concentrations the response matrix a can be used to emulate surface water and or groundwater concentration changes from the linked hydrologic model simply by multiplying a desired loading rate change vector by a following this multiplication the uncertainty of each of the model outputs constraints calculated with equation 1 can be used with a user specified risk to shift the simulated concentration change to the desired risk stance e g risk tolerant risk averse the entire sire emulation process requires a single matrix vector product and simple floating point arithmetic yet scales to incorporate large numbers of decision variables model outputs constraints and uncertain parameters 2 6 chance constrained linear programming the response matrix a in equation 3 can also be used to solve a linear programming lp problem of the form nocedal and wright 2006 4 maximize c t x subject to a x b x 0 where x is a vector of m decision variables herein nitrate loadings c is vector of m objective function coefficients or weights not to be confused with nitrate concentration c a is a response matrix previously defined and b is a vector of n specified constraint values herein changes in surface water and groundwater concentration that should not be exceeded using the lp problem implied by equation 4 in combination with the chance constraints in accordance with a specified level of risk constitutes ouu and is referred to as chance constrained linear programming cclp cclp yields an optimal spatial distribution of nitrate loading change that implicitly accounts for uncertainty constraints arising from parameter uncertainty by adjusting the constraint value to account for a user specified level of risk see white et al 2018 for further discussion 3 implementation to implement either euu or ouu with sire requires filling the response matrix a of equations 3 and 4 and the jacobian matrix j of equation 2 using finite difference approximations to partial first derivatives the model must be run once for each decision variable i e for each column of a and once for each parameter i e for each column of j note there is no computational penalty for including the simulated groundwater concentration at every active model node and the simulated surface water concentration in every active reach as constraints i e rows in a and j multiple simulation output times can also be included if the simulation of nitrate loading is transient herein all simulated output quantities are tracked so that the affect of potential new model output constraint locations can be evaluated without the requirement for populating a new a or j i e requiring re running the linked hydrologic model many times we use pestpp opt white et al 2018 to populate a and j matrices in a non intrusive parallelized fashion and to solve the cclp ouu problem the python van rossum 1995 code supporting modflow and mt3d flopy bakker 2014 was used to construct the linked hydrologic models the python code supporting pest pyemu white et al 2016 was used to construct the pest interface for these models recent modifications to pestpp opt allow the sire ouu problem to be solved without any additional model evaluations once the requisite a and j matrices have been filled the linear nature of the loading concentration relation allows modification of upper and lower bounds on decision variables linear relations between decision variables desired constraint values elements of b in equation 4 and the risk value to be altered and evaluated without requiring any additional model evaluations this allows decision makers the ability to rapidly evaluate the outcomes of possible resource management strategies and identify optimal risk based spatial nitrate loading change distribution in near real time during the decision making process 4 example applications the utility of sire in the context of euu and ouu is demonstrated for two example problems first euu is applied to a regional scale real world model of the hauraki plains new zealand yielding a risk based tool to efficiently evaluate how changes in nitrate loading associated with alternative management scenarios may affect nitrate concentrations in surface water and groundwater second cclp based ouu is applied to a complex synthetic model to demonstrate rapid identification of optimal land use practices 4 1 euu example 4 1 1 hauraki plains model description the sire euu technique was applied to a linked flow and transport model of the hauraki plains in new zealand fig 1 the model is described in white 2018 briefly the model has 7 layers 124 rows and 70 columns and uses a uniformly spaced 1 km grid modflow nwt niswonger et al 2011 was used to simulate steady state flow conditions mt3d usgs bedekar et al 2016 was used to simulate surface water and groundwater nitrate transport and groundwater nitrate first order decay over a period of 10 years while sire is completely general with respect to the choice of transport model time stepping herein we present outputs from a 10 year loading simulation for demonstration purposes the nitrate loading was treated as a spatially distributed mass loading rate in every active model node in the uppermost model layer nitrate is introduced into the groundwater system via a direct mass loading boundary condition this type of boundary condition does not affect the groundwater velocity field which is an important consideration to preserve the linear relation between land use loading and resulting groundwater surface water concentrations e g gorelick and remson 1982 denitrification was simulated using the rct package bedekar et al 2016 with a spatially variable first order decay rate based on the work of wilson et al 2018 the parameters used to represent model parameter uncertainty and hence comprise the σ θ matrix of equation 1 include horizontal and vertical hydraulic conductivity first order nitrate decay rate porosity initial nitrate concentration recharge rate groundwater abstraction rate conductance between surface water and groundwater spatially distributed properties and forcings are parameterized at the scale of the geologic model of white et al 2015 as zones of constant parameter values the inclusion of initial nitrate concentrations as parameters allows uncertainty in the legacy nitrate stored in the groundwater system to be propagated to the sire outputs a summary of the parameterization is presented in the supplementary material 4 1 2 sire demonstration a total of 3160 decision variables corresponding to nitrate loading rate in every active model node in uppermost layer are considered therefore the a matrix includes 3160 columns and 4703 rows one for each active surface water reach and groundwater node in the top layer at the end of the 10 year simulation period for the purposes of a demonstrating a sire based decision support tool the 3160 decision variables are grouped according to the spatial distribution of the five primary land use categories in the hauraki plains fig 1 during sire execution the simulated loading change for each active model node is scaled by the proportion of each land use category in the node in this way the decision making process can simulate effects of broad economic sector scale changes in nitrate loading from current conditions which are spatially distributed and vary at the node scale the hauraki plains sire was implemented in a jupyter notebook jupyter 2016 in this way decision makers and stakeholders can easily and efficiently evaluate the efficacy of a given sector scale nitrate loading change without needing to install any specialized software the sire implementation for the hauraki plains model executes in less than 10 s most of which is spent in the visualization of the results so the risk based surface water and groundwater nitrate concentration changes for a given sector scale nitrate loading change scenario can be rapidly evaluated compare this to the time and effort a practitioner must spend constructing datasets executing the forward model which takes approximately 20 min to run and then post process results into form for decision maker consumption figs 2 and 3 show images of input controls and resulting changes in input land use nitrate loading and emulated changes in surface water and groundwater nitrate concentrations using the sire demonstration notebook 4 1 3 sire verification the robustness of our implementation of sire was verified by comparing sire emulated concentrations to those from the linked hydrologic model that it emulates to this end a randomly generated node by node nitrate loading change scenario was constructed and evaluated with both the sire emulator and the modflow nwt mt3d usgs hauraki model to verify the deterministic portion of the sire process the concentration change outcomes yielded by sire the percent changes in nitrate loading inputs were generated from a multi variate uniform distribution u 0 8 1 2 3 160 this simulated change in nitrate loading represents a relatively large change in land use conditions compared to the range of changes allowed in the notebook implementation this test was designed to explore the extent of the linearity assumption in the emulation of loading concentration relation in a decision making context and also the verify performance of the sire implementation fig 4 presents the realized nitrate loading change scenario as well as the error in the emulated groundwater and surface water concentration changes in general the deterministic emulation agrees well with the simulated equivalent with the maximum error less than 0 6 percent over the range of simulated change in surface water and groundwater concentrations 2 55 mg l and 15 61 mg l respectively 4 2 ouu example 4 2 1 synthetic model description sire can also be used to optimize under uncertainty the spatial distribution of nitrate loading with respect to economic utility while meeting surface water concentration constraints we demonstrate this use of sire with a complex synthetic model the model comprises 3 layers 200 rows and 144 columns with a uniform horizontal grid discretization of 250 m modflow nwt niswonger et al 2011 was used to simulate steady state unsaturated groundwater and surface water flow conditions mt3d usgs bedekar et al 2016 was subsequently used to simulate nitrate transport over a 10 year period a schematic of the synthetic model is given in fig 5 spatially distributed nitrate inputs were time invariant and represented using a specified groundwater recharge concentration boundary condition fig 5 changes in land use nitrate loading were simulated by changing the concentration of the recharge flux nitrate transport was simulated explicitly within the groundwater domain through the surface water network and within the unsaturated zone a spatially variable first order denitrification rate was specified using the rct package 4 2 2 chance constrained linear programming problem definition the cclp ouu problem is characterized by the following decision variables nitrate concentration of groundwater recharge at every active uppermost layer model node within dairy land use areas this gives rise to 14 383 decision variables m the response matrix a therefore contains 14 383 columns relating nitrate concentration decision variables kg m3 to economic value is achieved using objective function coefficients see below direct constraints nitrate concentration decision variable values are allowed to vary between 0 and 0 04 kg n m3 40 mg l the upper bound value corresponds to a nitrate load of approximately 200 kg n ha yr which is considered a credible nitrate loading rate e g ledgard et al 1997 model derived constraints simulated surface water nitrate concentrations for all active reaches were tracked during population of a in this way the number and location of surface water concentration constraints used in the ouu solution can be easily changed by decision makers this strategy yields an a matrix with 1184 rows one for each surface water reach at the end of the 10 year simulation period the base case ouu scenario involves four surface water nitrate concentration constraints fig 5 risk neutral initial constraint values are based on the simulated surface water nitrate concentrations using initial maximum a priori parameter values objective function coefficients objective function coefficients comprising c in equation 4 are specified to scale the nitrate concentration based decision variables to monetary units this is achieved through i converting nitrate concentrations kg n m3 to annual nitrate masses kg n yr by multiplying each decision variable values i e recharge nitrate concentration in each node by the annual volumetric recharge rate of the corresponding node and ii subsequently converting the annual nitrate load to monetary values using a highly simplified approximation of the relationship between the nitrate mass applied and arising from a linearized cost abatement curve herein we assume that 1 kg n equates to 30 of cash operating profit national institute of water and atmospheric research 2010 the objective function is therefore given by φ c t x i 1 m w i d x i where w i is the annual volumetric recharge rate for a given decision variable and d is the kg n factor 30 we acknowledge the linearized cost abatement curve assumes the dairy producers are operating under a maximum profit objective and also ignores technological advances that may simultaneously improve the tradeoff between economic and environmental outcomes nevertheless this demonstrates the utility of sire to use a monetary basis for decision support and more complex external relationships linking cost to load can be easily implemented the parameters used to represent model input uncertainty and hence comprise the σ θ matrix of equation 1 are presented in the supplementary material a total of 802 parameters including horizontal and vertical aquifer conductivity surface water groundwater conductance boundary conductance effective porosity and denitrification rate are considered spatially distributed aquifer parameterization is achieved using pilot points doherty 2003 the implementation of the synthetic model sire based ouu like the earlier euu demonstration is presented in a jupyter notebook the sire ouu implementation executes in approximately 15 s this is considered very efficient given the size of this ouu problem which also requires considerable time for input output processes compare this with the approximately 30 min execution time of a single forward run of the synthetic model which then must be multiplied by both the number of nitrate loading decision variables and the number of adjustable parameters for the fosm calculations to yield comparable outputs from the sire process this example sire implementation allows decision makers and stakeholders to easily and quickly identify risk based optimal nitrate loading scenarios without relying on any specialized simulation software furthermore because the simulated surface water concentration for every reach was included in the a matrix constraints can easily be added and or removed and the ouu problem can be repeated without requiring any additional model evaluations 4 2 3 results fig 6 a shows how the optimal objective function varies with risk as expected the optimal objective function value reduces as the level of risk aversion increases in other words less money can be made from nitrate loading when considering a larger probability or chance that the true values of the concentration constraints are satisfied this trend continues until a certain point risk 0 57 where the ouu problem becomes infeasible i e there exists no portion of decision variable space that simultaneously satisfies all constraints at risk values exceeding 0 57 this constitutes a critical finding that it is not possible to be more than 57 confident all surface water concentration constraints are satisfied this infeasibility is attributable to the uncertainty in these simulated quantities estimated by fosm analysis for a strongly risk tolerant stance risk 0 05 the optimal nitrate input distribution corresponds to an objective function of about 78m larger than that achieved by taking a risk neutral stance risk 0 5 that is by considering a 5 chance that surface water concentration constraints are indeed satisfied compared to a risk neutral 50 chance a profit of 78m can be realized we acknowledge that a 5 risk tolerant solution is unlikely to be employed in a real world decision making context however this provides the opportunity for greater understanding of trade offs and also as an end member for stakeholder engagement for a maximally conservative or risk averse stance risk 0 57 the optimal objective function displays a reduction by some 64m compared to that for a risk neutral stance the cost of uncertainty in terms of the objective function arising from optimal nitrate loading is equivalent to 160m i e the objective function range across all feasible risk values in fig 6a fig 6b e shows how the optimal change in the spatial nitrate loading pattern varies with risk as well as with changes to the number of surface water concentration constraints as risk increases fig 6b d there is a significant reduction in the area displaying the maximum change in nitrate concentration value the shrinking of this region is most evident in proximity to and upgradient of constraint locations red triangles fig 6b e fig 6e shows how the optimal nitrate concentration distribution changes when an additional surface water concentration constraint red square fig 6e is introduced and when a risk neutral stance is taken the need to satisfy this additional constraint results in an optimal distribution displaying considerable differences in proximity to the constraint and an overall reduction in nitrate applied as reflected by a reduction in the optimal objective function value by 5m it is important to note that all of the land use ouu experimentation presented above were achieved without the need to re run the linked hydrologic model i e after the a and j matrices are populated each ouu solve took approximately 15 s this is considered extremely efficient given the size of this ouu problem 5 discussion this study has presented an efficient emulator sire that can effectively replace a complex linked hydrologic model that simulates the relation between nitrate loading changes and resulting surface water and groundwater nitrate concentration changes sire explicitly expresses and propagates model parameter uncertainty while also using the concept of risk and chance constraints e g wagner and gorelick 1987 white et al 2018 to yield a single answer as this is typically needed in the decision making context the desired level of risk can be adjusted at minimal computational cost for the purposes of evaluating explicit trade offs between the financial cost of reducing nitrate loading and the ecosystem benefit resulting from lower nitrate concentrations in surface waters the examples presented in this study display a reduction in the typical scenario evaluation and ouu workflow duration from days weeks to seconds these examples therefore demonstrate a highly effective means by which a complex numerical model and its associated model output uncertainty can be used to support real time interactive and spatially explicit decision making and resource management the computational savings afforded by sire ultimately make nitrate loading ouu possible during the course of the decision making process it is expected that the efficiency of sire can also be extended to other environmental management contexts where the relation between decision variables parameters and constraints can be approximated linearly the benefits of sire namely its efficiency scalability and non intrusiveness make it an attractive option for euu and ouu for decision support for example a form of sire has been developed by the authors for the purposes of rapid and risk based surface water depletion assessment and vulnerability mapping e g illangasekare and morel seytoux 1982 fienen et al 2018b additionally the relation between changes in groundwater abstraction and the resulting change in springflow and groundwater levels may also be a candidate relation for sire based emulation another possibility is the relation between groundwater abstraction and land subsidence ultimately further extensions of the sire approach depend on intelligent formulation of the emulated relation to ensure the first order approximation is representative of the simulated relations over the range of decision making interest the application of sire in the context of euu and ouu is demonstrated to scale effectively in both the decision variable space and the parameter space the hauraki plains sire implementation comprised 3160 decision variables and 1507 parameters requiring 4668 model runs to train the emulator the sire assisted ouu analysis for the synthetic model comprised 14 383 decision variables and 802 parameters requiring 15 185 runs for training the ratio between the number of model runs and the number of decision variables for training sire is considered to be few relative to alternative emulation strategies in the literature e g the gaussian process emulator of cui et al 2018 required 4000 model runs to train a deterministic emulator with 38 parameter dimensions as the number of decision variables increases so to does the number of forward simulations required to fill the reponse matrix at the core of the sire approach while the computing resources to evaluate the model hundreds of thousands of times do generally exist the ability to store the resulting response matrix in memory will become problematic in this setting users can formulated the pertubation runs needed to fill the columns of the response matrix as a parametric sweep and store the results as raw simulated outputs which can be written to file in blocks to minimize the memory usage the response coefficients can then be calculated on the fly during post processing the approach to euu and ouu was intentionally placed in the context of changes in nitrate loading and resulting model outputs rather than absolute outcomes for two reasons first providing emulated results in terms of differences cancels out any non linearities in the emulated loading concentration relation arising from non linear groundwater flow field effects this is especially important in transient flow models where seasonal fluctuation may for example affect surface water groundwater exchange patterns in non linear ways by utilizing the same groundwater flow field in both the base and the land use scenario case any non linearity induced in the relation between nitrate land use loading and resulting surface water groundwater concentrations is removed second providing emulated results as differences provides some defense against the ill effects of model error again as a result of cancellation e g white et al 2014 knowling et al 2019 additionally in many decision support settings the change in simulated emulated outcomes resulting from a change in model inputs can be more relevant than absolute simulated outputs as many policy and regulatory decisions are designed to affect change in environmental quality while sire could be applied to absolute outcomes rather than differences users must take care to ensure the linear relation is preserved that is that the groundwater and surface velocity vectors are not changing in time and that these velocity fields are not impacted by the change in land use nitrate loading e g gorelick and remson 1982 the efficiency and scalability of the sire approach is attributable to the use of the linearity assumption in both the relation between nitrate loading and surface water groundwater nitrate concentrations and the relation between parameters and surface water groundwater nitrate concentrations it has been demonstrated that for a relatively large nitrate loading change scenario the resulting maximum error in the context of the simulated range of concentration changes in the emulated surface water and groundwater concentrations is less than 0 6 of simulated range of concentration changes this provides further evidence for the validity of the linearity assumption between nitrate loading and concentrations i e in addition to that provided previously by e g gorelick and remson 1982 moosburner and wood 1980 spalding and exner 1993 mclay et al 2001 others have previously investigated the validity of the fosm assumptions used for uncertainty propagation in the context of linked hydrologic models e g dausman et al 2010 herckenrath et al 2011 these studies have shown that in general the results yielded by fosm are robust in a relative sense while it is recognized that the assumptions inherent to fosm analyses may make the calculated concentration uncertainties only estimates the ability to efficiently provide risk based euu and ouu outcomes resulting from high dimensional parameter space uncertainties far outweighs the approximate nature of the fosm estimates for the ouu analysis presented herein a linear relationship between nitrate loading mass and economic return was assumed based on work previously undertaken within the waikato region of new zealand national institute of water and atmospheric research 2010 this is however a gross simplification of complex dynamic feedbacks that exist within an economy a recent follow on work used ouu applied to a coupled hydrological model partial equilibrium farm systems model and wider computable general equilibrium cge model knowling et al 2020 this integrated approach investigated the risk based trade off between maximizing economic welfare across an entire economy and minimizing environmental impact as well as the validity or otherwise of the assumed linear model of economic value of nitrate there is an obvious reliance on the appropriateness of the underlying complex numerical model as to the robustness or otherwise of the sire emulated change in state variables therefore it is important to represent the underlying physical and biochemical processes appropriately for the purpose of the model as well as to implement appropriate and robust data assimilation history matching analyses 6 conclusions this study demonstrates sire a scalable risk based and non intrusive model emulation and optimization under uncertainty strategy combining two key simplifications firstly it exploits the linear relation between nitrate loading and surface water groundwater concentrations through the application of impulse response matrices in place of complex nutrient transport numerical models secondly fosm uncertainty estimation techniques are used to map uncertainty from model parameters to simulated surface water groundwater concentrations two integrated surface water groundwater nutrient transport models were used to demonstrate the utility of sire in euu and ouu contexts a complex synthetic model with 14 000 decision variables and a real world model hauraki plains new zealand with 3000 decision variables the emulated outputs from sire for the real world example were shown to be commensurate with the simulated outputs for a randomly generated nitrate loading scenario while taking less than 1 s to execute the synthetic ouu example demonstrated highly efficient exploration of risk as it relates to the optimal spatially distributed nitrate loading patterns subject to arbitrary and adjustable constraint locations and values both demonstrations were encoded into open source and user friendly interfaces serving as an example of how these euu and ouu strategies can be used to support decision making and for stakeholder engagement ultimately the sire approach facilitates risk based environmental resource management in near real time enabling decision makers to employ complex physically motivated models in ways not previously possible it is hoped that sire will provide a basis for better understanding of the relation between land use and environmental quality and ultimately better resource management decisions software availability the data files scripts and software used in the hauraki plains sire analysis are available at https doi org 10 5281 zenodo 3594085 and also at https github com jtwhite79 sire the data files scripts and software used in the synthetic model ouu are available at https doi org 10 5281 zenodo 3594091 and also at https github com mjknowling sire ouu the hauraki modflow nwt and mt3d usgs model files are available from waikato regional council upon request declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments and data availability we wish to acknowledge joon hwan kim market economics for assistance in formulating the linearized cost abatement curve we would also like to acknowledge brioch hemmings and zara rawlinson gns science for help building the hauraki plains model and john hadfield bevan jenkins and sung soo koh waikato regional council for providing several of the datasets for the hauraki plains model this research was performed as part of the smart models for aquifer management programme funded by the ministry of business innovation and employment new zealand as well as by a commercial contract with waikato regional council any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104657 
26063,most of the commonly available sensitivity analysis methods cannot reliably compute the interaction effect even though the sobol type methods that use monte carlo simulation can evaluate the interaction effect the result is either inaccurate or requires an extraordinary number of model runs to obtain a reasonable estimate in this study we evaluate the sparse polynomial chaos spc method as a reasonable way to estimate the interaction effect this method is evaluated on two mathematical test functions ishigami and sobol g and two hydrologic models hbv sask and sac sma our results show the spc method needs about a sample size of 30 to 70 times the number of dimensions of the parameter space to evaluate the interaction effects of hydrologic models our findings are significant for hydrologic simulation and model calibration as we aim to improve the understanding of complex interactions among model components and to reduce model uncertainty keywords hydrologic model sensitivity analysis interaction effect sparse polynomial chaos 1 introduction as more hydrological models are being developed model structure is getting increasingly complex and in many cases those models are over parameterized schoups et al 2008 sensitivity analysis sa is an important tool for assessing parametric uncertainty of hydrological models as it can determine the degree of influence of parameters on model simulations improve model identifiability and ultimately enhance model performance song et al 2015 there are a number of ways that model parameters influence model simulations from a variance decomposition view angle razavi and gupta 2015 the impact of variation of a single parameter on model simulation is known as the first order effect or the main effect the effect of simultaneous variations of two or more parameters on model simulation is known as higher order effect the sum of all the effects connected with a specific parameter is called the total effect in practice only the main effect the total effect and the second order effect which is also known as interaction effect are evaluated as parameter interaction effect plays a big role in hydrological simulation capability and in robust parameter optimization accurate calculation of interaction effect of a hydrological model is vitally important the commonly used local sa lsa method can only be used to assess the main effect by varying a single model parameter at a time lsa cannot consider higher order effect and its estimate of the main effect is also not reliable li et al 2013 to assess higher order effects global sa gsa methods are required there are three types of gsa methods which that are widely applied in hydrological modeling derivative based gsa regression based gsa and variance based gsa wang et al 2016 most derivative based e g morris one at a time moat morris 1991 and regression based e g multivariate adaptive regression splines mars friedman 1991 gsa methods are effective in assessing the main effect however they cannot be used to assess higher order effects variance based gsa methods are designed to compute the main effect as well as higher order effects for example the classical sobol method one of the commonly used variance based gsa methods does a good job of assessing the main effect in a relatively efficient manner gan et al 2014 however the sobol method usually requires an extra ordinary number of model runs to obtain a reasonable estimate of the second order effect typically thousands to hundreds of thousands tang et al 2007 wan et al 2015 for large scale hydrological model applications the cost of performing second order or higher order sensitivity analysis using classical sobol method would be unbearable in order to reduce the number of model runs needed to calculate different sensitivity indices i e the main effect and interaction effect meta model based sa methods have been proposed meta model based approach is generally implemented in two steps first a surrogate model is constructed then sa is performed on the surrogate model the advantage of a meta model based approach is that it requires a significantly smaller number of model runs to perform sa than traditional sa methods to construct a surrogate model a design of experiment doe approach is usually used to sample the parameter space then the model is run using those sampled parameter sets and the corresponding performance metrics i e the output of interest are computed finally a surrogate model is constructed based on the parameter objective function statistical relationship there exist a plethora of statistical methods to construct a surrogate model razavi et al 2012a for example the aforementioned mars method svm support vector machine method gpr gaussian process regression method rf random forest and ann artificial neural network method meta model based sobol method that uses monte carlo simulation has been used to perform sa of hydrological models by numerous researchers song et al 2012a b zhan et al 2013 wang et al 2016 gan et al 2017 because monte carlo approximations of the integrals required in sobol method can lead to some numerical errors the estimate of the interaction effect based on meta model based approach can be inaccurate for example the second order effect computed by the classical sobol method sometimes results in negative values of sensitivity indices which defies the definition of variance terms which must be positive tang et al 2007 furthermore the estimation of interaction effect usually requires an extraordinary number of model runs to obtain a reasonable estimate of the interaction effect for example zhan et al 2013 studied the parameter interaction effects of the distributed time variant gain model dtvgm with 14 parameters using the meta model based sobol method and it required 2600 dtvgm model runs and 100 000 meta model runs 186 times the number of parameters to obtain a reasonable estimate of interaction effect song et al 2012a studied xinanjiang hydrological model 7 parameters using the sobol method and it costs 1000 xinanjiang model runs and 100 000 meta model runs 286 times the number of parameters to avoid negative sensitivity indices further there is no guarantee that the interaction effect computed by the meta model based sobol method that uses monte carlo simulation is accurate as we will show in this study generally sa based on meta models involves two kinds of numerical errors one is the error between the surrogate model and the real response surface of the physical models the other is the monte carlo simulation error the later error will converge to zero when the sample size is infinite the superposition of the two errors will have a significant impact on the confidence of the calculation of sobol global sensitivity indices on the contrary if the monte carlo simulation error can be avoided the accuracy of global sensitivity indices can be greatly improved in this paper we investigate the use of the sparse polynomial chaos spc sa method sudret 2008 blatman 2009 blatman and sudret 2011 fajraoui et al 2012 marelli and sudret 2014 tang and zhou 2015 hu and zhang 2016 shao et al 2017 to calculate the interaction effect of the hydrological model parameters if polynomial chaos expansion is used as a surrogate model the mean and variance of output can be calculated directly by the expansion coefficients as well as the global sobol sensitivity indices can be calculated directly in other words the spc method can avoid monte carlo simulation error directly the rest of paper is organized as follows section2 introduces the spc method in section3 by using spc method the ishigami function 3 tunable parameters sobol function 30 tunable parameters as the mathematical function model cases and the hbv sask model 10 tunable parameters and sac sma model 13 tunable parameters as the hydrological model cases are used for illustration section4 presents a detailed discussion about the interaction effects finally a short conclusion is given in section5 2 sparse polynomial chaos method for sensitivity analysis 2 1 full polynomial chaos firstly the full polynomial chaos method is reviewed considering a general second order stochastic process in the probability space ω p f with space of events ω σ algebra p and probability measure f let us consider a stochastic model y ξ with model input ξ ξ 1 ξ 2 ω and model output y for a stochastic analysis of y the model y ξ may be approached as follows 2 1 y ξ a 0 ψ 0 i 1 1 a i 1 ψ 1 ξ i 1 i 1 1 i 2 1 i 1 a i 1 i 2 ψ 2 ξ i 1 ξ i 2 i 1 1 i 2 1 i 1 i 3 1 i 2 a i 1 i 2 i 3 ψ 3 ξ i 1 ξ i 2 ξ i 3 where ψ i ξ is i order orthogonal polynomial basis functions in the variables ξ a 0 a i 1 is the approximation coefficient here the expansion bases ψ i ξ are multi dimensional orthogonal polynomials defined as tensor products of the corresponding one dimensional polynomials bases ϕ k k 0 2 2 ψ i ξ k 1 n ϕ i k ξ k i k 1 n i k where n is the number of random variables in realistic applications we usually consider a finite number of terms to truncate 2 1 and we have 2 3 y ξ α a p n a α ψ α ξ a p n α n n α p where α is a multi index that identifies the components of the multivariate polynomials and a is the set of selected multi indices of multivariate polynomials and a α is the corresponding polynomial chaos expansions coefficients the number of the polynomial expansion term n is related to the number of random space independent variables used in the stochastic process system and it is also related to the degree of freedom i e the maximum number of orders of the polynomial basis when the number of random variables and the degree of freedom are given the number of the polynomial expansion term n in the random process is as follows 2 4 n p n p n 1 where p is the degree of freedom and n is the number of random variables the expression eq 2 1 is called the full polynomial chaos for polynomial chaos methods there are three key steps they are respectively the determination of orthogonal polynomial basis of model input parameters the calculation of the coefficients of polynomial chaos and estimators of accuracy of the polynomial chaos approximations for the determination of orthogonal polynomial basis according to the wiener askey polynomial chaos xiu and karniadakis 2003 there exist different optimal polynomials for different probability density functions e g normalized legendre resp hermite polynomials can be associated to a uniform resp gaussian probability density functions in this work the model parameters are considered to be uniformly distributed and the legendre polynomial chaos are chosen in the calculation process of coefficients of the polynomial expansion two methods for intrusive method and non intrusive method can be used to determine the coefficients of polynomial expansion xiu 2010 the intrusive method needs to adjust the original model and non intrusive method does not need to adjust the original model code in this way the non intrusive method is suitable for the study of most models the non intrusive method has two main methods the regression method and the projection method blatman and sudret 2011 here the regression based non intrusive method is chosen in this work the coefficients may be estimated by determining the l 2 projection of the response y ξ onto the space spanned by the polynomials ψ α ξ α p as follows 2 5 a argmin a r n e y ξ a t ψ ξ given a sampling of size n of the input random vector χ ξ 1 ξ n t the design of experimental and the corresponding model responses y y 1 y n t the ordinary least square solution of eq 2 5 is 2 6 a a t a 1 a t y where a a i j ψ j ξ i i 1 n j 1 c a r d a is the so called experimental matrix that contains the values of all the basis polynomials in the experimental design points after the polynomial coefficients are computed through the error estimation the polynomial chaos based model with the appropriate accuracy is successfully established the leave one out loo error estimation is chosen in this work the leave one out loo cross validation error ϵ l o o is designed by using cross validation it consists in building n meta models y p c i each one created on a reduced experimental design χ ξ i ξ j j 1 n j i and comparing its prediction on the excluded point ξ i with the real value y i blatman and sudret 2010 the leave one out cross validation error can be written as 2 7 ϵ l o o i 1 n y ξ i y p c i ξ i 2 i 1 n y ξ i μ ˆ y 2 where μ ˆ y is the sample mean of the experimental design response in practice when the results of a least square minimization are available there is no need to explicitly calculate n separate meta models after some algebra eq 2 7 can reduce to 2 8 ϵ l o o i 1 n y ξ i y p c ξ i 1 h i 2 i 1 n y ξ i μ ˆ y 2 where h i is the i th diagonal term of matrix a a t a 1 a t matrix a is defined in eq 2 6 and y p c is the pc expansion built up from the full experimental design χ however note that the number of full pc coefficients n increases exponentially with n and p since eq 2 4 thus the number of coefficients to be computed increases dramatically when n is large say n 10 this is known as the curse of dimensionality for this issue it is solved satisfactorily using specific methods to compute sparse polynomial chaos 2 2 sparse polynomial chaos suppose a be a non empty finite subset of n n with which the truncated polynomial chaos can be defined by 2 9 y ξ α a a α ψ α ξ the common truncation scheme in eq 2 3 corresponds to the choice a a p n since the curse of dimensionality problem the determination of truncation sets a of small cardinality is of interest thus we define that if the following condition is verified the truncated pc eq 2 9 is sparse 2 10 i n c a r d a c a r d a p n 1 generally polynomial chaos can be sparse from two aspects hypothesis of the structure of polynomial chaos and the numerical algorithm for solving polynomial chaos from the point of view of the structure of polynomial chaos according to the sparsity of effects principle montgomery 2004 the hyperbolic truncation or maximum interaction can be used from the point of view of the solution algorithm of polynomial chaos the least angle regression lar based or orthogonal matching pursuit omp based solution algorithm can be applied blatman and sudret 2011 doostan and owhadi 2011 2 3 sparse polynomial chaos for sensitivity analysis finally when we obtain the coefficients of eq 2 9 successfully eq 2 9 can be interpreted as a model response surface for y ξ the uncertainty and sensitivity analysis can be calculated directly as simple analytical functions of the spc coefficients for the expectation and variance we have 2 11 e y ξ a 0 v a r y ξ α a α 0 a α 2 for the sensitivity indices the spc can be rewritten in the form of the sobol decomposition 2 12 y ξ a 0 i 1 1 n α i i 1 a α ψ α ξ i 1 i s i 1 n α i i 1 i s a α ψ α ξ i 1 ξ i s α i 1 n a α ψ α ξ where i i 1 i s α α 1 α n α k 0 k i 1 i s 1 n due to the orthogonal property of the polynomial basis the partial variance can be derived analytically from the spc coefficients as follows 2 13 d i 1 i n α i i 1 i n a α 2 so the partial sensitivity indices for the subset of input variables ξ i 1 ξ i n is as follows 2 14 s i 1 i n d i 1 i n d note that once a spc approximation of the model response has been built as a metamodel compared with all sobol type sa methods which require monte carlo simulation to calculate the sensitivity indices the spc based sensitivity indices are computed analytically from the spc coefficients which is of a negligible computational cost 3 evaluation of parameter interaction effect of the analytical test problems and the hydrological models in this section the efficiency and effectiveness of the spc sa method are evaluated firstly two well known mathematical functions are investigated the 3 dimensional ishigami function and the 30 dimensional sobol g function which have analytical expression for sensitivity indices then the spc sa method is used to examine the parameter interaction effect of two hydrological models in real world application settings i e the 10 dimensional hydrologiska byråns vattenbalansavdelning university of saskatchewan hbv sask hydrological model and the 13 dimensional sacramento soil moisture accounting sac sma hydrological model both of which have no analytical expression for sensitivity indices 3 1 analytical test models the ishigami and sobol g functions firstly the ishigami function is chosen as our test case ishigami and homma 1990 its expression is shown as follows 3 1 y x s i n x 1 a s i n 2 x 2 b x 3 4 s i n x 1 where the input variables x 1 x 2 and x 3 are uniformly distributed over π π the variance d of y and the sobol sensitivity indices can be computed analytically as follows 3 2 d a 2 8 b π 4 5 b 2 π 8 18 1 2 d 1 b π 4 5 b 2 π 8 50 1 2 d 2 a 2 8 d 3 0 d 12 d 23 0 d 13 8 b 2 π 8 225 in this example a 7 and b 0 1 for the sparse polynomial chaos the omp based calculation method of the coefficients is selected and the adaptive polynomial degree is set as 2 p 15 based on loo cross validation error estimates in addition the classical sobol method see appendixa the gaussian process regression based gpr sa method see appendixb and the polynomial chaos kriging based pc kring sa method see appendixc are used for comparison purpose the gaussian process regression and polynomial chaos kriging are the commonly used meta models wang et al 2014 kersaudy et al 2015 gong et al 2015 2016 table1 shows the root mean square errors rmse of the estimates of the total first order and second order sobol indices of spc gpr and pc kring methods against theoretical values for sample sizes n 20 n 40 and n 80 and sobol method for the sample size n 200 000 respectively the negative values calculated by the sobol method and meta model based sobol method are all set to zero in this work the second analytical test problem is the sobol g function saltelli and sobol 1995 whose expression is as follows 3 3 y x i 1 n 4 x i 2 a i 1 a i where the input variables x are uniformly distributed over 0 1 and a are nonnegative the variance d of y and the sobol sensitivity indices can be computed analytically as follows 3 4 d i 1 n d i 1 1 d i 1 3 1 a i 2 d t i d i j i 1 d j s i 1 i n 1 d j 1 n d i j in this example n 30 and a 1 2 5 10 20 50 100 500 999 999 for the sparse polynomial chaos the omp based calculation method of the coefficients is selected and the adaptive polynomial degree is set as 2 p 5 based on loo cross validation error estimates in addition the classical sobol method the gaussian process regression based sa method and the polynomial chaos kriging based sa method are still chosen for comparison table2 displays the rmses of the estimates of the total first order and second order sobol indices of spc gpr and pc kring methods against theoretical values for sample sizes n 150 n 300 and n 600 and sobol method for n 200 000 respectively from the sensitivity indices results of the ishigami table1 and sobol g functions table2 it is easy to see that the spc method is more accurate in calculating all second order effects while identifying total and first order effects in particular when the sample size of the ishigami function is 80 and the sample size of the sobol g function is 300 the rmses of their second order indices against theoretical indices reach 0 in addition as in the case of the ishigami test function we can see the importance of interaction effect the parameter x 3 is of zero main effect but has strong interaction with parameter x 1 under which situation interaction effect should not be ignored it is essential to monitor and evaluate the convergence rate of the gsa methods using some efficient techniques nossent et al 2011 razavi et al 2012b sarrazin et al 2016 harenberg et al 2019 sheikholeslami et al 2019 as it can enable us to diagnose the convergence behavior of the gsa thus in order to test the convergence behavior the robustness analysis is designed to estimate first and second order indices for a set of increasingly larger experimental designs by means of bootstrap method using 100 bootstrap replicates efron 1979 for the ishigami function and the sobol g function the results of the convergence of the estimates of the first order indices for two most sensitive parameters e g parameters x 1 and x 2 of the ishigami function parameters x 1 and x 2 of the sobol g function and the largest second order indices e g parameter pair x 13 of the ishigami function parameter pair x 12 of the sobol g function are shown in figs 1 and 2 respectively the sobol total effect indices are not shown because their convergence behavior is essentially identical to that of the first order indices by the convergence analysis confirming that the estimation of interaction effects is accurate enough to make a solid conclusion 3 2 evaluation of parameter interaction effect of the hbv sask hydrological model the hbv sask model fig 3 is a conceptual rainfall runoff model which was coded at the university of saskatchewan for educational purposes based on an interpretation of the hydrologiska byråns vattenbalansavdelning model lindström et al 1997 here ten of the hbv sask model parameters are considered tunable these parameters are described in table3 the rainfall runoff model hbv sask used in this study has 12 tunable parameters as presented in gupta and razavi 2018 razavi and gupta 2019 razavi et al 2019 here we consider only 10 parameters because parameter 11 is the base of unit hydrograph for watershed routing in day and its default is 1 for small watersheds parameter 12 is the precipitation multiplier to address uncertainty in precipitation and its default is 1 here we chose their default values in the hbv sask model the oldman watershed is chosen as the study area the 1434 73 km 2 oldman watershed is located in the rocky mountains of alberta canada historical data is available for the period 1979 2008 from which the average annual precipitation rainfall snowfall is estimated to be 611mm and average annual streamflow to be 11 7 m 3 s at gauge 05aa023 on the oldman river runoff ratio 0 42 razavi and gupta 2019 to evaluate model responses as a function of different parameter values the rmse values between the simulated and observed daily streamflow discharge m 3 s is used as the objective function 3 5 r m s e 1 n i 1 n q s t q o t 2 where q s t and q o t are simulated and observed streamflow discharge values at time t n is the total number of observations there is no analytical values for the sensitivity indices of the hbv sask hydrology model in a real world application setting to obtain a reasonable estimate of the true sensitivity indices we performed sa using the classical sobol method that employs 1000 000 monte carlo simulations mcs see appendixa for details to approximate the true sensitivity indices to demonstrate the convergence of the spc method a set of nested experimental designs of increasing sample sizes 100 200 300 400 500 700 1000 are generated based on the sobol sequence sampling for the sparse polynomial chaos selecting the adaptive polynomial degree 2 p 5 and omp based calculation method of the coefficients the selections of adaptive polynomial degree and calculation method are based on the leave one out error estimation of spc method the leave one out errors of the experimental designs 100 200 300 400 500 700 1000 are 5 96 e 12 4 45 e 15 1 54 e 12 5 02 e 13 1 34 e 13 4 38 e 14 and 8 45 e 14 respectively again the gpr sa method and pc kriging sa method are used for comparison purpose 3 2 1 effectiveness of the spc method in calculating interaction effects firstly the effectiveness in estimating the total and first order sa indices by different meta model based sa methods is investigated parameter sensitivity rankings of mcs based sobol method with 1000 000 model evaluations and spc based sobol method with 1000 model evaluations are given in fig 4 which shows that the three most sensitive parameters are parameters 7 5 and 2 respectively based on both the first order and total sa indices the spc based sobol method provides excellent matches with mcs based sobol method fig 5 displays the rmse values of the estimates of the first order and total sa indices calculated by spc gpr and pc kriging sa methods compared with the gpr and pc kriging methods the spc method has much smaller rmse values for both the first order and total effects than the other two methods next the effectiveness in estimating the second order sa indices is considered the second order results for the mcs based sobol and spc based sobol methods are given in fig 6 which shows that parameter pairs 5 7 2 7 1 2 and 2 5 have the largest interaction effects note that when the sample size for the spc method reaches 300 or more the results of spc based and mcs based sobol methods are very similar fig 7 exhibits the rmse values of the estimates of the second order sensitivity indices calculated by spc gpr and pc kriging methods respectively from fig 7 the spc method shows a better convergence behavior than the gpr and pc kriging methods in addition for the training time of various methods the time consumption of spc method is the least for sample sizes 100 300 500 700 1000 the spc method can save 10 4 5 4 2 times against the gpr method and 12 9 13 7 11 times against the pc kriging method respectively the interaction between tt parameter 1 the air temperature threshold about melting freezing state of precipitation and co parameter 2 the base melt factor of snow is due to the melting freezing state of water before and after reaching ground co also has a strong correlation with fc parameter 5 the soil field capacity and frac parameter 7 the fraction of water released from soil to fast reservoir the reason of strong interaction between co fc and frac is that these three parameters control the state of water in this watershed frozen in snowpack stored in soil or flowing in the river channel these three parameters are also the most important parameters controlling the whole hydrological process in this watershed parameter tt only interacts with co as both of them are related to the process in snow cover 3 2 2 robustness of the spc method in calculating interaction effects the robustness of the spc method using different samples sizes in obtaining reliable estimates of different sa indices is analyzed in order to test the convergence behavior a robustness study is still designed to estimate first and second order indices for a set of increasingly larger experimental designs with a maximum of n 1000 confidence bounds for each estimate are calculated by means of a bootstrap method using 100 bootstrap replicates the results of the convergence of the estimates of the first order indices for three most sensitive parameters e g parameters 7 5 2 are shown in fig 8 violin graph the sobol total effect indices are not shown because their convergence behavior is essentially identical to that of the first order indices from fig 8 it is clear that reasonably good estimates of the first order indices are already obtained with 200 model evaluations i e the sample size is 20 times as large as the number of parameters for the robustness of the second order sa indices estimates the convergence behavior of the estimates of the sobol second order indices as a function of the experimental design sizes is shown in fig 9 for the four parameter pairs with the largest second order sobol indices e g pairs 5 7 2 7 1 2 2 5 fig 9 shows clearly that reasonably good estimates of the second order indices can be obtained with 300 model evaluations i e the sample size is 30 times as large as the number of parameters 3 3 evaluation of parameter interaction effect of the sac sma hydrological model the sac sma hydrological model fig 10 has a highly non monotonic non linear input parameter model output relationship this model is the most widely used hydrological model by the river forecast centers of the u s national weather service for catchment modeling and flood forecasting burnash et al 1973 here thirteen of the sac sma model parameters are considered tunable brazil 1988 these parameters are described in table4 the south branch potomac river basin near springfield west virginia in the u s is chosen as the study area historical precipitation potential evapotranspiration and streamflow observations from january 1 1960 to december 31 1979 are obtained from the model parameter estimation experiment mopex database for this study duan et al 2006 the hydrological simulations were run at a 6h time step over the entire data period the average annual runoff is 39 5 m 3 s average annual potential evapotranspiration is 762mm and average annual precipitation over this period is 1021mm wang et al 2014 to evaluate model responses under different parameters the root mean square error rmse is chosen as the objective function similarly we still performed sa using the classical sobol method that employs 1000 000 monte carlo simulations mcs to approximate the true sensitivity indices to demonstrate the convergence of the spc method a set of nested experimental designs of increasing size 130 260 390 520 650 910 1300 is generated based on the sobol sequence sampling for the sparse polynomial chaos selecting the adaptive polynomial degree 2 p 5 and omp based calculation method of the coefficients the leave one out errors of the experimental designs 130 260 390 520 650 910 1300 are 1 28 e 10 3 68 e 16 3 71 e 11 1 52 e 12 2 16 e 12 6 41 e 13 and 1 59 e 12 respectively again the gpr sa method and pc kriging sa method are still used for comparison purpose 3 3 1 effectiveness of the spc method in calculating interaction effects firstly the effectiveness in estimating the total and first order sa indices by different meta model based sa methods is investigated parameter sensitivity rankings of mcs based sobol method with 1000 000 model evaluations and spc based sobol method with 1300 model evaluations are given in fig 11 which shows that the two most sensitive parameters are parameters 5 and 4 respectively based on both the first order and total sa indices the spc based sobol method provides excellent matches with mcs based sobol method fig 12 displays the rmse values of the estimates of the first order and total sa indices calculated by spc gpr and pc kriging sa methods compared with the gpr and pc kriging methods spc method is also effective for identifying first and total order effects next the effectiveness in estimating the second order sa indices is considered the second order results for the mcs based sobol and spc based sobol methods are given in fig 13 which shows that parameter pairs 9 11 10 11 and 5 8 have the largest interaction effects note that when the sample size for the spc method reaches 910 or more the results of spc based and mcs based sobol methods are very similar fig 14 exhibits the rmse values of the estimates of the second order sensitivity indices calculated by spc gpr and pc kriging methods respectively similarly the spc method shows a better convergence behavior than the gpr and pc kriging methods in addition for the training time of various methods the time consumption of spc is the least for sample sets 130 390 650 910 1300 the spc method can save 2 3 2 1 1 times against the gpr method and 5 5 2 2 1 times against the pc kriging method respectively the lzpk parameter 11 which is about the lateral drainage rate of lower zone supplementary free water has significant interaction with the lsfsm parameter 9 and lzfpm parameter 10 because the three parameters are about the free water in the lower zone lzpk controls the drainage rate while lzfpm controls the storage capacity of the lower zone primary free water slower lzpk also have interactions with lzfsm which controls the storage of supplementary free water faster the strong interactions between the three parameters indicated that these processes are highly correlated and with only the observation of streamflow it will be not easy to identify the values of them the interaction between adimp parameter 5 and lztwm parameter 8 as well as many other moderate interaction effects indicated that in sac sma there are many not overwhelmingly strong but nonnegligible interactions that make a great challenge to parameter calibration the interaction effects are the inherent reason of equifinality and the quantification of interaction effects will provide a lot of useful information to understand the uncertainty of hydrological models 3 3 2 robustness of the spc method in calculating interaction effects the robustness of the spc method using different samples sizes in obtaining reliable estimates of different sa indices is analyzed in order to test the convergence behavior a robustness study is designed to estimate first and second order indices for a set of increasingly larger experimental designs with a maximum of n 2000 confidence bounds for each estimate are calculated by means of bootstrap method using 100 bootstrap replicates the results of the convergence of the estimates of the first order indices for six most sensitive parameters e g parameters 5 4 1 8 12 and 11 are shown in fig 15 violin graph similarly the sobol total effect indices are not shown because their convergence behavior is essentially identical to that of the first order indices from fig 15 it is clear that reasonably good estimates of the first order indices are already obtained with 390 model evaluations i e the sample size is 30 times as large as the number of parameters for the robustness of the second order sa indices estimates the convergence behavior of the estimates of the sobol second order indices as a function of the experimental design sizes is shown in fig 16 for the three parameter pairs with the largest second order sobol indices e g pairs 9 11 10 11 and 5 8 fig 16 shows clearly that reasonably good estimates of the second order indices can be obtained with 910 model evaluations i e the sample size is 70 times as large as the number of parameters 4 discussion as discussed in razavi and gupta 2015 the variance based approach is the most commonly used approach that provides a global measure of interaction effects the sobol type sensitivity analysis method is a variance based sa method however the way that sobol sa indices are computed can lead to negative variances as the formulation contains pluses and minuses terms due to sampling errors especially when the sample size is relatively small the minus terms can dominate and lead to negative values this work conducted an evaluation of parameter interaction effect based on sobol s definition of sa indices the second order interaction effects based on two meta model based sobol methods evaluated in this study i e gpr and pc kriging can result in negative values in sa indices this is not an artifact of the anova decomposition but rather is due to the sampling errors that are inherent in monte carlo simulations in those sobol methods the spc sa method presented in this study does not have this problem and provides more reliable estimates of the sa indices with the knowledge of not only the main effects but also the interaction effects the parameters can be optimized more accurately as in the case of the ishigami test function in this paper parameter x 3 is of zero main effect but has strong interaction with parameter x 1 under which situation interaction effects should not be ignored shi et al 2019 have shown that in analyzing the sensitivity of parameters of the earth system model of intermediate complexity loveclim the parameter interaction effects cannot be ignored in identifying the optimal model parameters their results showed that all ocean related parameters have shown only a little sensitivity in the time scale of thousands of years but as the ocean processes control the climate variability through their interactions with other earth system components such as land and atmospheric systems the ocean related parameters should be considered in parameter optimization huang et al 2018 who examined the parameter optimization of a single column community atmosphere model pointed out that the optimization metrics improved by 67 by considering interaction effects the sensitivities of model parameters are dependent on the choice of objective functions here the root mean square error rmse is chosen as the objective function which is commonly used in model calibration parameter identification the role of objective function s selection on the implementation and interpretation of parameter interaction evaluation is vital rmse depicts different behavior compared to other objective functions such as nash sutcliffe efficiency coefficient nse and kling gupta efficiency coefficient kge that would yield different results in recent studies gupta and razavi 2018 razavi and gupta 2019 the filtering role of objective functions when used in sensitivity analysis was discussed because sensitivity analysis is not only for parameter optimization it can have other applications one of the applications is to support a better understanding of the model behaviors for example interaction effects can improve understanding of the interaction processes and mechanisms in model simulations as discussed in razavi et al 2012b and harenberg et al 2019 due to the randomness inherent in does a robust numerical assessment is needed for a meta modeling method therefore for the robustness of the spc method in calculating interaction effects we apply the bootstrap resampling technique efron 1979 to spc method because the bootstrap resampling technique can easily be performed without adding to the overall computing cost of gsa the efficiency analysis does not require extra model evaluations our results show that the spc method needs about 300 samples 30 times the parameter dimension and 910 samples 70 times the parameter dimension to evaluate the interaction effects of the hbv sask and sac sma models respectively the computational cost for interaction effects can depend on the number of interaction effects for a n dimensional model its number of interaction effects is n n 1 2 the more the number of interaction effects the more complex the model may be the complexity of the original model structure is an important factor affecting the performance of meta modeling therefore the computational cost of interaction effects is closely related to the number of model parameters from the results of robustness assessment as the sample size increases the confidence intervals of interaction effects narrow it shows the robustness of our results by the robustness assessment and convergence analysis confirming that the estimation of interaction effects is accurate enough to make a solid conclusion in addition the impact of employing different sampling strategies on the results of sensitivity analysis is different here the sobol sequence sampling is chosen to construct a surrogate model wang et al 2014 finally it must be noted that a limitation of sobol based method is that it only exists in the case of independent input variables when the parameters are correlated the so called analysis of covariance ancova method may be needed to generalize the sobol decomposition for models with correlated inputs to calculate sensitivity indices caniou 2012 5 conclusion in this work the second order interaction effects of model parameters are studied the spc sa method was tested on four case studies including two mathematical functions namely the 3 dimensional ishigami and 30 dimensional sobol g function and two hydrological models namely 10 dimensional hbv sask model and 13 dimensional sac sma model compared with the classical sobol and meta model based sobol sa methods the effectiveness and efficiency of the spc method in the second order sensitivity analysis are shown the investigation we have carried out leads to reference takeaways for the analysis of hydrological models as well as for the practice of sensitivity analysis 1 which method can calculate all the interaction effects effectively and 2 how much computational cost is required to achieve the robust interaction effects when the expansion of spc is available the advantage of spc method is that the sobol indices at any order may be computed analytically hence as one of meta model based sa the approach not only avoids expensive computing but also avoids sampling errors from the monte carlo simulation the spc method is shown to be one effective and efficient sensitivity analysis method for calculating interaction effects it has much potential for further applications for instance the parameters may be optimized reasonably by combining main effects and interaction effects further studies should be as well focused on general application to other uncertainty quantification cases such as large complex dynamical system models which would be of great significance on practical application declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors gratefully acknowledge the support of the national basic research program of china no 2015cb953703 the strategic priority research program of the chinese academy of sciences no xda19070104 xda20060401 the state key laboratory of earth surface processes and resource ecology no 2017 kf 05 the fundamental research funds for the central universities beijing normal university research fund no 2015kjjca04 and the special fund for meteorological scientific research in public interest no gyhy201506002 mr heng wang gratefully acknowledges the scholarship support provided by china scholarship council no 201806040132 we thank the editor holger maier for his work we thank razavi saman and three anonymous reviewers for their insightful comments and suggestions that helped improve the manuscript in this study the experimental designs and sensitivity analysis methods we used come from uq pyl software package http www uq pyl com and uqlab software package https www uqlab com the hbv sask rainfall runoff model and data we used come from vars tool software package http vars tool com the sac sma rainfall runoff model and data we used come from uq pyl appendixa sobol sensitivity analysis method in sobol method sobol 1993 the variance of the model output is decomposed into components that result from individual parameters as well as parameter interactions conventionally the direct model output is replaced by a model performance measure such as rmse as used in this study the sensitivity of each parameter or parameter interaction is then assessed based on its contribution measured as a percentage to the total variance computed using a distribution of model responses assuming the parameters are independent the sobol variance decomposition is a 1 d y i d i i j d i j i j k d i j k d 12 n where d i is the measure of the sensitivity to model output y due to the i th component of the input parameter vector d i j is the portion of output variance that results due to the interaction between parameters the variable n defines the total number of parameters the sensitivity of parameters and interaction between parameters is obtained by normalizing the above formula a 2 1 i d i d y i j d i j d y i j k d i j k d y d 12 n d y the variance decomposition shown in eq a 2 can be used to define the sensitivity indices of different orders as a 3 s i d i d s i j d i j d s t i 1 d i d where s i is the first order sensitivity index main effect s i j is the second order sensitivity index interaction effect and s t i is the total order sensitivity index total effect the original sobol method required n 2 m 1 model runs to calculate all the first second order and total order sensitivity indices an enhancement of the method made by saltelli 2002 provides the first second and total order sensitivity indices using n 2 m 2 model runs in this study this modified version of sobol methodology is chosen to compute the first order second order and total order indices appendixb gaussian process regression based sobol sensitivity analysis method gaussian process regression gpr is a machine learning method based on statistical learning theory and bayesian theory a gaussian process regression meta model is described by the following equation santner et al 2003 rasmussen and williams 2006 dubourg 2011 lataniotis et al 2018 b 1 y x β t f x σ 2 z x ω the first term in eq b 1 β t f x is the mean value of the gaussian process i e trend and it consists of the regression coefficients β j j 1 p and the basis functions f j j 1 p the second term in eq b 1 consists of σ 2 the variance of the gaussian process and z x ω a zero mean unit variance stationary gaussian process the z x is fully determined by the auto correlation function between two input sample points r x x r x x θ due to stationarity where θ are hyper parameters to be computed the gaussian assumption states that the vector formed by the true model responses y and the prediction y ˆ x has a joint gaussian distribution defined by b 2 y ˆ x y n n 1 β f x t β f σ 2 1 r t x r x r where f is the information matrix of generic terms f i j f j x i i 1 n j 1 p r x is the vector of cross correlations between the prediction point x and each one of the observations whose terms read r i r x x i θ i 1 n r is the correlation matrix whose terms read r i j r x i x j θ i j 1 n then the mean and variance of the gaussian random variate y x a k a mean and variance of the gpr predictor can be calculated b 3 μ y ˆ x β f x t r x t r 1 y f β b 4 σ y ˆ 2 x σ 2 1 r x t r 1 r x u x t f t r 1 f 1 μ x where β f t r 1 f 1 f t r 1 y is the generalized least squares estimate of the underlying regression problem and u x f t r 1 r x f x then predictions for new points can be made in terms of the mean and variance of y ˆ x using eqs b 3 and b 4 finally combining with sobol method gaussian process regression based sensitivity analysis method can be carried out the sobol sensitivity indices can be calculated by means of samples of input parameters and the mean values of gpr output it needs to be estimated at the cost of n n 2 m 2 model evaluations when using sobol sampling in this work for the selection of correlation function and trend for gaussian process regression the matérn 5 2 covariance kernel is considered with a constant yet unknown trend the hyper parameters are estimated with the maximum likelihood method and are solved by using the hybrid genetic algorithm we select about 200 000 samples to calculate the sensitivity indices through sobol sampling method appendixc polynomial chaos kriging based sobol sensitivity analysis method kriging a k a gaussian process regression interpolates the local variations of y as a function of the neighboring experimental design points whereas polynomial chaos approximates well the global behavior of y by combining the global and local approximation of these techniques the polynomial chaos kriging metamodel is achieved the polynomial chaos kriging pc kriging is defined as a universal kriging model the trend of which consists of a set of orthonormal polynomials kersaudy et al 2015 schobi et al 2019 c 1 y x α a a α ψ α ξ σ 2 z x ω where α a a α ψ α ξ is a weighted sum of orthonormal polynomials describing the trend of the pc kriging model σ 2 and z x ω denote the variance and the zero mean unit variance stationary gaussian process respectively as introduced in appendixb hence pc kriging can be interpreted as a universal kriging model with a specific trend constructing a pc kriging model consists of two parts the determination of the optimal set of polynomials contained in the trend and the calibration of the kriging model the two parts can be combined in various ways sequential pc kriging and optimal pc kriging can be implemented finally combining with sobol method gaussian process regression based sensitivity analysis method can be carried out the sobol sensitivity indices can be calculated by means of samples of input parameters and the mean values of pc kriging output it needs to be estimated at the cost of n n 2 m 2 model evaluations when using sobol sampling in this work the sequential pc kriging is chosen the optimal set of polynomials is determined by sparse polynomial chaos based on least angle regression lar and the adaptive polynomial degree is set as 2 p 5 for the selection of correlation function and trend for gaussian process regression the matérn 5 2 covariance kernel is considered with a constant yet unknown trend the hyper parameters are estimated with the maximum likelihood method and are solved by using the hybrid genetic algorithm similarly we select about 200 000 samples to calculate the sensitivity indices through sobol sampling method 
26063,most of the commonly available sensitivity analysis methods cannot reliably compute the interaction effect even though the sobol type methods that use monte carlo simulation can evaluate the interaction effect the result is either inaccurate or requires an extraordinary number of model runs to obtain a reasonable estimate in this study we evaluate the sparse polynomial chaos spc method as a reasonable way to estimate the interaction effect this method is evaluated on two mathematical test functions ishigami and sobol g and two hydrologic models hbv sask and sac sma our results show the spc method needs about a sample size of 30 to 70 times the number of dimensions of the parameter space to evaluate the interaction effects of hydrologic models our findings are significant for hydrologic simulation and model calibration as we aim to improve the understanding of complex interactions among model components and to reduce model uncertainty keywords hydrologic model sensitivity analysis interaction effect sparse polynomial chaos 1 introduction as more hydrological models are being developed model structure is getting increasingly complex and in many cases those models are over parameterized schoups et al 2008 sensitivity analysis sa is an important tool for assessing parametric uncertainty of hydrological models as it can determine the degree of influence of parameters on model simulations improve model identifiability and ultimately enhance model performance song et al 2015 there are a number of ways that model parameters influence model simulations from a variance decomposition view angle razavi and gupta 2015 the impact of variation of a single parameter on model simulation is known as the first order effect or the main effect the effect of simultaneous variations of two or more parameters on model simulation is known as higher order effect the sum of all the effects connected with a specific parameter is called the total effect in practice only the main effect the total effect and the second order effect which is also known as interaction effect are evaluated as parameter interaction effect plays a big role in hydrological simulation capability and in robust parameter optimization accurate calculation of interaction effect of a hydrological model is vitally important the commonly used local sa lsa method can only be used to assess the main effect by varying a single model parameter at a time lsa cannot consider higher order effect and its estimate of the main effect is also not reliable li et al 2013 to assess higher order effects global sa gsa methods are required there are three types of gsa methods which that are widely applied in hydrological modeling derivative based gsa regression based gsa and variance based gsa wang et al 2016 most derivative based e g morris one at a time moat morris 1991 and regression based e g multivariate adaptive regression splines mars friedman 1991 gsa methods are effective in assessing the main effect however they cannot be used to assess higher order effects variance based gsa methods are designed to compute the main effect as well as higher order effects for example the classical sobol method one of the commonly used variance based gsa methods does a good job of assessing the main effect in a relatively efficient manner gan et al 2014 however the sobol method usually requires an extra ordinary number of model runs to obtain a reasonable estimate of the second order effect typically thousands to hundreds of thousands tang et al 2007 wan et al 2015 for large scale hydrological model applications the cost of performing second order or higher order sensitivity analysis using classical sobol method would be unbearable in order to reduce the number of model runs needed to calculate different sensitivity indices i e the main effect and interaction effect meta model based sa methods have been proposed meta model based approach is generally implemented in two steps first a surrogate model is constructed then sa is performed on the surrogate model the advantage of a meta model based approach is that it requires a significantly smaller number of model runs to perform sa than traditional sa methods to construct a surrogate model a design of experiment doe approach is usually used to sample the parameter space then the model is run using those sampled parameter sets and the corresponding performance metrics i e the output of interest are computed finally a surrogate model is constructed based on the parameter objective function statistical relationship there exist a plethora of statistical methods to construct a surrogate model razavi et al 2012a for example the aforementioned mars method svm support vector machine method gpr gaussian process regression method rf random forest and ann artificial neural network method meta model based sobol method that uses monte carlo simulation has been used to perform sa of hydrological models by numerous researchers song et al 2012a b zhan et al 2013 wang et al 2016 gan et al 2017 because monte carlo approximations of the integrals required in sobol method can lead to some numerical errors the estimate of the interaction effect based on meta model based approach can be inaccurate for example the second order effect computed by the classical sobol method sometimes results in negative values of sensitivity indices which defies the definition of variance terms which must be positive tang et al 2007 furthermore the estimation of interaction effect usually requires an extraordinary number of model runs to obtain a reasonable estimate of the interaction effect for example zhan et al 2013 studied the parameter interaction effects of the distributed time variant gain model dtvgm with 14 parameters using the meta model based sobol method and it required 2600 dtvgm model runs and 100 000 meta model runs 186 times the number of parameters to obtain a reasonable estimate of interaction effect song et al 2012a studied xinanjiang hydrological model 7 parameters using the sobol method and it costs 1000 xinanjiang model runs and 100 000 meta model runs 286 times the number of parameters to avoid negative sensitivity indices further there is no guarantee that the interaction effect computed by the meta model based sobol method that uses monte carlo simulation is accurate as we will show in this study generally sa based on meta models involves two kinds of numerical errors one is the error between the surrogate model and the real response surface of the physical models the other is the monte carlo simulation error the later error will converge to zero when the sample size is infinite the superposition of the two errors will have a significant impact on the confidence of the calculation of sobol global sensitivity indices on the contrary if the monte carlo simulation error can be avoided the accuracy of global sensitivity indices can be greatly improved in this paper we investigate the use of the sparse polynomial chaos spc sa method sudret 2008 blatman 2009 blatman and sudret 2011 fajraoui et al 2012 marelli and sudret 2014 tang and zhou 2015 hu and zhang 2016 shao et al 2017 to calculate the interaction effect of the hydrological model parameters if polynomial chaos expansion is used as a surrogate model the mean and variance of output can be calculated directly by the expansion coefficients as well as the global sobol sensitivity indices can be calculated directly in other words the spc method can avoid monte carlo simulation error directly the rest of paper is organized as follows section2 introduces the spc method in section3 by using spc method the ishigami function 3 tunable parameters sobol function 30 tunable parameters as the mathematical function model cases and the hbv sask model 10 tunable parameters and sac sma model 13 tunable parameters as the hydrological model cases are used for illustration section4 presents a detailed discussion about the interaction effects finally a short conclusion is given in section5 2 sparse polynomial chaos method for sensitivity analysis 2 1 full polynomial chaos firstly the full polynomial chaos method is reviewed considering a general second order stochastic process in the probability space ω p f with space of events ω σ algebra p and probability measure f let us consider a stochastic model y ξ with model input ξ ξ 1 ξ 2 ω and model output y for a stochastic analysis of y the model y ξ may be approached as follows 2 1 y ξ a 0 ψ 0 i 1 1 a i 1 ψ 1 ξ i 1 i 1 1 i 2 1 i 1 a i 1 i 2 ψ 2 ξ i 1 ξ i 2 i 1 1 i 2 1 i 1 i 3 1 i 2 a i 1 i 2 i 3 ψ 3 ξ i 1 ξ i 2 ξ i 3 where ψ i ξ is i order orthogonal polynomial basis functions in the variables ξ a 0 a i 1 is the approximation coefficient here the expansion bases ψ i ξ are multi dimensional orthogonal polynomials defined as tensor products of the corresponding one dimensional polynomials bases ϕ k k 0 2 2 ψ i ξ k 1 n ϕ i k ξ k i k 1 n i k where n is the number of random variables in realistic applications we usually consider a finite number of terms to truncate 2 1 and we have 2 3 y ξ α a p n a α ψ α ξ a p n α n n α p where α is a multi index that identifies the components of the multivariate polynomials and a is the set of selected multi indices of multivariate polynomials and a α is the corresponding polynomial chaos expansions coefficients the number of the polynomial expansion term n is related to the number of random space independent variables used in the stochastic process system and it is also related to the degree of freedom i e the maximum number of orders of the polynomial basis when the number of random variables and the degree of freedom are given the number of the polynomial expansion term n in the random process is as follows 2 4 n p n p n 1 where p is the degree of freedom and n is the number of random variables the expression eq 2 1 is called the full polynomial chaos for polynomial chaos methods there are three key steps they are respectively the determination of orthogonal polynomial basis of model input parameters the calculation of the coefficients of polynomial chaos and estimators of accuracy of the polynomial chaos approximations for the determination of orthogonal polynomial basis according to the wiener askey polynomial chaos xiu and karniadakis 2003 there exist different optimal polynomials for different probability density functions e g normalized legendre resp hermite polynomials can be associated to a uniform resp gaussian probability density functions in this work the model parameters are considered to be uniformly distributed and the legendre polynomial chaos are chosen in the calculation process of coefficients of the polynomial expansion two methods for intrusive method and non intrusive method can be used to determine the coefficients of polynomial expansion xiu 2010 the intrusive method needs to adjust the original model and non intrusive method does not need to adjust the original model code in this way the non intrusive method is suitable for the study of most models the non intrusive method has two main methods the regression method and the projection method blatman and sudret 2011 here the regression based non intrusive method is chosen in this work the coefficients may be estimated by determining the l 2 projection of the response y ξ onto the space spanned by the polynomials ψ α ξ α p as follows 2 5 a argmin a r n e y ξ a t ψ ξ given a sampling of size n of the input random vector χ ξ 1 ξ n t the design of experimental and the corresponding model responses y y 1 y n t the ordinary least square solution of eq 2 5 is 2 6 a a t a 1 a t y where a a i j ψ j ξ i i 1 n j 1 c a r d a is the so called experimental matrix that contains the values of all the basis polynomials in the experimental design points after the polynomial coefficients are computed through the error estimation the polynomial chaos based model with the appropriate accuracy is successfully established the leave one out loo error estimation is chosen in this work the leave one out loo cross validation error ϵ l o o is designed by using cross validation it consists in building n meta models y p c i each one created on a reduced experimental design χ ξ i ξ j j 1 n j i and comparing its prediction on the excluded point ξ i with the real value y i blatman and sudret 2010 the leave one out cross validation error can be written as 2 7 ϵ l o o i 1 n y ξ i y p c i ξ i 2 i 1 n y ξ i μ ˆ y 2 where μ ˆ y is the sample mean of the experimental design response in practice when the results of a least square minimization are available there is no need to explicitly calculate n separate meta models after some algebra eq 2 7 can reduce to 2 8 ϵ l o o i 1 n y ξ i y p c ξ i 1 h i 2 i 1 n y ξ i μ ˆ y 2 where h i is the i th diagonal term of matrix a a t a 1 a t matrix a is defined in eq 2 6 and y p c is the pc expansion built up from the full experimental design χ however note that the number of full pc coefficients n increases exponentially with n and p since eq 2 4 thus the number of coefficients to be computed increases dramatically when n is large say n 10 this is known as the curse of dimensionality for this issue it is solved satisfactorily using specific methods to compute sparse polynomial chaos 2 2 sparse polynomial chaos suppose a be a non empty finite subset of n n with which the truncated polynomial chaos can be defined by 2 9 y ξ α a a α ψ α ξ the common truncation scheme in eq 2 3 corresponds to the choice a a p n since the curse of dimensionality problem the determination of truncation sets a of small cardinality is of interest thus we define that if the following condition is verified the truncated pc eq 2 9 is sparse 2 10 i n c a r d a c a r d a p n 1 generally polynomial chaos can be sparse from two aspects hypothesis of the structure of polynomial chaos and the numerical algorithm for solving polynomial chaos from the point of view of the structure of polynomial chaos according to the sparsity of effects principle montgomery 2004 the hyperbolic truncation or maximum interaction can be used from the point of view of the solution algorithm of polynomial chaos the least angle regression lar based or orthogonal matching pursuit omp based solution algorithm can be applied blatman and sudret 2011 doostan and owhadi 2011 2 3 sparse polynomial chaos for sensitivity analysis finally when we obtain the coefficients of eq 2 9 successfully eq 2 9 can be interpreted as a model response surface for y ξ the uncertainty and sensitivity analysis can be calculated directly as simple analytical functions of the spc coefficients for the expectation and variance we have 2 11 e y ξ a 0 v a r y ξ α a α 0 a α 2 for the sensitivity indices the spc can be rewritten in the form of the sobol decomposition 2 12 y ξ a 0 i 1 1 n α i i 1 a α ψ α ξ i 1 i s i 1 n α i i 1 i s a α ψ α ξ i 1 ξ i s α i 1 n a α ψ α ξ where i i 1 i s α α 1 α n α k 0 k i 1 i s 1 n due to the orthogonal property of the polynomial basis the partial variance can be derived analytically from the spc coefficients as follows 2 13 d i 1 i n α i i 1 i n a α 2 so the partial sensitivity indices for the subset of input variables ξ i 1 ξ i n is as follows 2 14 s i 1 i n d i 1 i n d note that once a spc approximation of the model response has been built as a metamodel compared with all sobol type sa methods which require monte carlo simulation to calculate the sensitivity indices the spc based sensitivity indices are computed analytically from the spc coefficients which is of a negligible computational cost 3 evaluation of parameter interaction effect of the analytical test problems and the hydrological models in this section the efficiency and effectiveness of the spc sa method are evaluated firstly two well known mathematical functions are investigated the 3 dimensional ishigami function and the 30 dimensional sobol g function which have analytical expression for sensitivity indices then the spc sa method is used to examine the parameter interaction effect of two hydrological models in real world application settings i e the 10 dimensional hydrologiska byråns vattenbalansavdelning university of saskatchewan hbv sask hydrological model and the 13 dimensional sacramento soil moisture accounting sac sma hydrological model both of which have no analytical expression for sensitivity indices 3 1 analytical test models the ishigami and sobol g functions firstly the ishigami function is chosen as our test case ishigami and homma 1990 its expression is shown as follows 3 1 y x s i n x 1 a s i n 2 x 2 b x 3 4 s i n x 1 where the input variables x 1 x 2 and x 3 are uniformly distributed over π π the variance d of y and the sobol sensitivity indices can be computed analytically as follows 3 2 d a 2 8 b π 4 5 b 2 π 8 18 1 2 d 1 b π 4 5 b 2 π 8 50 1 2 d 2 a 2 8 d 3 0 d 12 d 23 0 d 13 8 b 2 π 8 225 in this example a 7 and b 0 1 for the sparse polynomial chaos the omp based calculation method of the coefficients is selected and the adaptive polynomial degree is set as 2 p 15 based on loo cross validation error estimates in addition the classical sobol method see appendixa the gaussian process regression based gpr sa method see appendixb and the polynomial chaos kriging based pc kring sa method see appendixc are used for comparison purpose the gaussian process regression and polynomial chaos kriging are the commonly used meta models wang et al 2014 kersaudy et al 2015 gong et al 2015 2016 table1 shows the root mean square errors rmse of the estimates of the total first order and second order sobol indices of spc gpr and pc kring methods against theoretical values for sample sizes n 20 n 40 and n 80 and sobol method for the sample size n 200 000 respectively the negative values calculated by the sobol method and meta model based sobol method are all set to zero in this work the second analytical test problem is the sobol g function saltelli and sobol 1995 whose expression is as follows 3 3 y x i 1 n 4 x i 2 a i 1 a i where the input variables x are uniformly distributed over 0 1 and a are nonnegative the variance d of y and the sobol sensitivity indices can be computed analytically as follows 3 4 d i 1 n d i 1 1 d i 1 3 1 a i 2 d t i d i j i 1 d j s i 1 i n 1 d j 1 n d i j in this example n 30 and a 1 2 5 10 20 50 100 500 999 999 for the sparse polynomial chaos the omp based calculation method of the coefficients is selected and the adaptive polynomial degree is set as 2 p 5 based on loo cross validation error estimates in addition the classical sobol method the gaussian process regression based sa method and the polynomial chaos kriging based sa method are still chosen for comparison table2 displays the rmses of the estimates of the total first order and second order sobol indices of spc gpr and pc kring methods against theoretical values for sample sizes n 150 n 300 and n 600 and sobol method for n 200 000 respectively from the sensitivity indices results of the ishigami table1 and sobol g functions table2 it is easy to see that the spc method is more accurate in calculating all second order effects while identifying total and first order effects in particular when the sample size of the ishigami function is 80 and the sample size of the sobol g function is 300 the rmses of their second order indices against theoretical indices reach 0 in addition as in the case of the ishigami test function we can see the importance of interaction effect the parameter x 3 is of zero main effect but has strong interaction with parameter x 1 under which situation interaction effect should not be ignored it is essential to monitor and evaluate the convergence rate of the gsa methods using some efficient techniques nossent et al 2011 razavi et al 2012b sarrazin et al 2016 harenberg et al 2019 sheikholeslami et al 2019 as it can enable us to diagnose the convergence behavior of the gsa thus in order to test the convergence behavior the robustness analysis is designed to estimate first and second order indices for a set of increasingly larger experimental designs by means of bootstrap method using 100 bootstrap replicates efron 1979 for the ishigami function and the sobol g function the results of the convergence of the estimates of the first order indices for two most sensitive parameters e g parameters x 1 and x 2 of the ishigami function parameters x 1 and x 2 of the sobol g function and the largest second order indices e g parameter pair x 13 of the ishigami function parameter pair x 12 of the sobol g function are shown in figs 1 and 2 respectively the sobol total effect indices are not shown because their convergence behavior is essentially identical to that of the first order indices by the convergence analysis confirming that the estimation of interaction effects is accurate enough to make a solid conclusion 3 2 evaluation of parameter interaction effect of the hbv sask hydrological model the hbv sask model fig 3 is a conceptual rainfall runoff model which was coded at the university of saskatchewan for educational purposes based on an interpretation of the hydrologiska byråns vattenbalansavdelning model lindström et al 1997 here ten of the hbv sask model parameters are considered tunable these parameters are described in table3 the rainfall runoff model hbv sask used in this study has 12 tunable parameters as presented in gupta and razavi 2018 razavi and gupta 2019 razavi et al 2019 here we consider only 10 parameters because parameter 11 is the base of unit hydrograph for watershed routing in day and its default is 1 for small watersheds parameter 12 is the precipitation multiplier to address uncertainty in precipitation and its default is 1 here we chose their default values in the hbv sask model the oldman watershed is chosen as the study area the 1434 73 km 2 oldman watershed is located in the rocky mountains of alberta canada historical data is available for the period 1979 2008 from which the average annual precipitation rainfall snowfall is estimated to be 611mm and average annual streamflow to be 11 7 m 3 s at gauge 05aa023 on the oldman river runoff ratio 0 42 razavi and gupta 2019 to evaluate model responses as a function of different parameter values the rmse values between the simulated and observed daily streamflow discharge m 3 s is used as the objective function 3 5 r m s e 1 n i 1 n q s t q o t 2 where q s t and q o t are simulated and observed streamflow discharge values at time t n is the total number of observations there is no analytical values for the sensitivity indices of the hbv sask hydrology model in a real world application setting to obtain a reasonable estimate of the true sensitivity indices we performed sa using the classical sobol method that employs 1000 000 monte carlo simulations mcs see appendixa for details to approximate the true sensitivity indices to demonstrate the convergence of the spc method a set of nested experimental designs of increasing sample sizes 100 200 300 400 500 700 1000 are generated based on the sobol sequence sampling for the sparse polynomial chaos selecting the adaptive polynomial degree 2 p 5 and omp based calculation method of the coefficients the selections of adaptive polynomial degree and calculation method are based on the leave one out error estimation of spc method the leave one out errors of the experimental designs 100 200 300 400 500 700 1000 are 5 96 e 12 4 45 e 15 1 54 e 12 5 02 e 13 1 34 e 13 4 38 e 14 and 8 45 e 14 respectively again the gpr sa method and pc kriging sa method are used for comparison purpose 3 2 1 effectiveness of the spc method in calculating interaction effects firstly the effectiveness in estimating the total and first order sa indices by different meta model based sa methods is investigated parameter sensitivity rankings of mcs based sobol method with 1000 000 model evaluations and spc based sobol method with 1000 model evaluations are given in fig 4 which shows that the three most sensitive parameters are parameters 7 5 and 2 respectively based on both the first order and total sa indices the spc based sobol method provides excellent matches with mcs based sobol method fig 5 displays the rmse values of the estimates of the first order and total sa indices calculated by spc gpr and pc kriging sa methods compared with the gpr and pc kriging methods the spc method has much smaller rmse values for both the first order and total effects than the other two methods next the effectiveness in estimating the second order sa indices is considered the second order results for the mcs based sobol and spc based sobol methods are given in fig 6 which shows that parameter pairs 5 7 2 7 1 2 and 2 5 have the largest interaction effects note that when the sample size for the spc method reaches 300 or more the results of spc based and mcs based sobol methods are very similar fig 7 exhibits the rmse values of the estimates of the second order sensitivity indices calculated by spc gpr and pc kriging methods respectively from fig 7 the spc method shows a better convergence behavior than the gpr and pc kriging methods in addition for the training time of various methods the time consumption of spc method is the least for sample sizes 100 300 500 700 1000 the spc method can save 10 4 5 4 2 times against the gpr method and 12 9 13 7 11 times against the pc kriging method respectively the interaction between tt parameter 1 the air temperature threshold about melting freezing state of precipitation and co parameter 2 the base melt factor of snow is due to the melting freezing state of water before and after reaching ground co also has a strong correlation with fc parameter 5 the soil field capacity and frac parameter 7 the fraction of water released from soil to fast reservoir the reason of strong interaction between co fc and frac is that these three parameters control the state of water in this watershed frozen in snowpack stored in soil or flowing in the river channel these three parameters are also the most important parameters controlling the whole hydrological process in this watershed parameter tt only interacts with co as both of them are related to the process in snow cover 3 2 2 robustness of the spc method in calculating interaction effects the robustness of the spc method using different samples sizes in obtaining reliable estimates of different sa indices is analyzed in order to test the convergence behavior a robustness study is still designed to estimate first and second order indices for a set of increasingly larger experimental designs with a maximum of n 1000 confidence bounds for each estimate are calculated by means of a bootstrap method using 100 bootstrap replicates the results of the convergence of the estimates of the first order indices for three most sensitive parameters e g parameters 7 5 2 are shown in fig 8 violin graph the sobol total effect indices are not shown because their convergence behavior is essentially identical to that of the first order indices from fig 8 it is clear that reasonably good estimates of the first order indices are already obtained with 200 model evaluations i e the sample size is 20 times as large as the number of parameters for the robustness of the second order sa indices estimates the convergence behavior of the estimates of the sobol second order indices as a function of the experimental design sizes is shown in fig 9 for the four parameter pairs with the largest second order sobol indices e g pairs 5 7 2 7 1 2 2 5 fig 9 shows clearly that reasonably good estimates of the second order indices can be obtained with 300 model evaluations i e the sample size is 30 times as large as the number of parameters 3 3 evaluation of parameter interaction effect of the sac sma hydrological model the sac sma hydrological model fig 10 has a highly non monotonic non linear input parameter model output relationship this model is the most widely used hydrological model by the river forecast centers of the u s national weather service for catchment modeling and flood forecasting burnash et al 1973 here thirteen of the sac sma model parameters are considered tunable brazil 1988 these parameters are described in table4 the south branch potomac river basin near springfield west virginia in the u s is chosen as the study area historical precipitation potential evapotranspiration and streamflow observations from january 1 1960 to december 31 1979 are obtained from the model parameter estimation experiment mopex database for this study duan et al 2006 the hydrological simulations were run at a 6h time step over the entire data period the average annual runoff is 39 5 m 3 s average annual potential evapotranspiration is 762mm and average annual precipitation over this period is 1021mm wang et al 2014 to evaluate model responses under different parameters the root mean square error rmse is chosen as the objective function similarly we still performed sa using the classical sobol method that employs 1000 000 monte carlo simulations mcs to approximate the true sensitivity indices to demonstrate the convergence of the spc method a set of nested experimental designs of increasing size 130 260 390 520 650 910 1300 is generated based on the sobol sequence sampling for the sparse polynomial chaos selecting the adaptive polynomial degree 2 p 5 and omp based calculation method of the coefficients the leave one out errors of the experimental designs 130 260 390 520 650 910 1300 are 1 28 e 10 3 68 e 16 3 71 e 11 1 52 e 12 2 16 e 12 6 41 e 13 and 1 59 e 12 respectively again the gpr sa method and pc kriging sa method are still used for comparison purpose 3 3 1 effectiveness of the spc method in calculating interaction effects firstly the effectiveness in estimating the total and first order sa indices by different meta model based sa methods is investigated parameter sensitivity rankings of mcs based sobol method with 1000 000 model evaluations and spc based sobol method with 1300 model evaluations are given in fig 11 which shows that the two most sensitive parameters are parameters 5 and 4 respectively based on both the first order and total sa indices the spc based sobol method provides excellent matches with mcs based sobol method fig 12 displays the rmse values of the estimates of the first order and total sa indices calculated by spc gpr and pc kriging sa methods compared with the gpr and pc kriging methods spc method is also effective for identifying first and total order effects next the effectiveness in estimating the second order sa indices is considered the second order results for the mcs based sobol and spc based sobol methods are given in fig 13 which shows that parameter pairs 9 11 10 11 and 5 8 have the largest interaction effects note that when the sample size for the spc method reaches 910 or more the results of spc based and mcs based sobol methods are very similar fig 14 exhibits the rmse values of the estimates of the second order sensitivity indices calculated by spc gpr and pc kriging methods respectively similarly the spc method shows a better convergence behavior than the gpr and pc kriging methods in addition for the training time of various methods the time consumption of spc is the least for sample sets 130 390 650 910 1300 the spc method can save 2 3 2 1 1 times against the gpr method and 5 5 2 2 1 times against the pc kriging method respectively the lzpk parameter 11 which is about the lateral drainage rate of lower zone supplementary free water has significant interaction with the lsfsm parameter 9 and lzfpm parameter 10 because the three parameters are about the free water in the lower zone lzpk controls the drainage rate while lzfpm controls the storage capacity of the lower zone primary free water slower lzpk also have interactions with lzfsm which controls the storage of supplementary free water faster the strong interactions between the three parameters indicated that these processes are highly correlated and with only the observation of streamflow it will be not easy to identify the values of them the interaction between adimp parameter 5 and lztwm parameter 8 as well as many other moderate interaction effects indicated that in sac sma there are many not overwhelmingly strong but nonnegligible interactions that make a great challenge to parameter calibration the interaction effects are the inherent reason of equifinality and the quantification of interaction effects will provide a lot of useful information to understand the uncertainty of hydrological models 3 3 2 robustness of the spc method in calculating interaction effects the robustness of the spc method using different samples sizes in obtaining reliable estimates of different sa indices is analyzed in order to test the convergence behavior a robustness study is designed to estimate first and second order indices for a set of increasingly larger experimental designs with a maximum of n 2000 confidence bounds for each estimate are calculated by means of bootstrap method using 100 bootstrap replicates the results of the convergence of the estimates of the first order indices for six most sensitive parameters e g parameters 5 4 1 8 12 and 11 are shown in fig 15 violin graph similarly the sobol total effect indices are not shown because their convergence behavior is essentially identical to that of the first order indices from fig 15 it is clear that reasonably good estimates of the first order indices are already obtained with 390 model evaluations i e the sample size is 30 times as large as the number of parameters for the robustness of the second order sa indices estimates the convergence behavior of the estimates of the sobol second order indices as a function of the experimental design sizes is shown in fig 16 for the three parameter pairs with the largest second order sobol indices e g pairs 9 11 10 11 and 5 8 fig 16 shows clearly that reasonably good estimates of the second order indices can be obtained with 910 model evaluations i e the sample size is 70 times as large as the number of parameters 4 discussion as discussed in razavi and gupta 2015 the variance based approach is the most commonly used approach that provides a global measure of interaction effects the sobol type sensitivity analysis method is a variance based sa method however the way that sobol sa indices are computed can lead to negative variances as the formulation contains pluses and minuses terms due to sampling errors especially when the sample size is relatively small the minus terms can dominate and lead to negative values this work conducted an evaluation of parameter interaction effect based on sobol s definition of sa indices the second order interaction effects based on two meta model based sobol methods evaluated in this study i e gpr and pc kriging can result in negative values in sa indices this is not an artifact of the anova decomposition but rather is due to the sampling errors that are inherent in monte carlo simulations in those sobol methods the spc sa method presented in this study does not have this problem and provides more reliable estimates of the sa indices with the knowledge of not only the main effects but also the interaction effects the parameters can be optimized more accurately as in the case of the ishigami test function in this paper parameter x 3 is of zero main effect but has strong interaction with parameter x 1 under which situation interaction effects should not be ignored shi et al 2019 have shown that in analyzing the sensitivity of parameters of the earth system model of intermediate complexity loveclim the parameter interaction effects cannot be ignored in identifying the optimal model parameters their results showed that all ocean related parameters have shown only a little sensitivity in the time scale of thousands of years but as the ocean processes control the climate variability through their interactions with other earth system components such as land and atmospheric systems the ocean related parameters should be considered in parameter optimization huang et al 2018 who examined the parameter optimization of a single column community atmosphere model pointed out that the optimization metrics improved by 67 by considering interaction effects the sensitivities of model parameters are dependent on the choice of objective functions here the root mean square error rmse is chosen as the objective function which is commonly used in model calibration parameter identification the role of objective function s selection on the implementation and interpretation of parameter interaction evaluation is vital rmse depicts different behavior compared to other objective functions such as nash sutcliffe efficiency coefficient nse and kling gupta efficiency coefficient kge that would yield different results in recent studies gupta and razavi 2018 razavi and gupta 2019 the filtering role of objective functions when used in sensitivity analysis was discussed because sensitivity analysis is not only for parameter optimization it can have other applications one of the applications is to support a better understanding of the model behaviors for example interaction effects can improve understanding of the interaction processes and mechanisms in model simulations as discussed in razavi et al 2012b and harenberg et al 2019 due to the randomness inherent in does a robust numerical assessment is needed for a meta modeling method therefore for the robustness of the spc method in calculating interaction effects we apply the bootstrap resampling technique efron 1979 to spc method because the bootstrap resampling technique can easily be performed without adding to the overall computing cost of gsa the efficiency analysis does not require extra model evaluations our results show that the spc method needs about 300 samples 30 times the parameter dimension and 910 samples 70 times the parameter dimension to evaluate the interaction effects of the hbv sask and sac sma models respectively the computational cost for interaction effects can depend on the number of interaction effects for a n dimensional model its number of interaction effects is n n 1 2 the more the number of interaction effects the more complex the model may be the complexity of the original model structure is an important factor affecting the performance of meta modeling therefore the computational cost of interaction effects is closely related to the number of model parameters from the results of robustness assessment as the sample size increases the confidence intervals of interaction effects narrow it shows the robustness of our results by the robustness assessment and convergence analysis confirming that the estimation of interaction effects is accurate enough to make a solid conclusion in addition the impact of employing different sampling strategies on the results of sensitivity analysis is different here the sobol sequence sampling is chosen to construct a surrogate model wang et al 2014 finally it must be noted that a limitation of sobol based method is that it only exists in the case of independent input variables when the parameters are correlated the so called analysis of covariance ancova method may be needed to generalize the sobol decomposition for models with correlated inputs to calculate sensitivity indices caniou 2012 5 conclusion in this work the second order interaction effects of model parameters are studied the spc sa method was tested on four case studies including two mathematical functions namely the 3 dimensional ishigami and 30 dimensional sobol g function and two hydrological models namely 10 dimensional hbv sask model and 13 dimensional sac sma model compared with the classical sobol and meta model based sobol sa methods the effectiveness and efficiency of the spc method in the second order sensitivity analysis are shown the investigation we have carried out leads to reference takeaways for the analysis of hydrological models as well as for the practice of sensitivity analysis 1 which method can calculate all the interaction effects effectively and 2 how much computational cost is required to achieve the robust interaction effects when the expansion of spc is available the advantage of spc method is that the sobol indices at any order may be computed analytically hence as one of meta model based sa the approach not only avoids expensive computing but also avoids sampling errors from the monte carlo simulation the spc method is shown to be one effective and efficient sensitivity analysis method for calculating interaction effects it has much potential for further applications for instance the parameters may be optimized reasonably by combining main effects and interaction effects further studies should be as well focused on general application to other uncertainty quantification cases such as large complex dynamical system models which would be of great significance on practical application declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors gratefully acknowledge the support of the national basic research program of china no 2015cb953703 the strategic priority research program of the chinese academy of sciences no xda19070104 xda20060401 the state key laboratory of earth surface processes and resource ecology no 2017 kf 05 the fundamental research funds for the central universities beijing normal university research fund no 2015kjjca04 and the special fund for meteorological scientific research in public interest no gyhy201506002 mr heng wang gratefully acknowledges the scholarship support provided by china scholarship council no 201806040132 we thank the editor holger maier for his work we thank razavi saman and three anonymous reviewers for their insightful comments and suggestions that helped improve the manuscript in this study the experimental designs and sensitivity analysis methods we used come from uq pyl software package http www uq pyl com and uqlab software package https www uqlab com the hbv sask rainfall runoff model and data we used come from vars tool software package http vars tool com the sac sma rainfall runoff model and data we used come from uq pyl appendixa sobol sensitivity analysis method in sobol method sobol 1993 the variance of the model output is decomposed into components that result from individual parameters as well as parameter interactions conventionally the direct model output is replaced by a model performance measure such as rmse as used in this study the sensitivity of each parameter or parameter interaction is then assessed based on its contribution measured as a percentage to the total variance computed using a distribution of model responses assuming the parameters are independent the sobol variance decomposition is a 1 d y i d i i j d i j i j k d i j k d 12 n where d i is the measure of the sensitivity to model output y due to the i th component of the input parameter vector d i j is the portion of output variance that results due to the interaction between parameters the variable n defines the total number of parameters the sensitivity of parameters and interaction between parameters is obtained by normalizing the above formula a 2 1 i d i d y i j d i j d y i j k d i j k d y d 12 n d y the variance decomposition shown in eq a 2 can be used to define the sensitivity indices of different orders as a 3 s i d i d s i j d i j d s t i 1 d i d where s i is the first order sensitivity index main effect s i j is the second order sensitivity index interaction effect and s t i is the total order sensitivity index total effect the original sobol method required n 2 m 1 model runs to calculate all the first second order and total order sensitivity indices an enhancement of the method made by saltelli 2002 provides the first second and total order sensitivity indices using n 2 m 2 model runs in this study this modified version of sobol methodology is chosen to compute the first order second order and total order indices appendixb gaussian process regression based sobol sensitivity analysis method gaussian process regression gpr is a machine learning method based on statistical learning theory and bayesian theory a gaussian process regression meta model is described by the following equation santner et al 2003 rasmussen and williams 2006 dubourg 2011 lataniotis et al 2018 b 1 y x β t f x σ 2 z x ω the first term in eq b 1 β t f x is the mean value of the gaussian process i e trend and it consists of the regression coefficients β j j 1 p and the basis functions f j j 1 p the second term in eq b 1 consists of σ 2 the variance of the gaussian process and z x ω a zero mean unit variance stationary gaussian process the z x is fully determined by the auto correlation function between two input sample points r x x r x x θ due to stationarity where θ are hyper parameters to be computed the gaussian assumption states that the vector formed by the true model responses y and the prediction y ˆ x has a joint gaussian distribution defined by b 2 y ˆ x y n n 1 β f x t β f σ 2 1 r t x r x r where f is the information matrix of generic terms f i j f j x i i 1 n j 1 p r x is the vector of cross correlations between the prediction point x and each one of the observations whose terms read r i r x x i θ i 1 n r is the correlation matrix whose terms read r i j r x i x j θ i j 1 n then the mean and variance of the gaussian random variate y x a k a mean and variance of the gpr predictor can be calculated b 3 μ y ˆ x β f x t r x t r 1 y f β b 4 σ y ˆ 2 x σ 2 1 r x t r 1 r x u x t f t r 1 f 1 μ x where β f t r 1 f 1 f t r 1 y is the generalized least squares estimate of the underlying regression problem and u x f t r 1 r x f x then predictions for new points can be made in terms of the mean and variance of y ˆ x using eqs b 3 and b 4 finally combining with sobol method gaussian process regression based sensitivity analysis method can be carried out the sobol sensitivity indices can be calculated by means of samples of input parameters and the mean values of gpr output it needs to be estimated at the cost of n n 2 m 2 model evaluations when using sobol sampling in this work for the selection of correlation function and trend for gaussian process regression the matérn 5 2 covariance kernel is considered with a constant yet unknown trend the hyper parameters are estimated with the maximum likelihood method and are solved by using the hybrid genetic algorithm we select about 200 000 samples to calculate the sensitivity indices through sobol sampling method appendixc polynomial chaos kriging based sobol sensitivity analysis method kriging a k a gaussian process regression interpolates the local variations of y as a function of the neighboring experimental design points whereas polynomial chaos approximates well the global behavior of y by combining the global and local approximation of these techniques the polynomial chaos kriging metamodel is achieved the polynomial chaos kriging pc kriging is defined as a universal kriging model the trend of which consists of a set of orthonormal polynomials kersaudy et al 2015 schobi et al 2019 c 1 y x α a a α ψ α ξ σ 2 z x ω where α a a α ψ α ξ is a weighted sum of orthonormal polynomials describing the trend of the pc kriging model σ 2 and z x ω denote the variance and the zero mean unit variance stationary gaussian process respectively as introduced in appendixb hence pc kriging can be interpreted as a universal kriging model with a specific trend constructing a pc kriging model consists of two parts the determination of the optimal set of polynomials contained in the trend and the calibration of the kriging model the two parts can be combined in various ways sequential pc kriging and optimal pc kriging can be implemented finally combining with sobol method gaussian process regression based sensitivity analysis method can be carried out the sobol sensitivity indices can be calculated by means of samples of input parameters and the mean values of pc kriging output it needs to be estimated at the cost of n n 2 m 2 model evaluations when using sobol sampling in this work the sequential pc kriging is chosen the optimal set of polynomials is determined by sparse polynomial chaos based on least angle regression lar and the adaptive polynomial degree is set as 2 p 5 for the selection of correlation function and trend for gaussian process regression the matérn 5 2 covariance kernel is considered with a constant yet unknown trend the hyper parameters are estimated with the maximum likelihood method and are solved by using the hybrid genetic algorithm similarly we select about 200 000 samples to calculate the sensitivity indices through sobol sampling method 
26064,coastlines are vulnerable to sea level rise erosion and flooding all predicted to worsen as climate change continues models are needed that incorporate dynamic processes within coupled economic and physical systems for decision makers including when to retreat from hazardous areas this paper presents a coupled geo economic model of beach width and property value with exogenous sea level rise discrete dynamic programming identifies optimal beach nourishment and managed retreat strategies as a new contribution we treat expected damages as costs which demonstrates that increased damage offsets some benefits of development along hazardous coastlines the model identifies managed retreat as preferable to beach nourishment when damages exceed a threshold regret associated with retreat depends on sea level rise damages and the timeframe of analysis considering dynamic complexity of coastal systems as this model does may produce climate adaptation plans that better address community needs and reduce risk over timescales of interest graphical abstract image 1 keywords managed retreat beach nourishment dynamic programming coastal management climate change adaptation sea level rise 1 introduction coastal areas are vulnerable to hazards such as flooding inundation and erosion which are predicted to worsen as climate continues to change and global sea level rises kron 2013 usgcrp 2018 wong et al 2014 along developed coastlines this can lead to high risk defined as the potential for uncertain events to cause adverse effects which is a function of the hazard the assets exposed and their vulnerability kron 2013 national research council 2014 wong et al 2014 complicating the management of this risk is the fact that future sea level rise slr is deeply uncertain meaning that although it is possible to bound it from above and below we cannot confidently make decisions using the evolving probability distributions available at this time hall et al 2016 as a result traditional expected utility theory cannot be used to manage risk from slr impacts instead it is advisable to adopt robust strategies considering the range of plausible futures using decision criteria such as worst case scenarios satisficing strategies or possible regret hall et al 2019 2016 hallegatte et al 2012 herman et al 2015 kunreuther et al 2013 lempert and collins 2007 maier et al 2016 walker et al 2013 the presence of strong interactions between humans and landscapes suggests that they cannot be meaningfully understood in isolation particularly in places where human occupied land is vulnerable to landscape processes and where measures are taken to protect land from damage lazarus et al 2011 werner and mcnamara 2007 without considering the interactions between human and environmental systems important emergent phenomena may be missed and thus the coupling between systems has implications for management of coastal locations and avoiding maladaptation di baldassarre et al 2013 lazarus et al 2016 murray et al 2013 there exists a growing body of literature treating coastlines as coupled human natural systems with a focus on beach nourishment and other erosion adaptation strategies in the united states u s and elsewhere gopalakrishnan et al 2011 hinkel et al 2013 jin et al 2013 landry 2011 lazarus et al 2016 2011 little and lin 2017 mcnamara et al 2015 murray et al 2013 roebeling et al 2018 slott et al 2008 smith et al 2009 there is also an understanding that slr impacts and adaptation can affect coastal real estate markets bernstein et al 2019 conyers et al 2019 gopalakrishnan et al 2011 mcnamara et al 2015 mcnamara and keeler 2013 ortega and taspinar 2018 treuer et al 2018 walsh et al 2019 in this paper we build on previous beach nourishment modeling studies to incorporate additional feedbacks time dependencies and adaptation strategies to test how a modified system architecture could influence optimal decisions in a modeling context 1 1 coastal risk reduction strategies beach nourishment and dune construction often carried out by the united states army corps of engineers usace in collaboration with a local non federal sponsor are common shoreline erosion control measures in the u s used to minimize risk national research council 2014 usace 2008a 2008b these are effective strategies for reducing damages from storm surge waves and tides and can therefore help mitigate some slr impacts usace 2008a however due to the dynamic nature of coastlines beaches continue to erode and accrete over time dean and dalrymple 2002 thus beach nourishment includes plans to refresh the sand over time as it erodes it is never intended as a permanent solution but as a method to reduce flood and erosion risks to developed areas and multi decadal projects require periodic input of money and sand both of which may be limited resources landry 2011 national research council 2014 usace 2008a furthermore although beaches and dunes can reduce storm impacts to ocean front property beach nourishment does not address back bay flooding in the low lying areas behind barrier islands and around estuaries mullin et al 2019 national research council 2014 there is evidence of a positive feedback between beach nourishment and coastal property value with nourishment raising property values and higher property value justifying further development gopalakrishnan et al 2011 mcnamara et al 2015 murray et al 2013 some studies have even suggested that beach nourishment and other hazard mitigation measures could encourage coastal development thus increasing risk indicating that as a coastal risk management strategy nourishment could rebound or even backfire armstrong et al 2016 burby 2006 cooper and mckenna 2008 keeler et al 2018 lazarus et al 2016 woodruff et al 2018 at the same time there is evidence that slr is causing some coastal property to decline in value bernstein et al 2019 mcalpine and porter 2018 ortega and taspinar 2018 in contrast to beach nourishment which can mitigate damages to ocean front property managed retreat involves relocating assets of value to less hazardous locations andré et al 2016 diaz 2016 hino et al 2017 proposed strategies for implementing retreat include rolling easements in which privately held land is automatically transferred to the state when sea level reaches the property purchase and lease back in which land is purchased from property owners who are then allowed to rent it back for as long as it is habitable sea level purchase options granting organizations the future right but not the obligation to purchase vulnerable land and voluntary buyouts sometimes using natural disasters as windows of opportunity for implementation henderson 2018 kousky 2014 titus 1998 the federal emergency management agency fema administers some property acquisition programs under which individuals may voluntarily sell their property and the land is used for open space recreation or wetlands with new development prohibited eli and unc ie 2017 fema 2015 although programs exist to aid retreat the decision to relocate is complex because it involves human factors such as fear of disruption to community resilience networks potentially higher costs of living elsewhere ties to particular geographic locations and livelihoods that rely on access to ecosystem goods and benefits provided by coastal areas binder et al 2015 dannenberg et al 2019 usgcrp 2018 retreat does not necessarily generate benefits under the current climate or the large range of possible future climate scenarios leading some to classify it as a low regrets option hino et al 2017 ipcc 2014 retreat cannot be easily reversed may lead to loss of cultural heritage and a sense of place or belonging hino et al 2017 roberts and andrei 2015 and could negatively affect mental and physical health dannenberg et al 2019 managed retreat can have racial and social justice implications particularly when decisions rely on cost benefit analysis siders 2019 and when indigenous or lower income populations are involved marino 2018 nevertheless relocation holds promise as a future adaptation strategy dannenberg et al 2019 dyckman et al 2014 hino et al 2017 kousky 2014 1 2 project appraisal in the u s context to be eligible for federal support a coastal risk management project must pass a cost benefit analysis based upon national economic development ned defined as the net monetized value of the national output of goods and services discounted to present values regional economic development environmental quality and other social effects accounts may also be considered in decision making council on environmental quality 2013 office of management and budget 2018 u s water resources council 1983 usace 2008c it is common in both project feasibility analyses and some academic studies of beach nourishment economics to calculate risk mitigation benefits as the reduction in economic damages under one management plan i e future conditions with a particular project compared to a baseline management plan often described as a future without project condition andré et al 2016 gravens et al 2007 landry 2011 usace 2008b this has the effect that increasing potential damages and mitigating those damages can raise the net present value npv of a potential strategy even if expected damages also increase it could be possible therefore to advance economic analysis of coastal management by altering how costs and benefits are quantified for federally supported projects in the u s evaluation of costs and benefits relies on output from lifecycle geophysical and economic models such as beach fx created and maintained by the usace engineer research and development center s coastal and hydraulics laboratory and the u s army engineer institute for water resources gravens et al 2007 beach fx and other lifecycle models provide detailed spatially resolved predictions of shoreline change and economic damages due to inundation wave attack and erosion gravens et al 2007 to improve computational efficiency beach fx does not optimize over the full range of possible management actions but instead evaluates costs and benefits of several pre selected shoreline management strategies gravens et al 2007 usace jacksonville district 2017a this optimization is done over a fixed time horizon usually 50 years usace 2008c and does not calculate payoffs at shorter or longer timescales like all models beach fx makes simplifying assumptions about the coupled human natural system for example despite evidence of continued coastal development applications of beach fx often assume that new development does not occur in the coastal zone usace jacksonville district 2017b 2017a 2017c usace wilmington district 2014 as a model designed to evaluate shore protection projects the management measures included in beach fx simulations are limited to emergency and planned nourishment gravens et al 2007 for coastal communities vulnerable to slr impacts beach nourishment can buy time until longer term solutions such as elevating structures or even managed retreat are implemented however in some locations shoreline management on its own will be insufficient to address slr impacts now or in the future and retreat may be necessary for some communities kousky 2014 usgcrp 2018 1 3 contributions of this work this paper investigates physical and economic dynamic interactions that may affect adaptation decisions we focus our analysis to identify conditions under which property buyouts such as those administered by local communities states or fema may be preferable to continued beach nourishment and dune construction specifically we identify globally optimal management strategies for a coupled dynamic coastal system under several plausible deterministic slr scenarios discrete dynamic programming uses recursion to find a sequence of decisions that optimizes a value function given a set of constraints within a dynamic system bertsekas 1987 rust 1996 we consider the sensitivity of results to several plausible slr futures as well as other uncertain model inputs we are primarily concerned with how the coupled dynamics affect optimal strategies using a novel method for valuing project costs and benefits the model contributes to a more complete understanding of coastal systems threatened by erosion and inundation and could therefore inform coastal management decisions we explain the model in section 2 and present the results from a series of simulation experiments in section 3 section 4 highlights model limitations and section 5 discusses insights that can be gained from this study 2 methods fig 1 provides a graphical representation of the main interactions captured by our model as sea level rises erosion is assumed to decrease beach width and increase hazard which leads to higher risk necessitating adaptation investments for which there are two choices beach nourishment increases beach width to mitigate hazard which reduces risk in the short term however in the longer term lower hazards can incentivize further development which raises risk alternatively managed retreat which is more costly than beach nourishment removes value from the hazardous area reducing risk managed retreat includes removal of public utilities and infrastructure thus also decreasing the need for repairs due to episodic slr and storm events and reducing associated public safety risks in case of evacuating these areas note that when nourishment is undertaken the system remains susceptible to the exogenous slr forcing while managed retreat reduces exposure to slr this model does not have predictive capability but is rather a tool for investigating the relative importance of processes included and evaluating the effect of dynamic interactions on cost benefit analysis of coastal risk management decisions we calibrate the model using data from hutchinson island in st lucie county on the central east coast of florida while true parameter values are unknown the hutchinson island data can constrain our estimates and allow us to conduct a series of tests conclusions are meant to be broadly applicable to ocean front property subject to slr and coastal erosion and do not prescribe management actions for hutchinson island in particular 2 1 dynamic model 2 1 1 beach width we use a model similar to that of smith et al 2009 and extended by cutler et al 2019 this model assumes a straight homogeneous coastline the entire cross shore width of beach is subject to linear erosion which increases with sea level according to a modified bruun rule bruun 1962 dean and houston 2016 rosati et al 2013 the nourished portion of beach width erodes exponentially with time since the last nourishment episode and we include a term for the expected storm induced erosion each year for years in which the beach is not nourished this gives 1 d x t d t μ x ˆ θ e θ τ r d s t d t h e 0 ε s t where μ is the nourished portion of the beach assumed to be 1 x ˆ is the width of the nourished beach θ is the exponential erosion rate of the nourished portion of the beach τ is years since last nourishment r is the slope of the active profile d s t d t is the rate of slr h is the bruun rule correction term and e 0 ε s t is the expected storm induced erosion where e 0 is the expected value of the stochastic storm induced erosion term in cutler et al 2019 initially we assume that ε 0 but test the effect of ε 0 representing the prediction that storms may become more intense as the climate warms emanuel 2013 little et al 2015 sobel et al 2016 we allow expected storm induced erosion to increase proportionally with sea level because there is a physically based correlation between slr and the power dissipation index of tropical cyclones in that both increase with warmer sea surface temperatures little et al 2015 note that here we are separating the effect of slr on gradual erosion captured with the bruun rule from the effect of climate change and slr on storm induced erosion following usace technical letter 1100 2 1 usace 2014 and usace engineer regulation 1100 2 8162 usace 2013 we assume that sea level rises according to 2 s t a t b t 2 2 t t i 1992 where t is years since the start of the simulation s t is local sea level relative to the start of the simulation t i is the start year for the simulation taken to be 2020 a is equal to the historic rate of sea level change accounting for eustatic and local effects b is the predicted acceleration in slr and 1992 is the midpoint of the current national tidal datum epoch to account for uncertainty in future slr three values of b are considered producing low intermediate and high slr scenarios usace 2014 2013 usace jacksonville district 2017a taking the time derivative of equation 2 as needed for the bruun rule in equation 1 we obtain d s t d t a 2 b t t i 1992 parameter values for equations 1 and 2 are taken from cutler et al 2019 based upon previous usace and academic modeling and observational studies these values are given in table 1 2 1 2 coastal development we assume that the total property value v is equal to a baseline property value a modified to account for the effects of beach width positive and sea level negative we also include the possibility for a to increase with time representing a continued demand for coastal property independent of environmental conditions or decrease with time corresponding to a gradual migration away from the coast we spatially aggregate property value so that v t is the entire value at risk in the study area in year t and formulate our equation for v t as 3 v t 1 d t a α x t β s t where a is the baseline property value which inherently grows by a percent d per year and α and β are constants with α 1 and 0 β 1 if α 1 there is no effect of beach width on property value and similarly if β 1 sea level has no effect on property value the exponents x t and s t are beach width and sea level in year t respectively we set a 6 69 10 8 the total value at risk in the study area estimated by usace jacksonville district 2017a for the year 2020 when the beach is completely eroded in their hedonic model of coastal property values gopalakrishnan et al 2011 find that when beach width is considered as endogenous to the geo economic system a one foot increase in beach width corresponds to approximately a one percent increase in property value converting to meters we get α 1 01 3 2808 1 03 lacking empirical evidence for the effect of slr on property value we use a mid range value of β 0 5 and test sensitivity to this assumption by examining how the model responds to 0 1 β 1 with β 0 5 we assume that all else being equal 1 m of slr decreases property value 50 which is roughly on the same order of magnitude as the observation that buyers discount property vulnerable to 1 foot of slr by 14 7 bernstein et al 2019 for our initial analysis we assume d 0 so that the baseline property value is constant with time and later let d vary from 0 05 to 0 05 2 1 3 costs and benefits this model calculates costs and benefits according to ned not because regional economic development environmental quality and other social effects are not important but because ned is critical for securing federal funding given that many communities depend upon federal support for implementing projects there is value in taking a ned perspective for this analysis even though local environmental and social factors are also important to consider in a final decision costs in present value terms accrued in each year t are split into the costs of nourishing c n t the costs of relocating c r t and the costs of damage c d t smith et al 2009 let the costs of nourishment depend on a constant term and a variable term that depends linearly on the width nourished we modify this slightly to allow for the possibility that sand will become more expensive in the future 4 c n t c 1 c 2 1 ξ t x ˆ x t where c 1 is the fixed cost of nourishment and c 2 is the variable cost which increases by an amount ξ every year we assume that given logistical constraints the nourishment interval cannot be shorter than four years thus if fewer than four years have passed since the last nourishment c n t is assumed to be infinite imposing a four year minimum nourishment interval we use the observation that in florida sand costs on average 9 55 per cubic meter hoagland et al 2012 and that the total cost of a 6 1 m nourishment episode over 5470 m of coastline using 224 000 m3 of sand is 14 million usace jacksonville district 2017a this gives c 1 3 5 10 5 per meter and c 2 1 2 10 7 initially we set ξ 0 so that the cost of sand is constant with time however as a limited resource it is possible that the cost of sand could increase as it becomes more scarce national research council 2014 we test the effect of the assumption that the cost of sand is constant by allowing ξ 0 in a sensitivity analysis as a first pass estimate for relocation costs we use 5 c r t v t this is based on the assumption that the dominant cost of relocation will be property buyouts at market value and represents costs to the government or other body responsible for implementing buyouts this is in agreement with the mission of fema funded buyout projects to offer pre flood fair market value to homeowners who experience severe repetitive flood damage eli and unc ie 2017 fema 2013 when valuing damages avoided relative to a baseline management strategy as is usually done it is possible to identify as optimal a strategy that increases total damage if it also increases and mitigates potential damages at a faster rate than an alternate strategy fig 2 we argue therefore that it is more appropriate to treat damages as costs than as potential benefits i e damages avoided this distinction becomes important in the presence of feedbacks that alter the value at risk i e potential damages depending on the actions taken our method is not fundamentally at odds with previous studies that treat avoided damages as benefits e g andré et al 2016 gravens et al 2007 mcnamara and werner 2008 because reducing damages lowers costs which has the same impact on npv as increasing benefits in taking this approach it is possible to expose the unintended consequence of increased flood risk due to further development enabled by shoreline protection coastal development can still contribute positively to npv if the development itself adds utility but this is separate from risk mitigation benefits we assume that the expected cost of damages in year t c d t will be proportional to the value at risk we let d 0 be the annual expected proportion of value damaged under no slr with beach width equal to 0 and assume that the total proportion damaged in a given year t will increase with sea level and decrease with beach width from these assumptions we obtain 6 c d t d 0 f s t g x t v t w where f and g are increasing functions v t is physical property value and w is other value including social and economic systems vulnerable to slr impacts without data to suggest an appropriate value for w we assume that it will be on the same order of magnitude as v and conduct a sensitivity analysis this gives us a best guess value of w 5 10 8 with a range of 2 10 8 w 10 9 using output from beach fx for hutchinson island over all slr scenarios both with and without the recommended plan of a 20 foot berm extension using each reach year as a separate observation we compile a set of triplets containing beach width damage and time usace jacksonville district 2016 for years in which there are multiple reported widths or damages we take an average weighted by the amount of time that passes between reported values so that the time element of each triplet is a single year from which we can calculate sea level according to equation 2 from these triplets we define the vector x to be the sum of the berm and dune width for each reach in each year s to be a vector of sea level values and d to be a vector of damages by reach normalized by the initial value at risk in the reach we then select all x and d values for which s 0 1 meters to represent damages that would occur under no slr we fit a generalized linear model using a binomial distribution and logarithmic link function to get d e 3 2 0 17 x 12 5 4 10 3 1 2 x where d is the expected proportion of value damaged under no slr x is berm width and 12 is the average dune width in meters according to the beach fx output from this we take d 0 5 4 10 3 and g x t κ x t with κ 1 2 fig 3 a by assumption f 0 1 the ground elevation of hutchinson island is generally less than 3 05 m 10 feet usace jacksonville district 2017a we therefore assume that with 3 m of slr and no berm all value on hutchinson island would be damaged that is 5 4 10 3 f 3 1 2 0 1 so f 3 185 with little evidence or precedent for the most appropriate functional form for f we consider several different possibilities for f a polynomial f s t 1 s t φ with φ 3 8 a linear function f s t 1 φ s t with φ 61 per meter an exponential function f s t φ s t with φ 5 7 and a concave negative decay function f s t 1 φ 1 e s t with φ 109 fig 3b we compare results using these four damage functions to examine sensitivity to the functional form used benefits are calculated as the recreational and habitat benefits of the beach b b t and the benefits of increased economic development in the study area b d t we assume that there is no inherent value to keeping the community in its current location while this is almost certainly not true from the perspective of a community it can be justified from the ned point of view if the national scale output of all goods and services produced at this location are transferrable to other places institute for water resources 2011 u s water resources council 1983 we do not include damage mitigation as benefits because as explained above the cost of damages is calculated in equation 6 we assume that the benefits of the beach in year t are proportional to beach width according to 7 b b t η x t based on information from the usace jacksonville district 2017a we assume 0 0929 square meters 1 square foot of land is worth 14 assuming that the study area is 5470 m long it follows that η 8 24 10 5 per meter of cross shore buildout we use taxes levied on property as a proxy for the benefits gained from additional development so that 8 b d t l v t where we set l equal to the average of the six different total final tax rates for st lucie county in 2018 giving l 22 st lucie county property appraiser 2018 2 2 dynamic optimization we use a discrete dynamic programming algorithm to identify the optimal management strategy given equations 1 8 in the model we include three possible management actions do nothing nourish the beach to a width equal to x ˆ or buyout all value at risk if the decision to nourish is made in year t then beach width in year t 1 is set equal to x ˆ if the decision to relocate is made in year t then at time t 1 v and w are set to 0 we acknowledge that this is a simplification and that in reality planning and implementing managed retreat takes many years dannenberg et al 2019 we further assume that retreat does not affect the width of the beach and thus after retreat the system continues to behave as before with the only difference being that v w 0 so there are no benefits from development b d t 0 and no costs due to damage c d t 0 for all t t r where t r is the time at which retreat is implemented we let m t represent the management action in year t with m t 0 representing the decision to do nothing m t 1 representing the decision to nourish and m t 2 representing the decision to relocate we solve the system using the publicly available mdpsolve software designed to solve discrete time markov decision problems in matlab fackler 2015 the software uses backward recursion value function iteration or policy iteration to identify the strategy that will optimize the discounted npv over a given time horizon t which may be finite or infinite we also simulate the system under the optimal strategy for 100 years mdpsolve requires that the system be formulated as a markov decision problem because equations 1 3 are all time dependent the system as formulated above does not meet this requirement fortunately it is possible to redefine the system using the following four state variables t time since the start of the simulation τ time since the last nourishment r time since the decision to relocate was made n whether or not the beach is nourished in a given year the initial values and state equations for these variables are given in table 2 note that these state equations depend only on the current state and the management action m making this is an autonomous system of discrete difference equations that can be solved as a markov decision problem using dynamic programming algorithms defining the system in this way has the additional benefit that although we solve it as a discrete system the only variable that needs to be discretized is time from these four state variables we can use the following equations to calculate sea level s beach width x and property value v at any point in the four dimensional state space 9 s t τ r n a t b t 2 2 t t i 1992 10 x t τ r n max 0 x ˆ n 1 μ x ˆ μ x ˆ e θ τ r s t τ r n s t τ 0 r n h τ e 0 τ ε 0 5 a t 2 b t 3 3 t 2 t i 1992 n 1 11 v t τ r n 1 d t a α x β s i f r 0 v t r τ r n i f 0 r 1 0 i f r 1 equation 10 is a closed form solution for equation 1 and equations 9 and 11 are equivalent to equations 2 and 3 respectively using equations 4 8 to calculate the costs and benefits and constant exponential discounting with a discount rate δ to calculate the present value of future costs and benefits we obtain the following objective function 12 n p v m a x t 0 t 1 1 δ t η x t τ r n l v t τ r n c 1 c 2 1 ξ t x ˆ x t τ r n m t 2 m t v t τ r n m t 2 m t 1 d 0 f s t τ r n g x t τ r n v t τ r n w where as before η is the land benefit l is the tax rate c 1 is the fixed cost of nourishment c 2 is the cost of sand ξ is the increase in the cost of sand x ˆ is the nourished width of the beach d 0 is the damage function constant f is the proportion of value damaged as a function of sea level g is the proportion of value damaged as a function of beach width w is the non structural value vulnerable to slr impacts m is the control variable with m 0 being no action m 1 being nourishment and m 2 being retreat and s t τ r n x t τ r n and v t τ r n are calculated according to equations 9 11 3 simulation experiments we wish to use the model to better understand the dynamics of the coupled geo economic system and how the optimal strategy depends on the modeled processes and feedbacks we first present the baseline results in section 3 1 using the default parameter values given in table 1 for each of the three slr scenarios and four damage functional forms for f s in equation 6 in section 3 2 we present a global sensitivity analysis on uncertain parameters based on the results of the sensitivity analysis we further explore the effect of the exponential development rate d in section 3 3 to better understand the impact of uncertainty in the system in section 3 4 for several different strategies we calculate possible regret over the course of the simulation through these experiments we hope to better understand the system dynamics and how interactions among the model variables affect the optimal strategy and npv 3 1 baseline results fig 4 shows the optimal time paths for beach width and property value for the default parameter values given in table 1 using each of the three slr scenarios and four damage functional forms for f s in equation 6 moving left to right across the columns of fig 4 i e from exponential to polynomial to linear to concave exponential decay damage functions the optimal time of relocation occurs earlier under high slr for intermediate slr retreat is only optimal using the concave exponential decay damage function and retreat is not optimal under low slr according to this model comparing figs 3b and 4 it is clear that when a greater proportion of value is damaged retreat is implemented sooner as would be expected with the concave damage function under intermediate slr it remains optimal to continue nourishing the beach even after it provides no risk reduction benefits because the beach has inherent value this does not occur under high slr because the erosion rate is high enough that the beach disappears before sufficient habitat and recreational benefits accrue to offset the cost of nourishment thus both slr and the impact of slr on damage costs affect the optimal time of relocation and nourishment decisions for each slr damage function combination we give the change in the value function at each iteration of the solver in appendix a for each of the five scenarios in fig 4 in which relocation occurs table 3 gives the time of relocation sea level at time of relocation and value of d 0 f s t r where d 0 5 4 10 3 is the proportion of value damaged under no slr with beach width equal to 0 the function f relates slr to expected damages and s t r is sea level at the time of relocation t r although there is a wide range of times and sea levels at which relocation occurs d 0 f s t r is always in the range of 0 1 0 3 spanning less than 20 of the range of d 0 f 0 0 to d 0 f 3 1 this suggests that relocation becomes optimal when the annual expected proportion of value damaged due to slr reaches a critical level and that this critical level is roughly constant across scenarios the model does not specify precisely this critical threshold but it provides evidence that it exists this is similar to previous modeling results showing that economic optimizers abandon coastal property at a critical risk threshold mcnamara and keeler 2013 and supports the suggestion of hall et al 2019 that the focus ought to shift from how much sea level will rise to when slr will trigger important risks it is also in line with the concept of adaptation tipping points kwadijk et al 2010 this evidence of a critical threshold could therefore motivate research to identify the level of damage in addition to the year or sea level that justifies managed retreat 3 2 sensitivity analysis to test model sensitivity to uncertainty in parameter values we conduct a global sensitivity analysis in which we vary the slr acceleration b discount rate δ impact of beach width on property value α impact of sea level on property value β baseline property value a amount of non physical value at risk w annual increase in cost of sand ξ increase in storm induced erosion with slr ε and tax rate l using each damage function minimum and maximum values for each parameter are in table 4 justification for the range of parameter values tested is given in appendix b we run the model using each damage function for 10 000 random combinations of parameter values and give the minimum and maximum of the optimized objective function npv over a 100 year simulation total number of nourishments in a 100 year simulation average size of each nourishment and time of relocation if it occurs in the 100 year simulation in table 5 from table 5 we see that each of these outputs varies widely over the range of parameter values tested we calculate first order sensitivity of each of these five model outputs to each input parameter where we define first order sensitivity s to the i t h input parameter as 13 s i v a r x i e x i y x i v a r y where e x i y x i is the expected value of output y when parameter i is fixed at a value x i and all other input parameters are allowed to vary over their full range v a r x i e x i y x i is the variance of e x i y x i over the full range of values parameter i can take and v a r y is the variance of output y over all parameter uncertainty first order sensitivity s i will always fall between 0 and 1 with larger values indicating higher sensitivity to parameter i saltelli et al 2008 we approximate s i numerically using the method described by saltelli et al 2008 we first define a uniform distribution for each input parameter using the minimum and maximum values given in table 4 and justified in appendix b this creates a 10 dimensional parameter space from which we draw 10 000 random points and run the model for each parameter combination we sort the output in ascending order for each input parameter and define small slices each of which contains 20 points e x i y x i is the mean value within each slice and v a r x i e x i y x i is the variance across all slices v a r y is the total variance of the output we then use equation 13 to calculate s i for each input parameter results are in tables 6 10 tables 6 10 show that the model is generally most sensitive to slr acceleration b exponential development rate d and baseline property value a this suggests that the decisions made depend strongly on slr and the amount of exposed value which is to be expected the optimized objective function and 100 year npv are also sensitive to the discount rate δ which is not surprising because this parameter controls how much future costs and benefits affect present value 3 3 exponential development rate in this section we further explore the impact of the exponential development rate d the sensitivity analysis revealed that model output is sensitive to this parameter tables 6 10 and yet many previous shoreline geo economic modeling studies assume that development does not occur in the coastal zone andré et al 2016 gravens et al 2007 smith et al 2009 fig 5 shows npv over the 100 year simulation for 0 05 d 0 05 where d is the baseline property value growth rate independent from environmental conditions setting d 0 represents continued coastal development or a situation in which property owners gradually leave the hazardous coastal zone without a buyout program under all damage functions and slr scenarios a positive growth rate has the potential to increase npv with all damage functions this growth is substantially lower under high slr because although development can contribute to benefits through an increased tax base it also places more value at risk leading to higher damages with an acceleration in slr the cost of these increased damages is greater and so offsets some of the benefits of development with more coastal development the negative impacts of slr are more pronounced captured by the difference between the npv of each slr scenario given this system architecture with different parameter values it is possible that a lower tax rate or more severe slr damages could cause npv to decrease with higher development this highlights the potential tradeoffs and risks around continued coastal development as a comparison for each strategy we also calculate the total damage d o p t i m a l equal to the damage over the course of the 100 year simulation according to equation 6 assuming the optimal actions are taken and a baseline damage value d b a s e l i n e equal to the damage over the course of the 100 year simulation according to equation 6 assuming no nourishment is undertaken and retreat is not implemented i e a future without project condition we then calculate an alternative npv which we call npv assuming the same optimal strategy is followed where the difference between d b a s e l i n e and d o p t i m a l is considered a benefit and we do not include the damages as costs 14 n p v d b a s e l i n e d o p t i m a l t 0 99 1 1 δ t η x t τ r n l v t τ r n c 1 c 2 1 ξ t x ˆ x t τ r n m t 2 m t v t τ r n m t 2 m t 1 we plot npv as a function of d for each slr scenario and each damage function in fig 6 as shown in fig 6 npv is larger for higher slr this illogical result highlights the problem that calculating avoided damages as benefits rewards situations with higher possible damages comparing figs 5 and 6 we see that there is a faster acceleration in npv than npv as d increases this is because according to npv there is no draw back to increased development thus basing decisions off of a cost benefit analysis using npv could lead to a perverse incentive to increase exposure in hazardous locations in contrast using npv as shown in fig 5 some of the benefits of development are offset by the increase in damages and there could come a point at which the increase in damages is greater than the increase in tax revenue this representation of the system better reflects the actual experience of coastal communities and does not contain the possibility of perverse incentives to increase exposure 3 4 robustness to sea level rise in this section we test the robustness of strategies optimized for one slr scenario to other slr scenarios robust optimization accounts for uncertainty in model inputs or assumptions by identifying solutions based on their performance across a range of scenarios lempert et al 2006 mortazavi naeini et al 2015 robustness metrics include identifying best or worst case scenarios measures of performance variability across scenarios satisficing metrics and regret based metrics giuliani and castelletti 2016 mcphail et al 2018 recognizing that the choice of metric can affect conclusions about robustness giuliani and castelletti 2016 mcphail et al 2018 we take a regret based approach and leave exploration of other robustness metrics to future work we simulate npv under each slr scenario using strategies optimized for each scenario and calculate possible regret when each strategy is followed regret for strategy i under scenario j in year t is defined as the difference between the npv from year 0 to year t under strategy i and the maximum possible npv under scenario j from year 0 to year t fig 7 shows the regret for the strategies optimized for low intermediate and high slr under each scenario as a function of time using each of the damage functions under intermediate and low slr the strategy optimized for high slr has the highest regret with regret increasing rapidly at the time when the high slr strategy indicates retreat should be undertaken corresponding to the fact that retreat is not a low regrets option and could incur large unnecessary costs fig 7 however under high slr not retreating could lead to large regret near the end of the 100 year simulation if damages are related to sea level through the linear or concave exponential decay damage function then the regret of not retreating under a high slr scenario could be greater than the regret of retreating under a lower slr scenario fig 7g l the vertical jumps in regret in fig 7 correspond to decisions to relocate which have very high costs that may be gradually offset as time goes on fig 7f i k l for example under high slr the optimization identifies retreat in year 77 using the polynomial damage function year 45 using the linear damage function and year 25 using the concave damage function in each of these cases the cumulative npv for the relocation strategy is lower than the nourishing strategy optimal under low and intermediate slr for 10 years at which point regret for the low and intermediate slr strategies begins to increase rapidly fig 7f i l it is therefore important to look more than a few years into the future to avoid strategies that may be maladaptive over longer time scales in may also be helpful to take advantage of adaptive planning and adaptation pathways comprised of a sequence of actions triggered by changes in environmental or social conditions called tipping points where actions and tipping points are chosen in such a way that they do not inhibit the ability to adapt in the future barnett et al 2014 haasnoot et al 2013 kwadijk et al 2010 wise et al 2014 4 model limitations and future work we have formulated this model as a deterministic system using annual expected values for stochastic processes this kept the state space to a manageable size so that we could use deterministic dynamic programming to identify a globally optimal strategy an interesting extension would be to formulate this as a stochastic model solved with adaptive dynamic programming this could more accurately represent the system and provide useful insights for adaptation planning conducting a more thorough analysis for d 0 could illuminate how the multiple processes driving property value interact to affect total exposure possibly leading to short term increases with longer term decreases in property value v t this dynamic could change the optimal strategy in interesting ways taking this one step further to better capture a possible feedback between coastal development and nourishment the property growth rate d could be endogenous to the system dependent on beach width so that property value in any given year depends on all previous states and actions as with the stochastic case this could cause the state space to blow up requiring different algorithms for the optimization additionally this model does not include spatial dynamics and longshore sediment transport which could change the dynamics in interesting ways for example mcnamara and werner 2008 use a spatially resolved geo economic model to predict boom and bust cycles in coastal tourism on barrier islands adding a spatial component to our model could help to understand if such dynamics are present in residential property vulnerable to slr and coastal erosion other previous work investigating coupled geo economic shoreline models suggests that including spatial dynamics can lead to emergent and potentially chaotic system behavior lazarus et al 2011 williams et al 2013 another study found that spatial patterns of property along a sandy coastline interact with erosion to determine nourishment frequencies deemed optimal from the perspective of an individual town mcnamara et al 2011 an interesting extension of our model would be to incorporate both spatial and property value dynamics to examine how shoreline management actions in one community may affect the decision to nourish or relocate in neighboring communities williams et al 2013 found that longshore sediment transport creates the possibility for free riders communities that nourish infrequently but experience low erosion rates due to the frequent nourishment activities of neighboring communities our dynamic optimization model could be used to explore how relocating a sucker community defined by williams et al 2013 as a community that nourishes frequently to the benefit of its neighbors affects decisions of the free riding communities it is possible that this could result in widespread managed retreat such emergent behavior could have profound impacts on local tax bases like previous shoreline modeling analyses the model presented here could fail to capture adaptive capacity of the most vulnerable members of society when value at risk is lower expected damages are lower providing less motivation for adapt in place strategies and it is more economical to implement a buyout however from a social justice perspective it may be preferable to prevent declines in property value rather than letting the increased risk from slr lower the cost of a buyout there exist multiple economic frameworks for prioritizing social justice these include prioritizing welfare increases for worse off members of society through the use of distributional weights based upon income or assuming that people are altruistic and derive benefit when others are able to meet their basic needs adler et al 2017 harberger 1984 1978 these methodologies have been well developed in the literature but have yet to be adopted for widespread u s decision making more specific to shoreline management differential taxes to fund beach nourishment have been proposed as one strategy to place a larger share of the financial burden on wealthier ocean front property owners who disproportionately benefit from nourishment projects mullin et al 2019 there is thus room to improve our model by expanding the analysis to include distributional weights measures of community resilience or the unequal allocation of beach nourishment benefits within a coupled dynamic framework 5 discussion and conclusion although this stylized model does not prescribe specific coastal adaptation plans we can use the results to expose the effects of dynamic interactions within and between the physical and economic systems on coastal risk when data were available for parameter tuning we used observations from hutchinson island florida however the economic and physical processes modeled are not unique to this location thus we can use the results from the simulation experiments and sensitivity analysis to better understand risk along other developed sandy coastlines in the u s we have shown that the benefits of coastal zone development are much lower if slr acceleration is at the higher end of the plausible range than under a lower slr scenario this is a key finding of the present work and highlights the importance of considering the effects on risk and adaptation costs of dynamic property value which has generally been excluded in previous quantitative analyses of beach nourishment including lifecycle models gravens et al 2007 and stylized economic models andré et al 2016 smith et al 2009 furthermore sensitivity to the exponential development rate d and baseline property value a tables 6 10 suggest that it is important to include dynamic property value in economic analyses of coastal risk and adaptation while it is fairly intuitive that increased development in hazardous locations could lead to larger losses it is possible to come to the opposite conclusion when using the conventional approach of valuing avoided damages as benefits this may create a perverse incentive to increase potential damages by developing hazardous locations if these damages can then be avoided through management actions by treating expected damages as costs in a coupled system we demonstrate here that increased damages may offset some of the benefits of development although for the parameter values and functional forms used in this study development continues to add positively to npv for the 100 year simulation the observed npv increase is due entirely to the benefits of development and not the increase in potential damages the system architecture is such that if the increase in damages were greater than the increased tax base the npv would decline thus this approach can remove the perverse incentive to develop hazardous locations suggesting that it could be useful to adopt this approach in lifecycle modeling studies because the expected proportion of value damaged can have a large impact on when to relocate fig 4 and table 3 identifying a threshold for damages at which point retreat becomes economically feasible and monitoring damages as sea level rises may help to avoid large losses in the future given the deep uncertainty in this system it may not be possible to predict far in advance when the critical level of damage indicating retreat will be reached but monitoring damages could give some warning as to when the system is approaching the threshold furthermore because the total amount of value not just structural property that can be damaged by slr impacts affects the optimal adaptation strategy it is important to comprehensively evaluate slr damages including social impacts business losses due to recurring flooding and maintenance costs to local infrastructure usace guidelines are to ensure that a selected plan will perform satisfactorily under a range of plausible futures usace 2014 2013 minimizing maximum possible regret frequently referred to as minimax regret or simply the minimax principle savage 1951 is a common approach for decision making under deep uncertainty and can be used to evaluate robustness giuliani and castelletti 2016 kunreuther et al 2013 mcphail et al 2018 walker et al 2013 for the model presented here the strategy with the minimum maximum regret and whether or not this strategy includes relocation in the 100 year simulation depends on the end year of the simulation and the damage function fig 7 because not all costs and benefits are realized immediately the timeframe of analysis matters when comparing alternatives additionally what gets included as damage both structural and non structural may affect the payoff of adaptation decisions similarly the choice of discount rate could bias decisions such that the welfare of future generations is considered less important than that of people alive today adler et al 2017 hanley 2001 lind 1995 in this model adaptation decisions are not highly sensitive to the discount rate tables 8 10 but a higher discount rate could result in putting off managed retreat until a later point in time although discount rates for federal analyses are fixed office of management and budget 2018 usace 2017b 2017a non federal partners may wish to consider the sensitivity of strategies to the discount rate and the ethical implications for future generations this simple model highlights some dynamic processes and feedbacks that could be important to consider when designing coastal management plans we have expanded previous shoreline modeling studies to show that including dynamic property value within a coupled system could change utilitarian decisions regarding coastal management at the federal level considering the complexity of coastal systems has the potential to produce climate adaptation plans that more effectively address community needs and reduce risk over timescales of interest in a deeply uncertain system software availability the model presented in this paper is available as open source matlab code from the github repository https github com emcutler coastal management declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported in part by an appointment to the u s army corps of engineers research participation program administered by the oak ridge institute for science and education through an interagency agreement between the u s department of energy doe and the u s army corps of engineers usace orise is managed by orau under doe contract number de sc0014664 support for m a was provided by an ipa assignment agreement between dartmouth and usace iwr all opinions expressed in this paper are the authors and do not necessarily reflect the policies and views of usace doe or orau orise appendix a model convergence tables a1 a4 give the change in the value function at each iteration for the results shown in fig 4 table a 1 model convergence using the exponential damage function under low intermediate and high slr table a 1 iteration change in value function low slr intermediate slr high slr 1 5 1 10 6 5 4 10 6 6 4 10 6 2 1 0 10 6 9 4 10 5 2 8 10 6 3 1 7 10 4 9 5 10 3 1 2 10 6 4 6 5 10 2 7 5 10 2 1 4 10 5 5 1 1 10 3 3 8 10 2 1 5 10 4 6 0 0 3 1 10 2 0 0 7 2 5 10 2 8 1 8 10 2 9 1 1 10 2 10 0 0 table a 2 model convergence using the polynomial damage function under low intermediate and high slr table a 2 iteration change in value function low slr intermediate slr high slr 1 5 1 10 6 4 6 10 6 7 0 10 6 2 1 1 10 6 1 1 10 6 6 6 10 6 3 1 7 10 4 9 5 10 3 1 9 10 6 4 6 5 10 2 7 5 10 2 2 9 10 5 5 1 1 10 3 3 8 10 2 2 1 10 4 6 0 0 3 1 10 2 0 0 7 2 5 10 2 8 1 8 10 2 9 1 1 10 2 10 0 0 table a 3 model convergence using the linear damage function under low intermediate and high slr table a 3 iteration change in value function low slr intermediate slr high slr 1 4 1 10 6 4 2 10 6 1 5 10 7 2 2 6 10 6 3 7 10 6 1 5 10 7 3 1 7 10 4 2 7 10 6 2 7 10 6 4 6 5 10 2 5 5 10 5 4 1 10 5 5 1 1 10 3 2 5 10 4 1 8 10 4 6 0 0 1 2 10 4 0 0 7 2 5 10 2 8 1 8 10 2 9 1 1 10 2 10 0 0 table a 4 model convergence using the concave damage function under low intermediate and high slr table a 4 iteration change in value function low slr intermediate slr high slr 1 4 3 10 6 1 4 10 7 2 1 10 7 2 3 7 10 6 1 3 10 7 2 0 10 7 3 3 6 10 6 3 2 10 6 1 7 10 6 4 9 0 10 4 4 1 10 5 1 9 10 5 5 2 0 10 4 2 2 10 4 3 9 10 4 6 1 2 10 4 3 1 10 2 0 0 7 8 5 10 3 2 5 10 2 8 0 0 1 8 10 2 9 1 1 10 2 10 0 0 appendix b sensitivity analysis parameter values here we present justification for the range of values included in the sensitivity analysis for the following parameters sea level rise acceleration b discount rate δ impact of beach width on property value α impact of sea level on property value β exponential development rate d baseline property value a amount of non physical value at risk w annual increase in cost of sand ξ increase in storm induced erosion with slr ε and tax rate l sea level rise acceleration following engineer regulation 1100 2 8162 usace 2013 and engineering technical letter 100 2 1 usace 2014 usace jacksonville district identifies three values of b corresponding to three slr scenarios used in the st lucie county coastal storm risk management project assessment usace jacksonville district 2017a we use the minimum of these three values 0 mm yr2 and the maximum of these values 0 113 mm yr2 as the minimum and maximum values for the global sensitivity analysis discount rate for federal executive branch decision making the office of management and budget omb uses a discount rate of 7 but usace also uses the federal water resources discount rate 2 75 for fiscal year 2018 in addition to the omb rate office of management and budget 2018 usace 2017a 2017b however the choice of discount rate when evaluating policies with long term impacts on subjective utility such as those related to climate change is an ethical judgment pertaining to the welfare of future generations ackerman and stanton 2010 beckerman and hepburn 2007 frank 2000 in their analysis of climate policy focusing on intergenerational fairness adler et al 2017 use a discount rate 0 3 with a central estimate of 1 thus for our values of δ we set the maximum equal to the omb rate of 7 and the minimum equal 1 dynamic property value parameters representing our large uncertainty in and lack of data regarding the value of β we let this parameter vary from 1 down to 0 1 we also test a wide range of values for α letting it vary from 1 to 1 2 much larger than our best guess value of α 1 03 taken from the observation that an additional 0 305 m 1 foot of beach width increases the value of existing assets by approximately 1 when beach width is assumed to be endogenous to the model gopalakrishnan et al 2016 baseline property value we vary the baseline property value a from one tenth to three times the best guess value of 6 69 10 8 this gives a minimum of 6 69 10 7 and maximum of 1 73 10 9 non physical value at risk without data to support an estimate for the non physical value at risk w in equation 6 we test a large range of values letting w vary from 2 10 8 to 10 9 rate of increasing cost of sand we vary ξ the annual rate of increase for the cost of sand from 0 to 0 1 assuming that a more than 10 increase in the cost of sand would be unrealistically large rate of increasing storm induced erosion to test sensitivity to the possibility of increasing storm induced erosion we vary ε the increase in storm induced erosion per meter of slr from 0 to 1 tax rate we vary the tax rate l from 0 8 to 1 2 times the best guess value of 22 this gives a minimum tax rate of 17 6 and a maximum of 26 4 
26064,coastlines are vulnerable to sea level rise erosion and flooding all predicted to worsen as climate change continues models are needed that incorporate dynamic processes within coupled economic and physical systems for decision makers including when to retreat from hazardous areas this paper presents a coupled geo economic model of beach width and property value with exogenous sea level rise discrete dynamic programming identifies optimal beach nourishment and managed retreat strategies as a new contribution we treat expected damages as costs which demonstrates that increased damage offsets some benefits of development along hazardous coastlines the model identifies managed retreat as preferable to beach nourishment when damages exceed a threshold regret associated with retreat depends on sea level rise damages and the timeframe of analysis considering dynamic complexity of coastal systems as this model does may produce climate adaptation plans that better address community needs and reduce risk over timescales of interest graphical abstract image 1 keywords managed retreat beach nourishment dynamic programming coastal management climate change adaptation sea level rise 1 introduction coastal areas are vulnerable to hazards such as flooding inundation and erosion which are predicted to worsen as climate continues to change and global sea level rises kron 2013 usgcrp 2018 wong et al 2014 along developed coastlines this can lead to high risk defined as the potential for uncertain events to cause adverse effects which is a function of the hazard the assets exposed and their vulnerability kron 2013 national research council 2014 wong et al 2014 complicating the management of this risk is the fact that future sea level rise slr is deeply uncertain meaning that although it is possible to bound it from above and below we cannot confidently make decisions using the evolving probability distributions available at this time hall et al 2016 as a result traditional expected utility theory cannot be used to manage risk from slr impacts instead it is advisable to adopt robust strategies considering the range of plausible futures using decision criteria such as worst case scenarios satisficing strategies or possible regret hall et al 2019 2016 hallegatte et al 2012 herman et al 2015 kunreuther et al 2013 lempert and collins 2007 maier et al 2016 walker et al 2013 the presence of strong interactions between humans and landscapes suggests that they cannot be meaningfully understood in isolation particularly in places where human occupied land is vulnerable to landscape processes and where measures are taken to protect land from damage lazarus et al 2011 werner and mcnamara 2007 without considering the interactions between human and environmental systems important emergent phenomena may be missed and thus the coupling between systems has implications for management of coastal locations and avoiding maladaptation di baldassarre et al 2013 lazarus et al 2016 murray et al 2013 there exists a growing body of literature treating coastlines as coupled human natural systems with a focus on beach nourishment and other erosion adaptation strategies in the united states u s and elsewhere gopalakrishnan et al 2011 hinkel et al 2013 jin et al 2013 landry 2011 lazarus et al 2016 2011 little and lin 2017 mcnamara et al 2015 murray et al 2013 roebeling et al 2018 slott et al 2008 smith et al 2009 there is also an understanding that slr impacts and adaptation can affect coastal real estate markets bernstein et al 2019 conyers et al 2019 gopalakrishnan et al 2011 mcnamara et al 2015 mcnamara and keeler 2013 ortega and taspinar 2018 treuer et al 2018 walsh et al 2019 in this paper we build on previous beach nourishment modeling studies to incorporate additional feedbacks time dependencies and adaptation strategies to test how a modified system architecture could influence optimal decisions in a modeling context 1 1 coastal risk reduction strategies beach nourishment and dune construction often carried out by the united states army corps of engineers usace in collaboration with a local non federal sponsor are common shoreline erosion control measures in the u s used to minimize risk national research council 2014 usace 2008a 2008b these are effective strategies for reducing damages from storm surge waves and tides and can therefore help mitigate some slr impacts usace 2008a however due to the dynamic nature of coastlines beaches continue to erode and accrete over time dean and dalrymple 2002 thus beach nourishment includes plans to refresh the sand over time as it erodes it is never intended as a permanent solution but as a method to reduce flood and erosion risks to developed areas and multi decadal projects require periodic input of money and sand both of which may be limited resources landry 2011 national research council 2014 usace 2008a furthermore although beaches and dunes can reduce storm impacts to ocean front property beach nourishment does not address back bay flooding in the low lying areas behind barrier islands and around estuaries mullin et al 2019 national research council 2014 there is evidence of a positive feedback between beach nourishment and coastal property value with nourishment raising property values and higher property value justifying further development gopalakrishnan et al 2011 mcnamara et al 2015 murray et al 2013 some studies have even suggested that beach nourishment and other hazard mitigation measures could encourage coastal development thus increasing risk indicating that as a coastal risk management strategy nourishment could rebound or even backfire armstrong et al 2016 burby 2006 cooper and mckenna 2008 keeler et al 2018 lazarus et al 2016 woodruff et al 2018 at the same time there is evidence that slr is causing some coastal property to decline in value bernstein et al 2019 mcalpine and porter 2018 ortega and taspinar 2018 in contrast to beach nourishment which can mitigate damages to ocean front property managed retreat involves relocating assets of value to less hazardous locations andré et al 2016 diaz 2016 hino et al 2017 proposed strategies for implementing retreat include rolling easements in which privately held land is automatically transferred to the state when sea level reaches the property purchase and lease back in which land is purchased from property owners who are then allowed to rent it back for as long as it is habitable sea level purchase options granting organizations the future right but not the obligation to purchase vulnerable land and voluntary buyouts sometimes using natural disasters as windows of opportunity for implementation henderson 2018 kousky 2014 titus 1998 the federal emergency management agency fema administers some property acquisition programs under which individuals may voluntarily sell their property and the land is used for open space recreation or wetlands with new development prohibited eli and unc ie 2017 fema 2015 although programs exist to aid retreat the decision to relocate is complex because it involves human factors such as fear of disruption to community resilience networks potentially higher costs of living elsewhere ties to particular geographic locations and livelihoods that rely on access to ecosystem goods and benefits provided by coastal areas binder et al 2015 dannenberg et al 2019 usgcrp 2018 retreat does not necessarily generate benefits under the current climate or the large range of possible future climate scenarios leading some to classify it as a low regrets option hino et al 2017 ipcc 2014 retreat cannot be easily reversed may lead to loss of cultural heritage and a sense of place or belonging hino et al 2017 roberts and andrei 2015 and could negatively affect mental and physical health dannenberg et al 2019 managed retreat can have racial and social justice implications particularly when decisions rely on cost benefit analysis siders 2019 and when indigenous or lower income populations are involved marino 2018 nevertheless relocation holds promise as a future adaptation strategy dannenberg et al 2019 dyckman et al 2014 hino et al 2017 kousky 2014 1 2 project appraisal in the u s context to be eligible for federal support a coastal risk management project must pass a cost benefit analysis based upon national economic development ned defined as the net monetized value of the national output of goods and services discounted to present values regional economic development environmental quality and other social effects accounts may also be considered in decision making council on environmental quality 2013 office of management and budget 2018 u s water resources council 1983 usace 2008c it is common in both project feasibility analyses and some academic studies of beach nourishment economics to calculate risk mitigation benefits as the reduction in economic damages under one management plan i e future conditions with a particular project compared to a baseline management plan often described as a future without project condition andré et al 2016 gravens et al 2007 landry 2011 usace 2008b this has the effect that increasing potential damages and mitigating those damages can raise the net present value npv of a potential strategy even if expected damages also increase it could be possible therefore to advance economic analysis of coastal management by altering how costs and benefits are quantified for federally supported projects in the u s evaluation of costs and benefits relies on output from lifecycle geophysical and economic models such as beach fx created and maintained by the usace engineer research and development center s coastal and hydraulics laboratory and the u s army engineer institute for water resources gravens et al 2007 beach fx and other lifecycle models provide detailed spatially resolved predictions of shoreline change and economic damages due to inundation wave attack and erosion gravens et al 2007 to improve computational efficiency beach fx does not optimize over the full range of possible management actions but instead evaluates costs and benefits of several pre selected shoreline management strategies gravens et al 2007 usace jacksonville district 2017a this optimization is done over a fixed time horizon usually 50 years usace 2008c and does not calculate payoffs at shorter or longer timescales like all models beach fx makes simplifying assumptions about the coupled human natural system for example despite evidence of continued coastal development applications of beach fx often assume that new development does not occur in the coastal zone usace jacksonville district 2017b 2017a 2017c usace wilmington district 2014 as a model designed to evaluate shore protection projects the management measures included in beach fx simulations are limited to emergency and planned nourishment gravens et al 2007 for coastal communities vulnerable to slr impacts beach nourishment can buy time until longer term solutions such as elevating structures or even managed retreat are implemented however in some locations shoreline management on its own will be insufficient to address slr impacts now or in the future and retreat may be necessary for some communities kousky 2014 usgcrp 2018 1 3 contributions of this work this paper investigates physical and economic dynamic interactions that may affect adaptation decisions we focus our analysis to identify conditions under which property buyouts such as those administered by local communities states or fema may be preferable to continued beach nourishment and dune construction specifically we identify globally optimal management strategies for a coupled dynamic coastal system under several plausible deterministic slr scenarios discrete dynamic programming uses recursion to find a sequence of decisions that optimizes a value function given a set of constraints within a dynamic system bertsekas 1987 rust 1996 we consider the sensitivity of results to several plausible slr futures as well as other uncertain model inputs we are primarily concerned with how the coupled dynamics affect optimal strategies using a novel method for valuing project costs and benefits the model contributes to a more complete understanding of coastal systems threatened by erosion and inundation and could therefore inform coastal management decisions we explain the model in section 2 and present the results from a series of simulation experiments in section 3 section 4 highlights model limitations and section 5 discusses insights that can be gained from this study 2 methods fig 1 provides a graphical representation of the main interactions captured by our model as sea level rises erosion is assumed to decrease beach width and increase hazard which leads to higher risk necessitating adaptation investments for which there are two choices beach nourishment increases beach width to mitigate hazard which reduces risk in the short term however in the longer term lower hazards can incentivize further development which raises risk alternatively managed retreat which is more costly than beach nourishment removes value from the hazardous area reducing risk managed retreat includes removal of public utilities and infrastructure thus also decreasing the need for repairs due to episodic slr and storm events and reducing associated public safety risks in case of evacuating these areas note that when nourishment is undertaken the system remains susceptible to the exogenous slr forcing while managed retreat reduces exposure to slr this model does not have predictive capability but is rather a tool for investigating the relative importance of processes included and evaluating the effect of dynamic interactions on cost benefit analysis of coastal risk management decisions we calibrate the model using data from hutchinson island in st lucie county on the central east coast of florida while true parameter values are unknown the hutchinson island data can constrain our estimates and allow us to conduct a series of tests conclusions are meant to be broadly applicable to ocean front property subject to slr and coastal erosion and do not prescribe management actions for hutchinson island in particular 2 1 dynamic model 2 1 1 beach width we use a model similar to that of smith et al 2009 and extended by cutler et al 2019 this model assumes a straight homogeneous coastline the entire cross shore width of beach is subject to linear erosion which increases with sea level according to a modified bruun rule bruun 1962 dean and houston 2016 rosati et al 2013 the nourished portion of beach width erodes exponentially with time since the last nourishment episode and we include a term for the expected storm induced erosion each year for years in which the beach is not nourished this gives 1 d x t d t μ x ˆ θ e θ τ r d s t d t h e 0 ε s t where μ is the nourished portion of the beach assumed to be 1 x ˆ is the width of the nourished beach θ is the exponential erosion rate of the nourished portion of the beach τ is years since last nourishment r is the slope of the active profile d s t d t is the rate of slr h is the bruun rule correction term and e 0 ε s t is the expected storm induced erosion where e 0 is the expected value of the stochastic storm induced erosion term in cutler et al 2019 initially we assume that ε 0 but test the effect of ε 0 representing the prediction that storms may become more intense as the climate warms emanuel 2013 little et al 2015 sobel et al 2016 we allow expected storm induced erosion to increase proportionally with sea level because there is a physically based correlation between slr and the power dissipation index of tropical cyclones in that both increase with warmer sea surface temperatures little et al 2015 note that here we are separating the effect of slr on gradual erosion captured with the bruun rule from the effect of climate change and slr on storm induced erosion following usace technical letter 1100 2 1 usace 2014 and usace engineer regulation 1100 2 8162 usace 2013 we assume that sea level rises according to 2 s t a t b t 2 2 t t i 1992 where t is years since the start of the simulation s t is local sea level relative to the start of the simulation t i is the start year for the simulation taken to be 2020 a is equal to the historic rate of sea level change accounting for eustatic and local effects b is the predicted acceleration in slr and 1992 is the midpoint of the current national tidal datum epoch to account for uncertainty in future slr three values of b are considered producing low intermediate and high slr scenarios usace 2014 2013 usace jacksonville district 2017a taking the time derivative of equation 2 as needed for the bruun rule in equation 1 we obtain d s t d t a 2 b t t i 1992 parameter values for equations 1 and 2 are taken from cutler et al 2019 based upon previous usace and academic modeling and observational studies these values are given in table 1 2 1 2 coastal development we assume that the total property value v is equal to a baseline property value a modified to account for the effects of beach width positive and sea level negative we also include the possibility for a to increase with time representing a continued demand for coastal property independent of environmental conditions or decrease with time corresponding to a gradual migration away from the coast we spatially aggregate property value so that v t is the entire value at risk in the study area in year t and formulate our equation for v t as 3 v t 1 d t a α x t β s t where a is the baseline property value which inherently grows by a percent d per year and α and β are constants with α 1 and 0 β 1 if α 1 there is no effect of beach width on property value and similarly if β 1 sea level has no effect on property value the exponents x t and s t are beach width and sea level in year t respectively we set a 6 69 10 8 the total value at risk in the study area estimated by usace jacksonville district 2017a for the year 2020 when the beach is completely eroded in their hedonic model of coastal property values gopalakrishnan et al 2011 find that when beach width is considered as endogenous to the geo economic system a one foot increase in beach width corresponds to approximately a one percent increase in property value converting to meters we get α 1 01 3 2808 1 03 lacking empirical evidence for the effect of slr on property value we use a mid range value of β 0 5 and test sensitivity to this assumption by examining how the model responds to 0 1 β 1 with β 0 5 we assume that all else being equal 1 m of slr decreases property value 50 which is roughly on the same order of magnitude as the observation that buyers discount property vulnerable to 1 foot of slr by 14 7 bernstein et al 2019 for our initial analysis we assume d 0 so that the baseline property value is constant with time and later let d vary from 0 05 to 0 05 2 1 3 costs and benefits this model calculates costs and benefits according to ned not because regional economic development environmental quality and other social effects are not important but because ned is critical for securing federal funding given that many communities depend upon federal support for implementing projects there is value in taking a ned perspective for this analysis even though local environmental and social factors are also important to consider in a final decision costs in present value terms accrued in each year t are split into the costs of nourishing c n t the costs of relocating c r t and the costs of damage c d t smith et al 2009 let the costs of nourishment depend on a constant term and a variable term that depends linearly on the width nourished we modify this slightly to allow for the possibility that sand will become more expensive in the future 4 c n t c 1 c 2 1 ξ t x ˆ x t where c 1 is the fixed cost of nourishment and c 2 is the variable cost which increases by an amount ξ every year we assume that given logistical constraints the nourishment interval cannot be shorter than four years thus if fewer than four years have passed since the last nourishment c n t is assumed to be infinite imposing a four year minimum nourishment interval we use the observation that in florida sand costs on average 9 55 per cubic meter hoagland et al 2012 and that the total cost of a 6 1 m nourishment episode over 5470 m of coastline using 224 000 m3 of sand is 14 million usace jacksonville district 2017a this gives c 1 3 5 10 5 per meter and c 2 1 2 10 7 initially we set ξ 0 so that the cost of sand is constant with time however as a limited resource it is possible that the cost of sand could increase as it becomes more scarce national research council 2014 we test the effect of the assumption that the cost of sand is constant by allowing ξ 0 in a sensitivity analysis as a first pass estimate for relocation costs we use 5 c r t v t this is based on the assumption that the dominant cost of relocation will be property buyouts at market value and represents costs to the government or other body responsible for implementing buyouts this is in agreement with the mission of fema funded buyout projects to offer pre flood fair market value to homeowners who experience severe repetitive flood damage eli and unc ie 2017 fema 2013 when valuing damages avoided relative to a baseline management strategy as is usually done it is possible to identify as optimal a strategy that increases total damage if it also increases and mitigates potential damages at a faster rate than an alternate strategy fig 2 we argue therefore that it is more appropriate to treat damages as costs than as potential benefits i e damages avoided this distinction becomes important in the presence of feedbacks that alter the value at risk i e potential damages depending on the actions taken our method is not fundamentally at odds with previous studies that treat avoided damages as benefits e g andré et al 2016 gravens et al 2007 mcnamara and werner 2008 because reducing damages lowers costs which has the same impact on npv as increasing benefits in taking this approach it is possible to expose the unintended consequence of increased flood risk due to further development enabled by shoreline protection coastal development can still contribute positively to npv if the development itself adds utility but this is separate from risk mitigation benefits we assume that the expected cost of damages in year t c d t will be proportional to the value at risk we let d 0 be the annual expected proportion of value damaged under no slr with beach width equal to 0 and assume that the total proportion damaged in a given year t will increase with sea level and decrease with beach width from these assumptions we obtain 6 c d t d 0 f s t g x t v t w where f and g are increasing functions v t is physical property value and w is other value including social and economic systems vulnerable to slr impacts without data to suggest an appropriate value for w we assume that it will be on the same order of magnitude as v and conduct a sensitivity analysis this gives us a best guess value of w 5 10 8 with a range of 2 10 8 w 10 9 using output from beach fx for hutchinson island over all slr scenarios both with and without the recommended plan of a 20 foot berm extension using each reach year as a separate observation we compile a set of triplets containing beach width damage and time usace jacksonville district 2016 for years in which there are multiple reported widths or damages we take an average weighted by the amount of time that passes between reported values so that the time element of each triplet is a single year from which we can calculate sea level according to equation 2 from these triplets we define the vector x to be the sum of the berm and dune width for each reach in each year s to be a vector of sea level values and d to be a vector of damages by reach normalized by the initial value at risk in the reach we then select all x and d values for which s 0 1 meters to represent damages that would occur under no slr we fit a generalized linear model using a binomial distribution and logarithmic link function to get d e 3 2 0 17 x 12 5 4 10 3 1 2 x where d is the expected proportion of value damaged under no slr x is berm width and 12 is the average dune width in meters according to the beach fx output from this we take d 0 5 4 10 3 and g x t κ x t with κ 1 2 fig 3 a by assumption f 0 1 the ground elevation of hutchinson island is generally less than 3 05 m 10 feet usace jacksonville district 2017a we therefore assume that with 3 m of slr and no berm all value on hutchinson island would be damaged that is 5 4 10 3 f 3 1 2 0 1 so f 3 185 with little evidence or precedent for the most appropriate functional form for f we consider several different possibilities for f a polynomial f s t 1 s t φ with φ 3 8 a linear function f s t 1 φ s t with φ 61 per meter an exponential function f s t φ s t with φ 5 7 and a concave negative decay function f s t 1 φ 1 e s t with φ 109 fig 3b we compare results using these four damage functions to examine sensitivity to the functional form used benefits are calculated as the recreational and habitat benefits of the beach b b t and the benefits of increased economic development in the study area b d t we assume that there is no inherent value to keeping the community in its current location while this is almost certainly not true from the perspective of a community it can be justified from the ned point of view if the national scale output of all goods and services produced at this location are transferrable to other places institute for water resources 2011 u s water resources council 1983 we do not include damage mitigation as benefits because as explained above the cost of damages is calculated in equation 6 we assume that the benefits of the beach in year t are proportional to beach width according to 7 b b t η x t based on information from the usace jacksonville district 2017a we assume 0 0929 square meters 1 square foot of land is worth 14 assuming that the study area is 5470 m long it follows that η 8 24 10 5 per meter of cross shore buildout we use taxes levied on property as a proxy for the benefits gained from additional development so that 8 b d t l v t where we set l equal to the average of the six different total final tax rates for st lucie county in 2018 giving l 22 st lucie county property appraiser 2018 2 2 dynamic optimization we use a discrete dynamic programming algorithm to identify the optimal management strategy given equations 1 8 in the model we include three possible management actions do nothing nourish the beach to a width equal to x ˆ or buyout all value at risk if the decision to nourish is made in year t then beach width in year t 1 is set equal to x ˆ if the decision to relocate is made in year t then at time t 1 v and w are set to 0 we acknowledge that this is a simplification and that in reality planning and implementing managed retreat takes many years dannenberg et al 2019 we further assume that retreat does not affect the width of the beach and thus after retreat the system continues to behave as before with the only difference being that v w 0 so there are no benefits from development b d t 0 and no costs due to damage c d t 0 for all t t r where t r is the time at which retreat is implemented we let m t represent the management action in year t with m t 0 representing the decision to do nothing m t 1 representing the decision to nourish and m t 2 representing the decision to relocate we solve the system using the publicly available mdpsolve software designed to solve discrete time markov decision problems in matlab fackler 2015 the software uses backward recursion value function iteration or policy iteration to identify the strategy that will optimize the discounted npv over a given time horizon t which may be finite or infinite we also simulate the system under the optimal strategy for 100 years mdpsolve requires that the system be formulated as a markov decision problem because equations 1 3 are all time dependent the system as formulated above does not meet this requirement fortunately it is possible to redefine the system using the following four state variables t time since the start of the simulation τ time since the last nourishment r time since the decision to relocate was made n whether or not the beach is nourished in a given year the initial values and state equations for these variables are given in table 2 note that these state equations depend only on the current state and the management action m making this is an autonomous system of discrete difference equations that can be solved as a markov decision problem using dynamic programming algorithms defining the system in this way has the additional benefit that although we solve it as a discrete system the only variable that needs to be discretized is time from these four state variables we can use the following equations to calculate sea level s beach width x and property value v at any point in the four dimensional state space 9 s t τ r n a t b t 2 2 t t i 1992 10 x t τ r n max 0 x ˆ n 1 μ x ˆ μ x ˆ e θ τ r s t τ r n s t τ 0 r n h τ e 0 τ ε 0 5 a t 2 b t 3 3 t 2 t i 1992 n 1 11 v t τ r n 1 d t a α x β s i f r 0 v t r τ r n i f 0 r 1 0 i f r 1 equation 10 is a closed form solution for equation 1 and equations 9 and 11 are equivalent to equations 2 and 3 respectively using equations 4 8 to calculate the costs and benefits and constant exponential discounting with a discount rate δ to calculate the present value of future costs and benefits we obtain the following objective function 12 n p v m a x t 0 t 1 1 δ t η x t τ r n l v t τ r n c 1 c 2 1 ξ t x ˆ x t τ r n m t 2 m t v t τ r n m t 2 m t 1 d 0 f s t τ r n g x t τ r n v t τ r n w where as before η is the land benefit l is the tax rate c 1 is the fixed cost of nourishment c 2 is the cost of sand ξ is the increase in the cost of sand x ˆ is the nourished width of the beach d 0 is the damage function constant f is the proportion of value damaged as a function of sea level g is the proportion of value damaged as a function of beach width w is the non structural value vulnerable to slr impacts m is the control variable with m 0 being no action m 1 being nourishment and m 2 being retreat and s t τ r n x t τ r n and v t τ r n are calculated according to equations 9 11 3 simulation experiments we wish to use the model to better understand the dynamics of the coupled geo economic system and how the optimal strategy depends on the modeled processes and feedbacks we first present the baseline results in section 3 1 using the default parameter values given in table 1 for each of the three slr scenarios and four damage functional forms for f s in equation 6 in section 3 2 we present a global sensitivity analysis on uncertain parameters based on the results of the sensitivity analysis we further explore the effect of the exponential development rate d in section 3 3 to better understand the impact of uncertainty in the system in section 3 4 for several different strategies we calculate possible regret over the course of the simulation through these experiments we hope to better understand the system dynamics and how interactions among the model variables affect the optimal strategy and npv 3 1 baseline results fig 4 shows the optimal time paths for beach width and property value for the default parameter values given in table 1 using each of the three slr scenarios and four damage functional forms for f s in equation 6 moving left to right across the columns of fig 4 i e from exponential to polynomial to linear to concave exponential decay damage functions the optimal time of relocation occurs earlier under high slr for intermediate slr retreat is only optimal using the concave exponential decay damage function and retreat is not optimal under low slr according to this model comparing figs 3b and 4 it is clear that when a greater proportion of value is damaged retreat is implemented sooner as would be expected with the concave damage function under intermediate slr it remains optimal to continue nourishing the beach even after it provides no risk reduction benefits because the beach has inherent value this does not occur under high slr because the erosion rate is high enough that the beach disappears before sufficient habitat and recreational benefits accrue to offset the cost of nourishment thus both slr and the impact of slr on damage costs affect the optimal time of relocation and nourishment decisions for each slr damage function combination we give the change in the value function at each iteration of the solver in appendix a for each of the five scenarios in fig 4 in which relocation occurs table 3 gives the time of relocation sea level at time of relocation and value of d 0 f s t r where d 0 5 4 10 3 is the proportion of value damaged under no slr with beach width equal to 0 the function f relates slr to expected damages and s t r is sea level at the time of relocation t r although there is a wide range of times and sea levels at which relocation occurs d 0 f s t r is always in the range of 0 1 0 3 spanning less than 20 of the range of d 0 f 0 0 to d 0 f 3 1 this suggests that relocation becomes optimal when the annual expected proportion of value damaged due to slr reaches a critical level and that this critical level is roughly constant across scenarios the model does not specify precisely this critical threshold but it provides evidence that it exists this is similar to previous modeling results showing that economic optimizers abandon coastal property at a critical risk threshold mcnamara and keeler 2013 and supports the suggestion of hall et al 2019 that the focus ought to shift from how much sea level will rise to when slr will trigger important risks it is also in line with the concept of adaptation tipping points kwadijk et al 2010 this evidence of a critical threshold could therefore motivate research to identify the level of damage in addition to the year or sea level that justifies managed retreat 3 2 sensitivity analysis to test model sensitivity to uncertainty in parameter values we conduct a global sensitivity analysis in which we vary the slr acceleration b discount rate δ impact of beach width on property value α impact of sea level on property value β baseline property value a amount of non physical value at risk w annual increase in cost of sand ξ increase in storm induced erosion with slr ε and tax rate l using each damage function minimum and maximum values for each parameter are in table 4 justification for the range of parameter values tested is given in appendix b we run the model using each damage function for 10 000 random combinations of parameter values and give the minimum and maximum of the optimized objective function npv over a 100 year simulation total number of nourishments in a 100 year simulation average size of each nourishment and time of relocation if it occurs in the 100 year simulation in table 5 from table 5 we see that each of these outputs varies widely over the range of parameter values tested we calculate first order sensitivity of each of these five model outputs to each input parameter where we define first order sensitivity s to the i t h input parameter as 13 s i v a r x i e x i y x i v a r y where e x i y x i is the expected value of output y when parameter i is fixed at a value x i and all other input parameters are allowed to vary over their full range v a r x i e x i y x i is the variance of e x i y x i over the full range of values parameter i can take and v a r y is the variance of output y over all parameter uncertainty first order sensitivity s i will always fall between 0 and 1 with larger values indicating higher sensitivity to parameter i saltelli et al 2008 we approximate s i numerically using the method described by saltelli et al 2008 we first define a uniform distribution for each input parameter using the minimum and maximum values given in table 4 and justified in appendix b this creates a 10 dimensional parameter space from which we draw 10 000 random points and run the model for each parameter combination we sort the output in ascending order for each input parameter and define small slices each of which contains 20 points e x i y x i is the mean value within each slice and v a r x i e x i y x i is the variance across all slices v a r y is the total variance of the output we then use equation 13 to calculate s i for each input parameter results are in tables 6 10 tables 6 10 show that the model is generally most sensitive to slr acceleration b exponential development rate d and baseline property value a this suggests that the decisions made depend strongly on slr and the amount of exposed value which is to be expected the optimized objective function and 100 year npv are also sensitive to the discount rate δ which is not surprising because this parameter controls how much future costs and benefits affect present value 3 3 exponential development rate in this section we further explore the impact of the exponential development rate d the sensitivity analysis revealed that model output is sensitive to this parameter tables 6 10 and yet many previous shoreline geo economic modeling studies assume that development does not occur in the coastal zone andré et al 2016 gravens et al 2007 smith et al 2009 fig 5 shows npv over the 100 year simulation for 0 05 d 0 05 where d is the baseline property value growth rate independent from environmental conditions setting d 0 represents continued coastal development or a situation in which property owners gradually leave the hazardous coastal zone without a buyout program under all damage functions and slr scenarios a positive growth rate has the potential to increase npv with all damage functions this growth is substantially lower under high slr because although development can contribute to benefits through an increased tax base it also places more value at risk leading to higher damages with an acceleration in slr the cost of these increased damages is greater and so offsets some of the benefits of development with more coastal development the negative impacts of slr are more pronounced captured by the difference between the npv of each slr scenario given this system architecture with different parameter values it is possible that a lower tax rate or more severe slr damages could cause npv to decrease with higher development this highlights the potential tradeoffs and risks around continued coastal development as a comparison for each strategy we also calculate the total damage d o p t i m a l equal to the damage over the course of the 100 year simulation according to equation 6 assuming the optimal actions are taken and a baseline damage value d b a s e l i n e equal to the damage over the course of the 100 year simulation according to equation 6 assuming no nourishment is undertaken and retreat is not implemented i e a future without project condition we then calculate an alternative npv which we call npv assuming the same optimal strategy is followed where the difference between d b a s e l i n e and d o p t i m a l is considered a benefit and we do not include the damages as costs 14 n p v d b a s e l i n e d o p t i m a l t 0 99 1 1 δ t η x t τ r n l v t τ r n c 1 c 2 1 ξ t x ˆ x t τ r n m t 2 m t v t τ r n m t 2 m t 1 we plot npv as a function of d for each slr scenario and each damage function in fig 6 as shown in fig 6 npv is larger for higher slr this illogical result highlights the problem that calculating avoided damages as benefits rewards situations with higher possible damages comparing figs 5 and 6 we see that there is a faster acceleration in npv than npv as d increases this is because according to npv there is no draw back to increased development thus basing decisions off of a cost benefit analysis using npv could lead to a perverse incentive to increase exposure in hazardous locations in contrast using npv as shown in fig 5 some of the benefits of development are offset by the increase in damages and there could come a point at which the increase in damages is greater than the increase in tax revenue this representation of the system better reflects the actual experience of coastal communities and does not contain the possibility of perverse incentives to increase exposure 3 4 robustness to sea level rise in this section we test the robustness of strategies optimized for one slr scenario to other slr scenarios robust optimization accounts for uncertainty in model inputs or assumptions by identifying solutions based on their performance across a range of scenarios lempert et al 2006 mortazavi naeini et al 2015 robustness metrics include identifying best or worst case scenarios measures of performance variability across scenarios satisficing metrics and regret based metrics giuliani and castelletti 2016 mcphail et al 2018 recognizing that the choice of metric can affect conclusions about robustness giuliani and castelletti 2016 mcphail et al 2018 we take a regret based approach and leave exploration of other robustness metrics to future work we simulate npv under each slr scenario using strategies optimized for each scenario and calculate possible regret when each strategy is followed regret for strategy i under scenario j in year t is defined as the difference between the npv from year 0 to year t under strategy i and the maximum possible npv under scenario j from year 0 to year t fig 7 shows the regret for the strategies optimized for low intermediate and high slr under each scenario as a function of time using each of the damage functions under intermediate and low slr the strategy optimized for high slr has the highest regret with regret increasing rapidly at the time when the high slr strategy indicates retreat should be undertaken corresponding to the fact that retreat is not a low regrets option and could incur large unnecessary costs fig 7 however under high slr not retreating could lead to large regret near the end of the 100 year simulation if damages are related to sea level through the linear or concave exponential decay damage function then the regret of not retreating under a high slr scenario could be greater than the regret of retreating under a lower slr scenario fig 7g l the vertical jumps in regret in fig 7 correspond to decisions to relocate which have very high costs that may be gradually offset as time goes on fig 7f i k l for example under high slr the optimization identifies retreat in year 77 using the polynomial damage function year 45 using the linear damage function and year 25 using the concave damage function in each of these cases the cumulative npv for the relocation strategy is lower than the nourishing strategy optimal under low and intermediate slr for 10 years at which point regret for the low and intermediate slr strategies begins to increase rapidly fig 7f i l it is therefore important to look more than a few years into the future to avoid strategies that may be maladaptive over longer time scales in may also be helpful to take advantage of adaptive planning and adaptation pathways comprised of a sequence of actions triggered by changes in environmental or social conditions called tipping points where actions and tipping points are chosen in such a way that they do not inhibit the ability to adapt in the future barnett et al 2014 haasnoot et al 2013 kwadijk et al 2010 wise et al 2014 4 model limitations and future work we have formulated this model as a deterministic system using annual expected values for stochastic processes this kept the state space to a manageable size so that we could use deterministic dynamic programming to identify a globally optimal strategy an interesting extension would be to formulate this as a stochastic model solved with adaptive dynamic programming this could more accurately represent the system and provide useful insights for adaptation planning conducting a more thorough analysis for d 0 could illuminate how the multiple processes driving property value interact to affect total exposure possibly leading to short term increases with longer term decreases in property value v t this dynamic could change the optimal strategy in interesting ways taking this one step further to better capture a possible feedback between coastal development and nourishment the property growth rate d could be endogenous to the system dependent on beach width so that property value in any given year depends on all previous states and actions as with the stochastic case this could cause the state space to blow up requiring different algorithms for the optimization additionally this model does not include spatial dynamics and longshore sediment transport which could change the dynamics in interesting ways for example mcnamara and werner 2008 use a spatially resolved geo economic model to predict boom and bust cycles in coastal tourism on barrier islands adding a spatial component to our model could help to understand if such dynamics are present in residential property vulnerable to slr and coastal erosion other previous work investigating coupled geo economic shoreline models suggests that including spatial dynamics can lead to emergent and potentially chaotic system behavior lazarus et al 2011 williams et al 2013 another study found that spatial patterns of property along a sandy coastline interact with erosion to determine nourishment frequencies deemed optimal from the perspective of an individual town mcnamara et al 2011 an interesting extension of our model would be to incorporate both spatial and property value dynamics to examine how shoreline management actions in one community may affect the decision to nourish or relocate in neighboring communities williams et al 2013 found that longshore sediment transport creates the possibility for free riders communities that nourish infrequently but experience low erosion rates due to the frequent nourishment activities of neighboring communities our dynamic optimization model could be used to explore how relocating a sucker community defined by williams et al 2013 as a community that nourishes frequently to the benefit of its neighbors affects decisions of the free riding communities it is possible that this could result in widespread managed retreat such emergent behavior could have profound impacts on local tax bases like previous shoreline modeling analyses the model presented here could fail to capture adaptive capacity of the most vulnerable members of society when value at risk is lower expected damages are lower providing less motivation for adapt in place strategies and it is more economical to implement a buyout however from a social justice perspective it may be preferable to prevent declines in property value rather than letting the increased risk from slr lower the cost of a buyout there exist multiple economic frameworks for prioritizing social justice these include prioritizing welfare increases for worse off members of society through the use of distributional weights based upon income or assuming that people are altruistic and derive benefit when others are able to meet their basic needs adler et al 2017 harberger 1984 1978 these methodologies have been well developed in the literature but have yet to be adopted for widespread u s decision making more specific to shoreline management differential taxes to fund beach nourishment have been proposed as one strategy to place a larger share of the financial burden on wealthier ocean front property owners who disproportionately benefit from nourishment projects mullin et al 2019 there is thus room to improve our model by expanding the analysis to include distributional weights measures of community resilience or the unequal allocation of beach nourishment benefits within a coupled dynamic framework 5 discussion and conclusion although this stylized model does not prescribe specific coastal adaptation plans we can use the results to expose the effects of dynamic interactions within and between the physical and economic systems on coastal risk when data were available for parameter tuning we used observations from hutchinson island florida however the economic and physical processes modeled are not unique to this location thus we can use the results from the simulation experiments and sensitivity analysis to better understand risk along other developed sandy coastlines in the u s we have shown that the benefits of coastal zone development are much lower if slr acceleration is at the higher end of the plausible range than under a lower slr scenario this is a key finding of the present work and highlights the importance of considering the effects on risk and adaptation costs of dynamic property value which has generally been excluded in previous quantitative analyses of beach nourishment including lifecycle models gravens et al 2007 and stylized economic models andré et al 2016 smith et al 2009 furthermore sensitivity to the exponential development rate d and baseline property value a tables 6 10 suggest that it is important to include dynamic property value in economic analyses of coastal risk and adaptation while it is fairly intuitive that increased development in hazardous locations could lead to larger losses it is possible to come to the opposite conclusion when using the conventional approach of valuing avoided damages as benefits this may create a perverse incentive to increase potential damages by developing hazardous locations if these damages can then be avoided through management actions by treating expected damages as costs in a coupled system we demonstrate here that increased damages may offset some of the benefits of development although for the parameter values and functional forms used in this study development continues to add positively to npv for the 100 year simulation the observed npv increase is due entirely to the benefits of development and not the increase in potential damages the system architecture is such that if the increase in damages were greater than the increased tax base the npv would decline thus this approach can remove the perverse incentive to develop hazardous locations suggesting that it could be useful to adopt this approach in lifecycle modeling studies because the expected proportion of value damaged can have a large impact on when to relocate fig 4 and table 3 identifying a threshold for damages at which point retreat becomes economically feasible and monitoring damages as sea level rises may help to avoid large losses in the future given the deep uncertainty in this system it may not be possible to predict far in advance when the critical level of damage indicating retreat will be reached but monitoring damages could give some warning as to when the system is approaching the threshold furthermore because the total amount of value not just structural property that can be damaged by slr impacts affects the optimal adaptation strategy it is important to comprehensively evaluate slr damages including social impacts business losses due to recurring flooding and maintenance costs to local infrastructure usace guidelines are to ensure that a selected plan will perform satisfactorily under a range of plausible futures usace 2014 2013 minimizing maximum possible regret frequently referred to as minimax regret or simply the minimax principle savage 1951 is a common approach for decision making under deep uncertainty and can be used to evaluate robustness giuliani and castelletti 2016 kunreuther et al 2013 mcphail et al 2018 walker et al 2013 for the model presented here the strategy with the minimum maximum regret and whether or not this strategy includes relocation in the 100 year simulation depends on the end year of the simulation and the damage function fig 7 because not all costs and benefits are realized immediately the timeframe of analysis matters when comparing alternatives additionally what gets included as damage both structural and non structural may affect the payoff of adaptation decisions similarly the choice of discount rate could bias decisions such that the welfare of future generations is considered less important than that of people alive today adler et al 2017 hanley 2001 lind 1995 in this model adaptation decisions are not highly sensitive to the discount rate tables 8 10 but a higher discount rate could result in putting off managed retreat until a later point in time although discount rates for federal analyses are fixed office of management and budget 2018 usace 2017b 2017a non federal partners may wish to consider the sensitivity of strategies to the discount rate and the ethical implications for future generations this simple model highlights some dynamic processes and feedbacks that could be important to consider when designing coastal management plans we have expanded previous shoreline modeling studies to show that including dynamic property value within a coupled system could change utilitarian decisions regarding coastal management at the federal level considering the complexity of coastal systems has the potential to produce climate adaptation plans that more effectively address community needs and reduce risk over timescales of interest in a deeply uncertain system software availability the model presented in this paper is available as open source matlab code from the github repository https github com emcutler coastal management declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported in part by an appointment to the u s army corps of engineers research participation program administered by the oak ridge institute for science and education through an interagency agreement between the u s department of energy doe and the u s army corps of engineers usace orise is managed by orau under doe contract number de sc0014664 support for m a was provided by an ipa assignment agreement between dartmouth and usace iwr all opinions expressed in this paper are the authors and do not necessarily reflect the policies and views of usace doe or orau orise appendix a model convergence tables a1 a4 give the change in the value function at each iteration for the results shown in fig 4 table a 1 model convergence using the exponential damage function under low intermediate and high slr table a 1 iteration change in value function low slr intermediate slr high slr 1 5 1 10 6 5 4 10 6 6 4 10 6 2 1 0 10 6 9 4 10 5 2 8 10 6 3 1 7 10 4 9 5 10 3 1 2 10 6 4 6 5 10 2 7 5 10 2 1 4 10 5 5 1 1 10 3 3 8 10 2 1 5 10 4 6 0 0 3 1 10 2 0 0 7 2 5 10 2 8 1 8 10 2 9 1 1 10 2 10 0 0 table a 2 model convergence using the polynomial damage function under low intermediate and high slr table a 2 iteration change in value function low slr intermediate slr high slr 1 5 1 10 6 4 6 10 6 7 0 10 6 2 1 1 10 6 1 1 10 6 6 6 10 6 3 1 7 10 4 9 5 10 3 1 9 10 6 4 6 5 10 2 7 5 10 2 2 9 10 5 5 1 1 10 3 3 8 10 2 2 1 10 4 6 0 0 3 1 10 2 0 0 7 2 5 10 2 8 1 8 10 2 9 1 1 10 2 10 0 0 table a 3 model convergence using the linear damage function under low intermediate and high slr table a 3 iteration change in value function low slr intermediate slr high slr 1 4 1 10 6 4 2 10 6 1 5 10 7 2 2 6 10 6 3 7 10 6 1 5 10 7 3 1 7 10 4 2 7 10 6 2 7 10 6 4 6 5 10 2 5 5 10 5 4 1 10 5 5 1 1 10 3 2 5 10 4 1 8 10 4 6 0 0 1 2 10 4 0 0 7 2 5 10 2 8 1 8 10 2 9 1 1 10 2 10 0 0 table a 4 model convergence using the concave damage function under low intermediate and high slr table a 4 iteration change in value function low slr intermediate slr high slr 1 4 3 10 6 1 4 10 7 2 1 10 7 2 3 7 10 6 1 3 10 7 2 0 10 7 3 3 6 10 6 3 2 10 6 1 7 10 6 4 9 0 10 4 4 1 10 5 1 9 10 5 5 2 0 10 4 2 2 10 4 3 9 10 4 6 1 2 10 4 3 1 10 2 0 0 7 8 5 10 3 2 5 10 2 8 0 0 1 8 10 2 9 1 1 10 2 10 0 0 appendix b sensitivity analysis parameter values here we present justification for the range of values included in the sensitivity analysis for the following parameters sea level rise acceleration b discount rate δ impact of beach width on property value α impact of sea level on property value β exponential development rate d baseline property value a amount of non physical value at risk w annual increase in cost of sand ξ increase in storm induced erosion with slr ε and tax rate l sea level rise acceleration following engineer regulation 1100 2 8162 usace 2013 and engineering technical letter 100 2 1 usace 2014 usace jacksonville district identifies three values of b corresponding to three slr scenarios used in the st lucie county coastal storm risk management project assessment usace jacksonville district 2017a we use the minimum of these three values 0 mm yr2 and the maximum of these values 0 113 mm yr2 as the minimum and maximum values for the global sensitivity analysis discount rate for federal executive branch decision making the office of management and budget omb uses a discount rate of 7 but usace also uses the federal water resources discount rate 2 75 for fiscal year 2018 in addition to the omb rate office of management and budget 2018 usace 2017a 2017b however the choice of discount rate when evaluating policies with long term impacts on subjective utility such as those related to climate change is an ethical judgment pertaining to the welfare of future generations ackerman and stanton 2010 beckerman and hepburn 2007 frank 2000 in their analysis of climate policy focusing on intergenerational fairness adler et al 2017 use a discount rate 0 3 with a central estimate of 1 thus for our values of δ we set the maximum equal to the omb rate of 7 and the minimum equal 1 dynamic property value parameters representing our large uncertainty in and lack of data regarding the value of β we let this parameter vary from 1 down to 0 1 we also test a wide range of values for α letting it vary from 1 to 1 2 much larger than our best guess value of α 1 03 taken from the observation that an additional 0 305 m 1 foot of beach width increases the value of existing assets by approximately 1 when beach width is assumed to be endogenous to the model gopalakrishnan et al 2016 baseline property value we vary the baseline property value a from one tenth to three times the best guess value of 6 69 10 8 this gives a minimum of 6 69 10 7 and maximum of 1 73 10 9 non physical value at risk without data to support an estimate for the non physical value at risk w in equation 6 we test a large range of values letting w vary from 2 10 8 to 10 9 rate of increasing cost of sand we vary ξ the annual rate of increase for the cost of sand from 0 to 0 1 assuming that a more than 10 increase in the cost of sand would be unrealistically large rate of increasing storm induced erosion to test sensitivity to the possibility of increasing storm induced erosion we vary ε the increase in storm induced erosion per meter of slr from 0 to 1 tax rate we vary the tax rate l from 0 8 to 1 2 times the best guess value of 22 this gives a minimum tax rate of 17 6 and a maximum of 26 4 
