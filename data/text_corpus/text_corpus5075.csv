index,text
25375,in hydrological modeling the longest flow path is an important feature used to characterize a catchment many existing gis platforms offer dedicated software tools for its identification and delineation generally implementing methods based on searching through the flow direction data unfortunately currently available algorithms for this task often turn out to be inefficient especially when working with modern large datasets moreover existing methods often rely on incorrect assumptions or perform calculations in a way that can lead to precision issues in this work new parallel algorithms were developed tested and presented measurements show that two of the newly proposed implementations are able to identify the longest flow paths in significantly less time compared with other existing methods keywords longest flow path gis hydrology parallel algorithms high performance computing openmp data availability the source code is freely available in a public github repository the data used for performance measurements originated from publicly available resources software availability software algorithms for finding the longest flow paths description source code of all algorithms developed tested and presented as part of this work including a simple measurement application developer bartłomiej kotyra contact address bartlomiej kotyra mail umcs pl language c 11 openmp libraries required gdal geospatial data abstraction library availability freely available at https github com bkotyra longest flow path 1 introduction in the area of hydrological modeling catchments are characterized and described using multiple features and various parameters one of the important ones is the longest flow path also referred to as the longest watercourse or the longest drainage path dawson et al 2006 huang and lee 2016 michailidi et al 2018 in the hydrological context a flow path is understood as a route followed by water draining from one point to another there are multiple flow paths of varying lengths in any given catchment the longest one leads to the watershed outlet usually starting at some location on the watershed boundary although this is not always the case cho 2020 the longest flow path is typically used to determine the attributes for hydrological model parameterization mainly the time of concentration and the lag time of a watershed michailidi et al 2018 sultan et al 2022 it can also be used as a basis for calculating some of the properties and characteristics of a catchment maathuis and wang 2006 jaffrés et al 2021 the longest flow path as well as other features derived from it may be useful in estimating certain flood related indices karalis et al 2014 latt et al 2015 both early and modern gis software packages consider the longest flow path as an essential element in hydrological modeling and provide tools for its delineation and analysis gallant and wilson 1996 maidment and morehouse 2002 merkel et al 2008 ramly and tahir 2016 recent literature draws attention to the time consuming nature of this operation and the low processing speed of the commonly used tools castro and maidment 2020 attempts are still being made to develop new more efficient algorithms for this task cho 2020 the geospatial data available today offers high resolution and unprecedented precision but this comes at the cost of significant dataset sizes creating new computational challenges sten et al 2016 at the same time modern hardware architectures make the access to multicore and many core processors more and more common programming standards like openmp make it relatively easy to achieve high computing performance on these devices due to parallelism chapman et al 2007 however designing and implementing parallel algorithms still remains a non trivial task leading to a significant gap between the existing technology and its practical applications tang and wang 2020 the available software often fails to efficiently use the underlying hardware and proves to be impractical in the context of modern large datasets the main motivation behind this research was the need for more efficient tools and techniques for finding the longest flow paths in geospatial data as can be concluded from a review of the available software and literature there is still room for significant improvement the goal of this study was to develop and present new fast raster based algorithms for solving this task with particular attention to the possibilities offered by modern multicore architectures the authors believe that this work improves upon existing solutions and takes a significant step towards higher performance the remainder of this chapter outlines the foundations of existing techniques for finding the longest flow paths in raster data and describes the available algorithms and software tools for this task section 2 specifies the constraints of the problem to be solved along with its specific variants and describes the approach used in this work to design and implement new more efficient solutions in section 3 all the developed algorithms a total of seven implementations including two reference ones are presented and discussed in detail section 4 describes the data and procedures used to measure and evaluate the performance of the proposed algorithms including comparisons with existing gis software the results of these measurements are presented and discussed in section 5 finally the conclusions of this study its limitations and possible future work are briefly summarized in section 6 1 1 existing techniques the existing literature related to finding the longest flow path as well as the available software tools generally refer to and build upon the approach based on square grid digital elevation models dems and their derivative data specifically flow direction rasters the basic concepts behind these methods are well known and have been widely used for decades o callaghan and mark 1984 jenson and domingue 1988 in the literature there are two significantly different approaches to the concept of flow direction in the single flow approach each raster cell points to one of its immediate neighbors directing all flow to a single downstream cell in the multiple flow approach the flow may be proportionally divided among more than one neighbor schäuble et al 2008 both approaches have multiple practical applications as some problems are naturally better expressed in terms of one or the other barták 2009 however different algorithms for determining the flow direction can lead to significantly varying results huang and lee 2016 the single flow approach is most often considered more appropriate for determining and analyzing flow paths as multiple flow algorithms lead to excessive dispersion and overestimated path lengths orlandini and moretti 2009 li et al 2020 the length of a given flow path is typically estimated as the sum of the distances between successive raster cells belonging to that path paz et al 2008 fig 1 illustrates this fundamental idea the flow path can be conceptually expressed as a line connecting the centers of consecutive downstream cells the center to center distance between orthogonal horizontal and vertical neighbors is considered equal to the dimension of a single cell and the distance between diagonal neighbors is calculated as the dimension of the cell multiplied by the square root of 2 1 414 although this measure is essentially a simple approximation it is generally regarded as being relatively consistent with the actual length of the flow path fairfield and leymarie 1991 paz et al 2008 the flow direction raster is used as fundamental data in these calculations as it allows any flow path to be traced cell by cell smith 1997 cho 2020 lindsay 2022 it should be noted that while this kind of approach may be considered unsuitable for some other hydrological modeling tasks i e requiring consideration of a wider range of factors it is broadly accepted and implemented as a standard method for quickly comparing flow paths when searching for the longest one the remaining sections of this paper deal with techniques algorithms and tools based on these foundations 1 1 1 algorithms described in the literature the earliest published works discussing the algorithmic determination of the longest flow path of which the authors are aware were presented in smith 1995 1997 the papers are focused on the architecture of a software system called the hydrologic data development system hdds it uses spatial analysis techniques to determine multiple hydrologic parameters including the length and location of the longest flow path the approach used in the hdds is based on square grid dems the procedure of determining the longest flow path begins with the preparation of two additional rasters which are calculated using the flow direction data in the first one each cell contains its downstream flow length which is defined as the downslope distance to a pour point the second one consists of upstream flow length values defined as the distance to the drainage divide the values of the corresponding cells from both rasters are then added together the highest values obtained in this way are equal to the length of the longest flow path importantly only cells belonging to the longest path can obtain the maximum value which allows them to be easily identified all other cells obtain lower values olivera and maidment 1998 presented another grid based geographic information system for hydrologic modeling the method for determining the longest flow path described here is generally the same as in smith 1997 the system requires both the upstream and downstream flow length rasters as input data these rasters are added together and then the set of cells with the highest sum is identified as the longest flow path this approach was later described in more detail by the same author in olivera 2001 cho 2020 proposed using a recursive approach to improve performance over existing techniques and reduce computational time needed the basic idea behind this method is to search the flow direction raster by starting at the outlet point and moving upstream recursively invoking the calculation of the longest flow path for topographically higher cells this approach makes it possible to avoid calculating downstream and upstream flow length rasters due to memory issues typical of recursion i e stack overflows an iterative version of the algorithm was also presented and recommended for use developing this concept further cho 2020 proposed a strategy based on hack s law hack 1957 to filter out some of the flow direction branches as early as possible and stop the recursive procedure from traversing them this method uses flow accumulation values calculated using flow direction data to estimate potential longest flow lengths the cells that cannot lead to the longest flow path are rejected early while this approach does avoid traversing some of the suboptimal areas it requires flow accumulation to be calculated which is a time consuming step in itself apart from the mentioned works little attention has been paid to the problem of algorithmic determination of the longest flow paths huang and lee 2016 investigated the impact of various flow direction algorithms on selected geomorphological properties including the longest watercourse but specific methods for determining the longest path were not discussed here related studies discussing the accuracy of flow paths derived from dems were presented in paz et al 2008 orlandini and moretti 2009 and li et al 2020 the authors of this work are unaware of any previous studies focused on developing parallel algorithms for identifying the longest flow paths 1 1 2 existing software tools the widespread use of the longest flow path in hydrological analysis has led to the development of dedicated tools for its delineation in various gis applications maidment and morehouse 2002 cho 2020 lindsay 2022 and hydrological modeling software arnold et al 1998 feldman 2000 depending on the tool different methods may be used to calculate the result on many gis platforms it is possible to delineate the longest flow path by following a procedure similar to the one proposed in smith 1997 even if a dedicated tool for this task is not available it may still be possible to identify the longest flow path as long as the platform provides the ability to compute flow length rasters for this kind of approach the flow direction raster is used as the main input data using dedicated tools to find the longest flow path is usually straightforward unfortunately the underlying algorithms and procedures are not always openly described by their developers the longest flow path tool from the arc hydro package requires a catchment or possibly multiple ones as a vector layer along with a flow direction raster djokic et al 2011 the vector layer attributes must include a hydroid field with unique values identifying each sub catchment it is important to note that this tool does not take outlet points as input and the algorithm can in fact delineate the longest path beyond the catchment boundary stopping at the edge of the available data therefore in order to correctly determine the path within a given catchment it is necessary to first remove the flow direction values outside its boundaries by setting them to nodata or to properly clip the raster to the area regardless of the characteristics of the drainage network arc hydro delineates only a single path by the underlying assumption the path can only begin in the watershed boundary zone which in fact is not always the case the result is returned as a vector layer it is worth noting that due to the widespread popularity of arc hydro some of its features including the longest flow path tool are often directly integrated into other hydrological modeling applications olivera et al 2003 merkel et al 2008 multiple modules for determining the longest flow path were developed and made available for the open source grass gis platform neteler et al 2012 the most recent one named r accumulate was described in detail by the developer in cho 2020 as input it takes outlet points marking the endpoints of the paths searched and a drainage direction raster which can be generated in the grass environment using the r watershed tool depending on the drainage network the end result may be a single path or multiple alternative ones the tool returns them as a vector layer where each path is a separate object whiteboxtools is another advanced although less known platform with a dedicated tool for determining the longest flow path lindsay 2016 2022 the longestflowpath tool operates on a depressionless dem calculating the flow direction internally along with a raster that defines one or more areas to be analyzed the design assumes that the longest flow paths must start at the watershed boundaries which again may not be true in some cases the results are returned as vector objects in recent years an increasing number of software solutions including gis and hydrological modeling platforms base their architecture on web services and cloud computing goodall et al 2011 ames et al 2012 vitolo et al 2015 gichamo et al 2020 just a single example of such a tool is scalgo live scalgo 2022 the platform designed and built on a web based architecture is intended for broadly understood water management implementing a wide set of raster algorithms the tool is able to automatically perform a variety of hydrological modeling operations including determining the longest flow path in a given catchment to the knowledge of the authors of this work there are currently no tools available that determine the longest flow paths using parallel algorithms 2 methods considering the numerous applications of the longest flow path in hydrological modeling as well as the existence of various software tools for its determination and the attempts to improve their performance it can be concluded that there is a need to develop new more efficient algorithms for solving this task 2 1 adopted terminology for referring to specific cell types in raster data this paper adopts a vocabulary based on that proposed in tarboton et al 1991 source cell a cell with no inflow neighbors the most upstream cell in a given flow path and thus its starting point link cell a cell with a single inflow neighbor junction cell a cell with more than one inflow neighbor where two or more drainage channels merge into one outlet cell the most downstream cell in a given catchment where all drainage is eventually concentrated 2 2 problem specification the key problem to be solved by each of the developed algorithms is to identify the longest flow path among all those ending at a given outlet point it is assumed that the underlying spatial data can be expressed in the form of square grid rasters the input dataset for the algorithms consists of a hydrologically correct flow direction raster generated by any single flow algorithm and the location possibly multiple locations of the outlet point for which the longest flow path is to be found as there are many single flow direction algorithms with different properties any of which could be preferred by the user this form of data was chosen as the basic input rather than the raw digital elevation model the location of the outlet cell possibly multiple outlet cells is determined by the user and considered as part of the input data while some existing tools require auxiliary input e g a watershed mask raster it is assumed that the algorithms considered in this work do not have access to any additional datasets any processing of the basic input data is considered part of the algorithm and therefore reflected in performance measurements it is important to note that in this work the resolution of the input data and thus the cell size is generally treated as irrelevant assuming that all cells in the raster have the same dimensions the process of identifying the longest flow path does not depend on their particular sizes in fact it is possible to find the longest path in a flow direction raster without knowing the resolution of the dataset for this reason all algorithms considered in this work were designed and implemented as resolution independent as a unit of measurement for comparing the lengths of different flow paths the dimension of a single raster cell was adopted an orthogonal step between cells is simply counted as 1 while a diagonal step is counted as the square root of 2 e g a flow path consisting of three vertical steps is considered to be 3 units long regardless of raster resolution in terms of the expected result it is important to allow the identified flow path to be precisely delineated usually by marking all raster cells belonging to it or generating a corresponding vector shape therefore the correct result of the algorithm will be the location of the starting point possibly multiple locations of such points of the longest flow path existing for the given input data this approach allows for high flexibility in the presentation and use of the result obtained assuming the availability of flow direction data delineating the path in any chosen form e g vector object or raster with labeled cells can be quickly performed by traversing successive cells from the starting point downwards it is important to emphasize that the longest flow path could potentially start from any cell within the catchment some existing tools implicitly assume that the longest path always starts at the watershed boundary but this is simply not correct and may lead to inaccurate results cho 2020 2 2 1 different variants of the problem considering the possible use cases the distinctive needs of the end users and the expected characteristics of the input data it is possible to define several variants of the task to be solved in some cases it is possible to find more than one longest flow path within the same catchment each being equally long fig 2 illustrates how different paths can be estimated to have the same length it is necessary to specify the expected behavior of the algorithm in such scenarios as this kind of decision may substantially reshape the task to be solved and consequently lead to a different algorithm design finding any of the longest paths might be sufficient in some use cases e g when only its length is relevant while identifying all of the alternatives may be required in others e g when the location of the selected path may affect subsequent stages of the analysis and it would be reasonable to leave this choice to the user in this paper both use cases are considered relevant for datasets where more than one longest flow path could be found the minimum requirement for all algorithms under development was the ability to correctly identify at least one of them each being considered a valid result however the possibility of identifying all alternative longest paths for the same outlet point was also included in some designs while not required of all algorithms this capability was considered valuable and implemented where the underlying concept was compatible another use case to consider is identifying the longest flow paths for multiple outlet points e g analyzing more than one catchment within the same dataset simultaneously as a practical scenario this capability is offered by some of the existing tools while it is always possible to sequentially execute the search for individual outlet points a design that allows them to be processed together in a single algorithm run could have significant performance advantages depending on how the calculations are organized it may be possible to reduce the time required to identify all the paths individually in this work the minimum requirement for all considered algorithms was to be able to work with a single outlet cell however for some designs the simultaneous identification of the longest flow paths for different outlet points would require little or virtually no additional computational cost as a valuable property this mode of operation was included in compatible implementations it should be noted that this capability is only considered for performance reasons and does not affect the results generated by the algorithm 2 3 precision issues and proposed solution although the main focus of this study is related to algorithmic efficiency and performance it is important to also highlight some common issues related to the accuracy of the calculations and their results in particular the typical way of implementing the path length estimation method can lead to unnecessary gradual accumulation of round off errors which in turn may prevent the correct identification of the longest flow path it should be emphasized that the issues discussed in this section are not related to imperfections in the input data but rather to the way algorithms and software tools perform their computations on them precision issues with floating point calculations are well known and have been considered since the early decades of computers wilkinson 1963 one of the classic problems is sequence summation where round off errors accumulate with each step and can quickly become significant linz 1970 it is well known that summing a set of floating point numbers by simply adding consecutive values to the total sum is not a reliable approach caprani 1975 as noted in paz et al 2008 the length of the flow path is usually calculated step by step accumulating the distances between successive cells across all the literature and available source code known to the authors of this work this operation is implemented using floating point calculations by simply adding consecutive distances to the total sum in practice this approach can lead to a noticeable degree of inaccuracy in path length estimates it is relatively easy to find a case where the accumulation of round off errors in existing gis software results in an inaccurate length measurement leading to the incorrect flow path being identified as the longest to clearly describe and demonstrate the issue two simple hypothetical flow paths of similar lengths were chosen path a consisted of 14143 vertically connected cells including the outlet cell having a total center to center length of 14142 units path b consisted of 10001 diagonally connected cells also including the outlet cell thus containing 10000 diagonal center to center distances and having a total length of 10000 2 14142 136 units although the difference between these estimates is relatively small the values indicate that path b is slightly longer and should be considered as the expected correct result of an algorithm searching for the longest flow path fig 3 illustrates the results obtained by following the identification method from smith 1997 under arcgis pro 3 0 2 a flow direction raster with both flow paths leading to the same outlet cell was prepared and used as input dataset it can be seen that although path a was measured as might be expected the length of path b was underestimated due to the accumulation of round off errors the value obtained in the flow length raster is lower than the correct sum of the distances between successive cells as a result path a was incorrectly selected as the longer one although the discrepancy between the calculated length and the expected mathematically correct value is not large the accumulated error is sufficient to reach the incorrect conclusion in this work an alternative approach is proposed instead of a floating point value a pair of integers can be used to precisely express the path length avoiding the accumulation of round off errors entirely the main idea is to count and store the numbers of orthogonal and diagonal steps between cells separately although eight distinct directions are considered in single flow rasters only these two kinds of distance are relevant for path length measurements in square grid data the length of a flow path can still be measured by traversing it step by step but instead of accumulating the distances between successive cells one of the two counters is incremented with each step only when the path length needs to be expressed as a single number the values are converted by multiplying the counter of diagonal steps by the square root of 2 and adding the number of orthogonal steps to it this method effectively minimizes the use of floating point calculations by relying primarily on error free integer incrementations fig 4 illustrates the concept it should be noted that this approach is mathematically equivalent to the method of accumulating distances between successive cells however the key difference lies in how the calculations are organized and therefore how they are performed by the machine in practical implementation the proposed approach avoids the accumulation of round off errors and leads to more precise results the tests carried out in the early stages of this work confirmed that this method correctly solves the case presented in fig 3 it should be emphasized that the concepts presented in this section are not directly related to algorithmic performance but to the accuracy of path length measurements all algorithms developed and presented in this work are based on the proposed approach 2 4 design and implementation all algorithms developed as part of this work were implemented in c code sections intended to be executed by multiple threads simultaneously were parallelized using openmp directives the data structure developed in kotyra et al 2021 was used to store the rasters in memory the cells are stored in a two dimensional array with an additional external single cell frame of neutral values due to the large number of gis algorithms accessing the values of the nearest neighboring cells this design often makes it possible to simplify the code and increase its efficiency the aim during the design and implementation phase was to achieve the shortest possible execution times while maintaining the maximum possible precision of the results it was assumed that the task of identifying the longest flow paths is solvable in linear time thus only algorithms with linear time complexity were considered in this work 3 developed algorithms 3 1 reference implementations as the first step in the development and evaluation of new algorithms the basic recursive approach was adapted and expanded upon two relatively straightforward implementations were prepared one sequential and the other attempting to parallelize computations using openmp tasks although both of them were treated mainly as reference solutions they were also the starting point for the development of entirely new algorithms and therefore they are presented here first only solving the fundamental version of the problem with a single outlet point and identifying any of the longest paths was required of the reference implementations it is worth noting that the developed adaptations of the recursive approach still follow the requirements and incorporate the concepts introduced in this work e g their methods of comparing path lengths are based on counting orthogonal and diagonal steps separately 3 1 1 recursive approach sequential the underlying idea of the recursive approach is to search through all flow paths within a catchment by starting at the outlet cell and climbing upstream contrary to the flow direction the recursive procedure calls itself for successive higher cells effectively traversing the entire catchment area in a depth first manner fig 5 illustrates the general concept the algorithm starts the search by calling the recursive procedure for the designated outlet cell the procedure uses the flow direction data to identify the inflow neighbors of the cell and recursively invokes itself for each of them the same process is repeated for successive cells effectively climbing up from the outlet towards topographically higher locations the earlier pending calls for lower cells wait for the subsequent invocations to complete and return their results conceptually the aim of the recursive procedure invoked for any given cell is to identify the longest flow path leading to that particular location thus the initial call for the outlet cell carries out the task of finding the longest flow path for the entire catchment the procedure accomplishes this by delegating subtasks partial searches covering smaller areas to its successive recursive calls similarly each partial search is broken down into even smaller subtasks which are then delegated to subsequent invocations the results obtained and returned for these partial searches are captured and used by earlier pending calls the result produced for any given cell contains the length of the longest flow path leading to this particular point along with the location of the source cell where that path begins since the source cells have no inflow neighbors and are thus the starting points of the flow paths the result returned for any of them contains a zero path length and the location of the cell itself for all other types of cells the procedure captures the results returned by its subsequent invocations increments the appropriate step counter in each of them selects the one with the longest path length and returns it as its own result ultimately the initial call for the outlet cell returns a catchment wide search result containing the length and starting location of the longest flow path leading to the outlet fig 6 illustrates the process the algorithm was implemented with the intention of minimizing its execution time the implementation has a linear time complexity and the time needed to perform the search depends directly on the number of cells belonging to the catchment the recursive climbing procedure does not cross the watershed boundary thus ignoring any cells beyond it it is worth noting that this solution does not require raster storage for partial results during calculations multiple well known problems are related to recursive implementations in general although this approach generally leads to relatively simple and efficient code it is also inherently related to memory issues i e stack overflows often making it impractical for larger datasets wallis et al 2009 since this approach was implemented mainly as a reference solution these issues are not discussed in detail here 3 1 2 recursive approach parallel recursive implementations frequently prove to be difficult counterintuitive or even impossible to efficiently parallelize qin and zhan 2012 stpiczyński 2018 as an attempt to introduce parallelism to the presented recursive algorithm an implementation based on openmp tasks was developed assuming a single outlet point only one thread starts the recursive procedure at the beginning however in this implementation openmp tasks are created for each identified inflow neighbor allowing other threads to join the processing and start performing partial computations this makes it possible for separate flow direction branches to be searched through in parallel unfortunately this approach introduces the additional overhead of creating and managing a large number of tasks the implementation prepared as part of this work aims to limit this cost by allowing the creation of new tasks only to a certain level of recursion above the limit a thread stops generating tasks and performs the rest of the computations in the branch on its own this idea optimistically assumes that it may be possible to reasonably distribute the work between threads at lower levels of recursion close to the outlet point however given the realistic underlying landform this may prove to be rare or even impossible considering that a large fraction of flow direction branches may contain only one or a few cells the benefits of parallelization may turn out to be moderate and easily outweighed by the overhead 3 2 new algorithms during the design and implementation phase two distinct general concepts for the new algorithms were developed in the following sections they are referred to as top down and double drop approaches based on these two fundamental ideas a total of five new algorithms were implemented including three sequential and two parallel versions 3 2 1 top down maximum length sequential the fundamental concept underlying the approach referred to here as top down is to traverse the raster along the flow direction starting from source cells tops the most upstream locations and moving downstream in a way this is the opposite of the recursive approach in a typical flow direction raster a large number of source cells are the starting points of flow paths that gradually merge and eventually end at the same outlet point approaching the problem from this perspective creates new possibilities for parallelization conceptually the algorithm can start from any source cell in the raster the flow path is traversed cell by cell along the flow direction while its length is measured by counting successive orthogonal and diagonal steps a naive implementation of this idea would be to fully traverse and measure each existing flow path separately sequentially starting at each source cell in the raster and moving downstream until the endpoint is reached and compare their lengths e g by recording the longest one found so far however this would not be a well performing approach since a large fraction of the cells in the raster would be unnecessarily traversed multiple times common parts of converging flow paths would be needlessly re measured for each of them a much more efficient solution would be to compare the partial flow paths as soon as they merge in junction cells and only proceed downstream with the longest one discarding the other candidates since it is already clear that they cannot be the longest path in the catchment incorporating this concept the algorithm starts traversing and measuring a flow path from its source cell and moves downstream until it encounters a junction cell here the traversing procedure is temporarily stopped and the algorithm starts another separate measurement from a different source cell only after measuring all partial flow paths leading to a given junction cell the algorithm proceeds downstream to further traverse and measure the longest of them with this approach each cell in the raster can only be traversed downstream once a junction cell cannot be passed until the longest upstream path leading to it is identified this property is directly related to the linear time complexity of the algorithm the first implementation of this approach called top down max for short uses a specific data structure to compute and store partial results namely a raster of two integer cells each cell containing two numbers a single cell can be used to store either a distance as separate numbers of orthogonal and diagonal steps or a location of another cell in the raster as row and column coordinates this concept was implemented as a union of two structures representing distance and location data separately allowing for two possible interpretations of the same cell in the code ultimately every source cell is used to store distance while every non source cell stores location initially all cells of the working raster are set to a special undetermined state being marked as not containing any valid value yet the source cells are then used as working memory to calculate store and compare the partial lengths of the analyzed flow paths the main part of the algorithm aims to fill the raster so that every other non source cell points to the source location where the longest path leading to that particular cell begins this then makes it possible to immediately locate the beginning of the longest flow path leading to any given point most importantly to the outlet cells specified in the input data extracting the source location for any completed cell can be done by checking the cell type as source cells are a special case being their own source locations and reading the coordinates stored in it fig 7 shows a conceptual example of how the search for the longest flow path is performed the algorithm identifies the source cells and starts the downstream traversing procedure for each of them the integers stored in each source cell are treated as orthogonal and diagonal step counters first set to zero and then incremented accordingly with each step along the path at the same time each traversed non source cell is updated with the coordinates of a source location that begins the longest currently known path to this particular cell eventually when this process is complete the outlet cell contains the final search result pointing to the source location of the longest flow path in the entire catchment for each non source cell encountered a path comparison operation is performed to select the longer of the two partial paths leading to that point the one currently being traversed and the one starting from a location previously stored in the cell if the cell still contains the undetermined value the current path is immediately selected the lengths of both partial paths distances traversed so far from their starting points to the junction are extracted from their source cells and compared and the junction cell is updated with the new source location if necessary if the path currently being traversed turns out to be shorter than the one already found for that cell the current path is discarded and the cell does not change its state thus comparisons of partial paths effectively take place in junction cells and only the selected longer path is further considered the lengths of the discarded paths stored in their source cells are no longer incremented link cells having a single inflow neighbor are only entered and updated once changing their state from undetermined to valid source coordinates it is necessary to establish the final value of the junction cell and thus identify the longest path leading to it before proceeding further to its downstream cells as noted earlier this requires traversing all partial paths leading to the junction cell and comparing their lengths using the path comparison operation in the junction for this reason the traversal procedure has to be temporarily stopped in cells for which the lengths of all inflow paths are not yet known only when the final value of the cell is determined the data needed to continue the downstream traverse is available this property was implemented using an inlet number matrix where each cell is first initialized with the number of its inflow neighbors except for the source cells which receive a predefined value for easy identification each time the traversing procedure reaches a given cell its inlet number in the matrix is decremented only when this value reaches zero meaning that all paths leading to this point are completely processed the algorithm is allowed to proceed to the subsequent downstream cells this technique of temporarily stopping computations in junction cells was previously described for flow accumulation algorithms zhou et al 2019 kotyra et al 2021 the processing order of the partial flow paths is irrelevant to the final result although this algorithm was developed with parallelization in mind ultimately only a sequential version was implemented the non atomic nature of the path comparison operation would require the heavy use of synchronization mechanisms to prevent data races between threads resulting in impractical overhead levels to address this issue another version of the top down approach was implemented with the intention of eliminating potential conflicts between threads through a different workflow 3 2 2 top down single update sequential the next algorithm called top down single for short is based on the top down max implementation but with some important modifications the key idea was to reorganize the workflow so that the value of each non source cell in the raster is determined and set only once fig 8 presents a conceptual example as in the top down max implementation the algorithm aims to fill each non source cell with the coordinates of the source point where the longest flow path leading to that particular cell begins however instead of storing and repeatedly updating the partial result the source location of the longest path to that point identified so far in the junction cell the algorithm leaves it uninitialized until the lengths of all its inflow paths are determined only when the values of all inflow neighbors are established the algorithm compares all paths leading to that point and determines the cell value this implementation also uses the inlet number matrix but here the decrementation to zero is required to enter and update the cell rather than to proceed downstream from it algorithm 1 presents a simplified pseudocode covering this concept it is worth noting that only junction cells are processed and updated in this specific way based on reaching out to their inflow neighbors once all their values are determined single inflow cells are processed in a straightforward manner and simply filled with the starting point of the path that is currently being advanced an important advantage of this design is that it no longer requires raster cells to be initialized as each cell is updated only once its previous state is irrelevant and does not affect the final result the algorithm has a linear time complexity each cell is traversed downstream only once 3 2 3 top down single update parallel the idea of a single cell update eliminates problematic conflicts between threads and makes it possible to parallelize this algorithm in a relatively simple manner in this implementation multiple threads traverse partial flow paths simultaneously starting from different source cells junction cells are still locations of potential conflicts but the single update workflow reduces the issue significantly in fact the only synchronization mechanism required is the atomicity of the decrementation in the inlet number matrix with direct capture of the result since different threads may reach the same junction cell and simultaneously attempt to decrement its number of unprocessed inflow paths it has to be ensured that this will not result in data races the last thread that decreases the value of the junction cell in the inlet number matrix reaching zero compares all paths leading to the cell and selects the longest one at this point it is guaranteed that all inflow neighbors contain correct values the thread updates the junction cell and proceeds to traverse its successive downstream cells the inlet number counters and their atomic decrementation are only needed for junction cells all other cells are traversed only once therefore there is no possibility of any conflict between threads as synchronization is not needed there it is not used and as a result non junction cells are updated in an unsynchronized straightforward manner in all the top down implementations developed in this work the final result is obtained by extracting the coordinates stored in the outlet cell specified in the input data each non source cell in the raster indicates where the longest path leading to this cell begins these coordinates are treated as the end result of the algorithm the only special though impractical case is a scenario where the specified outlet point is actually a source cell which does not store location but distance to ensure the correctness of the algorithm for each case this scenario has to be recognized and handled properly it should be emphasized that all the top down implementations have a natural ability to determine the longest flow paths for multiple outlet points in the same run as the main part of the algorithm fills all non source cells with the correct source coordinates determining the longest flow paths for other outlet points only requires extracting these additional starting locations thus determining the longest paths for multiple outlet points in the same run involves only a small negligible in practice additional cost each of these algorithms has been implemented in a way that allows both single and multiple outlet points to be taken as input and processed 3 2 4 double drop sequential another approach to the problem developed as part of this research is referred to in this paper as the double drop although it is also based on traversing flow paths along the flow direction the concept differs significantly from the top down approach the main idea behind the algorithm is relatively straightforward the most characteristic property is that it passes the same sections of the flow paths twice conceptually the traversing procedure can be started from any cell it does not have to be a source cell the algorithm first moves downstream along the flow direction evaluating whether a given flow path leads to the specified outlet point and measuring the distance traveled from the starting cell then the same path is traversed again this time recording the remaining distance to the outlet in each passed cell or marking it as not leading to the outlet point the algorithm also uses a working raster of two integer cells but here the content of each cell is always interpreted as a distance expressed as separate numbers of orthogonal and diagonal steps the main part of the algorithm aims to fill the raster so that each cell contains the path length from itself to the watershed outlet specified in the input or the predefined out of basin value if the path does not lead to the outlet fig 9 shows a conceptual example algorithm 2 presents a simplified pseudocode of this approach as the first step the working raster is initialized with the undetermined values the external frame is filled with out of basin values for easy handling of boundary crossings only the outlet cell is set to a valid distance value of zero being the correct distance to itself next the algorithm starts traversing and measuring subsequent flow paths gradually filling the raster with the calculated distances to the outlet point as mentioned the most distinctive property of this approach is that each partial flow path is traversed twice on the first pass of a given path no modifications are made to the raster only the distance traveled from the starting cell is measured using the orthogonal and diagonal step counters by incrementing them accordingly with each step as soon as the traverse procedure enters a cell with any value other than undetermined or crosses the raster boundary the second pass begins starting again from the same location and following the same path this time each cell passed through is modified and thus set to its final value if the first pass ends with crossing the raster boundary or entering a cell with the out of basin value the second pass sets all cells along the entire path to out of basin a special case of reaching a cell without a valid flow direction value is handled in the same way any cell marked with this value is already known not to lead to the outlet point specified in the input entering such a cell while traversing downstream means that the entire path is located outside the catchment cells containing a valid distance are known to lead to the outlet point and their value describes the remaining length of that path when the algorithm enters such a cell during the first pass the distance traversed so far downstream from the starting point is summed up with the value stored in that cell in this way the total distance from the starting point to the outlet cell is obtained the second pass fills all cells along the path with the remaining distance to the outlet point this time the orthogonal and diagonal step counters expressing the remaining distance are appropriately decremented with each step the algorithm records the length and starting location of the longest path identified so far comparisons between potential candidates are made when a cell with a valid distance is entered during the first pass and the total distance from the starting point to the outlet is calculated alternatively a simple one time raster scan to find the maximum length could be implemented at the end of the algorithm but this would require significantly more comparisons as with the top down approach the order in which the partial flow paths are traversed is irrelevant however the double drop algorithm does not require the traversing procedure to be started from a source cell in fact it can be started from any unprocessed cell regardless of its type this makes the detection of source cells unnecessary significantly simplifying the workflow once all cells are processed the algorithm returns the starting location of the longest identified flow path as the final result it should be emphasized that although this algorithm traverses each location twice it treats all types of cells in the same way and does not require detection or special handling of either source or junction cells this simple workflow could be considered a significant advantage of this approach the implementation has a linear time complexity 3 2 5 double drop parallel the parallel implementation of the double drop algorithm allows multiple threads to process partial flow paths simultaneously the fundamental idea remains the same as in the sequential version although some minor changes were introduced as long as there are unprocessed cells in the raster the threads select them as starting points and proceed to the downstream traversing procedure the same raster is being filled in multiple locations simultaneously allowing the threads to share their results with each other each thread individually stores the length and starting location of the longest path it has identified so far these local results are compared with the global maximum only at the end of the algorithm run this approach minimizes the need for synchronization between threads comparing large numbers of paths it is possible for multiple threads starting from different locations to traverse the same common part of their flow paths in the first pass simultaneously the design of the algorithm allows for such a scenario considering the cost of possible synchronization mechanisms as inviable apart from potential additional partially redundant work this property has no negative effects on the operation of the algorithm in the second pass such cases are generally eliminated early as the thread stops the filling procedure as soon as it reaches the first cell with a value other than undetermined it is worth noting that the double drop approach allows for straightforward determination of all alternative longest flow paths leading to the same outlet point as the length of the longest path is known at the end of the algorithm it is possible to simply search the entire raster for all cells containing a distance equal to the maximum length in this way the algorithm can return not just one but all alternative longest paths with little additional cost both implementations of this approach developed as part of this work include this mode of operation although this algorithm was designed to work with a single outlet cell it is possible to extend these implementations to handle multiple outlets in the same run this would require computing and storing additional data for each cell perhaps a single integer index indicating to which outlet point its flow path leads instead of being marked with out of basin cells flowing to other outlet points would store valid distances and still be easily identifiable while requiring more memory it would likely reduce the time needed for individual algorithm runs when working with multiple outlets 4 performance measurements 4 1 data the bystrzyca catchment was selected as the first source area for measurements and analyses it is a third order watershed with a total area of 94 km2 located in the south eastern part of poland square grid dem data with one meter resolution was obtained from the publicly available resources of the head office of geodesy and cartography gugik the source dataset was referenced in the pl 1992 system national geodetic coordinate system 1992 for poland the original data was processed to generate a wide variety of input datasets for performance measurements in the first stage the outlet point was determined and the entire catchment along with the longest flow path belonging to it was delineated subsequently the division into sub catchments was carried out adopting two different approaches as for the first one the main catchment was divided along the longest flow path every two kilometers starting from the watershed boundary this resulted in 47 sub catchments with varying areas the same procedure was carried out on the source data scaled down to 2 2 m 4 4 m 6 6 m 8 8 m and 10 10 m resolutions the generated datasets were intended mainly for comparing the execution times of various algorithms and tools as well as examining the relationship between their performance and the size of the input data as for the second approach the ten meter resolution data was used based on the drainage network a set of sub catchments was delineated with 3 0 km2 chosen as a surface threshold in this way 118 relatively small sub catchments were generated with areas ranging from 3 0 to 3 4 km2 these datasets were mainly intended for controlling and analyzing the results of the developed algorithms as well as comparing them with those generated by selected gis software additional source areas were selected in order to obtain larger more computationally demanding datasets four other watersheds located in poland were chosen the kamienna tanew barycz and wieprz with areas ranging from approximately 1 300 to 4 440 km2 the acquired dems with one meter resolution originating from the gugik were used to generate datasets ranging in size from approximately two to seven billion cells table 1 shows the details these datasets were intended for examining the performance and characteristics of the developed algorithms as well as comparing them with each other each flow direction raster used in performance measurements was prepared in two variants in the first kind referred to as full frames each cell contained a valid direction value in the second one simply referred to as basins all cells outside the catchment boundaries had a direction value set to none only cells belonging to the catchment contained a valid value previous research by the authors has shown that using data filtered in this way can have a noticeable impact on the computational times of some algorithms kotyra et al 2021 fig 10 shows both the selected source areas and sub catchments obtained with the methods described all datasets were precisely clipped to the selected catchments each row and column of every raster contained at least one cell belonging to the catchment 4 2 testing procedure for c algorithms before running any measurements all implementations were validated against a suite of automated tests the algorithms were tested for multiple scenarios including both standard and special cases as well as a variety of runtime configurations all files with input data intended for measurements were also verified for correctness performance measurements were based on repeatedly executing each algorithm on multiple datasets while measuring and recording computational times each test performed by an automated bash script consisted of restarting the measurement application loading input data files executing the selected algorithm and verifying the correctness of its output time measurements were started immediately after the input dataset was loaded into memory and stopped after the algorithm generated the final result as the recursive implementations were included in the tests stack overflows were expected in such cases the thread stack size was doubled in the runtime configuration more than once if needed and the test was repeated measurements were performed in two separate test environments machine a running under almalinux 8 4 was equipped with a dual intel xeon e5 2670 v3 processor 24 cores in total and 128 gb ram machine b with two operating systems windows 10 enterprise ltsc 64 bit and ubuntu 22 04 1 lts was equipped with a dual intel xeon cpu e5 2620 v4 processor 16 cores in total and 112 gb ram on machine b algorithm performance measurements were carried out under ubuntu all source code was compiled using the gnu c compiler version 8 4 1 on machine a version 11 2 0 on machine b with o3 level optimization and openmp support enabled parallel implementations were allowed to use all available cores in order to precisely compare the performance of the developed algorithms each of them was executed and measured 30 times on each of the eight large datasets kamienna tanew barycz and wieprz both full frame and basin variants for a fair comparison only the fundamental variant of the problem assuming one outlet point and requiring a single path to be identified was taken into account this procedure was performed on both machines two parallel implementations of the newly presented algorithms were also tested in a similar manner on a set of 564 sub catchments of the bystrzyca 47 areas each in six resolutions and two variants these measurements provided the basis for a more detailed analysis including comparisons with other existing software in order to examine the efficiency of the two new parallel algorithms in various multithreading configurations additional measurements were performed on machine a using the largest frame dataset barycz the algorithms were tested with thread limits ranging from 1 to 48 performance was measured 30 times for each configuration the task based recursive implementation requires a parameter that specifies the highest level of recursion at which new tasks can still be created based on the foundations of this concept a relatively low limit of 100 was selected and then doubled several times reaching 200 400 800 and 1600 separate tests were performed for each of these values it was assumed that this variety should allow for an overall evaluation of this approach 4 3 testing procedure for existing gis software to obtain reference computational times for the algorithms developed in this work the performance of the r accumulate tool was measured it was selected as the most recent module for determining the longest flow paths available on the grass gis platform the measurements presented in cho 2020 showed significant advantages of this tool over existing alternatives tests were performed using grass gis version 8 0 2 unfortunately due to the limitations inherent in this platform larger datasets greater than approximately 2 billion cells could not be used measurements were carried out on machine b under both windows 10 and ubuntu the r accumulate tool was executed 30 times on each of the selected datasets under both operating systems an automated script monitored the calculation runs and retrieved the times needed to generate the results 5 results and discussion 5 1 performance comparison of the developed algorithms tables 2 and 3 present the average execution times of the developed algorithms measured using the eight large datasets on both machines the two approaches to data preparation were denoted by ff for full frames and b for basins the parallel implementation of the double drop approach turned out to be the most efficient in the vast majority of cases table 3 however while the parallel version of the top down single algorithm performed slightly worse on average it turned out to be the fastest in some instances the performance differences between these two implementations are more apparent for the measurements made on machine b in favor of the double drop algorithm it appears that both the data characteristics basins versus full frames and the underlying hardware particularly the different number of cpu cores contribute to the relative performance of these algorithms among the sequential implementations the recursive approach turned out to achieve the shortest execution times in all cases on both machines table 2 the sequential double drop algorithm achieved times ranging from approximately 54 to 126 longer and the sequential top down implementations from 151 to 291 longer than the recursive algorithm as both the double drop and top down approaches are relatively more complex and require processing of all cells in the raster as opposed to the recursive algorithm which only visits cells belonging to the catchment these results are not surprising however the results turned out to be completely different for parallel implementations while the newly developed algorithms achieved significant speedups over their respective sequential versions the task based implementation did not improve the performance of the recursive approach the execution times of the parallel recursive implementation were on average several percent longer than those achieved by the sequential version this observation holds for all tested task creation limits moreover while the differences in computational times between the sequential and parallel implementations are relatively small there seems to be a strong positive correlation between the task creation limit and the average time achieved by the task based version this may suggest that increasing the limit further will only degrade the performance more the parallel double drop implementation achieved an average speedup of 11 2 on machine a and 8 5 on machine b across all the datasets for the top down single algorithm the average speedup was significantly higher 18 3 on machine a and 11 7 on machine b it is worth emphasizing that the sequential implementation of the double drop approach achieved on average 41 shorter times than the sequential top down single algorithm while the top down single approach achieves higher speedups the double drop algorithm seems to excel when the number of threads is lower this could help explain why while the double drop algorithm achieved the shortest execution times in the vast majority of cases the top down approach turned out to be slightly faster on a few occasions it is worth noting the differences in computational times achieved on both types of datasets for the recursive approach both sequential and parallel versions there is no particular difference between processing full frames and basins from the same source area however for all the other implementations the differences are significant depending on the algorithm calculations on the basin variant of the data with cells outside the catchment boundary set to none turned out to be approximately 20 to 31 faster on average as expected the recursive implementations both sequential and parallel repeatedly caused stack overflows and consequently application crashes in some cases it was necessary to double the stack size several times before the test could be successfully completed this can be considered as a confirmation that the recursive approach is in fact impractical especially for larger datasets 5 2 comparison with existing gis software across all the datasets used the newly proposed parallel algorithms achieved significantly better performance compared with the reference times generated using the r accumulate tool fig 11 shows a comparison of the average execution times obtained on machine b using the full frame datasets for the top down single algorithm the reference times were on average 10 3 times longer on ubuntu and 12 9 times longer on windows across all the datasets used it is worth noting that in some cases this ratio reached almost 17 on ubuntu and exceeded 30 on windows for the double drop algorithm the reference times obtained on ubuntu were on average 14 6 times longer exceeding 26 times in some cases on windows the average ratio was 18 1 reaching almost 46 for some datasets it is worth noting that the times obtained by the r accumulate tool are relatively similar on the two operating systems for smaller datasets but differ significantly for the larger ones the exact reason is unknown to the authors it is also necessary to emphasize that the r accumulate module operates in a specific environment of the grass gis platform not as a standalone tool for this reason these measurements were intended mainly to provide some external context rather than to serve as a direct comparison 5 3 scalability analysis fig 12 shows the average computational times of the two newly proposed parallel algorithms for different multithreading configurations the double drop algorithm clearly outperforms the top down single implementation for smaller numbers of threads however as can be seen in table 4 the top down single algorithm benefits more from parallel processing when the number of active threads is higher ultimately as the thread limit increases the differences between the execution times of these two algorithms seem to become more and more insignificant it is worth noting that both algorithms achieve high speedups even for relatively small numbers of threads 5 4 analysis of the generated results using the 118 relatively small test datasets covering the bystrzyca sub catchments in ten meter resolution the paths identified by the developed algorithms were analyzed and evaluated in addition to detailed validation the results were compared to the longest flow paths delineated under arcgis pro 3 0 2 using the flow length raster approach from smith 1997 locations courses and numbers of cells belonging to the paths confirmed the correctness of the generated results out of the 118 analyzed datasets the vast majority 104 cases were found to contain only a single longest flow path for the remaining 14 it was possible to identify more than one valid outcome in ten cases two different paths with the same maximum length were found four datasets contained as many as three equally valid alternatives however for all 14 datasets containing more than one longest flow path the differences between the alternative outcomes were rather small the number of differently located cells ranged from one to nine with a median of just two alternative source cells were located relatively close to each other in five out of 118 cases the longest flow path did not start at the watershed boundary but inside the catchment 6 conclusions in this work the existing algorithms and software tools for finding the longest flow paths were reviewed issues were identified with respect to both their performance and precision addressing the need for more efficient solutions new algorithms were developed tested and presented their performance was measured and compared with both other implementations and selected gis software measurements show that the two new parallel algorithms are able to identify the longest flow paths much more efficiently compared with existing alternatives these two implementations have distinct properties and therefore both of them could be considered useful and noteworthy depending on the context of use and the needs of a particular user e g requiring all alternative longest paths to be identified or only estimating maximum path lengths for multiple outlet points one may be more suitable than the other scalability analysis shows that both new algorithms achieve high speedups even for small numbers of threads this makes them suitable not only for high performance hardware but also for widely available low cost multicore devices it is worth noting the differences in the ability of these algorithms to solve specific variants of the problem the top down approach is naturally capable of working with multiple outlet points simultaneously but its data structure allows it to identify only a single longest path leading to each location on the other hand the double drop approach is designed to work with a single outlet point but is capable of recognizing all alternative longest paths at a low additional cost moreover the double drop implementations can be extended relatively easily to process multiple outlet points simultaneously thus the choice of a suitable implementation should depend on the needs of the end user both new algorithms achieve shorter execution times on the flow direction data with cells outside the studied catchment set to none this observation is consistent with the conclusions presented in kotyra et al 2021 regarding flow accumulation algorithms although this gain may not be large enough to intentionally prepare data in this way for just this operation it seems worthwhile to use such datasets when available it should be noted that the algorithms techniques and ideas presented in this work are based on the use of flow direction data although this kind of approach is generally accepted and used in virtually every available tool for finding the longest flow paths modeling the underlying terrain and drainage processes in this way may be considered a simplification and therefore a limitation of this study perhaps the development of algorithms and tools based on more complex models could be a valuable direction for future research credit authorship contribution statement bartłomiej kotyra conceptualization methodology software investigation formal analysis validation visualization writing łukasz chabudziński data curation validation visualization writing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25375,in hydrological modeling the longest flow path is an important feature used to characterize a catchment many existing gis platforms offer dedicated software tools for its identification and delineation generally implementing methods based on searching through the flow direction data unfortunately currently available algorithms for this task often turn out to be inefficient especially when working with modern large datasets moreover existing methods often rely on incorrect assumptions or perform calculations in a way that can lead to precision issues in this work new parallel algorithms were developed tested and presented measurements show that two of the newly proposed implementations are able to identify the longest flow paths in significantly less time compared with other existing methods keywords longest flow path gis hydrology parallel algorithms high performance computing openmp data availability the source code is freely available in a public github repository the data used for performance measurements originated from publicly available resources software availability software algorithms for finding the longest flow paths description source code of all algorithms developed tested and presented as part of this work including a simple measurement application developer bartłomiej kotyra contact address bartlomiej kotyra mail umcs pl language c 11 openmp libraries required gdal geospatial data abstraction library availability freely available at https github com bkotyra longest flow path 1 introduction in the area of hydrological modeling catchments are characterized and described using multiple features and various parameters one of the important ones is the longest flow path also referred to as the longest watercourse or the longest drainage path dawson et al 2006 huang and lee 2016 michailidi et al 2018 in the hydrological context a flow path is understood as a route followed by water draining from one point to another there are multiple flow paths of varying lengths in any given catchment the longest one leads to the watershed outlet usually starting at some location on the watershed boundary although this is not always the case cho 2020 the longest flow path is typically used to determine the attributes for hydrological model parameterization mainly the time of concentration and the lag time of a watershed michailidi et al 2018 sultan et al 2022 it can also be used as a basis for calculating some of the properties and characteristics of a catchment maathuis and wang 2006 jaffrés et al 2021 the longest flow path as well as other features derived from it may be useful in estimating certain flood related indices karalis et al 2014 latt et al 2015 both early and modern gis software packages consider the longest flow path as an essential element in hydrological modeling and provide tools for its delineation and analysis gallant and wilson 1996 maidment and morehouse 2002 merkel et al 2008 ramly and tahir 2016 recent literature draws attention to the time consuming nature of this operation and the low processing speed of the commonly used tools castro and maidment 2020 attempts are still being made to develop new more efficient algorithms for this task cho 2020 the geospatial data available today offers high resolution and unprecedented precision but this comes at the cost of significant dataset sizes creating new computational challenges sten et al 2016 at the same time modern hardware architectures make the access to multicore and many core processors more and more common programming standards like openmp make it relatively easy to achieve high computing performance on these devices due to parallelism chapman et al 2007 however designing and implementing parallel algorithms still remains a non trivial task leading to a significant gap between the existing technology and its practical applications tang and wang 2020 the available software often fails to efficiently use the underlying hardware and proves to be impractical in the context of modern large datasets the main motivation behind this research was the need for more efficient tools and techniques for finding the longest flow paths in geospatial data as can be concluded from a review of the available software and literature there is still room for significant improvement the goal of this study was to develop and present new fast raster based algorithms for solving this task with particular attention to the possibilities offered by modern multicore architectures the authors believe that this work improves upon existing solutions and takes a significant step towards higher performance the remainder of this chapter outlines the foundations of existing techniques for finding the longest flow paths in raster data and describes the available algorithms and software tools for this task section 2 specifies the constraints of the problem to be solved along with its specific variants and describes the approach used in this work to design and implement new more efficient solutions in section 3 all the developed algorithms a total of seven implementations including two reference ones are presented and discussed in detail section 4 describes the data and procedures used to measure and evaluate the performance of the proposed algorithms including comparisons with existing gis software the results of these measurements are presented and discussed in section 5 finally the conclusions of this study its limitations and possible future work are briefly summarized in section 6 1 1 existing techniques the existing literature related to finding the longest flow path as well as the available software tools generally refer to and build upon the approach based on square grid digital elevation models dems and their derivative data specifically flow direction rasters the basic concepts behind these methods are well known and have been widely used for decades o callaghan and mark 1984 jenson and domingue 1988 in the literature there are two significantly different approaches to the concept of flow direction in the single flow approach each raster cell points to one of its immediate neighbors directing all flow to a single downstream cell in the multiple flow approach the flow may be proportionally divided among more than one neighbor schäuble et al 2008 both approaches have multiple practical applications as some problems are naturally better expressed in terms of one or the other barták 2009 however different algorithms for determining the flow direction can lead to significantly varying results huang and lee 2016 the single flow approach is most often considered more appropriate for determining and analyzing flow paths as multiple flow algorithms lead to excessive dispersion and overestimated path lengths orlandini and moretti 2009 li et al 2020 the length of a given flow path is typically estimated as the sum of the distances between successive raster cells belonging to that path paz et al 2008 fig 1 illustrates this fundamental idea the flow path can be conceptually expressed as a line connecting the centers of consecutive downstream cells the center to center distance between orthogonal horizontal and vertical neighbors is considered equal to the dimension of a single cell and the distance between diagonal neighbors is calculated as the dimension of the cell multiplied by the square root of 2 1 414 although this measure is essentially a simple approximation it is generally regarded as being relatively consistent with the actual length of the flow path fairfield and leymarie 1991 paz et al 2008 the flow direction raster is used as fundamental data in these calculations as it allows any flow path to be traced cell by cell smith 1997 cho 2020 lindsay 2022 it should be noted that while this kind of approach may be considered unsuitable for some other hydrological modeling tasks i e requiring consideration of a wider range of factors it is broadly accepted and implemented as a standard method for quickly comparing flow paths when searching for the longest one the remaining sections of this paper deal with techniques algorithms and tools based on these foundations 1 1 1 algorithms described in the literature the earliest published works discussing the algorithmic determination of the longest flow path of which the authors are aware were presented in smith 1995 1997 the papers are focused on the architecture of a software system called the hydrologic data development system hdds it uses spatial analysis techniques to determine multiple hydrologic parameters including the length and location of the longest flow path the approach used in the hdds is based on square grid dems the procedure of determining the longest flow path begins with the preparation of two additional rasters which are calculated using the flow direction data in the first one each cell contains its downstream flow length which is defined as the downslope distance to a pour point the second one consists of upstream flow length values defined as the distance to the drainage divide the values of the corresponding cells from both rasters are then added together the highest values obtained in this way are equal to the length of the longest flow path importantly only cells belonging to the longest path can obtain the maximum value which allows them to be easily identified all other cells obtain lower values olivera and maidment 1998 presented another grid based geographic information system for hydrologic modeling the method for determining the longest flow path described here is generally the same as in smith 1997 the system requires both the upstream and downstream flow length rasters as input data these rasters are added together and then the set of cells with the highest sum is identified as the longest flow path this approach was later described in more detail by the same author in olivera 2001 cho 2020 proposed using a recursive approach to improve performance over existing techniques and reduce computational time needed the basic idea behind this method is to search the flow direction raster by starting at the outlet point and moving upstream recursively invoking the calculation of the longest flow path for topographically higher cells this approach makes it possible to avoid calculating downstream and upstream flow length rasters due to memory issues typical of recursion i e stack overflows an iterative version of the algorithm was also presented and recommended for use developing this concept further cho 2020 proposed a strategy based on hack s law hack 1957 to filter out some of the flow direction branches as early as possible and stop the recursive procedure from traversing them this method uses flow accumulation values calculated using flow direction data to estimate potential longest flow lengths the cells that cannot lead to the longest flow path are rejected early while this approach does avoid traversing some of the suboptimal areas it requires flow accumulation to be calculated which is a time consuming step in itself apart from the mentioned works little attention has been paid to the problem of algorithmic determination of the longest flow paths huang and lee 2016 investigated the impact of various flow direction algorithms on selected geomorphological properties including the longest watercourse but specific methods for determining the longest path were not discussed here related studies discussing the accuracy of flow paths derived from dems were presented in paz et al 2008 orlandini and moretti 2009 and li et al 2020 the authors of this work are unaware of any previous studies focused on developing parallel algorithms for identifying the longest flow paths 1 1 2 existing software tools the widespread use of the longest flow path in hydrological analysis has led to the development of dedicated tools for its delineation in various gis applications maidment and morehouse 2002 cho 2020 lindsay 2022 and hydrological modeling software arnold et al 1998 feldman 2000 depending on the tool different methods may be used to calculate the result on many gis platforms it is possible to delineate the longest flow path by following a procedure similar to the one proposed in smith 1997 even if a dedicated tool for this task is not available it may still be possible to identify the longest flow path as long as the platform provides the ability to compute flow length rasters for this kind of approach the flow direction raster is used as the main input data using dedicated tools to find the longest flow path is usually straightforward unfortunately the underlying algorithms and procedures are not always openly described by their developers the longest flow path tool from the arc hydro package requires a catchment or possibly multiple ones as a vector layer along with a flow direction raster djokic et al 2011 the vector layer attributes must include a hydroid field with unique values identifying each sub catchment it is important to note that this tool does not take outlet points as input and the algorithm can in fact delineate the longest path beyond the catchment boundary stopping at the edge of the available data therefore in order to correctly determine the path within a given catchment it is necessary to first remove the flow direction values outside its boundaries by setting them to nodata or to properly clip the raster to the area regardless of the characteristics of the drainage network arc hydro delineates only a single path by the underlying assumption the path can only begin in the watershed boundary zone which in fact is not always the case the result is returned as a vector layer it is worth noting that due to the widespread popularity of arc hydro some of its features including the longest flow path tool are often directly integrated into other hydrological modeling applications olivera et al 2003 merkel et al 2008 multiple modules for determining the longest flow path were developed and made available for the open source grass gis platform neteler et al 2012 the most recent one named r accumulate was described in detail by the developer in cho 2020 as input it takes outlet points marking the endpoints of the paths searched and a drainage direction raster which can be generated in the grass environment using the r watershed tool depending on the drainage network the end result may be a single path or multiple alternative ones the tool returns them as a vector layer where each path is a separate object whiteboxtools is another advanced although less known platform with a dedicated tool for determining the longest flow path lindsay 2016 2022 the longestflowpath tool operates on a depressionless dem calculating the flow direction internally along with a raster that defines one or more areas to be analyzed the design assumes that the longest flow paths must start at the watershed boundaries which again may not be true in some cases the results are returned as vector objects in recent years an increasing number of software solutions including gis and hydrological modeling platforms base their architecture on web services and cloud computing goodall et al 2011 ames et al 2012 vitolo et al 2015 gichamo et al 2020 just a single example of such a tool is scalgo live scalgo 2022 the platform designed and built on a web based architecture is intended for broadly understood water management implementing a wide set of raster algorithms the tool is able to automatically perform a variety of hydrological modeling operations including determining the longest flow path in a given catchment to the knowledge of the authors of this work there are currently no tools available that determine the longest flow paths using parallel algorithms 2 methods considering the numerous applications of the longest flow path in hydrological modeling as well as the existence of various software tools for its determination and the attempts to improve their performance it can be concluded that there is a need to develop new more efficient algorithms for solving this task 2 1 adopted terminology for referring to specific cell types in raster data this paper adopts a vocabulary based on that proposed in tarboton et al 1991 source cell a cell with no inflow neighbors the most upstream cell in a given flow path and thus its starting point link cell a cell with a single inflow neighbor junction cell a cell with more than one inflow neighbor where two or more drainage channels merge into one outlet cell the most downstream cell in a given catchment where all drainage is eventually concentrated 2 2 problem specification the key problem to be solved by each of the developed algorithms is to identify the longest flow path among all those ending at a given outlet point it is assumed that the underlying spatial data can be expressed in the form of square grid rasters the input dataset for the algorithms consists of a hydrologically correct flow direction raster generated by any single flow algorithm and the location possibly multiple locations of the outlet point for which the longest flow path is to be found as there are many single flow direction algorithms with different properties any of which could be preferred by the user this form of data was chosen as the basic input rather than the raw digital elevation model the location of the outlet cell possibly multiple outlet cells is determined by the user and considered as part of the input data while some existing tools require auxiliary input e g a watershed mask raster it is assumed that the algorithms considered in this work do not have access to any additional datasets any processing of the basic input data is considered part of the algorithm and therefore reflected in performance measurements it is important to note that in this work the resolution of the input data and thus the cell size is generally treated as irrelevant assuming that all cells in the raster have the same dimensions the process of identifying the longest flow path does not depend on their particular sizes in fact it is possible to find the longest path in a flow direction raster without knowing the resolution of the dataset for this reason all algorithms considered in this work were designed and implemented as resolution independent as a unit of measurement for comparing the lengths of different flow paths the dimension of a single raster cell was adopted an orthogonal step between cells is simply counted as 1 while a diagonal step is counted as the square root of 2 e g a flow path consisting of three vertical steps is considered to be 3 units long regardless of raster resolution in terms of the expected result it is important to allow the identified flow path to be precisely delineated usually by marking all raster cells belonging to it or generating a corresponding vector shape therefore the correct result of the algorithm will be the location of the starting point possibly multiple locations of such points of the longest flow path existing for the given input data this approach allows for high flexibility in the presentation and use of the result obtained assuming the availability of flow direction data delineating the path in any chosen form e g vector object or raster with labeled cells can be quickly performed by traversing successive cells from the starting point downwards it is important to emphasize that the longest flow path could potentially start from any cell within the catchment some existing tools implicitly assume that the longest path always starts at the watershed boundary but this is simply not correct and may lead to inaccurate results cho 2020 2 2 1 different variants of the problem considering the possible use cases the distinctive needs of the end users and the expected characteristics of the input data it is possible to define several variants of the task to be solved in some cases it is possible to find more than one longest flow path within the same catchment each being equally long fig 2 illustrates how different paths can be estimated to have the same length it is necessary to specify the expected behavior of the algorithm in such scenarios as this kind of decision may substantially reshape the task to be solved and consequently lead to a different algorithm design finding any of the longest paths might be sufficient in some use cases e g when only its length is relevant while identifying all of the alternatives may be required in others e g when the location of the selected path may affect subsequent stages of the analysis and it would be reasonable to leave this choice to the user in this paper both use cases are considered relevant for datasets where more than one longest flow path could be found the minimum requirement for all algorithms under development was the ability to correctly identify at least one of them each being considered a valid result however the possibility of identifying all alternative longest paths for the same outlet point was also included in some designs while not required of all algorithms this capability was considered valuable and implemented where the underlying concept was compatible another use case to consider is identifying the longest flow paths for multiple outlet points e g analyzing more than one catchment within the same dataset simultaneously as a practical scenario this capability is offered by some of the existing tools while it is always possible to sequentially execute the search for individual outlet points a design that allows them to be processed together in a single algorithm run could have significant performance advantages depending on how the calculations are organized it may be possible to reduce the time required to identify all the paths individually in this work the minimum requirement for all considered algorithms was to be able to work with a single outlet cell however for some designs the simultaneous identification of the longest flow paths for different outlet points would require little or virtually no additional computational cost as a valuable property this mode of operation was included in compatible implementations it should be noted that this capability is only considered for performance reasons and does not affect the results generated by the algorithm 2 3 precision issues and proposed solution although the main focus of this study is related to algorithmic efficiency and performance it is important to also highlight some common issues related to the accuracy of the calculations and their results in particular the typical way of implementing the path length estimation method can lead to unnecessary gradual accumulation of round off errors which in turn may prevent the correct identification of the longest flow path it should be emphasized that the issues discussed in this section are not related to imperfections in the input data but rather to the way algorithms and software tools perform their computations on them precision issues with floating point calculations are well known and have been considered since the early decades of computers wilkinson 1963 one of the classic problems is sequence summation where round off errors accumulate with each step and can quickly become significant linz 1970 it is well known that summing a set of floating point numbers by simply adding consecutive values to the total sum is not a reliable approach caprani 1975 as noted in paz et al 2008 the length of the flow path is usually calculated step by step accumulating the distances between successive cells across all the literature and available source code known to the authors of this work this operation is implemented using floating point calculations by simply adding consecutive distances to the total sum in practice this approach can lead to a noticeable degree of inaccuracy in path length estimates it is relatively easy to find a case where the accumulation of round off errors in existing gis software results in an inaccurate length measurement leading to the incorrect flow path being identified as the longest to clearly describe and demonstrate the issue two simple hypothetical flow paths of similar lengths were chosen path a consisted of 14143 vertically connected cells including the outlet cell having a total center to center length of 14142 units path b consisted of 10001 diagonally connected cells also including the outlet cell thus containing 10000 diagonal center to center distances and having a total length of 10000 2 14142 136 units although the difference between these estimates is relatively small the values indicate that path b is slightly longer and should be considered as the expected correct result of an algorithm searching for the longest flow path fig 3 illustrates the results obtained by following the identification method from smith 1997 under arcgis pro 3 0 2 a flow direction raster with both flow paths leading to the same outlet cell was prepared and used as input dataset it can be seen that although path a was measured as might be expected the length of path b was underestimated due to the accumulation of round off errors the value obtained in the flow length raster is lower than the correct sum of the distances between successive cells as a result path a was incorrectly selected as the longer one although the discrepancy between the calculated length and the expected mathematically correct value is not large the accumulated error is sufficient to reach the incorrect conclusion in this work an alternative approach is proposed instead of a floating point value a pair of integers can be used to precisely express the path length avoiding the accumulation of round off errors entirely the main idea is to count and store the numbers of orthogonal and diagonal steps between cells separately although eight distinct directions are considered in single flow rasters only these two kinds of distance are relevant for path length measurements in square grid data the length of a flow path can still be measured by traversing it step by step but instead of accumulating the distances between successive cells one of the two counters is incremented with each step only when the path length needs to be expressed as a single number the values are converted by multiplying the counter of diagonal steps by the square root of 2 and adding the number of orthogonal steps to it this method effectively minimizes the use of floating point calculations by relying primarily on error free integer incrementations fig 4 illustrates the concept it should be noted that this approach is mathematically equivalent to the method of accumulating distances between successive cells however the key difference lies in how the calculations are organized and therefore how they are performed by the machine in practical implementation the proposed approach avoids the accumulation of round off errors and leads to more precise results the tests carried out in the early stages of this work confirmed that this method correctly solves the case presented in fig 3 it should be emphasized that the concepts presented in this section are not directly related to algorithmic performance but to the accuracy of path length measurements all algorithms developed and presented in this work are based on the proposed approach 2 4 design and implementation all algorithms developed as part of this work were implemented in c code sections intended to be executed by multiple threads simultaneously were parallelized using openmp directives the data structure developed in kotyra et al 2021 was used to store the rasters in memory the cells are stored in a two dimensional array with an additional external single cell frame of neutral values due to the large number of gis algorithms accessing the values of the nearest neighboring cells this design often makes it possible to simplify the code and increase its efficiency the aim during the design and implementation phase was to achieve the shortest possible execution times while maintaining the maximum possible precision of the results it was assumed that the task of identifying the longest flow paths is solvable in linear time thus only algorithms with linear time complexity were considered in this work 3 developed algorithms 3 1 reference implementations as the first step in the development and evaluation of new algorithms the basic recursive approach was adapted and expanded upon two relatively straightforward implementations were prepared one sequential and the other attempting to parallelize computations using openmp tasks although both of them were treated mainly as reference solutions they were also the starting point for the development of entirely new algorithms and therefore they are presented here first only solving the fundamental version of the problem with a single outlet point and identifying any of the longest paths was required of the reference implementations it is worth noting that the developed adaptations of the recursive approach still follow the requirements and incorporate the concepts introduced in this work e g their methods of comparing path lengths are based on counting orthogonal and diagonal steps separately 3 1 1 recursive approach sequential the underlying idea of the recursive approach is to search through all flow paths within a catchment by starting at the outlet cell and climbing upstream contrary to the flow direction the recursive procedure calls itself for successive higher cells effectively traversing the entire catchment area in a depth first manner fig 5 illustrates the general concept the algorithm starts the search by calling the recursive procedure for the designated outlet cell the procedure uses the flow direction data to identify the inflow neighbors of the cell and recursively invokes itself for each of them the same process is repeated for successive cells effectively climbing up from the outlet towards topographically higher locations the earlier pending calls for lower cells wait for the subsequent invocations to complete and return their results conceptually the aim of the recursive procedure invoked for any given cell is to identify the longest flow path leading to that particular location thus the initial call for the outlet cell carries out the task of finding the longest flow path for the entire catchment the procedure accomplishes this by delegating subtasks partial searches covering smaller areas to its successive recursive calls similarly each partial search is broken down into even smaller subtasks which are then delegated to subsequent invocations the results obtained and returned for these partial searches are captured and used by earlier pending calls the result produced for any given cell contains the length of the longest flow path leading to this particular point along with the location of the source cell where that path begins since the source cells have no inflow neighbors and are thus the starting points of the flow paths the result returned for any of them contains a zero path length and the location of the cell itself for all other types of cells the procedure captures the results returned by its subsequent invocations increments the appropriate step counter in each of them selects the one with the longest path length and returns it as its own result ultimately the initial call for the outlet cell returns a catchment wide search result containing the length and starting location of the longest flow path leading to the outlet fig 6 illustrates the process the algorithm was implemented with the intention of minimizing its execution time the implementation has a linear time complexity and the time needed to perform the search depends directly on the number of cells belonging to the catchment the recursive climbing procedure does not cross the watershed boundary thus ignoring any cells beyond it it is worth noting that this solution does not require raster storage for partial results during calculations multiple well known problems are related to recursive implementations in general although this approach generally leads to relatively simple and efficient code it is also inherently related to memory issues i e stack overflows often making it impractical for larger datasets wallis et al 2009 since this approach was implemented mainly as a reference solution these issues are not discussed in detail here 3 1 2 recursive approach parallel recursive implementations frequently prove to be difficult counterintuitive or even impossible to efficiently parallelize qin and zhan 2012 stpiczyński 2018 as an attempt to introduce parallelism to the presented recursive algorithm an implementation based on openmp tasks was developed assuming a single outlet point only one thread starts the recursive procedure at the beginning however in this implementation openmp tasks are created for each identified inflow neighbor allowing other threads to join the processing and start performing partial computations this makes it possible for separate flow direction branches to be searched through in parallel unfortunately this approach introduces the additional overhead of creating and managing a large number of tasks the implementation prepared as part of this work aims to limit this cost by allowing the creation of new tasks only to a certain level of recursion above the limit a thread stops generating tasks and performs the rest of the computations in the branch on its own this idea optimistically assumes that it may be possible to reasonably distribute the work between threads at lower levels of recursion close to the outlet point however given the realistic underlying landform this may prove to be rare or even impossible considering that a large fraction of flow direction branches may contain only one or a few cells the benefits of parallelization may turn out to be moderate and easily outweighed by the overhead 3 2 new algorithms during the design and implementation phase two distinct general concepts for the new algorithms were developed in the following sections they are referred to as top down and double drop approaches based on these two fundamental ideas a total of five new algorithms were implemented including three sequential and two parallel versions 3 2 1 top down maximum length sequential the fundamental concept underlying the approach referred to here as top down is to traverse the raster along the flow direction starting from source cells tops the most upstream locations and moving downstream in a way this is the opposite of the recursive approach in a typical flow direction raster a large number of source cells are the starting points of flow paths that gradually merge and eventually end at the same outlet point approaching the problem from this perspective creates new possibilities for parallelization conceptually the algorithm can start from any source cell in the raster the flow path is traversed cell by cell along the flow direction while its length is measured by counting successive orthogonal and diagonal steps a naive implementation of this idea would be to fully traverse and measure each existing flow path separately sequentially starting at each source cell in the raster and moving downstream until the endpoint is reached and compare their lengths e g by recording the longest one found so far however this would not be a well performing approach since a large fraction of the cells in the raster would be unnecessarily traversed multiple times common parts of converging flow paths would be needlessly re measured for each of them a much more efficient solution would be to compare the partial flow paths as soon as they merge in junction cells and only proceed downstream with the longest one discarding the other candidates since it is already clear that they cannot be the longest path in the catchment incorporating this concept the algorithm starts traversing and measuring a flow path from its source cell and moves downstream until it encounters a junction cell here the traversing procedure is temporarily stopped and the algorithm starts another separate measurement from a different source cell only after measuring all partial flow paths leading to a given junction cell the algorithm proceeds downstream to further traverse and measure the longest of them with this approach each cell in the raster can only be traversed downstream once a junction cell cannot be passed until the longest upstream path leading to it is identified this property is directly related to the linear time complexity of the algorithm the first implementation of this approach called top down max for short uses a specific data structure to compute and store partial results namely a raster of two integer cells each cell containing two numbers a single cell can be used to store either a distance as separate numbers of orthogonal and diagonal steps or a location of another cell in the raster as row and column coordinates this concept was implemented as a union of two structures representing distance and location data separately allowing for two possible interpretations of the same cell in the code ultimately every source cell is used to store distance while every non source cell stores location initially all cells of the working raster are set to a special undetermined state being marked as not containing any valid value yet the source cells are then used as working memory to calculate store and compare the partial lengths of the analyzed flow paths the main part of the algorithm aims to fill the raster so that every other non source cell points to the source location where the longest path leading to that particular cell begins this then makes it possible to immediately locate the beginning of the longest flow path leading to any given point most importantly to the outlet cells specified in the input data extracting the source location for any completed cell can be done by checking the cell type as source cells are a special case being their own source locations and reading the coordinates stored in it fig 7 shows a conceptual example of how the search for the longest flow path is performed the algorithm identifies the source cells and starts the downstream traversing procedure for each of them the integers stored in each source cell are treated as orthogonal and diagonal step counters first set to zero and then incremented accordingly with each step along the path at the same time each traversed non source cell is updated with the coordinates of a source location that begins the longest currently known path to this particular cell eventually when this process is complete the outlet cell contains the final search result pointing to the source location of the longest flow path in the entire catchment for each non source cell encountered a path comparison operation is performed to select the longer of the two partial paths leading to that point the one currently being traversed and the one starting from a location previously stored in the cell if the cell still contains the undetermined value the current path is immediately selected the lengths of both partial paths distances traversed so far from their starting points to the junction are extracted from their source cells and compared and the junction cell is updated with the new source location if necessary if the path currently being traversed turns out to be shorter than the one already found for that cell the current path is discarded and the cell does not change its state thus comparisons of partial paths effectively take place in junction cells and only the selected longer path is further considered the lengths of the discarded paths stored in their source cells are no longer incremented link cells having a single inflow neighbor are only entered and updated once changing their state from undetermined to valid source coordinates it is necessary to establish the final value of the junction cell and thus identify the longest path leading to it before proceeding further to its downstream cells as noted earlier this requires traversing all partial paths leading to the junction cell and comparing their lengths using the path comparison operation in the junction for this reason the traversal procedure has to be temporarily stopped in cells for which the lengths of all inflow paths are not yet known only when the final value of the cell is determined the data needed to continue the downstream traverse is available this property was implemented using an inlet number matrix where each cell is first initialized with the number of its inflow neighbors except for the source cells which receive a predefined value for easy identification each time the traversing procedure reaches a given cell its inlet number in the matrix is decremented only when this value reaches zero meaning that all paths leading to this point are completely processed the algorithm is allowed to proceed to the subsequent downstream cells this technique of temporarily stopping computations in junction cells was previously described for flow accumulation algorithms zhou et al 2019 kotyra et al 2021 the processing order of the partial flow paths is irrelevant to the final result although this algorithm was developed with parallelization in mind ultimately only a sequential version was implemented the non atomic nature of the path comparison operation would require the heavy use of synchronization mechanisms to prevent data races between threads resulting in impractical overhead levels to address this issue another version of the top down approach was implemented with the intention of eliminating potential conflicts between threads through a different workflow 3 2 2 top down single update sequential the next algorithm called top down single for short is based on the top down max implementation but with some important modifications the key idea was to reorganize the workflow so that the value of each non source cell in the raster is determined and set only once fig 8 presents a conceptual example as in the top down max implementation the algorithm aims to fill each non source cell with the coordinates of the source point where the longest flow path leading to that particular cell begins however instead of storing and repeatedly updating the partial result the source location of the longest path to that point identified so far in the junction cell the algorithm leaves it uninitialized until the lengths of all its inflow paths are determined only when the values of all inflow neighbors are established the algorithm compares all paths leading to that point and determines the cell value this implementation also uses the inlet number matrix but here the decrementation to zero is required to enter and update the cell rather than to proceed downstream from it algorithm 1 presents a simplified pseudocode covering this concept it is worth noting that only junction cells are processed and updated in this specific way based on reaching out to their inflow neighbors once all their values are determined single inflow cells are processed in a straightforward manner and simply filled with the starting point of the path that is currently being advanced an important advantage of this design is that it no longer requires raster cells to be initialized as each cell is updated only once its previous state is irrelevant and does not affect the final result the algorithm has a linear time complexity each cell is traversed downstream only once 3 2 3 top down single update parallel the idea of a single cell update eliminates problematic conflicts between threads and makes it possible to parallelize this algorithm in a relatively simple manner in this implementation multiple threads traverse partial flow paths simultaneously starting from different source cells junction cells are still locations of potential conflicts but the single update workflow reduces the issue significantly in fact the only synchronization mechanism required is the atomicity of the decrementation in the inlet number matrix with direct capture of the result since different threads may reach the same junction cell and simultaneously attempt to decrement its number of unprocessed inflow paths it has to be ensured that this will not result in data races the last thread that decreases the value of the junction cell in the inlet number matrix reaching zero compares all paths leading to the cell and selects the longest one at this point it is guaranteed that all inflow neighbors contain correct values the thread updates the junction cell and proceeds to traverse its successive downstream cells the inlet number counters and their atomic decrementation are only needed for junction cells all other cells are traversed only once therefore there is no possibility of any conflict between threads as synchronization is not needed there it is not used and as a result non junction cells are updated in an unsynchronized straightforward manner in all the top down implementations developed in this work the final result is obtained by extracting the coordinates stored in the outlet cell specified in the input data each non source cell in the raster indicates where the longest path leading to this cell begins these coordinates are treated as the end result of the algorithm the only special though impractical case is a scenario where the specified outlet point is actually a source cell which does not store location but distance to ensure the correctness of the algorithm for each case this scenario has to be recognized and handled properly it should be emphasized that all the top down implementations have a natural ability to determine the longest flow paths for multiple outlet points in the same run as the main part of the algorithm fills all non source cells with the correct source coordinates determining the longest flow paths for other outlet points only requires extracting these additional starting locations thus determining the longest paths for multiple outlet points in the same run involves only a small negligible in practice additional cost each of these algorithms has been implemented in a way that allows both single and multiple outlet points to be taken as input and processed 3 2 4 double drop sequential another approach to the problem developed as part of this research is referred to in this paper as the double drop although it is also based on traversing flow paths along the flow direction the concept differs significantly from the top down approach the main idea behind the algorithm is relatively straightforward the most characteristic property is that it passes the same sections of the flow paths twice conceptually the traversing procedure can be started from any cell it does not have to be a source cell the algorithm first moves downstream along the flow direction evaluating whether a given flow path leads to the specified outlet point and measuring the distance traveled from the starting cell then the same path is traversed again this time recording the remaining distance to the outlet in each passed cell or marking it as not leading to the outlet point the algorithm also uses a working raster of two integer cells but here the content of each cell is always interpreted as a distance expressed as separate numbers of orthogonal and diagonal steps the main part of the algorithm aims to fill the raster so that each cell contains the path length from itself to the watershed outlet specified in the input or the predefined out of basin value if the path does not lead to the outlet fig 9 shows a conceptual example algorithm 2 presents a simplified pseudocode of this approach as the first step the working raster is initialized with the undetermined values the external frame is filled with out of basin values for easy handling of boundary crossings only the outlet cell is set to a valid distance value of zero being the correct distance to itself next the algorithm starts traversing and measuring subsequent flow paths gradually filling the raster with the calculated distances to the outlet point as mentioned the most distinctive property of this approach is that each partial flow path is traversed twice on the first pass of a given path no modifications are made to the raster only the distance traveled from the starting cell is measured using the orthogonal and diagonal step counters by incrementing them accordingly with each step as soon as the traverse procedure enters a cell with any value other than undetermined or crosses the raster boundary the second pass begins starting again from the same location and following the same path this time each cell passed through is modified and thus set to its final value if the first pass ends with crossing the raster boundary or entering a cell with the out of basin value the second pass sets all cells along the entire path to out of basin a special case of reaching a cell without a valid flow direction value is handled in the same way any cell marked with this value is already known not to lead to the outlet point specified in the input entering such a cell while traversing downstream means that the entire path is located outside the catchment cells containing a valid distance are known to lead to the outlet point and their value describes the remaining length of that path when the algorithm enters such a cell during the first pass the distance traversed so far downstream from the starting point is summed up with the value stored in that cell in this way the total distance from the starting point to the outlet cell is obtained the second pass fills all cells along the path with the remaining distance to the outlet point this time the orthogonal and diagonal step counters expressing the remaining distance are appropriately decremented with each step the algorithm records the length and starting location of the longest path identified so far comparisons between potential candidates are made when a cell with a valid distance is entered during the first pass and the total distance from the starting point to the outlet is calculated alternatively a simple one time raster scan to find the maximum length could be implemented at the end of the algorithm but this would require significantly more comparisons as with the top down approach the order in which the partial flow paths are traversed is irrelevant however the double drop algorithm does not require the traversing procedure to be started from a source cell in fact it can be started from any unprocessed cell regardless of its type this makes the detection of source cells unnecessary significantly simplifying the workflow once all cells are processed the algorithm returns the starting location of the longest identified flow path as the final result it should be emphasized that although this algorithm traverses each location twice it treats all types of cells in the same way and does not require detection or special handling of either source or junction cells this simple workflow could be considered a significant advantage of this approach the implementation has a linear time complexity 3 2 5 double drop parallel the parallel implementation of the double drop algorithm allows multiple threads to process partial flow paths simultaneously the fundamental idea remains the same as in the sequential version although some minor changes were introduced as long as there are unprocessed cells in the raster the threads select them as starting points and proceed to the downstream traversing procedure the same raster is being filled in multiple locations simultaneously allowing the threads to share their results with each other each thread individually stores the length and starting location of the longest path it has identified so far these local results are compared with the global maximum only at the end of the algorithm run this approach minimizes the need for synchronization between threads comparing large numbers of paths it is possible for multiple threads starting from different locations to traverse the same common part of their flow paths in the first pass simultaneously the design of the algorithm allows for such a scenario considering the cost of possible synchronization mechanisms as inviable apart from potential additional partially redundant work this property has no negative effects on the operation of the algorithm in the second pass such cases are generally eliminated early as the thread stops the filling procedure as soon as it reaches the first cell with a value other than undetermined it is worth noting that the double drop approach allows for straightforward determination of all alternative longest flow paths leading to the same outlet point as the length of the longest path is known at the end of the algorithm it is possible to simply search the entire raster for all cells containing a distance equal to the maximum length in this way the algorithm can return not just one but all alternative longest paths with little additional cost both implementations of this approach developed as part of this work include this mode of operation although this algorithm was designed to work with a single outlet cell it is possible to extend these implementations to handle multiple outlets in the same run this would require computing and storing additional data for each cell perhaps a single integer index indicating to which outlet point its flow path leads instead of being marked with out of basin cells flowing to other outlet points would store valid distances and still be easily identifiable while requiring more memory it would likely reduce the time needed for individual algorithm runs when working with multiple outlets 4 performance measurements 4 1 data the bystrzyca catchment was selected as the first source area for measurements and analyses it is a third order watershed with a total area of 94 km2 located in the south eastern part of poland square grid dem data with one meter resolution was obtained from the publicly available resources of the head office of geodesy and cartography gugik the source dataset was referenced in the pl 1992 system national geodetic coordinate system 1992 for poland the original data was processed to generate a wide variety of input datasets for performance measurements in the first stage the outlet point was determined and the entire catchment along with the longest flow path belonging to it was delineated subsequently the division into sub catchments was carried out adopting two different approaches as for the first one the main catchment was divided along the longest flow path every two kilometers starting from the watershed boundary this resulted in 47 sub catchments with varying areas the same procedure was carried out on the source data scaled down to 2 2 m 4 4 m 6 6 m 8 8 m and 10 10 m resolutions the generated datasets were intended mainly for comparing the execution times of various algorithms and tools as well as examining the relationship between their performance and the size of the input data as for the second approach the ten meter resolution data was used based on the drainage network a set of sub catchments was delineated with 3 0 km2 chosen as a surface threshold in this way 118 relatively small sub catchments were generated with areas ranging from 3 0 to 3 4 km2 these datasets were mainly intended for controlling and analyzing the results of the developed algorithms as well as comparing them with those generated by selected gis software additional source areas were selected in order to obtain larger more computationally demanding datasets four other watersheds located in poland were chosen the kamienna tanew barycz and wieprz with areas ranging from approximately 1 300 to 4 440 km2 the acquired dems with one meter resolution originating from the gugik were used to generate datasets ranging in size from approximately two to seven billion cells table 1 shows the details these datasets were intended for examining the performance and characteristics of the developed algorithms as well as comparing them with each other each flow direction raster used in performance measurements was prepared in two variants in the first kind referred to as full frames each cell contained a valid direction value in the second one simply referred to as basins all cells outside the catchment boundaries had a direction value set to none only cells belonging to the catchment contained a valid value previous research by the authors has shown that using data filtered in this way can have a noticeable impact on the computational times of some algorithms kotyra et al 2021 fig 10 shows both the selected source areas and sub catchments obtained with the methods described all datasets were precisely clipped to the selected catchments each row and column of every raster contained at least one cell belonging to the catchment 4 2 testing procedure for c algorithms before running any measurements all implementations were validated against a suite of automated tests the algorithms were tested for multiple scenarios including both standard and special cases as well as a variety of runtime configurations all files with input data intended for measurements were also verified for correctness performance measurements were based on repeatedly executing each algorithm on multiple datasets while measuring and recording computational times each test performed by an automated bash script consisted of restarting the measurement application loading input data files executing the selected algorithm and verifying the correctness of its output time measurements were started immediately after the input dataset was loaded into memory and stopped after the algorithm generated the final result as the recursive implementations were included in the tests stack overflows were expected in such cases the thread stack size was doubled in the runtime configuration more than once if needed and the test was repeated measurements were performed in two separate test environments machine a running under almalinux 8 4 was equipped with a dual intel xeon e5 2670 v3 processor 24 cores in total and 128 gb ram machine b with two operating systems windows 10 enterprise ltsc 64 bit and ubuntu 22 04 1 lts was equipped with a dual intel xeon cpu e5 2620 v4 processor 16 cores in total and 112 gb ram on machine b algorithm performance measurements were carried out under ubuntu all source code was compiled using the gnu c compiler version 8 4 1 on machine a version 11 2 0 on machine b with o3 level optimization and openmp support enabled parallel implementations were allowed to use all available cores in order to precisely compare the performance of the developed algorithms each of them was executed and measured 30 times on each of the eight large datasets kamienna tanew barycz and wieprz both full frame and basin variants for a fair comparison only the fundamental variant of the problem assuming one outlet point and requiring a single path to be identified was taken into account this procedure was performed on both machines two parallel implementations of the newly presented algorithms were also tested in a similar manner on a set of 564 sub catchments of the bystrzyca 47 areas each in six resolutions and two variants these measurements provided the basis for a more detailed analysis including comparisons with other existing software in order to examine the efficiency of the two new parallel algorithms in various multithreading configurations additional measurements were performed on machine a using the largest frame dataset barycz the algorithms were tested with thread limits ranging from 1 to 48 performance was measured 30 times for each configuration the task based recursive implementation requires a parameter that specifies the highest level of recursion at which new tasks can still be created based on the foundations of this concept a relatively low limit of 100 was selected and then doubled several times reaching 200 400 800 and 1600 separate tests were performed for each of these values it was assumed that this variety should allow for an overall evaluation of this approach 4 3 testing procedure for existing gis software to obtain reference computational times for the algorithms developed in this work the performance of the r accumulate tool was measured it was selected as the most recent module for determining the longest flow paths available on the grass gis platform the measurements presented in cho 2020 showed significant advantages of this tool over existing alternatives tests were performed using grass gis version 8 0 2 unfortunately due to the limitations inherent in this platform larger datasets greater than approximately 2 billion cells could not be used measurements were carried out on machine b under both windows 10 and ubuntu the r accumulate tool was executed 30 times on each of the selected datasets under both operating systems an automated script monitored the calculation runs and retrieved the times needed to generate the results 5 results and discussion 5 1 performance comparison of the developed algorithms tables 2 and 3 present the average execution times of the developed algorithms measured using the eight large datasets on both machines the two approaches to data preparation were denoted by ff for full frames and b for basins the parallel implementation of the double drop approach turned out to be the most efficient in the vast majority of cases table 3 however while the parallel version of the top down single algorithm performed slightly worse on average it turned out to be the fastest in some instances the performance differences between these two implementations are more apparent for the measurements made on machine b in favor of the double drop algorithm it appears that both the data characteristics basins versus full frames and the underlying hardware particularly the different number of cpu cores contribute to the relative performance of these algorithms among the sequential implementations the recursive approach turned out to achieve the shortest execution times in all cases on both machines table 2 the sequential double drop algorithm achieved times ranging from approximately 54 to 126 longer and the sequential top down implementations from 151 to 291 longer than the recursive algorithm as both the double drop and top down approaches are relatively more complex and require processing of all cells in the raster as opposed to the recursive algorithm which only visits cells belonging to the catchment these results are not surprising however the results turned out to be completely different for parallel implementations while the newly developed algorithms achieved significant speedups over their respective sequential versions the task based implementation did not improve the performance of the recursive approach the execution times of the parallel recursive implementation were on average several percent longer than those achieved by the sequential version this observation holds for all tested task creation limits moreover while the differences in computational times between the sequential and parallel implementations are relatively small there seems to be a strong positive correlation between the task creation limit and the average time achieved by the task based version this may suggest that increasing the limit further will only degrade the performance more the parallel double drop implementation achieved an average speedup of 11 2 on machine a and 8 5 on machine b across all the datasets for the top down single algorithm the average speedup was significantly higher 18 3 on machine a and 11 7 on machine b it is worth emphasizing that the sequential implementation of the double drop approach achieved on average 41 shorter times than the sequential top down single algorithm while the top down single approach achieves higher speedups the double drop algorithm seems to excel when the number of threads is lower this could help explain why while the double drop algorithm achieved the shortest execution times in the vast majority of cases the top down approach turned out to be slightly faster on a few occasions it is worth noting the differences in computational times achieved on both types of datasets for the recursive approach both sequential and parallel versions there is no particular difference between processing full frames and basins from the same source area however for all the other implementations the differences are significant depending on the algorithm calculations on the basin variant of the data with cells outside the catchment boundary set to none turned out to be approximately 20 to 31 faster on average as expected the recursive implementations both sequential and parallel repeatedly caused stack overflows and consequently application crashes in some cases it was necessary to double the stack size several times before the test could be successfully completed this can be considered as a confirmation that the recursive approach is in fact impractical especially for larger datasets 5 2 comparison with existing gis software across all the datasets used the newly proposed parallel algorithms achieved significantly better performance compared with the reference times generated using the r accumulate tool fig 11 shows a comparison of the average execution times obtained on machine b using the full frame datasets for the top down single algorithm the reference times were on average 10 3 times longer on ubuntu and 12 9 times longer on windows across all the datasets used it is worth noting that in some cases this ratio reached almost 17 on ubuntu and exceeded 30 on windows for the double drop algorithm the reference times obtained on ubuntu were on average 14 6 times longer exceeding 26 times in some cases on windows the average ratio was 18 1 reaching almost 46 for some datasets it is worth noting that the times obtained by the r accumulate tool are relatively similar on the two operating systems for smaller datasets but differ significantly for the larger ones the exact reason is unknown to the authors it is also necessary to emphasize that the r accumulate module operates in a specific environment of the grass gis platform not as a standalone tool for this reason these measurements were intended mainly to provide some external context rather than to serve as a direct comparison 5 3 scalability analysis fig 12 shows the average computational times of the two newly proposed parallel algorithms for different multithreading configurations the double drop algorithm clearly outperforms the top down single implementation for smaller numbers of threads however as can be seen in table 4 the top down single algorithm benefits more from parallel processing when the number of active threads is higher ultimately as the thread limit increases the differences between the execution times of these two algorithms seem to become more and more insignificant it is worth noting that both algorithms achieve high speedups even for relatively small numbers of threads 5 4 analysis of the generated results using the 118 relatively small test datasets covering the bystrzyca sub catchments in ten meter resolution the paths identified by the developed algorithms were analyzed and evaluated in addition to detailed validation the results were compared to the longest flow paths delineated under arcgis pro 3 0 2 using the flow length raster approach from smith 1997 locations courses and numbers of cells belonging to the paths confirmed the correctness of the generated results out of the 118 analyzed datasets the vast majority 104 cases were found to contain only a single longest flow path for the remaining 14 it was possible to identify more than one valid outcome in ten cases two different paths with the same maximum length were found four datasets contained as many as three equally valid alternatives however for all 14 datasets containing more than one longest flow path the differences between the alternative outcomes were rather small the number of differently located cells ranged from one to nine with a median of just two alternative source cells were located relatively close to each other in five out of 118 cases the longest flow path did not start at the watershed boundary but inside the catchment 6 conclusions in this work the existing algorithms and software tools for finding the longest flow paths were reviewed issues were identified with respect to both their performance and precision addressing the need for more efficient solutions new algorithms were developed tested and presented their performance was measured and compared with both other implementations and selected gis software measurements show that the two new parallel algorithms are able to identify the longest flow paths much more efficiently compared with existing alternatives these two implementations have distinct properties and therefore both of them could be considered useful and noteworthy depending on the context of use and the needs of a particular user e g requiring all alternative longest paths to be identified or only estimating maximum path lengths for multiple outlet points one may be more suitable than the other scalability analysis shows that both new algorithms achieve high speedups even for small numbers of threads this makes them suitable not only for high performance hardware but also for widely available low cost multicore devices it is worth noting the differences in the ability of these algorithms to solve specific variants of the problem the top down approach is naturally capable of working with multiple outlet points simultaneously but its data structure allows it to identify only a single longest path leading to each location on the other hand the double drop approach is designed to work with a single outlet point but is capable of recognizing all alternative longest paths at a low additional cost moreover the double drop implementations can be extended relatively easily to process multiple outlet points simultaneously thus the choice of a suitable implementation should depend on the needs of the end user both new algorithms achieve shorter execution times on the flow direction data with cells outside the studied catchment set to none this observation is consistent with the conclusions presented in kotyra et al 2021 regarding flow accumulation algorithms although this gain may not be large enough to intentionally prepare data in this way for just this operation it seems worthwhile to use such datasets when available it should be noted that the algorithms techniques and ideas presented in this work are based on the use of flow direction data although this kind of approach is generally accepted and used in virtually every available tool for finding the longest flow paths modeling the underlying terrain and drainage processes in this way may be considered a simplification and therefore a limitation of this study perhaps the development of algorithms and tools based on more complex models could be a valuable direction for future research credit authorship contribution statement bartłomiej kotyra conceptualization methodology software investigation formal analysis validation visualization writing łukasz chabudziński data curation validation visualization writing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
25376,the stage discharge relationship is affected by hysteresis especially in 1d river cross sections with very mild slopes and or where backwater effects occur to model these cases hydraulic property hp estimations e g hydraulic radius and conveyance are usually required as closed form functions of stage however these are typically unavailable for complex cross sections with overbanks for no sediment laden flows we created the hydrohp 1d tool to solve the 1d saint venant equations aided by the cross section hp stored in tables it also simulates irregular and composite hydraulic cross sections allows hydrodynamic simulation using the single channel method scm and proposes the divided channel method dcm for steady and unsteady flow analysis a wide range of applications in rivers such as the little washita river and the upper negro river and comparisons with noaa normal depth solver hec ras and with an analytical solution shows a promising scenario of applying hydrohp 1d to estimate 1d steady and unsteady hydrodynamics graphical abstract keywords saint venant equations unsteady rating curve hydrodynamic modeling divided channel method hysteresis channel routing data availability algorithms and data used are available in an open repository at https github com marcusnobrega eng hydrohp software availability name of software hydrohp 1d developer marcus nóbrega gomes júnior contact address department of hydraulics and sanitation university of são paulo são carlos school of engineering av trab são carlense 400 centro são carlos 13566 590 email marcusnobrega engcivil gmail com software required matlab version 2021a or higher microsoft excel 2013 or higher programming language matlab program size approximately 5 mb availability open source github license available in gomes jr 2022 1 introduction stage discharge relationships also referred to as rating curves are one of the main tools for hydraulic engineering these tools are dependent on cross section geometry longitudinal slope friction and shear stress variations two forms of rating curve derivation are typically performed biunivocal steady rating curves and looped unsteady rating curves the latter is derived from the application of newton s 2nd law where two dimensional space dimensions are orders of magnitude larger than vertical dimensions in this case the shallow water equations swe are derived from coupling mass balance friction and conservation of momentum in a partial differential system of equations usually numerically solved this type of modeling is also known as dynamic wave and is used in software as the storm water management model rossman et al 2010 for steady flow modeling however we simplify the shallow water dynamics by assuming the friction slope as the bottom slope and we apply a resistance equation such as manning s or darcy weisbach equations to compute friction losses in this case we assume a kinematic wave flood wave approximation in swe swe are usually applied to estimate the hydraulic flow behavior on one dimensional 1d channels and two dimensional 2d floodplain dynamics focusing on the 1d modeling the saint venant equations sve are the special case when the flow is assumed to be unidimensional with the same longitudinal velocity throughout the whole cross section in other words vertical acceleration is considered negligible these equations form a set of hyperbolic partial differential equations derived from 3d navier stokes equations applicable to solving the conservation of mass and momentum in channels the equations are versatile to represent flows at atmospheric pressure full pressurized pipe flows with the preissmann slot concept cunge 1980 and also sub atmospheric full pipe flows if adjustments are made in sve vasconcelos et al 2006 in most cases however no analytical solution is available especially for complex cases such as irregular cross sections therefore numerical methods are typically applied to solve the sve strelkoff 1970 zarmehi et al 2011 roohi et al 2020 chen et al 2013 in these cases classic method approaches use closed form equations representing hydraulic functions such as wetted area wetted perimeter channel s top width and the relative centroid position to the surface in terms of water surface depth these functions are functions of water surface depth and are relatively simple to derive for common regular shapes such as rectangular asymmetric trapezoid triangular or parabolic channels simões et al 2017 however natural rivers are generally defined by irregular geometry and closed form equations of hydraulic properties hp are not available as shown in farina et al 2015 and gleason 2015 abrupt increases in perimeters cause discontinuities in hydraulic radius functions that ultimately cause sharp variations in the rating curve in addition channels are generally conceptualized with a constant gauckler manning strickler coefficient here simply denoted as manning s coefficient for the entire main channel and sometimes with different values for overbanks petikas et al 2020 however manning s coefficient is an empirical representation of friction at the cross section perimeter representing the friction properties of the interface between the flow and the cross section therefore it probably changes with water surface depth te chow 2010 manning s equation was derived for fully turbulent steady uniform flow at normal depth with mild slopes which is the typical conditions of river flooding chanson 2004 although software such as the hydrologic engineering center river analysis system hec ras brunner 2016 and the stormwater management model swmm rossman et al 2010 have 1d full momentum solvers their hp algorithms to determine hydraulic properties are not explicitly specified in their manuals this poses a drawback for new modeling developments using other software packages such as matlab or python we overcome these issues here by developing two different algorithms to estimate hp in regular irregular and compound cross section channels and pinpointing various applications of the algorithms developed for steady and unsteady rating curve estimations the following section assesses the literature review on the aforementioned topics the tool developed in this paper is easy to use and can be used for teaching and educational purposes 1 1 literature review since the beginning of the 20th century the literature has shown that applications of stage discharge relationships are extensive the research carried out in garbrecht 1990 advanced the analytical definition of irregular cross section hydraulic functions using coupled power fitness functions for the main channel and overbank areas these functions were fitted using elevation and station data for each breakpoint in the surveyed channel the author created a methodology to derive these functions considering different roughnesses in the main channel and at the left and right overbanks similarly dingman 2007 derived analytical at station hp for more generalized cases using not only manning s equation but also chezy s another method for estimating hp in rivers using polynomial functions is found in hrafnkelsson et al 2021 instead of creating power law functions with constant exponents the authors proposed a depth varying term in the standard rating curve function to fit better river changing dynamics over time a significant model consideration in rating curve estimations is how friction is modeled the application in abril and knight 2004 shows a depth averaged model using the darcy weisbach friction model applied in a depth averaged unsteady model to determine unsteady rating curves in regular sections another rating curve method based on the conservation of the mass and momentum equations is described in dottori et al 2009 there is no consensus on which friction model should be applied for open channel rating curve models however manning s friction model is more common probably due to its simplicity and number of studies the propagation of the hydrograph in the channel length causes energy losses flow attenuation i e diffusive effects and flow delay i e kinematic effects in addition to these effects convective and local acceleration change the relationship between stage and discharge the rising and falling limb of the hydrograph converts a single biunivocal rating curve into a looped rating curve due to hysteresis petersen øverleir 2006 perret et al 2022 wolfs and willems 2014 the methodology presented in dottori et al 2009 is related to this issue and is particularly important in channels where local and convective acceleration play an important role i e channels with very mild slopes and large friction when using data to compare modeling results with observations one of the main problems is the lack of observation data for large flows especially because observations of low frequency flows that is relatively large flows are rare lang et al 2010 and difficult to obtain this poses the hydraulic modeling of rating curves as an important tool to determine the probabilities of exceedance of flows and depths that could lead to downstream flood damage moncoulon et al 2014 however when data are available on a large scale data driven algorithms can also be used to predict stage discharge in streams as shown in muste et al 2022 determining rating curves based on hydraulic models in river cross sections usually require detailed bathymetric data this gives rise to a drawback for ungauged rivers with neither flow observations to empirically fit a rating curve manfreda 2018 nor detailed cross section data zheng et al 2018 to model flow discharge relationships to this end the study conducted in zheng et al 2018 developed a continental scale flood mapping data set based on a 1 3 arc second 10 m grid cross section data derived directly from digital elevation models provided by usgs their method based on manning s uniform flow equation was comparable to products derived from the federal emergency management agency fema with substantially fewer data required compared to full momentum 2d models while presenting reasonable results in terms of the hydraulic limitations of various approaches to determine steady rating curves in open channels studies as presented in hosseini 2004 stephenson and kolovopoulos 1990 fernandes 2021a and khodashenas and paquier 1999 show that the interface within the main channel and the overbanks have shear stresses that are not considered when dividing the cross section into homogeneous parts in contrast when considering a single section method with a single average velocity for the inbank and overbanks discharge passes through discontinuities when flow depth reaches overbank levels with a relatively low hydraulic radius sahu et al 2011 following these cases two main alternatives have been implemented to determine normal discharges in channels in most hydraulic engineering procedures hosseini 2004 a dividing the channel into theoretically homogeneous sections brunner 2016 each with its own manning s roughness coefficient changing in terms of the stage or for each surveyed cross section break point moreover manning s coefficient could be considered constant changing only for in bank and over banks b calculating hydraulic properties as a single cross section scm the manning s roughness coefficient can vary for each break point with stage or assumed constant for inbank and overbank areas the fundamental difference between procedures a and b is the way the velocity is considered in a the velocity can be different for the inbank overbanks or even when the roughness changes under the inbank conditions therefore the flow is calculated as an algebraic sum of each flow in each discretized area of the cross section in procedure b however velocity is assumed uniform for the whole cross section this assumption implies that overbank areas typically with large perimeters and friction would have the same velocity as the inbank areas which might not always be a good assumption a more detailed theoretical alternative was developed in hosseini 2004 by the introduction of a coherence factor into modeled conveyances with cases a and b to correct velocities in the overbanks and the main channel however in general hydraulic engineering calculations which are often performed by engineers using pieces of software such as hec ras mainly focus on solutions using the aforementioned alternatives a or b shear stress occurs when a compound channel exchanges flow from the main channel to the overbanks and a momentum transfer hua et al 2007 occurs as mentioned the velocity in overbank areas is generally lower than in the main channel due to relatively high roughness coefficients in overbanks and relatively larger perimeters in these cases the energy and momentum equations can be corrected by the introduction of coriolis i e energy correction and boussinesq i e momentum correction coefficients however accurate determination of them is a taunting task as shown by the research conducted in yang et al 2018 the aforementioned literature shows different methods applications and correction factors but no consensus is found when modeling relatively common problems such as determining the normal discharge in a compound or irregular channel exploring these issues and providing more guidance related to uncertainty in different model conceptualizations is needed in this article we pinpoint their main differences the uncertainty in the rating curves has not been shown to be negligible in several studies as presented in domeneghetti et al 2012 ghanghas et al 2022 kuczera 1996 and clarke et al 2000 we want to explore these issues and provide more guidance related to uncertainty in different model conceptualizations and pinpoint their differences in our case studies the methods proposed herein can be used in various applications such as determining design rating curves in ungauged rivers aid in the design of 1d hydraulic works as channels gutters or low impact development facilities as vegetated swales especially in poorly gauged watersheds where stream gauges are typically unavailable this methodology can be a tool aiding planning and management of floods nonetheless even when stream gauges are available the correct measurement of high level flows is a taunting task because of the typical high turbulence and measurement noise in these cases the algorithms are developed in matlab and cross sectional information is input into the model in a xlsx file with all codes open source the model can be easily associated with hydrological models making the hydrohp model adaptable to assess the impacts of climate change in floods for instance the results of hydrohp 1d are displayed graphically in tables and printed as pdf automatically 1 2 paper objectives and contributions despite experimental and modeling research attached to the problem of determining flow rates in regular and irregular cross sections we observe from these aforementioned studies a lack of generalized algorithms to estimate hp in rivers of composite regular and irregular sections studies presented in hosseini 2004 fernandes 2021a make clear the impact of shear stress in compound regular cross sections however little has been researched for modeling more complex regular and irregular cross sections with single stage roughness or depth varying roughness coefficients the specific objectives of this study are described as follows develop novel generalized algorithms to determine hp for regular and irregular cross sections algorithm 2 focuses on the plane geometry while algorithm 2 available in the supplemental material focuses on the finite element method assess the uncertainty in modeled based rating curve estimations assess the role of manning s roughness coefficient in conveyance estimation develop a modified scm equivalent to the dcm model coupled with additional shear stresses at the interface within the overbanks and the main channel develop a method for estimating dynamic rating curves using a full momentum hydraulic model compare the dynamical rating curve estimations with the normal flow rating curve estimations the remainder of the paper is organized as follows section 2 develops the mathematical model used in the paper describing the hydraulic property functions used in the algorithms developed and detailing the governing equations in addition this section explains the methods used to assess rating curve uncertainty and the methods used to evaluate dynamical rating curves by hydraulic modeling next section 3 describes each numerical case study in which the methods were applied in this paper following the case studies section 4 shows the results of all numerical case studies and discusses their main implications as well as the strengths and weaknesses of the model in section 4 6 section 5 discusses the conclusions and future work finally the supplemental material details the governing equations and pseudocode for algorithm 2 data acquisition and cross section determination for synthetic cross sections tested in this paper the paper notation for this paper is introduced next paper s notation italicized boldface upper and lower case characters represent matrices and column vectors a is a scalar a is a vector and a is a matrix italicized regular upper and lower case characters represent scalars q is a scalar q is also a scalar the notation r denote a set of real numbers the notations n and n denote the set of natural and positive natural numbers a normally distributed random number with average μ and variance σ n 2 is notated by n μ σ n 2 given a vector x r n the notation x i j with i and j n represents a cut in x from ith to jth entries the notations r n and r m n denote a column vector with n elements and an m by n matrix in r 2 material and methods the process of estimating the hp of irregular cross sections although has been addressed by research conducted in garbrecht 1990 and dingman 2007 still lacks a general algorithm procedure capable of working in irregular regular and compound cross sections in this paper we solve this issue by providing two types of algorithms to determine hp assuming surveyed cross sectional data at different stations the algorithms and data used in this article are available in an open repository at gomes jr 2022 section 2 8 details governing equations valid for algorithm 1 whereas the supplemental material explains the same for algorithm 2 the following sub sections apply to both algorithms 2 1 cross section data here we follow the left right convention for the cross section data input data is organized in terms of breakpoints each breakpoint has its x and y coordinates manning s roughness coefficient is defined in three ways i it can be entered for each break point ii for each stage or iii a constant value for inbank and overbank the method used in the model can also be chosen i e scm or dcm the friction slope i e typically assumed as the bottom slope in normal flow rating curves is also entered all these aforementioned data are summarized in a xlsx file which is read in the matlab codes entries of elevation and horizontal distance within stations as well as manning s roughness coefficients within stations are the data required to simulate the irregular cross sectional shapes details of how to use the model and how to enter the input data can be found in the supplemental material moreover all the code is also included in detail subsequently we derive flow areas wetted perimeters hydraulic radius section factor ϕ conveyance k and interpolate cross section coordinates assuming linear relationships within known breakpoint data a schematic of a typical river cross section is presented in fig 1 where we detail the rapid increases in area and perimeter due to polygons formed for specific water surface depths 2 2 unique elevation values to avoid points with the same elevation or with the same station and to be able to treat all cross sections as irregular cross sections we define a tiny number σ 1 1 0 4 m and replace elevations and station values as follows 1a z i z i σ z i 1 z i i 1 2 n s t 1b x i x i σ x i 1 x i i 1 2 n s t where n s is the number of stations measured this allows us to use trapezoid formulas for determining areas and perimeters see fig 2 and to identify horizontal or vertical segments by calculating coordinate differences such that segments with a particular depth and station difference smaller than a threshold would be considered horizontal and or vertical segments we detail these cases in more detail in this section in 2 8 1 2 3 hydraulic radius once the area and perimeter are calculated for each segment the hydraulic radius can be calculated as the ratio within the flow area and its correspondent perimeter sturm 2021 such that 2 r h i a i p i i 1 2 n v t where n v is the number of vertical steps a i is the flow area p i is the perimeter and i is the i th vertical segment taken from the invert 2 4 section factor ϕ the section factor ϕ is given by sturm 2021 3 ϕ i a i r h i 2 3 i 1 2 n v t the previous equation can also be equal to q s 0 for uniform flow which indicates a biunivocal relationship of cross section properties with normal flow if ϕ increases monotonically sturm 2021 2 5 representative manning s coefficient for single channel method the values of n can change in terms of the surface roughness conditions for example in the overbanks the presence of vegetation leads to a reduced flow velocity compared to that in the main channel fernandes 2021b kidson et al 2006 we assume herein that the representative manning s coefficient for a cross section in terms of water surface elevation depends on each value of n for each sub segment if we are modeling using the single section method more specifically we assume that n changes with a non linear function of the perimeter p based on the hypothesis that the total cross sectional mean velocity is equal to the subarea mean velocity einstein 1934 brunner 2016 horton 1933 such that 4 n i j 1 i n l j 3 2 p l j n r j 3 2 p r j p i 2 3 where n l and n r are the manning s coefficients for left and right directions from the invert assuming piecewise continuous functions of the given n derived from surveyed data for each sub segment similarly p l and p r are the perimeters relative to the left and right slopes of the invert and p i are the calculated total perimeter of a single section further described in eq 10 2 6 conveyance factor the conveyance factor is derived with manning s roughness coefficient and ϕ the main hypothesis of manning s equation is that average velocity is constant throughout the whole cross section this is approximately the case when we model the cross section with a constant roughness coefficient with no discontinuities in hydraulic properties however two problems can occur first if ϕ is not a monotonic increasing function and if n increases the velocity might decrease so that the flow in a further section would be smaller than that of a section with lower depth this is the case for circular pipes sturm 2021 however this condition is rare in open irregular channels generally the function ϕ can have discontinuities and rapid changes when the flow reaches the overbank areas in these cases we can separate the channel and overbanks conveyances the method used to calculate the conveyance varies with different types of software such as hec ras brunner 2016 and mike 1d such that hec ras separates the main channel and the overbanks here we consider both methods the single cross section method scm and the divided cross section method dcm into conveyance calculations al khatib et al 2012 such that 5a k scm i 1 n i ϕ i i 1 2 n v t 5b k dcm i k m i k f i where k m and k f are the main channel and the overbank conveyances 2 7 centroid distance assuming a constant water density the vertical centroid coordinate taken from the invert elevation can be derived using the first moment of area as presented in eq 6 to solve the integral we assume a moving trapezoid from i to i 1 given by 6 y i 1 a i 0 i δ y a y d y j 1 i a j a j 1 y j y j 1 2 a i where δ y is the vertical discretization 2 8 algorithm 1 geometrical procedure with a while loops 2 8 1 cross section lateral angles in this subsection we show how to derive a generalized algorithm to estimate hydraulic properties in regular composite or irregular cross sections the differences between algorithms 1 and 2 are the first is based on plane geometry whereas the second is based on the finite element method the water surface elevation is assumed with no slope in the cross section and the longitudinal slope is constant throughout the whole section therefore we can use plane geometry to determine the areas and perimeters for a given water depth from the invert for each depth y evaluated we calculate the left and right tangents by defining vectors that represent the values of the breakpoints of the left l and right r of the invert let x l y l x r and y r be the coordinates left and right of invert for a y i i δ y we find the coordinates on the left and right larger i or smaller i than y i resulting in the following tangents 7a α l i y r i y r i x l i x l i 7b α r i y l i y l i x r i x r i the previous formulae are only valid in inclined segments i e lateral slopes are not 0 or π 2 considering the coordinates of two consecutive breakpoints 4 alternatives can occur we classify them as i vertical segment ii horizontal segment iii vertical and horizontal segment and iv regular segment this classification is derived from the vertical and horizontal distances within consecutive breakpoints such that if the differences are equal σ then a vertical and or horizontal segment is identified these cases are treated in the main code with a simple if statements 2 8 2 top width geometry assuming a finite y discretization δ y the increase of the top width b can be calculated with the angle defined in eq 7 however in the cases where a drastic change in x occurs from i to i 1 we can calculate b i 1 as 8 b i 1 b i δ b l i 1 δ b r i 1 δ y 1 α r i 1 α l i where δ b l and δ b r are the additional values of b that must be added from the left and right directions of the invert y axis to consider horizontal changes in areas without slopes 2 8 3 flow area calculations the basic idea for the area is to calculate trapezoid areas with bases b i 1 and b i and depths δ y however similar to the increase in top width mentioned in eq 8 the area in section i 1 can also have a drastic increase this is the case shown in fig 1 in the parts where the flow is retained within a surface in these cases the area calculation has to consider the polygon area formed by the surface generated from left and right overbanks such that 9 a i 1 b i 1 b i δ y 2 a t a i δ a l i δ a r i where δ a is the increase in area from discontinuities derived from the channel section from left and right directions calculated as a polygon area and a t is the area of the incremental trapezoid from i to i 1 2 8 4 perimeter in terms of water surface depth the cross section perimeter is a monotonic function in terms of y in parallel with calculations of a and b the perimeter function might also have discontinuities for example these sharp changes in the perimeter can also be due to water reaching overbanks or increasing horizontal distances due to the deposition of sand in the main channels the perimeter can be calculated in terms of lateral distances within the cross sections from incremental perimeters from overbanks and the base at the invert these distances can be calculated using the angles defined in eq 7 therefore we can write the perimeter functions as follows 10 p i 1 p i δ y 1 sin arctan α l 1 sin arctan α r f α δ p l i δ p r i as shown in the previous equation the perimeter function could have issues for undefined values of α to this end we treat these cases with if statement conditions in the algorithm identifying these points and assuming that f α 1 2 9 single channel method scm in this method there is no distinction between overbank and inbank flow conditions an example of a cross section where this hypothesis would be applicable is shown in fig 3 a b and c the stage roughness is determined in terms of the perimeter following eq 4 for inbank conditions this method is identical to the divided channel method described in the following section the scm method considers a stage conveyance curve that ultimately converts depth into flows such that 11 q i k i s 0 where q is the normal flow rate and s 0 is the bottom slope which is assumed as the friction slope for the normal flow estimates 2 10 divided channel method dcm herein let n fp n 0 1 2 t be the number of floodplains in a cross section as shown in fig 4 in this method the cross section is divided into main channel m and floodplains f the interface between the floodplains and the main channel introduces a new shear force that is considered by adding the wet perimeter in the main channel of a value equal to n fp y y m in the following subsection we define the governing equations to determine the hp for the dcm 2 10 1 correcting manning s coefficient a new system of equations is defined if we assume an additional shear stress acting in the interface between the main channel and the floodplains see fig 4 defining the break point divider we calculate the width of the main channel b m by extending the line segment from the divider to the opposite overbank hence the left and right sides of this line define the overbanks while inside of this segment towards the invert is considered the main channel in this paper we consider that the wet perimeter of the main channel increases by h f for each floodplain let n m be the final representative manning s coefficient for the main channel and n f be the representative manning s coefficient for the floodplains if we assume different velocities on the floodplain and the main channel we can estimate a new roughness coefficient n such that k scm k f k m in eq 5b resulting in eq 12f 12a y f y y m 12b a f a a m b m y f 12c p f p p m 12d p m p m n fp y f 12e a m a m b m y f 12f n a a p 2 3 1 n f a f a f p f 2 3 1 n c a m a m p m 2 3 where b m is the width of the main channel the variables a and p are determined by eqs 9 and 10 and the superscript represents values corrected for by shear stress induced by the interface within the main channel and floodplains this new roughness coefficient accounts for vertical division within the main channel and overbank roughness by including the increase in wet perimeter in the main channel in terms of h f while allowing the use of the total stage area and stage perimeter algorithms derived in section 2 8 therefore the normal flow can be calculated as follows 13 q dcm q scm 1 n a a p 2 3 k scm s 0 where q scm is the modified single section method to account for overbanks and k scm its respective conveyance 2 10 2 overbank areas and discontinuities in this model the elevation of the water surface is assumed to be continuous for the in bank and overbanks however in case the algorithm identifies discontinuities in cross section elevation i e elevation data are not monotonically increasing and find left and or right areas that are filled if water depth exceeds a threshold the model can generate two sets of polygons to represent these cases these polygons can increase areas and perimeters abruptly which may increase or decrease flow rates depending on the increased values let y d be the depth taken from the invert elevation that starts to have overbank flows see fig 1 if section i is such that y i y d the algorithm searches for left and right directions from the invert y axis to find breakpoints with depths smaller than y d if depths are found then a polygon can be defined and extra values of δ a δ p and δ b can be calculated from it in matlab this problem can be solved using the functions polyshape and polyarea 2 11 the 1d shallow water equations for the estimation of the 1d hydrodynamics we focus on solving the complete one dimensional saint venant equations gerbeau and perthame 2000 these equations are derived from the conservation of mass and momentum by applying newton s second law in a finite fluid element let d be the set of computational nodes from 1 to n x for a node i d let a x t be the cross section flow area at a given time t and space x and v x t be the depth average flow velocity all other states can be derived from these therefore we can write the saint venant equations as the following non linear hyperbolic system of partial differential equations for a node i at coordinate x and in time as follows 14a q x t x f x t t s x t 14b q a q t 14c f q β v a v g a y t 14d s 0 g a i o i f 14e i f n 2 q q r h 4 3 a 2 14f q v a 14g β 1 g n 2 r h 1 3 κ 2 where a is the cross section area v is the wave velocity g is the gravity acceleration y is the distance from the water surface to the centroid of the cross section i o is the bottom slope and i f is the friction slope β is the boussinesq coefficient for moment transfer corrections and κ is the von kármán s coefficient yang et al 2018 usually assumed as 0 41 both a and v are the main states solved for the longitudinal distance x and time t such that a a x t and v v x t in the following derivations of this paper we neglect the x and t indexes for easier notation as well as for other states dependent on a and v as shown in the previous sub equations functions describing the hydraulic radius and cross section centroid distance from the water surface are required therefore we first apply the hydraulic property estimation algorithm to determine these functions and solve the sve by using the tabled results to find the required geometric functions to solve all nodes together we collect the state vectors e g q s and f for each internal node from i 2 to n x 1 that is from the second node to the second to last node such that matrices q q 2 q 3 q n x 1 t f f 2 f 3 f n x 1 t and s s 2 s 3 s n x 1 t can be derived therefore we solve the vectorized set of hyperbolic partial differential equations for all nodes except the boundaries by numerically discretizing the problem using the lax friedrichs method lax 1954 this numerical scheme uses a forward discretization in time centered discretization in space and has first order accuracy in space in time kurganov 2018 expliciting eq 14a for q we can derive the following matrixwise expression such that 15 q t δ t 1 2 q b t q f t δ t 2 δ x f f t f b t 1 2 s b t s f t where subscripts b and f represent backward and forward states from each node i matrices q q b q f f f f b s b and s f r n x 2 2 the boundary conditions are applied in matrices from backwards and forwards for q and the source terms and flux terms for the boundary conditions are calculated using eqs 14c 14g the connection between the sve model with the hp estimator model is presented in fig 5 2 11 1 adaptive time stepping hydrohp 1d model has an adaptive time step scheme based on courant number c courant et al 1928 our model has to ensure c 1 to guarantee convergence due to the explicit finite difference scheme used to solve 1d sve hydrohp 1d spatial mesh is stationary with constant length δ x therefore the only manner to ensure computational stability is by changing the model time step δ t accordingly such that at the end of a computational time step we calculate the refreshed computational time step as follows 16 δ t t min i d c u i t g h m i t where c is the user defined courant number e g typically assumed between 0 5 and 1 for 1d hydrodynamic modeling u t q t a t is the flow velocity h m t a t b t t is a representative water depth for irregular channels and b t t is the top width 2 11 2 boundary conditions in order to represent feasible hydraulic conditions in 1d channels hydrohp 1d has various options for combinations of boundary conditions the current model version has 3 upstream boundary conditions and 2 downstream boundary conditions the first node of the mesh can be subjected to a nash inflow hydrograph e g see eq 21 tabular inflow hydrograph and tabular stage hydrograph tabular data is then interpolated to the model time step via piecewise cubic hermite interpolating polynomial method barker and mcdougall 2020 the downstream boundary conditions are twofold normal slope boundary condition or stage hydrograph boundary condition through the modeling of a wave function since hydrohp 1d uses the lax friedrichs method that uses a central difference in space the solution is only properly given in internal nodes therefore numerical issues in the boundary may arise when poorly discretized meshes are used akan and iyer 2021 when either stage hydrograph or inflow hydrograph boundary conditions are used in the borders of the domain a zero order extrapolation is used more details of the treatment of the boundary conditions is explained in the supplemental material section 3 case studies in this section we show various model applications for different cross sections first in section 3 1 we apply the model in single sections for triangular parabolic and semi elliptical semi parabolic cross section moreover the model is applied to estimate composite cross sections in section 3 2 followed by an example of an irregular cross section with left and right overbanks in section 3 3 the cross section data are shown in fig 3 the assumed data for the sections can be found in the supplemental material the variety of these sections was selected to represent the different common and complex shapes used in hydraulic design in the particular case of irregular cross section this represents a real river section in the little washita river garbrecht 1990 3 1 numerical case study 1 normal flow in complex regular cross sections in the first case study we show three examples of determining stage discharge relationships for well known cross sections the tested cases are cross sections of the following shapes a triangular b parabolic c semi elliptical semi parabolic these sections can be represented by single closed form equations for hp since the geometry functions are relatively easy to determine for the particular case of a we can determine the cross section with 3 coordinates however for cases b and c since geometry and slopes can change dramatically with x we discretize the depth into 1 cm steps and determine x in terms of y with the governing equations of b and c therefore the number of coordinates entered to determine shapes b and c is primarily dependent on the maximum depth of the channel 3 2 numerical case study 2 normal flow in composite regular cross sections another common type of cross section used in channels is the use of combinations of regular shapes in this numerical case study we estimate hp for the following cross sections d road gutter e composite triangular and rectangular and f successive trapezoid gabion channel we have chosen these sections to pinpoint common cross sections used in hydraulic engineering design as mentioned in 3 1 x is numerically discretized in terms of y for rapid changes in cross section shapes in these cases all sections of this case study are only changed by changing slopes when a particular threshold depth occurs d and f or by increasing b i see eq 8 3 3 numerical case study 3 irregular cross sections with overbanks in this case study we model a cross section of the river presented in garbrecht 1990 two stage roughness hypotheses were tested in this case study 3 3 1 single manning coefficient assuming a depth invariant stage roughness curve with a baseline roughness of n b 0 02 sm 1 3 for all break point segments we model the stage conveyance assuming certain eventually feasible values of roughness 3 3 2 depth varying coefficient in this subsection we test the role of uncertainty in the estimation of the roughness coefficient and its propagation in the conveyance three tests are performed as follows scenario i assumes the baseline scenario shown in the previous section scenario ii assumes a linear increasing stage roughness relationship mustaffa et al 2016 scenario iii considers a 2 order polynomial stage roughness relationship and scenario iv assesses a monte carlo stage roughness relationship with a known average variation μ and standard deviation σ n varying from a base roughness coefficient from scenario i this scenario also accounts for cases where the roughness coefficient decreases with stage as presented in alves et al 2020 a static scenarios the stage roughness relationships for the static scenarios i e only one depth series per scenario are shown as follows 17a n 1 n b 17b n 2 d y n b α d y 17c n 3 d y n b α d y 2 where d represents the left or right directions from the invert and α are coefficients that describe the variation of the roughness b monte carlo analysis the uncertainty in manning s coefficient is evaluated assuming an average error μ 30 and a standard deviation σ n 0 01 based on kim et al 2010 we perform 200 monte carlo simulations to estimate the eventual variations of roughness in the channel the signal in the following equation represents the cases where the roughness increases and decreases on average 18 n 4 d y n b 1 μ σ n n 0 1 although only one series of n 4 is shown in fig 8 we estimate the flow discharge for 200 cases 3 4 numerical case study 4 divided channel method vs single channel in this subsection we define the analysis performed to compare the differences between scm and dcm in modeling conveyances in regular composite and irregular cross sections we use the relative error index to calculate the percentage error between scm and dcm as follows 19 re k scm k dcm k dcm 100 similarly we compute the relative error between scm and hec ras re ras brunner 2016 analogy with eq 19 3 5 numerical case study 5 modeling unsteady state hydrodynamics and rating curves with a 1d sve model 3 5 1 non breaking wave propagation over a horizontal plane this section compares solutions given by hydrohp 1d hec ras 1d and an analytical solution for a 1d wave propagating over a horizontal plane the diffusive convective and inertial terms of sve play important roles in this case and are tested hunter et al 2005 developed an analytical solution of sve for this problem neglecting inertial and convective terms such that the water depth in position x at time t can be written as 20 h x t 7 3 c n 2 u 3 x u t 7 3 where u is the depth averaged velocity in the x direction c is a constant of integration solved by referring to the initial conditions of the problem i e h 0 t 0 0 n is the stage invariant roughness coefficient and t the elapsed time the parameters assumed are u 1 m s n 0 03 sm 1 3 and the channel width is assumed as 100 m results are compared within the developed model hec ras 1d full momentum solver and analytical solution given by eq 20 3 5 2 unsteady state inflow hydrograph and normal flow at the outlet in this section we compare the performance of hydrohp 1d against hec ras 1d model under different combinations of stage hydrographs and inflow hydrographs hec ras is set to use the 1d unsteady finite difference numerical solution with the skyline gaussian solver brunner 2016 which uses an implicit numerical scheme and solves a series of non linear system of equations whereas hydrohp 1d uses an explicit finite difference method for this numerical testing we adapt the problem 8 3 presented in akan and iyer 2021 to a different inflow hydrograph boundary condition with a relatively more complex cross section the problem consists in an unsteady state simulation of an open channel with n 0 025 sm 1 3 s 0 0 00025 m m with an inflow hydrograph boundary condition and a normal slope outlet boundary condition i f n x 0 00025 m m the channel length is 1097 88 m and the elevation of the first node is 0 274 m simulation time is 360 min and detailed outputs are retrieved each 10 min the differences from akan and iyer 2021 are the cross section number of nodes and the tested and the inflow hydrograph herein we apply hec ras and hydrohp 1d in the v notch see fig 3 for an inflow hydrograph modeled with a nash function i e eq 21 with a peak flow of 2 5 m 3 s time to peak of 3 hours baseflow of 0 25 m 3 s and β 8 5 moreover hec ras 1d and hydrohp 1d are discretized with 100 equally spaced cross sections hec ras is set to solve the fully implicit scheme with a fixed time step of 1 s while hydrohp 1d adaptive time step scheme see eq 16 is used with δ t min 0 5 s δ t max 5 s c 0 5 3 5 3 irregular cross section in this case study we simulate the diffusive and convective effects in flow propagation in an open channel with irregular geometry the goal is to determine looped rating curves and to estimate another possible source of uncertainty in stage discharge modeling holmes 2016 for this analysis we consider a 1000 m channel discretized into 50 sub reaches with 20 m length each the bottom slope was assumed as 2 1 0 4 to represent a feasible condition for rivers where convective and advective accelerations play important roles we assumed a normal depth outlet boundary condition meaning that the energy slope gradient at the outlet is the same as the bottom slope to simulate the effects of convective advective acceleration and the diffusive kinematic effects of the flood wave looped stage discharge curves are modeled using an inlet boundary condition modeled with a nash function hydrograph akan and iyer 2021 gomes jr et al 2023 which is defined as 21 q in t q b q p q b t t p exp 1 t t p γ where q in is the inflow hydrograph q b is the baseflow q p is the peak flow t p is the peak time and γ shapes the hydrograph the aforementioned parameters are typically derived from hydrology studies of the upstream watershed therefore they represent the degree to which surface and sub surface runoff are generated in detail q p is closely related to the watershed area shape infiltration and imperiousness rate in addition the factor γ shapes the rate of q in defining the flow acceleration therefore it is closely related to the aforementioned parameters of q p however the parameter q b is mostly related to groundwater properties and soil infiltration capacity as parameters for modeling we assumed q b 1 m 3 s q p 75 m 3 s and t p 2 h these values typically represent inbank conditions for the assumed river slope for normal flow the simulation time was assumed as t f 6 h and the computational time step was set as δ t 5 s in this study we test the possible effects of different hydrographs on unstable rating curves by varying γ from 2 to 10 with steps of two units to represent the effects of eventual urbanization in the watershed 3 5 4 v notch and rectangular weir in this sub section we test the effect of varying roughness for inbank and overbank conditions into unsteady rating curve modeling data assumed for this sub section are the same as the previous subsection except for q p and q b which were assumed as 2 of the values from the previous section as mentioned we model the channel with two roughness coefficients using the divided channel method the assumed roughness coefficients for inbank and overbanks were 0 020 and 0 035 respectively 3 5 5 upper negro river stage discharge curves compared to observed data the algorithms developed in this paper are applied to a cross section of the upper negro river located in the state of amazonas brazil see fig 11 the negro river is the seventh largest river in the world in terms of volume and is the largest affluent of the amazon river the largest in the world in terms of volume observed data of the negro river were retrieved from the agência nacional de águas ana database ana 2022 for the stream gauge code of 14330000 post processing in raw data from the ana database was performed in hidroapp available at https www labhidro ufsc br hidroapp souza et al 2021 the cross sections tested in this section are shown in fig 12 only years with consistent data were used in this analysis water surface flow slopes at this station vary monthly with the highest variations in september furthermore the slopes vary throughout the river so that the average slope value for the upper negro river can be assumed to be s 0 8 cm km marinho 2021 moreover manning s roughness coefficient varies with the depth of the water surface alves et al 2017 however in this section we fixed a single roughness and slope for the sake of simplicity and to test the model s ability to predict normal flows and their occurrence compared to the observed data we assume n 0 042 a value in accordance with previous estimates found in alves et al 2017 cross sectional data were only available for years after 1993 as a result the invert elevation of observed flow data from 1976 to 1999 was assumed to be the same as the last cross section observation in 1993 6 97 m 4 results and discussion 4 1 numerical case study 1 4 1 1 normal flow in complex regular cross sections the results of the model application for triangular parabolic semi elliptical semi parabolic cross sections are presented in fig 6 the errors within the developed model and the noaa normal depth solver are shown in fig 7 the hp values for y max are shown in table 1 all relative errors of the developed model and the noaa normal depth solver fault within a 10 difference as presented in fig 7 for cross sections t p and ep errors were below 3 for discharge area perimeter and top width and are almost negligible for velocity all hp curves are continuous and increase monotonically with y more specifically although being relatively complex shapes i e semi elliptical semi parabolic one could derive analytical expressions for perimeter area centroid and others to solve sve without requiring entering of hp values from tables examples of applications of sve with complex but closed form geometrical equations can be found in simões et al 2017 4 2 numerical case study 2 4 2 1 normal flow modeling in composite cross sections cross sections b c from fig 1 i e p ep results are also presented in fig 6 and normal flow errors are shown in fig 7 the errors were approximately negligible compared to the noaa solver and are mainly due to the introduction of noise σ into the x and y coordinates although the stage discharge relationship of sections p and ep was developed a very fine numerical discretization in x and y was required for an accurate estimate of hp for these sections we discretized y into 1 cm depth steps so that to define a 2 meter depth parabola and semi ellipse semi parabola cross sections we entered 400 coordinates in the model in addition in the model calculations we assumed σ 1 1 0 3 cm which means 1000 points each 1 cm ultimately to properly capture the right hp in highly detailed cross sections one has to define proper numerical discretizations to avoid instabilities the greatest errors occurred for the gabion g and road gutter gu cross sections these errors occurred mainly due to the introduction of noise σ from eq 1 and due to the conceptualization of the scm compared to the solution used in the noaa solver dcm 4 3 numerical case study 3 4 3 1 normal flow modeling in irregular cross sections we can see in fig 7 that the normal flow error was below 2 even with different model conceptualizations from the noaa solver and the model developed here however discontinuities occurred in geometrical functions such as area and perimeter as shown in fig 6 these sharp changes occurred when the flow diverted from the main channel to overbanks the conceptualization of the scm model is a depth based model assuming that the flow first propagates in the bank and after reaching the depth of the main channel y m see fig 10 the flow immediately diverts to the overbanks with the same elevation of the water surface elevation in reality more sophisticated software such as hec ras for example can successfully model cross section ineffective flow areas that could account for effective overbank flows brunner 2016 4 3 2 uncertainty in steady rating curves for inbank and overbank depths the analytical determination of stage conveyance figures for the irregular cross section g in fig 3 is shown in fig 9 two different behaviors were observed in the stage conveyance modeling first for inbank conditions flow is very similar to simpler cases of river rating curves as presented in westerberg et al 2011 and perumal et al 2007 for the same design conveyance water depths can have approximately 0 4 m difference for different scenarios of stage roughness as expected the scenario with more hydraulic capacity was scenario i which assumed a single manning s coefficient throughout the whole cross section on the contrary scenario iii i e stage variation of 2 orders in roughness was the scenario with a lower hydraulic capacity i e k part b of fig 9 shows the stage conveyance i e q s 0 after reaching y m and the results show that 100 relative differences can occur when comparing different methods such as scenario i and scenario iii for the same depth these results show the sensitivity of roughness coefficients in the modeling of rating curves and the importance of proper estimation of this parameter to accurately estimate flow discharges the monte carlo analysis shows how various can be the conveyance and the at station roughness coefficients assuming typical uncertainty in roughness estimation with μ 30 and σ n 0 01 as presented in part c of fig 9 part b of fig 9 shows the plots of k for scenarios 2 3 and 4 compared to k for scenario 1 i e the baseline scenario generally river cross sections are assumed with constant roughness coefficients and the results of this graph show that most of the time comparisons within k 1 and k i are below the 45 line indicating that assuming a single roughness coefficient could be an overestimation of the hydraulic capacity of the cross section up to 30 in inbank conditions and up to 100 in large overbank conditions however these results are obtained from a cross section with overbanks of approximately 400 m and the main channel of nearly 10 of the overbanks 4 4 numerical case study 4 4 4 1 scm x dcm methods the scm and dcm differ specifically in two criteria dcm herein considers an introduction of a shear stress acting at the interface between the overbanks and the main channel and considers an equivalent roughness that accounts for the divided conveyances the scm however considers a single section without the assumption of an increased perimeter in inbank overbank interfaces to this end the differences in this method are most observed in cases where this new perimeter is comparable to the scm perimeter increased by the overbank perimeter this result can be observed in table 1 where re was only 3 for the irregular cross section i e y f 0 4 m b m 32 72 m and p y max 452 44 m but was 40 for the v notch rectangular cross section the results of this section indicate that the scm and dcm methods are more agreeable for sections with large overbanks widths and low overbank depths furthermore they are the same for single sections for complex sections as shown in table 1 for sections t p and ep 4 5 numerical case study 5 4 5 1 non breaking wave comparing analytical solution with hydrohp 1d and hec ras the modeling results are presented in fig 13 and shows the water surface elevation for different durations varying from 12 min to 60 min it is noted that neither hec ras 1d model nor hydrohp 1d model should match the analytical solution the analytical solution is a simplification in 1d swe to consider only diffusive effects therefore in the interface between wet and dry sections hec ras 1d and hydrohp 1d should disagree with the analytical solution especially because inertial effects become to play important roles in these cases similar results of the ones presented in fig 13 are shown in bates et al 2010 hunter et al 2005 and dottori and todini 2011 indicating the inertial effects are responsible for changing the arrival time of the flood wave in the dry frontier compared to a fully diffusive like solution given by eq 20 hydrohp 1d and hec ras 1d model have slightly in the results of this problem due to differences related to mass conservation routines and numerical schemes used in both models 4 5 2 unsteady state inflow hydrograph and normal flow at the outlet the results of the simulation of this numerical study are presented in fig 14 the hydrohp 1d presented similar results with the hec ras 1d implicit model with root mean square errors rmse of flows and discharges of 0 041 m 3 s and 0 053 m respectively as shown in fig 14a b moreover the coefficient of determination r 2 for discharges and water surface elevations are 0 997 and 0 984 the largest differences between hec ras 1d and hydrohp 1d occurred in the inlet and in the falling limb of the hydrograph other studies indicate that the lax friedrichs method can present numerical issues in the boundaries akan and iyer 2021 kurganov 2018 and this could be one of the reasons for this difference between the two models furthermore since the discretization used for the solution of the partial differential equations is of first order accuracy numerical diffusivity errors can propagate even with the relatively fine mesh assumed i e δ x 11 09 m δ t min 0 5 s kurganov 2018 although some differences are found between both models the water surface elevation profiles shown in fig 14c i show the relatively small absolute differences between hec ras 1d and hydrohp 1d indicating that the model can predict not only discharges but also water surface elevations 4 5 3 hysteresis effect in rating curve modeling we compare results from steady and unsteady stage discharge modeling for the irregular cross section the little washita river in fig 3 g the results indicate that the average errors within normal flows and flows modeled with a hydrodynamic model are up to 50 e g see the stage at 2 5 m as presented in fig 15 the results of this analysis are similar to the results shown in muste and lee 2013 differences in the rising limb of the hydrograph are relatively negligible for first inbank conditions see part b at the stage of 3 m approximately however after reaching it a sharp change in the flow discharge occurs due to the increased flow area from the second inbank region the differences between the hydrodynamic cases of γ begin to increase after this stage although they are getting closer to the normal flow towards the maximum stage that indicates that local acceleration played a more important role at lower stages the falling limb of the hydrograph shows a similar trend with smaller flows for the highest γ s the results of this section can serve as a guide to model unsteady rating curves in the river and to assess the variation of complex rating curves variation for inbank conditions holmes 2016 the results of fig 16 show the results of the hydrodynamic simulation of the vr section of fig 3 with different roughness coefficients for inbank and overbank areas as shown in this figure the flow discharge can increase to 60 in the rising limb of the hydrograph while it can decrease to 25 in the falling limb this is particularly important when using v notch weirs with a known material rugosity with walls and banks retrofitted with rougher materials such as rocks and local vegetation stage discharge in weirs can carry large errors depending on the hydrograph properties of the inflow as shown in fig 16 in addition the role of changing the roughness of the overbanks is shown at the flow discharge rate of change after the stage of 1 m surprisingly even when the roughness of the overbanks increases the average roughness coefficient for the entire section using eq 12f rapidly decreases after the stage of 1 m and starts to increase reaching a value of 0 025 in the maximum stage this reduction is due to the rapid increase in the perimeter so that the average roughness coefficient would have to decrease to simulate the computation of the total conveyance as followed by eq 5b 4 5 4 normal flow rating curve modeling in upper negro river cross section dynamic changes over time have not played the most important role in the modeling of the rating curve except for 2008 as shown in fig 17 the invert elevation of the cross section of this year is 7 66 m whereas the invert elevations of the other years are 10 07 m 0 77 m which explains the difference in y for the same flow discharge however we can visually infer from fig 12 that the cross sections within years 2007 to 2009 have not dramatically changed except for the invert the difference in the invert elevation might be due to sediment deposition de almeida et al 2016 the daily hydrograph shown in fig 17 b shows average daily hydrographs the values shown in this plot are calculated as follows first the average of two stage measurements is calculated therefore instantaneous peaks are generally smoothed following this a fitted rating curve from ana is used to convert stage to flow resulting in this chart the values of flow represent daily averages as mentioned above whereas the plotted values of observations in fig 17 a are instantaneous observations in other words we can infer that observations are always upper bounded by daily average maximum observations since maximum observations fall mostly below 2 1 0 4 m 3 s while flow discharges have values above this threshold almost every year it indicates that the measured data for flow discharges in this cross section fail to capture the largest flows which can impact statistically based rating curves derived from linear regression methods souza et al 2021 clarke et al 2000 the looped behavior found in the observed data from 1976 1999 could be from two main reasons hysteresis effects errors in the assumed invert elevation dynamic change over the period due to deposition erosion and dunes formation or due to changes in manning s coefficient over time moreover a systematic uncertainty is observed when comparing the model with observations the model developed here assumes y max as the maximum value within the left and right directions from the invert break point therefore the flow is not necessarily calculated for the maximum cross section data z see fig 12 however observations of flows are found to be higher than these levels as shown in fig 17 values found at these stages are not modeled but can also be the result of hysteresis effects 4 6 strengths and weaknesses of the presented model the set of algorithms developed in this paper can be used to solve different types of hydraulic problems varying from regular concrete channels to complex shapes or irregular cross sections with overbanks and depth varying roughness some strengths and weaknesses of the model are presented as follows 4 6 1 strengths flexibility to simulate all kinds of single cross sections or composite sections with two overbanks simulate the role of stage varying roughness simulate the role of at station roughness variation assess the uncertainty in the estimation of rating curves by deploying monte carlo analysis no need to define an iterative problem to solve manning s equation as required by other models us department of commerce 2016 brunner 2016 either by performing polynomial approximations for cross section segments or by defining linear interpolations relatively fast computations and the possibility of parallelization for normal flow estimations for large scale cases simulate the hysteresis effect in unsteady rating curve modeling possibility to couple hydrohp 1d with land surface models or hydrological models due to the open source codes 4 6 2 weaknesses time invariant hydraulic parameters that could be conceptualized as time varying parameters to consider riverine changes due to erosion and deposition moreover this tool does not allow simulation of sediment laden flows that can contribute to time varying changes during flow propagation the model did not explain cases with more than 2 overbanks e g the case of the gabion channel is shown in fig 1 with results presented in fig 10 and fig 6 during the rising limb of the hydrograph in a cross section with overbanks flow is confined first to the inbank and later propagated to the overbanks after the level reaches the overbank level however this might not always be the case e g hec ras considers that flow can go for the overbanks even when the level has not reached the overbank level the change in the cross section by adding a noise σ can be important for channels with small dimensions e g road gutter cross section due to being significant in the calculations of hp moreover the proper definition of σ is required for simulating detailed cross sections as the parabolic and semi elliptical semi parabolic cross sections therefore more points and more computational efforts are required to simulate these aforementioned sections with accurate results the dcm method applied in this paper although it solves the problem of non monotonic conveyance curves by adding overbank and inbank conveyances due to the correction of n in eq 12e still carries discontinuities when sharp areas and perimeters are added in inbank conditions overall the modeling of the rating curve with fixed parameters for roughness and slope showed good results in the upper rio negro the model was able to predict flows within the cross section range for most cases some outliers are found and could be explained by different friction slopes or roughness coefficients however defining the proper stage roughness functions or the friction slopes is difficult and would require knowing a proper inflow hydrograph which is typically unavailable for resolutions higher than 1 day in brazil 5 conclusions and future work the development of two algorithms for hp estimations in regular composite and irregular cross sections capable of simulating stage varying roughness and at station varying roughness variation is performed the results of model validation indicate predictions of normal flows within average relative errors of approximately 5 when compared to an established normal depth solver moreover hydrohp 1d results are consistent with hec ras 1d model not only for a wave propagation condition but also for unsteady state simulation with inflow hydrograph at the inlet and normal slope at the outlet from the results we draw the following conclusions the developed algorithms can represent normal flow conditions for complex cases and irregular cross sections with the single method channel and divided method channel the greater differences within scm and dcm methods occurred for v notch cross sections and road gutter cross sections these are sections of relatively large overbanks compared to the cross section height only a 3 difference between scm and dcm occurred for the maximum water height in the irregular cross section indicating that flow discharges at the maximum stage for irregular channels with relatively large overbanks i e b m b f 0 1 where b f is the overbank width assuming maximum levels both methods are more comparable assuming feasible scenarios of stage roughness in an irregular channel inbank stage discharge for the same discharge can have a stage variation of 0 40 m for overbank stage discharge however this difference is typically smaller 0 3 m additionally for the same water depth k can increase to 100 indicating that the flow is highly influenced by the overbank areas and roughness the unsteady rating curves modeled in this paper for an irregular cross section indicated an increase of 50 and a decrease of 25 for the rising and falling limb of the hydrographs similar results were found for a composite v notch rectangular weir where errors between normal flow and unsteady flow would be up to 60 and 25 for the rising and falling limb of the hydrographs results of comparing hydrohp 1d with hec ras 1d in an open channel indicate that both models agree with r 2 0 97 and r 2 0 984 for discharge and water surface elevation and rmse 0 041 m 3 s and rmse 0 053 m for these same variables normal flow discharges in the negro river assuming a constant manning s roughness coefficient and bottom slope show good results without requiring calibrations or statistical analyses the results show that even with these assumptions the developed model can accurately predict stage discharge in real world stream gauges future application of the developed model will be in estimating rating curves for ungauged rivers in brazil where data in suitable resolution are sometimes available but little information regarding flow discharges is displayed furthermore we want to exploit the role of shear stress in more fundamental ways than by assuming a linear increase in the hydraulic perimeter on the main channel moreover future versions will allow entering different cross sections and interpolating them instead of simulating a single cross section in addition sediment laden flows and a movable overbed channel can be explored to explain dynamic changes on stage discharge during monitored events in remote sensed areas finally hydrohp 1d can be linked to land surface models allowing to simulate the impacts in flood routing of rivers and channels due to hydrologic regime alteration moreover hydrohp 1d can be used to quantify predict and scenarize the effects of extreme events of floods and droughts due to climate change in riverine networks credit authorship contribution statement marcus n gomes jr conceptualization methodology software validation formal analysis investigation data curation writing original draft writing review editing visualization luis m c rápalo writing review editing data curation resources paulo t s oliveira writing review editing marcio h giacomoni writing review editing funding acquisition visualization resources césar a f do lago writing review editing eduardo m mendiondo writing review editing funding acquisition visualization resources declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the authors appreciate the support from coordenação de aperfeiçoamento de pessoal de nível superior capes for funding this research this work was finantially supported by fapesp 22 08468 0 flash drought event evolution characteristics and the response mechanism to climate change the authors also appreciate the effort made by eng mateo hernándes sánchez who gently helped with the hec ras modeling appendix a supplementary data supplementary data related to this article can be found at https github com marcusnobrega eng hydrohp 1d the supplementary material have fully descriptions of hydrohp 1d input data sec i a data treatment from ana sec i b mathematical treatment at domain boundaries sec i c algorithm 2 for hp estimation sec i d and matlab codes of i hp estimator sec i e1 ii read input data for sve model sec i e2 iii sve model sec i e3 iv post processing scripts sec i e4 v video generator of sve states for irregular sections sec i e5 vi video generator of sve states for regular sections sec i 6 and vii detailed output algorithm sec i e7 supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2023 105733 appendix a supplementary data the following is the supplementary material related to this article mmc s1 
25376,the stage discharge relationship is affected by hysteresis especially in 1d river cross sections with very mild slopes and or where backwater effects occur to model these cases hydraulic property hp estimations e g hydraulic radius and conveyance are usually required as closed form functions of stage however these are typically unavailable for complex cross sections with overbanks for no sediment laden flows we created the hydrohp 1d tool to solve the 1d saint venant equations aided by the cross section hp stored in tables it also simulates irregular and composite hydraulic cross sections allows hydrodynamic simulation using the single channel method scm and proposes the divided channel method dcm for steady and unsteady flow analysis a wide range of applications in rivers such as the little washita river and the upper negro river and comparisons with noaa normal depth solver hec ras and with an analytical solution shows a promising scenario of applying hydrohp 1d to estimate 1d steady and unsteady hydrodynamics graphical abstract keywords saint venant equations unsteady rating curve hydrodynamic modeling divided channel method hysteresis channel routing data availability algorithms and data used are available in an open repository at https github com marcusnobrega eng hydrohp software availability name of software hydrohp 1d developer marcus nóbrega gomes júnior contact address department of hydraulics and sanitation university of são paulo são carlos school of engineering av trab são carlense 400 centro são carlos 13566 590 email marcusnobrega engcivil gmail com software required matlab version 2021a or higher microsoft excel 2013 or higher programming language matlab program size approximately 5 mb availability open source github license available in gomes jr 2022 1 introduction stage discharge relationships also referred to as rating curves are one of the main tools for hydraulic engineering these tools are dependent on cross section geometry longitudinal slope friction and shear stress variations two forms of rating curve derivation are typically performed biunivocal steady rating curves and looped unsteady rating curves the latter is derived from the application of newton s 2nd law where two dimensional space dimensions are orders of magnitude larger than vertical dimensions in this case the shallow water equations swe are derived from coupling mass balance friction and conservation of momentum in a partial differential system of equations usually numerically solved this type of modeling is also known as dynamic wave and is used in software as the storm water management model rossman et al 2010 for steady flow modeling however we simplify the shallow water dynamics by assuming the friction slope as the bottom slope and we apply a resistance equation such as manning s or darcy weisbach equations to compute friction losses in this case we assume a kinematic wave flood wave approximation in swe swe are usually applied to estimate the hydraulic flow behavior on one dimensional 1d channels and two dimensional 2d floodplain dynamics focusing on the 1d modeling the saint venant equations sve are the special case when the flow is assumed to be unidimensional with the same longitudinal velocity throughout the whole cross section in other words vertical acceleration is considered negligible these equations form a set of hyperbolic partial differential equations derived from 3d navier stokes equations applicable to solving the conservation of mass and momentum in channels the equations are versatile to represent flows at atmospheric pressure full pressurized pipe flows with the preissmann slot concept cunge 1980 and also sub atmospheric full pipe flows if adjustments are made in sve vasconcelos et al 2006 in most cases however no analytical solution is available especially for complex cases such as irregular cross sections therefore numerical methods are typically applied to solve the sve strelkoff 1970 zarmehi et al 2011 roohi et al 2020 chen et al 2013 in these cases classic method approaches use closed form equations representing hydraulic functions such as wetted area wetted perimeter channel s top width and the relative centroid position to the surface in terms of water surface depth these functions are functions of water surface depth and are relatively simple to derive for common regular shapes such as rectangular asymmetric trapezoid triangular or parabolic channels simões et al 2017 however natural rivers are generally defined by irregular geometry and closed form equations of hydraulic properties hp are not available as shown in farina et al 2015 and gleason 2015 abrupt increases in perimeters cause discontinuities in hydraulic radius functions that ultimately cause sharp variations in the rating curve in addition channels are generally conceptualized with a constant gauckler manning strickler coefficient here simply denoted as manning s coefficient for the entire main channel and sometimes with different values for overbanks petikas et al 2020 however manning s coefficient is an empirical representation of friction at the cross section perimeter representing the friction properties of the interface between the flow and the cross section therefore it probably changes with water surface depth te chow 2010 manning s equation was derived for fully turbulent steady uniform flow at normal depth with mild slopes which is the typical conditions of river flooding chanson 2004 although software such as the hydrologic engineering center river analysis system hec ras brunner 2016 and the stormwater management model swmm rossman et al 2010 have 1d full momentum solvers their hp algorithms to determine hydraulic properties are not explicitly specified in their manuals this poses a drawback for new modeling developments using other software packages such as matlab or python we overcome these issues here by developing two different algorithms to estimate hp in regular irregular and compound cross section channels and pinpointing various applications of the algorithms developed for steady and unsteady rating curve estimations the following section assesses the literature review on the aforementioned topics the tool developed in this paper is easy to use and can be used for teaching and educational purposes 1 1 literature review since the beginning of the 20th century the literature has shown that applications of stage discharge relationships are extensive the research carried out in garbrecht 1990 advanced the analytical definition of irregular cross section hydraulic functions using coupled power fitness functions for the main channel and overbank areas these functions were fitted using elevation and station data for each breakpoint in the surveyed channel the author created a methodology to derive these functions considering different roughnesses in the main channel and at the left and right overbanks similarly dingman 2007 derived analytical at station hp for more generalized cases using not only manning s equation but also chezy s another method for estimating hp in rivers using polynomial functions is found in hrafnkelsson et al 2021 instead of creating power law functions with constant exponents the authors proposed a depth varying term in the standard rating curve function to fit better river changing dynamics over time a significant model consideration in rating curve estimations is how friction is modeled the application in abril and knight 2004 shows a depth averaged model using the darcy weisbach friction model applied in a depth averaged unsteady model to determine unsteady rating curves in regular sections another rating curve method based on the conservation of the mass and momentum equations is described in dottori et al 2009 there is no consensus on which friction model should be applied for open channel rating curve models however manning s friction model is more common probably due to its simplicity and number of studies the propagation of the hydrograph in the channel length causes energy losses flow attenuation i e diffusive effects and flow delay i e kinematic effects in addition to these effects convective and local acceleration change the relationship between stage and discharge the rising and falling limb of the hydrograph converts a single biunivocal rating curve into a looped rating curve due to hysteresis petersen øverleir 2006 perret et al 2022 wolfs and willems 2014 the methodology presented in dottori et al 2009 is related to this issue and is particularly important in channels where local and convective acceleration play an important role i e channels with very mild slopes and large friction when using data to compare modeling results with observations one of the main problems is the lack of observation data for large flows especially because observations of low frequency flows that is relatively large flows are rare lang et al 2010 and difficult to obtain this poses the hydraulic modeling of rating curves as an important tool to determine the probabilities of exceedance of flows and depths that could lead to downstream flood damage moncoulon et al 2014 however when data are available on a large scale data driven algorithms can also be used to predict stage discharge in streams as shown in muste et al 2022 determining rating curves based on hydraulic models in river cross sections usually require detailed bathymetric data this gives rise to a drawback for ungauged rivers with neither flow observations to empirically fit a rating curve manfreda 2018 nor detailed cross section data zheng et al 2018 to model flow discharge relationships to this end the study conducted in zheng et al 2018 developed a continental scale flood mapping data set based on a 1 3 arc second 10 m grid cross section data derived directly from digital elevation models provided by usgs their method based on manning s uniform flow equation was comparable to products derived from the federal emergency management agency fema with substantially fewer data required compared to full momentum 2d models while presenting reasonable results in terms of the hydraulic limitations of various approaches to determine steady rating curves in open channels studies as presented in hosseini 2004 stephenson and kolovopoulos 1990 fernandes 2021a and khodashenas and paquier 1999 show that the interface within the main channel and the overbanks have shear stresses that are not considered when dividing the cross section into homogeneous parts in contrast when considering a single section method with a single average velocity for the inbank and overbanks discharge passes through discontinuities when flow depth reaches overbank levels with a relatively low hydraulic radius sahu et al 2011 following these cases two main alternatives have been implemented to determine normal discharges in channels in most hydraulic engineering procedures hosseini 2004 a dividing the channel into theoretically homogeneous sections brunner 2016 each with its own manning s roughness coefficient changing in terms of the stage or for each surveyed cross section break point moreover manning s coefficient could be considered constant changing only for in bank and over banks b calculating hydraulic properties as a single cross section scm the manning s roughness coefficient can vary for each break point with stage or assumed constant for inbank and overbank areas the fundamental difference between procedures a and b is the way the velocity is considered in a the velocity can be different for the inbank overbanks or even when the roughness changes under the inbank conditions therefore the flow is calculated as an algebraic sum of each flow in each discretized area of the cross section in procedure b however velocity is assumed uniform for the whole cross section this assumption implies that overbank areas typically with large perimeters and friction would have the same velocity as the inbank areas which might not always be a good assumption a more detailed theoretical alternative was developed in hosseini 2004 by the introduction of a coherence factor into modeled conveyances with cases a and b to correct velocities in the overbanks and the main channel however in general hydraulic engineering calculations which are often performed by engineers using pieces of software such as hec ras mainly focus on solutions using the aforementioned alternatives a or b shear stress occurs when a compound channel exchanges flow from the main channel to the overbanks and a momentum transfer hua et al 2007 occurs as mentioned the velocity in overbank areas is generally lower than in the main channel due to relatively high roughness coefficients in overbanks and relatively larger perimeters in these cases the energy and momentum equations can be corrected by the introduction of coriolis i e energy correction and boussinesq i e momentum correction coefficients however accurate determination of them is a taunting task as shown by the research conducted in yang et al 2018 the aforementioned literature shows different methods applications and correction factors but no consensus is found when modeling relatively common problems such as determining the normal discharge in a compound or irregular channel exploring these issues and providing more guidance related to uncertainty in different model conceptualizations is needed in this article we pinpoint their main differences the uncertainty in the rating curves has not been shown to be negligible in several studies as presented in domeneghetti et al 2012 ghanghas et al 2022 kuczera 1996 and clarke et al 2000 we want to explore these issues and provide more guidance related to uncertainty in different model conceptualizations and pinpoint their differences in our case studies the methods proposed herein can be used in various applications such as determining design rating curves in ungauged rivers aid in the design of 1d hydraulic works as channels gutters or low impact development facilities as vegetated swales especially in poorly gauged watersheds where stream gauges are typically unavailable this methodology can be a tool aiding planning and management of floods nonetheless even when stream gauges are available the correct measurement of high level flows is a taunting task because of the typical high turbulence and measurement noise in these cases the algorithms are developed in matlab and cross sectional information is input into the model in a xlsx file with all codes open source the model can be easily associated with hydrological models making the hydrohp model adaptable to assess the impacts of climate change in floods for instance the results of hydrohp 1d are displayed graphically in tables and printed as pdf automatically 1 2 paper objectives and contributions despite experimental and modeling research attached to the problem of determining flow rates in regular and irregular cross sections we observe from these aforementioned studies a lack of generalized algorithms to estimate hp in rivers of composite regular and irregular sections studies presented in hosseini 2004 fernandes 2021a make clear the impact of shear stress in compound regular cross sections however little has been researched for modeling more complex regular and irregular cross sections with single stage roughness or depth varying roughness coefficients the specific objectives of this study are described as follows develop novel generalized algorithms to determine hp for regular and irregular cross sections algorithm 2 focuses on the plane geometry while algorithm 2 available in the supplemental material focuses on the finite element method assess the uncertainty in modeled based rating curve estimations assess the role of manning s roughness coefficient in conveyance estimation develop a modified scm equivalent to the dcm model coupled with additional shear stresses at the interface within the overbanks and the main channel develop a method for estimating dynamic rating curves using a full momentum hydraulic model compare the dynamical rating curve estimations with the normal flow rating curve estimations the remainder of the paper is organized as follows section 2 develops the mathematical model used in the paper describing the hydraulic property functions used in the algorithms developed and detailing the governing equations in addition this section explains the methods used to assess rating curve uncertainty and the methods used to evaluate dynamical rating curves by hydraulic modeling next section 3 describes each numerical case study in which the methods were applied in this paper following the case studies section 4 shows the results of all numerical case studies and discusses their main implications as well as the strengths and weaknesses of the model in section 4 6 section 5 discusses the conclusions and future work finally the supplemental material details the governing equations and pseudocode for algorithm 2 data acquisition and cross section determination for synthetic cross sections tested in this paper the paper notation for this paper is introduced next paper s notation italicized boldface upper and lower case characters represent matrices and column vectors a is a scalar a is a vector and a is a matrix italicized regular upper and lower case characters represent scalars q is a scalar q is also a scalar the notation r denote a set of real numbers the notations n and n denote the set of natural and positive natural numbers a normally distributed random number with average μ and variance σ n 2 is notated by n μ σ n 2 given a vector x r n the notation x i j with i and j n represents a cut in x from ith to jth entries the notations r n and r m n denote a column vector with n elements and an m by n matrix in r 2 material and methods the process of estimating the hp of irregular cross sections although has been addressed by research conducted in garbrecht 1990 and dingman 2007 still lacks a general algorithm procedure capable of working in irregular regular and compound cross sections in this paper we solve this issue by providing two types of algorithms to determine hp assuming surveyed cross sectional data at different stations the algorithms and data used in this article are available in an open repository at gomes jr 2022 section 2 8 details governing equations valid for algorithm 1 whereas the supplemental material explains the same for algorithm 2 the following sub sections apply to both algorithms 2 1 cross section data here we follow the left right convention for the cross section data input data is organized in terms of breakpoints each breakpoint has its x and y coordinates manning s roughness coefficient is defined in three ways i it can be entered for each break point ii for each stage or iii a constant value for inbank and overbank the method used in the model can also be chosen i e scm or dcm the friction slope i e typically assumed as the bottom slope in normal flow rating curves is also entered all these aforementioned data are summarized in a xlsx file which is read in the matlab codes entries of elevation and horizontal distance within stations as well as manning s roughness coefficients within stations are the data required to simulate the irregular cross sectional shapes details of how to use the model and how to enter the input data can be found in the supplemental material moreover all the code is also included in detail subsequently we derive flow areas wetted perimeters hydraulic radius section factor ϕ conveyance k and interpolate cross section coordinates assuming linear relationships within known breakpoint data a schematic of a typical river cross section is presented in fig 1 where we detail the rapid increases in area and perimeter due to polygons formed for specific water surface depths 2 2 unique elevation values to avoid points with the same elevation or with the same station and to be able to treat all cross sections as irregular cross sections we define a tiny number σ 1 1 0 4 m and replace elevations and station values as follows 1a z i z i σ z i 1 z i i 1 2 n s t 1b x i x i σ x i 1 x i i 1 2 n s t where n s is the number of stations measured this allows us to use trapezoid formulas for determining areas and perimeters see fig 2 and to identify horizontal or vertical segments by calculating coordinate differences such that segments with a particular depth and station difference smaller than a threshold would be considered horizontal and or vertical segments we detail these cases in more detail in this section in 2 8 1 2 3 hydraulic radius once the area and perimeter are calculated for each segment the hydraulic radius can be calculated as the ratio within the flow area and its correspondent perimeter sturm 2021 such that 2 r h i a i p i i 1 2 n v t where n v is the number of vertical steps a i is the flow area p i is the perimeter and i is the i th vertical segment taken from the invert 2 4 section factor ϕ the section factor ϕ is given by sturm 2021 3 ϕ i a i r h i 2 3 i 1 2 n v t the previous equation can also be equal to q s 0 for uniform flow which indicates a biunivocal relationship of cross section properties with normal flow if ϕ increases monotonically sturm 2021 2 5 representative manning s coefficient for single channel method the values of n can change in terms of the surface roughness conditions for example in the overbanks the presence of vegetation leads to a reduced flow velocity compared to that in the main channel fernandes 2021b kidson et al 2006 we assume herein that the representative manning s coefficient for a cross section in terms of water surface elevation depends on each value of n for each sub segment if we are modeling using the single section method more specifically we assume that n changes with a non linear function of the perimeter p based on the hypothesis that the total cross sectional mean velocity is equal to the subarea mean velocity einstein 1934 brunner 2016 horton 1933 such that 4 n i j 1 i n l j 3 2 p l j n r j 3 2 p r j p i 2 3 where n l and n r are the manning s coefficients for left and right directions from the invert assuming piecewise continuous functions of the given n derived from surveyed data for each sub segment similarly p l and p r are the perimeters relative to the left and right slopes of the invert and p i are the calculated total perimeter of a single section further described in eq 10 2 6 conveyance factor the conveyance factor is derived with manning s roughness coefficient and ϕ the main hypothesis of manning s equation is that average velocity is constant throughout the whole cross section this is approximately the case when we model the cross section with a constant roughness coefficient with no discontinuities in hydraulic properties however two problems can occur first if ϕ is not a monotonic increasing function and if n increases the velocity might decrease so that the flow in a further section would be smaller than that of a section with lower depth this is the case for circular pipes sturm 2021 however this condition is rare in open irregular channels generally the function ϕ can have discontinuities and rapid changes when the flow reaches the overbank areas in these cases we can separate the channel and overbanks conveyances the method used to calculate the conveyance varies with different types of software such as hec ras brunner 2016 and mike 1d such that hec ras separates the main channel and the overbanks here we consider both methods the single cross section method scm and the divided cross section method dcm into conveyance calculations al khatib et al 2012 such that 5a k scm i 1 n i ϕ i i 1 2 n v t 5b k dcm i k m i k f i where k m and k f are the main channel and the overbank conveyances 2 7 centroid distance assuming a constant water density the vertical centroid coordinate taken from the invert elevation can be derived using the first moment of area as presented in eq 6 to solve the integral we assume a moving trapezoid from i to i 1 given by 6 y i 1 a i 0 i δ y a y d y j 1 i a j a j 1 y j y j 1 2 a i where δ y is the vertical discretization 2 8 algorithm 1 geometrical procedure with a while loops 2 8 1 cross section lateral angles in this subsection we show how to derive a generalized algorithm to estimate hydraulic properties in regular composite or irregular cross sections the differences between algorithms 1 and 2 are the first is based on plane geometry whereas the second is based on the finite element method the water surface elevation is assumed with no slope in the cross section and the longitudinal slope is constant throughout the whole section therefore we can use plane geometry to determine the areas and perimeters for a given water depth from the invert for each depth y evaluated we calculate the left and right tangents by defining vectors that represent the values of the breakpoints of the left l and right r of the invert let x l y l x r and y r be the coordinates left and right of invert for a y i i δ y we find the coordinates on the left and right larger i or smaller i than y i resulting in the following tangents 7a α l i y r i y r i x l i x l i 7b α r i y l i y l i x r i x r i the previous formulae are only valid in inclined segments i e lateral slopes are not 0 or π 2 considering the coordinates of two consecutive breakpoints 4 alternatives can occur we classify them as i vertical segment ii horizontal segment iii vertical and horizontal segment and iv regular segment this classification is derived from the vertical and horizontal distances within consecutive breakpoints such that if the differences are equal σ then a vertical and or horizontal segment is identified these cases are treated in the main code with a simple if statements 2 8 2 top width geometry assuming a finite y discretization δ y the increase of the top width b can be calculated with the angle defined in eq 7 however in the cases where a drastic change in x occurs from i to i 1 we can calculate b i 1 as 8 b i 1 b i δ b l i 1 δ b r i 1 δ y 1 α r i 1 α l i where δ b l and δ b r are the additional values of b that must be added from the left and right directions of the invert y axis to consider horizontal changes in areas without slopes 2 8 3 flow area calculations the basic idea for the area is to calculate trapezoid areas with bases b i 1 and b i and depths δ y however similar to the increase in top width mentioned in eq 8 the area in section i 1 can also have a drastic increase this is the case shown in fig 1 in the parts where the flow is retained within a surface in these cases the area calculation has to consider the polygon area formed by the surface generated from left and right overbanks such that 9 a i 1 b i 1 b i δ y 2 a t a i δ a l i δ a r i where δ a is the increase in area from discontinuities derived from the channel section from left and right directions calculated as a polygon area and a t is the area of the incremental trapezoid from i to i 1 2 8 4 perimeter in terms of water surface depth the cross section perimeter is a monotonic function in terms of y in parallel with calculations of a and b the perimeter function might also have discontinuities for example these sharp changes in the perimeter can also be due to water reaching overbanks or increasing horizontal distances due to the deposition of sand in the main channels the perimeter can be calculated in terms of lateral distances within the cross sections from incremental perimeters from overbanks and the base at the invert these distances can be calculated using the angles defined in eq 7 therefore we can write the perimeter functions as follows 10 p i 1 p i δ y 1 sin arctan α l 1 sin arctan α r f α δ p l i δ p r i as shown in the previous equation the perimeter function could have issues for undefined values of α to this end we treat these cases with if statement conditions in the algorithm identifying these points and assuming that f α 1 2 9 single channel method scm in this method there is no distinction between overbank and inbank flow conditions an example of a cross section where this hypothesis would be applicable is shown in fig 3 a b and c the stage roughness is determined in terms of the perimeter following eq 4 for inbank conditions this method is identical to the divided channel method described in the following section the scm method considers a stage conveyance curve that ultimately converts depth into flows such that 11 q i k i s 0 where q is the normal flow rate and s 0 is the bottom slope which is assumed as the friction slope for the normal flow estimates 2 10 divided channel method dcm herein let n fp n 0 1 2 t be the number of floodplains in a cross section as shown in fig 4 in this method the cross section is divided into main channel m and floodplains f the interface between the floodplains and the main channel introduces a new shear force that is considered by adding the wet perimeter in the main channel of a value equal to n fp y y m in the following subsection we define the governing equations to determine the hp for the dcm 2 10 1 correcting manning s coefficient a new system of equations is defined if we assume an additional shear stress acting in the interface between the main channel and the floodplains see fig 4 defining the break point divider we calculate the width of the main channel b m by extending the line segment from the divider to the opposite overbank hence the left and right sides of this line define the overbanks while inside of this segment towards the invert is considered the main channel in this paper we consider that the wet perimeter of the main channel increases by h f for each floodplain let n m be the final representative manning s coefficient for the main channel and n f be the representative manning s coefficient for the floodplains if we assume different velocities on the floodplain and the main channel we can estimate a new roughness coefficient n such that k scm k f k m in eq 5b resulting in eq 12f 12a y f y y m 12b a f a a m b m y f 12c p f p p m 12d p m p m n fp y f 12e a m a m b m y f 12f n a a p 2 3 1 n f a f a f p f 2 3 1 n c a m a m p m 2 3 where b m is the width of the main channel the variables a and p are determined by eqs 9 and 10 and the superscript represents values corrected for by shear stress induced by the interface within the main channel and floodplains this new roughness coefficient accounts for vertical division within the main channel and overbank roughness by including the increase in wet perimeter in the main channel in terms of h f while allowing the use of the total stage area and stage perimeter algorithms derived in section 2 8 therefore the normal flow can be calculated as follows 13 q dcm q scm 1 n a a p 2 3 k scm s 0 where q scm is the modified single section method to account for overbanks and k scm its respective conveyance 2 10 2 overbank areas and discontinuities in this model the elevation of the water surface is assumed to be continuous for the in bank and overbanks however in case the algorithm identifies discontinuities in cross section elevation i e elevation data are not monotonically increasing and find left and or right areas that are filled if water depth exceeds a threshold the model can generate two sets of polygons to represent these cases these polygons can increase areas and perimeters abruptly which may increase or decrease flow rates depending on the increased values let y d be the depth taken from the invert elevation that starts to have overbank flows see fig 1 if section i is such that y i y d the algorithm searches for left and right directions from the invert y axis to find breakpoints with depths smaller than y d if depths are found then a polygon can be defined and extra values of δ a δ p and δ b can be calculated from it in matlab this problem can be solved using the functions polyshape and polyarea 2 11 the 1d shallow water equations for the estimation of the 1d hydrodynamics we focus on solving the complete one dimensional saint venant equations gerbeau and perthame 2000 these equations are derived from the conservation of mass and momentum by applying newton s second law in a finite fluid element let d be the set of computational nodes from 1 to n x for a node i d let a x t be the cross section flow area at a given time t and space x and v x t be the depth average flow velocity all other states can be derived from these therefore we can write the saint venant equations as the following non linear hyperbolic system of partial differential equations for a node i at coordinate x and in time as follows 14a q x t x f x t t s x t 14b q a q t 14c f q β v a v g a y t 14d s 0 g a i o i f 14e i f n 2 q q r h 4 3 a 2 14f q v a 14g β 1 g n 2 r h 1 3 κ 2 where a is the cross section area v is the wave velocity g is the gravity acceleration y is the distance from the water surface to the centroid of the cross section i o is the bottom slope and i f is the friction slope β is the boussinesq coefficient for moment transfer corrections and κ is the von kármán s coefficient yang et al 2018 usually assumed as 0 41 both a and v are the main states solved for the longitudinal distance x and time t such that a a x t and v v x t in the following derivations of this paper we neglect the x and t indexes for easier notation as well as for other states dependent on a and v as shown in the previous sub equations functions describing the hydraulic radius and cross section centroid distance from the water surface are required therefore we first apply the hydraulic property estimation algorithm to determine these functions and solve the sve by using the tabled results to find the required geometric functions to solve all nodes together we collect the state vectors e g q s and f for each internal node from i 2 to n x 1 that is from the second node to the second to last node such that matrices q q 2 q 3 q n x 1 t f f 2 f 3 f n x 1 t and s s 2 s 3 s n x 1 t can be derived therefore we solve the vectorized set of hyperbolic partial differential equations for all nodes except the boundaries by numerically discretizing the problem using the lax friedrichs method lax 1954 this numerical scheme uses a forward discretization in time centered discretization in space and has first order accuracy in space in time kurganov 2018 expliciting eq 14a for q we can derive the following matrixwise expression such that 15 q t δ t 1 2 q b t q f t δ t 2 δ x f f t f b t 1 2 s b t s f t where subscripts b and f represent backward and forward states from each node i matrices q q b q f f f f b s b and s f r n x 2 2 the boundary conditions are applied in matrices from backwards and forwards for q and the source terms and flux terms for the boundary conditions are calculated using eqs 14c 14g the connection between the sve model with the hp estimator model is presented in fig 5 2 11 1 adaptive time stepping hydrohp 1d model has an adaptive time step scheme based on courant number c courant et al 1928 our model has to ensure c 1 to guarantee convergence due to the explicit finite difference scheme used to solve 1d sve hydrohp 1d spatial mesh is stationary with constant length δ x therefore the only manner to ensure computational stability is by changing the model time step δ t accordingly such that at the end of a computational time step we calculate the refreshed computational time step as follows 16 δ t t min i d c u i t g h m i t where c is the user defined courant number e g typically assumed between 0 5 and 1 for 1d hydrodynamic modeling u t q t a t is the flow velocity h m t a t b t t is a representative water depth for irregular channels and b t t is the top width 2 11 2 boundary conditions in order to represent feasible hydraulic conditions in 1d channels hydrohp 1d has various options for combinations of boundary conditions the current model version has 3 upstream boundary conditions and 2 downstream boundary conditions the first node of the mesh can be subjected to a nash inflow hydrograph e g see eq 21 tabular inflow hydrograph and tabular stage hydrograph tabular data is then interpolated to the model time step via piecewise cubic hermite interpolating polynomial method barker and mcdougall 2020 the downstream boundary conditions are twofold normal slope boundary condition or stage hydrograph boundary condition through the modeling of a wave function since hydrohp 1d uses the lax friedrichs method that uses a central difference in space the solution is only properly given in internal nodes therefore numerical issues in the boundary may arise when poorly discretized meshes are used akan and iyer 2021 when either stage hydrograph or inflow hydrograph boundary conditions are used in the borders of the domain a zero order extrapolation is used more details of the treatment of the boundary conditions is explained in the supplemental material section 3 case studies in this section we show various model applications for different cross sections first in section 3 1 we apply the model in single sections for triangular parabolic and semi elliptical semi parabolic cross section moreover the model is applied to estimate composite cross sections in section 3 2 followed by an example of an irregular cross section with left and right overbanks in section 3 3 the cross section data are shown in fig 3 the assumed data for the sections can be found in the supplemental material the variety of these sections was selected to represent the different common and complex shapes used in hydraulic design in the particular case of irregular cross section this represents a real river section in the little washita river garbrecht 1990 3 1 numerical case study 1 normal flow in complex regular cross sections in the first case study we show three examples of determining stage discharge relationships for well known cross sections the tested cases are cross sections of the following shapes a triangular b parabolic c semi elliptical semi parabolic these sections can be represented by single closed form equations for hp since the geometry functions are relatively easy to determine for the particular case of a we can determine the cross section with 3 coordinates however for cases b and c since geometry and slopes can change dramatically with x we discretize the depth into 1 cm steps and determine x in terms of y with the governing equations of b and c therefore the number of coordinates entered to determine shapes b and c is primarily dependent on the maximum depth of the channel 3 2 numerical case study 2 normal flow in composite regular cross sections another common type of cross section used in channels is the use of combinations of regular shapes in this numerical case study we estimate hp for the following cross sections d road gutter e composite triangular and rectangular and f successive trapezoid gabion channel we have chosen these sections to pinpoint common cross sections used in hydraulic engineering design as mentioned in 3 1 x is numerically discretized in terms of y for rapid changes in cross section shapes in these cases all sections of this case study are only changed by changing slopes when a particular threshold depth occurs d and f or by increasing b i see eq 8 3 3 numerical case study 3 irregular cross sections with overbanks in this case study we model a cross section of the river presented in garbrecht 1990 two stage roughness hypotheses were tested in this case study 3 3 1 single manning coefficient assuming a depth invariant stage roughness curve with a baseline roughness of n b 0 02 sm 1 3 for all break point segments we model the stage conveyance assuming certain eventually feasible values of roughness 3 3 2 depth varying coefficient in this subsection we test the role of uncertainty in the estimation of the roughness coefficient and its propagation in the conveyance three tests are performed as follows scenario i assumes the baseline scenario shown in the previous section scenario ii assumes a linear increasing stage roughness relationship mustaffa et al 2016 scenario iii considers a 2 order polynomial stage roughness relationship and scenario iv assesses a monte carlo stage roughness relationship with a known average variation μ and standard deviation σ n varying from a base roughness coefficient from scenario i this scenario also accounts for cases where the roughness coefficient decreases with stage as presented in alves et al 2020 a static scenarios the stage roughness relationships for the static scenarios i e only one depth series per scenario are shown as follows 17a n 1 n b 17b n 2 d y n b α d y 17c n 3 d y n b α d y 2 where d represents the left or right directions from the invert and α are coefficients that describe the variation of the roughness b monte carlo analysis the uncertainty in manning s coefficient is evaluated assuming an average error μ 30 and a standard deviation σ n 0 01 based on kim et al 2010 we perform 200 monte carlo simulations to estimate the eventual variations of roughness in the channel the signal in the following equation represents the cases where the roughness increases and decreases on average 18 n 4 d y n b 1 μ σ n n 0 1 although only one series of n 4 is shown in fig 8 we estimate the flow discharge for 200 cases 3 4 numerical case study 4 divided channel method vs single channel in this subsection we define the analysis performed to compare the differences between scm and dcm in modeling conveyances in regular composite and irregular cross sections we use the relative error index to calculate the percentage error between scm and dcm as follows 19 re k scm k dcm k dcm 100 similarly we compute the relative error between scm and hec ras re ras brunner 2016 analogy with eq 19 3 5 numerical case study 5 modeling unsteady state hydrodynamics and rating curves with a 1d sve model 3 5 1 non breaking wave propagation over a horizontal plane this section compares solutions given by hydrohp 1d hec ras 1d and an analytical solution for a 1d wave propagating over a horizontal plane the diffusive convective and inertial terms of sve play important roles in this case and are tested hunter et al 2005 developed an analytical solution of sve for this problem neglecting inertial and convective terms such that the water depth in position x at time t can be written as 20 h x t 7 3 c n 2 u 3 x u t 7 3 where u is the depth averaged velocity in the x direction c is a constant of integration solved by referring to the initial conditions of the problem i e h 0 t 0 0 n is the stage invariant roughness coefficient and t the elapsed time the parameters assumed are u 1 m s n 0 03 sm 1 3 and the channel width is assumed as 100 m results are compared within the developed model hec ras 1d full momentum solver and analytical solution given by eq 20 3 5 2 unsteady state inflow hydrograph and normal flow at the outlet in this section we compare the performance of hydrohp 1d against hec ras 1d model under different combinations of stage hydrographs and inflow hydrographs hec ras is set to use the 1d unsteady finite difference numerical solution with the skyline gaussian solver brunner 2016 which uses an implicit numerical scheme and solves a series of non linear system of equations whereas hydrohp 1d uses an explicit finite difference method for this numerical testing we adapt the problem 8 3 presented in akan and iyer 2021 to a different inflow hydrograph boundary condition with a relatively more complex cross section the problem consists in an unsteady state simulation of an open channel with n 0 025 sm 1 3 s 0 0 00025 m m with an inflow hydrograph boundary condition and a normal slope outlet boundary condition i f n x 0 00025 m m the channel length is 1097 88 m and the elevation of the first node is 0 274 m simulation time is 360 min and detailed outputs are retrieved each 10 min the differences from akan and iyer 2021 are the cross section number of nodes and the tested and the inflow hydrograph herein we apply hec ras and hydrohp 1d in the v notch see fig 3 for an inflow hydrograph modeled with a nash function i e eq 21 with a peak flow of 2 5 m 3 s time to peak of 3 hours baseflow of 0 25 m 3 s and β 8 5 moreover hec ras 1d and hydrohp 1d are discretized with 100 equally spaced cross sections hec ras is set to solve the fully implicit scheme with a fixed time step of 1 s while hydrohp 1d adaptive time step scheme see eq 16 is used with δ t min 0 5 s δ t max 5 s c 0 5 3 5 3 irregular cross section in this case study we simulate the diffusive and convective effects in flow propagation in an open channel with irregular geometry the goal is to determine looped rating curves and to estimate another possible source of uncertainty in stage discharge modeling holmes 2016 for this analysis we consider a 1000 m channel discretized into 50 sub reaches with 20 m length each the bottom slope was assumed as 2 1 0 4 to represent a feasible condition for rivers where convective and advective accelerations play important roles we assumed a normal depth outlet boundary condition meaning that the energy slope gradient at the outlet is the same as the bottom slope to simulate the effects of convective advective acceleration and the diffusive kinematic effects of the flood wave looped stage discharge curves are modeled using an inlet boundary condition modeled with a nash function hydrograph akan and iyer 2021 gomes jr et al 2023 which is defined as 21 q in t q b q p q b t t p exp 1 t t p γ where q in is the inflow hydrograph q b is the baseflow q p is the peak flow t p is the peak time and γ shapes the hydrograph the aforementioned parameters are typically derived from hydrology studies of the upstream watershed therefore they represent the degree to which surface and sub surface runoff are generated in detail q p is closely related to the watershed area shape infiltration and imperiousness rate in addition the factor γ shapes the rate of q in defining the flow acceleration therefore it is closely related to the aforementioned parameters of q p however the parameter q b is mostly related to groundwater properties and soil infiltration capacity as parameters for modeling we assumed q b 1 m 3 s q p 75 m 3 s and t p 2 h these values typically represent inbank conditions for the assumed river slope for normal flow the simulation time was assumed as t f 6 h and the computational time step was set as δ t 5 s in this study we test the possible effects of different hydrographs on unstable rating curves by varying γ from 2 to 10 with steps of two units to represent the effects of eventual urbanization in the watershed 3 5 4 v notch and rectangular weir in this sub section we test the effect of varying roughness for inbank and overbank conditions into unsteady rating curve modeling data assumed for this sub section are the same as the previous subsection except for q p and q b which were assumed as 2 of the values from the previous section as mentioned we model the channel with two roughness coefficients using the divided channel method the assumed roughness coefficients for inbank and overbanks were 0 020 and 0 035 respectively 3 5 5 upper negro river stage discharge curves compared to observed data the algorithms developed in this paper are applied to a cross section of the upper negro river located in the state of amazonas brazil see fig 11 the negro river is the seventh largest river in the world in terms of volume and is the largest affluent of the amazon river the largest in the world in terms of volume observed data of the negro river were retrieved from the agência nacional de águas ana database ana 2022 for the stream gauge code of 14330000 post processing in raw data from the ana database was performed in hidroapp available at https www labhidro ufsc br hidroapp souza et al 2021 the cross sections tested in this section are shown in fig 12 only years with consistent data were used in this analysis water surface flow slopes at this station vary monthly with the highest variations in september furthermore the slopes vary throughout the river so that the average slope value for the upper negro river can be assumed to be s 0 8 cm km marinho 2021 moreover manning s roughness coefficient varies with the depth of the water surface alves et al 2017 however in this section we fixed a single roughness and slope for the sake of simplicity and to test the model s ability to predict normal flows and their occurrence compared to the observed data we assume n 0 042 a value in accordance with previous estimates found in alves et al 2017 cross sectional data were only available for years after 1993 as a result the invert elevation of observed flow data from 1976 to 1999 was assumed to be the same as the last cross section observation in 1993 6 97 m 4 results and discussion 4 1 numerical case study 1 4 1 1 normal flow in complex regular cross sections the results of the model application for triangular parabolic semi elliptical semi parabolic cross sections are presented in fig 6 the errors within the developed model and the noaa normal depth solver are shown in fig 7 the hp values for y max are shown in table 1 all relative errors of the developed model and the noaa normal depth solver fault within a 10 difference as presented in fig 7 for cross sections t p and ep errors were below 3 for discharge area perimeter and top width and are almost negligible for velocity all hp curves are continuous and increase monotonically with y more specifically although being relatively complex shapes i e semi elliptical semi parabolic one could derive analytical expressions for perimeter area centroid and others to solve sve without requiring entering of hp values from tables examples of applications of sve with complex but closed form geometrical equations can be found in simões et al 2017 4 2 numerical case study 2 4 2 1 normal flow modeling in composite cross sections cross sections b c from fig 1 i e p ep results are also presented in fig 6 and normal flow errors are shown in fig 7 the errors were approximately negligible compared to the noaa solver and are mainly due to the introduction of noise σ into the x and y coordinates although the stage discharge relationship of sections p and ep was developed a very fine numerical discretization in x and y was required for an accurate estimate of hp for these sections we discretized y into 1 cm depth steps so that to define a 2 meter depth parabola and semi ellipse semi parabola cross sections we entered 400 coordinates in the model in addition in the model calculations we assumed σ 1 1 0 3 cm which means 1000 points each 1 cm ultimately to properly capture the right hp in highly detailed cross sections one has to define proper numerical discretizations to avoid instabilities the greatest errors occurred for the gabion g and road gutter gu cross sections these errors occurred mainly due to the introduction of noise σ from eq 1 and due to the conceptualization of the scm compared to the solution used in the noaa solver dcm 4 3 numerical case study 3 4 3 1 normal flow modeling in irregular cross sections we can see in fig 7 that the normal flow error was below 2 even with different model conceptualizations from the noaa solver and the model developed here however discontinuities occurred in geometrical functions such as area and perimeter as shown in fig 6 these sharp changes occurred when the flow diverted from the main channel to overbanks the conceptualization of the scm model is a depth based model assuming that the flow first propagates in the bank and after reaching the depth of the main channel y m see fig 10 the flow immediately diverts to the overbanks with the same elevation of the water surface elevation in reality more sophisticated software such as hec ras for example can successfully model cross section ineffective flow areas that could account for effective overbank flows brunner 2016 4 3 2 uncertainty in steady rating curves for inbank and overbank depths the analytical determination of stage conveyance figures for the irregular cross section g in fig 3 is shown in fig 9 two different behaviors were observed in the stage conveyance modeling first for inbank conditions flow is very similar to simpler cases of river rating curves as presented in westerberg et al 2011 and perumal et al 2007 for the same design conveyance water depths can have approximately 0 4 m difference for different scenarios of stage roughness as expected the scenario with more hydraulic capacity was scenario i which assumed a single manning s coefficient throughout the whole cross section on the contrary scenario iii i e stage variation of 2 orders in roughness was the scenario with a lower hydraulic capacity i e k part b of fig 9 shows the stage conveyance i e q s 0 after reaching y m and the results show that 100 relative differences can occur when comparing different methods such as scenario i and scenario iii for the same depth these results show the sensitivity of roughness coefficients in the modeling of rating curves and the importance of proper estimation of this parameter to accurately estimate flow discharges the monte carlo analysis shows how various can be the conveyance and the at station roughness coefficients assuming typical uncertainty in roughness estimation with μ 30 and σ n 0 01 as presented in part c of fig 9 part b of fig 9 shows the plots of k for scenarios 2 3 and 4 compared to k for scenario 1 i e the baseline scenario generally river cross sections are assumed with constant roughness coefficients and the results of this graph show that most of the time comparisons within k 1 and k i are below the 45 line indicating that assuming a single roughness coefficient could be an overestimation of the hydraulic capacity of the cross section up to 30 in inbank conditions and up to 100 in large overbank conditions however these results are obtained from a cross section with overbanks of approximately 400 m and the main channel of nearly 10 of the overbanks 4 4 numerical case study 4 4 4 1 scm x dcm methods the scm and dcm differ specifically in two criteria dcm herein considers an introduction of a shear stress acting at the interface between the overbanks and the main channel and considers an equivalent roughness that accounts for the divided conveyances the scm however considers a single section without the assumption of an increased perimeter in inbank overbank interfaces to this end the differences in this method are most observed in cases where this new perimeter is comparable to the scm perimeter increased by the overbank perimeter this result can be observed in table 1 where re was only 3 for the irregular cross section i e y f 0 4 m b m 32 72 m and p y max 452 44 m but was 40 for the v notch rectangular cross section the results of this section indicate that the scm and dcm methods are more agreeable for sections with large overbanks widths and low overbank depths furthermore they are the same for single sections for complex sections as shown in table 1 for sections t p and ep 4 5 numerical case study 5 4 5 1 non breaking wave comparing analytical solution with hydrohp 1d and hec ras the modeling results are presented in fig 13 and shows the water surface elevation for different durations varying from 12 min to 60 min it is noted that neither hec ras 1d model nor hydrohp 1d model should match the analytical solution the analytical solution is a simplification in 1d swe to consider only diffusive effects therefore in the interface between wet and dry sections hec ras 1d and hydrohp 1d should disagree with the analytical solution especially because inertial effects become to play important roles in these cases similar results of the ones presented in fig 13 are shown in bates et al 2010 hunter et al 2005 and dottori and todini 2011 indicating the inertial effects are responsible for changing the arrival time of the flood wave in the dry frontier compared to a fully diffusive like solution given by eq 20 hydrohp 1d and hec ras 1d model have slightly in the results of this problem due to differences related to mass conservation routines and numerical schemes used in both models 4 5 2 unsteady state inflow hydrograph and normal flow at the outlet the results of the simulation of this numerical study are presented in fig 14 the hydrohp 1d presented similar results with the hec ras 1d implicit model with root mean square errors rmse of flows and discharges of 0 041 m 3 s and 0 053 m respectively as shown in fig 14a b moreover the coefficient of determination r 2 for discharges and water surface elevations are 0 997 and 0 984 the largest differences between hec ras 1d and hydrohp 1d occurred in the inlet and in the falling limb of the hydrograph other studies indicate that the lax friedrichs method can present numerical issues in the boundaries akan and iyer 2021 kurganov 2018 and this could be one of the reasons for this difference between the two models furthermore since the discretization used for the solution of the partial differential equations is of first order accuracy numerical diffusivity errors can propagate even with the relatively fine mesh assumed i e δ x 11 09 m δ t min 0 5 s kurganov 2018 although some differences are found between both models the water surface elevation profiles shown in fig 14c i show the relatively small absolute differences between hec ras 1d and hydrohp 1d indicating that the model can predict not only discharges but also water surface elevations 4 5 3 hysteresis effect in rating curve modeling we compare results from steady and unsteady stage discharge modeling for the irregular cross section the little washita river in fig 3 g the results indicate that the average errors within normal flows and flows modeled with a hydrodynamic model are up to 50 e g see the stage at 2 5 m as presented in fig 15 the results of this analysis are similar to the results shown in muste and lee 2013 differences in the rising limb of the hydrograph are relatively negligible for first inbank conditions see part b at the stage of 3 m approximately however after reaching it a sharp change in the flow discharge occurs due to the increased flow area from the second inbank region the differences between the hydrodynamic cases of γ begin to increase after this stage although they are getting closer to the normal flow towards the maximum stage that indicates that local acceleration played a more important role at lower stages the falling limb of the hydrograph shows a similar trend with smaller flows for the highest γ s the results of this section can serve as a guide to model unsteady rating curves in the river and to assess the variation of complex rating curves variation for inbank conditions holmes 2016 the results of fig 16 show the results of the hydrodynamic simulation of the vr section of fig 3 with different roughness coefficients for inbank and overbank areas as shown in this figure the flow discharge can increase to 60 in the rising limb of the hydrograph while it can decrease to 25 in the falling limb this is particularly important when using v notch weirs with a known material rugosity with walls and banks retrofitted with rougher materials such as rocks and local vegetation stage discharge in weirs can carry large errors depending on the hydrograph properties of the inflow as shown in fig 16 in addition the role of changing the roughness of the overbanks is shown at the flow discharge rate of change after the stage of 1 m surprisingly even when the roughness of the overbanks increases the average roughness coefficient for the entire section using eq 12f rapidly decreases after the stage of 1 m and starts to increase reaching a value of 0 025 in the maximum stage this reduction is due to the rapid increase in the perimeter so that the average roughness coefficient would have to decrease to simulate the computation of the total conveyance as followed by eq 5b 4 5 4 normal flow rating curve modeling in upper negro river cross section dynamic changes over time have not played the most important role in the modeling of the rating curve except for 2008 as shown in fig 17 the invert elevation of the cross section of this year is 7 66 m whereas the invert elevations of the other years are 10 07 m 0 77 m which explains the difference in y for the same flow discharge however we can visually infer from fig 12 that the cross sections within years 2007 to 2009 have not dramatically changed except for the invert the difference in the invert elevation might be due to sediment deposition de almeida et al 2016 the daily hydrograph shown in fig 17 b shows average daily hydrographs the values shown in this plot are calculated as follows first the average of two stage measurements is calculated therefore instantaneous peaks are generally smoothed following this a fitted rating curve from ana is used to convert stage to flow resulting in this chart the values of flow represent daily averages as mentioned above whereas the plotted values of observations in fig 17 a are instantaneous observations in other words we can infer that observations are always upper bounded by daily average maximum observations since maximum observations fall mostly below 2 1 0 4 m 3 s while flow discharges have values above this threshold almost every year it indicates that the measured data for flow discharges in this cross section fail to capture the largest flows which can impact statistically based rating curves derived from linear regression methods souza et al 2021 clarke et al 2000 the looped behavior found in the observed data from 1976 1999 could be from two main reasons hysteresis effects errors in the assumed invert elevation dynamic change over the period due to deposition erosion and dunes formation or due to changes in manning s coefficient over time moreover a systematic uncertainty is observed when comparing the model with observations the model developed here assumes y max as the maximum value within the left and right directions from the invert break point therefore the flow is not necessarily calculated for the maximum cross section data z see fig 12 however observations of flows are found to be higher than these levels as shown in fig 17 values found at these stages are not modeled but can also be the result of hysteresis effects 4 6 strengths and weaknesses of the presented model the set of algorithms developed in this paper can be used to solve different types of hydraulic problems varying from regular concrete channels to complex shapes or irregular cross sections with overbanks and depth varying roughness some strengths and weaknesses of the model are presented as follows 4 6 1 strengths flexibility to simulate all kinds of single cross sections or composite sections with two overbanks simulate the role of stage varying roughness simulate the role of at station roughness variation assess the uncertainty in the estimation of rating curves by deploying monte carlo analysis no need to define an iterative problem to solve manning s equation as required by other models us department of commerce 2016 brunner 2016 either by performing polynomial approximations for cross section segments or by defining linear interpolations relatively fast computations and the possibility of parallelization for normal flow estimations for large scale cases simulate the hysteresis effect in unsteady rating curve modeling possibility to couple hydrohp 1d with land surface models or hydrological models due to the open source codes 4 6 2 weaknesses time invariant hydraulic parameters that could be conceptualized as time varying parameters to consider riverine changes due to erosion and deposition moreover this tool does not allow simulation of sediment laden flows that can contribute to time varying changes during flow propagation the model did not explain cases with more than 2 overbanks e g the case of the gabion channel is shown in fig 1 with results presented in fig 10 and fig 6 during the rising limb of the hydrograph in a cross section with overbanks flow is confined first to the inbank and later propagated to the overbanks after the level reaches the overbank level however this might not always be the case e g hec ras considers that flow can go for the overbanks even when the level has not reached the overbank level the change in the cross section by adding a noise σ can be important for channels with small dimensions e g road gutter cross section due to being significant in the calculations of hp moreover the proper definition of σ is required for simulating detailed cross sections as the parabolic and semi elliptical semi parabolic cross sections therefore more points and more computational efforts are required to simulate these aforementioned sections with accurate results the dcm method applied in this paper although it solves the problem of non monotonic conveyance curves by adding overbank and inbank conveyances due to the correction of n in eq 12e still carries discontinuities when sharp areas and perimeters are added in inbank conditions overall the modeling of the rating curve with fixed parameters for roughness and slope showed good results in the upper rio negro the model was able to predict flows within the cross section range for most cases some outliers are found and could be explained by different friction slopes or roughness coefficients however defining the proper stage roughness functions or the friction slopes is difficult and would require knowing a proper inflow hydrograph which is typically unavailable for resolutions higher than 1 day in brazil 5 conclusions and future work the development of two algorithms for hp estimations in regular composite and irregular cross sections capable of simulating stage varying roughness and at station varying roughness variation is performed the results of model validation indicate predictions of normal flows within average relative errors of approximately 5 when compared to an established normal depth solver moreover hydrohp 1d results are consistent with hec ras 1d model not only for a wave propagation condition but also for unsteady state simulation with inflow hydrograph at the inlet and normal slope at the outlet from the results we draw the following conclusions the developed algorithms can represent normal flow conditions for complex cases and irregular cross sections with the single method channel and divided method channel the greater differences within scm and dcm methods occurred for v notch cross sections and road gutter cross sections these are sections of relatively large overbanks compared to the cross section height only a 3 difference between scm and dcm occurred for the maximum water height in the irregular cross section indicating that flow discharges at the maximum stage for irregular channels with relatively large overbanks i e b m b f 0 1 where b f is the overbank width assuming maximum levels both methods are more comparable assuming feasible scenarios of stage roughness in an irregular channel inbank stage discharge for the same discharge can have a stage variation of 0 40 m for overbank stage discharge however this difference is typically smaller 0 3 m additionally for the same water depth k can increase to 100 indicating that the flow is highly influenced by the overbank areas and roughness the unsteady rating curves modeled in this paper for an irregular cross section indicated an increase of 50 and a decrease of 25 for the rising and falling limb of the hydrographs similar results were found for a composite v notch rectangular weir where errors between normal flow and unsteady flow would be up to 60 and 25 for the rising and falling limb of the hydrographs results of comparing hydrohp 1d with hec ras 1d in an open channel indicate that both models agree with r 2 0 97 and r 2 0 984 for discharge and water surface elevation and rmse 0 041 m 3 s and rmse 0 053 m for these same variables normal flow discharges in the negro river assuming a constant manning s roughness coefficient and bottom slope show good results without requiring calibrations or statistical analyses the results show that even with these assumptions the developed model can accurately predict stage discharge in real world stream gauges future application of the developed model will be in estimating rating curves for ungauged rivers in brazil where data in suitable resolution are sometimes available but little information regarding flow discharges is displayed furthermore we want to exploit the role of shear stress in more fundamental ways than by assuming a linear increase in the hydraulic perimeter on the main channel moreover future versions will allow entering different cross sections and interpolating them instead of simulating a single cross section in addition sediment laden flows and a movable overbed channel can be explored to explain dynamic changes on stage discharge during monitored events in remote sensed areas finally hydrohp 1d can be linked to land surface models allowing to simulate the impacts in flood routing of rivers and channels due to hydrologic regime alteration moreover hydrohp 1d can be used to quantify predict and scenarize the effects of extreme events of floods and droughts due to climate change in riverine networks credit authorship contribution statement marcus n gomes jr conceptualization methodology software validation formal analysis investigation data curation writing original draft writing review editing visualization luis m c rápalo writing review editing data curation resources paulo t s oliveira writing review editing marcio h giacomoni writing review editing funding acquisition visualization resources césar a f do lago writing review editing eduardo m mendiondo writing review editing funding acquisition visualization resources declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the authors appreciate the support from coordenação de aperfeiçoamento de pessoal de nível superior capes for funding this research this work was finantially supported by fapesp 22 08468 0 flash drought event evolution characteristics and the response mechanism to climate change the authors also appreciate the effort made by eng mateo hernándes sánchez who gently helped with the hec ras modeling appendix a supplementary data supplementary data related to this article can be found at https github com marcusnobrega eng hydrohp 1d the supplementary material have fully descriptions of hydrohp 1d input data sec i a data treatment from ana sec i b mathematical treatment at domain boundaries sec i c algorithm 2 for hp estimation sec i d and matlab codes of i hp estimator sec i e1 ii read input data for sve model sec i e2 iii sve model sec i e3 iv post processing scripts sec i e4 v video generator of sve states for irregular sections sec i e5 vi video generator of sve states for regular sections sec i 6 and vii detailed output algorithm sec i e7 supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2023 105733 appendix a supplementary data the following is the supplementary material related to this article mmc s1 
25377,as efforts to mitigate the effects of climate change grow reliable and thorough reporting of greenhouse gas emissions are essential for measuring progress towards international and domestic emissions reductions targets new zealand s national emissions inventories are currently reported between 15 to 27 months out of date we present a machine learning approach to nowcast dynamically estimate national greenhouse gas emissions in new zealand in advance of the national emissions inventory s release with just a two month latency due to current data availability key findings include an estimated 0 2 decrease in national gross emissions since 2020 as at july 2022 our study highlights the predictive power of a dynamic view of emissions intensive activities this methodology is a proof of concept that a machine learning approach can make sub annual estimates of national greenhouse gas emissions by sector with a relatively low error that could be of value for policy makers graphical abstract abbreviations nzggi new zealand greenhouse gas inventory ipcc intergovernmental panel on climate change ghg greenhouse gas ls bounded variable least squares rf random forest et extra trees mape mean absolute percentage error keywords random forest extra trees emissions prediction dynamic carbon tracker energy agriculture data availability the data used in this research is either publicly available via the cited source or the data was made available to the researchers through a data commercialisation agreement with one of the data partners of dot loves data the data provided by the data commercialisation agreement is therefore unavailable to access 1 introduction emissions from human activity are the key driver of the anthropogenic climate change observed since the onset of the industrial revolution c 1850 in an effort to mitigate our influence over the global climate the paris climate agreement aims to limit global temperature rise to well below 2 c and pursue efforts to limit warming to 1 5 c above pre industrial levels requiring significant action to reduce greenhouse gas emissions to net zero on an international scale reliable and thorough greenhouse gas inventories are fundamental in measuring progress towards these targets and provide insight into domestic and international emissions reduction policies and strategies yona et al 2020 guidelines for such inventories are published by the intergovernmental panel on climate change ipcc and were refined in 2019 the first major update since 2006 calvo buendia et al 2019 we develop a proof of concept that combines machine learning methods with indicators of dominant trends in new zealand s emissions profile in order to improve timeliness of emissions reporting with relatively low error and therefore nowcast new zealand s emissions in the sense described in iveroth et al 2022 the importance of greater timeliness is also highlighted in yona et al 2020 new zealand s national emissions are reported in the new zealand greenhouse gas inventory nzggi it is the official annual report of all anthropogenic emissions and removals of greenhouse gases ghgs associated with production which is currently released approximately 15 months after the end of the calendar year being reported on as determined by international reporting guidelines ministry for the environment 2021 multiple ghg types estimated by the nzggi are aggregated in terms of co2 equivalents co2 e 1 1 various research debates ways of aggregating ghgs see rogelj and schleussner 2021 and references therein we are bound to co2 e since it is used by the nzggi our indicator based model estimates co2 e directly rather than modelling each ghg type to compensate for limited data availability national emissions inventories are time consuming to produce at any given time national ghg emissions reported in the nzggi are between 15 and 27 months out of date methodologies for the release of quarterly and more timely emissions estimates are in the process of being developed in a number of countries huang et al 2021 iveroth et al 2022 statistics netherlands 2021 within new zealand statistics new zealand are developing quarterly emissions reporting statistics new zealand 2021 but as we discuss in section 4 1 4 this is not comparable to the nzggi in many ways this existing gap highlights the need for a more dynamic dataset that complements the nzggi to be developed numerous studies exist that estimate daily emissions i e emissions occurring within any given day by first developing a daily inventory of emissions forster et al 2020 develops a bottom up inventory using national mobility data finding that emissions reduced during the covid 19 pandemic by as much as 30 during april 2020 liu et al 2020b constructs a measure of co2 emissions by developing an inventory by country comprised of the sectors that are power generation industry transportation and household consumption each broken down by the three fossil fuel types which are coal oil and natural gas findings include that co2 emissions decreased by 8 8 in the first half of 2020 compared to the same period of the previous year in china quéré et al 2020 again develops an inventory using a bottom up approach comprised of six sectors finding that daily co2 emissions decreased by 17 by april 2020 compared to the mean 2019 levels with surface transport being the sector that influenced reduction most heavily further studies that estimate daily emissions by developing a daily inventory of emissions include liu et al 2020a oda et al 2021 zheng et al 2020 such inventories may be labour intensive to curate even more so than the nzggi at the annual level these studies have impressively specific results such as daily emissions estimates with a regional component while separating co2 and air pollutants huang et al 2021 however they need to focus on short windows of time to obtain a sufficiently detailed inventory in the first place this distinguishes our work which reports on emissions from 1990 up to two months prior to the present day we produce daily estimates of cumulative ghg emissions in every 365 day period since 1990 with a latency of less than two months in other words we produce daily estimates of annual emissions where annual is thought of as a rolling 365 day window since we provide daily estimates of annual emissions the estimate occurring on the last day of a given year is an estimate for the cumulative emissions that year this figure is comparable to the annual value coming from the nzggi for that year we compare a number of potential models in terms of prediction accuracy model stability and over fitting the approach can be applied to any sector and has so far been applied to the energy and agriculture sectors of the nzggi ministry for the environment 2021 these sectors were chosen to begin with for their relative contribution to the nzggi making up 90 0 of emissions in 2020 ministry for the environment 2022b the input variables for each sector are called indicators there is a growing precedent that a relatively small number of indicators can be sufficient to model broad greenhouse gas emissions systems recent examples including amaefule et al 2022 ulku and ulku 2022 using this approach we are able to model the energy and agriculture sectors with just four and six indicators respectively our models do not assume a relationship between indicators and the emissions of their sectors thus to explain the inner workings of the model we have to infer relationships between indicators and predictions by the model we use subject knowledge to verify that these relationships are sensible in addition we ensure our methodology is repeatable between editions of the nzggi we assess uncertainties carefully in terms of accuracy stability and over fitting in sections 3 1 1 to 3 1 3 additionally we compare our uncertainties to typical inter annual variation in section 3 2 to mitigate the concern that the error due to our model exceeds expected changes from one year to the next results are derived in section 3 using both of our high frequency and low latency of emissions reporting the insights resulting from our models could improve current understanding of emissions in a landscape changing day to day 2 methods 2 1 overview we use cross validation to compare a number of potential models that predict nzggi sector emissions using data on variables related to sector emissions a model selection protocol based on prediction accuracy model stability and over fitting is used to decide a preferred model we apply the preferred model to estimate the energy and agriculture sectors of the nzggi ministry for the environment 2021 these sectors were chosen to begin with for their size together making up 90 0 of emissions in 2020 ministry for the environment 2022b we call the independent variables used for each sector indicators which we describe in detail in section 2 2 our models are non parametric in the sense that we do not assume a data model thus we face the risk of selecting a model that cannot be explained we combat this risk in section 3 3 the main objective of the model is to attain high frequency and low latency outputs because of limited data availability to achieve the objective we make some simplifying choices 1 as described above our dependent variables are sector emissions rather than estimating every source of emissions and then aggregating and 2 we estimate carbon dioxide equivalents directly rather than estimating each ghg type individually and then aggregating such as in huang et al 2021 ulku and ulku 2022 the nzggi updates annually the latest edition at the time of writing reports until 2020 however our analysis is applicable to all editions of the nzggi therefore we discuss the model mostly with respect to any edition of the nzggi any results presented pertain to the 2020 edition dataset ministry for the environment 2022 existing research involves data sets of a wide variety of frequencies and time spans the most similar studies to ours in these terms are jayaraman et al 2022 ulku and ulku 2022 wen and yuan 2020 which have an annual frequency and time spans of 1990 2018 1990 2019 and 1997 2017 respectively we have a time span of 1990 2020 when using the latest edition of the nzggi we collect new indicator data daily so that predictions using the model can be made daily this means our outputs not only estimate year end values that mimic the nzggi but also interpolate between those year end estimates and leave us with a low latency of less than two months in practice a more detailed description of our daily estimates of emissions is given in section 1 2 2 indicators indicators are times series associated with natural variables that are chosen as input variables for the model of a given sector there are many variables that could serve as indicators of emissions in our model the indicators discussed in this paper are non exhaustive and this model could in fact be replicated with an entirely different set we therefore do not claim they are the most important of the potential indicators but they are sufficient in the sense that they effectively model their sectors at this point in time and are available dynamically our indicators are chosen for 1 their systematic relevance to ghgs 2 a resulting relationship with sector emissions predictions that is sensible and 3 the availability of the necessary data the indicators used in the model are summarised in table 1 each indicator represents an emissions intensive activity e g kgco2 e per km of petrol fuelled vehicle travel intensity i e emissions per unit of activity helps to account for the necessary dissociation of emissions with activity and productivity if an indicator considers intensity then we say it is direct for example one of our indicators measures the emissions potentials of new vehicles on new zealand roads based on motive power year of manufacture and gross vehicle mass the data comes from a national vehicle register that is updated monthly therefore our model accounts for variation in emissions intensity over time via indicators such as these for more information see appendix a 2 if an indicator is not direct we say it is indirect we use a combination of direct and indirect indicators we check for updates to our indicator data on a daily basis to ensure the data is as up to date as possible where a dataset is not updated on a daily basis the dataset is held constant until the present day and interpolated linearly between historic data points in order to provide a daily value in the extreme case where a dataset is updated only annually this would mean the data is held constant for up to 365 days in appendix we describe the construction of each indicator in detail given the hundreds of sources of emissions featured in the nzggi the indicators presented in table 1 appear scarce we acknowledge there are many potential indicators and as new datasets emerge these may model the sectors better than those datasets currently being used while the indicators used here are not necessarily optimal they are sufficient for this proof of concept it is important to note as previously discussed that the benefit of this approach is the limited number of variables required within the model furthermore the performance of regression models is generally sensitive to collinearity between indicators farrar and glauber 1967 the scarcity of useful data makes it difficult to find indicators with low collinearity we note that there exists high collinearity between indicators as shown in fig 1 while methods exist to aggregate correlated variables we keep our indicators as individual inputs for simplicity therefore we will prefer machine learning models that have no such requirements around collinearity of indicators certain machine learning models also allow non linear relationship between indicators and sectors ulku and ulku 2022 this is valuable because including non linear indicators can be critical for prediction accuracy wen and yuan 2020 we note that it is not intuitive that there is a negative correlation between mea and exm however there are possible explanations for this relationship such as the export inspected facilities where mea is counted having a limited scope or the variation in the value of exports over time outweighing an actual decrease in quantity of meat exports this requires further investigation we discuss this further in section 3 3 2 3 model selection 2 3 1 models we compare three models in our selection process bounded variable least squares stark and parker 1995 ls random forests breiman 2001 rf and extra trees geurts et al 2006 et bounded variable least squares stark and parker 1995 ls solves the usual linear least squares problem with the added constraint that fitted coefficients lie within user specified bounds we use the python programming language van rossum and drake 2009 for our analysis in particular ls is implemented with a linear least squares solver from the scipy project virtanen et al 2020 for each indicator we set the bounds to either 0 or 0 depending on the known systematic relationship between indicator and emissions this aligns with the use of a relation expected sign as in gonzález sánchez and martín ortega 2020 table 3 amaefule et al 2022 use such bounds to classify indicators as enablers or de enablers significant statistical analysis is required to verify the use of ls as demonstrated in gonzález sánchez and martín ortega 2020 as mentioned in section 2 2 collinearity and non linearity of indicators makes this infeasible in our setting in selecting our optimal model we will select against ls anyway because it is outperformed by the other models in terms of prediction accuracy see table 2 this suggests that the latent relationships are complex and or non linear we therefore omit statistical details relevant only to ls random forest breiman 2001 rf is an ensemble method that builds a collection of decision trees by incorporating a random component into their construction and makes a prediction by aggregating the predictions of each tree the technique is non parametric in the sense that rf does not assume a data model however there are various hyper parameters for the structure of the forest and its trees these are manipulated in order to tune the model to optimise its predictions this is discussed in detail in section 2 3 2 we implement rf using the random forest regressor by scikit learn pedregosa et al 2011 we elaborate on the random component mentioned above in the construction of each tree to aid the description of the next model each decision tree in the forest involves a random sample of indicators and data points in rf these are sampled with replacement in our application we have only 31 data points to begin with each year from 1990 to 2020 hence we expect we cannot afford sampling with replacement in the random component described above this motivates us to use et extra trees geurts et al 2006 et is similar to rf but is different as follows while the trees in rf involve sampling with replacement the trees in et involve sampling without replacement thus each tree in et is likely to learn from a more comprehensive sample of data in a similar study to ours jayaraman et al 2022 rf did not perform exceptionally well placing fourth among eight models considered we show in section 3 1 1 that our implementation of rf is more accurate than jayaraman et al 2022 we observe in section 3 1 2 that stability issues of rf are resolved by our inclusion of et we use the extra trees regressor by pedregosa et al 2011 to implement et 2 3 2 cross validation over fitting occurs when a fitted model performs worse on unseen data than on its training data cross validation is a technique used to assess for over fitting we follow a standard workflow that incorporates hyper parameter tuning when applicable since the tree based methods have hyper parameters to be tuned cross validation occurs at two levels 1 validating to find the best hyper parameters and 2 testing to ensure the model is not over fitting after retraining on the best hyper parameters thus we use a workflow that is best described as repeated double cross validation by filzmoser et al 2009 see also huang 2022 figure 7 4 this means that at the lowest level the data is split into three parts pre training validating and testing we use the train test split utility by pedregosa et al 2011 with a test size of 6 out of the 31 data points this gives a train test ratio of about 80 20 which is the same ratio as in wen and yuan 2020 if the model on hand is rf or et then hyper parameters need tuning randomised search through a given hyper parameter space is used to find the best hyper parameters we use randomised search instead of its predecessor grid search because randomised search is known to be better at navigating situations where hyper parameters have differing importance bergstra and bengio 2012 figure 1 randomised search cross validation by pedregosa et al 2011 with 5 folds implements the randomised search which repeatedly splits the 25 training data points into 20 for pre training and 5 for validating the hyper parameter space used both for rf and et is defined with the ranges number of trees 200 201 219 maximum depth of tree 1 2 3 minimum number of samples required to split 2 3 4 and minimum number of samples required to be at a leaf node 2 3 4 these were chosen by running a large number of investigations to find a large hyper parameter space in which the models were stable in the sense we describe in section 2 3 3 fitting each model on a particular train test split of the data as described above defines a single simulation in our cross validation we run 30 such simulations this makes our cross validation more comprehensive than jayaraman et al 2022 ulku and ulku 2022 which consider only one simulation each 2 3 3 model outcomes the objective is to select the optimal model in terms of certain model outcomes the model outcomes used in chen et al 2018 are over fitting prediction accuracy and execution time the bounds on the parameters described earlier ensure that the use of the attributes remains aligned with any latent data structures execution time is not important to us since our data sets are small so we omit this outcome a novelty in our approach is to include an outcome about the stability of the structure of the model different models have different structures so the notion of stability has to adapt to each model for ls we consider the distribution during cross validation of the fitted coefficients in the linear model for rf and et instead of coefficients we consider the values of indicator importance in the tuned estimator a model is considered more stable if it has a smaller variation in the distribution above these are described in more detail in section 3 1 2 and pictured in fig 2 thus our objective is to find the optimal model in terms of the model outcomes 1 prediction accuracy in terms of mean absolute percentage error mape 100 n x x test x x ˆ x where x ˆ is the prediction for each x in a given test set x test of size n in cross validation like in chen et al 2018 2 model stability in terms of distribution of fitted coefficients indicator importance as described above and 3 over fitting in terms of the average difference of mape between training and testing data like chen et al 2018 3 results 3 1 framework to assess models to predict sector level greenhouse gas emissions in national inventories cross validation was applied to compare a range of models of greenhouse gas emissions by sector in terms of a comprehensive list of model outcomes section 2 3 prediction accuracy stability and over fitting a machine learning model extra trees section 2 3 1 was found to perform the best according to our selection protocol section 3 1 4 relationships between sector emissions and indicators were inferred from the model section 3 3 and subject knowledge was used to explain these relationships outliers in the relationships were identified via mahalanobis distance and explained in context 3 1 1 prediction accuracy statistics for prediction accuracy are given in table 2 which shows et is optimal across both sectors our results are comparable to those found in jayaraman et al 2022 due to the similarity in methods and initial data in both settings mape is used to measure error when predicting annual values of co2 e our models have resulted in higher accuracy than those presented jayaraman et al 2022 the comparable model to ls in jayaraman et al 2022 is elasticnet since it too is a regularised linear model the comparable model to both rf and et in jayaraman et al 2022 is random forest ls achieves a median mape of 2 0 in agriculture and 3 7 in energy whereas elasticnet achieves a mape of 19 462 in one application of the model and 41 619 in another jayaraman et al 2022 tables ii and iii et respectively rf achieves a median mape of 1 3 respectively 1 4 in agriculture and 3 2 respectively 3 6 in energy whereas random forest in jayaraman et al 2022 achieves a mape of 9 470 in one application and 21 522 in another jayaraman et al 2022 tables ii and iii our higher accuracy is possibly due to our use of hyper parameter tuning there is no discussion of tuning in jayaraman et al 2022 3 1 2 model stability recall from section 2 3 3 that ls respectively et and rf is considered to be more stable if it has smaller variation in the distributions of its fitted coefficients respectively indicator importance values these distributions are pictured in fig 2 we observe stability can vary in a number of ways firstly the order of indicators by median value depends on the model for example in rf the median value for fim quantity of imported fertilisers is slightly higher than that of cow number of dairy cattle whereas in et the box and whiskers for cow is entirely above the upper quartile for fim secondly notice simply how long the boxes and whiskers in some models are and therefore how unstable the models can be et has consistently short boxes and whiskers on the other hand in rf for energy the length of the box and whiskers for the two most important indicators are long especially in comparison to the box and whiskers for the same indicators under et this instability in rf is undesirable since it demonstrates high uncertainty about the impact of each indicator in the model lastly models can be more or less polarising about the coefficient importance of an indicator notice ls is very polarising since it tends to assign either very high or low coefficients this is particularly evident in the agriculture sector see fig 2 b three of the six indicators coefficients are always 0 this annihilation of indicators is a common feature of a ls because of the bounding of variables this is likely to be a manifestation of collinearity highlighting the importance of variable stability as one of our model outcomes in summary we observe in fig 2 that et is the most stable compared to rf and ls 3 1 3 over fitting ls had the least over fitting compared to et and rf that is the maximum difference in mape between training and testing in ls was 1 9 compared to 3 1 for both et and rf in agriculture and was 2 8 compared to 4 2 for rf and 3 6 for et in energy see table 3 that said all models performed well as demonstrated by means in table 2 these values are acceptable in that similar or higher values of over fitting are expressed in chen et al 2018 table 4 recall from section 2 3 2 that there are only 31 objects in our dataset it is well known that over fitting is more common for smaller samples yet we successfully mitigated over fitting as above moreover aside from chen et al 2018 we found it is not common in the literature to explicitly analyse over fitting at all for example mutascu 2022 kotlar et al 2022 jayaraman et al 2022 ulku and ulku 2022 do not mention over fitting the study wen and yuan 2020 mentions over fitting and the use of training and testing sets but does not report any measures of over fitting 3 1 4 selection our selection protocol is as follows given the models ls rf and et remove the model performing worst in terms of prediction accuracy from the remaining two models remove the model performing worst in terms of model stability the remaining model is selected provided it is not over fitting over fitting is regarded as a lower priority compared to accuracy because prediction accuracy is measured on the test set so it mitigates over fitting implicitly also over fitting is regarded as a lower priority than stability since an unstable model cannot be explained in section 4 1 5 we compare our selection protocol to that in chen et al 2018 we apply the selection protocol now the measures in table 2 demonstrate ls is the least accurate so we remove ls from the running the accumulated length of rf box and whiskers fig 2 is greater than that of et so rf is removed for being less stable than et the remaining model is et and we recall that its over fitting values were acceptable 3 2 uncertainty in model compared to inter annual variation throughout section 3 1 we recorded uncertainties of our models from the point of view of the model outcomes established in section 2 3 3 in particular this involved optimising prediction accuracy by minimising residuals however no matter how small the residuals are an outstanding concern is whether the residuals exceed typical levels of inter annual variation of nzggi emissions estimates in which case the predictions would be useless since the accuracy of the predictions would be outweighed by expected variation between years we have mitigated this concern by comparing our residuals to inter annual variation measured in terms of year on year differences in nzggi emissions estimates for both sectors the maximum upper quartile median and lower quartile values are all significantly lower for our residuals than for inter annual variation the following values are given as percentages of the average value for the sector for the energy sector the lower quartile median and upper quartile for our residuals are 0 54 1 73 and 2 89 respectively whereas they are 1 20 4 08 and 5 04 for inter annual variation for the agriculture sector the lower quartile median and upper quartile for our residuals are 0 26 0 54 and 1 27 respectively whereas they are 0 47 1 00 and 1 71 for inter annual variation that is we find the uncertainty of the model tends to be significantly below typical inter annual variation we also consider the following year wise paired view comparing inter annual variation to our residuals for each year we divide the relevant value of inter annual variation with our residual that year and compute the median of these fractions the median for the energy sector was 1 804 meaning inter annual variation tends to be 80 higher than our residuals while the median for the agriculture sector was 1 322 meaning inter annual variation tends to be 32 higher than our residuals 3 3 explanation of model our selected model et does not presume a data model hence in order to check the model is sensible can be explained we must infer relationships between inputs and outputs a posteriori a relationship is inferred between an indicator and its sector if a multivariate trend is present we find every indicator has an inferred relationship with its sector each inferred relationship is tested for outliers by calculating mahalanobis distances ghorbani 2019 and calculating associated p values on a χ 2 distribution with 1 degree of freedom then inferred relationships and their outliers are explained using subject knowledge inferred relationships must be understood in the context of the indicators used changing the selection of indicators would lead to variation in the inferred relationships in this way our methodology has a limited capacity to make precise absolute claims about drivers of emissions with this in mind we give two examples of inferred relationships and their explanations the indicator exf food exports for horticulture for agriculture has a logarithmic trend see fig 3 a this is to be expected in horticulture since smaller farms are known to have higher average costs which are associated to emissions thus as production increases we would expect emissions to increase at a high rate initially and then at a lower rate for higher levels of production outliers in this inferred relationship occurred between years 2001 and 2006 this coincides with a surge of agriculture emissions this surge is largely due to enteric fermentation in non dairy cattle category code 3 a 1 in the nzggi ministry for the environment 2021 however exf is not related to non dairy cattle so it is natural that the indicator could have as low values as it does in spite of the surge all indicators had unsurprisingly positive inferred relationships with emissions except for the following example on average predicted agriculture emissions decreased while mea weight of meat produced at export inspected facilities increased see fig 3 b since every animal is a source of emissions if animals are removed from the stock at a higher and higher rate in order to support higher levels of meat production this must lead to a reduction in emissions with all else being equal indeed other factors will come into play in order to support the continuous turnover of the stock recognising such other factors would be the role of other indicators like cow which counts the total number of dairy cows therefore we stand by the negative relationship observed between mea and emissions in the agriculture sector we also emphasise that we do not imply there is a direct causal relationship between animals lives being ended and a decrease in emissions rather there is an explainable relationship that should be viewed not in isolation but in the context of other interactive factors in the model in this way the negative relationship between agriculture emissions and meat production can be explained no outliers were identified in this inferred relationship 3 4 preliminary forecast of new zealand greenhouse gas inventory we observe similar trends between our emissions estimates and the nzggi see fig 4 however our model underestimates the magnitude of peaks and troughs in the agriculture sector and to a lesser degree in the energy sector this is a known weakness of machine learning in predicting extremes kotlar et al 2022 we estimate agriculture emissions in 2021 were 39 286 ktco2 e compared to 39 299 ktco2 e in 2020 which is a decrease of about 0 03 we estimate energy emissions in 2021 were 32 388 ktco2 e compared to 32 069 ktco2 e in 2020 which is an increase of about 0 99 3 5 energy emissions in 2020 covid 19 lockdown we estimate total energy emissions were 32 576 ktco2 e in 2019 higher than the 2020 figure above but only by about 1 58 this comes in spite of several covid 19 lockdowns indicators in our model for energy appeared to recover from their shocks too quickly for lockdowns to have lasting impact for the year of 2020 for example consider transport which accounted for 41 88 of energy emissions in 2020 ministry for the environment 2022b changes in state highway traffic volumes at selected telemetry sites across the country a component of indicators in our model for energy were closely related to changes in covid 19 alert levels see fig 5 thus traffic volumes recovered before the end of the year as did emissions in the energy sector 3 6 link between emissions and economic productivity because we maintain our indicator data on a daily frequency our model can make predictions on a daily basis of emissions occurring during 365 day windows which aligns with the nzggi on 31 december each year an advantage this gives us is visualising more granular trends than if we only had annual predictions we demonstrate the value of this through an example an historical comparison to the global financial crisis gfc suggests a continued link between emissions and economic productivity observe in fig 6 that the steep descent of emissions during the gfc beginning in 2008 has a similar gradient to that of covid 19 lockdown from 25 march to 26 april 2020 given both the gfc and covid 19 lockdowns are associated to significant economic impact this raises the question of how exactly emissions is associated with economic activity the release of emissions budgets by the new zealand government in 2021 highlighted a turning point for the dissociation of economic activity and emissions budgets can be monitored closely with our dynamic estimates 4 discussion 4 1 strengths 4 1 1 meeting the needs of nowcasting the outputs of the model are daily predictions of cumulative emissions in 365 day periods per sector which coincide with the national inventory at the end of each year this comes with a time lag of about two months given our current indicator data therefore our model addresses four of five needed directions of improvements to emissions statistics demanded in iveroth et al 2022 time lag frequency sector level activity and accuracy discussed in section 3 1 1 among our other model outcomes the fifth direction is coverage of sources of emissions which we discuss in section 4 2 4 thus we meet a growing demand for nowcasting see references in section 1 4 1 2 repeatability our emissions estimates anchor to and complement the nzggi a key strength is repeatability for every new edition of the nzggi we hope this continuity of information on emissions will hold policies and actions to account 4 1 3 comparison to methodologies using bespoke inventories innovative techniques have been developed to investigate covid 19 impacts on daily emissions forster et al 2020 liu et al 2020a b oda et al 2021 quéré et al 2020 zheng et al 2020 with impressive results e g daily emissions with a regional component separating co2 and air pollutants huang et al 2021 however these studies are limited by depending on bespoke and labour intensive daily inventories of emissions to mitigate the labour they compensate in various ways firstly the studies focus on short windows of time whereas our work reports from 1990 up to two months prior to the present day greenhouse gas emissions systems change gradually so our long term perspective is valuable secondly scaling repeating methods in these studies would be costly since it would require curating another bespoke inventory in particular scaling the methods to a national inventory like the nzggi is hard to foresee the 15 month delay before each edition of the nzggi is released suggests the annual nzggi is labour intensive enough lastly the daily inventories used in these studies try to reproduce third party estimates of countries national emissions rather than official estimates this leads to a fragmented picture of emissions our methodology aligns with the existing national inventory 4 1 4 local quarterly emissions reports statistics new zealand statsnz has begun experimental reporting on quarterly emissions statistics new zealand 2021 we comment on distinctions between our methodologies here statsnz reports are more dynamic in one way than ours estimating emissions occurring within any given quarter whereas our estimates are always for 365 day periods however our estimates are more dynamic than theirs in the sense that we provide daily predictions of annual periods lastly statsnz reports currently involve a different classification of industries compared to the sectors in the nzggi 4 1 5 novelty of our approach the review by debone et al 2021 of 776 articles on emissions predictions suggests tree based models are not the most widespread we did find a handful of studies using tree based models for emissions predictions chen et al 2018 jayaraman et al 2022 kotlar et al 2022 lin et al 2021 wen and yuan 2020 yang and zhou 2020 but their applications vary from ours hence our research is novel in terms of the application of tree based models to estimating the nzggi furthermore the performance of our methods suggests that techniques that can robustly handle data that potentially violates the assumptions of regression are most suitable such as random forest and extra trees the most similar studies to ours by methodology and application were chen et al 2018 jayaraman et al 2022 ulku and ulku 2022 we remark on points of difference that are strengths of our approach re training our model day after day is operationally necessary to incorporate the latest data and give the latest estimates this raises a question of the stability of the structure of the model between training which was assessed in section 3 1 2 the comparison studies do not assess stability our selection protocol from section 3 1 4 is decision driven instead of quantitatively driven as in chen et al 2018 figure 5 which is more definitive but less flexible than ours that is chen et al 2018 only accounts for average performance whereas our protocol allows us to take into account the distribution of performance throughout simulations this is important because model selection has its subtleties regarding over fitting ng 1997 provides evidence that finding the optimal model is not the same as finding the model with the least over fitting we discuss a blend of the protocols in section 4 2 1 recall our selection protocol is based on three model outcomes accuracy stability and over fitting cross validation is used to gather information these outcomes compared to similar studies jayaraman et al 2022 ulku and ulku 2022 our analysis and results 1 employ a more thorough cross validation approach section 2 3 2 2 demonstrate higher accuracy relative to contemporary methods referenced in section 3 1 1 and 3 assess the extent of over fitting section 3 1 3 4 2 limitations limitations are considered in three types methodological limitations ways we are limited due to our approach in theory operational limitations limited availability of data and explanatory and predictive limitations limitations on the how well the model explains the system at hand and on the predictions we can make with the model 4 2 1 methodological limitations a key limitation of the methodology is that it treats high level aggregates directly by taking sectors emissions as dependent variables we could enhance the model by partitioning sectors whenever we find that a source can be modelled separately into the source and not the source parts the extreme case of this approach would be to model every source and sink individually also the methodology estimates co2 e variables directly modelling each greenhouse gas type is of interest from both a policy and practical perspective our model estimates the nzggi which carries the imperfections of the ipcc guidelines it follows yona et al 2020 thus our model limited in every way the nzggi is limited as described in section 4 1 5 the protocol we use to select our preferred model is less definitive than in chen et al 2018 in order to make our protocol more definitive but maintain a view of the variation of model performance between simulations future research could investigate the following approach define an overall performance score by computing differences between upper and lower quartiles for each model outcome accuracy stability and over fitting multiplying the difference by the mean of the model outcome when the model outcome is supposed to be minimised accuracy and stability and taking a weighted sum of these components the optimal model for a given sector would be the model that minimises this score for that sector if different sectors have different optimal models the score could be broadened to aggregate over sectors in order to determine a general optimal model 4 2 2 operational limitations data availability is the main limitation to the operational quality of the model in particular limited availability leaves us with a latency of two months for this reason we are always investigating new sources of dynamic and near real time data to improve the model 4 2 3 explanatory and predictive limitations whilst our model selection prioritises predictive power over explanatory power breiman 2001 we ensure that our models are explainable to this end we 1 selected indicators with known systematic relationships with emissions and 2 verified that the relationships between indicators and predictions were sensible future research will involve increasing the explanatory power of the model in order for prediction accuracy statistics in table 2 to apply to future estimates we assume there is no change from one nzggi edition to the next we measure how bad an assumption this is by the extent of the changes between editions for example from the 2018 edition to the 2019 edition in the agriculture sector the emissions for each year reported on changed by between 4 2 and 5 2 whereas from the 2019 edition to the 2020 edition in the energy sector the reports changed by between 0 5 and 1 6 this presents a moving target for our model estimating compound uncertainty due to these nzggi amendments is non trivial because amendments are non uniform as noted above reports for some years are amended to be higher while some years are amended to be lower this dependence on the inventory at hand is a common disadvantage across any model estimating an inventory that updates over time however the advantage of using an updating inventory is the longevity of the applicability of our model that is since our model can be applied to any edition of the nzggi we continually revise our analysis according to new editions of the nzggi each year 4 2 4 indicator limitations the quality of any model depends on the quality of the data to which it is fit therefore it is important to note limitations of our indicators their constructions are described in appendix they have a limited coverage of the many sources of emissions and coverage is an important direction for improvement of emissions information iveroth et al 2022 screening a larger number of indicators when data becomes available will be beneficial for the coverage of our model and a formal indicator selection such as in wen and yuan 2020 will be necessary many indicators do not account for emissions intensities multiplying indicator values by a coefficient from an ipcc look up table is not useful because models tend to be mostly scale independent thus a time dependent analysis of emissions intensity for the given activity is necessary we present a preliminary model with a limited number of indicators covering only energy and agriculture we recognise the importance of dynamically modelling the remaining sectors of the nzggi fortunately the majority of the work has been done resulting in a generic sector model modelling the remaining sectors only requires trialling new data sources when they become available and assessing their outcomes 5 conclusions this study develops a proof of concept that there is a framework by which machine learning can be used in combination with a small number of indicators to model sector level emissions estimates at a higher frequency and lower latency to complement the new zealand greenhouse gas inventory nzggi and any other relevant methodologies currently investigating greenhouse gas emissions in new zealand estimates are reported every day for the cumulative emissions in the 365 day period ending that day which gives a more continuous stream of emissions statistics with a time lag of approximately two months the high frequency allows us to compare shocks that take effect in time periods shorter than one year for example our results show the covid 19 lockdown from 25 march to 26 april 2020 in new zealand coincided with a rate of decrease in emissions similar to that of the global financial crisis of 2008 the low latency allows us to forecast up to 2021 the latest edition of the nzggi reporting only up to 2020 to estimate total emissions in the agriculture and energy sectors we estimate agriculture emissions in 2021 were 39 286 ktco2 e and energy emissions in 2021 were 32 388 ktco2 e which represents a decrease of 0 03 and an increase of 0 99 respectively since 2020 in the face of rapid policy and technological change these insights based on our model can improve current understanding of emissions in a landscape changing day to day we evaluate uncertainties in our methodology and performance of a number of models in terms of prediction accuracy stability over fitting and by comparison to typical inter annual variation this assessment is more rigorous than contemporary studies in many ways the selected model carries an error margin of 1 314 for agriculture and 3 194 for energy median during simulation study the model is explainable in the sense that indicators used have systematic relationships with the emissions in their sectors and the model fits sensible quantitative relationships between each indicator and the predictions it makes that said the model is not designed to tell us which indicators of emissions are stronger drivers than others this study has highlighted the demand for the release of relevant data sets in a more timely fashion more granular data would lead to significant improvements to the model we also emphasise the need for the continuation of a bottom up emissions model as presented in national greenhouse gas inventories this would provide a test bed for a more dynamic approach and to directly measure the role of technological advances across the broad spectrum of emissions we note that although methodologies provided in this paper were able to estimate sub annual inventory style estimates within new zealand transferring this methodology to other countries would prove challenging due to the nature of data used in this analysis our methodology and emissions estimates will continue to be updated and developed to assess the ongoing variability in new zealand s emissions further research will be undertaken for various purposes as discussed in section 4 not least of which in order to estimate emissions by gas type providing direct comparison against progress towards split gas targets and regional emissions estimates that support more direct action at a local level we also plan to expand our estimates to other sectors of the nzggi although note that our current methodology dynamically reports on sectors making up 90 0 of new zealand s emissions declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank the wider dot loves data team for their ongoing support and advice during the development of the dynamic carbon tracker especially moritz lenz for important guidance in the literature and methodological support we would also like to thank louisa howse for their suggestions in the early stages of method development along with those at transpower and emstradepoint for their continued interest the project has been jointly funded by transpower callaghan innovation and dot loves data appendix indicators table 1 collates details of indicators used as independent variables in sector models descriptions of these indicators are given in the following subsections a 1 liquid and electric energy this indicator comprises of an aggregate of traffic volumes and the estimated carbon emissions due to national electricity generation dataset energy market services 2022 each component is described below the increasing rate of electric vehicles draws a systematic connection between the components which partly motivates their aggregation the components are normalised and added together with equal weight to give liquid and electric energy 2 2 this aggregation was undertaken initially to avoid collinear indicators when we were considering only models that demanded this since the introduction of the tree based models it is now feasible to remove this aggregate which we will undertake in future research equal weighting is chosen to give the model equal view of both variables since we are interested in trends rather than magnitude of sources of emissions traffic volumes variable waka kotahi new zealand transport agency provides an application programming interface for daily traffic volumes at sites around the country dataset new zealand transport agency 2022b dating back to 2018 we restrict our attention to telemetry sites because of their accuracy and frequency we extend the data back to 1990 by matching it with annual historic records in pdfs and xlsxs dataset new zealand transport agency 2019 this gives a unified record of traffic volumes with many site descriptions a difficulty is that the description of a given site can change over time to connect the traffic volumes per site in spite of changing descriptions we built an adjacency table between the site descriptions and a manual selection of keywords e g ngauranga drury and taupiri where a keyword is adjacent to a site description if the key word appears in the site description sites being established or disestablished over time could lead to over or under estimates respectively of the increase in traffic volumes over time for this reason we restricted to only those sites that have run continuously from 1990 to the present day the traffic volume variable is the time series given by summing the series for each of these sites electricity generation variable the estimated carbon emissions due to national electricity generation is defined in the em6 application programming interface integration guide dataset energy market services 2022 a 2 emissions intensity from new vehicle registrations this indicator captures the emissions potential of new zealand s vehicles based on motive power year of manufacture and gross vehicle mass previous studies have highlighted the importance of including a view of the country s vehicle fleet gonzález sánchez and martín ortega 2020 vehicles in the motor vehicle register dataset new zealand transport agency 2022a are assigned an emissions potential kgco2 e km based on locally approved ipcc developed emissions factors ministry for the environment 2022a the emissions factors consider motive power petrol diesel petrol hybrid electric lpg plugin petrol hybrid petrol electric hybrid and vehicle year assuming a gross vehicle mass of 2000 3000 kg we assign to each vehicle the relevant emissions factor after adjusting for its actual gross vehicle mass the adjustment applies the principle that fuel economy is proportional to vehicle mass which is comparable to the proportions based approach to understanding environmental impact of vehicles in terms of their mass pero et al 2017 other studies use a congestion index liu et al 2020b and may match traffic flow data with a vehicle database to differentiate gasoline from diesel powered vehicles huang et al 2021 our view has an advantage over these methodologies by differentiating vehicles by more motive power categories as well as by vehicle age and gross vehicle mass a 3 emissions associated to national traffic volume this indicator is the product of the emissions potential of the fleet and the traffic volumes variable from appendix a 1 estimating the emissions potential of the fleet involved estimating the half life of a vehicle on new zealand roads and using this in an exponentially weighted mean over newly registered vehicles each carrying their own emissions potential recall our indicator of emissions intensity of vehicles registered onto new zealand roads from appendix a 2 based on the new zealand motor vehicle register mvr a limitation of this indicator is that we do not know how long these vehicles spend on the road there does exist a dashboard tool that provides a vehicle status feature for the mvr at an aggregated level which tells whether a vehicle is currently active or inactive however the api that provides the mvr to us vehicle by vehicle does not include vehicle status therefore this feature was not readily available to us in order to calculate how long vehicles spend on the road instead we opted to use more readily available data for this purpose our estimate of the half life of a vehicle is as follows we record how long the vehicles were not on new zealand roads by taking the mean difference in features from the mvr vehicle year and first nz registration year we denote this mean difference by y b f for years before registration next a local scrappage report ministry of transport 2009 can indicate typical ages of vehicles when they are de registered which reported an average of 16 7 years and 18 6 years in wellington and christchurch respectively thus we take mean 16 7 18 6 y b f to be the half life of vehicles on new zealand roads then we use the above half life to take the exponentially weighted moving mean of the emissions intensity from new vehicle registrations indicator this constitutes an exponential decay model in terms of the average lifespan of vehicles there is a precedent for this in the literature for example oguchi and fuse 2015 equation 1 this gives a variable indicative of the fleet s emissions potential over time the product of the fleet emissions potential and the state highway traffic volumes variable forms this indicator the half life used above is a rough estimate sufficient only for this proof of concept in future research we plan to investigate refining our estimate of half life this could involve accessing the data in the mvr behind the dashboard mentioned above that provides vehicle status as well as providing a national view of vehicle activity this could be updated whenever the mvr is updated which would be more dynamic that above where the half life estimate is deduced from a historic scrappage report ministry of transport 2009 a 4 weight of coal production we extract quarterly values of total coal production from table 1 quarterly tonnes of the quarterly coal supply transformation consumption tonnes publicly available spreadsheet data tables for coal dataset ministry of business innovation and employment 2022 coal is used on a significant scale for energy in the metal mineral chemical and manufacturing industries and construction ministry for the environment 2020 moreover an increase in coal fired electricity generation drove an increase in public electricity and heat production emissions between 2018 and 2019 ministry for the environment 2020 a 5 number of dairy cattle we extract annual values for total dairy cattle including bobby calves from dataset statistics new zealand 2022a in 2020 enteric fermentation by dairy cattle made up 35 6 of emissions in the agriculture sector and has increased by 65 5 since 1990 ministry for the environment 2022b this increase is primarily driven by increases in dairy cattle numbers ministry for the environment 2020 page 11 which is the purpose of this indicator a 6 weight of meat produced at export inspected facilities we collate weekly weight statistics of beef and sheep meat production for export reported by the new zealand meat board from certain export inspected processing facilities dating back to the 2017 18 season we sum the weight of all types of meat produced across the country the series is retrofitted back to 1990 with monthly values of total above livestock excluding game from dataset statistics new zealand 2022d a 7 quantity of imported fertilisers we extract monthly values for the quantity of imported fertilisers as in dataset statistics new zealand 2022c use of synthetic nitrogen fertilisers is a driver of emissions trends in agriculture ministry for the environment 2020 page 71 the use of fertiliser consumption as an indicator has been effective in amaefule et al 2022 a 8 food exports food export data is used as a general measure of food production the indicators food exports for meat food exports for aquaculture and dairy and food exports for horticulture are given by extracting monthly values for live animals meat and edible meat offal fish crustaceans molluscs dairy produce and other animal products and vegetables fruit and prepared foodstuffs beverages and tobacco respectively from dataset statistics new zealand 2022b we acknowledge that that this is a price based indicator and that price variations as well as production amounts are therefore included further development would be to inflation adjust this indicator to isolate the production component or replace it entirely with a mass based food export dataset 
25377,as efforts to mitigate the effects of climate change grow reliable and thorough reporting of greenhouse gas emissions are essential for measuring progress towards international and domestic emissions reductions targets new zealand s national emissions inventories are currently reported between 15 to 27 months out of date we present a machine learning approach to nowcast dynamically estimate national greenhouse gas emissions in new zealand in advance of the national emissions inventory s release with just a two month latency due to current data availability key findings include an estimated 0 2 decrease in national gross emissions since 2020 as at july 2022 our study highlights the predictive power of a dynamic view of emissions intensive activities this methodology is a proof of concept that a machine learning approach can make sub annual estimates of national greenhouse gas emissions by sector with a relatively low error that could be of value for policy makers graphical abstract abbreviations nzggi new zealand greenhouse gas inventory ipcc intergovernmental panel on climate change ghg greenhouse gas ls bounded variable least squares rf random forest et extra trees mape mean absolute percentage error keywords random forest extra trees emissions prediction dynamic carbon tracker energy agriculture data availability the data used in this research is either publicly available via the cited source or the data was made available to the researchers through a data commercialisation agreement with one of the data partners of dot loves data the data provided by the data commercialisation agreement is therefore unavailable to access 1 introduction emissions from human activity are the key driver of the anthropogenic climate change observed since the onset of the industrial revolution c 1850 in an effort to mitigate our influence over the global climate the paris climate agreement aims to limit global temperature rise to well below 2 c and pursue efforts to limit warming to 1 5 c above pre industrial levels requiring significant action to reduce greenhouse gas emissions to net zero on an international scale reliable and thorough greenhouse gas inventories are fundamental in measuring progress towards these targets and provide insight into domestic and international emissions reduction policies and strategies yona et al 2020 guidelines for such inventories are published by the intergovernmental panel on climate change ipcc and were refined in 2019 the first major update since 2006 calvo buendia et al 2019 we develop a proof of concept that combines machine learning methods with indicators of dominant trends in new zealand s emissions profile in order to improve timeliness of emissions reporting with relatively low error and therefore nowcast new zealand s emissions in the sense described in iveroth et al 2022 the importance of greater timeliness is also highlighted in yona et al 2020 new zealand s national emissions are reported in the new zealand greenhouse gas inventory nzggi it is the official annual report of all anthropogenic emissions and removals of greenhouse gases ghgs associated with production which is currently released approximately 15 months after the end of the calendar year being reported on as determined by international reporting guidelines ministry for the environment 2021 multiple ghg types estimated by the nzggi are aggregated in terms of co2 equivalents co2 e 1 1 various research debates ways of aggregating ghgs see rogelj and schleussner 2021 and references therein we are bound to co2 e since it is used by the nzggi our indicator based model estimates co2 e directly rather than modelling each ghg type to compensate for limited data availability national emissions inventories are time consuming to produce at any given time national ghg emissions reported in the nzggi are between 15 and 27 months out of date methodologies for the release of quarterly and more timely emissions estimates are in the process of being developed in a number of countries huang et al 2021 iveroth et al 2022 statistics netherlands 2021 within new zealand statistics new zealand are developing quarterly emissions reporting statistics new zealand 2021 but as we discuss in section 4 1 4 this is not comparable to the nzggi in many ways this existing gap highlights the need for a more dynamic dataset that complements the nzggi to be developed numerous studies exist that estimate daily emissions i e emissions occurring within any given day by first developing a daily inventory of emissions forster et al 2020 develops a bottom up inventory using national mobility data finding that emissions reduced during the covid 19 pandemic by as much as 30 during april 2020 liu et al 2020b constructs a measure of co2 emissions by developing an inventory by country comprised of the sectors that are power generation industry transportation and household consumption each broken down by the three fossil fuel types which are coal oil and natural gas findings include that co2 emissions decreased by 8 8 in the first half of 2020 compared to the same period of the previous year in china quéré et al 2020 again develops an inventory using a bottom up approach comprised of six sectors finding that daily co2 emissions decreased by 17 by april 2020 compared to the mean 2019 levels with surface transport being the sector that influenced reduction most heavily further studies that estimate daily emissions by developing a daily inventory of emissions include liu et al 2020a oda et al 2021 zheng et al 2020 such inventories may be labour intensive to curate even more so than the nzggi at the annual level these studies have impressively specific results such as daily emissions estimates with a regional component while separating co2 and air pollutants huang et al 2021 however they need to focus on short windows of time to obtain a sufficiently detailed inventory in the first place this distinguishes our work which reports on emissions from 1990 up to two months prior to the present day we produce daily estimates of cumulative ghg emissions in every 365 day period since 1990 with a latency of less than two months in other words we produce daily estimates of annual emissions where annual is thought of as a rolling 365 day window since we provide daily estimates of annual emissions the estimate occurring on the last day of a given year is an estimate for the cumulative emissions that year this figure is comparable to the annual value coming from the nzggi for that year we compare a number of potential models in terms of prediction accuracy model stability and over fitting the approach can be applied to any sector and has so far been applied to the energy and agriculture sectors of the nzggi ministry for the environment 2021 these sectors were chosen to begin with for their relative contribution to the nzggi making up 90 0 of emissions in 2020 ministry for the environment 2022b the input variables for each sector are called indicators there is a growing precedent that a relatively small number of indicators can be sufficient to model broad greenhouse gas emissions systems recent examples including amaefule et al 2022 ulku and ulku 2022 using this approach we are able to model the energy and agriculture sectors with just four and six indicators respectively our models do not assume a relationship between indicators and the emissions of their sectors thus to explain the inner workings of the model we have to infer relationships between indicators and predictions by the model we use subject knowledge to verify that these relationships are sensible in addition we ensure our methodology is repeatable between editions of the nzggi we assess uncertainties carefully in terms of accuracy stability and over fitting in sections 3 1 1 to 3 1 3 additionally we compare our uncertainties to typical inter annual variation in section 3 2 to mitigate the concern that the error due to our model exceeds expected changes from one year to the next results are derived in section 3 using both of our high frequency and low latency of emissions reporting the insights resulting from our models could improve current understanding of emissions in a landscape changing day to day 2 methods 2 1 overview we use cross validation to compare a number of potential models that predict nzggi sector emissions using data on variables related to sector emissions a model selection protocol based on prediction accuracy model stability and over fitting is used to decide a preferred model we apply the preferred model to estimate the energy and agriculture sectors of the nzggi ministry for the environment 2021 these sectors were chosen to begin with for their size together making up 90 0 of emissions in 2020 ministry for the environment 2022b we call the independent variables used for each sector indicators which we describe in detail in section 2 2 our models are non parametric in the sense that we do not assume a data model thus we face the risk of selecting a model that cannot be explained we combat this risk in section 3 3 the main objective of the model is to attain high frequency and low latency outputs because of limited data availability to achieve the objective we make some simplifying choices 1 as described above our dependent variables are sector emissions rather than estimating every source of emissions and then aggregating and 2 we estimate carbon dioxide equivalents directly rather than estimating each ghg type individually and then aggregating such as in huang et al 2021 ulku and ulku 2022 the nzggi updates annually the latest edition at the time of writing reports until 2020 however our analysis is applicable to all editions of the nzggi therefore we discuss the model mostly with respect to any edition of the nzggi any results presented pertain to the 2020 edition dataset ministry for the environment 2022 existing research involves data sets of a wide variety of frequencies and time spans the most similar studies to ours in these terms are jayaraman et al 2022 ulku and ulku 2022 wen and yuan 2020 which have an annual frequency and time spans of 1990 2018 1990 2019 and 1997 2017 respectively we have a time span of 1990 2020 when using the latest edition of the nzggi we collect new indicator data daily so that predictions using the model can be made daily this means our outputs not only estimate year end values that mimic the nzggi but also interpolate between those year end estimates and leave us with a low latency of less than two months in practice a more detailed description of our daily estimates of emissions is given in section 1 2 2 indicators indicators are times series associated with natural variables that are chosen as input variables for the model of a given sector there are many variables that could serve as indicators of emissions in our model the indicators discussed in this paper are non exhaustive and this model could in fact be replicated with an entirely different set we therefore do not claim they are the most important of the potential indicators but they are sufficient in the sense that they effectively model their sectors at this point in time and are available dynamically our indicators are chosen for 1 their systematic relevance to ghgs 2 a resulting relationship with sector emissions predictions that is sensible and 3 the availability of the necessary data the indicators used in the model are summarised in table 1 each indicator represents an emissions intensive activity e g kgco2 e per km of petrol fuelled vehicle travel intensity i e emissions per unit of activity helps to account for the necessary dissociation of emissions with activity and productivity if an indicator considers intensity then we say it is direct for example one of our indicators measures the emissions potentials of new vehicles on new zealand roads based on motive power year of manufacture and gross vehicle mass the data comes from a national vehicle register that is updated monthly therefore our model accounts for variation in emissions intensity over time via indicators such as these for more information see appendix a 2 if an indicator is not direct we say it is indirect we use a combination of direct and indirect indicators we check for updates to our indicator data on a daily basis to ensure the data is as up to date as possible where a dataset is not updated on a daily basis the dataset is held constant until the present day and interpolated linearly between historic data points in order to provide a daily value in the extreme case where a dataset is updated only annually this would mean the data is held constant for up to 365 days in appendix we describe the construction of each indicator in detail given the hundreds of sources of emissions featured in the nzggi the indicators presented in table 1 appear scarce we acknowledge there are many potential indicators and as new datasets emerge these may model the sectors better than those datasets currently being used while the indicators used here are not necessarily optimal they are sufficient for this proof of concept it is important to note as previously discussed that the benefit of this approach is the limited number of variables required within the model furthermore the performance of regression models is generally sensitive to collinearity between indicators farrar and glauber 1967 the scarcity of useful data makes it difficult to find indicators with low collinearity we note that there exists high collinearity between indicators as shown in fig 1 while methods exist to aggregate correlated variables we keep our indicators as individual inputs for simplicity therefore we will prefer machine learning models that have no such requirements around collinearity of indicators certain machine learning models also allow non linear relationship between indicators and sectors ulku and ulku 2022 this is valuable because including non linear indicators can be critical for prediction accuracy wen and yuan 2020 we note that it is not intuitive that there is a negative correlation between mea and exm however there are possible explanations for this relationship such as the export inspected facilities where mea is counted having a limited scope or the variation in the value of exports over time outweighing an actual decrease in quantity of meat exports this requires further investigation we discuss this further in section 3 3 2 3 model selection 2 3 1 models we compare three models in our selection process bounded variable least squares stark and parker 1995 ls random forests breiman 2001 rf and extra trees geurts et al 2006 et bounded variable least squares stark and parker 1995 ls solves the usual linear least squares problem with the added constraint that fitted coefficients lie within user specified bounds we use the python programming language van rossum and drake 2009 for our analysis in particular ls is implemented with a linear least squares solver from the scipy project virtanen et al 2020 for each indicator we set the bounds to either 0 or 0 depending on the known systematic relationship between indicator and emissions this aligns with the use of a relation expected sign as in gonzález sánchez and martín ortega 2020 table 3 amaefule et al 2022 use such bounds to classify indicators as enablers or de enablers significant statistical analysis is required to verify the use of ls as demonstrated in gonzález sánchez and martín ortega 2020 as mentioned in section 2 2 collinearity and non linearity of indicators makes this infeasible in our setting in selecting our optimal model we will select against ls anyway because it is outperformed by the other models in terms of prediction accuracy see table 2 this suggests that the latent relationships are complex and or non linear we therefore omit statistical details relevant only to ls random forest breiman 2001 rf is an ensemble method that builds a collection of decision trees by incorporating a random component into their construction and makes a prediction by aggregating the predictions of each tree the technique is non parametric in the sense that rf does not assume a data model however there are various hyper parameters for the structure of the forest and its trees these are manipulated in order to tune the model to optimise its predictions this is discussed in detail in section 2 3 2 we implement rf using the random forest regressor by scikit learn pedregosa et al 2011 we elaborate on the random component mentioned above in the construction of each tree to aid the description of the next model each decision tree in the forest involves a random sample of indicators and data points in rf these are sampled with replacement in our application we have only 31 data points to begin with each year from 1990 to 2020 hence we expect we cannot afford sampling with replacement in the random component described above this motivates us to use et extra trees geurts et al 2006 et is similar to rf but is different as follows while the trees in rf involve sampling with replacement the trees in et involve sampling without replacement thus each tree in et is likely to learn from a more comprehensive sample of data in a similar study to ours jayaraman et al 2022 rf did not perform exceptionally well placing fourth among eight models considered we show in section 3 1 1 that our implementation of rf is more accurate than jayaraman et al 2022 we observe in section 3 1 2 that stability issues of rf are resolved by our inclusion of et we use the extra trees regressor by pedregosa et al 2011 to implement et 2 3 2 cross validation over fitting occurs when a fitted model performs worse on unseen data than on its training data cross validation is a technique used to assess for over fitting we follow a standard workflow that incorporates hyper parameter tuning when applicable since the tree based methods have hyper parameters to be tuned cross validation occurs at two levels 1 validating to find the best hyper parameters and 2 testing to ensure the model is not over fitting after retraining on the best hyper parameters thus we use a workflow that is best described as repeated double cross validation by filzmoser et al 2009 see also huang 2022 figure 7 4 this means that at the lowest level the data is split into three parts pre training validating and testing we use the train test split utility by pedregosa et al 2011 with a test size of 6 out of the 31 data points this gives a train test ratio of about 80 20 which is the same ratio as in wen and yuan 2020 if the model on hand is rf or et then hyper parameters need tuning randomised search through a given hyper parameter space is used to find the best hyper parameters we use randomised search instead of its predecessor grid search because randomised search is known to be better at navigating situations where hyper parameters have differing importance bergstra and bengio 2012 figure 1 randomised search cross validation by pedregosa et al 2011 with 5 folds implements the randomised search which repeatedly splits the 25 training data points into 20 for pre training and 5 for validating the hyper parameter space used both for rf and et is defined with the ranges number of trees 200 201 219 maximum depth of tree 1 2 3 minimum number of samples required to split 2 3 4 and minimum number of samples required to be at a leaf node 2 3 4 these were chosen by running a large number of investigations to find a large hyper parameter space in which the models were stable in the sense we describe in section 2 3 3 fitting each model on a particular train test split of the data as described above defines a single simulation in our cross validation we run 30 such simulations this makes our cross validation more comprehensive than jayaraman et al 2022 ulku and ulku 2022 which consider only one simulation each 2 3 3 model outcomes the objective is to select the optimal model in terms of certain model outcomes the model outcomes used in chen et al 2018 are over fitting prediction accuracy and execution time the bounds on the parameters described earlier ensure that the use of the attributes remains aligned with any latent data structures execution time is not important to us since our data sets are small so we omit this outcome a novelty in our approach is to include an outcome about the stability of the structure of the model different models have different structures so the notion of stability has to adapt to each model for ls we consider the distribution during cross validation of the fitted coefficients in the linear model for rf and et instead of coefficients we consider the values of indicator importance in the tuned estimator a model is considered more stable if it has a smaller variation in the distribution above these are described in more detail in section 3 1 2 and pictured in fig 2 thus our objective is to find the optimal model in terms of the model outcomes 1 prediction accuracy in terms of mean absolute percentage error mape 100 n x x test x x ˆ x where x ˆ is the prediction for each x in a given test set x test of size n in cross validation like in chen et al 2018 2 model stability in terms of distribution of fitted coefficients indicator importance as described above and 3 over fitting in terms of the average difference of mape between training and testing data like chen et al 2018 3 results 3 1 framework to assess models to predict sector level greenhouse gas emissions in national inventories cross validation was applied to compare a range of models of greenhouse gas emissions by sector in terms of a comprehensive list of model outcomes section 2 3 prediction accuracy stability and over fitting a machine learning model extra trees section 2 3 1 was found to perform the best according to our selection protocol section 3 1 4 relationships between sector emissions and indicators were inferred from the model section 3 3 and subject knowledge was used to explain these relationships outliers in the relationships were identified via mahalanobis distance and explained in context 3 1 1 prediction accuracy statistics for prediction accuracy are given in table 2 which shows et is optimal across both sectors our results are comparable to those found in jayaraman et al 2022 due to the similarity in methods and initial data in both settings mape is used to measure error when predicting annual values of co2 e our models have resulted in higher accuracy than those presented jayaraman et al 2022 the comparable model to ls in jayaraman et al 2022 is elasticnet since it too is a regularised linear model the comparable model to both rf and et in jayaraman et al 2022 is random forest ls achieves a median mape of 2 0 in agriculture and 3 7 in energy whereas elasticnet achieves a mape of 19 462 in one application of the model and 41 619 in another jayaraman et al 2022 tables ii and iii et respectively rf achieves a median mape of 1 3 respectively 1 4 in agriculture and 3 2 respectively 3 6 in energy whereas random forest in jayaraman et al 2022 achieves a mape of 9 470 in one application and 21 522 in another jayaraman et al 2022 tables ii and iii our higher accuracy is possibly due to our use of hyper parameter tuning there is no discussion of tuning in jayaraman et al 2022 3 1 2 model stability recall from section 2 3 3 that ls respectively et and rf is considered to be more stable if it has smaller variation in the distributions of its fitted coefficients respectively indicator importance values these distributions are pictured in fig 2 we observe stability can vary in a number of ways firstly the order of indicators by median value depends on the model for example in rf the median value for fim quantity of imported fertilisers is slightly higher than that of cow number of dairy cattle whereas in et the box and whiskers for cow is entirely above the upper quartile for fim secondly notice simply how long the boxes and whiskers in some models are and therefore how unstable the models can be et has consistently short boxes and whiskers on the other hand in rf for energy the length of the box and whiskers for the two most important indicators are long especially in comparison to the box and whiskers for the same indicators under et this instability in rf is undesirable since it demonstrates high uncertainty about the impact of each indicator in the model lastly models can be more or less polarising about the coefficient importance of an indicator notice ls is very polarising since it tends to assign either very high or low coefficients this is particularly evident in the agriculture sector see fig 2 b three of the six indicators coefficients are always 0 this annihilation of indicators is a common feature of a ls because of the bounding of variables this is likely to be a manifestation of collinearity highlighting the importance of variable stability as one of our model outcomes in summary we observe in fig 2 that et is the most stable compared to rf and ls 3 1 3 over fitting ls had the least over fitting compared to et and rf that is the maximum difference in mape between training and testing in ls was 1 9 compared to 3 1 for both et and rf in agriculture and was 2 8 compared to 4 2 for rf and 3 6 for et in energy see table 3 that said all models performed well as demonstrated by means in table 2 these values are acceptable in that similar or higher values of over fitting are expressed in chen et al 2018 table 4 recall from section 2 3 2 that there are only 31 objects in our dataset it is well known that over fitting is more common for smaller samples yet we successfully mitigated over fitting as above moreover aside from chen et al 2018 we found it is not common in the literature to explicitly analyse over fitting at all for example mutascu 2022 kotlar et al 2022 jayaraman et al 2022 ulku and ulku 2022 do not mention over fitting the study wen and yuan 2020 mentions over fitting and the use of training and testing sets but does not report any measures of over fitting 3 1 4 selection our selection protocol is as follows given the models ls rf and et remove the model performing worst in terms of prediction accuracy from the remaining two models remove the model performing worst in terms of model stability the remaining model is selected provided it is not over fitting over fitting is regarded as a lower priority compared to accuracy because prediction accuracy is measured on the test set so it mitigates over fitting implicitly also over fitting is regarded as a lower priority than stability since an unstable model cannot be explained in section 4 1 5 we compare our selection protocol to that in chen et al 2018 we apply the selection protocol now the measures in table 2 demonstrate ls is the least accurate so we remove ls from the running the accumulated length of rf box and whiskers fig 2 is greater than that of et so rf is removed for being less stable than et the remaining model is et and we recall that its over fitting values were acceptable 3 2 uncertainty in model compared to inter annual variation throughout section 3 1 we recorded uncertainties of our models from the point of view of the model outcomes established in section 2 3 3 in particular this involved optimising prediction accuracy by minimising residuals however no matter how small the residuals are an outstanding concern is whether the residuals exceed typical levels of inter annual variation of nzggi emissions estimates in which case the predictions would be useless since the accuracy of the predictions would be outweighed by expected variation between years we have mitigated this concern by comparing our residuals to inter annual variation measured in terms of year on year differences in nzggi emissions estimates for both sectors the maximum upper quartile median and lower quartile values are all significantly lower for our residuals than for inter annual variation the following values are given as percentages of the average value for the sector for the energy sector the lower quartile median and upper quartile for our residuals are 0 54 1 73 and 2 89 respectively whereas they are 1 20 4 08 and 5 04 for inter annual variation for the agriculture sector the lower quartile median and upper quartile for our residuals are 0 26 0 54 and 1 27 respectively whereas they are 0 47 1 00 and 1 71 for inter annual variation that is we find the uncertainty of the model tends to be significantly below typical inter annual variation we also consider the following year wise paired view comparing inter annual variation to our residuals for each year we divide the relevant value of inter annual variation with our residual that year and compute the median of these fractions the median for the energy sector was 1 804 meaning inter annual variation tends to be 80 higher than our residuals while the median for the agriculture sector was 1 322 meaning inter annual variation tends to be 32 higher than our residuals 3 3 explanation of model our selected model et does not presume a data model hence in order to check the model is sensible can be explained we must infer relationships between inputs and outputs a posteriori a relationship is inferred between an indicator and its sector if a multivariate trend is present we find every indicator has an inferred relationship with its sector each inferred relationship is tested for outliers by calculating mahalanobis distances ghorbani 2019 and calculating associated p values on a χ 2 distribution with 1 degree of freedom then inferred relationships and their outliers are explained using subject knowledge inferred relationships must be understood in the context of the indicators used changing the selection of indicators would lead to variation in the inferred relationships in this way our methodology has a limited capacity to make precise absolute claims about drivers of emissions with this in mind we give two examples of inferred relationships and their explanations the indicator exf food exports for horticulture for agriculture has a logarithmic trend see fig 3 a this is to be expected in horticulture since smaller farms are known to have higher average costs which are associated to emissions thus as production increases we would expect emissions to increase at a high rate initially and then at a lower rate for higher levels of production outliers in this inferred relationship occurred between years 2001 and 2006 this coincides with a surge of agriculture emissions this surge is largely due to enteric fermentation in non dairy cattle category code 3 a 1 in the nzggi ministry for the environment 2021 however exf is not related to non dairy cattle so it is natural that the indicator could have as low values as it does in spite of the surge all indicators had unsurprisingly positive inferred relationships with emissions except for the following example on average predicted agriculture emissions decreased while mea weight of meat produced at export inspected facilities increased see fig 3 b since every animal is a source of emissions if animals are removed from the stock at a higher and higher rate in order to support higher levels of meat production this must lead to a reduction in emissions with all else being equal indeed other factors will come into play in order to support the continuous turnover of the stock recognising such other factors would be the role of other indicators like cow which counts the total number of dairy cows therefore we stand by the negative relationship observed between mea and emissions in the agriculture sector we also emphasise that we do not imply there is a direct causal relationship between animals lives being ended and a decrease in emissions rather there is an explainable relationship that should be viewed not in isolation but in the context of other interactive factors in the model in this way the negative relationship between agriculture emissions and meat production can be explained no outliers were identified in this inferred relationship 3 4 preliminary forecast of new zealand greenhouse gas inventory we observe similar trends between our emissions estimates and the nzggi see fig 4 however our model underestimates the magnitude of peaks and troughs in the agriculture sector and to a lesser degree in the energy sector this is a known weakness of machine learning in predicting extremes kotlar et al 2022 we estimate agriculture emissions in 2021 were 39 286 ktco2 e compared to 39 299 ktco2 e in 2020 which is a decrease of about 0 03 we estimate energy emissions in 2021 were 32 388 ktco2 e compared to 32 069 ktco2 e in 2020 which is an increase of about 0 99 3 5 energy emissions in 2020 covid 19 lockdown we estimate total energy emissions were 32 576 ktco2 e in 2019 higher than the 2020 figure above but only by about 1 58 this comes in spite of several covid 19 lockdowns indicators in our model for energy appeared to recover from their shocks too quickly for lockdowns to have lasting impact for the year of 2020 for example consider transport which accounted for 41 88 of energy emissions in 2020 ministry for the environment 2022b changes in state highway traffic volumes at selected telemetry sites across the country a component of indicators in our model for energy were closely related to changes in covid 19 alert levels see fig 5 thus traffic volumes recovered before the end of the year as did emissions in the energy sector 3 6 link between emissions and economic productivity because we maintain our indicator data on a daily frequency our model can make predictions on a daily basis of emissions occurring during 365 day windows which aligns with the nzggi on 31 december each year an advantage this gives us is visualising more granular trends than if we only had annual predictions we demonstrate the value of this through an example an historical comparison to the global financial crisis gfc suggests a continued link between emissions and economic productivity observe in fig 6 that the steep descent of emissions during the gfc beginning in 2008 has a similar gradient to that of covid 19 lockdown from 25 march to 26 april 2020 given both the gfc and covid 19 lockdowns are associated to significant economic impact this raises the question of how exactly emissions is associated with economic activity the release of emissions budgets by the new zealand government in 2021 highlighted a turning point for the dissociation of economic activity and emissions budgets can be monitored closely with our dynamic estimates 4 discussion 4 1 strengths 4 1 1 meeting the needs of nowcasting the outputs of the model are daily predictions of cumulative emissions in 365 day periods per sector which coincide with the national inventory at the end of each year this comes with a time lag of about two months given our current indicator data therefore our model addresses four of five needed directions of improvements to emissions statistics demanded in iveroth et al 2022 time lag frequency sector level activity and accuracy discussed in section 3 1 1 among our other model outcomes the fifth direction is coverage of sources of emissions which we discuss in section 4 2 4 thus we meet a growing demand for nowcasting see references in section 1 4 1 2 repeatability our emissions estimates anchor to and complement the nzggi a key strength is repeatability for every new edition of the nzggi we hope this continuity of information on emissions will hold policies and actions to account 4 1 3 comparison to methodologies using bespoke inventories innovative techniques have been developed to investigate covid 19 impacts on daily emissions forster et al 2020 liu et al 2020a b oda et al 2021 quéré et al 2020 zheng et al 2020 with impressive results e g daily emissions with a regional component separating co2 and air pollutants huang et al 2021 however these studies are limited by depending on bespoke and labour intensive daily inventories of emissions to mitigate the labour they compensate in various ways firstly the studies focus on short windows of time whereas our work reports from 1990 up to two months prior to the present day greenhouse gas emissions systems change gradually so our long term perspective is valuable secondly scaling repeating methods in these studies would be costly since it would require curating another bespoke inventory in particular scaling the methods to a national inventory like the nzggi is hard to foresee the 15 month delay before each edition of the nzggi is released suggests the annual nzggi is labour intensive enough lastly the daily inventories used in these studies try to reproduce third party estimates of countries national emissions rather than official estimates this leads to a fragmented picture of emissions our methodology aligns with the existing national inventory 4 1 4 local quarterly emissions reports statistics new zealand statsnz has begun experimental reporting on quarterly emissions statistics new zealand 2021 we comment on distinctions between our methodologies here statsnz reports are more dynamic in one way than ours estimating emissions occurring within any given quarter whereas our estimates are always for 365 day periods however our estimates are more dynamic than theirs in the sense that we provide daily predictions of annual periods lastly statsnz reports currently involve a different classification of industries compared to the sectors in the nzggi 4 1 5 novelty of our approach the review by debone et al 2021 of 776 articles on emissions predictions suggests tree based models are not the most widespread we did find a handful of studies using tree based models for emissions predictions chen et al 2018 jayaraman et al 2022 kotlar et al 2022 lin et al 2021 wen and yuan 2020 yang and zhou 2020 but their applications vary from ours hence our research is novel in terms of the application of tree based models to estimating the nzggi furthermore the performance of our methods suggests that techniques that can robustly handle data that potentially violates the assumptions of regression are most suitable such as random forest and extra trees the most similar studies to ours by methodology and application were chen et al 2018 jayaraman et al 2022 ulku and ulku 2022 we remark on points of difference that are strengths of our approach re training our model day after day is operationally necessary to incorporate the latest data and give the latest estimates this raises a question of the stability of the structure of the model between training which was assessed in section 3 1 2 the comparison studies do not assess stability our selection protocol from section 3 1 4 is decision driven instead of quantitatively driven as in chen et al 2018 figure 5 which is more definitive but less flexible than ours that is chen et al 2018 only accounts for average performance whereas our protocol allows us to take into account the distribution of performance throughout simulations this is important because model selection has its subtleties regarding over fitting ng 1997 provides evidence that finding the optimal model is not the same as finding the model with the least over fitting we discuss a blend of the protocols in section 4 2 1 recall our selection protocol is based on three model outcomes accuracy stability and over fitting cross validation is used to gather information these outcomes compared to similar studies jayaraman et al 2022 ulku and ulku 2022 our analysis and results 1 employ a more thorough cross validation approach section 2 3 2 2 demonstrate higher accuracy relative to contemporary methods referenced in section 3 1 1 and 3 assess the extent of over fitting section 3 1 3 4 2 limitations limitations are considered in three types methodological limitations ways we are limited due to our approach in theory operational limitations limited availability of data and explanatory and predictive limitations limitations on the how well the model explains the system at hand and on the predictions we can make with the model 4 2 1 methodological limitations a key limitation of the methodology is that it treats high level aggregates directly by taking sectors emissions as dependent variables we could enhance the model by partitioning sectors whenever we find that a source can be modelled separately into the source and not the source parts the extreme case of this approach would be to model every source and sink individually also the methodology estimates co2 e variables directly modelling each greenhouse gas type is of interest from both a policy and practical perspective our model estimates the nzggi which carries the imperfections of the ipcc guidelines it follows yona et al 2020 thus our model limited in every way the nzggi is limited as described in section 4 1 5 the protocol we use to select our preferred model is less definitive than in chen et al 2018 in order to make our protocol more definitive but maintain a view of the variation of model performance between simulations future research could investigate the following approach define an overall performance score by computing differences between upper and lower quartiles for each model outcome accuracy stability and over fitting multiplying the difference by the mean of the model outcome when the model outcome is supposed to be minimised accuracy and stability and taking a weighted sum of these components the optimal model for a given sector would be the model that minimises this score for that sector if different sectors have different optimal models the score could be broadened to aggregate over sectors in order to determine a general optimal model 4 2 2 operational limitations data availability is the main limitation to the operational quality of the model in particular limited availability leaves us with a latency of two months for this reason we are always investigating new sources of dynamic and near real time data to improve the model 4 2 3 explanatory and predictive limitations whilst our model selection prioritises predictive power over explanatory power breiman 2001 we ensure that our models are explainable to this end we 1 selected indicators with known systematic relationships with emissions and 2 verified that the relationships between indicators and predictions were sensible future research will involve increasing the explanatory power of the model in order for prediction accuracy statistics in table 2 to apply to future estimates we assume there is no change from one nzggi edition to the next we measure how bad an assumption this is by the extent of the changes between editions for example from the 2018 edition to the 2019 edition in the agriculture sector the emissions for each year reported on changed by between 4 2 and 5 2 whereas from the 2019 edition to the 2020 edition in the energy sector the reports changed by between 0 5 and 1 6 this presents a moving target for our model estimating compound uncertainty due to these nzggi amendments is non trivial because amendments are non uniform as noted above reports for some years are amended to be higher while some years are amended to be lower this dependence on the inventory at hand is a common disadvantage across any model estimating an inventory that updates over time however the advantage of using an updating inventory is the longevity of the applicability of our model that is since our model can be applied to any edition of the nzggi we continually revise our analysis according to new editions of the nzggi each year 4 2 4 indicator limitations the quality of any model depends on the quality of the data to which it is fit therefore it is important to note limitations of our indicators their constructions are described in appendix they have a limited coverage of the many sources of emissions and coverage is an important direction for improvement of emissions information iveroth et al 2022 screening a larger number of indicators when data becomes available will be beneficial for the coverage of our model and a formal indicator selection such as in wen and yuan 2020 will be necessary many indicators do not account for emissions intensities multiplying indicator values by a coefficient from an ipcc look up table is not useful because models tend to be mostly scale independent thus a time dependent analysis of emissions intensity for the given activity is necessary we present a preliminary model with a limited number of indicators covering only energy and agriculture we recognise the importance of dynamically modelling the remaining sectors of the nzggi fortunately the majority of the work has been done resulting in a generic sector model modelling the remaining sectors only requires trialling new data sources when they become available and assessing their outcomes 5 conclusions this study develops a proof of concept that there is a framework by which machine learning can be used in combination with a small number of indicators to model sector level emissions estimates at a higher frequency and lower latency to complement the new zealand greenhouse gas inventory nzggi and any other relevant methodologies currently investigating greenhouse gas emissions in new zealand estimates are reported every day for the cumulative emissions in the 365 day period ending that day which gives a more continuous stream of emissions statistics with a time lag of approximately two months the high frequency allows us to compare shocks that take effect in time periods shorter than one year for example our results show the covid 19 lockdown from 25 march to 26 april 2020 in new zealand coincided with a rate of decrease in emissions similar to that of the global financial crisis of 2008 the low latency allows us to forecast up to 2021 the latest edition of the nzggi reporting only up to 2020 to estimate total emissions in the agriculture and energy sectors we estimate agriculture emissions in 2021 were 39 286 ktco2 e and energy emissions in 2021 were 32 388 ktco2 e which represents a decrease of 0 03 and an increase of 0 99 respectively since 2020 in the face of rapid policy and technological change these insights based on our model can improve current understanding of emissions in a landscape changing day to day we evaluate uncertainties in our methodology and performance of a number of models in terms of prediction accuracy stability over fitting and by comparison to typical inter annual variation this assessment is more rigorous than contemporary studies in many ways the selected model carries an error margin of 1 314 for agriculture and 3 194 for energy median during simulation study the model is explainable in the sense that indicators used have systematic relationships with the emissions in their sectors and the model fits sensible quantitative relationships between each indicator and the predictions it makes that said the model is not designed to tell us which indicators of emissions are stronger drivers than others this study has highlighted the demand for the release of relevant data sets in a more timely fashion more granular data would lead to significant improvements to the model we also emphasise the need for the continuation of a bottom up emissions model as presented in national greenhouse gas inventories this would provide a test bed for a more dynamic approach and to directly measure the role of technological advances across the broad spectrum of emissions we note that although methodologies provided in this paper were able to estimate sub annual inventory style estimates within new zealand transferring this methodology to other countries would prove challenging due to the nature of data used in this analysis our methodology and emissions estimates will continue to be updated and developed to assess the ongoing variability in new zealand s emissions further research will be undertaken for various purposes as discussed in section 4 not least of which in order to estimate emissions by gas type providing direct comparison against progress towards split gas targets and regional emissions estimates that support more direct action at a local level we also plan to expand our estimates to other sectors of the nzggi although note that our current methodology dynamically reports on sectors making up 90 0 of new zealand s emissions declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank the wider dot loves data team for their ongoing support and advice during the development of the dynamic carbon tracker especially moritz lenz for important guidance in the literature and methodological support we would also like to thank louisa howse for their suggestions in the early stages of method development along with those at transpower and emstradepoint for their continued interest the project has been jointly funded by transpower callaghan innovation and dot loves data appendix indicators table 1 collates details of indicators used as independent variables in sector models descriptions of these indicators are given in the following subsections a 1 liquid and electric energy this indicator comprises of an aggregate of traffic volumes and the estimated carbon emissions due to national electricity generation dataset energy market services 2022 each component is described below the increasing rate of electric vehicles draws a systematic connection between the components which partly motivates their aggregation the components are normalised and added together with equal weight to give liquid and electric energy 2 2 this aggregation was undertaken initially to avoid collinear indicators when we were considering only models that demanded this since the introduction of the tree based models it is now feasible to remove this aggregate which we will undertake in future research equal weighting is chosen to give the model equal view of both variables since we are interested in trends rather than magnitude of sources of emissions traffic volumes variable waka kotahi new zealand transport agency provides an application programming interface for daily traffic volumes at sites around the country dataset new zealand transport agency 2022b dating back to 2018 we restrict our attention to telemetry sites because of their accuracy and frequency we extend the data back to 1990 by matching it with annual historic records in pdfs and xlsxs dataset new zealand transport agency 2019 this gives a unified record of traffic volumes with many site descriptions a difficulty is that the description of a given site can change over time to connect the traffic volumes per site in spite of changing descriptions we built an adjacency table between the site descriptions and a manual selection of keywords e g ngauranga drury and taupiri where a keyword is adjacent to a site description if the key word appears in the site description sites being established or disestablished over time could lead to over or under estimates respectively of the increase in traffic volumes over time for this reason we restricted to only those sites that have run continuously from 1990 to the present day the traffic volume variable is the time series given by summing the series for each of these sites electricity generation variable the estimated carbon emissions due to national electricity generation is defined in the em6 application programming interface integration guide dataset energy market services 2022 a 2 emissions intensity from new vehicle registrations this indicator captures the emissions potential of new zealand s vehicles based on motive power year of manufacture and gross vehicle mass previous studies have highlighted the importance of including a view of the country s vehicle fleet gonzález sánchez and martín ortega 2020 vehicles in the motor vehicle register dataset new zealand transport agency 2022a are assigned an emissions potential kgco2 e km based on locally approved ipcc developed emissions factors ministry for the environment 2022a the emissions factors consider motive power petrol diesel petrol hybrid electric lpg plugin petrol hybrid petrol electric hybrid and vehicle year assuming a gross vehicle mass of 2000 3000 kg we assign to each vehicle the relevant emissions factor after adjusting for its actual gross vehicle mass the adjustment applies the principle that fuel economy is proportional to vehicle mass which is comparable to the proportions based approach to understanding environmental impact of vehicles in terms of their mass pero et al 2017 other studies use a congestion index liu et al 2020b and may match traffic flow data with a vehicle database to differentiate gasoline from diesel powered vehicles huang et al 2021 our view has an advantage over these methodologies by differentiating vehicles by more motive power categories as well as by vehicle age and gross vehicle mass a 3 emissions associated to national traffic volume this indicator is the product of the emissions potential of the fleet and the traffic volumes variable from appendix a 1 estimating the emissions potential of the fleet involved estimating the half life of a vehicle on new zealand roads and using this in an exponentially weighted mean over newly registered vehicles each carrying their own emissions potential recall our indicator of emissions intensity of vehicles registered onto new zealand roads from appendix a 2 based on the new zealand motor vehicle register mvr a limitation of this indicator is that we do not know how long these vehicles spend on the road there does exist a dashboard tool that provides a vehicle status feature for the mvr at an aggregated level which tells whether a vehicle is currently active or inactive however the api that provides the mvr to us vehicle by vehicle does not include vehicle status therefore this feature was not readily available to us in order to calculate how long vehicles spend on the road instead we opted to use more readily available data for this purpose our estimate of the half life of a vehicle is as follows we record how long the vehicles were not on new zealand roads by taking the mean difference in features from the mvr vehicle year and first nz registration year we denote this mean difference by y b f for years before registration next a local scrappage report ministry of transport 2009 can indicate typical ages of vehicles when they are de registered which reported an average of 16 7 years and 18 6 years in wellington and christchurch respectively thus we take mean 16 7 18 6 y b f to be the half life of vehicles on new zealand roads then we use the above half life to take the exponentially weighted moving mean of the emissions intensity from new vehicle registrations indicator this constitutes an exponential decay model in terms of the average lifespan of vehicles there is a precedent for this in the literature for example oguchi and fuse 2015 equation 1 this gives a variable indicative of the fleet s emissions potential over time the product of the fleet emissions potential and the state highway traffic volumes variable forms this indicator the half life used above is a rough estimate sufficient only for this proof of concept in future research we plan to investigate refining our estimate of half life this could involve accessing the data in the mvr behind the dashboard mentioned above that provides vehicle status as well as providing a national view of vehicle activity this could be updated whenever the mvr is updated which would be more dynamic that above where the half life estimate is deduced from a historic scrappage report ministry of transport 2009 a 4 weight of coal production we extract quarterly values of total coal production from table 1 quarterly tonnes of the quarterly coal supply transformation consumption tonnes publicly available spreadsheet data tables for coal dataset ministry of business innovation and employment 2022 coal is used on a significant scale for energy in the metal mineral chemical and manufacturing industries and construction ministry for the environment 2020 moreover an increase in coal fired electricity generation drove an increase in public electricity and heat production emissions between 2018 and 2019 ministry for the environment 2020 a 5 number of dairy cattle we extract annual values for total dairy cattle including bobby calves from dataset statistics new zealand 2022a in 2020 enteric fermentation by dairy cattle made up 35 6 of emissions in the agriculture sector and has increased by 65 5 since 1990 ministry for the environment 2022b this increase is primarily driven by increases in dairy cattle numbers ministry for the environment 2020 page 11 which is the purpose of this indicator a 6 weight of meat produced at export inspected facilities we collate weekly weight statistics of beef and sheep meat production for export reported by the new zealand meat board from certain export inspected processing facilities dating back to the 2017 18 season we sum the weight of all types of meat produced across the country the series is retrofitted back to 1990 with monthly values of total above livestock excluding game from dataset statistics new zealand 2022d a 7 quantity of imported fertilisers we extract monthly values for the quantity of imported fertilisers as in dataset statistics new zealand 2022c use of synthetic nitrogen fertilisers is a driver of emissions trends in agriculture ministry for the environment 2020 page 71 the use of fertiliser consumption as an indicator has been effective in amaefule et al 2022 a 8 food exports food export data is used as a general measure of food production the indicators food exports for meat food exports for aquaculture and dairy and food exports for horticulture are given by extracting monthly values for live animals meat and edible meat offal fish crustaceans molluscs dairy produce and other animal products and vegetables fruit and prepared foodstuffs beverages and tobacco respectively from dataset statistics new zealand 2022b we acknowledge that that this is a price based indicator and that price variations as well as production amounts are therefore included further development would be to inflation adjust this indicator to isolate the production component or replace it entirely with a mass based food export dataset 
25378,process based models for inversion and forward estimation have supported our understanding of earth environment and hazards processes these methods are often applied on spatial data without accounting for their spatial nature and uncertainty using an example of reconstructing past volcanic eruption characteristics and associated tephra fallout from different sets of field observation we demonstrate the importance of making the best use of data related uncertainty and spatial information in inversion and forward estimation we present strategies for 1 the selection of appropriate cost functions accounting for their behaviour and implied distribution of residuals 2 the treatment of differential uncertainty when combining multiple data and 3 the leveraging of both model and data when estimating the spatial distribution of output results show that a data informed choice of cost function and accounting for uncertainty and spatial characteristics of data leads to consistent improvements in model predictive performance for both inversion and forward models keywords tephra inversion model calibration kriging kelud volcano data availability i have shared my code data through a github link in the software and data availability statement 1 introduction process based models are important tools used to represent physical phenomena and processes in natural hazards analysis in the inversion or calibration setting they are used to estimate parameters that cannot be directly measured through calibration to the observed response of the hazard system examples include estimating floodplain characteristics based on observations of past floods estimating earthquake rupture characteristics based on recordings from seismic stations or estimating volcanic eruption source parameters based on measurements of the deposited tephra ejected particles of all sizes across the impacted region li et al 2022 georgoudas et al 2007 connor and connor 2006 in the forward estimation setting process based models are used to estimate the response of the hazard system based on calibrated or assumed parameters examples include flood simulation using a calibrated inundation model uhlenbrook et al 2004 estimating ground motion intensity for a given earthquake magnitude and location worden et al 2018 wang et al 2022 or estimating tephra accumulation given eruption and wind parameters hurst and turner 1999 folch et al 2009 such applications of inversion and forward estimation frameworks merge the imperfect knowledge derived from data with that of process based models in a way that utilises the reinforcing characteristics of both models and data as attention turns to such learning from data problems recent advancements in optimisation and uncertainty quantification underline the issue of treating inversion forward estimation problems with a black box perspective that ignores the characteristics of both models and data hollós et al 2022 cabaneros and hughes 2022 biegler et al 2011 isaac et al 2015 oden et al 2010 despite advancements in data collection technologies e g remote sensing and the growing complexity of process based models inversion forward estimation frameworks often overlook important data characteristics such as distribution uncertainty and spatial properties willcox et al 2021 thus there is a need to be strategic about how the inversion forward estimation framework uses limited and uncertain data in this paper we study this issue using the example of reconstructing past volcanic eruption characteristics and associated tephra fallout from observations across the impacted area the application follows a typical inversion forward estimation workflow shown in fig 1 the inversion and forward estimation workflows can be conducted sequentially as visualised in fig 1 or separately in the inversion calibration setting tephra fallout models are used to estimate eruption source parameters esps e g tephra mass total grain size distribution plume height and plume profile and empirical parameters e g fall time threshold diffusion coefficient from observations of tephra characteristics such as deposit thickness and grain size typically collected through field surveys in the impacted area as in any inversion process an automated search routine finds the parameters of a model in such a way that the modelled response best approximates the observed data the search routine optimises a cost function also known as loss function or objective function which calculates the agreement between the model estimates of tephra deposits and the observations this process can estimate eruption parameters for past events which is an important input for estimating the potential size and intensity of future eruptions and their associated consequences newhall and self 1982 carey and sparks 1986 pieri and baloga 1986 armienti et al 1988 scarpati et al 1993 mastin et al 2009 stohl et al 2011 pouget et al 2013 madankan et al 2014 bear crozier et al 2020 in the forward estimation setting tephra fallout models can map the distribution of tephra accumulation the map can serve multiple purposes including estimating eruption impacts on communities le pennec et al 2012 wardman et al 2012 magill et al 2013 biass et al 2017 assessing the vulnerability of buildings to tephra loading williams et al 2020 hayes et al 2019 spence et al 2005 or quantifying risk to agriculture gómez romero et al 2006 ayris and delmelle 2012 thompson et al 2017 given the importance and broad applicability of process based models this study focuses not on the models themselves but on how they are calibrated with limited and uncertain data and how they can best utilise such data in both inversion calibration and forward estimation settings we explore these through the study of tephra fallout from the 2014 eruption of kelud volcano and the use of the tephra2 model of bonadonna et al 2005 as the process based model the inversion workflow follows connor and connor 2006 s algorithm the study does not make any particular claim or conclusions about the characteristics of the kelud eruption or the accuracy of the tephra2 both of which have been extensively studied by others maeno et al 2019 caudron et al 2015 hargie et al 2019 suzuki and iguchi 2019 connor et al 2011 connor and connor 2006 instead this study focuses on methodological contributions into 1 the choice of cost functions in the calibration process what is the impact of this choice what are the implied assumptions linked with this choice 2 making use of multiple data sources with varying uncertainty how can we benefit from all data while still accounting for varying uncertainty 3 combining model estimates and data in the forward model spatial estimate how can we make use of both model results and observed data to estimate and map tephra accumulation how do we account for varying uncertainty of the data in this estimation we believe that the recommendations can benefit researchers interested in improving their estimates when conducting inversion and estimation of spatially distributed data the approach to select the cost function treat uncertain data and generate spatial estimates can also be applied to other earth and environmental models the paper proceeds as follows section 2 explains the current state of the art and limitations of interest in using process based models for inversion forward estimation of tephra fallout section 3 introduces our test case modelling the tephra fallout from the 2014 eruption of the kelud volcano using tephra2 the methods section section 4 illustrate the proposed improvements for model calibration and spatial estimation we provide a combined results and discussion in section 5 and the conclusions in section 6 2 current approach and limitations tephra2 is an accessible and popular process based model for inversion connor and connor 2006 and estimation of tephra fallout used in many volcanology studies e g volentik et al 2010 costa et al 2012 bonadonna and costa 2013 mannen 2014 bonadonna et al 2015 connor et al 2019 constantinescu et al 2021 williams et al 2020 it solves the advection diffusion equation analytically using wind and eruptive inputs providing tephra deposit mass accumulation and grain size distribution 2 1 choice of cost function in the current version of the tephra2 inversion code three possible cost functions are available to define the goodness of agreement or the model fit 1 m e a n s q u a r e e r r o r m s e 1 n i 1 n y i x i 2 2 c h i s q u a r e e r r o r 1 n i 1 n y i x i 2 x i 3 t o k y o l o g e r r o r i 1 n l o g y i x i 2 w h e r e l o g y i x i 0 i f y i x i 0 where y i is the modelled output x i is the observation and n is the total number of data points the tephra2 inversion algorithm as introduced in the paper by connor and connor 2006 utilised the chi square cost function in their work connor and connor 2006 highlighted that this cost function provides equal treatment to measurements of both thin and thick deposits during optimisation there are no additional instances in the tephra modelling literature where the selection of the cost function is discussed beyond this study volentik et al 2010 costa et al 2012 bonadonna and costa 2013 mannen 2014 bonadonna et al 2015 connor et al 2019 constantinescu et al 2021 scollo et al 2008 fontijn et al 2012 outside the field of tephra fallout modelling multiple studies have brought attention to the fact that different measures of model performance implied in the choice of cost function may satisfy different desirable characteristics chen et al 2017 makridakis 1993 armstrong and fildes 1995 certain cost functions penalise different magnitudes and directions of forecast error differently walther and moore 2005 an underestimation may not have the same penalty as an overestimation morley et al 2018 some characteristics may also be relevant for pragmatic purposes such as their interpretability such considerations help in narrowing down desirable cost functions applicable to an application beyond the cost functions characteristics the takeaway from these studies is that no metric is inherently better for all applications importantly choosing a cost function implies making an assumption on the type of distribution of the residuals where residuals are the difference between the modelled output and the observations engle 1993 hence in its correct application based on the assumed residual distribution a cost function is optimal for instance hodson 2022 presented a theoretical justification that root mean square error is optimal for normal gaussian residuals while mean absolute error mae is optimal for laplacian residuals in section 4 2 we implement this knowledge of the cost functions characteristics and inherent assumptions on the residuals to demonstrate how an optimal cost function may be selected for a test case we investigate the extent that the cost function affects the resulting estimates of tephra fallout and the associated residuals 2 2 treatment of varying uncertainty in data for the same deposit several factors may contribute to varying uncertainty between data points depending on how when and where measurements are collected engwell et al 2013 bonadonna et al 2015 according to engwell et al 2013 there are two types of uncertainties in tephra thickness both of which are rarely quantified or reported in field studies and literature uncertainty can be due to natural variation which is related to the physical processes of deposition preservation and remobilisation the second type is observational uncertainty or those uncertainties related to differing measurement techniques measurements that are most reliable and contain the least uncertainty are those taken from a well preserved deposit i e those taken soon after an eruption has ended in areas with little deposit reworking pyle 2016 blong et al 2017 however such conditions are often difficult to meet even for recent eruptions and impossible when studying older eruptions large portions of tephra deposits may also be inaccessible while they are still well preserved walker and croasdale 1971 field campaigns conducted at a significantly later time after the eruption might acquire measurements subjected to post eruption processes e g compaction soil formation bioturbation and remobilisation engwell et al 2013 or local weather conditions e g rain or wind hayes et al 2002 wilson et al 2011 arnalds et al 2013 blong et al 2017 oishi et al 2018 dominguez et al 2020 given limited data it is advantageous to leverage all available data however it becomes increasingly important to appropriately address uncertainty when integrating multiple data sources with differing levels of uncertainty the current tephra2 inversion algorithm treats all data equally potentially leading to biased tephra load estimates if relative differences in uncertainty between datasets are ignored to address this issue we propose a calibration approach in section 4 3 that accounts for the reliability of the data by weighing the observations in the cost function our analysis demonstrates the importance of considering uncertainties in fitting the model to the data however note that our approach does not aim to determine the optimal weights for the measurements and we do not quantify the absolute uncertainties in the data 2 3 making use of model and data the best fit modelled output from the forward estimation may diverge from observations in a spatially structured way due to model approximations unaccounted processes and uncertainties inherent in any model while these model data disagreements may not cause issues for applications such as tephra volume estimation when spatial aggregates are used they affect forward forecast or prediction performance when spatially explicit estimates are the focus and the process based models are unable to capture the spatial complexity one might observe in the field hence if the goal of the forward estimation is to obtain the spatial predictions that best agree with observations the process based model alone may not serve as the best tool assimilation techniques can be used to combine the model output with observations and improve the quality of the model estimates for example studies on volcanic ash concentration forecasting apply ensemble methods such as ensemble kalman filters to provide optimal estimates of the spatial distribution of ash dispersal in the atmosphere e g osores et al 2020 pardini et al 2020 such assimilation methods are online in the sense that they actively interact with the model over a time period of interest observations of tephra deposits are temporally too sparse to conduct an online assimilation thus we assimilate the model with observations using statistical interpolation to estimate the time averaged state of the system from post eruption surveys several statistical interpolation techniques have been used for assimilation in the geoscience and environmental literature these include regression techniques horalek et al 2005 kalman filtering denby et al 2008 and kriging methods thompson et al 2014 blond et al 2003 kassteele et al 2006 studies have shown that kriging is the most effective spatial interpolation approach johnston et al 2001 horálek et al 2006 in section 4 4 we apply residual kriging to combine the model output and data the proposed spatial estimation method here we call model data fusion harnesses both the process based model and the spatial structure of the data to improve the estimated spatial distribution of tephra load in addition to treating the tephra fallout data as spatial and accounting for their spatial characteristics the estimation method considers observations as imperfect versions of the true process with uncertainties always associated with them in line with the varying data uncertainties mentioned in section 2 2 we investigate two variations of the estimation method one approach weighs all the data points equally while the other accounts for the relative uncertainty between different groups of data 3 test case 2014 kelud eruption 3 1 eruption characteristics the 2014 eruption of kelud volcano located in east java indonesia was selected as a test case for this study the explosive activity started at 22 50 local time on 13 february 2014 and lasted for over four hours global volcanism program 2014 the eruptive activity finally declined on 14 17 february 2014 the total erupted volume was estimated to be 0 25 0 50 km 3 bulk deposit volume 0 14 0 28 km 3 in dense rock equivalent and the mass eruption rate was 6 5 2 8 1 0 7 kg s maeno et al 2019 the impacts of tephra fallout were widespread with over 76 000 people evacuated forty regional flights cancelled and rerouted and more than 26 000 buildings destroyed or damaged global volcanism program 2014 williams et al 2020 ifrc 2014 a documentation of the eruption sequence by maeno et al 2019 and numerical simulations by tanaka et al 2016 highlighted that the dispersal process and tephra accumulation were affected by the local winds at high altitudes of 17 km above sea level the umbrella cloud was affected by strong winds from the east low altitudes of 5 km above sea level were affected by winds from the southwest transporting tephra to the northeast and causing a bilobate tephra deposit published isopachs of accumulated tephra for this eruption fig 2 show a subtle bilobate feature that is attributed to the dynamics and evolution of the eruption and the local winds around the volcano maeno et al 2019 3 2 tephra fall data the study utilises two sets of tephra fall data each derived from field surveys by different teams at various times after the eruption the analysis takes the data in terms of load when only thickness measurements are available they are converted to loads based on a deposit bulk density of 1400 kg m 3 measured by maeno et al 2019 the first dataset here called dataset 1 is the tephra load data obtained from a field study of thickness by universitas gadjah mada ugm anggorowati and harijoko 2015 and later used in inversion modelling by williams et al 2020 the measurements were taken 2 3 days post eruption at 81 locations within 2 to 60 km of the vent williams et al 2020 the other data set here called dataset 2 consists of tephra load data converted from thickness measurements from a geological survey by maeno et al 2019 conducted a month after the eruption dataset 2 provides load information at 50 locations including areas up to 75 km north of the vent that dataset 1 does not cover for this study we exclude the six eyewitness reports also presented by maeno et al 2019 four outliers were removed from the raw datasets as they were an order of magnitude different than nearby data potentially related to issues of data collection or significant deposit reworking being outside the range of the other observations these were deemed to have a disproportionately strong influence over the model fits after removing these outliers the dataset used in our analyses consists of 127 points we present a map of the datasets in fig 3 3 3 previous inversion study the value of using inversion and forward estimation with process based models for kelud volcano has been presented in recent literature williams et al 2020 applied inversion as one of the methods to support remote assessment of tephra fall building damage and vulnerability assessment of buildings around kelud volcano in williams et al 2020 s study thickness measurements were inverted to estimate the best fit source parameters which were later used to map a continuous deposit such application is important to enhance our knowledge of risk to tephra hazards on the scale of buildings or regions especially when there is a high likelihood of future damaging eruptions from kelud maeno et al 2019 4 methods 4 1 model setup for tephra2 inversion the parameter bounds used in the inversion algorithm are shown in table 1 the ranges of values are selected based on studies in the literature of the 2014 kelud eruption and a recent inversion for the same eruption in williams et al 2020 for this study some of the ranges of values are widened relative to williams et al 2020 s to ensure that the inversion solution converges within the limits set we use the default optimisation routine from connor and connor 2006 s inversion algorithm the nelder mead simplex method the inversion used a fixed wind profile based on the wind dataset from the european centre for medium range weather forecasts ecmwf era interim reanalysis for the midnight of 13 february 2014 dee et al 2011 4 2 evaluation of cost function this paper examines five cost functions including those currently in the tephra2 code mean square error chi square error and three additional functions 4 m e a n a b s o l u t e e r r o r m a e 1 n i 1 n y i x i 5 m e a n a b s o l u t e p e r c e n t a g e e r r o r m a p e 100 n i 1 n y i x i x i 6 m e a n s q u a r e l o g e r r o r m s l e 1 n i 1 n l o g y i 1 x i 1 2 where y i refers to the model estimate x i is the observation and n is the total number of data points instead of tokyo log error eq 3 this study uses msle eq 6 a more well known cost function in statistics and machine learning in cases where the observed and predicted values are strictly positive as in the case of tephra load data the tokyo log function is essentially the same as msle in msle s formula the 1 s attached to the observed value x i and predicted value y i ensure mathematical stability since log 0 is undefined and both y and x can be 0 we propose a two step approach to evaluate the choice of cost function for the tephra2 inversion first the choice should be guided by the properties of the cost function summarised in table 2 and discussed in section 4 2 1 the second step is to run test inversions using the shortlisted cost functions and analyse the resulting residuals using goodness of fit tests as described in section 4 2 2 4 2 1 selection based on cost functions properties the foundation of any cost function is the model residual defined as ɛ y x where x refers to the observed value and y is the predicted value of the model cost functions almost always include a transformation of the observed value predicted value or the residual this transformation influences the model estimates according to these properties 1 order dependence 2 sensitivity to outliers and 3 symmetry the equations for the transformed residuals associated to the cost functions are shown in table 2 order dependence describes how the function covers orders of magnitude for the residual for instance an order dependent cost function may utilise the relative error ɛ x e g in the chi square cost function or the log transform of the observed and predicted values e g in mean square log error msle on the other hand a cost function that is not order dependent also known as scale dependent functions keep the residual ɛ as is in the function to retain the units scale of the observed and modelled values walther and moore 2005 if the modeller wishes to balance the treatment of distal and proximal deposits order dependent cost functions may be desirable since order dependent cost functions are scaled to the magnitude of the measured value observations made in thin parts of the deposit are equally important as observations made in the thicker parts of the deposit this aspect is important due to a few reasons tephra deposits thin exponentially with distance from the vent and outcrops mapped across a single deposit may span orders of magnitude pyle 1989 distal measurement of tephra accumulations are important to inform the extrapolation of the thinning rate beyond the outermost isopach examples of common order dependent cost functions include chi square msle and mape some cost functions are more sensitive to outliers than others cost functions with squared residuals for instance penalise large residuals more heavily than small residuals common examples are mse and root mean square error rmse both of which minimise to the same solution if we prefer to constrain penalty on large errors and make the function more resistant to outliers the absolute residual ɛ can be used instead of ɛ 2 for instance with the mean absolute error mae mae may be more appropriate for instances when the residuals are not normally distributed when large model residuals do not have to be weighted heavily and when the presence of outliers is a significant issue lastly the symmetry of the cost function relates to the treatment of underestimation versus overestimation mse mae and chi square treat residuals symmetrically msle applies more penalty for underestimation than overestimation mean absolute percentage error mape penalises overestimation more than underestimation 4 2 2 selection based on cost functions assumption on residuals the choice of cost function implies a distribution of residuals so in addition to their theoretical properties we can examine the validity of the cost functions by investigating the distribution of resulting residuals using statistical tests such as the kolmogorov smirnov k s the cramér von mises cvm and the anderson darling a d as well as the shapiro wilk s w ramachandran and tsokos 2015 stephens 1986 these tests use different metrics or test statistics to measure how similar the empirical distribution of the residuals is to its assumed one for example the k s test statistic is 7 d max f 0 ϵ f n ϵ where f 0 ϵ is the assumed cumulative distribution function and f n ϵ is the empirical cumulative distribution of the residuals the larger d is the more unlikely the residuals to have been generated from the assumed distribution goodness of fit can also be assessed graphically using quantile quantile q q plots which compare the empirical quantiles from the model fit to the theoretical quantiles from their assumed distributions while both statistical tests and q q plots are useful indicators of goodness of fit they both assume that the residuals are themselves independent and identically distributed 4 3 weighting the data in the cost function in order to account for different levels of uncertainty inherent in our datasets we propose to weigh each data point based on its uncertainty relative to a reference dataset in this way less reliable data have correspondingly less influence on the inversion the reference dataset may be an individual measurement or a group of measurements having the least measurement uncertainty in this work we use the prior information that dataset 2 has a relatively larger uncertainty than dataset 1 because of the delay in field data acquisition between the two sets of data thus we set dataset 1 as the reference dataset in the inversion algorithm points in this reference dataset are assigned a weight of 1 other datasets are then assigned weights relative to the uncertainty of the reference data for example the points from a dataset assumed to be twice as uncertain as the reference dataset may be assigned a weight w of 0 5 using weights based on relative uncertainty leads to a weighted cost function where each data point is weighted relative to the reference dataset with our proposed weighting scheme the weighted mse cost function for instance can be written as 8 1 n i 1 n w i y i x i 2 where w i indicates the uncertainty weight assigned to the data point i for our application we set dataset 2 s weight to 0 5 we evaluate the effectiveness of the weighted cost function based on the accuracy of the forward estimates on unseen out of sample data the test set consists of a random subset 20 of dataset 1 while the training set consists of the rest of the dataset 1 points and all of dataset 2 fig 4 a test set should ideally consist of data that closely resembles the true deposit so a stratified test set was selected to represent thin and thick deposits in relatively equal proportion as the true deposit we conducted inversion using the training points the parameter ranges in table 1 and the weighted form of the cost function for all the inversions we generate forward estimates of tephra load on the locations of the test points to assess predictive performance we compare the test errors between the unweighted inversion and weighted inversion the test errors are calculated following the formula of the cost function used in the inversion 4 4 combining model estimates and data to estimate the spatial distribution of tephra load across the entire area we propose to combine both forward model estimates as well as the data the process consists of three general steps 1 conduct an inversion forward estimation to model the distribution of tephra fallout 2 interpolate the residuals using spatial statistics techniques and 3 combine the interpolated residuals with the forward model we test and present two different spatial statistics techniques for the interpolation step the result is an improved estimate of tephra load that combines the information of the model and data our methods use kriging one of many interpolation techniques that estimate values at unobserved locations using a limited set of known spatial data in kriging the interpolation accounts for the spatial arrangement of the data in such a way that points nearby the site of interest are given more weight than those farther away this approach differs from another popular method inverse distance weighted interpolation in that we do not assume a spatial distribution model beforehand but estimate it from data another advantage of kriging is that we can estimate the uncertainty associated with the interpolated value 4 4 1 unweighted model data fusion we describe the procedure for an unweighted model data fusion approach using kriging in this approach there is no consideration of differential uncertainty between individual or groups of data we employ simple kriging in the following methods assuming that the difference between model and data across the study area has a mean of zero and follows a multivariate normal distribution wackernagel 2003 the kriging process involves selecting a variogram model that describes the spatial relationship between each pair of data points quantifying the degree of variation in the sampled values based on their spatial distance often the parameters of the variogram model are user defined or estimated using statistical software through approaches like maximum likelihood and least squares estimation we typically assume stationarity and isotropy implying that the variogram and the distribution of residuals do not vary with location or direction 1 inversion and forward modelling a run tephra2 inversion following the model setup in section 4 1 to find the best fit parameters use the most appropriate cost function identified using the steps in section 4 2 b estimate the load of tephra fallout at all sites sampled and unsampled using the best fit parameters from the tephra2 inversion 2 kriging a predict the residuals at all sites sampled and unsampled sites using simple kriging interpolation note that in the model data fusion we predict residuals at the sampled sites to incorporate the short range uncertainty at the observation locations the residual at any prediction location s o can be calculated as 9 z ˆ s o i 1 n λ i z s i where z s i is the residual at an observed location which is calculated using the equation in table 2 that corresponds to the cost function used in step 1a n is the number of observed sites λ i is the kriging weight applied to the value at the observed location the kriging weights λ i are calculated from a variogram model and a covariance matrix σ which describes the spatial relationships among the values at the observed sites and the prediction location the supplementary information section s5 provides a step by step procedure for simple kriging interpolation that includes formulas for λ i and the covariance matrix σ 3 fusion a back transform the kriging predictions z ˆ s o to produce residuals with the same units as the data see the supplementary material section s3 for sample formulas to use when back transforming the kriging predictions b add the tephra load estimates from step1b to the back transformed residuals from step 3a this produces an updated map of tephra fallout that accounts for both the model and the observations 4 4 2 weighted model data fusion in practice we could have different amounts of uncertainty associated with different datasets collected as we do in our case study due to different survey teams measuring tephra load at different time points after the eruption to account for varying data uncertainty in the interpolation procedure presented in section 4 4 1 we can use an extension of simple kriging introduced by worden et al 2018 which they use in the context of earthquake ground motion intensity estimation the method starts by identifying a reference dataset a subset of the data with least uncertainty then we define the additional uncertainty associated with other datasets relative to this reference dataset for the kelud test case we set dataset 1 as the reference dataset because it was acquired the earliest after the eruption we can set an uncertainty weight of w k 1 1 for dataset 1 and w k 2 0 5 for dataset 2 more generally for datasets other than the reference dataset dataset k 1 we will always have w k 1 based on these uncertainty weights we can define an adjustment factor that relates the variance of the reference dataset to other datasets dataset k 1 as a k w k the procedure follows the same steps as in section 4 4 1 except for step 2b with an adjustment factor the equation for the residual at any prediction location originally eq 9 would become 10 z ˆ s o i 1 n λ i a k z s i k where z s i k is the residual at observation site i associated with dataset k while λ i is the adjusted kriging weight calculated from an adjusted covariance matrix σ the adjusted covariance matrix can be written as 11 σ ω σ where ω a a t a is the vector of adjustment factors corresponding to the observation sites s 1 s n and denotes the element wise multiplication note that the covariance matrix and variogram model parameters were estimated using the reference dataset only similar to the simple kriging approach in section 4 4 1 we can estimate the uncertainty associated with the interpolated values also following steps 3a and 3b in section 4 4 1 we can add the interpolated residuals to the optimised model estimates the result is an updated map of the tephra load that accounts for the model estimates the observations and the varying uncertainty between different sets of data 4 4 3 comparison of the kriging methods we assess the performance of the kriging methods in sections 4 4 1 and 4 4 2 using leave one out cross validation loocv cross validation is a widely used technique for comparing the test performance of different models loocv specifically involves holding out a single observation for testing in each iteration maximising the total number of points used for testing this approach allows us to estimate the out of sample performance of the model shao 1993 detailed steps for the loocv procedure are provided in the supplementary information section s6 we can also check if the kriging interpolation overfits the training points to confirm this we compare the loocv errors to pre kriging training errors calculated using the same performance metrics root mean square error chi square and mean square log error from the process based model fit since in loocv we are conducting out of sample estimation loocv errors which are higher than the training errors from the tephra2 fit would suggest that kriging overfits the data 5 results and discussion our study shows that there is a significant influence on the inversion results and forward spatial estimates due to 1 the choice of the cost function in inversion 2 the treatment and accounting of differential uncertainty of the data and 3 the treatment and accounting of the spatial characteristics of the data in the following subsections we detail these in turn 5 1 use of a suitable cost function in order to identify the characteristics of cost functions most applicable to the test case we assessed the distribution of data values and the presence of outliers the data cover a wide range of values near zero to 168 kg m 2 see figs 3c and 3d so we select an order dependent cost function for a balanced treatment of data across different orders of magnitude we also decided to use a cost function that is not sensitive to outliers because of the presence of relatively few large values located close to the vent using table 2 as a guide the cost functions that satisfy such characteristics are msle and chi square although mape is also order dependent and most interpretable among all the cost functions in table 2 because the errors it produces are in terms of percentages we identify it as inappropriate for the test case for reasons beyond its sensitivity to outliers the statistics literature has provided evidence of its numerous weaknesses as a cost function mape results to consistently low estimated values when used for optimisation tofallis 2015 small actual values less than one result to extremely large mapes while zero actual values yields infinite errors kim and kim 2016 numerous small data are observed in the test case which is a common occurrence in tephra fallout data we also provide a short summary of issues associated with mape in the supplementary information s2 between msle and chi square we identify msle as the more suitable cost function because the residuals associated to the inversion with msle best adhered to the cost function s statistical assumption this was demonstrated with msle giving the largest p values across all the goodness of fit tests in table 3 the q q plots in fig 5 also support the conclusion because the matched theoretical and empirical quantiles lie along the diagonal line for msle the q q plots for the cost functions with underlying laplace distributions mae and mape are given in the supplementary information section s1 in summary our analyses emphasise that the selection of cost function needs to be a conscious choice in the inversion of source parameters an aspect often overlooked each cost function comes with assumptions regarding how the model would fit the data our results illustrated in fig 5 reveal that different cost functions can significantly impact the residuals of the best fit modelled output more importantly residual analysis can be used to draw conclusions on the appropriateness of the selected cost function through our presented methods we found that msle performs well and has characteristics well suited for the case study 5 2 weighting the cost function based on varying uncertainty the purpose of adding uncertainty based weights to the cost function in the inversion is to fit the model to the more reliable data while still considering the information provided by other less reliable data in the test case dataset 2 is the less reliable dataset compared to dataset 1 nevertheless dataset 2 is important to consider as it provides information in the deposit that might not be captured by dataset 1 for instance only dataset 2 consists of load data that are less than 1 kg m 2 these relatively small measurements are located in areas north and south of the vent where dataset 1 does not cover fig 3 the improved fit to the more reliable data can be visualised by looking at how the distribution of residuals change when using a weighted inversion versus those of an unweighted inversion for the test case s weighted inversion the residuals at the locations of the reference dataset dataset 1 became more concentrated around zero compared to those of the unweighted inversion fig 6a this difference in the distributions in fig 6a indicate that the weighted inversion was successful in terms of fitting better to the reference dataset as expected the distribution of residuals for dataset 2 from the weighted inversion does not indicate an improved fit fig 6b the residual distribution for dataset 2 only show a subtle increase in positive residuals when the inversion was weighted we assessed how uncertainty based weights in the cost function impact the best fit modelled output of an inversion forward estimation analysis for tephra fallout following the method in section 4 3 we conducted the inversion using msle the cost function that worked best based on the results in section 5 1 since the selected cost function was msle the test errors were calculated using the msle formula eq 6 when no weights were applied to the training points the test error is 0 05 whereas with weights applied the test error is 0 03 table 4 using weights in the inversion resulted in a lower test error which implies that using weights resulted in a better predictive performance at out of sample locations we checked whether using different cost functions would result in the same behaviour thus we repeated the inversions using the same train test split uncertainty weights and initial parameter ranges but using cost functions other than msle the resulting test errors shown in table 4 indicate that using weights in the inversion consistently reduces test errors and thus improves the prediction capability of the inversion forward estimation analysis 5 3 influence on the source parameters thus far the presented methodological contributions on the inversion use of appropriate cost functions and applying weights to the cost function were investigated in terms of their impacts on the best fit modelled outputs and the associated residuals acknowledging that the primary focus of an inversion is to optimise for the best fit source parameters we summarise the best fit parameter values from inversions using different cost functions in table 5 and for different weighting schemes in table 6 the purpose of the paper is to not make claims about the true source parameters for the test case instead we highlight that the source parameters are sensitive to the cost function and weighting in the inversion in section 5 1 we discuss how the order dependence and outlier sensitivity properties of cost functions may influence the model fit to large often proximal to the vent and small values often distal in section 5 2 the weighted approach allows a better fit to the more reliable data no matter the data s magnitude the relationship of the source parameters to these characteristics such as the estimation of thicker deposits near the vent or thinner distal deposits is connected to the physics of tephra transport for instance a larger plume height may lead to more tephra deposition at distal sites due to tephra being dispersed farther downwind while a lower plume height may lead to thicker proximal deposit because of less wind advection the relationship of observations made in distal and proximal sites to source parameters such as the erupted mass and plume height parameters have been studied by suzuki et al 1983 bonadonna et al 2005 and yang et al 2021 a full understanding of the sensitivity of the proposed methods to each source parameter would require more extensive sensitivity analyses and preferably a test case with a bigger dataset parameter sampling algorithms such as markov chain monte carlo algorithms are popular for evaluating the benefits of new inversion methods e g white et al 2017 yang et al 2021 such methods may run the inversion thousands of times using different starting seeds to produce a range of fitted parameter values the algorithms can extract confidence intervals for the source parameters which can represent the impact of the new methods on the optimised parameters while such sensitivity analysis is out of scope of this paper our analysis highlights the importance of cost function selection and weighted inversion when varying data uncertainties are present when such studies are implemented in this way our work can contribute towards improving our understanding of relationships between data uncertainty and source parameters in the inversion 5 4 model data fusion we provided a methodology in section 4 4 to combine observations and model estimates in such a way that the relative uncertainties between data are accounted for the methodology increases the value of the information from the data by using them to examine the spatial structure in the model residuals which can be used together with the information from the model to fill gaps at unsampled locations here we present an analysis of the new estimates of tephra load and associated residuals produced by fusion methods one advantage of using kriging in the fusion methods is the ability to produce not only the kriging estimates but also the uncertainties associated to the new estimates we provide a map of uncertainties related to the new estimates of tephra load in this section in the supplementary information figure s 4 the manually contoured isopachs for the 2014 kelud event by maeno et al 2019 fig 2 show a bilobate shape as influenced by the wind conditions during the tephra dispersion however this two lobe pattern was not captured by tephra2 without the additional spatial estimation using its residuals fig 7 shown in fig 7 are tephra deposit maps prior and after implementing an unweighted fusion approach described in section 4 4 1 when msle was chosen as the cost function we see that the model data fusion approach results in tephra load contours with a two lobed feature in line with the manually contoured isopachs and the wind shear that occurred across the eruption column the maps in fig 7 also illustrate how the measurements influence the resulting tephra deposit after the fusion the fusion approach fig 7b accounted for dataset 2 s small measurements southeast of the vent load values ranging from less than 1 to 10 kg m 2 in fig 3 resulting in lower values of load in this area compared to those in fig 7a due to the influence of thick deposit measurements near the vent the fusion approach produced higher peak load values 140 kg m 2 compared to those in the best fit modelled output 100 kg m 2 possibly reflecting sedimentation from the column s edge implementing a weighted fusion approach section 4 4 2 to combine the model in fig 7a and data results to the tephra deposit map in fig 9b while it is difficult to compare the results from the unweighted and weighted fusion based on the contour maps we can look at the loocv to compare their spatial estimation performance to compare the two approaches fairly we evaluate estimation performance using metrics corresponding to the cost function assumed the results in table 7 show that both weighted and unweighted model data fusion approaches lead to smaller loocv errors and they are smallest for the weighted fusion approach the same result is seen when other correlation structures such as exponential and gaussian functions are used instead of the matérn variogram see supplementary information table s 1 this highlights the advantage of making use of data in the spatial estimation and the importance of accounting for data uncertainties for generating spatial estimates comparing the loocv errors to pre kriging training errors also confirms that the approach is not overfitting we conduct a further examination of the residuals following weighted and unweighted fusion fig 8 presents histograms of these residuals separately for the locations of datasets 1 and 2 when utilising msle for inversion and fusion we observe that using the weighted fusion approach reduces the spread variability of the residuals of dataset 1 points indicating an improved fit this is expected since the weighted approach prioritises the fit to the more reliable dataset on the other hand the weighted fusion results to a wider spread in the residuals of dataset 2 which shows the influence of applying less weight the importance placed on the fitted process based model and the data can be balanced by the nugget parameter in the variogram model of the kriging procedures the nugget determines the spatial variability of the modelled quantities in short distances and hence affects the smoothness of the predicted residual surface the larger the nugget the smoother the surface we demonstrate this variation in smoothness in relation to the nugget value in fig 9 through the use of different spatial models of varying nuggets for the msle case indeed the nugget can be a fitted parameter using maximum likelihood estimation for example however when the fitted nugget value becomes too low the predictions are forced to perfectly fit the data this may be an issue for the context of tephra modelling because the data is inherently uncertain and sparse in addition there is often little data at close pairwise distances to get reliable estimates of the nugget this may create constraints that are physically unrealistic and do not account for the uncertainty in the data therefore it is often better to treat the nugget as a tuning parameter which is selected as a modelling choice based on expert knowledge we select the spatial model in fig 9a for the model data fusion in this paper the fusion methods use and build upon simple kriging which comes with an assumption that the mean of the residuals is zero alternatively if the spatial factors underlying the residuals are known regression kriging may be used instead of simple kriging in addition to the variogram that describes the stochastic spatial relationship between residuals we could use spatial covariates such as topography or distance from the vent to model their mean surface trend model in our analysis we checked that the assumption of isotropy was not violated supplementary information section s7 we also confirmed the strength of the spatial correlation which can tell us if kriging would be suitable as an interpolator and if the form of residual is appropriate note that extensions towards non stationary and anisotropic distributions can be made but are out of scope for this paper the results show how the model data fusion approach improves the modelled output of tephra2 through the use of data model data fusion with tephra2 may not necessarily yield results similar to those obtained from a more complex model that captures the physical processes of the eruption event better than tephra2 the influence of accounting for the spatial structure and uncertainty in the data is independent from the effect of utilising improved assumptions for eruption and atmospheric dynamics tephra2 however can be substituted with any other model in the model data fusion to better capture the physical processes of the eruption and atmospheric condition the use of data in the fusion approach can improve the irreducible errors that even complex models cannot capture due to model approximations and unaccounted processes inherent in any model 6 conclusions we demonstrate several methodological improvements in the inversion and forward estimation modelling processes when using limited uncertain and spatial data we test these using a case study of the eruption of the kelud volcano in 2014 using data on tephra thickness collected from two field surveys we conduct a model inversion to estimate the eruption source parameters and use these in a forward model to estimate the spatial distribution of tephra in locations where no data was collected we make no specific conclusions about the eruption itself or the true distribution of tephra but instead use the test case to demonstrate important considerations and approaches to treating data in inversion forward estimation problems our methodological contributions come in three components 1 the selection of appropriate cost functions accounting for their behaviour and their associated implied distribution of residuals 2 the treatment of differential uncertainty when combining multiple data sets and 3 the leveraging of both forward model and data when estimating the spatial distribution of output in the inversion setting this study places attention on the choice of the cost function to define the best fit between modelled output and observations this modelling choice receives little attention but has a significant impact on the results as seen in fig 5 we demonstrated how the characteristics of cost functions and their inherent assumptions on the distribution of model residuals can be used to inform the selection of appropriate cost functions we also demonstrate the use of data weighting in the inversion process to account for the differential uncertainty in data varying levels of uncertainties associated with different measurements are taken into account by weighting each observation s contribution to the cost function accordingly in the forward estimation we propose to combine the forward modelled output with measured data using spatial statistics techniques the output is an updated map of tephra fallout that accounts for both the model and the observations both the uncertainty in tephra measurements and their spatial correlation are accounted for in the fusion approach the importance placed in the model and data can be balanced by the choice of the spatial model and its parameters one of the strengths of the weighting approach in both inversion and forward model settings is that where multiple datasets are available the approach enables making use of all data even if some are highly uncertain while accounting for this uncertainty with the increasing availability of high volume low reliability information e g crowd sourced data from social media these can nonetheless be used to improve the modelling this work serves applications where the inversion and forward prediction workflows are conducted sequentially and applications where they are done separately while the test case is focused on tephra2 the methods are applicable to other models that reconstruct the process of tephra fallout such as ashfall and fall3d hurst and turner 1999 folch et al 2009 beyond applications related to tephra fallout the guidance could be relevant to those conducting any inversion or calibration modelling using spatial data in earth environment and hazards analysis code and data availability software name tephra2 lead developers laura j connor lconnor usf edu costanza bonadonna costanza cas usf edu see bonadonna et al 2005 and connor and connor 2006 year first available 2013 version used in the study 2 0 updated 01 27 2018 program language c license gnu general public license v3 0 availability https github com geoscience community codes tephra2 program size 963 kb hardware requirements the inversion model requires mpi message passing interface libraries and should be run on a computing cluster with multiple compute nodes the forward model can be compiled without mpi installed we have modified the tephra2 source code described above to accommodate different cost function types and weighted cost functions for the kriging presented in the study to combine model estimates and data we developed scripts written in the r programming language r 4 0 2 r core team 2020 the modified tephra2 source code r script for kriging and processed data are available in github https github com ntu dasl sg tephra spatial public this repository was created by maricar l rabonza corresponding author in 2022 the authors conducted inversion calculations using the komodo computing cluster managed by the asian school of the environment contact edwin tan edwintan ntu edu sg and nanyang technological university s high performance computing center hpcsupport ntu edu sg remote access to this computing cluster is possible with pc mac linux credit authorship contribution statement maricar rabonza designed the research writing original draft discussed the results commented on the manuscript michele nguyen designed the research discussed the results commented on the manuscript sebastien biass provided guidance in conducting tephra inversion modelling discussed the results commented on the manuscript susanna jenkins discussed the results commented on the manuscript benoit taisne discussed the results commented on the manuscript david lallemant designed the research discussed the results commented on the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was funded by the earth observatory of singapore and is eos contribution number 529 this research is supported by the national research foundation singapore and the singapore ministry of education under the nrf nrff2018 06 award we are grateful to edwin tan for his support in the use of the high performance computing cluster komodo in the earth observatory of singapore we thank george williams for his support and insights related to the tephra load datasets appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2023 105750 appendix a supplementary data the following is the supplementary material related to this article mmc s1 kriging and loocv steps fusion result uncertainty and mape 
25378,process based models for inversion and forward estimation have supported our understanding of earth environment and hazards processes these methods are often applied on spatial data without accounting for their spatial nature and uncertainty using an example of reconstructing past volcanic eruption characteristics and associated tephra fallout from different sets of field observation we demonstrate the importance of making the best use of data related uncertainty and spatial information in inversion and forward estimation we present strategies for 1 the selection of appropriate cost functions accounting for their behaviour and implied distribution of residuals 2 the treatment of differential uncertainty when combining multiple data and 3 the leveraging of both model and data when estimating the spatial distribution of output results show that a data informed choice of cost function and accounting for uncertainty and spatial characteristics of data leads to consistent improvements in model predictive performance for both inversion and forward models keywords tephra inversion model calibration kriging kelud volcano data availability i have shared my code data through a github link in the software and data availability statement 1 introduction process based models are important tools used to represent physical phenomena and processes in natural hazards analysis in the inversion or calibration setting they are used to estimate parameters that cannot be directly measured through calibration to the observed response of the hazard system examples include estimating floodplain characteristics based on observations of past floods estimating earthquake rupture characteristics based on recordings from seismic stations or estimating volcanic eruption source parameters based on measurements of the deposited tephra ejected particles of all sizes across the impacted region li et al 2022 georgoudas et al 2007 connor and connor 2006 in the forward estimation setting process based models are used to estimate the response of the hazard system based on calibrated or assumed parameters examples include flood simulation using a calibrated inundation model uhlenbrook et al 2004 estimating ground motion intensity for a given earthquake magnitude and location worden et al 2018 wang et al 2022 or estimating tephra accumulation given eruption and wind parameters hurst and turner 1999 folch et al 2009 such applications of inversion and forward estimation frameworks merge the imperfect knowledge derived from data with that of process based models in a way that utilises the reinforcing characteristics of both models and data as attention turns to such learning from data problems recent advancements in optimisation and uncertainty quantification underline the issue of treating inversion forward estimation problems with a black box perspective that ignores the characteristics of both models and data hollós et al 2022 cabaneros and hughes 2022 biegler et al 2011 isaac et al 2015 oden et al 2010 despite advancements in data collection technologies e g remote sensing and the growing complexity of process based models inversion forward estimation frameworks often overlook important data characteristics such as distribution uncertainty and spatial properties willcox et al 2021 thus there is a need to be strategic about how the inversion forward estimation framework uses limited and uncertain data in this paper we study this issue using the example of reconstructing past volcanic eruption characteristics and associated tephra fallout from observations across the impacted area the application follows a typical inversion forward estimation workflow shown in fig 1 the inversion and forward estimation workflows can be conducted sequentially as visualised in fig 1 or separately in the inversion calibration setting tephra fallout models are used to estimate eruption source parameters esps e g tephra mass total grain size distribution plume height and plume profile and empirical parameters e g fall time threshold diffusion coefficient from observations of tephra characteristics such as deposit thickness and grain size typically collected through field surveys in the impacted area as in any inversion process an automated search routine finds the parameters of a model in such a way that the modelled response best approximates the observed data the search routine optimises a cost function also known as loss function or objective function which calculates the agreement between the model estimates of tephra deposits and the observations this process can estimate eruption parameters for past events which is an important input for estimating the potential size and intensity of future eruptions and their associated consequences newhall and self 1982 carey and sparks 1986 pieri and baloga 1986 armienti et al 1988 scarpati et al 1993 mastin et al 2009 stohl et al 2011 pouget et al 2013 madankan et al 2014 bear crozier et al 2020 in the forward estimation setting tephra fallout models can map the distribution of tephra accumulation the map can serve multiple purposes including estimating eruption impacts on communities le pennec et al 2012 wardman et al 2012 magill et al 2013 biass et al 2017 assessing the vulnerability of buildings to tephra loading williams et al 2020 hayes et al 2019 spence et al 2005 or quantifying risk to agriculture gómez romero et al 2006 ayris and delmelle 2012 thompson et al 2017 given the importance and broad applicability of process based models this study focuses not on the models themselves but on how they are calibrated with limited and uncertain data and how they can best utilise such data in both inversion calibration and forward estimation settings we explore these through the study of tephra fallout from the 2014 eruption of kelud volcano and the use of the tephra2 model of bonadonna et al 2005 as the process based model the inversion workflow follows connor and connor 2006 s algorithm the study does not make any particular claim or conclusions about the characteristics of the kelud eruption or the accuracy of the tephra2 both of which have been extensively studied by others maeno et al 2019 caudron et al 2015 hargie et al 2019 suzuki and iguchi 2019 connor et al 2011 connor and connor 2006 instead this study focuses on methodological contributions into 1 the choice of cost functions in the calibration process what is the impact of this choice what are the implied assumptions linked with this choice 2 making use of multiple data sources with varying uncertainty how can we benefit from all data while still accounting for varying uncertainty 3 combining model estimates and data in the forward model spatial estimate how can we make use of both model results and observed data to estimate and map tephra accumulation how do we account for varying uncertainty of the data in this estimation we believe that the recommendations can benefit researchers interested in improving their estimates when conducting inversion and estimation of spatially distributed data the approach to select the cost function treat uncertain data and generate spatial estimates can also be applied to other earth and environmental models the paper proceeds as follows section 2 explains the current state of the art and limitations of interest in using process based models for inversion forward estimation of tephra fallout section 3 introduces our test case modelling the tephra fallout from the 2014 eruption of the kelud volcano using tephra2 the methods section section 4 illustrate the proposed improvements for model calibration and spatial estimation we provide a combined results and discussion in section 5 and the conclusions in section 6 2 current approach and limitations tephra2 is an accessible and popular process based model for inversion connor and connor 2006 and estimation of tephra fallout used in many volcanology studies e g volentik et al 2010 costa et al 2012 bonadonna and costa 2013 mannen 2014 bonadonna et al 2015 connor et al 2019 constantinescu et al 2021 williams et al 2020 it solves the advection diffusion equation analytically using wind and eruptive inputs providing tephra deposit mass accumulation and grain size distribution 2 1 choice of cost function in the current version of the tephra2 inversion code three possible cost functions are available to define the goodness of agreement or the model fit 1 m e a n s q u a r e e r r o r m s e 1 n i 1 n y i x i 2 2 c h i s q u a r e e r r o r 1 n i 1 n y i x i 2 x i 3 t o k y o l o g e r r o r i 1 n l o g y i x i 2 w h e r e l o g y i x i 0 i f y i x i 0 where y i is the modelled output x i is the observation and n is the total number of data points the tephra2 inversion algorithm as introduced in the paper by connor and connor 2006 utilised the chi square cost function in their work connor and connor 2006 highlighted that this cost function provides equal treatment to measurements of both thin and thick deposits during optimisation there are no additional instances in the tephra modelling literature where the selection of the cost function is discussed beyond this study volentik et al 2010 costa et al 2012 bonadonna and costa 2013 mannen 2014 bonadonna et al 2015 connor et al 2019 constantinescu et al 2021 scollo et al 2008 fontijn et al 2012 outside the field of tephra fallout modelling multiple studies have brought attention to the fact that different measures of model performance implied in the choice of cost function may satisfy different desirable characteristics chen et al 2017 makridakis 1993 armstrong and fildes 1995 certain cost functions penalise different magnitudes and directions of forecast error differently walther and moore 2005 an underestimation may not have the same penalty as an overestimation morley et al 2018 some characteristics may also be relevant for pragmatic purposes such as their interpretability such considerations help in narrowing down desirable cost functions applicable to an application beyond the cost functions characteristics the takeaway from these studies is that no metric is inherently better for all applications importantly choosing a cost function implies making an assumption on the type of distribution of the residuals where residuals are the difference between the modelled output and the observations engle 1993 hence in its correct application based on the assumed residual distribution a cost function is optimal for instance hodson 2022 presented a theoretical justification that root mean square error is optimal for normal gaussian residuals while mean absolute error mae is optimal for laplacian residuals in section 4 2 we implement this knowledge of the cost functions characteristics and inherent assumptions on the residuals to demonstrate how an optimal cost function may be selected for a test case we investigate the extent that the cost function affects the resulting estimates of tephra fallout and the associated residuals 2 2 treatment of varying uncertainty in data for the same deposit several factors may contribute to varying uncertainty between data points depending on how when and where measurements are collected engwell et al 2013 bonadonna et al 2015 according to engwell et al 2013 there are two types of uncertainties in tephra thickness both of which are rarely quantified or reported in field studies and literature uncertainty can be due to natural variation which is related to the physical processes of deposition preservation and remobilisation the second type is observational uncertainty or those uncertainties related to differing measurement techniques measurements that are most reliable and contain the least uncertainty are those taken from a well preserved deposit i e those taken soon after an eruption has ended in areas with little deposit reworking pyle 2016 blong et al 2017 however such conditions are often difficult to meet even for recent eruptions and impossible when studying older eruptions large portions of tephra deposits may also be inaccessible while they are still well preserved walker and croasdale 1971 field campaigns conducted at a significantly later time after the eruption might acquire measurements subjected to post eruption processes e g compaction soil formation bioturbation and remobilisation engwell et al 2013 or local weather conditions e g rain or wind hayes et al 2002 wilson et al 2011 arnalds et al 2013 blong et al 2017 oishi et al 2018 dominguez et al 2020 given limited data it is advantageous to leverage all available data however it becomes increasingly important to appropriately address uncertainty when integrating multiple data sources with differing levels of uncertainty the current tephra2 inversion algorithm treats all data equally potentially leading to biased tephra load estimates if relative differences in uncertainty between datasets are ignored to address this issue we propose a calibration approach in section 4 3 that accounts for the reliability of the data by weighing the observations in the cost function our analysis demonstrates the importance of considering uncertainties in fitting the model to the data however note that our approach does not aim to determine the optimal weights for the measurements and we do not quantify the absolute uncertainties in the data 2 3 making use of model and data the best fit modelled output from the forward estimation may diverge from observations in a spatially structured way due to model approximations unaccounted processes and uncertainties inherent in any model while these model data disagreements may not cause issues for applications such as tephra volume estimation when spatial aggregates are used they affect forward forecast or prediction performance when spatially explicit estimates are the focus and the process based models are unable to capture the spatial complexity one might observe in the field hence if the goal of the forward estimation is to obtain the spatial predictions that best agree with observations the process based model alone may not serve as the best tool assimilation techniques can be used to combine the model output with observations and improve the quality of the model estimates for example studies on volcanic ash concentration forecasting apply ensemble methods such as ensemble kalman filters to provide optimal estimates of the spatial distribution of ash dispersal in the atmosphere e g osores et al 2020 pardini et al 2020 such assimilation methods are online in the sense that they actively interact with the model over a time period of interest observations of tephra deposits are temporally too sparse to conduct an online assimilation thus we assimilate the model with observations using statistical interpolation to estimate the time averaged state of the system from post eruption surveys several statistical interpolation techniques have been used for assimilation in the geoscience and environmental literature these include regression techniques horalek et al 2005 kalman filtering denby et al 2008 and kriging methods thompson et al 2014 blond et al 2003 kassteele et al 2006 studies have shown that kriging is the most effective spatial interpolation approach johnston et al 2001 horálek et al 2006 in section 4 4 we apply residual kriging to combine the model output and data the proposed spatial estimation method here we call model data fusion harnesses both the process based model and the spatial structure of the data to improve the estimated spatial distribution of tephra load in addition to treating the tephra fallout data as spatial and accounting for their spatial characteristics the estimation method considers observations as imperfect versions of the true process with uncertainties always associated with them in line with the varying data uncertainties mentioned in section 2 2 we investigate two variations of the estimation method one approach weighs all the data points equally while the other accounts for the relative uncertainty between different groups of data 3 test case 2014 kelud eruption 3 1 eruption characteristics the 2014 eruption of kelud volcano located in east java indonesia was selected as a test case for this study the explosive activity started at 22 50 local time on 13 february 2014 and lasted for over four hours global volcanism program 2014 the eruptive activity finally declined on 14 17 february 2014 the total erupted volume was estimated to be 0 25 0 50 km 3 bulk deposit volume 0 14 0 28 km 3 in dense rock equivalent and the mass eruption rate was 6 5 2 8 1 0 7 kg s maeno et al 2019 the impacts of tephra fallout were widespread with over 76 000 people evacuated forty regional flights cancelled and rerouted and more than 26 000 buildings destroyed or damaged global volcanism program 2014 williams et al 2020 ifrc 2014 a documentation of the eruption sequence by maeno et al 2019 and numerical simulations by tanaka et al 2016 highlighted that the dispersal process and tephra accumulation were affected by the local winds at high altitudes of 17 km above sea level the umbrella cloud was affected by strong winds from the east low altitudes of 5 km above sea level were affected by winds from the southwest transporting tephra to the northeast and causing a bilobate tephra deposit published isopachs of accumulated tephra for this eruption fig 2 show a subtle bilobate feature that is attributed to the dynamics and evolution of the eruption and the local winds around the volcano maeno et al 2019 3 2 tephra fall data the study utilises two sets of tephra fall data each derived from field surveys by different teams at various times after the eruption the analysis takes the data in terms of load when only thickness measurements are available they are converted to loads based on a deposit bulk density of 1400 kg m 3 measured by maeno et al 2019 the first dataset here called dataset 1 is the tephra load data obtained from a field study of thickness by universitas gadjah mada ugm anggorowati and harijoko 2015 and later used in inversion modelling by williams et al 2020 the measurements were taken 2 3 days post eruption at 81 locations within 2 to 60 km of the vent williams et al 2020 the other data set here called dataset 2 consists of tephra load data converted from thickness measurements from a geological survey by maeno et al 2019 conducted a month after the eruption dataset 2 provides load information at 50 locations including areas up to 75 km north of the vent that dataset 1 does not cover for this study we exclude the six eyewitness reports also presented by maeno et al 2019 four outliers were removed from the raw datasets as they were an order of magnitude different than nearby data potentially related to issues of data collection or significant deposit reworking being outside the range of the other observations these were deemed to have a disproportionately strong influence over the model fits after removing these outliers the dataset used in our analyses consists of 127 points we present a map of the datasets in fig 3 3 3 previous inversion study the value of using inversion and forward estimation with process based models for kelud volcano has been presented in recent literature williams et al 2020 applied inversion as one of the methods to support remote assessment of tephra fall building damage and vulnerability assessment of buildings around kelud volcano in williams et al 2020 s study thickness measurements were inverted to estimate the best fit source parameters which were later used to map a continuous deposit such application is important to enhance our knowledge of risk to tephra hazards on the scale of buildings or regions especially when there is a high likelihood of future damaging eruptions from kelud maeno et al 2019 4 methods 4 1 model setup for tephra2 inversion the parameter bounds used in the inversion algorithm are shown in table 1 the ranges of values are selected based on studies in the literature of the 2014 kelud eruption and a recent inversion for the same eruption in williams et al 2020 for this study some of the ranges of values are widened relative to williams et al 2020 s to ensure that the inversion solution converges within the limits set we use the default optimisation routine from connor and connor 2006 s inversion algorithm the nelder mead simplex method the inversion used a fixed wind profile based on the wind dataset from the european centre for medium range weather forecasts ecmwf era interim reanalysis for the midnight of 13 february 2014 dee et al 2011 4 2 evaluation of cost function this paper examines five cost functions including those currently in the tephra2 code mean square error chi square error and three additional functions 4 m e a n a b s o l u t e e r r o r m a e 1 n i 1 n y i x i 5 m e a n a b s o l u t e p e r c e n t a g e e r r o r m a p e 100 n i 1 n y i x i x i 6 m e a n s q u a r e l o g e r r o r m s l e 1 n i 1 n l o g y i 1 x i 1 2 where y i refers to the model estimate x i is the observation and n is the total number of data points instead of tokyo log error eq 3 this study uses msle eq 6 a more well known cost function in statistics and machine learning in cases where the observed and predicted values are strictly positive as in the case of tephra load data the tokyo log function is essentially the same as msle in msle s formula the 1 s attached to the observed value x i and predicted value y i ensure mathematical stability since log 0 is undefined and both y and x can be 0 we propose a two step approach to evaluate the choice of cost function for the tephra2 inversion first the choice should be guided by the properties of the cost function summarised in table 2 and discussed in section 4 2 1 the second step is to run test inversions using the shortlisted cost functions and analyse the resulting residuals using goodness of fit tests as described in section 4 2 2 4 2 1 selection based on cost functions properties the foundation of any cost function is the model residual defined as ɛ y x where x refers to the observed value and y is the predicted value of the model cost functions almost always include a transformation of the observed value predicted value or the residual this transformation influences the model estimates according to these properties 1 order dependence 2 sensitivity to outliers and 3 symmetry the equations for the transformed residuals associated to the cost functions are shown in table 2 order dependence describes how the function covers orders of magnitude for the residual for instance an order dependent cost function may utilise the relative error ɛ x e g in the chi square cost function or the log transform of the observed and predicted values e g in mean square log error msle on the other hand a cost function that is not order dependent also known as scale dependent functions keep the residual ɛ as is in the function to retain the units scale of the observed and modelled values walther and moore 2005 if the modeller wishes to balance the treatment of distal and proximal deposits order dependent cost functions may be desirable since order dependent cost functions are scaled to the magnitude of the measured value observations made in thin parts of the deposit are equally important as observations made in the thicker parts of the deposit this aspect is important due to a few reasons tephra deposits thin exponentially with distance from the vent and outcrops mapped across a single deposit may span orders of magnitude pyle 1989 distal measurement of tephra accumulations are important to inform the extrapolation of the thinning rate beyond the outermost isopach examples of common order dependent cost functions include chi square msle and mape some cost functions are more sensitive to outliers than others cost functions with squared residuals for instance penalise large residuals more heavily than small residuals common examples are mse and root mean square error rmse both of which minimise to the same solution if we prefer to constrain penalty on large errors and make the function more resistant to outliers the absolute residual ɛ can be used instead of ɛ 2 for instance with the mean absolute error mae mae may be more appropriate for instances when the residuals are not normally distributed when large model residuals do not have to be weighted heavily and when the presence of outliers is a significant issue lastly the symmetry of the cost function relates to the treatment of underestimation versus overestimation mse mae and chi square treat residuals symmetrically msle applies more penalty for underestimation than overestimation mean absolute percentage error mape penalises overestimation more than underestimation 4 2 2 selection based on cost functions assumption on residuals the choice of cost function implies a distribution of residuals so in addition to their theoretical properties we can examine the validity of the cost functions by investigating the distribution of resulting residuals using statistical tests such as the kolmogorov smirnov k s the cramér von mises cvm and the anderson darling a d as well as the shapiro wilk s w ramachandran and tsokos 2015 stephens 1986 these tests use different metrics or test statistics to measure how similar the empirical distribution of the residuals is to its assumed one for example the k s test statistic is 7 d max f 0 ϵ f n ϵ where f 0 ϵ is the assumed cumulative distribution function and f n ϵ is the empirical cumulative distribution of the residuals the larger d is the more unlikely the residuals to have been generated from the assumed distribution goodness of fit can also be assessed graphically using quantile quantile q q plots which compare the empirical quantiles from the model fit to the theoretical quantiles from their assumed distributions while both statistical tests and q q plots are useful indicators of goodness of fit they both assume that the residuals are themselves independent and identically distributed 4 3 weighting the data in the cost function in order to account for different levels of uncertainty inherent in our datasets we propose to weigh each data point based on its uncertainty relative to a reference dataset in this way less reliable data have correspondingly less influence on the inversion the reference dataset may be an individual measurement or a group of measurements having the least measurement uncertainty in this work we use the prior information that dataset 2 has a relatively larger uncertainty than dataset 1 because of the delay in field data acquisition between the two sets of data thus we set dataset 1 as the reference dataset in the inversion algorithm points in this reference dataset are assigned a weight of 1 other datasets are then assigned weights relative to the uncertainty of the reference data for example the points from a dataset assumed to be twice as uncertain as the reference dataset may be assigned a weight w of 0 5 using weights based on relative uncertainty leads to a weighted cost function where each data point is weighted relative to the reference dataset with our proposed weighting scheme the weighted mse cost function for instance can be written as 8 1 n i 1 n w i y i x i 2 where w i indicates the uncertainty weight assigned to the data point i for our application we set dataset 2 s weight to 0 5 we evaluate the effectiveness of the weighted cost function based on the accuracy of the forward estimates on unseen out of sample data the test set consists of a random subset 20 of dataset 1 while the training set consists of the rest of the dataset 1 points and all of dataset 2 fig 4 a test set should ideally consist of data that closely resembles the true deposit so a stratified test set was selected to represent thin and thick deposits in relatively equal proportion as the true deposit we conducted inversion using the training points the parameter ranges in table 1 and the weighted form of the cost function for all the inversions we generate forward estimates of tephra load on the locations of the test points to assess predictive performance we compare the test errors between the unweighted inversion and weighted inversion the test errors are calculated following the formula of the cost function used in the inversion 4 4 combining model estimates and data to estimate the spatial distribution of tephra load across the entire area we propose to combine both forward model estimates as well as the data the process consists of three general steps 1 conduct an inversion forward estimation to model the distribution of tephra fallout 2 interpolate the residuals using spatial statistics techniques and 3 combine the interpolated residuals with the forward model we test and present two different spatial statistics techniques for the interpolation step the result is an improved estimate of tephra load that combines the information of the model and data our methods use kriging one of many interpolation techniques that estimate values at unobserved locations using a limited set of known spatial data in kriging the interpolation accounts for the spatial arrangement of the data in such a way that points nearby the site of interest are given more weight than those farther away this approach differs from another popular method inverse distance weighted interpolation in that we do not assume a spatial distribution model beforehand but estimate it from data another advantage of kriging is that we can estimate the uncertainty associated with the interpolated value 4 4 1 unweighted model data fusion we describe the procedure for an unweighted model data fusion approach using kriging in this approach there is no consideration of differential uncertainty between individual or groups of data we employ simple kriging in the following methods assuming that the difference between model and data across the study area has a mean of zero and follows a multivariate normal distribution wackernagel 2003 the kriging process involves selecting a variogram model that describes the spatial relationship between each pair of data points quantifying the degree of variation in the sampled values based on their spatial distance often the parameters of the variogram model are user defined or estimated using statistical software through approaches like maximum likelihood and least squares estimation we typically assume stationarity and isotropy implying that the variogram and the distribution of residuals do not vary with location or direction 1 inversion and forward modelling a run tephra2 inversion following the model setup in section 4 1 to find the best fit parameters use the most appropriate cost function identified using the steps in section 4 2 b estimate the load of tephra fallout at all sites sampled and unsampled using the best fit parameters from the tephra2 inversion 2 kriging a predict the residuals at all sites sampled and unsampled sites using simple kriging interpolation note that in the model data fusion we predict residuals at the sampled sites to incorporate the short range uncertainty at the observation locations the residual at any prediction location s o can be calculated as 9 z ˆ s o i 1 n λ i z s i where z s i is the residual at an observed location which is calculated using the equation in table 2 that corresponds to the cost function used in step 1a n is the number of observed sites λ i is the kriging weight applied to the value at the observed location the kriging weights λ i are calculated from a variogram model and a covariance matrix σ which describes the spatial relationships among the values at the observed sites and the prediction location the supplementary information section s5 provides a step by step procedure for simple kriging interpolation that includes formulas for λ i and the covariance matrix σ 3 fusion a back transform the kriging predictions z ˆ s o to produce residuals with the same units as the data see the supplementary material section s3 for sample formulas to use when back transforming the kriging predictions b add the tephra load estimates from step1b to the back transformed residuals from step 3a this produces an updated map of tephra fallout that accounts for both the model and the observations 4 4 2 weighted model data fusion in practice we could have different amounts of uncertainty associated with different datasets collected as we do in our case study due to different survey teams measuring tephra load at different time points after the eruption to account for varying data uncertainty in the interpolation procedure presented in section 4 4 1 we can use an extension of simple kriging introduced by worden et al 2018 which they use in the context of earthquake ground motion intensity estimation the method starts by identifying a reference dataset a subset of the data with least uncertainty then we define the additional uncertainty associated with other datasets relative to this reference dataset for the kelud test case we set dataset 1 as the reference dataset because it was acquired the earliest after the eruption we can set an uncertainty weight of w k 1 1 for dataset 1 and w k 2 0 5 for dataset 2 more generally for datasets other than the reference dataset dataset k 1 we will always have w k 1 based on these uncertainty weights we can define an adjustment factor that relates the variance of the reference dataset to other datasets dataset k 1 as a k w k the procedure follows the same steps as in section 4 4 1 except for step 2b with an adjustment factor the equation for the residual at any prediction location originally eq 9 would become 10 z ˆ s o i 1 n λ i a k z s i k where z s i k is the residual at observation site i associated with dataset k while λ i is the adjusted kriging weight calculated from an adjusted covariance matrix σ the adjusted covariance matrix can be written as 11 σ ω σ where ω a a t a is the vector of adjustment factors corresponding to the observation sites s 1 s n and denotes the element wise multiplication note that the covariance matrix and variogram model parameters were estimated using the reference dataset only similar to the simple kriging approach in section 4 4 1 we can estimate the uncertainty associated with the interpolated values also following steps 3a and 3b in section 4 4 1 we can add the interpolated residuals to the optimised model estimates the result is an updated map of the tephra load that accounts for the model estimates the observations and the varying uncertainty between different sets of data 4 4 3 comparison of the kriging methods we assess the performance of the kriging methods in sections 4 4 1 and 4 4 2 using leave one out cross validation loocv cross validation is a widely used technique for comparing the test performance of different models loocv specifically involves holding out a single observation for testing in each iteration maximising the total number of points used for testing this approach allows us to estimate the out of sample performance of the model shao 1993 detailed steps for the loocv procedure are provided in the supplementary information section s6 we can also check if the kriging interpolation overfits the training points to confirm this we compare the loocv errors to pre kriging training errors calculated using the same performance metrics root mean square error chi square and mean square log error from the process based model fit since in loocv we are conducting out of sample estimation loocv errors which are higher than the training errors from the tephra2 fit would suggest that kriging overfits the data 5 results and discussion our study shows that there is a significant influence on the inversion results and forward spatial estimates due to 1 the choice of the cost function in inversion 2 the treatment and accounting of differential uncertainty of the data and 3 the treatment and accounting of the spatial characteristics of the data in the following subsections we detail these in turn 5 1 use of a suitable cost function in order to identify the characteristics of cost functions most applicable to the test case we assessed the distribution of data values and the presence of outliers the data cover a wide range of values near zero to 168 kg m 2 see figs 3c and 3d so we select an order dependent cost function for a balanced treatment of data across different orders of magnitude we also decided to use a cost function that is not sensitive to outliers because of the presence of relatively few large values located close to the vent using table 2 as a guide the cost functions that satisfy such characteristics are msle and chi square although mape is also order dependent and most interpretable among all the cost functions in table 2 because the errors it produces are in terms of percentages we identify it as inappropriate for the test case for reasons beyond its sensitivity to outliers the statistics literature has provided evidence of its numerous weaknesses as a cost function mape results to consistently low estimated values when used for optimisation tofallis 2015 small actual values less than one result to extremely large mapes while zero actual values yields infinite errors kim and kim 2016 numerous small data are observed in the test case which is a common occurrence in tephra fallout data we also provide a short summary of issues associated with mape in the supplementary information s2 between msle and chi square we identify msle as the more suitable cost function because the residuals associated to the inversion with msle best adhered to the cost function s statistical assumption this was demonstrated with msle giving the largest p values across all the goodness of fit tests in table 3 the q q plots in fig 5 also support the conclusion because the matched theoretical and empirical quantiles lie along the diagonal line for msle the q q plots for the cost functions with underlying laplace distributions mae and mape are given in the supplementary information section s1 in summary our analyses emphasise that the selection of cost function needs to be a conscious choice in the inversion of source parameters an aspect often overlooked each cost function comes with assumptions regarding how the model would fit the data our results illustrated in fig 5 reveal that different cost functions can significantly impact the residuals of the best fit modelled output more importantly residual analysis can be used to draw conclusions on the appropriateness of the selected cost function through our presented methods we found that msle performs well and has characteristics well suited for the case study 5 2 weighting the cost function based on varying uncertainty the purpose of adding uncertainty based weights to the cost function in the inversion is to fit the model to the more reliable data while still considering the information provided by other less reliable data in the test case dataset 2 is the less reliable dataset compared to dataset 1 nevertheless dataset 2 is important to consider as it provides information in the deposit that might not be captured by dataset 1 for instance only dataset 2 consists of load data that are less than 1 kg m 2 these relatively small measurements are located in areas north and south of the vent where dataset 1 does not cover fig 3 the improved fit to the more reliable data can be visualised by looking at how the distribution of residuals change when using a weighted inversion versus those of an unweighted inversion for the test case s weighted inversion the residuals at the locations of the reference dataset dataset 1 became more concentrated around zero compared to those of the unweighted inversion fig 6a this difference in the distributions in fig 6a indicate that the weighted inversion was successful in terms of fitting better to the reference dataset as expected the distribution of residuals for dataset 2 from the weighted inversion does not indicate an improved fit fig 6b the residual distribution for dataset 2 only show a subtle increase in positive residuals when the inversion was weighted we assessed how uncertainty based weights in the cost function impact the best fit modelled output of an inversion forward estimation analysis for tephra fallout following the method in section 4 3 we conducted the inversion using msle the cost function that worked best based on the results in section 5 1 since the selected cost function was msle the test errors were calculated using the msle formula eq 6 when no weights were applied to the training points the test error is 0 05 whereas with weights applied the test error is 0 03 table 4 using weights in the inversion resulted in a lower test error which implies that using weights resulted in a better predictive performance at out of sample locations we checked whether using different cost functions would result in the same behaviour thus we repeated the inversions using the same train test split uncertainty weights and initial parameter ranges but using cost functions other than msle the resulting test errors shown in table 4 indicate that using weights in the inversion consistently reduces test errors and thus improves the prediction capability of the inversion forward estimation analysis 5 3 influence on the source parameters thus far the presented methodological contributions on the inversion use of appropriate cost functions and applying weights to the cost function were investigated in terms of their impacts on the best fit modelled outputs and the associated residuals acknowledging that the primary focus of an inversion is to optimise for the best fit source parameters we summarise the best fit parameter values from inversions using different cost functions in table 5 and for different weighting schemes in table 6 the purpose of the paper is to not make claims about the true source parameters for the test case instead we highlight that the source parameters are sensitive to the cost function and weighting in the inversion in section 5 1 we discuss how the order dependence and outlier sensitivity properties of cost functions may influence the model fit to large often proximal to the vent and small values often distal in section 5 2 the weighted approach allows a better fit to the more reliable data no matter the data s magnitude the relationship of the source parameters to these characteristics such as the estimation of thicker deposits near the vent or thinner distal deposits is connected to the physics of tephra transport for instance a larger plume height may lead to more tephra deposition at distal sites due to tephra being dispersed farther downwind while a lower plume height may lead to thicker proximal deposit because of less wind advection the relationship of observations made in distal and proximal sites to source parameters such as the erupted mass and plume height parameters have been studied by suzuki et al 1983 bonadonna et al 2005 and yang et al 2021 a full understanding of the sensitivity of the proposed methods to each source parameter would require more extensive sensitivity analyses and preferably a test case with a bigger dataset parameter sampling algorithms such as markov chain monte carlo algorithms are popular for evaluating the benefits of new inversion methods e g white et al 2017 yang et al 2021 such methods may run the inversion thousands of times using different starting seeds to produce a range of fitted parameter values the algorithms can extract confidence intervals for the source parameters which can represent the impact of the new methods on the optimised parameters while such sensitivity analysis is out of scope of this paper our analysis highlights the importance of cost function selection and weighted inversion when varying data uncertainties are present when such studies are implemented in this way our work can contribute towards improving our understanding of relationships between data uncertainty and source parameters in the inversion 5 4 model data fusion we provided a methodology in section 4 4 to combine observations and model estimates in such a way that the relative uncertainties between data are accounted for the methodology increases the value of the information from the data by using them to examine the spatial structure in the model residuals which can be used together with the information from the model to fill gaps at unsampled locations here we present an analysis of the new estimates of tephra load and associated residuals produced by fusion methods one advantage of using kriging in the fusion methods is the ability to produce not only the kriging estimates but also the uncertainties associated to the new estimates we provide a map of uncertainties related to the new estimates of tephra load in this section in the supplementary information figure s 4 the manually contoured isopachs for the 2014 kelud event by maeno et al 2019 fig 2 show a bilobate shape as influenced by the wind conditions during the tephra dispersion however this two lobe pattern was not captured by tephra2 without the additional spatial estimation using its residuals fig 7 shown in fig 7 are tephra deposit maps prior and after implementing an unweighted fusion approach described in section 4 4 1 when msle was chosen as the cost function we see that the model data fusion approach results in tephra load contours with a two lobed feature in line with the manually contoured isopachs and the wind shear that occurred across the eruption column the maps in fig 7 also illustrate how the measurements influence the resulting tephra deposit after the fusion the fusion approach fig 7b accounted for dataset 2 s small measurements southeast of the vent load values ranging from less than 1 to 10 kg m 2 in fig 3 resulting in lower values of load in this area compared to those in fig 7a due to the influence of thick deposit measurements near the vent the fusion approach produced higher peak load values 140 kg m 2 compared to those in the best fit modelled output 100 kg m 2 possibly reflecting sedimentation from the column s edge implementing a weighted fusion approach section 4 4 2 to combine the model in fig 7a and data results to the tephra deposit map in fig 9b while it is difficult to compare the results from the unweighted and weighted fusion based on the contour maps we can look at the loocv to compare their spatial estimation performance to compare the two approaches fairly we evaluate estimation performance using metrics corresponding to the cost function assumed the results in table 7 show that both weighted and unweighted model data fusion approaches lead to smaller loocv errors and they are smallest for the weighted fusion approach the same result is seen when other correlation structures such as exponential and gaussian functions are used instead of the matérn variogram see supplementary information table s 1 this highlights the advantage of making use of data in the spatial estimation and the importance of accounting for data uncertainties for generating spatial estimates comparing the loocv errors to pre kriging training errors also confirms that the approach is not overfitting we conduct a further examination of the residuals following weighted and unweighted fusion fig 8 presents histograms of these residuals separately for the locations of datasets 1 and 2 when utilising msle for inversion and fusion we observe that using the weighted fusion approach reduces the spread variability of the residuals of dataset 1 points indicating an improved fit this is expected since the weighted approach prioritises the fit to the more reliable dataset on the other hand the weighted fusion results to a wider spread in the residuals of dataset 2 which shows the influence of applying less weight the importance placed on the fitted process based model and the data can be balanced by the nugget parameter in the variogram model of the kriging procedures the nugget determines the spatial variability of the modelled quantities in short distances and hence affects the smoothness of the predicted residual surface the larger the nugget the smoother the surface we demonstrate this variation in smoothness in relation to the nugget value in fig 9 through the use of different spatial models of varying nuggets for the msle case indeed the nugget can be a fitted parameter using maximum likelihood estimation for example however when the fitted nugget value becomes too low the predictions are forced to perfectly fit the data this may be an issue for the context of tephra modelling because the data is inherently uncertain and sparse in addition there is often little data at close pairwise distances to get reliable estimates of the nugget this may create constraints that are physically unrealistic and do not account for the uncertainty in the data therefore it is often better to treat the nugget as a tuning parameter which is selected as a modelling choice based on expert knowledge we select the spatial model in fig 9a for the model data fusion in this paper the fusion methods use and build upon simple kriging which comes with an assumption that the mean of the residuals is zero alternatively if the spatial factors underlying the residuals are known regression kriging may be used instead of simple kriging in addition to the variogram that describes the stochastic spatial relationship between residuals we could use spatial covariates such as topography or distance from the vent to model their mean surface trend model in our analysis we checked that the assumption of isotropy was not violated supplementary information section s7 we also confirmed the strength of the spatial correlation which can tell us if kriging would be suitable as an interpolator and if the form of residual is appropriate note that extensions towards non stationary and anisotropic distributions can be made but are out of scope for this paper the results show how the model data fusion approach improves the modelled output of tephra2 through the use of data model data fusion with tephra2 may not necessarily yield results similar to those obtained from a more complex model that captures the physical processes of the eruption event better than tephra2 the influence of accounting for the spatial structure and uncertainty in the data is independent from the effect of utilising improved assumptions for eruption and atmospheric dynamics tephra2 however can be substituted with any other model in the model data fusion to better capture the physical processes of the eruption and atmospheric condition the use of data in the fusion approach can improve the irreducible errors that even complex models cannot capture due to model approximations and unaccounted processes inherent in any model 6 conclusions we demonstrate several methodological improvements in the inversion and forward estimation modelling processes when using limited uncertain and spatial data we test these using a case study of the eruption of the kelud volcano in 2014 using data on tephra thickness collected from two field surveys we conduct a model inversion to estimate the eruption source parameters and use these in a forward model to estimate the spatial distribution of tephra in locations where no data was collected we make no specific conclusions about the eruption itself or the true distribution of tephra but instead use the test case to demonstrate important considerations and approaches to treating data in inversion forward estimation problems our methodological contributions come in three components 1 the selection of appropriate cost functions accounting for their behaviour and their associated implied distribution of residuals 2 the treatment of differential uncertainty when combining multiple data sets and 3 the leveraging of both forward model and data when estimating the spatial distribution of output in the inversion setting this study places attention on the choice of the cost function to define the best fit between modelled output and observations this modelling choice receives little attention but has a significant impact on the results as seen in fig 5 we demonstrated how the characteristics of cost functions and their inherent assumptions on the distribution of model residuals can be used to inform the selection of appropriate cost functions we also demonstrate the use of data weighting in the inversion process to account for the differential uncertainty in data varying levels of uncertainties associated with different measurements are taken into account by weighting each observation s contribution to the cost function accordingly in the forward estimation we propose to combine the forward modelled output with measured data using spatial statistics techniques the output is an updated map of tephra fallout that accounts for both the model and the observations both the uncertainty in tephra measurements and their spatial correlation are accounted for in the fusion approach the importance placed in the model and data can be balanced by the choice of the spatial model and its parameters one of the strengths of the weighting approach in both inversion and forward model settings is that where multiple datasets are available the approach enables making use of all data even if some are highly uncertain while accounting for this uncertainty with the increasing availability of high volume low reliability information e g crowd sourced data from social media these can nonetheless be used to improve the modelling this work serves applications where the inversion and forward prediction workflows are conducted sequentially and applications where they are done separately while the test case is focused on tephra2 the methods are applicable to other models that reconstruct the process of tephra fallout such as ashfall and fall3d hurst and turner 1999 folch et al 2009 beyond applications related to tephra fallout the guidance could be relevant to those conducting any inversion or calibration modelling using spatial data in earth environment and hazards analysis code and data availability software name tephra2 lead developers laura j connor lconnor usf edu costanza bonadonna costanza cas usf edu see bonadonna et al 2005 and connor and connor 2006 year first available 2013 version used in the study 2 0 updated 01 27 2018 program language c license gnu general public license v3 0 availability https github com geoscience community codes tephra2 program size 963 kb hardware requirements the inversion model requires mpi message passing interface libraries and should be run on a computing cluster with multiple compute nodes the forward model can be compiled without mpi installed we have modified the tephra2 source code described above to accommodate different cost function types and weighted cost functions for the kriging presented in the study to combine model estimates and data we developed scripts written in the r programming language r 4 0 2 r core team 2020 the modified tephra2 source code r script for kriging and processed data are available in github https github com ntu dasl sg tephra spatial public this repository was created by maricar l rabonza corresponding author in 2022 the authors conducted inversion calculations using the komodo computing cluster managed by the asian school of the environment contact edwin tan edwintan ntu edu sg and nanyang technological university s high performance computing center hpcsupport ntu edu sg remote access to this computing cluster is possible with pc mac linux credit authorship contribution statement maricar rabonza designed the research writing original draft discussed the results commented on the manuscript michele nguyen designed the research discussed the results commented on the manuscript sebastien biass provided guidance in conducting tephra inversion modelling discussed the results commented on the manuscript susanna jenkins discussed the results commented on the manuscript benoit taisne discussed the results commented on the manuscript david lallemant designed the research discussed the results commented on the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was funded by the earth observatory of singapore and is eos contribution number 529 this research is supported by the national research foundation singapore and the singapore ministry of education under the nrf nrff2018 06 award we are grateful to edwin tan for his support in the use of the high performance computing cluster komodo in the earth observatory of singapore we thank george williams for his support and insights related to the tephra load datasets appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2023 105750 appendix a supplementary data the following is the supplementary material related to this article mmc s1 kriging and loocv steps fusion result uncertainty and mape 
25379,an hourly water temperature time series recorded at the gairn catchment scotland is analysed here along with seven covariates modelling river temperature time series is important due to its influence on stream biochemical processes and aquatic ecology due to its high complexity the dynamics of the water temperature is investigated through non homogeneous markov switching autoregressive models msarms in order to efficiently tackle the non linearity non normality non stationarity and long memory of the series which are issues usually not considered by previous approaches to water temperature modelling msarms are observed state dependent autoregressive processes driven by an unobserved or hidden markov chain bayesian inference model choice and stochastic variable selection are performed numerically by markov chain monte carlo algorithms hence it is possible to efficiently fit the data reconstruct the sequence of hidden states restore the missing values classify the observations into a few regimes and select the covariates that drive both the observed and the hidden process providing new insight on water temperature dynamics our proposal is very general and flexible and can be applied to any kind of environmental time series keywords marginal likelihood metropolis within gibbs non linearity non normality non stationarity stochastic variable selection data availability the data set and all of the codes used in this work are archived on the public repository figshare https figshare com at the doi https doi org 10 6084 m9 figshare 21801004 the codes were implemented in digital visual fortran 6 0 with calls to the imsl library and run on a laptop hp probook 6570b 3rd generation intel core i5 3210m 2 50 ghz 3 mb l3 cache 35 w 2 cores the run times of the estimation codes were 200 min for the whole series 101000 iterations t 46224 m 3 p 5 one selected covariate 230 min for the intermediate series 260000 iterations t 19440 m 3 p 6 two selected covariates and 12 min for the short series 510000 iterations t 1200 m 1 p 3 three selected covariates respectively computing time can be highly reduced by parallelising the code an r package for msarms is in preparation 1 introduction river temperature and its temporal variability at sub daily to seasonal scales influence in stream biochemical processes and aquatic ecology e g the life cycle and habitat of salmonid species acuña et al 2008 elliott and elliott 2010 zheng and bayani cardenas 2018 the dominant controls on river temperature and its variations are thermal inputs into the river system and the entire catchment further influences include hydrology e g stream flow volume snow melt base flow contribution and channel and landscape characteristics such as riparian vegetation toffolon and piccolroaz 2015 dick et al 2017 albertson et al 2018 information on the temporal variability of river temperature and its drivers is important for understanding temperature dependent environmental processes but also for environmental management and planning merriam and petty 2019 jackson et al 2018 caissie et al 2017 therefore modelling approaches are applied to estimate river temperatures under potential future climate and land cover changes buccola et al 2016 cao et al 2016 reconstruct past river temperature moatar and gailhard 2006 pohle et al 2019 and to extend the spatial coverage of river temperature observations jackson et al 2017 glose et al 2017 river temperature has been modelled using physical based models based on the energy balance cox and bolte 2007 cheng and wiley 2016 glose et al 2017 as well as statistical approaches of varying complexity ranging from simple regression models mohseni et al 1998 via generalised additive models gams jackson et al 2018 pohle et al 2019 to machine learning approaches sapin et al 2017 zhu et al 2019 modelling water temperature time series is complicated by a range of issues including non normality non linearity non stationarity and long memory non normality is observed when the data density is multimodal or asymmetric or kurtic and the data cannot be considered as realisations from a gaussian process non linearity is assumed when the whole series does not show the same statistical peculiarities over all the observations but they can be classified into a few homogeneous groups each one with specific characteristics e g different means and or different variances non linearity can also be assumed when the series exhibits asymmetries e g when peaks are sharper or more rounded than the troughs and or when the cycles increase at a different rate from which they decrease weak non stationarity is caused by generating processes having time varying means and autocovariances finally when the series shows high autocorrelations at the higher lags with a slow decay the observations are realisations from a long memory process in this work we investigate the dynamic variability of water temperature by analysing an hourly water temperature time series automatically recorded in the gairn catchment scotland for more than five years along with some covariates we propose an alternative approach to address the complexities of water temperature time series modelling using markov switching autoregressive models msarms this class of models is a popular tool within the econometric community to model complex time series they have been widely applied in monetary financial and macro economic statistics following seminal work by hamilton 1989 1990 1993 although they are extremely powerful msarms have been considered quite rarely in other disciplines including environmental sciences among the few applications in hydrology lu and berliner 1999 and gelati et al 2010 modelled runoff processes birkel et al 2012 isotope signatures spezia et al 2021 turbidity measurements our approach allows efficiently tackling all complex issues of the series such as non linearity non normality non stationarity and long memory which are usually not considered by previous approaches to water temperature modelling available in the literature msarms are pairs of discrete time stochastic processes one observed and one latent or hidden the hidden process is a finite state markov chain whereas the observed process given the markov chain is conditionally autoregressive the dynamics of the observed process is driven by the dynamics of the latent one so that each observation depends on the contemporary state of the markov chain by this theoretical structure msarms allow i modelling non linear and non normal time series by assuming that different autoregressions each one depending on a hidden state alternate according to the markovian regime switching ii classifying the observations into a small number of homogeneous groups labelled as the regimes of the markov chain the slow decay of the autocorrelation function acf is due to both the non linearity of the series and the automatic recording of the data at a high temporal frequency habib 2020 as pointed out by diebolt and inoue 2001 non linear time series with structural changes produce realisations that appear to have long memory so structural change and long memory are effectively different labels for the same phenomenon given that structural changes can be efficiently described by stochastic regime switching models we adopted msarms to highlight the changes in the water temperature dynamics classify the observations into a few states and fit the long memory process of the water temperature dynamics seven covariates were also incorporated into the model through both the hidden markov chain the transition probabilities are time varying and dependent on the dynamics of these exogenous variables and the observed process the state dependent exogenous variables are added to the past observations thus we have time varying means and autocovariances and hence a non stationary model the covariates are river flow air temperature rainfall wind speed wind direction radiation and soil temperature the data set is also characterised by periodicities the hourly temperatures vary according to the dynamics of the year and of the 24 h of the day hence both an annual and a state dependent daily harmonic component are added to the observed process in this paper msarms are proposed within the bayesian framework bayesian inference model choice and variable selection are performed numerically by markov chain monte carlo mcmc algorithms the basic scheme for parameter estimation in the observed process is gibbs sampling which also allows both restoration of the missing values occurring within the series of observations and reconstruction of the sequence of hidden states two random walk metropolis moves are used to estimate the parameters of the hidden markov chain adding extra steps with no further coding activity to the basic metropolis within gibbs scheme we can also compute the marginal likelihood of the various competing models through the mcmc sample this procedure enables us to select the best model within a set of models varying for the number of hidden states and the order of the autoregressive processes the exogenous deterministic variables appearing in the observed process may be different in any state and they may be different from those affecting the transition probabilities the transition matrix is affected by two sets of covariates possibly different from each other and different from those in the observed process one for the transitions from a lower to a higher state and another for the transition from a higher to a lower state the selection of the covariates appearing in each state dependent autoregression and in the two triangles of the transition matrix is performed stochastically through an mcmc algorithm several approaches have been introduced in the bayesian literature for selecting explanatory variables in regression models an exhaustive review on bayesian variable selection can be found in o hara and sillanpää 2009 the most intuitive approach is to associate each covariate with a binary indicator when the indicator equals one the corresponding variable is included in the model whereas when it equals zero the covariate is excluded among the different available methods paroli and spezia 2008 proposed the metropolised kuo mallick mkmk method based on the metropolisation of the algorithm of kuo and mallick 1998 they demonstrated in the case of non homogeneous hidden markov models and msarms with covariates that the mkmk method improves the performance of the competing techniques especially when the explanatory variables are strongly correlated and or when the complexity of the model is high in mkmk the better performance in selecting the explanatory variables is due to the acceptance in a single block of the indicators of the explanatory variables because blocking increases the precision of the estimators and improves the mixing of the mcmc algorithm therefore we propose msarms to the hydrological community to model complex time series i e non linear non normal non stationary and with a long memory within the bayesian paradigm a water temperature time series along with a few covariates are used to illustrate the methodology the application is based on the implementation and the application of mcmc algorithms which stochastically enable model choice variable selection fitting of the actual data classification of the observations into a few regimes and reconstruction of the missing values the structure of the paper is as follows the study site the instruments that automatically recorded the data and the available time series are described in section 2 the msarm used to analyse the sequence of hourly water temperature observations under the bayesian paradigm is illustrated in section 3 all results from the water temperature application are illustrated in section 4 a discussion and some conclusions finalise the paper the mcmc algorithms implemented for bayesian inference variable selection and model choice are presented in the annexed auxiliary material where further tables and figures are also available 2 study site and data the gairn catchment is a 147 km2 subcatchment of the river dee in north east scotland coordinates of the sampling point 320272 easting and 801792 northing according to the british national grid see map in fig 1 the mean altitude of the catchment is 560 m above sea level and ranges from 220 m at the catchment outlet to 1150 m at the western watershed divide the climate is temperate the long term mean air temperature recorded ca 10 km west of the catchment outlet is 6 8 c standard reference period 1981 2010 january mean 1 5 c july mean 13 5 c the long term mean total annual precipitation is 1010 mm reference period 1981 2010 highest precipitation in october 148 mm and lowest in june 75 mm the catchment land use is dominated by heather 40 heather grassland 10 montane habitats 28 acid grassland 9 bog 9 and broadleaved and coniferous woodland around 1 each according to lcm 2007 morton et al 2011 river temperature data at the catchment outlet were recorded using calibrated tinytag aquatic 2tg 4100 dataloggers gemini data loggers reading accuracy 0 01 c as maximum temperature of 15 min intervals meteorological records were provided from the weather station at the culardoch experimental site britton and fisher 2007 at the southern watershed divide discharge data at the catchment outlet station invergairn were provided by the scottish environment protection agency a time series of water temperatures recorded hourly in the river gairn from 16th august 2012 to 23rd november 2017 is analysed here the length of the series is 46224 points i e 1926 days more than five years with 328 missing values 0 71 of the total number of observations the range of the series is between 0 02 c and 22 41 c the contemporary series of the hourly river flows is also available we also studied intermediate series of water temperature from 13th june 2014 to 31st august 2016 19440 observations 810 days more than two years with 209 missing values 1 08 along with three covariates flow air temperature rainfall finally a short series was considered 1200 observations 50 days with no missing values recorded from 18th august to 6th october 2012 along with seven covariates flow air temperature rainfall wind speed wind direction radiation soil temperature the length of series of the exogenous variables was limited by the need to not have missing values in these deterministic sequences this is because missing values within the covariates might bias the results of our analyses the water temperature time series is reproduced in fig 2a in order to have a better picture of the dynamics of the series the first 1600 observations are plotted as well fig 2b the two plots exhibit asymmetries supporting the application of a non linear model another major issue of the water temperature time series is the non normality that clearly emerges from the histogram of the series fig 3a the histogram shows the presence of many peaks multimodality and a relevant asymmetry the long memory of the water temperature process is noticeable in fig 3b along with the daily periodicity the slow decay of the acf with high autocorrelations also at the higher lags was assumed to be due to the regime switching process and then tackled from the non linear side diebolt and inoue 2001 the long memory process was confirmed by the results of the qu test on the de seasonalised series performed through the package longmemoryts leschinski et al 2022 the dynamics of the water temperatures was analysed by non stationary models where both the observed and the hidden process were functions of different subsets of exogenous variables stochastically selected from an initial set of seven covariates flow air temperature and soil temperature were standardised centring the variable and dividing by its standard deviation to avoid any distortion due to the magnitude of the values and or their variability on the other hand rainfall was neither centred to preserve the zero values associated with the absence of rain nor rescaled because the variance was less than one whereas radiation was in the short range of 0 and 0 62 and thus not transformed finally wind speed a positive variable and wind direction a circular variable between 0 and 2 π were mapped onto the real line between and according to the transformation of ailliot et al 2015 which preserves circularity and then the two new variables u and v were standardised i e u w i n d s p e e d c o s w i n d d i r e c t i o n v w i n d s p e e d s i n w i n d d i r e c t i o n the time series of the seven exogenous variables are shown in figures 1 and 2 of the auxiliary material along with their transformations when adopted because of the non linearity the non normality and the long memory of the series msarms are an appropriate tool to analyse the dynamics of the water temperature time series and produce an helpful interpretation of both the observed and the latent phenomena 3 markov switching autoregressive models msarms are discrete time stochastic processes y t x t such that y t is a sequence of observed events on the real line whereas x t is an unobserved finite state markov chain hidden behind the observed process the sequence y t given x t must satisfy both the order p dependence and the contemporary dependence condition each random variable y t t p 1 t depends on the p previous observations y t 1 y t p and its conditional distribution given the whole sequence of hidden states depends only on the contemporary hidden state or regime x t let s x 1 m be the state space of the hidden markov chain where γ γ j i is the m m transition matrix with γ j i p x t i x t 1 j 0 γ j i 1 for any i j s x and any t p 1 t when x t i t p 1 t i 1 m the equation describing the msarm of order m p henceforth msar m p is 1 y t μ i τ 1 p φ i τ y t τ e t where e t is a state dependent normal white noise process of variance λ i 1 with μ the vector of the m intercepts μ i φ φ i τ the matrix of the m p autoregressive coefficients τ 1 p and λ the vector of the m precisions λ i besides the state dependent autoregressions the observed process may also include both explanatory variables and periodic components let z t z t 1 z t q be a vector of q exogenous variables recorded at time t t p t 1 and z t 1 z p z t z t 1 be the matrix of all covariates recorded at the previous time with respect to the current observation thus 1 can be generalised as 2 y t μ i τ 1 p φ i τ y t τ k 1 q θ i k z t 1 k η t β t i e t with θ θ i k the matrix of the m q covariate coefficients i 1 m k 1 q in formula 2 η t is a harmonic component of periodicity 2 s η t j 1 s η 1 j cos π j t s η 2 j sin π j t s where s is the number of relevant harmonics s s with η η 1 1 η 2 1 η 1 s η 2 s β t i is a harmonic component of periodicity 2 r depending on the current state i of the markov chain β t i j 1 r β i 1 j cos π j t r β i 2 j sin π j t r where r is the number of relevant harmonics r r with β β 1 1 1 β 1 2 1 β 1 1 r β 1 2 r β m 1 r β m 2 r in the case of an annual periodicity 2 s 365 whereas for a daily cycle 2 r 24 the hidden markov chain can also be generalised by assuming the transition probabilities are time varying and hence modelled as logistic functions of the covariates γ t γ j i t γ j i t p x t i x t 1 j with 3 l o g i t γ j i t l n γ j i t γ j j t α j i 0 k 1 q α j i k z t 1 k where α α j i is the m m matrix either of the vectors α j i 0 α j i 1 α j i q if j i or the null vector if j i some of the q state dependent exogenous variables affecting the observed process might be redundant according to the state visited by the hidden markov chain redundancies in the covariates might also be present in those appearing in the time varying transition probabilities a selection of the most relevant explanatory variables is recommended for a better interpretation of both the observed and the latent phenomenon in order to perform a stochastic selection within the different sets of covariates each exogenous variable is associated with a binary indicator when the indicator equals one the corresponding variable is included in the model whereas when it equals zero the covariate is excluded let ω ω i k be the matrix of the m q indicators associated with the set of q exogenous variables i 1 m k 1 q hence 2 can be generalised as 4 y t μ i τ 1 p φ i τ y t τ k 1 q θ i k ω i k z t 1 k η t β t i e t further let δ k k 1 q be the vector of indicators associated with the set of q covariates affecting the transitions from a higher state to a lower one lower triangle of the transition matrix and ψ k k 1 q be the vector of indicators associated with the set of q covariates affecting the transitions from a lower state to a higher one higher triangle of the transition matrix therefore 3 can be generalised as l o g i t γ j i t α j i 0 k 1 q α j i k δ k z t 1 k i j i k 1 q α j i k ψ k z t 1 k i j i where i a is the indicator function which assumes the value of one when a is true and zero otherwise the vector of the unknown parameters of a msar m p is μ λ φ θ η β α ω δ ψ x t y m i s s where x t x p 1 x t is the sequence of hidden states and y m i s s the set of all missing values occurring within y t y 1 y t in our application y t is the sequence of the water temperatures with q 1 when n 46 224 q 3 when n 19 440 q 7 when n 1200 r and s are both equal to one z t 1 is the sequence of the standardised flows z t 2 is the sequence of the standardised air temperatures z t 3 is the sequence of the rainfalls z t 4 and z t 5 are the sequences of the standardised transformations u and v of the wind speeds and directions z t 6 is the sequences of the solar radiations and z t 7 is the sequence of the standardised soil temperatures from 4 the generic distribution of y t given the p past observations and the current hidden state x t i is conditionally normal with mean μ i τ 1 p φ i τ y t τ k 1 q θ i k ω i k z t 1 k η t β t i and precision λ i whereas the marginal distribution of each observation is a mixture of normals whose mixing distribution is given by the probability of the markov chain to visit the states at that time the mixing distribution can be obtained as the left eigenvector of the transition matrix associated with the unit eigenvalue δ t δ t γ t where δ t δ 1 t δ m t with δ i t p x t i for any i s x and t p 1 t for a better interpretability of the results we assume the hidden regimes persist for all of the 24 h of the day therefore it can serve our purpose to replace the time t subscript with the day d and hour h subscripts so that t d 1 24 h where d 1 d t 24 and h 1 24 and to assume x d 1 24 1 x d 1 24 h x 24 d for any d 1 d x d x 1 x d thus the time varying transition probabilities are computed as functions of the covariates recorded at 12 am l o g i t γ j i d α j i 0 k 1 q α j i k δ k z d 2 24 12 k i j i k 1 q α j i k ψ k z d 2 24 12 k i j i with d 2 d for our bayesian inference we set the following independent priors all μ i φ i τ θ i k and α j i k are vague normals of mean 0 and variance 10 vectors η h u and β i h v are multivariate normals mvns whose mean is the null vector and covariance matrix is a diagonal matrix with all diagonal entries equal to 10 all λ i are non informative gamma of parameters 0 01 whose mean is 1 and variance 100 all ω i k δ k and ψ k are bernoulli of parameter 0 5 a priori the probability of including a covariate is equal to the probability of its exclusion all missing values are uniform over the range of y t j i 1 m τ 1 p k 0 q h 1 2 u 1 s v 1 r model 1 and hence models 2 and 4 are unidentifiable in data fitting when we have m states we have m ways to label them when the priors are invariant to the different state permutation m different models are interchangeable by permuting their labelling this is the so called label switching problem see among others mcculloch and tsay 1994 frühwirth schnatter 2001 spezia 2009 and it can be overcome by placing some parameters in increasing or decreasing order experience birkel et al 2012 spezia et al 2021 suggests that in hydrological time series hidden states can be ordered by increasing levels of variability thus this constraint is included in the prior 5 p λ m i 1 m λ i i λ 1 λ 2 λ m 4 results the dynamics of the water temperature time series fig 2 was analysed by applying the msarms described in section 3 and the mcmc algorithms fully described in the auxiliary material we considered three sequences of data i the whole time series along with the standardised river flow as an exogenous variable t 46 224 q 1 ii an intermediate series along with standardised flow standardised air temperature and rainfall as exogenous variables t 19 440 q 3 iii a short series along with standardised flow standardised air temperature rainfall radiation standardised soil temperature and standardised variables u and v as exogenous variables t 1200 q 7 representing wind speed and wind direction according to the transformation of ailliot et al 2015 described in section 2 the seven dynamic covariates are reproduced in figures 1 and 2 of the auxiliary material we only worked with sequences of covariates with no missing values to not bias the results of our analyses this explains why we had three series of different lengths and different numbers of covariates for each of the three series the bayesian analysis was developed in four steps variable selection model choice prior coherence and parameter estimation for the whole the mcmc algorithms were run for 100000 iterations after a burn in of 1000 iterations thinning the sample at every 10th iterations in all steps for the intermediate series 250000 iterations after a burn in of 10000 iterations thinning at every 25th iterations for the short 500000 iterations after a burn in of 10000 iterations thinning at every 50th iterations when computing the marginal likelihood the number of additional iterations for each parameter were a fifth of those of the sample 4 1 variable selection and model choice for the whole series we considered 21 alternative models for any number of hidden states m 1 2 3 and autoregressive order p 0 1 6 models with m 3 were discarded because they always produced empty regimes that is only three regimes were visited by the hidden markov chain within the sequence of observations we have 328 missing values gathered in three large groups of 119 138 and 71 consecutive points missing observations could be filled by simulated values that allowed analysing the whole series as a single block for each of these competing models we stochastically selected if the single covariate we had flow had influenced both the observations and the time varying transition probabilities the results of the selection are reported in table 1 the best model is that with three hidden states and autoregressive order equal to five i e five hours flow was selected in the observed process for the first two regimes but not the third while it was not selected in the unobserved process that is the hidden markov chain was homogeneous this means that flow is an important driver of river temperature in two states but not in the third state when it is the only available covariate and that all states might be driven by another important set of unobserved variables represented by the latent process for the intermediate series we considered again 21 alternative models for any m 1 2 3 and p 0 1 6 having empty regimes for m 3 within the sequence of observations we have 209 missing values gathered in two large groups of 138 and 71 consecutive points that could be filled by simulated values for each of these competing models we stochastically selected if the three available covariates flow air temperature rainfall had influenced both the observations and the time varying transition probabilities the results of the selection are reported in table 2 the best model is that with three hidden states and autoregressive order equal to six i e six hours we did not try any larger autoregressive order because six was demonstrated to be sufficient air temperature was always selected in both the observed and the hidden process while flow was selected for the second state of the observed process only for the short series we considered 14 alternative models for any m 1 2 and p 0 1 6 having empty regimes for m 2 for each of these competing models we stochastically selected the covariates within a set of seven flow air temperature rainfall transformed wind speed transformed wind direction radiation soil temperature the results of the selection are reported in table 3 the best model is that with no hidden states m 1 and autoregressive order equal to three i e three hours air temperature radiation and soil temperature were selected in the observed process the hidden chain is not involved in this model 4 2 prior coherence once the best model was available then we needed to check the coherence of the constraint on the precisions in fact as mentioned in section 3 an identifiability constraint was included in the prior based on previous knowledge in hydrological time series analysis experience suggested to place the precisions in decreasing order i e increasing levels of the state dependent variability before moving to parameter estimation we needed to check if the constraint on the precisions was representative of the data and respected the geometry of the posterior density thus we plotted the values of the intercepts and the precisions sampled from the unconstrained posterior frühwirth schnatter 2001 from figure 3 in the auxiliary material it is evident that we had three separated groups of precisions for the whole series that can be ordered decreasingly while there is no separation between the intercepts groups looking at figure 4 in the auxiliary material we came to the same conclusions for the intermediate series this operation was not necessary for the short series because we have one regime only 4 3 parameter estimation after the dimensions of the model were chosen the exogenous variable selected and the identifiability constraint checked the whole set of parameters for the whole the intermediate and the short series could be estimated first the estimates for the whole series were computed see table 1 of the auxiliary material the flow in the first two states of the observed process have a coefficient very close to zero while it was excluded in the third state also the annual periodicity has coefficients very close to zero that means that the annual cycle could be explained sufficiently well by the switching dynamics of the water temperature the transition matrix of the homogeneous hidden markov chain the flow was not selected in the latent process is 0 772 0 210 0 018 0 214 0 718 0 068 0 136 0 633 0 231 the permanence probabilities of the three hidden states i e 0 772 0 718 and 0 231 decrease as the labels of the state i e 1 2 3 increase as a consequence the corresponding mean time spent in a state decreases as well 4 39 3 55 and 1 30 days the mean time spent in each state has a geometric distribution of parameter the reciprocal of the complement to one of the corresponding diagonal probability we could also reconstruct the sequence of the hidden states by computing the posterior mode of x d over the mcmc sample fig 4a the markov chain shows an annual dynamics which anticipates the annual dynamics of the water temperatures the hidden markov chain visited states 1 2 and 3 a total of 914 times 912 times 100 times respectively given the latent regimes the observations were classified into the three hidden states with increasing levels of variability fig 5 state 1 is mostly associated with observations in the decreasing part and the bottom of the cycle state 2 mostly with the increasing part the top and the beginning of the decreasing part of the cycle and state 3 mostly with the increasing part of the cycle the model fit was very satisfactory as shown by the comparison of actual and fitted values see the whole series in fig 6a and the two sub series of 1000 observations in figures 5a and 5b in the auxiliary material the model performance was assessed by the root mean squared error r m s e and the mean absolute error m a e which are very low they are 0 137 0 6 of the range of the data and 0 072 0 3 respectively all observations were within the 99 credibility interval one of the major issues encountered in previous studies was the modelling of the long memory process by using msarms with uncorrelated noises this study was able to explain the slow decay of the autocorrelations in terms of stochastic regime switching confirming what was stated by diebolt and inoue 2001 the non linearity of the water temperature time series was well captured by the dynamics of the hidden states and the very low level of correlation in the residuals is evident figures 6a and 6b in the auxiliary material normality of the various series of state dependent residuals figures 7a 7b 7c in the auxiliary material was also verified through the shapiro wilk and jarque bera normality tests although the acf decreases close to zero at the first lag figures 6a and 6b in the auxiliary material show the presence of weak daily and annual patterns in the acf dynamics to remove these periodicities we added a few features to eq 2 twelve alternative models with m 3 and p 5 were tried we considered four models adding observations at distant lags i e y t 24 y t 24 y t 48 y t 24 y t 48 y t 72 and y t 24 y t 48 y t 72 y t 96 the same four models were run again with two or four harmonic components in both the daily and the annual periodicity there was no increase in the marginal likelihoods and the periodicities in the acfs were not removed by adding extra complexity in the model this fact suggested that these patterns might be removed by including some periodic covariates in the observed process as we tried both for the intermediate and the short series next the full set of parameters of the intermediate series was estimated the estimates are reproduced in table 2 of the auxiliary material again the annual periodicity has coefficients very close to zero this indicates that the annual fluctuations were mostly explained by the autoregressions and the covariates in order to have a clearer picture we also computed the mean transition matrix between the three states the mean of the 809 time varying transition matrices 0 674 0 308 0 018 0 288 0 630 0 082 0 482 0 257 0 261 the permanence probabilities i e 0 674 0 630 and 0 261 decrease as the labels of the state i e 1 2 3 increase and the corresponding hypothetical mean time spent in a state decreases as well 3 07 2 70 and 1 35 days after that we reconstructed the sequence of the hidden states by computing the posterior mode of x d over the mcmc sample fig 4b the markov chain shows again an annual dynamics which anticipates the annual dynamics of the water temperatures the estimated hidden states in this series are the same of those obtained in the whole series during the same time interval of the intermediate series apart from a small number of days 4 due to the different covariates the hidden markov chain visited states 1 2 and 3 a total of 377 days 376 days 57 days respectively given the latent regimes the observations were classified into the three hidden states with increasing levels of variability fig 7 again state 1 is mostly associated with observations in the decreasing part and the bottom of the cycle state 2 mostly with the increasing part the top and the beginning of the decreasing part of the cycle and state 3 mostly with the increasing part of the cycle the model fit was very satisfactory as shown by the comparison of actual and fitted values see the whole series in figure 6b and two sub series of 1000 observations in figure 5 of the auxiliary material the r m s e and m a e are very low 0 131 0 6 of the range of the data and 0 074 0 3 respectively all observations were within the 99 credibility interval the non linearity of the water temperature time series was well captured by the switching dynamics of the hidden states and the very low level of correlation in the residuals is evident figures 6a and 6b in the auxiliary material normality of the various series of state dependent residuals figures 7d 7e 7f in the auxiliary material was also verified through the shapiro wilk and jarque bera normality tests the acf decreases close to zero at the first lag figures 6a and 6b in the auxiliary material but weak daily and annual patterns are present in the acf dynamics twelve alternative models with m 3 and p 5 were tried to remove these periodicities from the residuals we considered four models adding observations at distant lags i e y t 24 y t 24 y t 48 y t 24 y t 48 y t 72 and y t 24 y t 48 y t 72 y t 96 to eq 2 the same four models were run again with two and four harmonic components in both the daily and the annual periodicity although there was an increase in the marginal likelihoods the periodicities in the acfs were not removed and the extra complexity added to the model was not beneficial the inclusion in the model of the whole set of seven covariates might be helpful in the analysis of the short series to remove the weak periodicities from the residuals finally the whole set of parameters of the short series was estimated the estimates are reproduced in table 3 of the auxiliary material the model fit was very satisfactory as shown by the comparison of actual and fitted values see the whole series in figure 7c and two sub series of 100 observations in figures 5e and 5f of the auxiliary material the r m s e and m a e are very low 0 098 0 7 of the range of the data and 0 068 0 5 respectively all observations were within the 99 credibility interval the residuals were normal and non correlated normality figure 7g in the auxiliary material was also verified through the shapiro wilk and jarque bera normality tests the acf decreases close to zero from the first lag figures 6a and 6b in the auxiliary material and does not present any periodic pattern this is due to the inclusion of the solar radiation and the soil temperature in the observed process it would be very interesting to verify this result on a longer multi state series 5 discussion and conclusions we demonstrated that msarms are a useful tool to reconstruct high frequency environmental time series with exogenous variables a fully bayesian model was developed and we applied our methodology to study the dynamic evolution of hourly water temperature in the river gairn scotland it is evident that the complexity of the series non linearity non normality non stationarity long memory was efficiently tackled by our msarms the computational machinery we used is based on mcmc algorithms of the metropolis within gibbs type they allowed us to perform bayesian model choice variable selection and inference including the reconstruction of the hidden markov chain and the classification of the observations into a finite number of regimes as well as the generation of the missing values the classification of the observations followed the increasing variability constraint as set in the prior formula 5 to tackle the identifiability problem of the model and confirmed by figure 3 of the auxiliary material the same algorithms also allowed us to choose the best model i e the number of hidden states and the autoregressive order among many alternatives the choice was done by computing the logarithm of the marginal likelihood the choice of the bayesian approach and the implementation of mcmc algorithms give clear practical advantages handling the missing values as unknown parameters allows restoring these observations after placing a prior on them further it is possible to stochastically select the covariates that affect both the observed and the hidden process this can be done in a single run of the algorithm and not repeatedly by removing a covariate at the time such as in the stepwise regression finally the reconstruction of the sequence of the hidden states is obtained in a single block rather than for each individual state the flexibility of the msarms was demonstrated by the three applications we considered for the whole series with a single covariate the best model had three hidden states and autoregressions of the fifth order thus the non linear model m 3 worked better than the corresponding linear model m 1 flow was relevant in the observed process for two states only while it was not selected in the hidden process and the markov chain was homogeneous for the intermediate series with three covariates the best model had three hidden states and autoregressions of the sixth order again the non linear model m 3 worked better than the corresponding linear model m 1 flow was relevant in the observed process for one state only while air temperature was always selected both in the observed and the hidden process the markov chain was non homogeneous and the transition probabilities time varying for the short series with seven covariates we obtained that the best model was the linear autoregression of the sixth order without a hidden markov chain m 1 air temperature solar radiation and soil temperature were the relevant variables to explain the water temperature dynamics thus discharge is a proxy for water temperature modelling when no other more directly related variables are available in those situations the latent states will help to model the long term dynamics in the absence of true predictors with a physical meaning as we saw in the first two applications the hidden regimes can have an interpretation related to the seasonality in fact the markov chain shows an annual dynamics which anticipates the annual dynamics of the water temperatures the observations in the decreasing part and the bottom of the cycle are those with the lowest variability and associated with state 1 those in the increasing part the top and the beginning of the decreasing part of the cycle have a higher level of variability and are associated with state 2 finally in state 3 we have the observations with the highest level of variability and they are mostly in the increasing part of the cycle it is not surprising that for the short series 50 days i e no annual periodicity the model is not multi state it would be interesting to see what happens when considering the seven covariates on longer series that is if the same covariates are selected in a non linear model m 1 in this work we considered only series of covariates with no missing values thus we handled 50 day long series when working with seven covariates 810 day long series with three covariates and a 1926 day long series with a single covariate this because we did not want that our analyses were biased by the missing values within the exogenous variables in the future we are interested in dealing with missing values in the deterministic sequences by either considering physical projections of these variables if available or reconstructing the missing values after assuming the exogenous variables as stochastic msarms can be extended in the future to apply them more straightforwardly in fully gibbs sampling algorithms without the tuning factors of the random walk metropolis moves in fact in the most general case the hidden markov chain is non homogeneous and the transition probabilities depends on the exogenous variables via multinomial logit link functions see formula 3 a few alternative auxiliary variable methods were developed by holmes and held 2006 frühwirth schnatter and frühwirth 2007 and polson et al 2013 to perform bayesian inference in multinomial logit models for independent data via the gibbs sampling these data augmentation techniques were applied to non homogeneous hidden markov chain by meligkotsidou and dellaportas 2011 kaufmann 2015 holsclaw et al 2017 and koki et al 2020 a systematic comparison via simulation studies of the different auxiliary variable techniques based on the gibbs sampling is needed in the light of data augmentation methods applied in the stochastic variable selection under conditions of non stationarity this approach may be better suited for simulation of future changes under climate change stein 2020 following lu and berliner 1999 and gelati et al 2010 as it may be better suited to simulate future projections under conditions of non stationarity meligkotsidou and dellaportas 2011 koki et al 2020 projecting beyond observed periods is a known problem this approach might be more robust in that sense if projections in the future of the covariates are available to conclude we believe that we have demonstrated a valuable approach to explain the hourly variability of the water temperature in the river gairn and this can be used to reconstruct complex long term environmental time series and may help to further a better understanding of the impacts of climatic changes on the hydrology and the ecology of a catchment our study provides a novel application of the suitability of the msarms in hydrological time series analysis and environmental sciences in general we hope our work can motivate other scientists to approach msarms and give their highly structured time series exhibiting non linearity non normality non stationarity long memory a valuable interpretation modelling river temperature from covariates gives insight into physical processes governing river temperature dynamics air temperature is identified as the most important covariate as in many other river temperature studies showing that river and air temperature respond to thermal inputs in a relatively similar way johnson et al 2013 also solar radiation representing direct thermal input and soil temperature representing warming of the catchment are important this is consistent with other studies including day length related to solar radiation and thus thermal input and cumulative air temperature representing warming of the catchment to simulate river temperature e g pohle et al 2019 loerke et al 2022 however also flow plays a role during the summer state 2 showing the importance of hydrology rainfall runoff processes on river temperature dynamics during the biologically relevant summer low flow periods as also found by e g arora et al 2016 over the past decades scientific understanding in hydrology has been greatly aided by long term data rainfall flow with high temporal and spatial resolution however water quality observations are typically collected at lower temporal resolution leading to higher uncertainties in models and limitations in process understanding kirchner 2006 this is especially the case in upland areas which are disproportionally important for water supply but also riverine water quality e g alexander et al 2017 bol et al 2018 yet often lack long term monitoring data meanwhile high resolution water quality data from sensors can be a powerful line of evidence to facilitate stakeholder buy in and adaptive management however collecting high quality sensor data in real world river catchments is challenging as sensors are susceptible to fouling extreme environmental events and occasional malfunction again upland catchments are more prone to these challenges also due to limited accessibility hence processing of a large quantities of high resolution data is challenging and requires rigorous data quality control identification of spurious data points and filling of gaps ideally in an objective automated way rather than manual inspection to our knowledge this work demonstrates the first application of msarms to the analysis of river water temperature data and provides the first example of msarms applied to the analysis of high frequency hydrological data by performing inference variable selection and model choice under the bayesian paradigm the high performance of msarms compared to other river temperature modelling approaches such as gams jackson et al 2018 pohle et al 2019 highlights the potential of the msarm modelling approach for river temperature this can also be explained by river temperature time series exhibiting generally comparable properties as air temperature for which msarms have been successfully applied e g by monbet and ailliot 2017 the general modelling approach is transferable to other catchments the model predicting river temperature from covariates can be used to reconstruct high resolution river temperature for the past based on covariates but also for quality control when applied to weather forecast or climate scenarios rather than observed covariates the model can be used to simulate high resolution river temperature in short and long term futures declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the scottish government s rural and environment science and analytical services division uk comments from katherine whyte and two anonymous referees improved the quality of the final paper appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2023 105751 appendix a supplementary data the following is the supplementary material related to this article mmc s1 
25379,an hourly water temperature time series recorded at the gairn catchment scotland is analysed here along with seven covariates modelling river temperature time series is important due to its influence on stream biochemical processes and aquatic ecology due to its high complexity the dynamics of the water temperature is investigated through non homogeneous markov switching autoregressive models msarms in order to efficiently tackle the non linearity non normality non stationarity and long memory of the series which are issues usually not considered by previous approaches to water temperature modelling msarms are observed state dependent autoregressive processes driven by an unobserved or hidden markov chain bayesian inference model choice and stochastic variable selection are performed numerically by markov chain monte carlo algorithms hence it is possible to efficiently fit the data reconstruct the sequence of hidden states restore the missing values classify the observations into a few regimes and select the covariates that drive both the observed and the hidden process providing new insight on water temperature dynamics our proposal is very general and flexible and can be applied to any kind of environmental time series keywords marginal likelihood metropolis within gibbs non linearity non normality non stationarity stochastic variable selection data availability the data set and all of the codes used in this work are archived on the public repository figshare https figshare com at the doi https doi org 10 6084 m9 figshare 21801004 the codes were implemented in digital visual fortran 6 0 with calls to the imsl library and run on a laptop hp probook 6570b 3rd generation intel core i5 3210m 2 50 ghz 3 mb l3 cache 35 w 2 cores the run times of the estimation codes were 200 min for the whole series 101000 iterations t 46224 m 3 p 5 one selected covariate 230 min for the intermediate series 260000 iterations t 19440 m 3 p 6 two selected covariates and 12 min for the short series 510000 iterations t 1200 m 1 p 3 three selected covariates respectively computing time can be highly reduced by parallelising the code an r package for msarms is in preparation 1 introduction river temperature and its temporal variability at sub daily to seasonal scales influence in stream biochemical processes and aquatic ecology e g the life cycle and habitat of salmonid species acuña et al 2008 elliott and elliott 2010 zheng and bayani cardenas 2018 the dominant controls on river temperature and its variations are thermal inputs into the river system and the entire catchment further influences include hydrology e g stream flow volume snow melt base flow contribution and channel and landscape characteristics such as riparian vegetation toffolon and piccolroaz 2015 dick et al 2017 albertson et al 2018 information on the temporal variability of river temperature and its drivers is important for understanding temperature dependent environmental processes but also for environmental management and planning merriam and petty 2019 jackson et al 2018 caissie et al 2017 therefore modelling approaches are applied to estimate river temperatures under potential future climate and land cover changes buccola et al 2016 cao et al 2016 reconstruct past river temperature moatar and gailhard 2006 pohle et al 2019 and to extend the spatial coverage of river temperature observations jackson et al 2017 glose et al 2017 river temperature has been modelled using physical based models based on the energy balance cox and bolte 2007 cheng and wiley 2016 glose et al 2017 as well as statistical approaches of varying complexity ranging from simple regression models mohseni et al 1998 via generalised additive models gams jackson et al 2018 pohle et al 2019 to machine learning approaches sapin et al 2017 zhu et al 2019 modelling water temperature time series is complicated by a range of issues including non normality non linearity non stationarity and long memory non normality is observed when the data density is multimodal or asymmetric or kurtic and the data cannot be considered as realisations from a gaussian process non linearity is assumed when the whole series does not show the same statistical peculiarities over all the observations but they can be classified into a few homogeneous groups each one with specific characteristics e g different means and or different variances non linearity can also be assumed when the series exhibits asymmetries e g when peaks are sharper or more rounded than the troughs and or when the cycles increase at a different rate from which they decrease weak non stationarity is caused by generating processes having time varying means and autocovariances finally when the series shows high autocorrelations at the higher lags with a slow decay the observations are realisations from a long memory process in this work we investigate the dynamic variability of water temperature by analysing an hourly water temperature time series automatically recorded in the gairn catchment scotland for more than five years along with some covariates we propose an alternative approach to address the complexities of water temperature time series modelling using markov switching autoregressive models msarms this class of models is a popular tool within the econometric community to model complex time series they have been widely applied in monetary financial and macro economic statistics following seminal work by hamilton 1989 1990 1993 although they are extremely powerful msarms have been considered quite rarely in other disciplines including environmental sciences among the few applications in hydrology lu and berliner 1999 and gelati et al 2010 modelled runoff processes birkel et al 2012 isotope signatures spezia et al 2021 turbidity measurements our approach allows efficiently tackling all complex issues of the series such as non linearity non normality non stationarity and long memory which are usually not considered by previous approaches to water temperature modelling available in the literature msarms are pairs of discrete time stochastic processes one observed and one latent or hidden the hidden process is a finite state markov chain whereas the observed process given the markov chain is conditionally autoregressive the dynamics of the observed process is driven by the dynamics of the latent one so that each observation depends on the contemporary state of the markov chain by this theoretical structure msarms allow i modelling non linear and non normal time series by assuming that different autoregressions each one depending on a hidden state alternate according to the markovian regime switching ii classifying the observations into a small number of homogeneous groups labelled as the regimes of the markov chain the slow decay of the autocorrelation function acf is due to both the non linearity of the series and the automatic recording of the data at a high temporal frequency habib 2020 as pointed out by diebolt and inoue 2001 non linear time series with structural changes produce realisations that appear to have long memory so structural change and long memory are effectively different labels for the same phenomenon given that structural changes can be efficiently described by stochastic regime switching models we adopted msarms to highlight the changes in the water temperature dynamics classify the observations into a few states and fit the long memory process of the water temperature dynamics seven covariates were also incorporated into the model through both the hidden markov chain the transition probabilities are time varying and dependent on the dynamics of these exogenous variables and the observed process the state dependent exogenous variables are added to the past observations thus we have time varying means and autocovariances and hence a non stationary model the covariates are river flow air temperature rainfall wind speed wind direction radiation and soil temperature the data set is also characterised by periodicities the hourly temperatures vary according to the dynamics of the year and of the 24 h of the day hence both an annual and a state dependent daily harmonic component are added to the observed process in this paper msarms are proposed within the bayesian framework bayesian inference model choice and variable selection are performed numerically by markov chain monte carlo mcmc algorithms the basic scheme for parameter estimation in the observed process is gibbs sampling which also allows both restoration of the missing values occurring within the series of observations and reconstruction of the sequence of hidden states two random walk metropolis moves are used to estimate the parameters of the hidden markov chain adding extra steps with no further coding activity to the basic metropolis within gibbs scheme we can also compute the marginal likelihood of the various competing models through the mcmc sample this procedure enables us to select the best model within a set of models varying for the number of hidden states and the order of the autoregressive processes the exogenous deterministic variables appearing in the observed process may be different in any state and they may be different from those affecting the transition probabilities the transition matrix is affected by two sets of covariates possibly different from each other and different from those in the observed process one for the transitions from a lower to a higher state and another for the transition from a higher to a lower state the selection of the covariates appearing in each state dependent autoregression and in the two triangles of the transition matrix is performed stochastically through an mcmc algorithm several approaches have been introduced in the bayesian literature for selecting explanatory variables in regression models an exhaustive review on bayesian variable selection can be found in o hara and sillanpää 2009 the most intuitive approach is to associate each covariate with a binary indicator when the indicator equals one the corresponding variable is included in the model whereas when it equals zero the covariate is excluded among the different available methods paroli and spezia 2008 proposed the metropolised kuo mallick mkmk method based on the metropolisation of the algorithm of kuo and mallick 1998 they demonstrated in the case of non homogeneous hidden markov models and msarms with covariates that the mkmk method improves the performance of the competing techniques especially when the explanatory variables are strongly correlated and or when the complexity of the model is high in mkmk the better performance in selecting the explanatory variables is due to the acceptance in a single block of the indicators of the explanatory variables because blocking increases the precision of the estimators and improves the mixing of the mcmc algorithm therefore we propose msarms to the hydrological community to model complex time series i e non linear non normal non stationary and with a long memory within the bayesian paradigm a water temperature time series along with a few covariates are used to illustrate the methodology the application is based on the implementation and the application of mcmc algorithms which stochastically enable model choice variable selection fitting of the actual data classification of the observations into a few regimes and reconstruction of the missing values the structure of the paper is as follows the study site the instruments that automatically recorded the data and the available time series are described in section 2 the msarm used to analyse the sequence of hourly water temperature observations under the bayesian paradigm is illustrated in section 3 all results from the water temperature application are illustrated in section 4 a discussion and some conclusions finalise the paper the mcmc algorithms implemented for bayesian inference variable selection and model choice are presented in the annexed auxiliary material where further tables and figures are also available 2 study site and data the gairn catchment is a 147 km2 subcatchment of the river dee in north east scotland coordinates of the sampling point 320272 easting and 801792 northing according to the british national grid see map in fig 1 the mean altitude of the catchment is 560 m above sea level and ranges from 220 m at the catchment outlet to 1150 m at the western watershed divide the climate is temperate the long term mean air temperature recorded ca 10 km west of the catchment outlet is 6 8 c standard reference period 1981 2010 january mean 1 5 c july mean 13 5 c the long term mean total annual precipitation is 1010 mm reference period 1981 2010 highest precipitation in october 148 mm and lowest in june 75 mm the catchment land use is dominated by heather 40 heather grassland 10 montane habitats 28 acid grassland 9 bog 9 and broadleaved and coniferous woodland around 1 each according to lcm 2007 morton et al 2011 river temperature data at the catchment outlet were recorded using calibrated tinytag aquatic 2tg 4100 dataloggers gemini data loggers reading accuracy 0 01 c as maximum temperature of 15 min intervals meteorological records were provided from the weather station at the culardoch experimental site britton and fisher 2007 at the southern watershed divide discharge data at the catchment outlet station invergairn were provided by the scottish environment protection agency a time series of water temperatures recorded hourly in the river gairn from 16th august 2012 to 23rd november 2017 is analysed here the length of the series is 46224 points i e 1926 days more than five years with 328 missing values 0 71 of the total number of observations the range of the series is between 0 02 c and 22 41 c the contemporary series of the hourly river flows is also available we also studied intermediate series of water temperature from 13th june 2014 to 31st august 2016 19440 observations 810 days more than two years with 209 missing values 1 08 along with three covariates flow air temperature rainfall finally a short series was considered 1200 observations 50 days with no missing values recorded from 18th august to 6th october 2012 along with seven covariates flow air temperature rainfall wind speed wind direction radiation soil temperature the length of series of the exogenous variables was limited by the need to not have missing values in these deterministic sequences this is because missing values within the covariates might bias the results of our analyses the water temperature time series is reproduced in fig 2a in order to have a better picture of the dynamics of the series the first 1600 observations are plotted as well fig 2b the two plots exhibit asymmetries supporting the application of a non linear model another major issue of the water temperature time series is the non normality that clearly emerges from the histogram of the series fig 3a the histogram shows the presence of many peaks multimodality and a relevant asymmetry the long memory of the water temperature process is noticeable in fig 3b along with the daily periodicity the slow decay of the acf with high autocorrelations also at the higher lags was assumed to be due to the regime switching process and then tackled from the non linear side diebolt and inoue 2001 the long memory process was confirmed by the results of the qu test on the de seasonalised series performed through the package longmemoryts leschinski et al 2022 the dynamics of the water temperatures was analysed by non stationary models where both the observed and the hidden process were functions of different subsets of exogenous variables stochastically selected from an initial set of seven covariates flow air temperature and soil temperature were standardised centring the variable and dividing by its standard deviation to avoid any distortion due to the magnitude of the values and or their variability on the other hand rainfall was neither centred to preserve the zero values associated with the absence of rain nor rescaled because the variance was less than one whereas radiation was in the short range of 0 and 0 62 and thus not transformed finally wind speed a positive variable and wind direction a circular variable between 0 and 2 π were mapped onto the real line between and according to the transformation of ailliot et al 2015 which preserves circularity and then the two new variables u and v were standardised i e u w i n d s p e e d c o s w i n d d i r e c t i o n v w i n d s p e e d s i n w i n d d i r e c t i o n the time series of the seven exogenous variables are shown in figures 1 and 2 of the auxiliary material along with their transformations when adopted because of the non linearity the non normality and the long memory of the series msarms are an appropriate tool to analyse the dynamics of the water temperature time series and produce an helpful interpretation of both the observed and the latent phenomena 3 markov switching autoregressive models msarms are discrete time stochastic processes y t x t such that y t is a sequence of observed events on the real line whereas x t is an unobserved finite state markov chain hidden behind the observed process the sequence y t given x t must satisfy both the order p dependence and the contemporary dependence condition each random variable y t t p 1 t depends on the p previous observations y t 1 y t p and its conditional distribution given the whole sequence of hidden states depends only on the contemporary hidden state or regime x t let s x 1 m be the state space of the hidden markov chain where γ γ j i is the m m transition matrix with γ j i p x t i x t 1 j 0 γ j i 1 for any i j s x and any t p 1 t when x t i t p 1 t i 1 m the equation describing the msarm of order m p henceforth msar m p is 1 y t μ i τ 1 p φ i τ y t τ e t where e t is a state dependent normal white noise process of variance λ i 1 with μ the vector of the m intercepts μ i φ φ i τ the matrix of the m p autoregressive coefficients τ 1 p and λ the vector of the m precisions λ i besides the state dependent autoregressions the observed process may also include both explanatory variables and periodic components let z t z t 1 z t q be a vector of q exogenous variables recorded at time t t p t 1 and z t 1 z p z t z t 1 be the matrix of all covariates recorded at the previous time with respect to the current observation thus 1 can be generalised as 2 y t μ i τ 1 p φ i τ y t τ k 1 q θ i k z t 1 k η t β t i e t with θ θ i k the matrix of the m q covariate coefficients i 1 m k 1 q in formula 2 η t is a harmonic component of periodicity 2 s η t j 1 s η 1 j cos π j t s η 2 j sin π j t s where s is the number of relevant harmonics s s with η η 1 1 η 2 1 η 1 s η 2 s β t i is a harmonic component of periodicity 2 r depending on the current state i of the markov chain β t i j 1 r β i 1 j cos π j t r β i 2 j sin π j t r where r is the number of relevant harmonics r r with β β 1 1 1 β 1 2 1 β 1 1 r β 1 2 r β m 1 r β m 2 r in the case of an annual periodicity 2 s 365 whereas for a daily cycle 2 r 24 the hidden markov chain can also be generalised by assuming the transition probabilities are time varying and hence modelled as logistic functions of the covariates γ t γ j i t γ j i t p x t i x t 1 j with 3 l o g i t γ j i t l n γ j i t γ j j t α j i 0 k 1 q α j i k z t 1 k where α α j i is the m m matrix either of the vectors α j i 0 α j i 1 α j i q if j i or the null vector if j i some of the q state dependent exogenous variables affecting the observed process might be redundant according to the state visited by the hidden markov chain redundancies in the covariates might also be present in those appearing in the time varying transition probabilities a selection of the most relevant explanatory variables is recommended for a better interpretation of both the observed and the latent phenomenon in order to perform a stochastic selection within the different sets of covariates each exogenous variable is associated with a binary indicator when the indicator equals one the corresponding variable is included in the model whereas when it equals zero the covariate is excluded let ω ω i k be the matrix of the m q indicators associated with the set of q exogenous variables i 1 m k 1 q hence 2 can be generalised as 4 y t μ i τ 1 p φ i τ y t τ k 1 q θ i k ω i k z t 1 k η t β t i e t further let δ k k 1 q be the vector of indicators associated with the set of q covariates affecting the transitions from a higher state to a lower one lower triangle of the transition matrix and ψ k k 1 q be the vector of indicators associated with the set of q covariates affecting the transitions from a lower state to a higher one higher triangle of the transition matrix therefore 3 can be generalised as l o g i t γ j i t α j i 0 k 1 q α j i k δ k z t 1 k i j i k 1 q α j i k ψ k z t 1 k i j i where i a is the indicator function which assumes the value of one when a is true and zero otherwise the vector of the unknown parameters of a msar m p is μ λ φ θ η β α ω δ ψ x t y m i s s where x t x p 1 x t is the sequence of hidden states and y m i s s the set of all missing values occurring within y t y 1 y t in our application y t is the sequence of the water temperatures with q 1 when n 46 224 q 3 when n 19 440 q 7 when n 1200 r and s are both equal to one z t 1 is the sequence of the standardised flows z t 2 is the sequence of the standardised air temperatures z t 3 is the sequence of the rainfalls z t 4 and z t 5 are the sequences of the standardised transformations u and v of the wind speeds and directions z t 6 is the sequences of the solar radiations and z t 7 is the sequence of the standardised soil temperatures from 4 the generic distribution of y t given the p past observations and the current hidden state x t i is conditionally normal with mean μ i τ 1 p φ i τ y t τ k 1 q θ i k ω i k z t 1 k η t β t i and precision λ i whereas the marginal distribution of each observation is a mixture of normals whose mixing distribution is given by the probability of the markov chain to visit the states at that time the mixing distribution can be obtained as the left eigenvector of the transition matrix associated with the unit eigenvalue δ t δ t γ t where δ t δ 1 t δ m t with δ i t p x t i for any i s x and t p 1 t for a better interpretability of the results we assume the hidden regimes persist for all of the 24 h of the day therefore it can serve our purpose to replace the time t subscript with the day d and hour h subscripts so that t d 1 24 h where d 1 d t 24 and h 1 24 and to assume x d 1 24 1 x d 1 24 h x 24 d for any d 1 d x d x 1 x d thus the time varying transition probabilities are computed as functions of the covariates recorded at 12 am l o g i t γ j i d α j i 0 k 1 q α j i k δ k z d 2 24 12 k i j i k 1 q α j i k ψ k z d 2 24 12 k i j i with d 2 d for our bayesian inference we set the following independent priors all μ i φ i τ θ i k and α j i k are vague normals of mean 0 and variance 10 vectors η h u and β i h v are multivariate normals mvns whose mean is the null vector and covariance matrix is a diagonal matrix with all diagonal entries equal to 10 all λ i are non informative gamma of parameters 0 01 whose mean is 1 and variance 100 all ω i k δ k and ψ k are bernoulli of parameter 0 5 a priori the probability of including a covariate is equal to the probability of its exclusion all missing values are uniform over the range of y t j i 1 m τ 1 p k 0 q h 1 2 u 1 s v 1 r model 1 and hence models 2 and 4 are unidentifiable in data fitting when we have m states we have m ways to label them when the priors are invariant to the different state permutation m different models are interchangeable by permuting their labelling this is the so called label switching problem see among others mcculloch and tsay 1994 frühwirth schnatter 2001 spezia 2009 and it can be overcome by placing some parameters in increasing or decreasing order experience birkel et al 2012 spezia et al 2021 suggests that in hydrological time series hidden states can be ordered by increasing levels of variability thus this constraint is included in the prior 5 p λ m i 1 m λ i i λ 1 λ 2 λ m 4 results the dynamics of the water temperature time series fig 2 was analysed by applying the msarms described in section 3 and the mcmc algorithms fully described in the auxiliary material we considered three sequences of data i the whole time series along with the standardised river flow as an exogenous variable t 46 224 q 1 ii an intermediate series along with standardised flow standardised air temperature and rainfall as exogenous variables t 19 440 q 3 iii a short series along with standardised flow standardised air temperature rainfall radiation standardised soil temperature and standardised variables u and v as exogenous variables t 1200 q 7 representing wind speed and wind direction according to the transformation of ailliot et al 2015 described in section 2 the seven dynamic covariates are reproduced in figures 1 and 2 of the auxiliary material we only worked with sequences of covariates with no missing values to not bias the results of our analyses this explains why we had three series of different lengths and different numbers of covariates for each of the three series the bayesian analysis was developed in four steps variable selection model choice prior coherence and parameter estimation for the whole the mcmc algorithms were run for 100000 iterations after a burn in of 1000 iterations thinning the sample at every 10th iterations in all steps for the intermediate series 250000 iterations after a burn in of 10000 iterations thinning at every 25th iterations for the short 500000 iterations after a burn in of 10000 iterations thinning at every 50th iterations when computing the marginal likelihood the number of additional iterations for each parameter were a fifth of those of the sample 4 1 variable selection and model choice for the whole series we considered 21 alternative models for any number of hidden states m 1 2 3 and autoregressive order p 0 1 6 models with m 3 were discarded because they always produced empty regimes that is only three regimes were visited by the hidden markov chain within the sequence of observations we have 328 missing values gathered in three large groups of 119 138 and 71 consecutive points missing observations could be filled by simulated values that allowed analysing the whole series as a single block for each of these competing models we stochastically selected if the single covariate we had flow had influenced both the observations and the time varying transition probabilities the results of the selection are reported in table 1 the best model is that with three hidden states and autoregressive order equal to five i e five hours flow was selected in the observed process for the first two regimes but not the third while it was not selected in the unobserved process that is the hidden markov chain was homogeneous this means that flow is an important driver of river temperature in two states but not in the third state when it is the only available covariate and that all states might be driven by another important set of unobserved variables represented by the latent process for the intermediate series we considered again 21 alternative models for any m 1 2 3 and p 0 1 6 having empty regimes for m 3 within the sequence of observations we have 209 missing values gathered in two large groups of 138 and 71 consecutive points that could be filled by simulated values for each of these competing models we stochastically selected if the three available covariates flow air temperature rainfall had influenced both the observations and the time varying transition probabilities the results of the selection are reported in table 2 the best model is that with three hidden states and autoregressive order equal to six i e six hours we did not try any larger autoregressive order because six was demonstrated to be sufficient air temperature was always selected in both the observed and the hidden process while flow was selected for the second state of the observed process only for the short series we considered 14 alternative models for any m 1 2 and p 0 1 6 having empty regimes for m 2 for each of these competing models we stochastically selected the covariates within a set of seven flow air temperature rainfall transformed wind speed transformed wind direction radiation soil temperature the results of the selection are reported in table 3 the best model is that with no hidden states m 1 and autoregressive order equal to three i e three hours air temperature radiation and soil temperature were selected in the observed process the hidden chain is not involved in this model 4 2 prior coherence once the best model was available then we needed to check the coherence of the constraint on the precisions in fact as mentioned in section 3 an identifiability constraint was included in the prior based on previous knowledge in hydrological time series analysis experience suggested to place the precisions in decreasing order i e increasing levels of the state dependent variability before moving to parameter estimation we needed to check if the constraint on the precisions was representative of the data and respected the geometry of the posterior density thus we plotted the values of the intercepts and the precisions sampled from the unconstrained posterior frühwirth schnatter 2001 from figure 3 in the auxiliary material it is evident that we had three separated groups of precisions for the whole series that can be ordered decreasingly while there is no separation between the intercepts groups looking at figure 4 in the auxiliary material we came to the same conclusions for the intermediate series this operation was not necessary for the short series because we have one regime only 4 3 parameter estimation after the dimensions of the model were chosen the exogenous variable selected and the identifiability constraint checked the whole set of parameters for the whole the intermediate and the short series could be estimated first the estimates for the whole series were computed see table 1 of the auxiliary material the flow in the first two states of the observed process have a coefficient very close to zero while it was excluded in the third state also the annual periodicity has coefficients very close to zero that means that the annual cycle could be explained sufficiently well by the switching dynamics of the water temperature the transition matrix of the homogeneous hidden markov chain the flow was not selected in the latent process is 0 772 0 210 0 018 0 214 0 718 0 068 0 136 0 633 0 231 the permanence probabilities of the three hidden states i e 0 772 0 718 and 0 231 decrease as the labels of the state i e 1 2 3 increase as a consequence the corresponding mean time spent in a state decreases as well 4 39 3 55 and 1 30 days the mean time spent in each state has a geometric distribution of parameter the reciprocal of the complement to one of the corresponding diagonal probability we could also reconstruct the sequence of the hidden states by computing the posterior mode of x d over the mcmc sample fig 4a the markov chain shows an annual dynamics which anticipates the annual dynamics of the water temperatures the hidden markov chain visited states 1 2 and 3 a total of 914 times 912 times 100 times respectively given the latent regimes the observations were classified into the three hidden states with increasing levels of variability fig 5 state 1 is mostly associated with observations in the decreasing part and the bottom of the cycle state 2 mostly with the increasing part the top and the beginning of the decreasing part of the cycle and state 3 mostly with the increasing part of the cycle the model fit was very satisfactory as shown by the comparison of actual and fitted values see the whole series in fig 6a and the two sub series of 1000 observations in figures 5a and 5b in the auxiliary material the model performance was assessed by the root mean squared error r m s e and the mean absolute error m a e which are very low they are 0 137 0 6 of the range of the data and 0 072 0 3 respectively all observations were within the 99 credibility interval one of the major issues encountered in previous studies was the modelling of the long memory process by using msarms with uncorrelated noises this study was able to explain the slow decay of the autocorrelations in terms of stochastic regime switching confirming what was stated by diebolt and inoue 2001 the non linearity of the water temperature time series was well captured by the dynamics of the hidden states and the very low level of correlation in the residuals is evident figures 6a and 6b in the auxiliary material normality of the various series of state dependent residuals figures 7a 7b 7c in the auxiliary material was also verified through the shapiro wilk and jarque bera normality tests although the acf decreases close to zero at the first lag figures 6a and 6b in the auxiliary material show the presence of weak daily and annual patterns in the acf dynamics to remove these periodicities we added a few features to eq 2 twelve alternative models with m 3 and p 5 were tried we considered four models adding observations at distant lags i e y t 24 y t 24 y t 48 y t 24 y t 48 y t 72 and y t 24 y t 48 y t 72 y t 96 the same four models were run again with two or four harmonic components in both the daily and the annual periodicity there was no increase in the marginal likelihoods and the periodicities in the acfs were not removed by adding extra complexity in the model this fact suggested that these patterns might be removed by including some periodic covariates in the observed process as we tried both for the intermediate and the short series next the full set of parameters of the intermediate series was estimated the estimates are reproduced in table 2 of the auxiliary material again the annual periodicity has coefficients very close to zero this indicates that the annual fluctuations were mostly explained by the autoregressions and the covariates in order to have a clearer picture we also computed the mean transition matrix between the three states the mean of the 809 time varying transition matrices 0 674 0 308 0 018 0 288 0 630 0 082 0 482 0 257 0 261 the permanence probabilities i e 0 674 0 630 and 0 261 decrease as the labels of the state i e 1 2 3 increase and the corresponding hypothetical mean time spent in a state decreases as well 3 07 2 70 and 1 35 days after that we reconstructed the sequence of the hidden states by computing the posterior mode of x d over the mcmc sample fig 4b the markov chain shows again an annual dynamics which anticipates the annual dynamics of the water temperatures the estimated hidden states in this series are the same of those obtained in the whole series during the same time interval of the intermediate series apart from a small number of days 4 due to the different covariates the hidden markov chain visited states 1 2 and 3 a total of 377 days 376 days 57 days respectively given the latent regimes the observations were classified into the three hidden states with increasing levels of variability fig 7 again state 1 is mostly associated with observations in the decreasing part and the bottom of the cycle state 2 mostly with the increasing part the top and the beginning of the decreasing part of the cycle and state 3 mostly with the increasing part of the cycle the model fit was very satisfactory as shown by the comparison of actual and fitted values see the whole series in figure 6b and two sub series of 1000 observations in figure 5 of the auxiliary material the r m s e and m a e are very low 0 131 0 6 of the range of the data and 0 074 0 3 respectively all observations were within the 99 credibility interval the non linearity of the water temperature time series was well captured by the switching dynamics of the hidden states and the very low level of correlation in the residuals is evident figures 6a and 6b in the auxiliary material normality of the various series of state dependent residuals figures 7d 7e 7f in the auxiliary material was also verified through the shapiro wilk and jarque bera normality tests the acf decreases close to zero at the first lag figures 6a and 6b in the auxiliary material but weak daily and annual patterns are present in the acf dynamics twelve alternative models with m 3 and p 5 were tried to remove these periodicities from the residuals we considered four models adding observations at distant lags i e y t 24 y t 24 y t 48 y t 24 y t 48 y t 72 and y t 24 y t 48 y t 72 y t 96 to eq 2 the same four models were run again with two and four harmonic components in both the daily and the annual periodicity although there was an increase in the marginal likelihoods the periodicities in the acfs were not removed and the extra complexity added to the model was not beneficial the inclusion in the model of the whole set of seven covariates might be helpful in the analysis of the short series to remove the weak periodicities from the residuals finally the whole set of parameters of the short series was estimated the estimates are reproduced in table 3 of the auxiliary material the model fit was very satisfactory as shown by the comparison of actual and fitted values see the whole series in figure 7c and two sub series of 100 observations in figures 5e and 5f of the auxiliary material the r m s e and m a e are very low 0 098 0 7 of the range of the data and 0 068 0 5 respectively all observations were within the 99 credibility interval the residuals were normal and non correlated normality figure 7g in the auxiliary material was also verified through the shapiro wilk and jarque bera normality tests the acf decreases close to zero from the first lag figures 6a and 6b in the auxiliary material and does not present any periodic pattern this is due to the inclusion of the solar radiation and the soil temperature in the observed process it would be very interesting to verify this result on a longer multi state series 5 discussion and conclusions we demonstrated that msarms are a useful tool to reconstruct high frequency environmental time series with exogenous variables a fully bayesian model was developed and we applied our methodology to study the dynamic evolution of hourly water temperature in the river gairn scotland it is evident that the complexity of the series non linearity non normality non stationarity long memory was efficiently tackled by our msarms the computational machinery we used is based on mcmc algorithms of the metropolis within gibbs type they allowed us to perform bayesian model choice variable selection and inference including the reconstruction of the hidden markov chain and the classification of the observations into a finite number of regimes as well as the generation of the missing values the classification of the observations followed the increasing variability constraint as set in the prior formula 5 to tackle the identifiability problem of the model and confirmed by figure 3 of the auxiliary material the same algorithms also allowed us to choose the best model i e the number of hidden states and the autoregressive order among many alternatives the choice was done by computing the logarithm of the marginal likelihood the choice of the bayesian approach and the implementation of mcmc algorithms give clear practical advantages handling the missing values as unknown parameters allows restoring these observations after placing a prior on them further it is possible to stochastically select the covariates that affect both the observed and the hidden process this can be done in a single run of the algorithm and not repeatedly by removing a covariate at the time such as in the stepwise regression finally the reconstruction of the sequence of the hidden states is obtained in a single block rather than for each individual state the flexibility of the msarms was demonstrated by the three applications we considered for the whole series with a single covariate the best model had three hidden states and autoregressions of the fifth order thus the non linear model m 3 worked better than the corresponding linear model m 1 flow was relevant in the observed process for two states only while it was not selected in the hidden process and the markov chain was homogeneous for the intermediate series with three covariates the best model had three hidden states and autoregressions of the sixth order again the non linear model m 3 worked better than the corresponding linear model m 1 flow was relevant in the observed process for one state only while air temperature was always selected both in the observed and the hidden process the markov chain was non homogeneous and the transition probabilities time varying for the short series with seven covariates we obtained that the best model was the linear autoregression of the sixth order without a hidden markov chain m 1 air temperature solar radiation and soil temperature were the relevant variables to explain the water temperature dynamics thus discharge is a proxy for water temperature modelling when no other more directly related variables are available in those situations the latent states will help to model the long term dynamics in the absence of true predictors with a physical meaning as we saw in the first two applications the hidden regimes can have an interpretation related to the seasonality in fact the markov chain shows an annual dynamics which anticipates the annual dynamics of the water temperatures the observations in the decreasing part and the bottom of the cycle are those with the lowest variability and associated with state 1 those in the increasing part the top and the beginning of the decreasing part of the cycle have a higher level of variability and are associated with state 2 finally in state 3 we have the observations with the highest level of variability and they are mostly in the increasing part of the cycle it is not surprising that for the short series 50 days i e no annual periodicity the model is not multi state it would be interesting to see what happens when considering the seven covariates on longer series that is if the same covariates are selected in a non linear model m 1 in this work we considered only series of covariates with no missing values thus we handled 50 day long series when working with seven covariates 810 day long series with three covariates and a 1926 day long series with a single covariate this because we did not want that our analyses were biased by the missing values within the exogenous variables in the future we are interested in dealing with missing values in the deterministic sequences by either considering physical projections of these variables if available or reconstructing the missing values after assuming the exogenous variables as stochastic msarms can be extended in the future to apply them more straightforwardly in fully gibbs sampling algorithms without the tuning factors of the random walk metropolis moves in fact in the most general case the hidden markov chain is non homogeneous and the transition probabilities depends on the exogenous variables via multinomial logit link functions see formula 3 a few alternative auxiliary variable methods were developed by holmes and held 2006 frühwirth schnatter and frühwirth 2007 and polson et al 2013 to perform bayesian inference in multinomial logit models for independent data via the gibbs sampling these data augmentation techniques were applied to non homogeneous hidden markov chain by meligkotsidou and dellaportas 2011 kaufmann 2015 holsclaw et al 2017 and koki et al 2020 a systematic comparison via simulation studies of the different auxiliary variable techniques based on the gibbs sampling is needed in the light of data augmentation methods applied in the stochastic variable selection under conditions of non stationarity this approach may be better suited for simulation of future changes under climate change stein 2020 following lu and berliner 1999 and gelati et al 2010 as it may be better suited to simulate future projections under conditions of non stationarity meligkotsidou and dellaportas 2011 koki et al 2020 projecting beyond observed periods is a known problem this approach might be more robust in that sense if projections in the future of the covariates are available to conclude we believe that we have demonstrated a valuable approach to explain the hourly variability of the water temperature in the river gairn and this can be used to reconstruct complex long term environmental time series and may help to further a better understanding of the impacts of climatic changes on the hydrology and the ecology of a catchment our study provides a novel application of the suitability of the msarms in hydrological time series analysis and environmental sciences in general we hope our work can motivate other scientists to approach msarms and give their highly structured time series exhibiting non linearity non normality non stationarity long memory a valuable interpretation modelling river temperature from covariates gives insight into physical processes governing river temperature dynamics air temperature is identified as the most important covariate as in many other river temperature studies showing that river and air temperature respond to thermal inputs in a relatively similar way johnson et al 2013 also solar radiation representing direct thermal input and soil temperature representing warming of the catchment are important this is consistent with other studies including day length related to solar radiation and thus thermal input and cumulative air temperature representing warming of the catchment to simulate river temperature e g pohle et al 2019 loerke et al 2022 however also flow plays a role during the summer state 2 showing the importance of hydrology rainfall runoff processes on river temperature dynamics during the biologically relevant summer low flow periods as also found by e g arora et al 2016 over the past decades scientific understanding in hydrology has been greatly aided by long term data rainfall flow with high temporal and spatial resolution however water quality observations are typically collected at lower temporal resolution leading to higher uncertainties in models and limitations in process understanding kirchner 2006 this is especially the case in upland areas which are disproportionally important for water supply but also riverine water quality e g alexander et al 2017 bol et al 2018 yet often lack long term monitoring data meanwhile high resolution water quality data from sensors can be a powerful line of evidence to facilitate stakeholder buy in and adaptive management however collecting high quality sensor data in real world river catchments is challenging as sensors are susceptible to fouling extreme environmental events and occasional malfunction again upland catchments are more prone to these challenges also due to limited accessibility hence processing of a large quantities of high resolution data is challenging and requires rigorous data quality control identification of spurious data points and filling of gaps ideally in an objective automated way rather than manual inspection to our knowledge this work demonstrates the first application of msarms to the analysis of river water temperature data and provides the first example of msarms applied to the analysis of high frequency hydrological data by performing inference variable selection and model choice under the bayesian paradigm the high performance of msarms compared to other river temperature modelling approaches such as gams jackson et al 2018 pohle et al 2019 highlights the potential of the msarm modelling approach for river temperature this can also be explained by river temperature time series exhibiting generally comparable properties as air temperature for which msarms have been successfully applied e g by monbet and ailliot 2017 the general modelling approach is transferable to other catchments the model predicting river temperature from covariates can be used to reconstruct high resolution river temperature for the past based on covariates but also for quality control when applied to weather forecast or climate scenarios rather than observed covariates the model can be used to simulate high resolution river temperature in short and long term futures declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was funded by the scottish government s rural and environment science and analytical services division uk comments from katherine whyte and two anonymous referees improved the quality of the final paper appendix a supplementary data supplementary material related to this article can be found online at https doi org 10 1016 j envsoft 2023 105751 appendix a supplementary data the following is the supplementary material related to this article mmc s1 
